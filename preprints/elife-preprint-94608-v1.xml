<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">94608</article-id>
<article-id pub-id-type="doi">10.7554/eLife.94608</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.94608.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Perceptual error based on Bayesian cue combination drives implicit motor adaptation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4192-4088</contrib-id>
<name>
<surname>Zhang</surname>
<given-names>Zhaoran</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Huijun</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Tianyang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Nie</surname>
<given-names>Zixuan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5098-3808</contrib-id>
<name>
<surname>Wei</surname>
<given-names>Kunlin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>School of Psychological and Cognitive Sciences, Peking University</institution>, Beijing, <country>China</country></aff>
<aff id="a2"><label>2</label><institution>Beijing Key Laboratory of Behavior and Mental Health</institution>, Beijing, <country>China</country></aff>
<aff id="a3"><label>3</label><institution>Peking-Tsinghua Center for Life Sciences, Peking University</institution>, Beijing, <country>China</country></aff>
<aff id="a4"><label>4</label><institution>National Key Laboratory of General Artificial Intelligence</institution>, Beijing, <country>China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Orban de Xivry</surname>
<given-names>Jean-Jacques</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>KU Leuven</institution>
</institution-wrap>
<city>Leuven</city>
<country>Belgium</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Makin</surname>
<given-names>Tamar R</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence to: Kunlin Wei, School of Psychological and Cognitive Sciences Peking University, 5 Yiheyuan Road, Beijing, 100871, China <email>wei.kunlin@pku.edu.cn</email></corresp>
<fn id="n1"><label><bold><sup>†</sup></bold></label><p>Zhaoran Zhang and Huijun Wang are co-first authors.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-01-26">
<day>26</day>
<month>01</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP94608</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-11-23">
<day>23</day>
<month>11</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-11-23">
<day>23</day>
<month>11</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.11.23.568442"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Zhang et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Zhang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-94608-v1.pdf"/>
<abstract>
<title>Abstract</title><p>The sensorimotor system can recalibrate itself without our conscious awareness, a type of procedural learning whose computational mechanism remains undefined. Recent findings on implicit motor adaptation, such as over-learning from minor perturbations and swift saturation for increasing perturbation size, challenge existing theories based on sensory errors. We argue that perceptual error, arising from the optimal combination of movement-related cues, is the primary driver of implicit adaptation. Central to our theory is the linear relationship between the sensory uncertainty of visual cues and perturbation, validated through perceptual psychophysics (Experiment 1). Our theory predicts diverse features of implicit adaptation across a spectrum of perturbation conditions on trial-by-trial basis (Experiment 2) and explains proprioception changes and their relation to visual perturbation (Experiment 3). By altering visual uncertainty in perturbation, we induced unique adaptation responses (Experiment 4). Overall, our perceptual error framework outperforms existing models, suggesting that Bayesian cue integration underpins the sensorimotor system’s implicit adaptation.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>To achieve and sustain effective motor performance, humans consistently recalibrate their sensorimotor systems to adapt to both internal and external environmental disturbances (<xref ref-type="bibr" rid="c5">Berniker &amp; Kording, 2008</xref>; <xref ref-type="bibr" rid="c46">Shadmehr et al., 2010</xref>; <xref ref-type="bibr" rid="c62">Wolpert et al., 2011</xref>). For instance, transitioning to a high-sensitivity gaming mouse, which drives cursor movement at an accelerated rate compared to a standard computer mouse, may initially result in decreased performance in computer-related tasks. However, humans are capable of rapidly adapting to this new visuomotor mapping within a short period of time. While conscious corrections can facilitate this adaptation process, our sensorimotor system often times adapts itself implicitly without our conscious efforts (<xref ref-type="bibr" rid="c2">Albert et al., 2021</xref>; <xref ref-type="bibr" rid="c24">Krakauer et al., 2019</xref>).</p>
<p>While recent research has intensively examined the interplay between explicit and implicit learning systems (<xref ref-type="bibr" rid="c1">Albert et al., 2022</xref>; <xref ref-type="bibr" rid="c32">Miyamoto et al., 2020</xref>), several characteristics of implicit motor adaptation have emerged that challenge traditional theories. Conventionally, motor adaptation is conceptualized as error-based learning, in which learning accrues in proportion to the motor error experienced (<xref ref-type="bibr" rid="c9">Cheng &amp; Sabes, 2006</xref>; <xref ref-type="bibr" rid="c12">Donchin et al., 2003</xref>; <xref ref-type="bibr" rid="c53">Thoroughman &amp; Shadmehr, 2000</xref>). However, implicit adaptation exhibits an overcompensation phenomenon where the extent of adaptation surpasses the error induced by visual perturbations (<xref ref-type="bibr" rid="c19">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="c34">Morehead et al., 2017</xref>). Additionally, implicit adaptation manifests a saturation effect; it increases with perturbations but plateaus across a broad range of larger perturbations (<xref ref-type="bibr" rid="c7">Bond &amp; Taylor, 2015</xref>; <xref ref-type="bibr" rid="c19">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="c34">Morehead et al., 2017</xref>; <xref ref-type="bibr" rid="c36">Neville &amp; Cressman, 2018</xref>). These observations of overcompensation and saturation are incongruent with prevailing state-space updating models, which presuppose that incremental learning constitutes only a fraction of the motor error (<xref ref-type="bibr" rid="c30">McDougle et al., 2015</xref>; <xref ref-type="bibr" rid="c48">Smith et al., 2006</xref>). Another aspect of implicit adaptation that remains mechanistically unexplained pertains to its impact on proprioception. In traditional motor adaptation, proprioception is biased towards the visual perturbation, maintaining a stable bias throughout the adaptation process (<xref ref-type="bibr" rid="c40">Ruttle et al., 2016</xref>, <xref ref-type="bibr" rid="c42">2021</xref>). In contrast, implicit adaptation initially biases proprioceptive localization of the hand towards the visual perturbation, but this bias gradually drifts in the opposite direction over time (<xref ref-type="bibr" rid="c58">Tsay et al., 2020</xref>).</p>
<p>Causal inference of motor errors has been suggested to explain the discounting of large perturbations (<xref ref-type="bibr" rid="c60">Wei &amp; Körding, 2009</xref>). However, the causal inference account predicts a decline in adaptation with increasing perturbation, diverging from the observed ramp-like saturation effect. (<xref ref-type="bibr" rid="c55">Tsay, Kim, et al., 2022</xref>) recently synthesized existing evidence to propose that implicit adaptation reaches an upper bound set by cerebellar error correction mechanisms, reflected in a ramp-like influence of vision on proprioception (<xref ref-type="bibr" rid="c55">Tsay, Kim, et al., 2022</xref>). While this ramp function could explain the observed saturation, the postulate of an upper bound on visual influence lacks empirical validation. Some research supports the idea of saturation in proprioceptive recalibration (<xref ref-type="bibr" rid="c33">Modchalingam et al., 2019</xref>), yet other studies suggest a linear increase with visual perturbations (<xref ref-type="bibr" rid="c39">Rossi et al., 2021</xref>; <xref ref-type="bibr" rid="c43">Salomonczyk et al., 2011</xref>). Additionally, current models fall short of quantitatively capturing the time-dependent shifts in proprioceptive bias associated with implicit adaptation.</p>
<p>In this study, we put forth a unified model that aims to account for the distinct features of implicit adaptation, based on the Bayesian combination of movement-related cues. Prior models have overlooked the fact that visual uncertainty related to the perturbation increases with the size of the perturbation as the cursor moves further from the point of fixation and into the visual periphery (<xref ref-type="bibr" rid="c21">Klein &amp; Levi, 1987</xref>; <xref ref-type="bibr" rid="c27">Levi et al., 1987</xref>). This is particularly pertinent for implicit adaptation that is widely investigated by the so-called error-clamp paradigm, in which participants are instructed to fixate on the target and disregard the perturbing cursor. Moreover, conventional theories of motor adaptation attribute motor error to the sensory modality of the perturbation, i.e., visual errors for visual perturbations (<xref ref-type="bibr" rid="c55">Tsay, Kim, et al., 2022</xref>; <xref ref-type="bibr" rid="c60">Wei &amp; Körding, 2009</xref>). We propose an alternative: perceptual error drives implicit adaptation, as the perturbed sensory feedback influences the perception of the effector and, subsequently, motor adaptation. Through a series of experiments, we aim to demonstrate that combining eccentricity-induced visual uncertainty (Experiment 1) with a traditional motor adaptation model (state-space model) and a classical perception model (Bayesian cue combination) can explain both over-compensation and saturation effects (Experiment 2), as well as the time-dependent changes in proprioceptive bias (<xref ref-type="bibr" rid="c58">Tsay et al., 2020</xref>). Finally, to provide causal evidence supporting our Perceptual Error Adaptation (PEA) model, we manipulated visual uncertainty and observed that subsequent adaptation was attenuated for large perturbations but not for small ones—a finding that contradicts existing models but aligns well with the PEA model. Across the board, our model outperforms those based on ramp error-correction (<xref ref-type="bibr" rid="c55">Tsay, Kim, et al., 2022</xref>) and causal inference of errors (<xref ref-type="bibr" rid="c60">Wei &amp; Körding, 2009</xref>), offering a more parsimonious explanation for the salient features of implicit adaptation.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>The perceptual error adaptation model with varying visual uncertainty</title>
<p>We start by acknowledging that the perceptual estimation of effector position is dynamically updated and influenced by sensory perturbations during motor adaptation. For implicit adaptation studied via the error-clamp paradigm, participants are required to bring their hand to the target while ignoring the direction-clamped cursor (<xref ref-type="bibr" rid="c34">Morehead et al., 2017</xref>). Accordingly, the perceptual estimation of the hand movement direction relies on three noisy sensory cues: the visual cue from the cursor, the proprioceptive cue from the hand, and the sensory prediction of the reaching action (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). Without loss of generality, we posit that each cue is governed by an independent Gaussian distribution: the visual cue <italic>x</italic><sub><italic>v</italic></sub> follows <inline-formula><inline-graphic xlink:href="568442v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>μ</italic> is the cursor direction and <inline-formula><inline-graphic xlink:href="568442v1_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is visual variance, the proprioceptive cue <italic>x</italic><sub><italic>p</italic></sub> follows <inline-formula><inline-graphic xlink:href="568442v1_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>x</italic><sub>ℎ<italic>and</italic></sub> is the hand movement direction and <inline-formula><inline-graphic xlink:href="568442v1_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is proprioceptive variance, and the sensory prediction cue <italic>x</italic><sub>u</sub> follows <inline-formula><inline-graphic xlink:href="568442v1_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>T</italic> is the target direction and <inline-formula><inline-graphic xlink:href="568442v1_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is prediction variance. Participants aim for the target, expecting their hand to reach it. Using the Bayesian cue combination framework (<xref ref-type="bibr" rid="c6">Berniker &amp; Kording, 2011</xref>), the perceived hand location <inline-formula><inline-graphic xlink:href="568442v1_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> on trial <italic>n</italic> can be derived:
<disp-formula id="eqn1">
<graphic xlink:href="568442v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>This estimated hand position is derived using maximum likelihood estimation from the three noisy cues. Given that the clamped cursor deviates the target by <italic>μ</italic>, the visual cue <italic>x</italic><sub><italic>v</italic></sub> biases the hand estimate <italic>x̂</italic><sub><italic>Hand</italic></sub> towards the cursor’s direction. This deviation from the target direction <italic>T</italic> constitutes the perceptual error, which drives adaptation on the subsequent trial <italic>n+1</italic> (<xref rid="eqn2" ref-type="disp-formula">Eq. 2</xref>). Consisting with existing models (<xref ref-type="bibr" rid="c1">Albert et al., 2022</xref>; <xref ref-type="bibr" rid="c9">Cheng &amp; Sabes, 2006</xref>; <xref ref-type="bibr" rid="c15">Herzfeld et al., 2014</xref>; <xref ref-type="bibr" rid="c30">McDougle et al., 2015</xref>), trial-to-trial adaptation is modeled using a state-space equation:
<disp-formula id="eqn2">
<graphic xlink:href="568442v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>A</italic> is the retention rate capturing inter-movement forgetting and <italic>B</italic> is the learning rate capturing the proportion of error corrected within a trial. The interplay between forgetting and learning dictates the overall learning extent, i.e., the asymptote of <italic>x</italic><sub><italic>p</italic></sub>:
<disp-formula id="eqn3">
<graphic xlink:href="568442v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Thus, the positive influence of perturbation size <italic>μ</italic> on the adaptation extent is counterbalanced by the rise in visual uncertainty <italic>σ</italic><sub><italic>v</italic></sub>, since sensory uncertainty of various visual stimuli increases linearly with eccentricity (<xref ref-type="bibr" rid="c21">Klein &amp; Levi, 1987</xref>; <xref ref-type="bibr" rid="c27">Levi et al., 1987</xref>). As participants are instructed to fixate on the target, an increase in <italic>μ</italic> lead to increased eccentricity. Hence, we model this linear increase in visual uncertainty by
<disp-formula id="eqn4">
<graphic xlink:href="568442v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>a</italic> and <italic>b</italic> are free parameters. We conducted simulations of implicit adaptation with varying error clamp size (<italic>μ</italic>). The model simulation closely resembles the saturated adaptation in three independent experiments (<xref ref-type="bibr" rid="c19">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="c34">Morehead et al., 2017</xref>). In fact, our PEA model predicts a concave adaptation pattern, contrasting with the ramp pattern suggested by the PReMo model (<xref ref-type="bibr" rid="c55">Tsay, Kim, et al., 2022</xref>). In Experiment 1, we aim to validate the assumption of a linear increase in visual uncertainty (<xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref>); in Experiment 2, we seek to verify whether implicit adaptation adheres to a concave pattern as prescribed by the PEA model. Subsequent experiments, namely Experiments 3 and 4, will test the model’s additional novel predictions concerning changes in proprioception and the impact of experimentally manipulated visual uncertainty on adaptation, respectively.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>The Perceptual Error Adaptation (PEA) model for error-clamp adaptation. <bold>(A)</bold> Illustration of involved sensorimotor cues for estimating hand direction <italic>x̂</italic><sub><italic>Hand</italic></sub>. The clamped cursor, the hand, and the sensory prediction of the reaching action provide the visual (<italic>x</italic><sub><italic>v</italic></sub>), proprioceptive (<italic>x</italic><sub><italic>p</italic></sub>), and the sensory prediction cue (<italic>x</italic><sub><italic>u</italic></sub>) of movement direction, respectively. The hand direction estimate is assumed to be based on maximum likelihood cue combination. <bold>(B)</bold> Assuming a linear dependency of visual uncertainty on eccentricity, the PEA model predicts that implicit adaptation extent is a concave function of perturbation size <italic>μ</italic>, a pattern qualitatively aligning with previous findings (<xref ref-type="bibr" rid="c19">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="c34">Morehead et al., 2017</xref>).</p></caption>
<graphic xlink:href="568442v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Experiment 1: Visual Uncertainty Increases Linearly with Perturbation Size</title>
<p>To quantify visual uncertainty in a standard error-clamp adaptation setting, we employed psychometric methods. Occluded from seeing their actual hand, participants (n=18) made repetitive reaches to a target presented 10 cm straight head while an error-clamped cursor moving concurrently with one of three perturbation sizes (i.e., 4°, 16° and 64°), randomized trial-by-trial. In alignment with the error-clamp paradigm, participants were instructed to fixate on the target and to ignore the rotated cursor feedback. Eye-tracking confirmed compliance with these instructions (<xref ref-type="fig" rid="figs1">Figure S1</xref>). Perturbation directions were counter-balanced across trials, with equal probability of clockwise (CW) and counterclockwise (CCW) rotation. Post-movement, participants were required to judge the cursor’s rotation direction (CW or CCW) relative to a briefly displayed reference point (<xref rid="fig2" ref-type="fig">Figure 2A</xref> &amp; <xref rid="fig6" ref-type="fig">Figure 6A</xref>). Employing this two-alternative forced-choice (2AFC) task and the Parameter Estimation by Sequential Testing (PEST) procedure (<xref ref-type="bibr" rid="c28">Lieberman &amp; Pentland, 1982</xref>), we derived psychometric functions for visual discrimination (<xref rid="fig6" ref-type="fig">Figures 6</xref> and <xref ref-type="fig" rid="figs2">Figure S2</xref>). Our findings reveal a significant increase in visual uncertainty (<italic>σ</italic><sub><italic>v</italic></sub>) with perturbation size, for both CW and CCW rotations (Friedman test, CW direction: χ<sup>2</sup>(2) = 34.11, p = 4e-8; CCW: χ<sup>2</sup>(2) = 26.47, p = 2e-6). Given the symmetry for the two directions, we collapsed data from both directions, and confirmed the linear relationship between <italic>σ</italic><sub><italic>v</italic></sub> and <italic>μ</italic> by a generalized linear model: <italic>σ</italic><sub><italic>v</italic></sub> = <italic>a</italic> + <italic>bμ</italic>, with <italic>a</italic> = 1.853 and <italic>b</italic> = 0.309, R<sup>2</sup> = 0.255 (F = 51.6, p = 2.53e-9). The 95% confidence intervals (CI) for <italic>a</italic> and <italic>b</italic> are [0.440, 3.266] and [0.182, 0.435], respectively. The intercept was similar to the visual uncertainty estimated in a previous study (Tsay, <xref ref-type="bibr" rid="c3">Avraham, et al., 2021</xref>). The linear dependency indicates a striking seven-fold increase in visual uncertainty from a 4° perturbation to a 64° perturbation (22.641 ± 6.024° vs. 3.172 ± 0.453°).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Experiment 1 measuring the dependency of visual uncertainty on perturbation size. <bold>(A)</bold> The 2AFC task for judging the cursor motion direction. In an exemplary trial, the participant reaches to a target while a direction-clamped cursor moves concurrently, serving as an error-clamp perturbation. Following a 1000-ms blank masking period, a reference point appears for 150ms, either clockwise or counterclockwise from the clamped cursor. The participant is then asked to making a binary judgement regarding the direction of the clamped cursor relative to the reference point. <bold>(B)</bold> The visual uncertainty, obtained from psychometrical estimation based on the 2AFC, is plotted as a function of perturbation size. Both individual estimates (red dots) and group-level statistics (boxplots) are shown. Positive angles correspond to CW rotations, negative angles to CCW rotations. <bold>(C)</bold> Collapsing data from both rotation directions, we observe that visual uncertainty closely follows a linear function of perturbation size. The dark gray line and its shaded region denote the regression line and its 95% confidence interval, respectively. The purple line is generated with the values of <italic>a</italic> and <italic>b</italic> fitted from data in Experiment 2 with <italic>a</italic> and <italic>b</italic> treated as free parameters (See Methods for details).</p></caption>
<graphic xlink:href="568442v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Results and model fitting of Experiment 2. <bold>(A)</bold> Implicit adaptation to error clamps of varying sizes is depicted; colored dot-lines and colored shading area represent the mean and standard error for each participant group. The light gray area indicates trials with error-clamp perturbations. Adaptation starts after baseline, gradually asymptotes to its final extent, and then decays with null feedback during washout. Different perturbation sizes result in distinct adaptation rates and extents. Group averages and standard error across participants are shown, along with predictions (colored solid lines) from the PEA model. <bold>(B)</bold> The adaptation extent (cycle 100-110) exhibits a nonlinear dependency on perturbation size, conforming to a concave function as prescribed by PEA (purple line). Color dots and error bars denote the mean and standard error across participants in each group. <bold>(C)-(D)</bold> The same data fitted with the PReMo model and the causal inference model. See more details, refer to <xref ref-type="fig" rid="figs3">Figure S3</xref>.</p></caption>
<graphic xlink:href="568442v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Proprioceptive data fitting and results from Experiment 3. <bold>(A)</bold> The data from (<xref ref-type="bibr" rid="c58">Tsay et al., 2020</xref>) are presented alongside the fitting of the PEA model. Participants adapting to a 30° error-clamp perturbation were required to report the location of their adapted hand using visual aids of numbers. The report was provided when the hand stayed at the end of movement. Initially, the proprioceptive estimate of the hand is biased toward the visual cursor (negative in the plot) and then gradually shifts toward the hand (positive in the plot). This trend is accurately captured by the PEA model: lines represent model fitting results, with the adapted hand direction in indigo and the reported hand direction in blue. The hand direction estimate (<italic>x̂</italic><sub><italic>Hand</italic></sub>, <xref rid="eqn1" ref-type="disp-formula">Eq.1</xref>) following a reach movement is shown in red. <bold>(B)-(C)</bold> Model simulations for proprioceptive bias from the PEA and PReMo models. Color gradients denote the simulations with varying ratio between the weights of <italic>x̂</italic><sub><italic>Hand</italic></sub> and <italic>x</italic><sub><italic>p</italic></sub>, the two cues available for estimating the hand direction. Note that the two models prescribe distinct profiles for the dependency of proprioception bias on perturbation size. <bold>(D)</bold> Experimental design. A reaching block, either with or without visual perturbations, is followed by a proprioception test block. The size and direction of the visual perturbation vary across blocks. The proprioception test is conducted when the hand is passively moved to a target (red dots) situated near the reaching target (blue dots). <bold>(E)</bold> The observed proprioceptive bias as a function of perturbation size. Data from the three proprioception test trials are separately plotted. The first trial reveals proprioception biases that form a concave function of perturbation size.</p></caption>
<graphic xlink:href="568442v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Results of Experiment 4. <bold>(A)-(C)</bold> Model simulations for single-trial learning under different visual uncertainty levels, shown separately for the PEA, PReMo and causal inference models. Blue curves represent simulated learning based on model parameters estimated from Experiment 2. Curves with red gradient indicate simulations with increasing levels of visual uncertainty, color coded by the ratio of visual uncertainty for the blurred cursor to that of the clear cursor. <bold>(D)</bold> Experimental design. Following 60 baseline trials without perturbations, participants completed 15 mini-blocks of error-clamp adaptation over three successive days. Each mini-block features 12 different types of error-clamp perturbations, distinguished by two cursor presentations (blurred or clear cursor) and six clamp sizes. Each perturbation trial, varied randomly in perturbation type, is flanked by two no-feedback trials. The change in hand direction between these two no-feedback trials serves to quantify singe-trial learning. <bold>(E)</bold> The single-trial learning with the blurred cursor is less than that with the clear cursor, but the difference is non-monotonic across perturbation size (*** denote p &lt; 0.001).</p></caption>
<graphic xlink:href="568442v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Design of Experiment 1. <bold>(A)</bold> Top-down view of the setup in visual discrimination task. The reference point (yellow) was presented either CW or CCW relative to the clamped cursor (dashed circle), which has a perturbation size <italic>μ</italic>. <bold>(B)</bold> Trial structure of the visual discrimination task. Purple rectangles represent error-clamped trials with varying perturbation size, rectangles with yellow edges represent the ensuing visual discrimination test for each perturbation size. <bold>(C)-(D)</bold> Exemplary sequences of the reference point: These sequences illustrate the deviation of the reference point from the cursor (C) and the changing step size across trials (D), following the PEST algorithm. Individual trials are represented by blue dots. Yellow and red dots mark the initiation and termination of each round of trials, respectively. In each round, the reference point starts on either the CW or CCW side of the cursor; In the subsequent round, it starts on the opposite side.</p></caption>
<graphic xlink:href="568442v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<title>Experiment 2: Visual Uncertainty Modulated Perceptual Error Accounts for Overcompensation and Saturation in Implicit Adaptation</title>
<p>The critical test of the PEA model lies in its ability to employ the linear function of visual uncertainty obtained from Experiment 1 to precisely explain key features of implicit adaptation. Earlier research mostly scrutinized smaller perturbation angles when reporting saturation effects (<xref ref-type="bibr" rid="c7">Bond &amp; Taylor, 2015</xref>; <xref ref-type="bibr" rid="c19">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="c34">Morehead et al., 2017</xref>). In contrast, Experiment 2 involved seven participant groups (n = 84) to characterize implicit adaptation across an extensive range of perturbation sizes (i.e., 2°, 4°, 8°, 16°, 32°, 64°, and 95°). After 30 baseline training cycles without perturbations, each group underwent 80 cycles of error-clamped reaching and 10 washout cycles without visual feedback (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). We replicated key features of implicit adaptation: it incrementally reached a plateau, and then declined during washout. Small perturbations led to overcompensation beyond visual errors (for 2°, 4°, 8°, 16° clamp sizes). Across perturbation sizes, the faster the early adaptation the larger the adaptation extent (<xref ref-type="fig" rid="figs4">Figure S4</xref>). Critically, the adaptation extent displayed a concave pattern: increasing steeply for smaller perturbations and tapering off for larger ones (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). A one-way ANOVA revealed a significant group difference in adaptation extent (F(6,83) = 12.108, p = 1.543e-09). Planned contrasts indicated that 8°, 16°, and 32° perturbations did not differ from each other (all p&gt;0.417, with Tukey-Kramer correction), consistent with earlier evidence of invariant implicit adaptation (<xref ref-type="bibr" rid="c19">Kim et al., 2018</xref>). However, 64° and 95° perturbations led to significantly reduced adaptation extents compared to 8° (p = 3.194e-05 and 5.509e-06, respectively), supporting the concave pattern as a more accurate portrayal of implicit adaptation across varying perturbation size.</p>
<p>Importantly, the PEA model, when augmented with visual uncertainty data from Experiment 1, precisely predicts this size-dependent adaptation behavior (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). Beyond adaptation extent, the model also accurately predicts the trial-by-trial adaptation across all seven participant groups, employing a single parameter set (R<sup>2</sup> = 0.975; <xref rid="fig3" ref-type="fig">Figure 3A</xref>). The model had only four free parameters (<italic>A</italic> = 0.974, <italic>B</italic> = 0.208, <italic>σ</italic><sub><italic>p</italic></sub>= 11.119°, <italic>σ</italic><sub><italic>u</italic></sub> = 5.048°; <xref ref-type="table" rid="tbls1">Table S1</xref>). Remarkably, both the retention rate <italic>A</italic> and learning rate <italic>B</italic> are consistent with previous studies focusing on visuomotor rotation adaptation (<xref ref-type="bibr" rid="c1">Albert et al., 2022</xref>). We also quantified proprioceptive uncertainty (<italic>σ</italic><sub><italic>p</italic></sub>) in a subset of participants (n=13) using a similar 2AFC procedure as in Experiment 1. We found that <italic>σ</italic><sub><italic>p</italic></sub> was 9.737°±5.598° (<xref ref-type="fig" rid="figs6">Figure S6</xref>), which did not statistically differ from the <italic>σ</italic><sub><italic>p</italic></sub> value obtained from the model fitting (two-tailed t-test, p = 0.391). In summary, the perceptual parameters obtained in Experiment 1, when incorporated into the PEA model, effectively explain the implicit adaptation behaviors observed in different participant groups in Experiment 2.</p>
<p>In comparative analysis, the PReMo model yields a substantially lower R<sup>2</sup> value of 0.749 (<xref ref-type="fig" rid="figs3">Figure S3B</xref>). It tends to underestimate adaptation for medium-size perturbations and overestimate it for large ones (<xref rid="fig3" ref-type="fig">Figure 3C</xref>; see also <xref ref-type="fig" rid="figs3">Figure S3B</xref> for trial-by-trial fitting). Another alternative is the causal inference model, previously shown to account for nonlinearity in motor learning (<xref ref-type="bibr" rid="c31">Mikulasch et al., 2022</xref>; <xref ref-type="bibr" rid="c60">Wei &amp; Körding, 2009</xref>). Although this model has been suggested for implicit adaptation (Tsay, <xref ref-type="bibr" rid="c3">Avraham, et al., 2021</xref>), it fails to reproduce the observed concave adaptation pattern (<xref ref-type="fig" rid="figs3">Figures S3C</xref> and <xref ref-type="fig" rid="figs3">3D</xref>). The model aligns well with adaptations to medium-size perturbations (8°, 16°, and 32°) but falls short for small and large ones, yielding an R<sup>2</sup> value of 0.711 (see <xref ref-type="fig" rid="figs3">Figure S3C</xref> for trial-by-trial fits). Model comparison metrics strongly favor the PEA model over both the PReMo and causal inference models, as evidenced by AIC scores of 2255, 3543, and 3283 for the PEA, PReMo, and causal inference models, respectively (<xref ref-type="table" rid="tbls2">Table S2</xref>). In summary, it is the eccentricity-induced visual uncertainty that most accurately accounts for the implicit adaptation profile across a broad spectrum of perturbation sizes, rather than saturated visual influence or causal inference of error.</p>
</sec>
<sec id="s2d">
<title>Experiment 3: Cue Combination Accounts for Changes in Proprioception During Implicit Adaptation</title>
<p>Motor adaptation not only recalibrates the motor system but also alters proprioception (<xref ref-type="bibr" rid="c39">Rossi et al., 2021</xref>) and even vision (<xref ref-type="bibr" rid="c47">Simani et al., 2007</xref>). In traditional motor adaptation involving both explicit and implicit components, the perceived hand location is initially biased towards the visual perturbation and subsequently stabilizes (<xref ref-type="bibr" rid="c40">Ruttle et al., 2016</xref>). However, in implicit adaptation, the perceived hand location initially aligns with but later drifts away from the visual feedback (<xref ref-type="bibr" rid="c58">Tsay et al., 2020</xref>). The PReMo model proposes that this drift comprises two phases: initial proprioceptive recalibration and subsequent visual recalibration (<xref ref-type="bibr" rid="c55">Tsay, Kim, et al., 2022</xref>), however, this assumption is lack of empirical validation. In contrast, we suggest that the perceived hand location is based on the same Bayesian cue combination principle. In this framework, the perceived hand location at the end of each reach is influenced by both the proprioceptive cue (<italic>x</italic><sub><italic>p</italic></sub>) and the estimated hand position under the influence of clamped feedback (<italic>x̂</italic><sub><italic>Hand</italic></sub>, <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref>).</p>
<p>During early adaptation, <italic>x̂</italic><sub><italic>Hand</italic></sub> is biased towards the clamped feedback, while <italic>x</italic><sub><italic>p</italic></sub> remains near the target as the motor system has yet to adapt (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). This results in an initial negative proprioceptive bias. As adaptation progresses, although <italic>x̂</italic><sub><italic>Hand</italic></sub> remains biased, <italic>x</italic><sub><italic>p</italic></sub> gradually shifts in the positive direction due to adaptation, resulting in an increasingly positive proprioceptive bias. Remarkably, the PEA model can predict these temporal changes in proprioception with high accuracy (R<sup>2</sup> = 0.982; <xref rid="fig4" ref-type="fig">Figure 4A</xref>).</p>
<p>If the hand estimate <italic>x̂</italic><sub><italic>Hand</italic></sub>indeed influences proprioceptive recalibration during adaptation, our PEA model can make specific quantitative predictions about the relationship between proprioception changes and visual perturbation size. While traditional visuomotor paradigms suggest either invariant (<xref ref-type="bibr" rid="c33">Modchalingam et al., 2019</xref>) or linear increases in proprioceptive recalibration with visual-proprioceptive discrepancy (<xref ref-type="bibr" rid="c43">Salomonczyk et al., 2011</xref>), the PEA model prescribes a concave function in relation to visual perturbation size (<xref rid="fig4" ref-type="fig">Figure 4B</xref>).</p>
<p>To empirically test this prediction, Experiment 3 (n=11) measured participants’ proprioceptive recalibration during implicit adaptation, using a procedure similar to the error-clamp perturbations in Experiment 2. After each block of six adaptation trials, participants’ right hands were passively moved by a robotic manipulandum, and they indicated the perceived direction of their right hand using a visually represented “dial” controlled by their left hand (<xref rid="fig7" ref-type="fig">Figure 7B</xref>). This method quantifies proprioceptive recalibration during adaptation (<xref ref-type="bibr" rid="c10">Cressman &amp; Henriques, 2009</xref>). Each adaptation block was followed by three such proprioception test trials. The alternating design between adaptation and proprioception test blocks allowed us to assess proprioceptive biases across varying perturbation sizes, which consisted of ±10°, ±20°, ±40°, and ±80°, to covering a wide range (<xref rid="fig4" ref-type="fig">Figure 4D</xref>).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Setup for measuring proprioceptive recalibration in Experiment 3. <bold>(A)</bold> Reaching movement with error-clamped cursor, performed by the right hand holding a robot handle. <bold>(B)</bold> Passive movement in the proprioception test. The right hand was passively moved to the unseen target (<bold><italic>h</italic></bold><italic>R</italic>), depicted here as a small black dot. A red hollow circle with an expanding radius appears on the screen during passive movement, signaling the increasing distance between the hand and the start position. Subsequently, participants used their left hand to report the right-hand location (<bold><italic>h</italic></bold><italic>p</italic>) by aligning a red rectangle on the red circle, which is displayed at the target distance.</p></caption>
<graphic xlink:href="568442v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Our findings confirmed a typical proprioceptive recalibration effect, as the perceived hand direction was biased towards the visual perturbation (<xref rid="fig4" ref-type="fig">Figure 4E</xref>). Importantly, the bias in the initial proprioception test trial exhibited a concave function of perturbation size. A one-way repeated-measures ANOVA revealed a significant effect of perturbation size (F(3,30)=3.603,p=0.036), with the 20° and 40° conditions displaying significantly greater proprioceptive bias compared to the 80° condition (pairwise comparisons: 20° v.s. 80°, p = 0.034; 40° v.s. 80°, p = 0.003). The bias was significantly negative for 20° and 40° conditions (p = 0.005 and p = 0.007, respectively with one-tailed t-test), but not for 10° and 80° condition (p = 0.083 and p = 0.742, respectively). This concave pattern aligns well with the PEA model’s predictions (<xref rid="fig4" ref-type="fig">Figure 4B</xref>), further consolidating its explanatory power.</p>
<p>This stands in contrast to the PReMo model, which assumes a saturation for the influence of the visual cue on the hand estimate (<xref rid="eqn12" ref-type="disp-formula">Eq. 12</xref>-<xref rid="eqn13" ref-type="disp-formula">13</xref>). As a result, PReMo’s predicted proprioceptive bias follows a ramp function, deviating substantially from our empirical findings (<xref rid="fig4" ref-type="fig">Figure 4C</xref>). The causal inference model, which mainly focuses on the role of visual feedback in error correction, lacks the capability to directly predict changes in proprioceptive recalibration.</p>
<p>Interestingly, we observed that the proprioceptive bias reduced to insignificance by the third trial in each proprioception test block (one-tailed t-test, all <italic>p</italic> &gt; 0.18; <xref rid="fig4" ref-type="fig">Figure 4E</xref>, yellow line). This suggests that the influence from implicit adaptation – manifested here as trial-by-trial updates of the perceived hand estimate <italic>x̂</italic><sub><italic>Hand</italic></sub> – decays rapidly over time.</p>
</sec>
<sec id="s2e">
<title>Experiment 4: Differential Impact of Upregulated Visual Uncertainty on Implicit Adaptation Across Perturbation Sizes</title>
<p>Thus far, we have presented both empirical and computational evidence underscoring the pivotal role of perceptual error and visual uncertainty in implicit adaptation. It is crucial to note, however, that this evidence is arguably correlational, arising from natural variations in visual uncertainty as a function of perturbation size. To transition from correlation to causation, Experiment 4 (n = 19) sought to directly manipulate visual uncertainty by blurring the cursor, thereby offering causal support for the role of multimodal perceptual error in implicit adaptation.</p>
<p>By increasing visual uncertainty via cursor blurring, we hypothesized a corresponding decrease in adaptation across all perturbation sizes. Notably, the PEA model predicts a size-dependent attenuation in adaptation: the reduction is less marked for smaller perturbations and more pronounced for larger ones (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). This prediction diverges significantly from those of competing models. The PReMo model, operating under the assumption of a saturation effect for large visual perturbations, predicts that cursor blurring will only influence adaptation to smaller perturbations, leaving adaptation to larger perturbations unaffected (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). The causal inference model makes an even more nuanced prediction: it anticipates that the blurring will lead to a substantial reduction in adaptation for small perturbations, a diminishing effect for medium perturbations, and a potential reversal for large perturbations (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). This prediction results from the model’s core concept that causal attribution of the cursor to self-action—which directly dictates the magnitude of adaptation—decreases for small perturbations but increases for large ones when overall visual uncertainty is elevated.</p>
<p>Starting from the above predictions, Experiment 4 was designed to assess the impact of elevated visual uncertainty across small (4°), medium (16°), and large (64°) perturbation sizes. Visual uncertainty was augmented by superimposing a Gaussian blurring mask on the cursor (<xref ref-type="bibr" rid="c8">Burge et al., 2008</xref>). Each participant performed reaching tasks with either a standard or blurred clamped cursor for a single trial, bracketed by two null trials devoid of cursor feedback (<xref rid="fig5" ref-type="fig">Figure 5D</xref>). These three-trial mini-blocks permitted the quantification of one-trial learning as the directional difference of movements between the two null trials. To preclude the cumulative effect of adaptation, perturbation sizes and directions were randomized across mini-blocks.</p>
<p>Crucially, our findings corroborated the predictions of the PEA model: visual uncertainty significantly diminished adaptation for medium and large perturbations (16° and 64°), while leaving adaptation for small perturbations (4°) largely unaffected (<xref rid="fig5" ref-type="fig">Figure 5E</xref>). A two-way repeated-measures ANOVA, with two levels of uncertainty and three levels of perturbation size, revealed a significant main effect of increased visual uncertainty in reducing implicit adaptation (F(1,18) = 42.255, p = 4.112e-06). Furthermore, this effect interacted with perturbation size (F(2,36) = 5.391, p = 0.012). Post-hoc analyses demonstrated that elevated visual uncertainty significantly attenuated adaptation for large perturbations (p = 2.877e-04, d = 0.804 for 16°; p = 1.810e-05, d = 1.442 for 64°) but exerted no such effect on small perturbations (p = 0.108, d = 0.500). These empirical outcomes are not congruent with the predictions of either the PReMo or the causal inference models (<xref rid="fig5" ref-type="fig">Figure 5B</xref> and <xref rid="fig5" ref-type="fig">5C</xref>). This lends compelling empirical support to the primacy of perceptual error in driving implicit adaptation, as posited by our PEA model.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study, we elucidate the central role of perceptual error, derived from multimodal sensorimotor cue integration, in governing implicit motor adaptation. Utilizing the classical error-clamp paradigm, we uncover that the overcompensation observed in response to small perturbations arises from a sustained perceptual error related to hand localization, and the saturation effect commonly reported in implicit adaptation is not an intrinsic characteristic but is attributable to increasing sensory uncertainty with increasing visual perturbation eccentricity—a factor hitherto neglected in existing models of sensorimotor adaptation. Contrary to conventional theories that describe implicit adaptation as either saturated or invariant (<xref ref-type="bibr" rid="c19">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="c55">Tsay, Kim, et al., 2022</xref>), our data reveal a concave dependency of implicit adaptation on visual perturbation size, characterized by diminishing adaptation in response to larger perturbations. Notably, our Perceptual Error Adaptation (PEA) model, calibrated using perceptual parameters from one set of participants, provides a robust account of implicit adaptation in separate groups subjected to varying perturbations. The model further successfully captures the perceptual consequences of implicit adaptation, such as the continuous shifts in proprioceptive localization during the adaptation process (<xref ref-type="bibr" rid="c58">Tsay et al., 2020</xref>) and its correlation with perturbation size. Lastly, we manipulated visual uncertainty independently of perturbation size and demonstrated that this selectively attenuated adaptation in the context of larger perturbations while leaving smaller perturbations unaffected. These empirical results, inconsistent with predictions from existing models, underscore the conceptual and quantitative superiority of our PEA model. In summary, our findings advocate for a revised understanding of implicit motor adaptation, suggesting that it is governed by Bayesian cue combination-based perceptual estimation of effector localization.</p>
<p>Bayesian cue combination has been established as a foundational principle in various perceptual phenomena, both intra- and inter-modally (<xref ref-type="bibr" rid="c45">Seilheimer et al., 2014</xref>). It has also been implicated in motor adaptation (<xref ref-type="bibr" rid="c8">Burge et al., 2008</xref>; <xref ref-type="bibr" rid="c13">He et al., 2016</xref>; <xref ref-type="bibr" rid="c22">Körding &amp; Wolpert, 2004</xref>; <xref ref-type="bibr" rid="c61">Wei &amp; Körding, 2010</xref>). However, previous studies have largely focused on experimentally manipulating sensory cue uncertainty to observe its effects on adaptation (<xref ref-type="bibr" rid="c8">Burge et al., 2008</xref>; <xref ref-type="bibr" rid="c61">Wei &amp; Körding, 2010</xref>), similar to our Experiment 4. What has been largely overlooked is the natural covariance between visual uncertainty and perturbation size, which, when incorporated into classical state-space models, provides a compelling explanation for implicit adaptation.</p>
<p>The causal inference framework (<xref ref-type="bibr" rid="c60">Wei &amp; Körding, 2009</xref>) fails to adequately predict sensorimotor changes in implicit adaptation. For instance, it underestimates the adaptation extent for large perturbations and incorrectly predicts that increasing visual uncertainty would augment, rather than reduce, adaptation to large perturbations. We postulate that causal inference is more relevant to motor learning dominated by explicit processes, such as traditional visuomotor rotations, rather than in implicit adaptations where cue combination is obligatory.</p>
<p>Similar to our PEA model, the PReMo model also incorporates the integration of multiple sensory cues. But two models differ fundamentally in their conceptualization of how these cues contribute to the error signal. The PReMo model posits two intermediate perceptual variables with Bayesian cue integration: a visual estimate of the cursor and a proprioceptive estimate of the hand (<xref ref-type="bibr" rid="c55">Tsay, Kim, et al., 2022</xref>). The final error signal in PReMo is presumed to be a proprioceptive error, not from further Bayesian cue combination, but from a visual-to-proprioceptive bias that is governed by a predetermined, ramp-like visual influence that saturates around a 6–7° visual-proprioceptive discrepancy (<xref rid="eqn13" ref-type="disp-formula">Eq. 13</xref>). These assumptions lack empirical validation. Our findings in Experiment 3 indicate that proprioceptive recalibration follows a concave function with respect to visual perturbation size, contradicting the ramp-like function assumed by PReMo. Moreover, the presupposed ramp-like visual influence generates a rigid prediction for a ramp-like adaptation extent profile, which is at odds with the concave adaptation pattern we observed in Experiment 2 and in a similar study involving trial-by-trial learning (<xref ref-type="bibr" rid="c54">Tsay, Avraham, et al., 2021</xref>). Furthermore, PReMo predicts that increasing visual uncertainty will selectively reduce adaptation to small perturbations while sparing large ones. This is inconsistent with our findings in Experiment 4, which demonstrated that increased visual uncertainty substantially impacted adaptation more to larger perturbations than to small ones. Lastly, PReMo’s reliance on a proprioceptive bias constrains its ability to account for the temporal shifts in perceived hand location during adaptation (<xref ref-type="bibr" rid="c58">Tsay et al., 2020</xref>). In contrast to PEA’s unified approach, PReMo must resort to separate mechanisms of proprioceptive and visual recalibration at different phases of adaptation to explain these shifts. In summary, the PReMo model’s assumptions introduce limitations that make it less consistent with empirical observations, particularly concerning the nonlinearities observed in both motoric and perceptual aspects of implicit adaptation.</p>
<p>Our research contributes to an ongoing debate concerning the driving forces behind error-based motor learning, specifically addressing the question of whether implicit adaptation is driven by target error or sensory prediction error (<xref ref-type="bibr" rid="c1">Albert et al., 2022</xref>; <xref ref-type="bibr" rid="c18">Izawa &amp; Shadmehr, 2011</xref>; <xref ref-type="bibr" rid="c26">Leow et al., 2020</xref>; <xref ref-type="bibr" rid="c29">Mazzoni &amp; Krakauer, 2006</xref>; <xref ref-type="bibr" rid="c30">McDougle et al., 2015</xref>; <xref ref-type="bibr" rid="c32">Miyamoto et al., 2020</xref>; <xref ref-type="bibr" rid="c51">Taylor &amp; Ivry, 2011</xref>; <xref ref-type="bibr" rid="c59">Tseng et al., 2007</xref>). Most empirical data fueling this debate stem from traditional motor adaptation paradigms where explicit and implicit learning co-occur and interact. In these paradigms—visuomotor rotation being a prime example— target error is defined as the disparity between the target and the perturbed cursor, while sensory prediction error is the disparity between the predicted and actual cursor. Both types of error are sensory (specifically, visual) in nature, yet they differ due to the misalignment between the predicted or desired cursor direction and the target direction, which is induced by explicit learning (<xref ref-type="bibr" rid="c52">Taylor et al., 2014</xref>).</p>
<p>By employing the error-clamp paradigm, our study was able to isolate implicit learning, thereby eliminating potential confounds from explicit learning. Interestingly, in this paradigm, the target error and sensory prediction error effectively refer to the same visual discrepancy, as both the predicted and target directions are aligned. Despite this, classical state-space models, which utilize this visual error, fail to account for the nuanced features of implicit adaptation (<xref ref-type="bibr" rid="c55">Tsay, Kim, et al., 2022</xref>). In contrast, our PEA model reframes the perturbing cursor as a visual cue influencing the perceptual estimation of hand location, rather than as a source of visual error. The resultant bias in hand estimation from the desired target serves as the actual error signal. This leads us to posit that the error signal driving implicit sensorimotor adaptation is fundamentally perceptual, rather than sensory. From a normative standpoint, this perceptual error could be construed either as a predictive or performance error (<xref ref-type="bibr" rid="c1">Albert et al., 2022</xref>), but importantly, it is not tied to a specific modality (i.e., vision or proprioception). Instead, it directly pertains to the perceptual estimate that is crucial for task execution, i.e., bringing the hand to the target. The concept of perceptual error-driven learning can be extrapolated to various motor adaptation paradigms, including those involving explicit learning. For instance, in visuomotor rotation tasks, explicit learning manifests as a deviation in the aiming direction from the visual target, whereas implicit learning manifests as a further deviation the actual hand position from this aiming direction (<xref ref-type="bibr" rid="c52">Taylor et al., 2014</xref>). Even in the presence of explicit learning, the perturbed cursor continues to bias the perceptual estimate of the hand, thereby potentially driving implicit adaptation. In this scenario, the perceptual error is defined as the difference between the perceptual estimate of the hand and the altered aiming direction, which serves as the new “target” when explicit learning is in play. Our PEA model would predict similar saturation effects in implicit adaptation for this conventional adaptation paradigm, similar to for the error-clamp paradigm. Indeed, evidence from the conventional adaptation paradigm suggests that its implicit adaptation follows either a saturation effect (<xref ref-type="bibr" rid="c7">Bond &amp; Taylor, 2015</xref>; <xref ref-type="bibr" rid="c36">Neville &amp; Cressman, 2018</xref>) or a concave pattern (<xref ref-type="bibr" rid="c55">Tsay, Haith, et al., 2022</xref>) across a range of perturbation sizes. Furthermore, according to the PEA framework, this perceptual error is anchored on the aiming target, thereby naturally predicting that implicit and explicit adaptations should interact in a complementary manner, a notion that aligns with recent theories on their interaction (<xref ref-type="bibr" rid="c1">Albert et al., 2022</xref>; <xref ref-type="bibr" rid="c32">Miyamoto et al., 2020</xref>). Future research is warranted to further investigate the role of perceptual error in driving implicit learning across diverse motor learning paradigms.</p>
<p>Our study provides a new angle on explaining proprioceptive changes during motor adaptation, advocating for a Bayesian cue combination framework. Previously, the change in proprioceptive hand localization during motor adaptation has been ascribed to visual-proprioceptive discrepancy-induced recalibration (<xref ref-type="bibr" rid="c41">Ruttle et al., 2018</xref>; <xref ref-type="bibr" rid="c44">Salomonczyk et al., 2013</xref>) and/or altered sensory prediction driven by the adapted forward internal model (<xref ref-type="bibr" rid="c35">Mostafa et al., 2019</xref>; <xref ref-type="bibr" rid="c50">‘t Hart &amp; Henriques, 2016</xref>). To dissect these components, researchers have often compared proprioceptive localization in actively moved (<xref ref-type="bibr" rid="c58">Tsay et al., 2020</xref>) versus passively placed (passive localization, e.g., Experiment 3) hands during adaptation, attributing the smaller bias in passive localization to recalibration alone. The difference between the two is then considered to reflect altered sensory prediction due to motor adaptation (Mostafa et al., 2019; <xref ref-type="bibr" rid="c39">Rossi et al., 2021</xref>). But these conceptual divisions lack computational models for validation. For instance, researchers have proposed that proprioceptive recalibration in visuomotor adaptation is either a fixed proportion (e.g., 20%) of the visual-proprioceptive discrepancy (<xref ref-type="bibr" rid="c14">Henriques &amp; Cressman, 2012</xref>; <xref ref-type="bibr" rid="c42">Ruttle et al., 2021</xref>) or largely invariant (<xref ref-type="bibr" rid="c33">Modchalingam et al., 2019</xref>). In fact, cross-sensory calibration typically follows the Bayesian principle, as shown in other task paradigms other than motor adaptation (<xref ref-type="bibr" rid="c49">Stetson et al., 2006</xref>; <xref ref-type="bibr" rid="c63">Wozny &amp; Shams, 2011</xref>). Our Experiment 3 shows that proprioceptive recalibration exhibits a concave, instead of invariant or proportional, dependency to visual perturbation size, a finding follows the Bayesian principles of cue combination. Our results also confirm that the critical cue for passive localization is the biased perceived hand position (<italic>x̂</italic><sub><italic>Hand</italic></sub>) fueled by adaptation.</p>
<p>The same Bayesian framework applies to active localization, though this time <italic>x̂</italic><sub><italic>Hand</italic></sub> is to be combined with the proprioceptive cue from the adapted hand. In this sense, active localization indeed serves as a multifaceted reflection of both the internal model and proprioceptive recalibration (Mostafa et al., 2019; <xref ref-type="bibr" rid="c39">Rossi et al., 2021</xref>). Specifically, the proprioceptive cue continuously drifts by the adapted internal model, while the perceived hand position encapsulates the effects of proprioceptive recalibration. During the initial stages of perturbation, the immediate negative bias in active localization is predominantly attributable to rapid proprioceptive recalibration. This is evidenced by a sudden shift in the estimated hand position (<italic>x̂</italic><sub><italic>Hand</italic></sub>; <xref rid="fig4" ref-type="fig">Figure 4A</xref>), occurring before the internal model has had sufficient time to adapt.</p>
<p>Then, why does active localization in traditional motor adaptation paradigms yield a largely stable bias (<xref ref-type="bibr" rid="c40">Ruttle et al., 2016</xref>, <xref ref-type="bibr" rid="c42">2021</xref>)? We postulate that the rapid explicit learning leads to a quick asymptotic adaptation, while previous investigations have predominantly measured active localization after adaptation has plateaued (<xref ref-type="bibr" rid="c14">Henriques &amp; Cressman, 2012</xref>; <xref ref-type="bibr" rid="c33">Modchalingam et al., 2019</xref>; Mostafa et al., 2019; <xref ref-type="bibr" rid="c43">Salomonczyk et al., 2011</xref>, <xref ref-type="bibr" rid="c44">2013</xref>; <xref ref-type="bibr" rid="c54">Tsay, Kim, et al., 2021</xref>). Consequently, these studies may overlook the evolving effect of the adaptation. In contrast, the gradual nature of implicit adaptation provides a unique opportunity to uncover the underlying mechanisms governing changes in proprioception during the adaptation process.</p>
<p>Notably, our model aligns with previous findings that show a positive correlation between proprioceptive recalibration and motor adaptation based on individual differences (<xref ref-type="bibr" rid="c42">Ruttle et al., 2021</xref>; <xref ref-type="bibr" rid="c44">Salomonczyk et al., 2013</xref>; <xref ref-type="bibr" rid="c56">Tsay, Kim, et al., 2021</xref>). Unlike existing theories that posit proprioceptive recalibration either as a component of (<xref ref-type="bibr" rid="c33">Modchalingam et al., 2019</xref>; <xref ref-type="bibr" rid="c35">Mostafa et al., 2019</xref>; <xref ref-type="bibr" rid="c42">Ruttle et al., 2021</xref>) or a driver for implicit adaptation (<xref ref-type="bibr" rid="c57">Tsay, Kim, et al., 2022</xref>), our PEA model provides a mechanistic and empirically testable framework. It posits that the misestimation of hand position (<italic>x̂</italic><sub><italic>Hand</italic></sub>) —induced by the recent perturbation—serves as the driving factor for both implicit adaptation and changes in proprioception. This misestimation is perturbation-dependent, resulting in both implicit adaptation and proprioceptive recalibration exhibiting a concave profile relative to perturbation size. Updated on a trial-by-trial basis, this misestimation exerts immediate effects, manifesting as an abrupt negative bias (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). Additionally, its influence decays rapidly, becoming negligible within three trials (<xref rid="fig6" ref-type="fig">Figure 6C</xref>). These converging lines of evidence strongly suggest that perceptual misestimation of hand position is central to the process of proprioceptive recalibration during adaptation.</p>
<p>Our findings contribute nuanced perspectives to the modulation of implicit learning rate by factors beyond visual perturbation size. Previous studies have shown that environmental inconsistency -- defined as the inconsistency of visual errors -- reduced the rate (<xref ref-type="bibr" rid="c15">Herzfeld et al., 2014</xref>; <xref ref-type="bibr" rid="c17">Hutter &amp; Taylor, 2018</xref>) or asymptote (<xref ref-type="bibr" rid="c2">Albert et al., 2021</xref>) of implicit adaptation. Baseline motor variance in unperturbed conditions has been shown to increase implicit adaptation rate, proposed as a sign of better exploratory learning (<xref ref-type="bibr" rid="c64">Wu et al., 2014</xref>). These studies interpret such phenomena as parametric changes in the learning rate in relation to visual errors, conceptualized as alterations to the <italic>B</italic> parameter in existing models. However, apparent change in learning rate to visual errors does not necessarily signify parametric modification, but may attribute to other factors that influence the use of visual cues (<xref ref-type="bibr" rid="c13">He et al., 2016</xref>), such as visual uncertainty in our case. Previous research has also pointed to alternative factors like error discounting based on causal inference of error (<xref ref-type="bibr" rid="c60">Wei &amp; Körding, 2009</xref>), proprioceptive uncertainty (<xref ref-type="bibr" rid="c42">Ruttle et al., 2021</xref>; <xref ref-type="bibr" rid="c54">Tsay, Kim, et al., 2021</xref>), and state estimation uncertainty (<xref ref-type="bibr" rid="c13">He et al., 2016</xref>; <xref ref-type="bibr" rid="c61">Wei &amp; Körding, 2010</xref>). Our work suggests a shift in perspective: the driving error signal for implicit learning should be considered as perceptual, rather than merely visual. This paradigmatic shift could serve as a cornerstone for future research aimed at understanding how learning rates adapt under varying conditions.</p>
<p>Our new framework opens avenues for exploring the memory characteristics of implicit learning. Traditional motor adaptation often exhibits ‘savings,’ or accelerated relearning upon re-exposure to a perturbation (<xref ref-type="bibr" rid="c11">Della-Maggiore &amp; McIntosh, 2005</xref>; <xref ref-type="bibr" rid="c16">Huberdeau et al., 2019</xref>; <xref ref-type="bibr" rid="c23">Krakauer et al., 2005</xref>; <xref ref-type="bibr" rid="c25">Landi et al., 2011</xref>). In contrast, implicit adaptation has been found to exhibit a decreased learning rate during re-adaptation (<xref ref-type="bibr" rid="c3">Avraham et al., 2021</xref>), a phenomenon attributed to conditioning (<xref ref-type="bibr" rid="c3">Avraham et al., 2021</xref>) or associative learning mechanisms (<xref ref-type="bibr" rid="c4">Avraham et al., 2022</xref>). Investigating this ‘anti-saving’ effect will yield insights into the unique memory properties of implicit learning. Although our current PEA model is structured around single-epoch learning and does not directly address this question, it does raise new, testable hypotheses. For example, is the reduced adaptation rate during relearning attributable to a down-weighting of perturbed visual feedback in cue combination, or does it reflect a parametric alteration in the learning rate? Another noteworthy aspect of implicit learning is its remarkably slow decay rate. It has been observed that the number of trials required to washout the implicit adaptation exceeds the number of trials needed to establish it (<xref ref-type="bibr" rid="c3">Avraham et al., 2021</xref>; <xref ref-type="bibr" rid="c58">Tsay et al., 2020</xref>). In the context of our perceptual error framework, this raises the possibility that washout phases might be governed by state updating involving a distinct set of sensorimotor cues or an alternative updating mechanism, such as memory formation and selection (<xref ref-type="bibr" rid="c37">Oh &amp; Schweighofer, 2019</xref>).</p>
<sec id="s3a">
<title>Methods Participants</title>
<p>We recruited 115 college students from Peking University (77 females, 38 males, 22.05 ± 2.82 years, mean ± SD). Participants were all right-handed according to the Edinburgh handedness inventory (<xref ref-type="bibr" rid="c38">Oldfield, 1971</xref>) and had normal or corrected-to-normal vision. Participants were naïve to the purpose of the experiment and provided written informed consent, which was approved by the Institutional Review Board of the School of Psychological and Cognitive Sciences, Peking University. Participants received monetary compensation upon completion of the experiment.</p>
</sec>
<sec id="s3b">
<title>Apparatus</title>
<p>In Experiment 1, 2 and 4, participants were seated in front of a vertically-placed LCD screen (29.6 x 52.7 cm, Dell, Round Rock, TX, US). They performed the movement task with their right hand, holding a stylus and slide it on a horizontally placed digitizing tablet (48.8 x 30.5 cm, Intuos 4 PTK-1240, Wacom, Saitama, Japan). In Experiment 1, a keyboard was provided to the participants’ left hand to enable them to report the direction of visual stimuli in the discrimination task. A customized wooden shelter was placed above the tablet to block the peripheral vision of the right arm. In Experiment 1 and 4, participants placed their chin on a chin rest attached on the wooden shelter to stabilize their head. Their eye movement was recorded by an eye tracker (Tobii pro nano, Tobii, Danderyd Municipality, Sweden) affixed at the lower edge of the screen. The sampling rate was 160-200 Hz for the tablet and 60 Hz for the eye tracker.</p>
<p>Experiment 3 was conducted using the KINARM planar robotic manipulandum with a virtual-reality system (BKIN Technologies Ltd., Kingston, Canada). Participants seated in a chair and held the robot handles with their left and right hands (<xref rid="fig7" ref-type="fig">Figure 7</xref>). The movement task was performed with the right handle and the left handle was used to indicate the perceived direction of right hand in the proprioception test. A semi-silvered mirror was placed below the eye level to block the vision of the hands and the robotic manipulandum; it also served as a display monitor.</p>
</sec>
<sec id="s3c">
<title>Experiment 1: measuring visual uncertainty in error-clamp adaptation</title>
<p>Eighteen among twenty participants finished the reaching with clamped error feedback and visual discrimination task in 3 consecutive days, two participants withdrew during the experiment. Participants made reaching movement by sliding the stylus from a start position at the center of the workspace to towards a target (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). The start position, the target, and the cursor were represented by a gray dot, a blue cross and a white dot on the screen, respectively. All these elements had a diameter of 5mm. The procedure of the motor and visual discrimination task is illustrated in <xref rid="fig2" ref-type="fig">Figure 2A</xref>. To initiate a trial, participates moved the cursor into the start position. Following an 800ms holding period, a target appeared 10 cm away in twelve o’clock direction and participants were instructed to slide through the target rapidly while maintaining a straight hand trajectory. The trial terminated when the distance between the hand and the start position exceeded 10 cm, regardless of whether the target was hit. A warning message, “too slow”, would appear on the screen if participants failed to complete the trial within 300 ms after initiating the movement. Each practice day began with 60 standard reaching trials, during which veridical feedback about hand location was provided by the cursor. The target would change from blue to green if the cursor successfully passed through it. In subsequent visual clamp trials, the cursor moved along a predetermined direction set by the perturbation angle, while its position was updated in real-time based on the hand’s location. The cursor’s distance from the start position was equal to the distance between the hand and the start position until the end of the trial.</p>
<p>Following each trial, the cursor remained frozen at its final position for an additional 800 ms before disappearing. The visual discrimination task commenced 1000 ms thereafter. A yellow reference point, located 10 cm from the start position, was displayed for 150 ms near the cursor’s final position (<xref rid="fig2" ref-type="fig">Figure 2A</xref> &amp; <xref rid="fig6" ref-type="fig">Figure 6A</xref>). Subsequently, all visual stimuli, except for the blue cross at the start position, were removed from the screen. Participants were then required to judge whether the reference point was situated in a clockwise (CW) or counterclockwise (CCW) direction relative to the cursor’s final position and to report their judgment by pressing a key on the keyboard. Participants were informed that they no longer controlled the direction of cursor movement during the task. They were instructed to fixate their gaze on either the start position or the blue cross during the motor task, while actively ignoring the white cursor. During the discrimination task, they were required to maintain their gaze on the blue cross. Eye movements were monitored in real-time using an eye tracker. Participants received a warning if their gaze was detected outside a 75-pixel-wide band-shaped region centered on the line of gaze four consecutive times during the experiment (<xref ref-type="fig" rid="figs1">Figure S1</xref>).</p>
<p>In each trial, the angular deviation between the error-clamped cursor and the reference point was determined using a PEST procedure (<xref ref-type="bibr" rid="c28">Lieberman &amp; Pentland, 1982</xref>). <xref rid="fig6" ref-type="fig">Figure 6C</xref>- D illustrates the evolution of the deviation angle and step size for an exemplary participant experiencing a -16° perturbation. In each round, the deviation commenced at 30° (indicated by yellow points in <xref rid="fig6" ref-type="fig">Figure 6C-D</xref>) and was altered by one step size following each trial. The initial step size was set at 10° and was halved whenever the direction judgment changed (i.e., from “CW” to “CCW” or vice versa). For a specific perturbation angle, the initial deviation always started from the CW direction for the first round and flipped the direction at the beginning of the next round. A round terminated either when the step size fell below a predefined criterion (indicated by the red line in <xref rid="fig6" ref-type="fig">Figure 6D</xref>) or when the trial count exceeded 30. Six perturbation angles were randomly interleaved (<xref rid="fig6" ref-type="fig">Figure 6B</xref>), and the experiment concluded when four complete rounds of the PEST procedure had been completed for each perturbation angle. Consequently, the total number of trials varied among participants and across practice days. Additionally, for some perturbation angles, more than four complete rounds could be conducted in a single day.</p>
</sec>
<sec id="s3d">
<title>Experiment 2: Motor adaptation with different perturbation size</title>
<p>Eighty-four participants were randomly allocated into seven groups, each comprising 12 individuals. Each group performed a motor adaptation task featuring clamped visual feedback at different perturbation angles: 2°, 4°, 8°, 16°, 32°, 64°, and 95°. As in Experiment 1, participants were instructed to slide rapidly and directly through the target, which was represented by a blue dot rather than a cross. In each trial, the target appeared at one of four possible locations (45°, 135°, 225° or 315° counter-clockwise from the positive x-axis). The sequence of target locations was randomized, yet constrained so that all four positions appeared in cycles of four trials. Each group commenced with a baseline session that included 15 cycles of reaching trials with veridical feedback, followed by 15 cycles without visual feedback. Subsequently, during the perturbation session, participants completed 80 cycles of training trials featuring the error-clamped cursor with one perturbation angle (i.e., clamp size), depending on their group assignment. To assess the aftereffect, a session comprising 10 cycles of movement without visual feedback was administered.</p>
</sec>
<sec id="s3e">
<title>Experiment 3: Proprioception test with different perturbation sizes</title>
<p>Eleven participants were recruited for testing their proprioception recalibration. This experiment incorporated two types of trials: reaching trials and proprioception test trials. During the reaching trials, participants were instructed to aim for a target, which could appear at one of three possible locations (25°, 45°, or 65° counter-clockwise from the positive x-axis, as represented by light blue dots in <xref rid="fig4" ref-type="fig">Figure 4C</xref>, right panel). The task was similar to those in Experiments 1 and 2, with the key difference being that participants performed the task using KINARM robots (as depicted in <xref rid="fig7" ref-type="fig">Figure 7A</xref>). The dimensions and relative distances of the visual stimuli remained consistent with those used in Experiments 1 and 2. As in previous experiments, three kinds of visual feedback were provided during different sessions: no visual feedback, veridical feedback, and feedback featuring an error-clamped cursor.</p>
<p>In the proprioception test, participants were instructed to hold the robot’s right handle and wait for passive movement by the robot to one of six proprioception targets (small red dots in <xref rid="fig4" ref-type="fig">Figure 4C</xref>, right panel). These targets were spaced at 10° intervals, ranging from 20° to 70° counter-clockwise from the positive x-axis, and flanked the three reaching targets. The passive movement lasted for 1,000 ms and followed a straight-line path at a speed consistent with a minimum jerk velocity profile. During this movement, a ring with a 10 cm radius, centered at the start position, was displayed on the screen (depicted as a red arc in <xref rid="fig7" ref-type="fig">Figure 7B</xref>). The cursor was also replaced by a ring, its radius expanding as the hand moved toward the proprioception target.</p>
<p>After the right hand reached the proprioception target, participants were instructed to maintain their right hand’s position. Using the left handle, they were then asked to indicate the perceived location of their right hand. The position of the left handle was mapped to the rotation of a “dial,” which was constrained to the target arc.</p>
<p>The position of <bold><italic>h</italic></bold><italic>p</italic> was displayed on the target arc as a small red rectangle (a visual “dial,” as shown in <xref rid="fig7" ref-type="fig">Figure 7B</xref>). Participants were instructed to indicate the location of their right hand by moving the red rectangle to the position they perceived as accurate. The final position of <bold><italic>h</italic></bold><italic>p</italic> was recorded when its angular velocity remained below 1 degree/second for a duration exceeding 1000 ms. The proprioceptive bias was then calculated as the angular deviation between the actual hand position (<bold><italic>h</italic></bold><italic>R</italic>) and the perceived hand position (<bold><italic>h</italic></bold><italic>p</italic>).</p>
<p>Reaching trials and proprioception test trials were organized into blocks (<xref rid="fig4" ref-type="fig">Figure 4D</xref>). Each reaching block consisted of 6 trials, targeting 3 different locations with 2 repetitions each. Each reaching block was followed by a proprioception test block consisting of 3 trials. In these test trials, the robot moved the participant’s right hand toward a target position near one of the three reaching targets. These test targets were randomly chosen from six possible locations (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, right panel). The entire experiment comprised 40 reaching blocks and 40 subsequent proprioception test blocks. The first four reaching blocks provided veridical cursor feedback, the next four offered no cursor feedback, and the remaining 32 featured one of eight possible perturbation sizes (±10°, ±20°, ±40°, and ±80°). The size of the perturbation was randomized between blocks.</p>
</sec>
<sec id="s3f">
<title>Experiment 4: upregulating visual uncertainty affects implicit adaptation</title>
<p>Nineteen participants from Experiment 1 completed Experiment 4. The reaching task employed the same setup as in Experiment 1. However, instead of performing perceptual judgments of cursor motion direction, participants engaged in movements with one of three types of cursor feedback: veridical feedback, no feedback, and feedback with clamped perturbation. To assess the influence of visual uncertainty on implicit learning, we modified the cursor to appear blurred in half of the clamped trials. The blurring mask had a diameter of 6.8 mm, and the color intensity decreased from the cursor’s center following a two-dimensional Gaussian distribution with <italic>σx</italic> = <italic>σy</italic> = 1.4 mm. As depicted in <xref rid="fig5" ref-type="fig">Figure 5D</xref>, participants underwent the same procedures across three consecutive days. Each day consisted of 60 baseline trials, followed by 15 training blocks designed to assess single-trial learning. Within each training block, 12 trials featured an error-clamped cursor, each flanked by a trial without feedback. The difference between two adjacent no-feedback trials served as a measure of single-trial learning at specific perturbation sizes. Each of the 12 perturbation trials was randomly assigned one of 12 possible perturbations, comprising two cursor presentations (blurred or clear) and six clamp sizes (±4°, ±16°, ±64°).</p>
</sec>
<sec id="s3g">
<title>Data analysis</title>
<sec id="s3g1">
<title>Processing of kinematic data</title>
<p>In Experiments 1, 2, and 4, hand kinematic data were collected online at a sampling rate ranging between 160 and 200 Hz and subsequently resampled offline to 125 Hz. The movement direction of the hand was determined by the vector connecting the start position to the hand position at the point where it crossed 50% of the target distance, i.e., 5 cm from the start position.</p>
<p>In Experiment 3, hand positions and velocities were directly acquired from the KINARM robot at a fixed sampling rate of 1 kHz. The raw kinematic data were smoothed using a fifth-order Savitzky-Golay filter with a window length of 50 ms. Owing to the high temporal resolution and reliable velocity profiles provided by the KINARM system, the heading direction in Experiment 3 was calculated as the vector connecting the start position to the hand position at the point of peak velocity.</p>
</sec>
<sec id="s3g2">
<title>Psychometric curve</title>
<p>For the visual discrimination task, data of all three days were pooled together, the probability of responding that “the reference point was in the counter-clockwise direction of the cursor” was calculate as <italic>p</italic> for all angle differences (<xref ref-type="fig" rid="figs2">Figure S2</xref>). At each perturbation size, a logistic function was used to fit the probability distribution for individual participants:
<disp-formula id="eqn5">
<graphic xlink:href="568442v1_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where k is the slope and x0 is the origin of the logistic function. The visual uncertainty was defined as the angle differences between 25% and 75% of the logistic function:
<disp-formula id="eqn6">
<graphic xlink:href="568442v1_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s3g3">
<title>Statistical analysis</title>
<p>In Experiment 1, since the visual uncertainty <italic>σ</italic><sub><italic>v</italic></sub>follows a non-negative skewed distribution among participants, it violated the assumption of the ANOVA test. We thus applied Friedman’s nonparametric test to determine whether <italic>σ</italic><sub><italic>v</italic></sub> changes with the perturbation angle <italic>μ</italic>. Specifically, <italic>σ</italic><sub><italic>v</italic></sub>for both positive and negative <italic>μ</italic> were subjected to Friedman’s test separately, with <italic>μ</italic> serving as the factor. Given the symmetry between positive and negative <italic>μ</italic>, we pool the data to quantify the linear dependency of <italic>σ</italic><sub><italic>v</italic></sub> on the absolute <italic>μ</italic> (<xref rid="eqn4" ref-type="disp-formula">Eq. 4</xref>). Because <italic>σ</italic><sub><italic>v</italic></sub> is expected to be always positive, we assume that it is generated from a gamma distribution rather than a normal distribution. Thus, the data was fitted by a generalized linear regression model with the absolute value of <italic>μ</italic> as independent variable and <italic>σ</italic><sub><italic>v</italic></sub> as dependent variable.</p>
<p>In Experiment 2, the adaptation extent was defined as the mean hand angles in the last 10 cycles in the perturbation phase (cycle 101-110). A one-way ANOVA with perturbation size serving as the factor to examine its influence on the adaptation extent. Pairwise post-hoc comparisons were conducted using Tukey-Kramer correction.</p>
<p>In Experiment 3, proprioceptive recalibration was quantified as the angular difference between the perceived and actual hand directions. A one-way repeated-measures ANOVA was conducted on the data of first trial, using perturbation size as the within-subject factor. Greenhouse-Geisser corrections were applied when the assumption of sphericity was violated (<xref ref-type="bibr" rid="c20">Kirk, 1968</xref>). Multiple pairwise comparisons were conducted among different perturbation sizes for the first proprioception test. To determine if the proprioceptive biases were significantly different from zero, one-tailed (left) <italic>t</italic>-tests were conducted separately for the first and third proprioception test trials at each perturbation size.</p>
<p>In Experiment 4, the single-trial learning data was subjected to a 2 (visual uncertainty) x 3 (perturbation size) repeated-measures ANOVA. Greenhouse-Geisser corrections were applied as above, and the simple main effect of visual uncertainty was tested for each of the three perturbation sizes.</p>
</sec>
</sec>
<sec id="s3h">
<title>Model fitting and simulations</title>
<sec id="s3h1">
<title>Perceptual Error Adaptation (PEA) model</title>
<sec id="s3h1a">
<title>Model fitting for adaptation extent as a function of perturbation size</title>
<p>To fit the adaptation extent data from three different experiments in previous studies in (<xref ref-type="bibr" rid="c19">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="c34">Morehead et al., 2017</xref>), <xref rid="eqn3" ref-type="disp-formula">Eq. 3</xref> and <xref rid="eqn4" ref-type="disp-formula">Eq. 4</xref> were modified for simplification. To avoid overfitting of the small dataset, we reduced the number of model parameters by assuming that <italic>x̂</italic><sub><italic>Hand</italic></sub> asymptote to the target direction in the final adaptation trials that are used for computing adaptation extent, thus the retention rate <italic>A</italic> ≡ 1. Insert <xref rid="eqn4" ref-type="disp-formula">Eq. 4</xref> to <xref rid="eqn3" ref-type="disp-formula">Eq. 3</xref>, the asymptote hand angle with different perturbation size is:
<disp-formula id="eqn7">
<graphic xlink:href="568442v1_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Two ratio parameters <italic>R</italic><sub>1,<italic>ext</italic></sub> = <italic>σ</italic><sub><italic>p</italic></sub>⁄<italic>a</italic> and <italic>R</italic><sub>2,<italic>ext</italic></sub> = <italic>b</italic>⁄<italic>a</italic> were used in data fitting. Three datasets were fitted separately.</p>
</sec>
<sec id="s3h1b">
<title>Model fitting for trial-by-trial adaptation and proprioception changes</title>
<p>The trial-by-trial changes of adaptation (<xref rid="fig3" ref-type="fig">Figure 3A</xref>) and of proprioceptive localization (<xref rid="fig4" ref-type="fig">Figure 4A</xref>) was fitted with <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref>, <xref rid="eqn2" ref-type="disp-formula">Eq. 2</xref>, and <xref rid="eqn4" ref-type="disp-formula">Eq. 4</xref> based on the mean performance of all participants. The PEA model only had four free parameters, Θ = [<italic>σ</italic><sub><italic>u</italic></sub>, <italic>σ</italic><sub><italic>p</italic></sub>, <italic>A</italic>, <italic>B</italic>]. The slope <italic>a</italic> and intercept <italic>b</italic> in <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref> were obtained by psychometric tests from Experiment 1 (see statistical analysis). The reported hand position (<italic>x</italic><sub><italic>report</italic></sub>, blue dots in <xref rid="fig4" ref-type="fig">Figure 4A</xref>) was based on the proprioceptive cue <italic>x</italic><sub><italic>p</italic></sub>and the estimated hand <italic>x̂</italic><sub><italic>Hand</italic></sub>from the reaching trial. With the Bayesian cue combination assumption, the reported hand position was biased by <italic>x</italic><sub><italic>p</italic></sub>with a ratio determined by the variance of <italic>x</italic><sub><italic>p</italic></sub> and <italic>x̂</italic><sub><italic>Hand</italic></sub>:
<disp-formula id="eqn8">
<graphic xlink:href="568442v1_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="568442v1_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="568442v1_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are the variance of <italic>x̂</italic><italic><sub>Hand</sub></italic> and <italic>x</italic><sub>p</sub> respectively. To verify if the slope <italic>b</italic> and intercept <italic>a</italic> obtained from Experiment 1 are consistent across experiments, they were also estimated by fitting data from Experiment 2 (<xref rid="fig3" ref-type="fig">Figure 3</xref>). In this case, the model fitting was performed with 6 free parameters, Θ = [<italic>σ</italic><sub><italic>u</italic></sub>, <italic>σ</italic><sub><italic>p</italic></sub>, <italic>a</italic>, <italic>b, A</italic>, <italic>B</italic>]. The fitted values of <italic>a</italic> and <italic>b</italic> are fallen into the 95% CI of estimated parameters in Experiment 1 (purple line in <xref rid="fig2" ref-type="fig">Figure 2C</xref>, see details in <xref ref-type="table" rid="tbls1">Table S1</xref>).</p>
<p>The dependence of proprioceptive recalibration on perturbation size (<xref rid="fig4" ref-type="fig">Figure 4B</xref>) were simulated by the PEA model with the parameter values estimated from Experiment 2. We assumed that the proprioceptive bias results from the influence of a biased hand estimate (<italic>x̂</italic><sub><italic>Hand</italic></sub>) during adaptation and the influence is quantified as a percentage of its deviation from the true hand location:
<disp-formula id="eqn9">
<graphic xlink:href="568442v1_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the actual hand location is 0, <italic>R</italic><sub><italic>p</italic></sub> is the percentage of influence, and <italic>x̂</italic><sub><italic>Hand</italic></sub> is determined by <xref rid="eqn1" ref-type="disp-formula">Eq.1</xref>. In simulation, <italic>R</italic><sub><italic>p</italic></sub> varied from 0.05 to 0.8 to estimate the overall dependence of proprioceptive recalibration on perturbation size.</p>
</sec>
<sec id="s3h1c">
<title>Model fitting and simulation for single-trial learning</title>
<p>In the single-trial learning paradigm (<xref ref-type="fig" rid="figs5">Figure S5</xref>), the average movement direction across trials aligns with the target direction since the visual perturbations are evenly distributed in both directions. Thus, the sensory cue <italic>x</italic><sub><italic>u</italic></sub> and <italic>x</italic><sub><italic>p</italic></sub> have the same mean. For modeling single-trial learning, instead of having two separate cues, we assume a combined cue of <italic>x</italic><sub><italic>u</italic></sub> and <italic>x</italic><sub><italic>p</italic></sub> to follow <inline-formula><inline-graphic xlink:href="568442v1_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>T</italic> is the target direction, <inline-formula><inline-graphic xlink:href="568442v1_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> represents the variance of integrated sensory signal of <italic>x</italic><sub><italic>u</italic></sub> and <italic>x</italic><sub><italic>p</italic></sub>. Single-trial learning was quantified as the difference between the two null trials before and after the perturbation trial. As the perturbation size in the triplet of trials varied randomly, we assume that the effects of different perturbations are independent. Thus, single-trial learning was modeled as learning from the current perturbation without history effect. It follows the equations modified from <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref> and <xref rid="eqn2" ref-type="disp-formula">2</xref>:
<disp-formula id="eqn10">
<graphic xlink:href="568442v1_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn11">
<graphic xlink:href="568442v1_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>x</italic><sub><italic>v</italic></sub>is the visual perturbation, <italic>W</italic><sub><italic>int</italic></sub>and <italic>W</italic><sub><italic>v</italic></sub>are the weights of the cues, <italic>σ</italic><sub><italic>v</italic></sub>is the standard deviation of the visual cue specified by <xref rid="eqn4" ref-type="disp-formula">Eq. 4</xref>. Parameter set Θ = [<italic>σ</italic><sub><italic>int</italic></sub>, <italic>a</italic>, <italic>b, B</italic>] was fitted to the average data from all participants. Model simulations (<xref rid="fig5" ref-type="fig">Figure 5A</xref>) were performed with the same single-trial learning equations. For the clear cursor condition, we used the same parameter values estimated from Experiment 2 (see details in <xref ref-type="table" rid="tbls1">Table S1</xref>). For the blurred cursor condition, the standard deviation of visual cue was changed to:
<disp-formula id="eqn12">
<graphic xlink:href="568442v1_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
for the simulation of the increase in visual uncertainty, the ratio <italic>R</italic><sub><italic>v</italic></sub> varied from 1.1 to 3.</p>
</sec>
</sec>
<sec id="s3h2">
<title>PReMo model</title>
<p>We used the PReMo model to fit the average adaptation extent obtained from Experiment 2 (<xref rid="fig3" ref-type="fig">Figure 3C</xref> &amp; <xref ref-type="fig" rid="figs3">Figure S3B</xref>). Following the study by (<xref ref-type="bibr" rid="c55">Tsay, Kim, et al., 2022</xref>), the hand position at trial <italic>n+1</italic> is:
<disp-formula id="eqn13">
<graphic xlink:href="568442v1_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where
<disp-formula id="eqn14">
<graphic xlink:href="568442v1_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<disp-formula id="eqn15">
<graphic xlink:href="568442v1_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<p>In data fitting, we used two parameters to represent the ratio between sensory cues: <italic>R</italic><sub>1</sub> = <inline-formula><inline-graphic xlink:href="568442v1_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The data were fitted with the parameter set Θ = [<italic>R</italic>, <inline-formula><inline-graphic xlink:href="568442v1_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="568442v1_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the saturation angle, <italic>η<sub>p</sub></italic> is a scaling factor, <italic>A</italic> is the retention rate and <italic>B</italic> is the learning rate. For simulating the proprioceptive localization of the hand (<xref rid="fig4" ref-type="fig">Figure 4C</xref>), the parameter values estimated from Experiment 2 were used. The bias of hand estimation in the proprioception trials is determined as: <italic>x</italic><sub><italic>bias</italic></sub> = −(0 −<italic>x</italic><sub><italic>per</italic></sub>)<italic>R</italic><sub><italic>p</italic></sub>, where ratio <italic>R</italic><sub><italic>p</italic></sub> varies from 0.05 to 0.8. Thus, similar to the PEA model simulation, the proprioceptive bias is a fraction of the bias in the hand estimation from the adaptation trials. Single-trial learning (<xref rid="fig5" ref-type="fig">Figure 5B</xref>) was simulated with:
<disp-formula id="eqn16">
<graphic xlink:href="568442v1_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>x</italic><sub><italic>per</italic></sub>is determined by <xref rid="eqn12" ref-type="disp-formula">Eq. 12</xref> and <xref rid="eqn13" ref-type="disp-formula">Eq. 13</xref>. For the clear condition, we used the parameter values estimated from Experiment 2 with PReMo. For the blurred cursor condition, the standard deviation of visual signal <italic>σ</italic><sub><italic>v</italic>,<italic>blur</italic></sub> increases with a ratio <italic>R</italic><sub><italic>v</italic></sub>, as in <xref rid="eqn12" ref-type="disp-formula">Eq. 12</xref>.</p>
</sec>
<sec id="s3h3">
<title>Causal inference model</title>
<p>The causal inference model by (<xref ref-type="bibr" rid="c60">Wei &amp; Körding, 2009</xref>) was used to fit the data of Experiment 2 (<xref rid="fig3" ref-type="fig">Figure 3D</xref> &amp; <xref ref-type="fig" rid="figs3">Figure S3C</xref>). The hand position at trial n+1 is updated by learning from visual error at trial n:
<disp-formula id="eqn17">
<graphic xlink:href="568442v1_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>A</italic> and <italic>B</italic> are the retention and learning rates, respectively; <italic>T</italic> is the target direction. Specifically for this model, the learning from error is modulated by the probability (<italic>p</italic>) of causal attribution of visual error to the action or proprioception:
<disp-formula id="eqn18">
<graphic xlink:href="568442v1_eqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>x</italic><sub><italic>v</italic>,<italic>n</italic></sub> is the visual cue at trial n. <italic>S</italic> and <italic>C</italic> are the scaling factors, and <italic>σ</italic> is the standard deviation of the integrated cue combining visual and proprioceptive cues, following
<disp-formula id="eqn19">
<graphic xlink:href="568442v1_eqn19.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Thus, the data were fitted with five parameters Θ = [<italic>σ</italic>, <italic>S</italic>, <italic>C</italic>, <italic>A</italic>, <italic>B</italic>]. For simulating single-trial learning with cursor blurring (<xref rid="fig5" ref-type="fig">Figure 5C</xref>), the ratio between <italic>σ</italic><sub><italic>v</italic></sub> and <italic>σ</italic><sub><italic>p</italic></sub>is fixed as ½. The single-trial leaning was determined as:
<disp-formula id="eqn20">
<graphic xlink:href="568442v1_eqn20.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>p</italic> is determined by <xref rid="eqn18" ref-type="disp-formula">Eq. 18</xref>. Put <xref rid="eqn12" ref-type="disp-formula">Eq. 12</xref> and <xref rid="eqn19" ref-type="disp-formula">Eq. 19</xref> into <inline-formula><inline-graphic xlink:href="568442v1_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, we can calculate the standard deviation of the integrated sensory signal for the blurred cursor:</p>
<p><inline-formula><inline-graphic xlink:href="568442v1_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> Simulation was performed with <italic>R</italic> ranging from 1.1 to 3.</p>
</sec>
<sec id="s3h4">
<title>Data fitting</title>
<p>All data were fitted using MATLAB (2022b, MathWorks, Natick, MA, US) build-in function <italic>fmincon</italic> with 100 randomly sampled initial values of parameter sets. See <xref ref-type="fig" rid="figs1">Table S1</xref> and <xref ref-type="table" rid="tbls2">Table S2</xref> for the fitted parameter values and comparisons between different models.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<title>Data availability</title>
<p>Data presented in this work are available at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.24503926.v1">https://doi.org/10.6084/m9.figshare.24503926.v1</ext-link>.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Albert</surname>, <given-names>S. T.</given-names></string-name>, <string-name><surname>Jang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Modchalingam</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Marius’t Hart</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Henriques</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lerner</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Della-Maggiore</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Haith</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name>, &amp; <string-name><surname>Shadmehr</surname>, <given-names>R</given-names></string-name>. (<year>2022</year>). <article-title>Competition between parallel sensorimotor learning systems</article-title>. <source>ELife</source>, <volume>11</volume>, <fpage>e65361</fpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Albert</surname>, <given-names>S. T.</given-names></string-name>, <string-name><surname>Jang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sheahan</surname>, <given-names>H. R.</given-names></string-name>, <string-name><surname>Teunissen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Vandevoorde</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Herzfeld</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Shadmehr</surname>, <given-names>R</given-names></string-name>. (<year>2021</year>). <article-title>An implicit memory of errors limits human sensorimotor adaptation</article-title>. <source>Nature Human Behaviour</source>, <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Avraham</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Morehead</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>H. E.</given-names></string-name>, &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name>. (<year>2021</year>). <article-title>Reexposure to a sensorimotor perturbation produces opposite effects on explicit and implicit learning processes</article-title>. <source>PLoS Biology</source>, <volume>19</volume>(<issue>3</issue>), <fpage>e3001147</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Avraham</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Breska</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ivry</surname>, <given-names>R. B.</given-names></string-name>, &amp; <string-name><surname>McDougle</surname>, <given-names>S. D</given-names></string-name>. (<year>2022</year>). <article-title>Contextual effects in sensorimotor adaptation adhere to associative learning rules</article-title>. <source>ELife</source>, <volume>11</volume>, <fpage>e75801</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Berniker</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Kording</surname>, <given-names>K</given-names></string-name>. (<year>2008</year>). <article-title>Estimating the sources of motor errors for adaptation and generalization</article-title>. <source>Nature Neuroscience</source>, <volume>11</volume>(<issue>12</issue>), <fpage>1454</fpage>–<lpage>1461</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Berniker</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Kording</surname>, <given-names>K</given-names></string-name>. (<year>2011</year>). <article-title>Bayesian approaches to sensory integration for motor control</article-title>. <source>Wiley Interdisciplinary Reviews. Cognitive Science</source>, <volume>2</volume>(<issue>4</issue>), <fpage>419</fpage>–<lpage>428</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Bond</surname>, <given-names>K. M.</given-names></string-name>, &amp; <string-name><surname>Taylor</surname>, <given-names>J. A</given-names></string-name>. (<year>2015</year>). <article-title>Flexible explicit but rigid implicit learning in a visuomotor adaptation task</article-title>. <source>Journal of Neurophysiology</source>, <volume>113</volume>(<issue>10</issue>), <fpage>3836</fpage>–<lpage>3849</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Burge</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name>, &amp; <string-name><surname>Banks</surname>, <given-names>M. S</given-names></string-name>. (<year>2008</year>). <article-title>The statistical determinants of adaptation rate in human reaching</article-title>. <source>Journal of Vision</source>, <volume>8</volume>(<issue>4</issue>), <fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Cheng</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Sabes</surname>, <given-names>P. N</given-names></string-name>. (<year>2006</year>). <article-title>Modeling sensorimotor learning with linear dynamical systems</article-title>. <source>Neural Computation</source>, <volume>18</volume>(<issue>4</issue>), <fpage>760</fpage>–<lpage>793</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Cressman</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P</given-names></string-name>. (<year>2009</year>). <article-title>Sensory recalibration of hand position following visuomotor adaptation</article-title>. <source>Journal of Neurophysiology</source>, <volume>102</volume>(<issue>6</issue>), <fpage>3505</fpage>–<lpage>3518</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Della-Maggiore</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>McIntosh</surname>, <given-names>A. R</given-names></string-name>. (<year>2005</year>). <article-title>Time course of changes in brain activity and functional connectivity associated with long-term adaptation to a rotational transformation</article-title>. <source>Journal of Neurophysiology</source>, <volume>93</volume>(<issue>4</issue>), <fpage>2254</fpage>–<lpage>2262</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Donchin</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Francis</surname>, <given-names>J. T.</given-names></string-name>, &amp; <string-name><surname>Shadmehr</surname>, <given-names>R</given-names></string-name>. (<year>2003</year>). <article-title>Quantifying generalization from trial-by-trial behavior of adaptive systems that learn with basis functions: theory and experiments in human motor control</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>23</volume>(<issue>27</issue>), <fpage>9032</fpage>–<lpage>9045</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Liang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Abdollahi</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bittmann</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Körding</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Wei</surname>, <given-names>K</given-names></string-name>. (<year>2016</year>). <article-title>The statistical determinants of the speed of motor learning</article-title>. <source>PLoS Computational Biology</source>, <volume>12</volume>(<issue>9</issue>), <fpage>e1005023</fpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Henriques</surname>, <given-names>D. Y. P.</given-names></string-name>, &amp; <string-name><surname>Cressman</surname>, <given-names>E. K</given-names></string-name>. (<year>2012</year>). <article-title>Visuomotor adaptation and proprioceptive recalibration</article-title>. <source>Journal of Motor Behavior</source>, <volume>44</volume>(<issue>6</issue>), <fpage>435</fpage>–<lpage>444</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Herzfeld</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Vaswani</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Marko</surname>, <given-names>M. K.</given-names></string-name>, &amp; <string-name><surname>Shadmehr</surname>, <given-names>R</given-names></string-name>. (<year>2014</year>). <article-title>A memory of errors in sensorimotor learning</article-title>. <source>Science</source>, <volume>345</volume>(<issue>6202</issue>), <fpage>1349</fpage>–<lpage>1353</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Huberdeau</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name>, &amp; <string-name><surname>Haith</surname>, <given-names>A. M</given-names></string-name>. (<year>2019</year>). <article-title>Practice induces a qualitative change in the memory representation for visuomotor learning</article-title>. <source>Journal of Neurophysiology</source>, <volume>122</volume>(<issue>3</issue>), <fpage>1050</fpage>–<lpage>1059</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Hutter</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Taylor</surname>, <given-names>J. A</given-names></string-name>. (<year>2018</year>). <article-title>Relative sensitivity of explicit reaiming and implicit motor adaptation</article-title>. <source>Journal of Neurophysiology</source>, <volume>120</volume>(<issue>5</issue>), <fpage>2640</fpage>–<lpage>2648</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Izawa</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Shadmehr</surname>, <given-names>R</given-names></string-name>. (<year>2011</year>). <article-title>Learning from sensory and reward prediction errors during motor adaptation</article-title>. <source>PLoS Computational Biology</source>, <volume>7</volume>(<issue>3</issue>), <fpage>e1002012</fpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Kim</surname>, <given-names>H. E.</given-names></string-name>, <string-name><surname>Morehead</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Parvin</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Moazzezi</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name>. (<year>2018</year>). <article-title>Invariant errors reveal limitations in motor correction rather than constraints on error sensitivity</article-title>. <source>Communications Biology</source>, <volume>1</volume>(<issue>1</issue>), <fpage>19</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="book"><string-name><surname>Kirk</surname>, <given-names>R</given-names></string-name>. (<year>1968</year>). <source>Experimental design: Procedures for the behavioral sciences (Vol. 1–f)</source>. <publisher-name>Brooks/Cole</publisher-name>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Klein</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Levi</surname>, <given-names>D. M</given-names></string-name>. (<year>1987</year>). <article-title>Position sense of the peripheral retina</article-title>. <source>JOSA A</source>, <volume>4</volume>(<issue>8</issue>), <fpage>1543</fpage>–<lpage>1553</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Körding</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Wolpert</surname>, <given-names>D. M</given-names></string-name>. (<year>2004</year>). <article-title>Bayesian integration in sensorimotor learning</article-title>. <source>Nature</source>, <volume>427</volume>(<issue>6971</issue>), <fpage>244</fpage>–<lpage>247</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Ghez</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Ghilardi</surname>, <given-names>M. F</given-names></string-name>. (<year>2005</year>). <article-title>Adaptation to visuomotor transformations: consolidation, interference, and forgetting</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>25</volume>(<issue>2</issue>), <fpage>473</fpage>– <lpage>478</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Hadjiosif</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wong</surname>, <given-names>A. L.</given-names></string-name>, &amp; <string-name><surname>Haith</surname>, <given-names>A. M</given-names></string-name>. (<year>2019</year>). <article-title>Motor learning</article-title>. <source>Comprehensive Physiology</source>, <volume>9</volume>(<issue>2</issue>), <fpage>613</fpage>–<lpage>663</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Landi</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Baguear</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Della-Maggiore</surname>, <given-names>V</given-names></string-name>. (<year>2011</year>). <article-title>One week of motor adaptation induces structural changes in primary motor cortex that predict long-term memory one year later</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>31</volume>(<issue>33</issue>), <fpage>11808</fpage>–<lpage>11813</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Leow</surname>, <given-names>L.-A.</given-names></string-name>, <string-name><surname>Marinovic</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>de Rugy</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Carroll</surname>, <given-names>T. J.</given-names></string-name> (<year>2020</year>). <article-title>Task errors drive memories that improve sensorimotor adaptation</article-title>. <source>Journal of Neuroscience</source>, <volume>40</volume>(<issue>15</issue>), <fpage>3075</fpage>–<lpage>3088</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Levi</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Klein</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Yap</surname>, <given-names>Y. L</given-names></string-name>. (<year>1987</year>). <article-title>Positional uncertainty in peripheral and amblyopic vision</article-title>. <source>Vision Research</source>, <volume>27</volume>(<issue>4</issue>), <fpage>581</fpage>–<lpage>597</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Lieberman</surname>, <given-names>H. R.</given-names></string-name>, &amp; <string-name><surname>Pentland</surname>, <given-names>A. P</given-names></string-name>. (<year>1982</year>). <article-title>Microcomputer-based estimation of psychophysical thresholds: the best PEST</article-title>. <source>Behavior Research Methods &amp; Instrumentation</source>, <volume>14</volume>(<issue>1</issue>), <fpage>21</fpage>–<lpage>25</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Mazzoni</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Krakauer</surname>, <given-names>J. W</given-names></string-name>. (<year>2006</year>). <article-title>An implicit plan overrides an explicit strategy during visuomotor adaptation</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>26</volume>(<issue>14</issue>), <fpage>3642</fpage>–<lpage>3645</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>McDougle</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Bond</surname>, <given-names>K. M.</given-names></string-name>, &amp; <string-name><surname>Taylor</surname>, <given-names>J. A</given-names></string-name>. (<year>2015</year>). <article-title>Explicit and Implicit Processes Constitute the Fast and Slow Processes of Sensorimotor Learning</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>35</volume>(<fpage>26</fpage>).</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Mikulasch</surname>, <given-names>F. A.</given-names></string-name>, <string-name><surname>Rudelt</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Priesemann</surname>, <given-names>V</given-names></string-name>. (<year>2022</year>). <article-title>Visuomotor mismatch responses as a hallmark of explaining away in causal inference</article-title>. <source>Neural Computation</source>, <volume>35</volume>(<issue>1</issue>), <fpage>27</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Miyamoto</surname>, <given-names>Y. R.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>M. A</given-names></string-name>. (<year>2020</year>). <article-title>Implicit adaptation compensates for erratic explicit strategy in human motor learning</article-title>. <source>Nature Neuroscience</source>, <volume>23</volume>(<issue>3</issue>), <fpage>443</fpage>– <lpage>455</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Modchalingam</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vachon</surname>, <given-names>C. M.</given-names></string-name>, ‘<string-name><surname>t Hart</surname>, <given-names>B. M.</given-names></string-name>, &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P.</given-names></string-name> (<year>2019</year>). <article-title>The effects of awareness of the perturbation during motor adaptation on hand localization</article-title>. <source>PloS One</source>, <volume>14</volume>(<issue>8</issue>), <fpage>e0220884</fpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Morehead</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Parvin</surname>, <given-names>D. E.</given-names></string-name>, &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name>. (<year>2017</year>). <article-title>Characteristics of implicit sensorimotor adaptation revealed by task-irrelevant clamped feedback</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>29</volume>(<issue>6</issue>), <fpage>1061</fpage>–<lpage>1074</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Mostafa</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>‘t Hart</surname>, <given-names>B. M.</given-names></string-name>, &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P.</given-names></string-name> (<year>2019</year>). <article-title>Motor learning without moving: proprioceptive and predictive hand localization after passive visuoproprioceptive discrepancy training</article-title>. <source>PloS One</source>, <volume>14</volume>(<issue>8</issue>), <fpage>e0221861</fpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Neville</surname>, <given-names>K. M.</given-names></string-name>, &amp; <string-name><surname>Cressman</surname>, <given-names>E. K</given-names></string-name>. (<year>2018</year>). <article-title>The influence of awareness on explicit and implicit contributions to visuomotor adaptation over time</article-title>. <source>Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale</source>, <volume>236</volume>(<issue>7</issue>), <fpage>2047</fpage>–<lpage>2059</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Oh</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Schweighofer</surname>, <given-names>N</given-names></string-name>. (<year>2019</year>). <article-title>Minimizing Precision-Weighted Sensory Prediction Errors via Memory Formation and Switching in Motor Adaptation</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>39</volume>(<issue>46</issue>), <fpage>9237</fpage>– <lpage>9250</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Oldfield</surname>, <given-names>R. C</given-names></string-name>. (<year>1971</year>). <article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title>. <source>Neuropsychologia</source>, <volume>9</volume>(<issue>1</issue>), <fpage>97</fpage>–<lpage>113</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Rossi</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bastian</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Therrien</surname>, <given-names>A. S</given-names></string-name>. (<year>2021</year>). <article-title>Mechanisms of proprioceptive realignment in human motor learning</article-title>. <source>Current Opinion in Physiology</source>, <volume>20</volume>, <fpage>186</fpage>–<lpage>197</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Ruttle</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Cressman</surname>, <given-names>E. K.</given-names></string-name>, <string-name><surname>‘t Hart</surname>, <given-names>B. M.</given-names></string-name>, &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P.</given-names></string-name> (<year>2016</year>). <article-title>Time course of reach adaptation and proprioceptive recalibration during visuomotor learning</article-title>. <source>PloS One</source>, <volume>11</volume>(<issue>10</issue>), <fpage>e0163695</fpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Ruttle</surname>, <given-names>J. E.</given-names></string-name>, ‘<string-name><surname>t Hart</surname>, <given-names>B. M.</given-names></string-name>, &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P.</given-names></string-name> (<year>2018</year>). <article-title>The fast contribution of visual-proprioceptive discrepancy to reach aftereffects and proprioceptive recalibration</article-title>. <source>PloS One</source>, <volume>13</volume>(<issue>7</issue>), <fpage>e0200621</fpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Ruttle</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>‘t Hart</surname>, <given-names>B. M.</given-names></string-name>, &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P.</given-names></string-name> (<year>2021</year>). <article-title>Implicit motor learning within three trials</article-title>. <source>Scientific Reports</source>, <volume>11</volume>(<issue>1</issue>), <fpage>1627</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Salomonczyk</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Cressman</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P</given-names></string-name>. (<year>2011</year>). <article-title>Proprioceptive recalibration following prolonged training and increasing distortions in visuomotor adaptation</article-title>. <source>Neuropsychologia</source>, <volume>49</volume>(<issue>11</issue>), <fpage>3053</fpage>–<lpage>3062</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Salomonczyk</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Cressman</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P</given-names></string-name>. (<year>2013</year>). <article-title>The role of the cross-sensory error signal in visuomotor adaptation</article-title>. <source>Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale</source>, <volume>228</volume>(<issue>3</issue>), <fpage>313</fpage>–<lpage>325</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Seilheimer</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Rosenberg</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Angelaki</surname>, <given-names>D. E</given-names></string-name>. (<year>2014</year>). <article-title>Models and processes of multisensory cue combination</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>25</volume>, <fpage>38</fpage>–<lpage>46</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Shadmehr</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Krakauer</surname>, <given-names>J. W</given-names></string-name>. (<year>2010</year>). <article-title>Error correction, sensory prediction, and adaptation in motor control</article-title>. <source>Annual Review of Neuroscience</source>, <volume>33</volume>, <fpage>89</fpage>–<lpage>108</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Simani</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>McGuire</surname>, <given-names>L. M. M.</given-names></string-name>, &amp; <string-name><surname>Sabes</surname>, <given-names>P. N</given-names></string-name>. (<year>2007</year>). <article-title>Visual-shift adaptation is composed of separable sensory and task-dependent effects</article-title>. <source>Journal of Neurophysiology</source>, <volume>98</volume>(<issue>5</issue>), <fpage>2827</fpage>–<lpage>2841</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Ghazizadeh</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Shadmehr</surname>, <given-names>R</given-names></string-name>. (<year>2006</year>). <article-title>Interacting adaptive processes with different timescales underlie short-term motor learning</article-title>. <source>PLoS Biology</source>, <volume>4</volume>(<issue>6</issue>), <fpage>e179</fpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Stetson</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cui</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Montague</surname>, <given-names>P. R.</given-names></string-name>, &amp; <string-name><surname>Eagleman</surname>, <given-names>D. M</given-names></string-name>. (<year>2006</year>). <article-title>Motor-sensory recalibration leads to an illusory reversal of action and sensation</article-title>. <source>Neuron</source>, <volume>51</volume>(<issue>5</issue>), <fpage>651</fpage>–<lpage>659</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>‘t Hart</surname>, <given-names>B. M.</given-names></string-name>, &amp; <string-name><surname>Henriques</surname>, <given-names>D. Y. P.</given-names></string-name> (<year>2016</year>). <article-title>Separating predicted and perceived sensory consequences of motor learning</article-title>. <source>PloS One</source>, <volume>11</volume>(<issue>9</issue>), <fpage>e0163556</fpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Taylor</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name>. (<year>2011</year>). <article-title>Flexible cognitive strategies during motor learning</article-title>. <source>PLoS Computational Biology</source>, <volume>7</volume>(<issue>3</issue>), <fpage>e1001096</fpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Taylor</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name>, &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name>. (<year>2014</year>). <article-title>Explicit and implicit contributions to learning in a sensorimotor adaptation task</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>34</volume>(<issue>8</issue>), <fpage>3023</fpage>–<lpage>3032</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Thoroughman</surname>, <given-names>K. A.</given-names></string-name>, &amp; <string-name><surname>Shadmehr</surname>, <given-names>R</given-names></string-name>. (<year>2000</year>). <article-title>Learning of action through adaptive combination of motor primitives</article-title>. <source>Nature</source>, <volume>407</volume>(<issue>6805</issue>), <fpage>742</fpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Tsay</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Avraham</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>H. E.</given-names></string-name>, <string-name><surname>Parvin</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name>. (<year>2021</year>). <article-title>The effect of visual uncertainty on implicit motor adaptation</article-title>. <source>Journal of Neurophysiology</source>, <volume>125</volume>(<issue>1</issue>), <fpage>12</fpage>–<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Tsay</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Haith</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Ivry</surname>, <given-names>R. B.</given-names></string-name>, &amp; <string-name><surname>Kim</surname>, <given-names>H. E</given-names></string-name>. (<year>2022</year>). <article-title>Interactions between sensory prediction error and task error during implicit motor learning</article-title>. <source>PLoS Computational Biology</source>, <volume>18</volume>(<issue>3</issue>), <fpage>e1010005</fpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Tsay</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>H. E.</given-names></string-name>, <string-name><surname>Parvin</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Stover</surname>, <given-names>A. R.</given-names></string-name>, &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name>. (<year>2021</year>). <article-title>Individual differences in proprioception predict the extent of implicit sensorimotor adaptation</article-title>. <source>Journal of Neurophysiology</source>, <volume>125</volume>(<issue>4</issue>), <fpage>1307</fpage>–<lpage>1321</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Tsay</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Haith</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name>. (<year>2022</year>). <article-title>Understanding implicit sensorimotor adaptation as a process of proprioceptive re-alignment</article-title>. <source>ELife</source>, <volume>11</volume>, <fpage>e76639</fpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Tsay</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Parvin</surname>, <given-names>D. E.</given-names></string-name>, &amp; <string-name><surname>Ivry</surname>, <given-names>R. B</given-names></string-name>. (<year>2020</year>). <article-title>Continuous reports of sensed hand position during sensorimotor adaptation</article-title>. <source>Journal of Neurophysiology</source>, <volume>124</volume>(<issue>4</issue>), <fpage>1122</fpage>–<lpage>1130</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Tseng</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Shadmehr</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Bastian</surname>, <given-names>A. J</given-names></string-name>. (<year>2007</year>). <article-title>Sensory prediction errors drive cerebellum-dependent adaptation of reaching</article-title>. <source>Journal of Neurophysiology</source>, <volume>98</volume>(<issue>1</issue>), <fpage>54</fpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Wei</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Körding</surname>, <given-names>K</given-names></string-name>. (<year>2009</year>). <article-title>Relevance of error: what drives motor adaptation?</article-title> <source>Journal of Neurophysiology</source>, <volume>101</volume>(<issue>2</issue>), <fpage>655</fpage>–<lpage>664</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Wei</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Körding</surname>, <given-names>K</given-names></string-name>. (<year>2010</year>). <article-title>Uncertainty of feedback and state estimation determines the speed of motor adaptation</article-title>. <source>Frontiers in Computational Neuroscience</source>, <volume>4</volume>, <fpage>11</fpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Flanagan</surname>, <given-names>J. R</given-names></string-name>. (<year>2011</year>). <article-title>Principles of sensorimotor learning</article-title>. <source>Nature Reviews. Neuroscience</source>, <volume>12</volume>(<issue>12</issue>), <fpage>739</fpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Wozny</surname>, <given-names>D. R.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L</given-names></string-name>. (<year>2011</year>). <article-title>Recalibration of auditory space following milliseconds of cross-modal discrepancy</article-title>. <source>Journal of Neuroscience</source>, <volume>31</volume>(<issue>12</issue>), <fpage>4607</fpage>–<lpage>4612</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>H. G.</given-names></string-name>, <string-name><surname>Miyamoto</surname>, <given-names>Y. R.</given-names></string-name>, <string-name><surname>Castro</surname>, <given-names>L. N. G.</given-names></string-name>, <string-name><surname>Ölveczky</surname>, <given-names>B. P.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>M. A</given-names></string-name>. (<year>2014</year>). <article-title>Temporal structure of motor variability is dynamically regulated and predicts motor learning ability</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>(<issue>2</issue>), <fpage>312</fpage>.</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Supplementary Materials</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><p>Heat map of eye fixations during the 2AFC task in Experiment 1. The screen is partitioned into 10×10 pixel grids, and the cumulative number of gaze samples in each grid is recorded. Data from all participants, aggregated across each day of practice, are presented. The color map signifies the normalized count of gaze samples in each grid. Data are separately displayed for the three distinct phases of a trial, as delineated by the columns on the left, middle, and right. These correspond to periods during hand movement, the appearance of the visual mask and reference point, and the time allotted for manual response. On average, 95.06%, 89.93%, and 86.55% of gaze samples fall within the ±50-pixel range of the central line during these three phases, respectively. These results corroborate that participant adhered to the instructions and refrained from looking at the cursor during the visual discrimination task.</p></caption>
<graphic xlink:href="568442v1_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
<caption><p>Performance of an exemplary participant in Experiment 1. Six panels display the psychometric curves corresponding to different error-clamp sizes. The x-axis denotes the angular deviation between the clamped cursor and the reference point (as depicted in <xref rid="fig6" ref-type="fig">Figure 6A</xref>). A negative value implies that the reference point appears on the counterclockwise (CCW) side of the clamped cursor. The blue dots represent the proportion of trials in which the participant reported that “the yellow point is on the clockwise (CW) side of the clamped cursor” for various angular deviations between these two. Data were aggregated from all trials across three days of the experiment. The gray-shaded region represents the interquartile range (25th to 75th percentile) of the psychometric curve, and the width of this shaded region serves as an indicator of the amplitude of visual uncertainty.</p></caption>
<graphic xlink:href="568442v1_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3.</label>
<caption><p>Model fitting for observed implicit adaptation in Experiment 2. This supplementary figure provides a comprehensive evaluation of the three competing models: the PEA model, the PReMo model, and the causal inference model. <bold>(A)</bold> Results of PEA Model Fitting: The layout of these plots mirrors that of <xref rid="fig3" ref-type="fig">Figures 3A</xref> and <xref rid="fig3" ref-type="fig">3B</xref>, serving as a direct comparison between the empirical data and the predictions made by the PEA model. <bold>(B)</bold> Results of PReMo Model Fitting: The left panel is a duplicate of <xref rid="fig3" ref-type="fig">Figure 3C</xref>, while the right panel presents the trial-by-trial data fitting. This juxtaposition allows for a nuanced evaluation of the PReMo model’s performance at both the aggregate and individual trial levels. <bold>(C)</bold> Results of Causal Inference Model: The arrangement of these plots is consistent with panels (A) and (B), facilitating a straightforward comparison of all three models. For a detailed assessment of the quality of model fitting and subsequent model comparisons, please refer to <xref ref-type="table" rid="tbls1">Table S1</xref> and <xref ref-type="table" rid="tbls2">Table S2</xref>.</p></caption>
<graphic xlink:href="568442v1_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4.</label>
<caption><p>Correlation between initial learning rate and adaptation extent in Experiment 2. For each participant, the initial learning rate is calculated as the change in hand angle between the 1<sup>st</sup> and 10<sup>th</sup> cycle, divided by 10. The adaptation extent is defined as the average hand angle across the last 10 adaptation cycles. When pooling data across all perturbation sizes, a significant correlation is observed between the initial learning rate and the adaptation extent.</p></caption>
<graphic xlink:href="568442v1_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5.</label>
<caption><p>Model fitting of single-trial learning from Experiment 2 of (Tsay, <xref ref-type="bibr" rid="c3">Avraham, et al., 2021</xref>). Blue dots represent the mean single-trial learning across varying perturbation size, with error bars represent denoting standard errors across participants. The left, middle and right panel present the fitting results for the PEA, PReMo, and causal inference models, respectively. For additional details, refer to the Methods, Results, and <xref ref-type="table" rid="tbls1">Table S1</xref> &amp; <xref ref-type="table" rid="tbls2">S2</xref>.</p></caption>
<graphic xlink:href="568442v1_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S6.</label>
<caption><p>Proprioception uncertainty estimation results. Thirteen participants from Experiment 1 participated in a proprioception discrimination task to measure their proprioceptive uncertainty in the setting of the error-clamp adaptation. The setup paralleled that used for estimating visual uncertainty in Experiment 1. In each trial, participants initially held their hand at the starting position. They were instructed to relax their arm while the experimenter, seated on the other side of the monitor, pulled their hand to a proprioceptive target near the straight-ahead target. The arms of the experimenter and the participant were blocked from the view of the participant. After 0.8 seconds, a yellow reference point appeared. The angular deviation between the participant’s hand and this reference was determined using the same PEST procedure employed in Experiment 1. Participants indicated, by pressing left or right arrow keys by their left hand, whether the reference point appeared on the CW or CCW side of their actual right-hand position. The maximum deviation allowed was 30°, with an initial step size of 10° and a stop threshold of 0.5°. This task was conducted over six runs across three consecutive days. Similar to <xref ref-type="fig" rid="figs2">Figure S2</xref>, panel (A) to (M) show the psychometric curves for each participant with data from the three days pooled together. (N) and (O) present the measured proprioceptive uncertainty and bias for all participants (gray dots) and their mean ± standard deviation (red error bars).</p></caption>
<graphic xlink:href="568442v1_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table S1.</label><caption><title>Model fitting and simulation parameters with the PEA model.</title></caption>
<graphic xlink:href="568442v1_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Table S2.</label><caption><title>Model comparisons.</title></caption>
<graphic xlink:href="568442v1_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94608.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Orban de Xivry</surname>
<given-names>Jean-Jacques</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>KU Leuven</institution>
</institution-wrap>
<city>Leuven</city>
<country>Belgium</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents a <bold>valuable</bold> finding on the influence of visual uncertainty and Bayesian cue combination on implicit motor adaptation in young healthy participants. The evidence supporting the claims of the authors is <bold>solid</bold>, although a better discussion of the link between the model variables and the outcomes of related behavioral experiments would strengthen the conclusions. The work will be of interest to researchers in sensory cue integration and motor learning.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94608.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This valuable study demonstrates a novel mechanism by which implicit motor adaptation saturates for large visual errors in a principled normative Bayesian manner. Additionally, the study revealed two notable empirical findings: visual uncertainty increases for larger visual errors in the periphery, and proprioceptive shifts/implicit motor adaptation are non-monotonic, rather than ramp-like. This study is highly relevant for researchers in sensory cue integration and motor learning. However, I find some areas where statistical quantification is incomplete, and the contextualization of previous studies to be puzzling.</p>
<p>Issue #1: Contextualization of past studies.</p>
<p>While I agree that previous studies have focused on how sensory errors drive motor adaptation (e.g., Burge et al., 2008; Wei and Kording, 2009), I don't think the PReMo model was contextualized properly. Indeed, while PReMo should have adopted clearer language - given that proprioception (sensory) and kinaesthesia (perception) have been used interchangeably, something we now make clear in our new study (Tsay, Chandy, et al. 2023) - PReMo's central contribution is that a perceptual error drives implicit adaptation (see Abstract): the mismatch between the felt (perceived) and desired hand position. The current paper overlooks this contribution. I encourage the authors to contextualize PReMo's contribution more clearly throughout. Not mentioned in the current study, for example, PReMo accounts for the continuous changes in perceived hand position in Figure 4 (Figure 7 in the PReMo study).</p>
<p>There is no doubt that the current study provides important additional constraints on what determines perceived hand position: Firstly, it offers a normative Bayesian perspective in determining perceived hand position. PReMo suggests that perceived hand position is determined by integrating motor predictions with proprioception, then adding a proprioceptive shift; PEA formulates this as the optimal integration of these three inputs. Secondly, PReMo assumed visual uncertainty to remain constant for different visual errors; PEA suggests that visual uncertainty ought to increase (but see Issue #2).</p>
<p>Issue #2: Failed replication of previous results on the effect of visual uncertainty.</p>
<p>2a. A key finding of this paper is that visual uncertainty linearly increases in the periphery; a constraint crucial for explaining the non-monotonicity in implicit adaptation. One notable methodological deviation from previous studies is the requirement to fixate on the target: Notably, in the current experiments, participants were asked to fixate on the target, a constraint not imposed in previous studies. In a free-viewing environment, visual uncertainty may not attenuate as fast, and hence, implicit adaptation does not attenuate as quickly as that revealed in the current design with larger visual errors. Seems like this current fixation design, while important, needs to be properly contextualized considering how it may not represent most implicit adaptation experiments.</p>
<p>2b. Moreover, the current results - visual uncertainty attenuates implicit adaptation in response to large, but not small, visual errors - deviates from several past studies that have shown that visual uncertainty attenuates implicit adaptation to small, but not large, visual errors (Tsay, Avraham, et al. 2021; Makino, Hayashi, and Nozaki, n.d.; Shyr and Joshi 2023). What do the authors attribute this empirical difference to? Would this free-viewing environment also result in the opposite pattern in the effect of visual uncertainty on implicit adaptation for small and large visual errors?</p>
<p>2c. In the current study, the measure of visual uncertainty might be inflated by brief presentation times of comparison and referent visual stimuli (only 150 ms; our previous study allowed for a 500 ms viewing time to make sure participants see the comparison stimuli). Relatedly, there are some individuals whose visual uncertainty is greater than 20 degrees standard deviation. This seems very large, and less likely in a free-viewing environment.</p>
<p>2d. One important confound between clear and uncertain (blurred) visual conditions is the number of cursors on the screen. The number of cursors may have an attenuating effect on implicit adaptation simply due to task-irrelevant attentional demands (Parvin et al. 2022), rather than that of visual uncertainty. Could the authors provide a figure showing these blurred stimuli (gaussian clouds) in the context of the experimental paradigm? Note that we addressed this confound in the past by comparing participants with and without low vision, where only one visual cursor is provided for both groups (Tsay, Tan, et al. 2023).</p>
<p>Issue #3: More methodological details are needed.</p>
<p>3a. It's unclear why, in Figure 4, PEA predicts an overshoot in terms of perceived hand position from the target. In PReMo, we specified a visual shift in the perceived target position, shifted towards the adapted hand position, which may result in overshooting of the perceived hand position with this target position. This visual shift phenomenon has been discovered in previous studies (e.g., (Simani, McGuire, and Sabes 2007)).</p>
<p>3b. The extent of implicit adaptation in Experiment 2, especially with smaller errors, is unclear. The implicit adaptation function seems to be still increasing, at least by visual inspection. Can the authors comment on this trend, and relatedly, show individual data points that help the reader appreciate the variability inherent to these data?</p>
<p>3c. The same participants were asked to return for multiple days/experiments. Given that the authors acknowledge potential session effects, with attenuation upon re-exposure to the same rotation (Avraham et al. 2021), how does re-exposure affect the current results? Could the authors provide clarity, perhaps a table, to show shared participants between experiments and provide evidence showing how session order may not be impacting results?</p>
<p>3d. The number of trials per experiment should be detailed more clearly in the Methods section (e.g., Exp 4). Moreover, could the authors please provide relevant code on how they implemented their computational models? This would aid in future implementation of these models in future work. I, for one, am enthusiastic to build on PEA.</p>
<p>3f. In addition to predicting a correlation between proprioceptive shift and implicit adaptation on a group level, both PReMo and PEA (but not causal inference) predict a correlation between individual differences in proprioceptive shift and proprioceptive uncertainty with the extent of implicit adaptation (Tsay, Kim, et al. 2021). Interestingly, shift and uncertainty are independent (see Figures 4F and 6C in Tsay et al, 2021). Does PEA also predict independence between shift and uncertainty? It seems like PEA does predict a correlation.</p>
<p>References:</p>
<p>Avraham, Guy, Ryan Morehead, Hyosub E. Kim, and Richard B. Ivry. 2021. &quot;Reexposure to a Sensorimotor Perturbation Produces Opposite Effects on Explicit and Implicit Learning Processes.&quot; PLoS Biology 19 (3): e3001147.</p>
<p>
Makino, Yuto, Takuji Hayashi, and Daichi Nozaki. n.d. &quot;Divisively Normalized Neuronal Processing of Uncertain Visual Feedback for Visuomotor Learning.&quot;</p>
<p>
Parvin, Darius E., Kristy V. Dang, Alissa R. Stover, Richard B. Ivry, and J. Ryan Morehead. 2022. &quot;Implicit Adaptation Is Modulated by the Relevance of Feedback.&quot; BioRxiv. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2022.01.19.476924">https://doi.org/10.1101/2022.01.19.476924</ext-link>.</p>
<p>
Shyr, Megan C., and Sanjay S. Joshi. 2023. &quot;A Case Study of the Validity of Web-Based Visuomotor Rotation Experiments.&quot; Journal of Cognitive Neuroscience, October, 1-24.</p>
<p>
Simani, M. C., L. M. M. McGuire, and P. N. Sabes. 2007. &quot;Visual-Shift Adaptation Is Composed of Separable Sensory and Task-Dependent Effects.&quot; Journal of Neurophysiology 98 (5): 2827-41.</p>
<p>
Tsay, Jonathan S., Guy Avraham, Hyosub E. Kim, Darius E. Parvin, Zixuan Wang, and Richard B. Ivry. 2021. &quot;The Effect of Visual Uncertainty on Implicit Motor Adaptation.&quot; Journal of Neurophysiology 125 (1): 12-22.</p>
<p>
Tsay, Jonathan S., Anisha M. Chandy, Romeo Chua, R. Chris Miall, Jonathan Cole, Alessandro Farnè, Richard B. Ivry, and Fabrice R. Sarlegna. 2023. &quot;Implicit Motor Adaptation and Perceived Hand Position without Proprioception: A Kinesthetic Error May Be Derived from Efferent Signals.&quot; BioRxiv. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2023.01.19.524726">https://doi.org/10.1101/2023.01.19.524726</ext-link>.</p>
<p>
Tsay, Jonathan S., Hyosub E. Kim, Darius E. Parvin, Alissa R. Stover, and Richard B. Ivry. 2021. &quot;Individual Differences in Proprioception Predict the Extent of Implicit Sensorimotor Adaptation.&quot; Journal of Neurophysiology, March. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00585.2020">https://doi.org/10.1152/jn.00585.2020</ext-link>.</p>
<p>
Tsay, Jonathan S., Steven Tan, Marlena Chu, Richard B. Ivry, and Emily A. Cooper. 2023. &quot;Low Vision Impairs Implicit Sensorimotor Adaptation in Response to Small Errors, but Not Large Errors.&quot; Journal of Cognitive Neuroscience, January, 1-13.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94608.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The authors present the Perceptual Error Adaptation (PEA) model, a computational approach offering a unified explanation for behavioral results that are inconsistent with standard state-space models. Beginning with the conventional state-space framework, the paper introduces two innovative concepts. Firstly, errors are calculated based on the perceived hand position, determined through Bayesian integration of visual, proprioceptive, and predictive cues. Secondly, the model accounts for the eccentricity of vision, proposing that the uncertainty of cursor position increases with distance from the fixation point. This elegantly simple model, with minimal free parameters, effectively explains the observed plateau in motor adaptation under the implicit motor adaptation paradigm using the error-clamp method. Furthermore, the authors experimentally manipulate visual cursor uncertainty, a method established in visuomotor studies, to provide causal evidence. Their results show that the adaptation rate correlates with perturbation sizes and visual noise, uniquely explained by the PEA model and not by previous models. Therefore, the study convincingly demonstrates that implicit motor adaptation is a process of Bayesian cue integration</p>
<p>Strengths:</p>
<p>
In the past decade, numerous perplexing results in visuomotor rotation tasks have questioned their underlying mechanisms. Prior models have individually addressed aspects like aiming strategies, motor adaptation plateaus, and sensory recalibration effects. However, a unified model encapsulating these phenomena with a simple computational principle was lacking. This paper addresses this gap with a robust Bayesian integration-based model. Its strength lies in two fundamental assumptions: motor adaptation's influence by visual eccentricity, a well-established vision science concept, and sensory estimation through Bayesian integration. By merging these well-founded principles, the authors elucidate previously incongruent and diverse results with an error-based update model. The incorporation of cursor feedback noise manipulation provides causal evidence for their model. The use of eye-tracking in their experimental design, and the analysis of adaptation studies based on estimated eccentricity, are particularly elegant. This paper makes a significant contribution to visuomotor learning research.</p>
<p>Weaknesses:</p>
<p>
The paper provides a comprehensive account of visuomotor rotation paradigms, addressing incongruent behavioral results with a solid Bayesian integration model. However, its focus is narrowly confined to visuomotor rotation, leaving its applicability to broader motor learning paradigms, such as force field adaptation, saccadic adaptation, and de novo learning paradigms, uncertain. The paper's impact on the broader fields of neuroscience and cognitive science may be limited due to this specificity. While the paper excellently demonstrates that specific behavioral results in visuomotor rotation can be explained by Bayesian integration, a general computational principle, its contributions to other motor learning paradigms remain to be explored. The paper would benefit from a discussion on the model's generality and its limitations, particularly in relation to the undercompensating effects in other motor learning paradigms.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94608.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>
In this paper, the authors model motor adaptation as a Bayesian process that combines visual uncertainty about the error feedback, uncertainty about proprioceptive sense of hand position, and uncertainty of predicted (=planned) hand movement with a learning and retention rate as used in state space models. The model is built with results from several experiments presented in the paper and is compared with the PReMo model (Tsay, Kim, et al., 2022) as well as a cue combination model (Wei &amp; Körding, 2009). The model and experiments demonstrate the role of visual uncertainty about error feedback in implicit adaptation.</p>
<p>In the introduction, the authors notice that implicit adaptation (as measured in error-clamp-based paradigms) does not saturate at larger perturbations, but decreases again (e.g. Moorehead et al., 2017 shows no adaptation at 135{degree sign} and 175{degree sign} perturbations). They hypothesized that visual uncertainty about cursor position increases with larger perturbations since the cursor is further from the fixated target. This could decrease the importance assigned to visual feedback which could explain lower asymptotes.</p>
<p>The authors characterize visual uncertainty for 3 rotation sizes in the first experiment, and while this experiment could be improved, it is probably sufficient for the current purposes. Then the authors present a second experiment where adaptation to 7 clamped errors is tested in different groups of participants. The models' visual uncertainty is set using a linear fit to the results from experiment 1, and the remaining 4 parameters are then fit to this second data set. The 4 parameters are 1) proprioceptive uncertainty, 2) uncertainty about the predicted hand position, 3) a learning rate, and 4) a retention rate. The authors' Perceptual Error Adaptation model (&quot;PEA&quot;) predicts asymptotic levels of implicit adaptation much better than both the PReMo model (Tsay, Kim et al., 2022), which predicts saturated asymptotes, or a causal inference model (Wei &amp; Körding, 2007) which predicts no adaptation for larger rotations. In a third experiment, the authors test their model's predictions about proprioceptive recalibration, but unfortunately, compare their data with an unsuitable other data set. Finally, the authors conduct a fourth experiment where they put their model to the test. They measure implicit adaptation with increased visual uncertainty, by adding blur to the cursor, and the results are again better in line with their model (predicting overall lower adaptation) than with the PReMo model (predicting equal saturation but at larger perturbations) or a causal inference model (predicting equal peak adaptation, but shifted to larger rotations). In particular, the model fits experiment 2 and the results from experiment 4 show that the core idea of the model has merit: increased visual uncertainty about errors dampens implicit adaptation.</p>
<p>Strengths</p>
<p>
In this study, the authors propose a Perceptual Error Adaptation model (&quot;PEA&quot;) and the work combines various ideas from the field of cue combination, Bayesian methods, and new data sets, collected in four experiments using various techniques that test very different components of the model. The central component of visual uncertainty is assessed in the first experiment. The model uses 4 other parameters to explain implicit adaptation. These parameters are 1) learning and 2) retention rate, as used in popular state space models, and the uncertainty (variance) of 3) predicted and 4) proprioceptive hand position. In particular, the authors observe that asymptotes for implicit learning do not saturate, as claimed before, but decrease again when rotations are very large and that this may have to do with visual uncertainty (e.g. Tsay et al., 2021, J Neurophysiol 125, 12-22). The final experiment confirms predictions of the fitted model about what happens when visual uncertainty is increased (overall decrease of adaptation). By incorporating visual uncertainty depending on retinal eccentricity, the predictions of the PEA model for very large perturbations are notably different from and better than, the predictions of the two other models it is compared to. That is, the paper provides strong support for the idea that visual uncertainty of errors matters for implicit adaptation.</p>
<p>Weaknesses</p>
<p>
Although the authors don't say this, the &quot;concave&quot; function that shows that adaptation does not saturate for larger rotations has been shown before, including in papers cited in this manuscript.</p>
<p>The first experiment, measuring visual uncertainty for several rotation sizes in error-clamped paradigms has several shortcomings, but these might not be so large as to invalidate the model or the findings in the rest of the manuscript. There are two main issues we highlight here. First, the data is not presented in units that allow comparison with vision science literature. Second, the 1 second delay between the movement endpoint and the disappearance of the cursor, and the presentation of the reference marker, may have led to substantial degradation of the visual memory of the cursor endpoint. That is, the experiment could be overestimating the visual uncertainty during implicit adaptation.</p>
<p>The paper's third experiment relies to a large degree on reproducing patterns found in one particular paper, where the reported hand positions - as a measure of proprioceptive sense of hand position - are given and plotted relative to an ever-present visual target, rather than relative to the actual hand position. That is, 1) since participants actively move to a visual target, the reported hand positions do not reflect proprioception, but mostly the remembered position of the target participants were trying to move to, and 2) if the reports are converted to a difference between the real and reported hand position (rather than the difference between the target and the report), those would be on the order of ~20{degree sign} which is roughly two times larger than any previously reported proprioceptive recalibration, and an order of magnitude larger than what the authors themselves find (1-2{degree sign}) and what their model predicts. Experiment 3 is perhaps not crucial to the paper, but it nicely provides support for the idea that proprioceptive recalibration can occur with error-clamped feedback.</p>
<p>Perhaps the largest caveat to the study is that it assumes that people do not look at the only error feedback available to them (and can explicitly suppress learning from it). This was probably true in the experiments used in the manuscript, but unlikely to be the case in most of the cited literature. Ignoring errors and suppressing adaptation would also be a disastrous strategy to use in the real world, such that our brains may not be very good at this. So the question remains to what degree - if any - the ideas behind the model generalize to experiments without fixation control, and more importantly, to real-life situations.</p>
<p>Specific comments:</p>
<p>
A small part of the manuscript relies on replicating or modeling the proprioceptive recalibration in a study we think does NOT measure proprioceptive recalibration (Tsay, Parvin &amp; Ivry, JNP, 2020). In this study, participants reached for a visual target with a clamped cursor, and at the end of the reach were asked to indicate where they thought their hand was. The responses fell very close to the visual target both before and after the perturbation was introduced. This means that the difference between the actual hand position, and the reported/felt hand position gets very large as soon as the perturbation is introduced. That is, proprioceptive recalibration would necessarily have roughly the same magnitude as the adaptation displayed by participants. That would be several times larger than those found in studies where proprioceptive recalibration is measured without a visual anchor. The data is plotted in a way that makes it seem like the proprioceptive recalibration is very small, as they plot the responses relative to the visual target, and not the discrepancy between the actual and reported hand position. It seems to us that this study mostly measures short-term visual memory (of the target location). What is astounding about this study is that the responses change over time to begin with, even if only by a tiny amount. Perhaps this indicates some malleability of the visual system, but it is hard to say for sure.</p>
<p>Regardless, the results of that study do not form a solid basis for the current work and they should be removed. We would recommend making use of the dataset from the same authors, who improved their methods for measuring proprioception shifts just a year later (Tsay, Kim, Parvin, Stover, and Ivry, JNP, 2021). Although here the proprioceptive shifts during error-clamp adaptation (Exp 2) were tiny, and not quite significant (p&lt;0.08), the reports are relative to the actual location of the passively placed unseen hand, measured in trials separate from those with reach adaptation and therefore there is no visual target to anchor their estimates to.</p>
<p>Experiment 1 measures visual uncertainty with increased rotation size. The authors cite relevant work on this topic (Levi &amp; Klein etc) which has found a linear increase in uncertainty of the position of more and more eccentrically displayed stimuli.</p>
<p>First, this is a question where the reported stimuli and effects could greatly benefit from comparisons with the literature in vision science, and the results might even inform it. In order for that to happen, the units for the reported stimuli and effects should (also) be degrees of visual angle (dva).</p>
<p>As far as we know, all previous work has investigated static stimuli, where with moving stimuli, position information from several parts of the visual field are likely integrated over time in a final estimate of position at the end of the trajectory (a Kalman filter type process perhaps). As far as we know, there are no studies in vision science on the uncertainty of the endpoint of moving stimuli. So we think that the experiment is necessary for this study, but there are some areas where it could be improved.</p>
<p>Then, the linear fit is done in the space of the rotation size, but not in the space of eccentricity relative to fixation, and these do not necessarily map onto each other linearly. If we assume that the eye-tracker and the screen were at the closest distance the manufacturer reports it to work accurately at (45 cm), we would get the largest distances the endpoints are away from fixation in dva. Based on that assumed distance between the participant and monitor, we converted the rotation angles to distances between fixation and the cursor endpoint in degrees visual angle: 0.88, 3.5, and 13.25 dva (ignoring screen curvature, or the absence of it). The ratio between the perturbation angle and retinal distance to the endpoint is roughly 0.221, 0.221, and 0.207 if the minimum distance is indeed used - which is probably fine in this case. But still, it would be better to do fit in the relevant perceptual coordinate system.</p>
<p>The first distance (4 deg rotation; 0.88 dva offset between fixation and stimulus) is so close to fixation (even at the assumed shortest distance between eye and screen) that it can be considered foveal and falls within the range of noise of eye-trackers + that of the eye for fixating. There should be no uncertainty on or that close to the fovea. The variability in the data is likely just measurement noise. This also means that a linear fit will almost always go through this point, somewhat skewing the results toward linearity. The advantage is that the estimate of the intercept (measurement noise) is going to be very good. Unfortunately, there are only 2 other points measured, which (if used without the closest point) will always support a linear fit. Therefore, the experiment does not seem suitable to test linearity, only to characterize it, which might be sufficient for the current purposes. We'd understand if the effort to do a test of linearity using many more rotations requires too much effort. But then it should be made much clearer that the experiment assumes linearity and only serves to characterize the assumed linearity.</p>
<p>Final comment after the consultation session:</p>
<p>
There were a lot of discussions about the actual interpretation of the behavioral data from this paper with regards to past papers (Tsay et al. 2020 or 2021), and how it matches the different variables of the model. The data from Tsay 2020 combined both proprioceptive information (Xp) and prediction about hand position (Xu) because it involves active movements. On the other hand, Tsay et al. 2021 is based on passive movements and could provide a better measure of Xp alone. We would encourage you to clarify how each of the variables used in the model is mapped onto the outcomes of the cited behavioral experiments.</p>
<p>The reviewers discussed this point extensively during the consultation process. The results reported in the Tsay 2020 study reflect both proprioception and prediction. However, having a visual target contributes more than just prediction, it is likely an anchor in the workspace that draws the response to it. Such that the report is dominated by short-term visual memory of the target (which is not part of the model). However, in the current Exp 3, as in most other work investigating proprioception, this is calculated relative to the actual direction.</p>
<p>The solution is fairly simple. In Experiment 3 in the current study, Xp is measured relative to the hand without any visual anchors drawing responses, and this is also consistent with the reference used in the Tsay et al 2021 study and from many studies in the lab of D. Henriques (none of which also have any visual reach target when measuring proprioceptive estimates). So we suggest using a different data set that also measures Xp without any other influences, such as the data from Tsay et al 2021 instead.</p>
<p>These issues with the data are not superficial and can not be solved within the model. Data with correctly measured biases (relative to the hand) that are not dominated by irrelevant visual attractors would actually be informative about the validity of the PEA model. Dr. Tsay has so much other that we recommend using a more to-the-point data set that could actually validate the PEA model.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94608.1.sa4</article-id>
<title-group>
<article-title>Author Response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Zhaoran</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4192-4088</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Huijun</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Tianyang</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Nie</surname>
<given-names>Zixuan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wei</surname>
<given-names>Kunlin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5098-3808</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We take the liberty to thank all of you for your constructive and inspiring comments, which will help us substantially improve the final version of the paper. Before our final revision with details, I am writing this provisional letter to have a quick response to our reviewers’ comments.</p>
<p>I first give a quick and short summary for your public reviews, then respond point-by-point.</p>
<disp-quote content-type="editor-comment">
<p>Editors:</p>
<p>1. More discussion is needed.</p>
<p>2. More discussion about eye fixation during adaptation. Discuss why increasing visual uncertainty by blurring the cursor in the present study produces the opposite findings of previous studies (Tsay et al., 2021; Makino et al., 2023).</p>
<p>3. Discuss the broad impact of the current model.</p>
<p>4. Share the codes and the metadata (instead of the current data format).</p>
</disp-quote>
<p>Response: This is a concise summary of the major concerns listed in the public review. Given these concerns are easy to address, we are giving a quick but point-to-point response for now. The elaborate version will be put into our formal revision.</p>
<disp-quote content-type="editor-comment">
<p>**Reviewer 1: **</p>
<p>1. More credit should be given to the PReMo model: a) The PReMo model also proposes that perceptual error drives implicit adaptation, as in a new publication in Tsay et al., 2023, which was not public at the time of the current writing; and b) The PReMo model can account for some dataset, e.g. Fig 4A.</p>
</disp-quote>
<p>Response: We will add this new citation and point out that the new paper also uses the term perceptual error. We will also point out that the PReMo model has the potential to explain Fig 4A, though for now, it assumes an additional visual shift to explain the positive proprioceptive changes relative to the target. We would expand the discussion about the comparison between the two models.</p>
<disp-quote content-type="editor-comment">
<p>1. The present study produced an opposite finding of a previous finding, i.e., upregulating visual uncertainty (by cursor blurring here) decreases adaptation for large perturbations but less so for small perturbations, while previous studies have shown the opposite (by using a cursor cloud; Tsay et al., 2021; Makino et al., 2023). This needs explanation.</p>
</disp-quote>
<p>Response: Using the cursor cloud (Tsay et al., 2021, Makino et al., 2023) to modulate visual uncertainty has inherent drawbacks that make it unsuitable for testing the sensory uncertainty effect for visuomotor rotation. For the error clamp paradigm, the error is defined as angular deviation. The cursor cloud consists of multiple cursors spanning over a range of angles, which affects both the sensory uncertainty (the intended outcome) AND the sensory estimate of angles (the error itself, the undesired outcome). In Bayesian terms, the cursor cloud aims to modulate the sigma of a distribution (sigma_v in our model), but it additionally affects the mean of the distribution (mu). This unnecessary confound is avoided by using cursor blurring, which is still a cursor with its center (mu) unchanged from an un-blurred cursor. Furthermore, as correctly pointed out in the original paper by Tsay et al., 2021, the cursor cloud often overlaps with the visual target. This “target hit” would affect adaptation, possibly via a reward learning mechanism (See Kim et al., 2019 eLife). This is a second confound that accompanies the cursor cloud. We will expand our discussion to explain the discrepancy between our findings and previous findings.</p>
<disp-quote content-type="editor-comment">
<p>1. The estimation of visual uncertainty (our exp1) required people to fixate on the target, while this might not reflect the actual scenario during adaptation where people are free to look wherever they want.</p>
</disp-quote>
<p>Response: Our data shows otherwise: in a typical error-clamp setting, people fixate on the target for the majority of the time. For our Exp1, the fixation on the straight line between the starting position and the target is 86%-95% (as shown in Figure S1). We also collected eye-tracking data in our Exp4, which is a typical error-clamp experiment. More than 95% of gaze falls with +/- 50 pixels around the center of the screen, even slightly higher than Exp1. We will provide this part of the data in the revision. In fact, we designed our Exp1 to mimic the eye-tracking pattern as in typical error-clamp learning with carefully executed pilot experiments.</p>
<p>This high percentage of fixating on the target is not surprising: the error-clamp task requires participants to use their hands to move towards the target and to ignore the cursor. In fact, we would also like to point out that the high percentage of fixation on the aiming target is also true for conventional visuomotor rotation, which involves strategic re-aiming (shown in de Brouwer et al. 2018; Bromberg et al. 2019; we have an upcoming paper to show this). This is one reason that our new theory would also apply to other types of motor adaptation.</p>
<disp-quote content-type="editor-comment">
<p>1. More methodology details are needed. E.g., a figure showing the visual blurring, a figure showing individual data, a table showing data from individual sessions, code sharing, and a possible new correlational analysis.</p>
</disp-quote>
<p>Response: All these additional methodological/analysis information will be provided. We were self-limited by writing a short paper, but the revision would be extended for all these details.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2:</bold></p>
<p>1. More discussions are needed since the focus of this study is narrowly confined to visuomotor rotation. “A general computational principle, and its contributions to other motor learning paradigms remain to be explored”.</p>
</disp-quote>
<p>Response: This is a great suggestion since we also think our original Discussion has not elaborated on the possible broad impact of our theory. Our model is not limited to the error-clamp adaptation, where the participants were explicitly told to ignore the rotated cursor. The error-clamp paradigm is one rare example that implicit motor learning can be isolated in a nearly idealistic way. Our findings thus imply two key aspects of implicit adaptation: 1) localizing one’s effector is implicitly processed and continuously used to update the motor plan; 2) Bayesian cue combination is at the core of integrating multimodal feedback and motor-related cues (motor prediction cue in our model) when forming procedural knowledge for action control.</p>
<p>We will propose that the same two principles should be applied to various kinds of motor adaptation and motor skill learning, which constitutes motor learning in general. Most of our knowledge about motor adaptation is from visuomotor rotation, prism adaptation, force field adaptation, and saccadic adaptation. The first three types all involve localizing one’s effector under the influence of perturbed sensory feedback, and they also have implicit learning. We believe they can be modeled by variants of our model, or at least we should consider using the two principles above to think of their computational nature.
For skill learning, especially for de novo learning, the area still lacks a fundamental computational model that accounts for the skill acquisition process on the level of relevant movement cues. Our model suggests a promising route, i.e., repetitive movements with a Bayesian cue combination of movement-related cues might underlie the implicit process of motor skills.</p>
<p>We will add more discussion on the possible broad implications of our model in the revision.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 3:</bold></p>
<p>1. Similar to Reviewer 1, raised the concern about whether people’s fixation in typical motor adaptation settings is similar to the fixation that we instructed in our Exp1.</p>
</disp-quote>
<p>Response: see above.</p>
<disp-quote content-type="editor-comment">
<p>1. Similar to Reviewer 2, the concern was raised about whether our new theory is applicable to a broad context. Especially, error clamp appears to be a strange experimental manipulation that has no real-life appeal, “(i)Ignoring errors and suppressing adaptation would also be a disastrous strategy to use in the real world”.</p>
</disp-quote>
<p>Response: about the broad impact of our model, please see responses to Reviewer 2 above. We agree that ignoring errors (and thus “trying” to suppress adaptation) should not be a movement strategy for real-world intentional tasks. However, even in real life, we constantly attend to one thing and do the other thing; that’s when implicit motor processes are in charge. Furthermore, it is this exact “ignoring” instruction that elicits the implicit adaptation that we can work on. In this sense, the error-clamp paradigm is a great vehicle to isolate implicit adaptation and allows us to unpack its cognitive mechanism.</p>
<disp-quote content-type="editor-comment">
<p>1. In Exp1, the 1s delay between the movement end and the presentation of the reference cursor might inflate the actual visual uncertainty.</p>
</disp-quote>
<p>Response: The 1s delay of the reference cursor would not inflate the estimate of visual uncertainty. Our Exp1 used a similar paradigm by visual science (e.g., White, Levi, and Aitsebaomo, Vision Research, 1992), which shows that delay does not lead to an obvious increase in visual uncertainty over a broad range of values (from 0.2s to &gt;1s, see their Figure 5-6). We will add more methodology justifications in our revision.</p>
<disp-quote content-type="editor-comment">
<p>1. Our Fig4A used Tsay et al., 2021 data, which, in the reviewer’s view, is not an appropriate measure of proprioceptive bias. The reason is that in this dataset, “participants actively move to a visual target, the reported hand positions do not reflect proprioception, but mostly the remembered position of the target participants were trying to move to.”</p>
</disp-quote>
<p>Response: We agree that Tsay et al., 2021 study used an unconventional way to measure the influence of implicit adaptation on proprioception. And, their observed “proprioceptive changes” should not be called “proprioceptive bias” which is conventionally a reserved term for measuring the difference between the estimated hand location relative to the actual hand location (and better to be a passively moved hand). However, we think their dataset is still subject to the same Bayesian cue combination principle and thus can be modeled. Our modeling of this dataset includes all relevant cues: the implicitly perceived hand position and the proprioceptive cue (given that the hand stays at the movement end). Both cues are in the extrinsic coordinates, which happened to set the target position as zero. But where to set the zero (whether it is the target or the actual hand location) does not matter for the model fitting. Note that our Exp4 is also based on PEA modeling of proprioceptive bias, and this time the data is presented relative to the actual location.</p>
<p>In the revision, we would keep the current Fig4A and start to call the data as proprioceptive change as opposed to proprioceptive bias to follow the convention.</p>
</body>
</sub-article>
</article>