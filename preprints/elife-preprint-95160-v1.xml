<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95160</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95160</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95160.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Sensory-memory interactions via modular structure explain errors in visual working memory</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Yang</surname>
<given-names>Jun</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Hanqi</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9936-5293</contrib-id>
<name>
<surname>Lim</surname>
<given-names>Sukbin</given-names>
</name>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Weiyang College, Tsinghua University</institution>, Beijing, 100084, <country>People’s Republic of China</country></aff>
<aff id="a2"><label>2</label><institution>Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning, NYU Shanghai</institution>, 567 West Yangsi Road, Shanghai, 200126, <country>People’s Republic of China</country></aff>
<aff id="a3"><label>3</label><institution>Neural Science, NYU Shanghai</institution>, 567 West Yangsi Road, Shanghai, 200126, <country>People’s Republic of China</country></aff>
<aff id="a4"><label>4</label><institution>NYU-ECNU Institute of Brain and Cognitive Science at NYU Shanghai</institution>, 3663 Zhongshan Road North, Shanghai, 200062, <country>People’s Republic of China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Wei</surname>
<given-names>Xue-Xin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>UT Austin</institution>
</institution-wrap>
<city>Austin</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence: <email>sukbin.lim@nyu.edu</email></corresp>
<fn fn-type="con"><p><bold>Author Contributions</bold> J. Y., H. Z., and S.L. designed and performed research. All authors prepared the figures and wrote the manuscript.</p></fn>
<fn fn-type="others"><p><bold>Competing Interests</bold> The authors declare no competing financial interests.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-04-04">
<day>04</day>
<month>04</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95160</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-01-03">
<day>03</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-01-03">
<day>03</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.11.09.566396"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Yang et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Yang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95160-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Errors in stimulus estimation reveal how stimulus representation changes during cognitive processes. Repulsive bias and minimum variance observed near cardinal axes are well-known error patterns typically associated with visual orientation perception. Recent experiments suggest that these errors continuously evolve during working memory, posing a challenge that neither static sensory models nor traditional memory models can address. Here, we demonstrate that these evolving errors, maintaining characteristic shapes, require network interaction between two distinct modules. Each module fulfills efficient sensory encoding and memory maintenance, which cannot be achieved simultaneously in a single-module network. The sensory module exhibits heterogeneous tuning with strong inhibitory modulation reflecting natural orientation statistics. While the memory module, operating alone, supports homogeneous representation via continuous attractor dynamics, the fully connected network forms discrete attractors with moderate drift speed and nonuniform diffusion processes. Together, our work underscores the significance of sensory-memory interaction in continuously shaping stimulus representation during working memory.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Abstract changes and minor modification in the text</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The brain does not faithfully represent external stimuli. Even for low-level features like orientation, spatial frequency, or color of visual stimuli, their internal representations are thought to be modified by a range of cognitive processes, including perception, memory, and decision (<xref ref-type="bibr" rid="c17">Geisler 2008</xref>, <xref ref-type="bibr" rid="c52">Webster 2015</xref>, <xref rid="c3" ref-type="bibr">Bays, Schneegans et al. 2022</xref>). Experimental studies quantified such modification by analyzing behavior data or decoding neural activities. For instance, biases of errors, the systematic deviation from the original stimuli, observed in estimation tasks have been used as indirect evidence (<xref ref-type="bibr" rid="c54">Wei and Stocker 2017</xref>).</p>
<p>One important source of biases is adaptation to environmental statistics, such as nonuniform stimulus distribution in nature. Cardinal repulsion, which refers to the systematic shift away from the horizontal and vertical orientations observed in many perceptual tasks, is one of the examples (<xref rid="c11" ref-type="bibr">de Gardelle, Kouider et al. 2010</xref>). Theoretical works suggest that such a bias pattern reflects the prevalence of the cardinal orientations in natural scenes (<xref rid="c19" ref-type="bibr">Girshick, Landy et al. 2011</xref>). Similarly, the variance of errors for orientation stimuli was found to be inversely proportional to the stimulus statistics, minimum at cardinal and maximum at oblique orientations (<xref rid="c47" ref-type="bibr">van Bergen, Ma et al. 2015</xref>). It was postulated that the dependence of biases and variance of errors on natural statistics results from sensory encoding optimized to enhance precision around the most common stimuli (<xref ref-type="bibr" rid="c16">Ganguli and Simoncelli 2014</xref>, <xref ref-type="bibr" rid="c53">Wei and Stocker 2015</xref>, <xref ref-type="bibr" rid="c54">Wei and Stocker 2017</xref>).</p>
<p>On the other hand, there is a growing body of evidence indicating that error patterns are not solely influenced by sensory encoding but are also shaped by memory processes. In delayed estimation tasks, where participants are presented with stimuli followed by a delay period during which they rely on their working memory for estimation, it has been observed that representations of orientation or color stimuli undergo gradual and continuous modifications throughout the delay period (<xref rid="c35" ref-type="bibr">Panichello, DePasquale et al. 2019</xref>, <xref ref-type="bibr" rid="c2">Bae 2021</xref>, <xref rid="c20" ref-type="bibr">Gu, Lee et al. 2023</xref>). Such dynamic error patterns are inconsistent with sensory encoding models, most of which only establish a static relationship between stimuli and internal representations. Traditional working memory models are not suitable either. Most of them are constructed to faithfully maintain information about stimuli during the delay period, and thus, the memory representation has a similar geometry as that of the stimuli (<xref ref-type="bibr" rid="c51">Wang 2001</xref>, <xref ref-type="bibr" rid="c27">Khona and Fiete 2022</xref>). For continuous stimuli such as orientation, location, direction, or color, all stimuli are equally maintained in ring-like memory activities, predicting no biases (<xref ref-type="bibr" rid="c59">Zhang 1996</xref>, <xref rid="c9" ref-type="bibr">Compte, Brunel et al. 2000</xref>, <xref ref-type="bibr" rid="c6">Burak and Fiete 2009</xref>).</p>
<p>How can we explain error patterns in both perception and working memory tasks? Here, we claim that not a single module but a two-module network with recursive interaction is required. Each module has a distinct role — sensory encoding and memory maintenance. To illustrate this, we use orientation stimuli and examine how their representations change during the delayed estimation tasks. We employ two approaches to find solutions for generating correct error patterns. The first extends previously suggested sensory encoding models, while the second modifies low-dimensional memory models based on attractor dynamics. These approaches are integrated into the network models, which link network connectivity to neuronal tuning properties and behavioral error patterns and reveal the attractor dynamics through low-dimensional projection. Our results show that the sensory-memory interacting networks outperform single-module networks with better control over the shapes and evolution of dynamic error patterns. Furthermore, our network models emphasize the importance of inhibitory tuning in sensory circuits for generating correct error patterns under typical associative learning of natural statistics. Finally, we provide testable predictions regarding the effect of perturbations in sensory-memory interactions on error patterns in delayed estimation tasks.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Low-dimensional attractor models</title>
<p>In natural images, cardinal orientations are the most prevalent (<bold><xref rid="fig1" ref-type="fig">Figure 1A</xref></bold>). Error patterns in estimation tasks show dependence on such natural statistics, such as biases away from cardinal orientations where the variance of errors is nonetheless minimal (<bold><xref rid="fig1" ref-type="fig">Figure 1B,C</xref></bold>). In delayed estimation tasks, such a bias pattern is consolidated in time (<bold><xref rid="fig1" ref-type="fig">Figure 1B</xref></bold>). Also, experimental data suggested that estimation errors increase with a longer delay (<xref rid="c56" ref-type="bibr">Wimmer, Nykamp et al. 2014</xref>, <xref ref-type="bibr" rid="c41">Schneegans and Bays 2018</xref>), while the precision is still highest at cardinal orientations (<xref rid="c48" ref-type="bibr">van den Berg, Shin et al. 2012</xref>, <xref ref-type="bibr" rid="c4">Bays 2014</xref>, <xref rid="c47" ref-type="bibr">van Bergen, Ma et al. 2015</xref>). Thus, we assumed that the variance of errors increases as keeping its characteristic shape (<bold><xref rid="fig1" ref-type="fig">Figure 1C</xref></bold>). To explain these errors across orientations and over time, we first explored the underlying working memory mechanism. We considered low-dimensional attractor models with input noise that describe the drift and diffusion of the memory states. Here, we show that two prominent classes of previously suggested models are inconsistent with experimental observations and examine what modification to the models is required.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Error patterns of orientation stimuli in delayed-estimation tasks and low-dimensional attractor models. (<bold>A-C</bold>) Characteristic patterns of natural statistics of orientation stimuli <italic>θ</italic> (<bold>A</bold>), bias (<bold>B</bold>), and standard deviation (SD; <bold>C</bold>) during the delay period observed experimentally. Cardinal orientations are predominant in natural images (<bold>A</bold>). Bias and SD increase during the delay period, keeping patterns of repulsive bias (<bold>B</bold>) and minimum variance (<bold>C</bold>) around cardinal orientations. These characteristic patterns are visualized using trigonometric functions, and the range is normalized by their maximum values. Red vertical lines correspond to representative cardinal and oblique orientations, and with a periodicity of the error patterns, we only show the grey-shaded range in the remaining panels. (<bold>D-L</bold>) Comparison of different attractor models. (<bold>D-F</bold>) Continuous attractors with constant noise. Energy potential is flat (<bold>D</bold>), resulting in no bias (<bold>E</bold>) and uniform SD with uniform noise (<bold>F</bold>). (<bold>G-L</bold>) Discrete attractors with constant (<bold>G-I</bold>) and nonuniform noise (<bold>J-L</bold>). The discrete attractor models have potential hills and wells at cardinal and oblique orientations, respectively (<bold>G,J</bold>). While the bias patterns depend only on the energy landscape (<bold>H,K</bold>), SD representing variability also depends on noise (<bold>I,L</bold>). For the correct SD pattern (<bold>L</bold>), uneven noise with its maxima at the obliques (<bold>J</bold>) is required. Bias and SD patterns in the attractor models were obtained by running one-dimensional drift-diffusion models (see <bold>Methods</bold>).</p></caption>
<graphic xlink:href="566396v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The most widely accepted model for working memory of orientation stimuli has continuous attractor dynamics, which assumes that all orientations are equally encoded and maintained (<bold><xref rid="fig1" ref-type="fig">Figure 1D-F</xref></bold>). Each attractor corresponds to the memory state for different stimuli and forms a continuous ring following the geometry of orientation stimuli. The dynamics along continuous attractors are conceptually represented as movement along a flat energy landscape (<bold><xref rid="fig1" ref-type="fig">Figure 1D</xref></bold>). Without external input, there is no systematic shift of mean activity, that is, no drift during the delay period (<bold><xref rid="fig1" ref-type="fig">Figure 1E</xref></bold>). Also, under the assumption of equal influence of noise for all orientations, the variance of errors is spatially flat with constant diffusion along the ring, while the overall magnitude increases over time due to the accumulation of noise (<bold><xref rid="fig1" ref-type="fig">Figure 1F</xref></bold>).</p>
<p>While such continuous attractor models have been considered suitable for memory storage of continuous stimuli, they cannot capture drift dynamics observed during the delay period. Instead, discrete attractor models with uneven energy landscapes have been suggested with the energy wells corresponding to discrete attractors (<bold><xref rid="fig1" ref-type="fig">Figure 1G-I</xref></bold>). As evolution toward a few discrete attractors creates drift dynamics, the bias increases during the delay (<bold><xref rid="fig1" ref-type="fig">Figure 1H</xref></bold>). Also, discrete attractor models naturally produce nonuniform variance patterns. Even with constant noise along the ring, variance becomes minimum/maximum at the attractors/repellers due to the drift dynamics (<bold><xref rid="fig1" ref-type="fig">Figure 1I</xref></bold>). However, discrete attractor models with constant noise yield inconsistent results when inferring the locus of attractors from the bias and variance patterns observed in data. Cardinal orientations should be the repeller to account for cardinal repulsion. In contrast, the minimum variance observed at the cardinal orientations suggests they should be the attractors.</p>
<p>How can such inconsistency be resolved? One possible solution is discrete attractor models with nonuniform noise amplitude (<bold><xref rid="fig1" ref-type="fig">Figure 1J</xref></bold>). Let’s consider that attractors are formed at oblique orientations to generate correct bias patterns (<bold><xref rid="fig1" ref-type="fig">Figure 1K</xref></bold>). Additionally, we assumed that noise has the highest amplitude at the obliques. When the difference in the noise amplitude is large enough to overcome the attraction toward the obliques, the models can produce correct variance patterns, maximum at the obliques and minimum at cardinal orientations (<bold><xref rid="fig1" ref-type="fig">Figure 1L</xref></bold>). In sum, unlike two prominent memory models, continuous attractors or discrete attractors with constant noise, discrete attractors with maximum noise at the obliques could reproduce experimentally observed error patterns of orientation stimuli. Note that these attractor models often simplify the full network dynamics.</p>
<p>Namely, the drift and diffusion terms are derived by projecting network dynamics onto low-dimensional memory states (<xref ref-type="bibr" rid="c7">Burak and Fiete 2012</xref>, <xref ref-type="bibr" rid="c10">Darshan and Rivkind 2022</xref>). Thus, it is still in question whether there exist memory networks that can implement attractor dynamics with correct drift and diffusion terms.</p>
</sec>
<sec id="s2b">
<title>Bayesian sensory model and extension</title>
<p>Before exploring full memory network models, we note that previous theoretical works for sensory processing suggested that Bayesian inference with efficient coding could generate the repulsive bias and the lowest variance at cardinal orientations (<xref ref-type="bibr" rid="c53">Wei and Stocker 2015</xref>, <xref ref-type="bibr" rid="c54">Wei and Stocker 2017</xref>). Efficient coding theory suggests the sensory system should enhance the sensitivity around more common stimuli. For orientation stimuli, precision should be highest around cardinal directions, which could be achieved by sharpening the likelihood functions. Equipped with Bayesian optimal readout, such a sensory system could reproduce correct error patterns observed in perceptual tasks for various visual stimuli, including orientations (<bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Extension of Bayesian sensory models. (<bold>A</bold>) Schematics of extension to memory processing. We adapted the previous Bayesian models (<xref ref-type="bibr" rid="c53">Wei and Stocker 2015</xref>) for sensory encoding where <italic>θ</italic> and <inline-formula><inline-graphic xlink:href="566396v2_inline27.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are the input and output of sensory modules. We added a memory module where it maintains <inline-formula><inline-graphic xlink:href="566396v2_inline27a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> with the addition of memory noise <italic>ξ</italic>. The output of the memory module, <inline-formula><inline-graphic xlink:href="566396v2_inline28.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, is fed back to the sensory module as the input for the next iteration. (<bold>B</bold>) Illustration of the first iteration of sensory-memory interaction. Prior distribution follows the natural statistics (top), resulting in a sharper likelihood function near cardinal orientations (middle). Combining prior and likelihood functions leads to the posterior distribution of decoded <inline-formula><inline-graphic xlink:href="566396v2_inline29.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (light colors at the bottom), which is broadened with the addition of memory noise (dark colors at the bottom). Different curves correspond to different initial <italic>θ</italic>. (<bold>C</bold>) Bias (top) and SD (bottom) patterns obtained from decoded <inline-formula><inline-graphic xlink:href="566396v2_inline30.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for the 1<sup>st</sup>, 2<sup>nd</sup>, and 3<sup>rd</sup> iterations.</p></caption>
<graphic xlink:href="566396v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>However, such models only account for the relationship between external and perceived stimuli during sensory processing, resulting in static error patterns. Here, we extended the framework so that the system can maintain information about the stimulus after its offset while bias and variance of errors grow in time (<bold><xref rid="fig2" ref-type="fig">Figure 2A</xref></bold>). We added a memory stage to Bayesian sensory models such that the memory stage receives the output of the sensory stage and returns it as the input after the maintenance. For instance, let’s denote the external orientation stimulus given during the stimulus period as <italic>θ</italic><sub>1</sub>. The sensory stage receives <italic>θ</italic><sub>1</sub> as input and generates the perceived orientation, <inline-formula><inline-graphic xlink:href="566396v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which varies trial-to-trial with sensory noise (<bold><xref rid="fig2" ref-type="fig">Figure 2B</xref></bold>). Through the memory stage, <inline-formula><inline-graphic xlink:href="566396v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is returned as the input to the sensory stage for the next iteration with the addition of memory noise <italic>ξ</italic><sub>1</sub>.</p>
<p>Such a recursive process mimics interactions between sensory and memory systems where the sensory system implements efficient coding and Bayesian inference, and the memory system faithfully maintains information. As the recursive process iterates, the distribution of the internal representation of orientation broadens due to the accumulation of noise from the sensory and memory systems. This leads to an increase of bias and variance at each step while keeping their characteristic shapes (<bold><xref rid="fig2" ref-type="fig">Figure 2C</xref></bold>). Thus, recurrent interaction between sensory and memory systems during the delay period, each of which meets different demands, successfully reproduces correct error patterns observed in both perception and memory tasks.</p>
</sec>
<sec id="s2c">
<title>Network models with sensory and memory modules</title>
<p>Next, we construct network models capturing the sensory-memory interactions formalized under the Bayesian framework. We consider two-module networks where each module corresponds to the sensory and memory systems. To generate orientation selectivity, both modules have a columnar architecture where neurons in each column have a similar preference for orientation (<bold><xref rid="fig3" ref-type="fig">Figure 3A</xref></bold>). However, their connectivity structures are different (<bold><xref rid="fig3" ref-type="fig">Figure 3B</xref></bold>). The memory module in isolation resembles the traditional ring attractor network with a strong and homogeneous recurrent connection. This enables the memory module in isolation to maintain information about all orientations equally during the delay period (<bold><xref rid="fig3" ref-type="fig">Figure 3B-F</xref></bold>, right). Conversely, the recurrent connectivity strengths in the sensory module are relatively weak, such that without connection to the memory module, the activities during the delay period decay back to the baseline levels (<bold><xref rid="fig3" ref-type="fig">Figure 3B</xref></bold>, left). Furthermore, the connectivity strengths across columns are heterogeneous, particularly stronger at the obliques. As a result, the tuning curves near cardinal orientations can be sharper and denser, consistent with experimental observations showing a larger number of cardinally tuned neurons (<xref rid="c32" ref-type="bibr">Li, Peterson et al. 2003</xref>, <xref rid="c43" ref-type="bibr">Shen, Tao et al. 2014</xref>) and their narrower tuning (<xref rid="c32" ref-type="bibr">Li, Peterson et al. 2003</xref>, <xref rid="c28" ref-type="bibr">Kreile, Bonhoeffer et al. 2011</xref>) (<bold><xref rid="fig3" ref-type="fig">Figure 3C-F</xref></bold>, left). Different response activities of the two modules in isolation are demonstrated in their response manifolds as sparser representations around cardinal orientations in the sensory module, compared to the ring-like geometry of the memory module (<bold><xref rid="fig3" ref-type="fig">Figure 3F</xref></bold>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Network models of sensory and memory circuits in isolation. (<bold>A</bold>) Schematics of columnar architecture for orientation selectivity. Neurons in the same column have similar preferred orientations, and recurrent connections are a combination of local excitation and global inhibition, represented as triangles and circles, respectively. (<bold>B</bold>-<bold>F</bold>) Connectivity and tuning properties of the sensory network (left column) and memory network (right column). (<bold>B</bold>) Example connectivity strengths. We indexed neurons by <italic>ψ</italic> ranging uniformly from 0° to 180°. The connectivity strengths depend only on <italic>ψ</italic>’s of the presynaptic and postsynaptic neurons. Each curve shows the connectivity strengths from presynaptic neuron <italic>ψ</italic> to an example postsynaptic neuron. Unlike the homogeneous connectivity in the memory network (right), the sensory connectivity is heterogeneous, and its degree is denoted by <italic>α</italic>. (<bold>C</bold>) Heterogeneous tuning curves for different stimulus <italic>θ</italic> in the sensory network in the stimulus period (left) and homogeneous ones in the memory network in the delay period (right). The memory network can sustain persistent activity in isolation, while the sensory network cannot. (<bold>D</bold>) Histograms of the preferred orientations. We measured the maximum of the tuning curve of each neuron, denoted as <inline-formula><inline-graphic xlink:href="566396v2_inline31.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<bold>Methods</bold>). The heterogeneous sensory network has more cardinally tuned neurons. (<bold>E</bold>) Widths of tuning curves measured at the half maximum of the tuning curves (<bold>Methods</bold>). The sensory tuning curves sharpen around cardinal orientations. Each neuron is labeled with its index <italic>ψ</italic> as in (<bold>B</bold>). (<bold>F</bold>) Neural manifolds projected onto the first two principal components of activities during the stimulus period (left) and during the delay period (right). The neural manifold of the sensory network resembles a curved ellipsoid, while the manifold corresponding to the homogeneous memory network is a perfect ring.</p></caption>
<graphic xlink:href="566396v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>For sensory-memory interacting networks, we connected the two modules with inter-module connections set to be stronger between neurons with similar orientation selectivity (<bold><xref rid="fig4" ref-type="fig">Figure 4A</xref></bold>). Activity profiles in both modules follow that of the sensory module — heterogeneous with narrower and denser tuning curves around cardinal orientations, leading to higher sensitivity (<bold><xref rid="fig4" ref-type="fig">Figure 4B</xref></bold>). Such activity pattern is maintained even during the delay period when recurrent connections in the memory module support activities of both sensory and memory modules (<bold><xref rid="fig4" ref-type="fig">Figure 4B</xref></bold>, right). Note that while sensory activities convey stimulus information during the delay period, their overall firing rates are much lower than those during the stimulus period with weak interconnection strengths. Such low firing rates may lead to both positive and negative evidence of sustained activity in early sensory areas (<xref rid="c31" ref-type="bibr">Leavitt, Mendoza-Halliday et al. 2017</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Network model with interacting sensory and memory modules. (<bold>A</bold>) Schematic of two-module architecture. The sensory and memory modules are connected via feedforward and feedback connectivity to form a closed loop. The sensory module receives external input with orientation <italic>θ</italic> while internal representation is decoded from the memory module, denoted as <inline-formula><inline-graphic xlink:href="566396v2_inline32.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. (<bold>B</bold>) Tuning curves of sensory (upper panels) and memory (lower panels) modules at the end of the stimulus epoch (i.e., the beginning of the delay epoch; left panels) and during the delay period (right panels). Note that while both modules can sustain persistent activity in the delay period, the firing rates of the sensory module are significantly lower than those in the stimulus period (upper right). (<bold>C-E</bold>) Bias (<bold>C</bold>), standard deviation (SD; <bold>D</bold>), and Fisher information (FI; <bold>E</bold>) patterns evaluated at 1, 2.5, and 4 seconds into the delay, consistent with the characteristic patterns observed experimentally (<bold><xref ref-type="fig" rid="fig1">Figure 1A-C</xref></bold>). While FI decays due to noise accumulation, it is largest around cardinal orientations, corresponding to a smaller discrimination threshold (<bold>E</bold>). In (<bold>C</bold>) and (<bold>D</bold>), shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption>
<graphic xlink:href="566396v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>When the internal representation of the orientation stimulus is read from the memory module, the sensory-memory interacting network exhibits repulsive bias and minimum variance at cardinal orientations, inheriting from efficient sensory coding (<bold><xref rid="fig4" ref-type="fig">Figure 4C,D</xref></bold>). Such bias increases during the delay period with increasing asymmetry of tuning widths despite lower firing rates than the stimulus period (<bold><xref rid="fig4S1" ref-type="fig">Figure 4 – Figure supplement 1</xref></bold>). At the same time, errors gradually increase due to noise accumulation in time, as in typical memory networks (<xref rid="c9" ref-type="bibr">Compte, Brunel et al. 2000</xref>, <xref ref-type="bibr" rid="c7">Burak and Fiete 2012</xref>). We obtained Fisher information measuring sensitivity at each orientation from the neural responses (see <bold>Methods</bold>). Opposite to the variance of errors, Fisher information is highest at cardinal orientations, while it decreases during the delay period (<bold><xref rid="fig4" ref-type="fig">Figure 4E</xref></bold>). Thus, the sensory-memory interacting network model that mechanistically embodies the extension of the Bayesian sensory model correctly reproduces the error patterns observed in delayed estimation tasks.</p>
</sec>
<sec id="s2d">
<title>Analysis of low-dimensional memory states</title>
<p>To further understand the mechanisms of generating the correct error patterns in sensory-memory interacting networks, we analyzed the network dynamics during the delay period. For this, we identified the low-dimensional manifold that has slow dynamics during the delay period, which corresponds to the memory states (<bold><xref rid="fig5" ref-type="fig">Figure 5A</xref></bold>). We projected the dynamics along this manifold to obtain the drift and diffusion terms (<bold><xref rid="fig5" ref-type="fig">Figure 5A-C</xref></bold>; <bold><xref rid="fig5S1" ref-type="fig">Figure 5 – Figure supplement 1</xref></bold>). The drift term shows similar patterns to cardinal repulsion (<bold><xref rid="fig5" ref-type="fig">Figure 5B,E</xref></bold>). Integrating this drift for orientation yields the energy function, which is minimum at the obliques (<bold><xref rid="fig5" ref-type="fig">Figure 5D</xref></bold>). This suggests that the network implements discrete attractor dynamics with attractors formed at the obliques. The diffusion term is also uneven — the noise amplitude is maximum at the obliques so that despite attraction toward them, the variance of errors can be maximum (<bold><xref rid="fig5" ref-type="fig">Figure 5C,F</xref></bold>). The nonuniform characteristics of both drift and diffusion processes align with the solution identified in low-dimensional memory models (<bold><xref rid="fig1" ref-type="fig">Figure 1J-L</xref></bold>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Low-dimensional dynamics along memory manifold and its parameter dependence. (<bold>A</bold>) Low-dimensional projection along the memory states. Left panel: The memory manifold projected to the first two PCs associated with the vector fields. Right panel: Example drift-diffusion trajectories along the memory manifold starting at <italic>θ</italic> = 112.5<sup>°</sup>. (<bold>B,C</bold>) Velocity (<bold>B</bold>) and noise coefficients (<bold>C</bold>) corresponding to drift and diffusion processes. Different grey scales represent different heterogeneity degrees in the sensory module, <italic>α</italic> in <bold><xref ref-type="fig" rid="fig3">Figure 3B</xref></bold>. The velocity with which the remembered orientation drifts to the obliques in a noise-free network (<bold>B</bold>). A larger noise coefficient around the obliques overcomes the underlying drift dynamics and causes the standard deviation pattern to reach its maxima at the obliques (<bold>C</bold>). (<bold>D</bold>) Equivalent one-dimensional energy potential derived from the velocity in (<bold>B</bold>). (<bold>E,F</bold>) Example bias (<bold>E</bold>) and standard deviation (<bold>F</bold>) patterns at 4s into the delay. The shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption>
<graphic xlink:href="566396v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, we examined how heterogeneity of connectivity in the sensory module affects the dynamics along the memory states. The magnitude of heterogeneity is denoted as <italic>α</italic>, and larger <italic>α</italic> represents a larger asymmetry of connectivity strengths at cardinal and oblique orientations (<bold><xref rid="fig3" ref-type="fig">Figure 3B</xref></bold>, left). When <italic>α</italic> increases, the asymmetry of drift and energy levels becomes more prominent, leading to a more rapid increase in bias (<bold><xref rid="fig5" ref-type="fig">Figure 5B,D,E</xref></bold>). The diffusion term is also more asymmetric, compensating for stronger attraction to the obliques (<bold><xref rid="fig5" ref-type="fig">Figure 5C</xref></bold>). Thus, for larger <italic>α</italic>, the variability of errors is still higher at the obliques (<bold><xref rid="fig5" ref-type="fig">Figure 5F</xref></bold>). We also note that the overall increase of intermodal connectivity strengths has similar effects, which enhances the influence of the heterogeneity in the sensory module on the network dynamics (<bold><xref rid="fig5S2" ref-type="fig">Figure 5 – Figure supplement 2</xref></bold>).</p>
<p>In sum, network models emphasizing the interactions between sensory and memory modules can embrace solutions found under two separate approaches, one among low-dimensional memory models with attractor dynamics and the other by extending the Bayesian sensory system. The asymmetry of drift and diffusion terms, and thereby, the bias and variability of errors, are determined by the degree of heterogeneity in the sensory module or its influence on the overall dynamics.</p>
</sec>
<sec id="s2e">
<title>Importance of heterogeneously tuned inhibition</title>
<p>We showed that network models realizing sensory-memory interactions reproduce correct error patterns, where each module has a different connectivity structure. Previous work suggested that such a heterogeneous connection of the sensory system may arise from experience-dependent synaptic modification (<xref ref-type="bibr" rid="c34">Olshausen and Field 1996</xref>, <xref rid="c60" ref-type="bibr">Zylberberg, Murphy et al. 2011</xref>). For example, typical Hebbian learning is thought to potentiate connectivity strengths between neurons whose preferred stimuli are more frequently encountered. For orientations, cardinal directions are predominant in natural scenes. Thus, if experience-dependent learning occurs mainly at the excitatory synapses, the excitatory connections near cardinal orientations become stronger in the sensory module. This is opposite to the previously discussed case where the sensory module has the strongest connection at the obliques. With the strongest excitatory connections at cardinal orientations, the error patterns are reversed, resulting in cardinal attraction instead of repulsion, and the lowest variance occurs at the obliques.</p>
<p>Inhibitory synaptic connections can also be modified through learning (<xref rid="c50" ref-type="bibr">Vogels, Froemke et al. 2013</xref>, <xref rid="c26" ref-type="bibr">Khan, Poort et al. 2018</xref>, <xref rid="c30" ref-type="bibr">Larisch, Gonner et al. 2021</xref>). Here, we considered that experience-dependent learning exists in both excitatory and inhibitory pathways and similarly shapes their connectivity (<bold><xref rid="fig6" ref-type="fig">Figure 6A</xref></bold>). We assumed that excitatory and inhibitory connections are segregated and stronger near cardinal orientations (<bold><xref rid="fig6" ref-type="fig">Figure 6B</xref></bold>). We modulated the heterogeneity degree of both excitatory and inhibitory connections, denoted as <italic>α</italic> and <italic>β</italic>, respectively (<bold><xref rid="fig6" ref-type="fig">Figure 6B-D</xref></bold>). The ratio between <italic>α</italic> and <italic>β</italic> determines the direction and magnitude of bias and variance patterns (<bold><xref rid="fig6" ref-type="fig">Figure 6C,D</xref></bold>). For relatively larger <italic>α</italic>, the network shows cardinal attraction and minimum variance of errors at the obliques (<bold><xref rid="fig6" ref-type="fig">Figure 6E</xref></bold>). Reversely, for relatively larger <italic>β</italic> with stronger modulation in inhibitory connections, the network reproduced cardinal repulsion and minimum variance of errors at cardinal orientations, consistent with experiments (<bold><xref rid="fig6" ref-type="fig">Figure 6F</xref></bold>). With a larger difference between <italic>α</italic> and <italic>β</italic>, such patterns of bias and variance are potentiated and minimum Fisher information across orientations decreases, corresponding to memory loss (<bold><xref rid="fig6" ref-type="fig">Figure 6C,D</xref></bold>; <bold><xref rid="fig6S1" ref-type="fig">Figure 6 – Figure supplement 1</xref></bold>). Thus, this emphasizes the important role of heterogeneously tuned inhibition in shaping the sensory response for higher precision at cardinal orientations and enabling the sensory-memory interacting network to generate correct error patterns.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Stronger inhibitory synaptic modulation is required for correct error patterns. (<bold>A</bold>) Segregation of excitatory (blue) and inhibitory (red) synaptic pathways. (<bold>B</bold>) Example excitatory (left) and inhibitory (right) connectivity strengths of the sensory module. The heterogeneity degrees of excitatory and inhibitory connections are denoted by <italic>α</italic> and <italic>β</italic>, respectively. Unlike combined excitation and inhibition in <bold><xref ref-type="fig" rid="fig3">Figure 3B</xref></bold>, the connectivity strengths are maximal around cardinal orientations. (<bold>C,D</bold>) Bias with stimulus at 22.5<sup>°</sup> (<bold>C</bold>) and standard deviation (SD) index (<bold>D</bold>) estimated at 1s into the delay for different values of <italic>α</italic> and <italic>β</italic>. SD index compares the SD at the cardinal and oblique orientations (<bold>Methods</bold>). (<bold>E,F</bold>) Example bias (left) and SD (right) patterns when excitatory modulation overwhelms inhibitory modulation (α = 0.068,<italic>β</italic> = 0.04; <bold>E</bold>) and when inhibitory modulation is stronger (α = 0.03, <italic>β</italic> = 0.08; <bold>F</bold>). In (<bold>C</bold>) and (<bold>D</bold>), green (yellow) pentagrams mark the parameters used in (<bold>E</bold>) and (<bold>F</bold>). Stronger inhibitory modulation is required for correct bias and variance patterns (<bold>F</bold> and green regions in <bold>C</bold> and <bold>D</bold>). In (<bold>E</bold>) and (<bold>F</bold>), shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption>
<graphic xlink:href="566396v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2f">
<title>Comparison to alternative circuit structures</title>
<p>So far, we have shown the sufficiency of sensory-memory interacting networks with different connectivity structures featuring heterogeneous-homogeneous recurrent connections within each module. Here, we explore whether such architecture is necessary by comparing its performance with alternative circuit structures for sensory-memory interactions. We assumed that sensory and memory modules still serve their distinctive functions, namely, sensory encoding and memory maintenance, with weak/strong recurrent connections in sensory/memory modules. On the other hand, the heterogeneity of connections in other circuits might differ as homogeneous-homogeneous, homogeneous-heterogeneous, and heterogeneous-heterogeneous connections for sensory-memory modules.</p>
<p>Circuits with homogeneous connections in both sensory and memory modules are similar to previous continuous attractor models for working memory, such that the energy landscape and noise amplitude are uniform for all orientations (<bold><xref rid="fig1" ref-type="fig">Figure 1D-F</xref></bold>). Such architecture is not suitable as it generates no bias in errors and flat variance patterns. This leaves the latter two types of configurations, which require heterogeneous connections within the memory module. With a strong recurrent connection within the memory module, its heterogeneous activity pattern dominates overall activities in sensory-memory interacting networks, which makes it analogous to an isolated memory module. Thus, we examined the property of the memory module alone, which can maintain memory while generating heterogeneous responses without connection to the sensory module (<bold><xref rid="fig7" ref-type="fig">Figure 7</xref></bold>).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Network model with memory module only. (<bold>A</bold>) Schematics of one-module network with heterogeneous and strong recurrent connections that enable both efficient coding and memory maintenance. (<bold>B</bold>) Example tuning curves at the end of the stimulus epoch (left) and at 4s into the delay epoch (right). (<bold>C,D</bold>) Bias with stimulus at 22.5<sup>°</sup> (<bold>C</bold>) and standard deviation (SD) index (<bold>D</bold>) estimated at 1s into the delay for different heterogeneity degrees of excitatory and inhibitory connections, denoted by <italic>α</italic> and <italic>β</italic>. For the parameters that generate reasonable bias patterns, the SD index is always negative, which indicates that the SD pattern is inconsistent with experimental findings. (<bold>E</bold>) Bias (left), and SD (right) patterns in the delay. While the bias pattern is correct, the SD reaches maxima around cardinal orientations, unlike the experiments. In (<bold>C</bold>) and (<bold>D</bold>), the yellow pentagram marks the parameters used in (<bold>E</bold>).</p></caption>
<graphic xlink:href="566396v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To generate the correct bias pattern, we assumed that excitatory and inhibitory pathways in the memory module are stronger near cardinal orientations, as we previously considered for the sensory module in the sensory-memory interacting network (<bold><xref rid="fig7" ref-type="fig">Figure 7A,B</xref></bold>). However, memory circuits with heterogeneous connections have problems in maintaining the information and reproducing correct error patterns (<bold><xref rid="fig7" ref-type="fig">Figure 7C-E</xref></bold>). First, memory circuits alone require fine-tuning of heterogeneity whose range generating a moderate drift speed is at least one order of magnitude smaller than that of the two-module network (<bold><xref rid="fig7" ref-type="fig">Figure 7C,D</xref></bold>). Deviation from this range results in fast drift toward oblique orientations, leading to rapid loss of information during the delay period (<bold><xref rid="fig6S1" ref-type="fig">Figure 6 – Figure supplement 1</xref></bold>). Second, despite the correct bias direction, the variance pattern is reversed such that the variance of errors is minimal at the oblique orientations (<bold><xref rid="fig7" ref-type="fig">Figure 7E</xref></bold>). Varying the heterogeneity in excitatory and inhibitory connections shows that such rapid drift and reversed error patterns are prevalent across different parameters (<bold><xref rid="fig7" ref-type="fig">Figure 7C,D</xref></bold>).</p>
<p>To understand why a heterogeneous memory circuit alone fails to reproduce correct error patterns, we compared its low-dimensional dynamics along the memory states to that of the sensory-memory interacting networks. For the network with a similar range of bias and variance on average, we compared their energy landscape and noise amplitude, which vary similarly in both networks with minimum energy level and maximum noise at the oblique orientations (<bold><xref rid="fig7S1" ref-type="fig">Figure 7 – Figure supplement 1</xref></bold>). However, the energy difference between cardinal and oblique orientations in a single memory-circuit model is bigger than that in a sensory-memory interacting network. In contrast, the difference in noise amplitude is smaller. The attraction at the obliques is much stronger, leading to the correct bias patterns, but too rapid an increase. Also, smaller differences in noise amplitude cannot overcome strong drift dynamics, leading to the minimum variance of errors at the obliques and reversed variance patterns. This suggests that compared to a heterogenous memory circuit alone, interactions between heterogeneous sensory and homogeneous memory modules are advantageous due to better control of energy and noise difference at cardinal/oblique orientations.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>While higher association areas have long been considered as a locus of working memory (<xref rid="c40" ref-type="bibr">Roussy, Mendoza-Halliday et al. 2021</xref>, <xref ref-type="bibr" rid="c33">Mejias and Wang 2022</xref>), recent human studies found memory signals in early sensory areas, prompting a re-evaluation of their role in working memory (<xref ref-type="bibr" rid="c57">Xu 2020</xref>, <xref rid="c1" ref-type="bibr">Adam, Rademaker et al. 2022</xref>). Our work extends the traditional memory models (<xref ref-type="bibr" rid="c51">Wang 2001</xref>, <xref ref-type="bibr" rid="c27">Khona and Fiete 2022</xref>) with novel insights into the significance of stimulus-specific sensory areas. We showed how sensory-memory interactions can elucidate changes in the internal representation of orientation stimuli and their behavioral readout during memory tasks. The observed error patterns suggest that the network meets two demands simultaneously: efficient encoding that reflects natural statistics and memory maintenance for successful retrieval of stimuli after a delay. Achieving both demands for orientation stimuli conflicts in a one-module network. Efficient encoding necessitates asymmetrical connections, resulting in inconsistent bias and variance patterns and overly rapid drift in the one-module network unless fine-tuned. In contrast, connecting sensory and memory modules can generate error patterns correctly and with less need for fine-tuning heterogeneity for slow drift. Efficient coding of natural statistics in the sensory module underscores the role of inhibitory plasticity. Low-dimensional projection onto memory states reveals that drift and diffusion processes governing working memory dynamics closely resemble the bias and variance patterns derived under Bayesian sensory models. It also elucidates how the magnitudes of bias and variance change depending on the heterogeneity of sensory connections.</p>
<p>Our model makes testable predictions to differentiate two-module and one-module networks using perturbation, such as transcranial magnetic stimulation (TMS). Many studies have found that during the delay period, TMS can intervene with the feedforward signal from sensory areas through which working memory is consolidated (<xref rid="c48" ref-type="bibr">van de Ven, Jacobs et al. 2012</xref>) (but see (<xref rid="c1" ref-type="bibr">Adam, Rademaker et al. 2022</xref>) for mixed effects of TMS and related debate). Under such perturbations, the ability to maintain information in the memory module will not be affected due to strong recurrent connections in both two-module and one-module networks. However, we expect different effects on bias patterns — in the two-module network, the bias will stop systematically drifting towards the obliques, reducing systematic repulsion (<bold><xref rid="fig8" ref-type="fig">Figure 8</xref></bold>). This accompanies the nonincreasing heterogeneity of tuning curves after the disruption, marked by their tuning width indices (see <bold>Methods</bold>). In contrast, in the one-module network, perturbation does not incur changes in error patterns as memory activities are less dependent on the sensory module during the delay period. Thus, perturbation studies can be used to reveal the role of the sensory module in shaping the error patterns during working memory. Note that our model cannot predict the effects of distractors during working memory, as such effects do not experimentally lead to changes in error patterns (<xref rid="c38" ref-type="bibr">Rademaker, Chunharas et al. 2019</xref>). The effect of distractors and direct intervention in the inter-module connections may differ due to potential differences in the encoding of distractors compared to task-relevant stimuli. More advanced models are required to comprehensively understand the influence of distractors and the processing of ongoing visual stimuli or the storage of multiple stimuli.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><p>Effect of perturbations in sensory-memory interaction on error patterns. (<bold>A,B</bold>) Example bias (<bold>A</bold>) and standard deviation (<bold>B</bold>) patterns when we assumed that TMS is applied to interrupt the feedforward signal from 2.5s into the delay. Shaded areas mark the ±s.e.m. of 1000 realizations. (<bold>C,D</bold>) Evolution of bias with example cue orientation at <italic>θ</italic> = 18<sup>°</sup> (<bold>C</bold>) and the tuning width indices in the memory network (WI; <bold>C</bold>) representing the asymmetry of tuning widths at cardinal and oblique orientations (<bold>Methods</bold>). Two vertical dashed lines mark the end of the stimulus epoch and the beginning of TMS disruption, respectively. Solid and dashed curves correspond to with and without perturbations, respectively. Both bias (<bold>C</bold>) and WI (<bold>D</bold>) stop increasing when TMS is on (<bold>C,D</bold>).</p></caption>
<graphic xlink:href="566396v2_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Our work suggests biologically plausible network mechanisms for the previously postulated efficient coding and Bayesian inference principles, relating network connectivity to tuning properties and error patterns. Previous normative explanations for systematic bias observed in perception tasks also suggested possible neural substrates for efficient coding, such as asymmetrical gain, width, or density of tuning curves across stimulus features (<xref ref-type="bibr" rid="c16">Ganguli and Simoncelli 2014</xref>, <xref ref-type="bibr" rid="c53">Wei and Stocker 2015</xref>). Our work narrowed the mechanism to denser and narrower tuning curves at cardinal orientations, consistent with neurophysiological recordings in the visual cortex (<xref rid="c32" ref-type="bibr">Li, Peterson et al. 2003</xref>, <xref rid="c28" ref-type="bibr">Kreile, Bonhoeffer et al. 2011</xref>, <xref rid="c43" ref-type="bibr">Shen, Tao et al. 2014</xref>). We implemented a population vector decoder reflecting neuronal preferred orientations, which approximates Bayesian optimal readout (<xref ref-type="bibr" rid="c14">Fischer 2010</xref>). Compared to a previous work adapting efficient coding theories with static tuning curves to account for error patterns in working memory tasks (<xref ref-type="bibr" rid="c45">Taylor and Bays 2018</xref>), our extension to memory processes demonstrated how neural activities and behavior readout change dynamically during the delay period. Notably, recent work combined dynamic change of signal amplitude with static tuning curves to capture different time courses of estimation precision during sensory encoding and memory maintenance (<xref ref-type="bibr" rid="c46">Tomić and Bays 2023</xref>). Our network models embody such phenomenological models as the networks exhibit changes in overall firing rates after the stimulus offset.</p>
<p>Like our study, a few recent studies have employed attractor dynamics to explain dynamic error patterns observed for visual color memory (<xref rid="c35" ref-type="bibr">Panichello, DePasquale et al. 2019</xref>, <xref ref-type="bibr" rid="c36">Pollock and Jazayeri 2020</xref>, <xref ref-type="bibr" rid="c13">Eissa and Kilpatrick 2022</xref>). Behavior studies showed attractive bias and minimum variance around the prevalent colors, which one-module discrete attractor models could reproduce. However, these models cannot be generalized to other visual stimuli, such as orientations, spatial locations, or directions, of which the responses show repulsive bias away from the common stimuli (<xref ref-type="bibr" rid="c54">Wei and Stocker 2017</xref>). Also, a one-module network storing color memory requires fine-tuned heterogeneity for moderate drift speed. While the desired low-dimensional manifold and drift dynamics can be engineered in the one-module network (<xref ref-type="bibr" rid="c36">Pollock and Jazayeri 2020</xref>), its biological mechanism needs further investigation. The two-module network considered in our study also requires fine-tuning of homogeneity in the memory module and heterogeneity in the sensory module. However, the condition of asymmetrical connections in the sensory module is less stringent as they have a weaker influence on the entire dynamics than those in the memory module. Fine-tuning of homogeneous connections in the memory module can be mediated through activity-dependent plasticity, such as short-term facilitation (<xref rid="c25" ref-type="bibr">Itskov, Hansel et al. 2011</xref>, <xref ref-type="bibr" rid="c22">Hansel and Mato 2013</xref>, <xref rid="c42" ref-type="bibr">Seeholzer, Deger et al. 2019</xref>) or long-term plasticity (<xref rid="c39" ref-type="bibr">Renart, Song et al. 2003</xref>, <xref ref-type="bibr" rid="c21">Gu and Lim 2022</xref>). Also, recent work showed that continuous attractors formed under unstructured, heterogeneous connections are robust against synaptic perturbations (<xref ref-type="bibr" rid="c10">Darshan and Rivkind 2022</xref>). Thus, the two-module networks can control the drift speed better with possible additional mechanisms that promote homogeneous memory states. It needs further exploration whether they can be generalized to other stimuli like color, possibly involving additional categorical structures (<xref rid="c24" ref-type="bibr">Hardman, Vergauwe et al. 2017</xref>, <xref rid="c37" ref-type="bibr">Pratte, Park et al. 2017</xref>).</p>
<p>The modularity structure in the brain is thought to be advantageous for fast adaptation to changing environments (<xref ref-type="bibr" rid="c44">Simon 1995</xref>, <xref rid="c8" ref-type="bibr">Cole, Reynolds et al. 2013</xref>, <xref ref-type="bibr" rid="c15">Frankland and Greene 2020</xref>). Recent works showed that recurrent neural networks trained for multiple cognitive tasks form clustered neural activities and modular dynamic motifs to repurpose shared functions for flexible computation (<xref rid="c58" ref-type="bibr">Yang, Joglekar et al. 2019</xref>, <xref rid="c12" ref-type="bibr">Driscoll, Shenoy et al. 2022</xref>). Resonant with these computational findings, an fMRI study showed that shared representation across distinct visual stimuli emerges during the delay period (<xref ref-type="bibr" rid="c29">Kwak and Curtis 2022</xref>). Although our work focuses on a single task, it highlights the necessity of having dedicated sensory and memory modules, and a memory module with ring geometry can be repurposed for various visual stimuli such as motion, spatial location, and color. It is reminiscent of the flexible working memory model, which proposes connections between multiple sensory modules and a control module (<xref ref-type="bibr" rid="c5">Bouchacourt and Buschman 2019</xref>). However, a key distinction lies in the role of the control module. Unlike the flexible working memory model that loses memory without sensory-control interactions, our work suggests that the memory module can independently maintain memory, while interaction with the sensory module continuously shapes the internal representation, potentially consolidating prior beliefs regarding natural statistics. The sensory-memory interaction and network architecture derived from dynamic changes of single stimulus representation can be a cornerstone for future studies in more complex conditions, such as under the stream of visual inputs (<xref ref-type="bibr" rid="c57">Xu 2020</xref>, <xref rid="c1" ref-type="bibr">Adam, Rademaker et al. 2022</xref>) or with high or noisy memory loads (<xref rid="c3" ref-type="bibr">Bays, Schneegans et al. 2022</xref>).</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Low-dimensional attractor models</title>
<p>To illustrate error patterns in different low-dimensional attractor models shown in <bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>, we considered a one-dimensional stochastic differential equation given a
<disp-formula id="eqn1">
<graphic xlink:href="566396v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>θ</italic><sub><italic>t</italic></sub> and <italic>W</italic><sub><italic>t</italic></sub> are orientation and standard Brownian motion at time t. We assumed that the drift and noise coefficients <italic>µ</italic> and <italic>σ</italic> only depend on <italic>θ</italic><sub><italic>t</italic></sub> where <inline-formula><inline-graphic xlink:href="566396v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> with diffusion coefficient <italic>𝒟</italic>.</p>
<p>For continuous attractor models in <bold><xref rid="fig1" ref-type="fig">Figure 1D-F</xref></bold>, <italic>µ</italic> and <italic>σ</italic> were set to be constant as <italic>µ</italic> = 0 and <italic>σ</italic> = 2°. For discrete attractor models in <bold><xref rid="fig1" ref-type="fig">Figure 1G-L</xref></bold>, we assumed that the energy function <italic>U</italic> (<italic>θ</italic><sub><italic>t</italic></sub>) is proportional to cos(4<italic>θ</italic><sub><italic>t</italic></sub>) (<bold><xref rid="fig1" ref-type="fig">Figure 1G,J</xref></bold>) so that the drift term <italic>µ</italic>(<italic>θ</italic><sub><italic>t</italic></sub>) = sin(4<italic>θ</italic><sub><italic>t</italic></sub>) with <inline-formula><inline-graphic xlink:href="566396v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. In these attractor models, the constant noise in <bold><xref rid="fig1" ref-type="fig">Figure 1G-I</xref></bold> is <italic>σ</italic> = 2° and the nonuniform noise in <bold><xref rid="fig1" ref-type="fig">Figure 1J-L</xref></bold> is <italic>σ</italic> = 2°(1 − cos(4<italic>θ</italic><sub><italic>t</italic></sub>)). The biases and standard deviation (SD) of errors were plotted at T = 1, 2, and 3 with 50,000 iterations. For the numerical simulation, dt = 0.01.</p>
</sec>
<sec id="s4b">
<title>Bayesian sensory models and extension</title>
<p>In <bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold>, we first constructed the sensory inference process, which receives orientation input <italic>θ</italic>, forms a corresponding noisy sensory representation <italic>m</italic> given <italic>θ</italic>, and then infers <inline-formula><inline-graphic xlink:href="566396v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> as an estimate of the input orientation from the encoded representation <italic>m</italic>. This inference is made in a Bayesian manner based on likelihood function <italic>p(m</italic>|<italic>θ</italic>) and orientation prior <italic>q</italic>(<italic>θ</italic>).</p>
<p>To construct <italic>p(m</italic>|<italic>θ</italic>), we followed the procedure given in (<xref ref-type="bibr" rid="c53">Wei and Stocker 2015</xref>), and the summary is as follows. We started from the sensory space of <inline-formula><inline-graphic xlink:href="566396v2_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where both discriminability and Fisher Information <inline-formula><inline-graphic xlink:href="566396v2_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are uniform, and all likelihood functions <inline-formula><inline-graphic xlink:href="566396v2_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are homogeneous von Mises functions. And since <italic>J</italic>(<italic>θ</italic>)<italic>∝</italic>(<italic>q</italic>(<italic>θ</italic>))<sup>2</sup>under the efficient coding condition, the sensory space of <inline-formula><inline-graphic xlink:href="566396v2_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and the stimulus space of <italic>θ</italic> can be mapped by forward and backward mappings <italic>F</italic>(<italic>θ</italic>) and <inline-formula><inline-graphic xlink:href="566396v2_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>F</italic>(<italic>θ</italic>) is the cumulative distribution function of prior <italic>q</italic>(<italic>θ</italic>). Thus, likelihood functions <italic>p(m</italic>|<italic>θ</italic>) can be obtained by taking homogeneous von Mises likelihoods in the sensory space and transforming them back to the stimulus space using <italic>F</italic><sup>−1</sup>. To sum up the upper half of the procedural diagram in <bold><xref rid="fig2" ref-type="fig">Figure 2A</xref></bold>, the sensory module receives <italic>θ</italic>, encodes it in <italic>m</italic> following <italic>p(m</italic>|<italic>θ</italic>), and decodes <italic>θ</italic> using likelihood functions and prior <italic>q</italic>(<italic>θ</italic>).</p>
<p>As an extension to include a memory process, the decoded <inline-formula><inline-graphic xlink:href="566396v2_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is passed on to the memory module, where <inline-formula><inline-graphic xlink:href="566396v2_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is maintained with the addition of memory noise <italic>ξ</italic>. The output of the memory module, <inline-formula><inline-graphic xlink:href="566396v2_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, is fed back to the sensory module as the new input. This completes one iteration of sensory-memory interaction. The whole process is then repeated recursively, resulting in increased biases and standard deviations in the <italic>θ</italic> statistics at subsequent iterations (call them <italic>θ</italic><sub><italic>i</italic></sub> for the input of iteration <italic>i</italic>).</p>
<p>For <bold><xref rid="fig2" ref-type="fig">Figure 2B-C</xref></bold>, we set the von Mises sensory-space likelihoods to be <inline-formula><inline-graphic xlink:href="566396v2_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with <italic>κ</italic><sub><italic>m</italic></sub> = 250. These likelihood functions are transformed by <inline-formula><inline-graphic xlink:href="566396v2_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>q</italic>(<italic>θ</italic>) = 3 + cos (4<italic>θ</italic>). Each internal representation <italic>m</italic> is sampled from <italic>p(m</italic>|<italic>θ</italic>), after which <inline-formula><inline-graphic xlink:href="566396v2_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is estimated as the mean of the posterior <italic>p(θ</italic>|<italic>m)q(θ</italic>). With the parameters chosen above, the inferred samples of <inline-formula><inline-graphic xlink:href="566396v2_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula> after the first iteration have a circular standard deviation of <italic>σ</italic><sub><italic>θ</italic></sub> ≈ 1.3° at cardinal orientations. To have comparable memory and sensory noise levels, we set the memory noise <italic>ξ ~ 𝒩</italic> (0,1.3°). The first three iterations’ output statistics are plotted in <bold><xref rid="fig2" ref-type="fig">Figure 2C</xref></bold>, i.e., bias(<italic>θ</italic><sub>1</sub>), bias(<italic>θ</italic><sub>2</sub>), bias(<italic>θ</italic><sub>3</sub>), and SD(<italic>θ</italic><sub>1</sub>), SD(<italic>θ</italic><sub>2</sub>), SD(<italic>θ</italic><sub>3</sub>). The statistics were computed from 10,000 iterations of the simulation. The magnitude of biases and standard deviations vary for different sensory or memory noise levels, while the overall patterns and the increasing temporal trend are unchanged (not shown).</p>
</sec>
<sec id="s4c">
<title>Firing rate models</title>
<p>For network models, we considered sensory circuits with heterogeneous connections (<bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold>), memory circuits with homogeneous connections (<bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold>) and heterogeneous connections (<bold><xref rid="fig7" ref-type="fig">Figure 7</xref></bold>), and sensory-memory interacting circuits (<bold><xref rid="fig4" ref-type="fig">Figure 4</xref>-<xref ref-type="fig" rid="fig6">6</xref>, <xref ref-type="fig" rid="fig8">8</xref></bold>). In all cases, the activities of neurons are described by their firing rates and synaptic states, denoted by <italic>r</italic> and <italic>s</italic>. For columnar structure encoding orientation stimuli, we indexed the neurons by uniformly assigning them indices <inline-formula><inline-graphic xlink:href="566396v2_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for <italic>i</italic> from 1 to <italic>N</italic> where <italic>N</italic> is the number of neurons in each population. For sensory or memory networks alone, the dynamics of neuron <italic>i</italic> are described by the following equations,
<disp-formula id="eqn2">
<graphic xlink:href="566396v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the superscripts <italic>i</italic> and <italic>j</italic> are the neuronal indices, and the subscript <italic>k</italic> is either s or m, representing sensory or memory circuits. For the sensory-memory interacting network, the dynamics are given as
<disp-formula id="eqn3">
<graphic xlink:href="566396v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where activities and synaptic inputs are represented in the vector and matrix multiplication form, shown in bold cases. The additional subscripts f and b represent feedforward and backward connections between sensory and memory modules.</p>
<p>In both <xref ref-type="disp-formula" rid="eqn2">Eqs. (2)</xref> and <xref ref-type="disp-formula" rid="eqn3">(3)</xref>, <italic>s</italic>(<italic>t</italic>) is the low pass filtered <italic>r</italic>(<italic>t</italic>) with synaptic time constant <italic>τ</italic> and with the addition of <italic>ξ</italic> approximating Poisson noise. We modeled <italic>ξ</italic> as the Gaussian process with covariance ⟨<italic>ξ</italic><sup><italic>i</italic></sup>(<italic>t</italic>)<italic>ξ</italic><sup><italic>i</italic></sup>(<italic>t</italic>′) ⟩ = <italic>r</italic><sup><italic>i</italic></sup>(<italic>t</italic>)δ<sub><italic>ij</italic></sub>δ(<italic>t</italic> − <italic>t</italic>′), following (<xref ref-type="bibr" rid="c7">Burak and Fiete 2012</xref>). We assumed that the rate dynamics are relatively fast such that <italic>r</italic> (<italic>t</italic>) equals the input current-output rate transfer function <italic>f</italic>. The input current is the sum of external input <italic>I</italic><sub>ext</sub> and the synaptic currents from other neurons in the network, which are the postsynaptic states <italic>s</italic><sup><italic>j</italic></sup> weighted by synaptic strengths <italic>W</italic><sup><italic>ij</italic></sup>. The transfer function <italic>f</italic> has the Naka–Rushton form (<xref ref-type="bibr" rid="c55">Wilson 1999</xref>) given as
<disp-formula id="eqn4">
<graphic xlink:href="566396v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where [⋅]<sub>+</sub> denotes the linear rectification function. The transfer functions differ in the sensory and memory modules, denoted as <italic>f</italic><sub>s</sub> and <italic>f</italic><sub>m</sub>, respectively.</p>
</sec>
<sec id="s4d">
<title>Synaptic inputs in network models</title>
<p>Note that for all network models, we only considered excitatory neurons under the assumption that the inhibitory synaptic pathways have relatively fast dynamics. Thus, recurrent connectivity strengths, <italic>W</italic><sub>s</sub> and <italic>W</italic><sub>m</sub> within sensory and memory modules, reflect summed excitation and inhibition, and thus, can have either positive or negative signs. On the other hand, we assumed that intermodal interactions, <italic>W</italic><sub>f</sub> and <italic>W</italic><sub>b</sub>, are dominantly excitatory and, thus, can be only positive.</p>
<p>All <italic>W</italic>’s can be defined using neuronal indices of post- and presynaptic neurons as
<disp-formula id="eqn5">
<graphic xlink:href="566396v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For <italic>W</italic><sub>s</sub> without segregating excitation and inhibition in <bold><xref rid="fig3" ref-type="fig">Figure 3</xref>-<xref ref-type="fig" rid="fig5">5</xref></bold>, <italic>N</italic>is the population size of sensory module, <italic>N</italic><sub>s</sub>, and <italic>J</italic><sub>s</sub> is the sum of a constant global inhibition and a short-range excitatory connection as
<disp-formula id="eqn6">
<graphic xlink:href="566396v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>α</italic> &gt; 0 represents the heterogeneity degree of excitatory connectivity, and <italic>λ</italic><sub>E</sub> is the width of local excitatory connections.</p>
<p>When we segregated excitation and inhibition and considered the heterogeneity of inhibitory connection in <bold><xref rid="fig6" ref-type="fig">Figure 6</xref></bold> and <bold><xref ref-type="fig" rid="fig8">8</xref></bold>, <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref> is replaced with
<disp-formula id="eqn7">
<graphic xlink:href="566396v2_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>β</italic> &gt; 0is the degree of heterogeneity of inhibitory connections. Note the signs of modulation change in <xref ref-type="disp-formula" rid="eqn6">Eqs. (6)</xref> and <xref ref-type="disp-formula" rid="eqn7">(7)</xref> such that when only excitation is modulated in <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref>, the connectivity strengths near the obliques are strong. In contrast, when excitation and inhibition are both modulated in <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>, the connectivity strengths near cardinal orientations are strong.</p>
<p>For the memory module, <italic>N</italic> is the population size of the memory module, <italic>N</italic><sub>m</sub> in <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>. Without heterogeneity in <bold><xref rid="fig3" ref-type="fig">Figure 3</xref>-<xref ref-type="fig" rid="fig6">6</xref></bold> and <bold><xref ref-type="fig" rid="fig8">8</xref></bold>, <italic>J</italic><sub>m</sub> is defined as
<disp-formula id="eqn8">
<graphic xlink:href="566396v2_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In contrast, for the one-module network model in <bold><xref rid="fig7" ref-type="fig">Figure 7</xref></bold>, the connectivity of the memory module is heterogeneous, as in the sensory module in <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>, and is defined as
<disp-formula id="eqn9">
<graphic xlink:href="566396v2_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The feedforward and feedback connectivity are similarly defined as
<disp-formula id="eqn10">
<graphic xlink:href="566396v2_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Note the connectivity strength is normalized by the size of the presynaptic population so that the total synaptic current remains the same for different population sizes.</p>
<p>For the external inputs with orientation <italic>θ, I</italic><sub>ext,s</sub> in the sensory module is modeled as
<disp-formula id="eqn11">
<graphic xlink:href="566396v2_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>ε</italic> ∈ (0,0.5] determines the stimulus tuning of the input, <italic>λ</italic><sub>ext,s</sub> determines the width, and <italic>C</italic> describes the contrast (<xref ref-type="bibr" rid="c23">Hansel and Sompolinsky 1998</xref>).</p>
<p>For the memory network not connected to the sensory module in <bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold> and <bold><xref ref-type="fig" rid="fig7">7</xref></bold>, we assumed stimulus-specific input as
<disp-formula id="eqn12">
<graphic xlink:href="566396v2_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>I</italic><sub>c,m</sub> is a constant background input. When the memory module receives the inputs from the sensory population in <bold><xref rid="fig4" ref-type="fig">Figure 4</xref>-<xref ref-type="fig" rid="fig6">6</xref></bold> and <bold><xref ref-type="fig" rid="fig8">8</xref></bold>, we assumed <inline-formula><inline-graphic xlink:href="566396v2_inline19.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is constant as <italic>I</italic> <sub>c,m</sub>.</p>
</sec>
<sec id="s4e">
<title>Analysis of network activities</title>
<p>We used population vector decoding to extract the internal representation of orientation and quantified how such representation deviated from the original stimulus. We also examined how tuning properties and Fisher information change during the delay period.</p>
<p>Note that while we indexed neurons uniformly with <italic>ψ</italic><sub><italic>i</italic></sub> between 0<sup>°</sup> and 180<sup>°</sup>, the maximum of the tuning curve of neuron <italic>ψ</italic><sub><italic>i</italic></sub> can change dynamically and differ from <italic>ψ</italic><sub><italic>i</italic></sub>. We defined the preferred feature (PF) of neuron <italic>i</italic> as the maximum of its tuning curve when the tuning curve reaches a steady state in the presence of external input. For numerical estimation, we set the stimulus-present encoding epoch to 5 seconds to obtain the steady states of tuning curves. The tuning width is given as the full width at half maximum (FWHM) of the tuning curve. To estimate PF and FWHM, we did a cubic spline interpolation to increase the number of sample orientations to 1000. The tuning width index (WI) is given as
<disp-formula id="eqn13">
<graphic xlink:href="566396v2_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To estimate the internal representation of orientation in the network models, denoted as <inline-formula><inline-graphic xlink:href="566396v2_inline20.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, we utilized the population vector decoder (PVD) (<xref rid="c18" ref-type="bibr">Georgopoulos, Schwartz et al. 1986</xref>)
<disp-formula id="eqn14">
<graphic xlink:href="566396v2_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>N</italic> denotes the number of neurons and <inline-formula><inline-graphic xlink:href="566396v2_inline21.gif" mimetype="image" mime-subtype="gif"/></inline-formula> denotes the PF of neuron <italic>j</italic>. The orientation is always decoded from the memory network tuning curves <italic>r</italic><sub>m</sub>(<italic>t</italic>) except for <bold><xref rid="fig8" ref-type="fig">Figure 8A</xref></bold>. The estimation bias <inline-formula><inline-graphic xlink:href="566396v2_inline22.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Since the bias is typically small enough, we computed the estimation standard deviation (SD) as the SD of bias using linear statistics. The SD index is defined as
<disp-formula id="eqn15">
<graphic xlink:href="566396v2_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The Fisher information (FI) is estimated by assuming that the likelihood function <italic>p(r</italic>∣<italic>θ</italic>) is Gaussian. Thus, we can estimate the FI of memory neuron <italic>i</italic> with index <italic>ψ</italic><sub><italic>i</italic></sub> based on the empirical mean and variance of the firing rate at time <italic>t</italic> as
<disp-formula id="eqn16">
<graphic xlink:href="566396v2_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and the total FI is the summation of the FI of all memory neurons, given as FI(<italic>t</italic>) = ∑<sub><italic>i</italic></sub> FI (ψ<sub><italic>i</italic></sub>, <italic>t</italic>).</p>
</sec>
<sec id="s4f">
<title>Drift and diffusivity in network models</title>
<p>Although the modulation breaks the continuity of the ring attractor and forms two discrete attractors at the obliques, there is still a one-dimensional trajectory <inline-formula><inline-graphic xlink:href="566396v2_inline23.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to which the noise-free dynamics quickly converge. We can linearize the system in the vicinity of this trajectory if the noise is small (<xref ref-type="bibr" rid="c7">Burak and Fiete 2012</xref>). Note that the dynamics of the synaptic variables in <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref> can be put into the following form
<disp-formula id="eqn17">
<graphic xlink:href="566396v2_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and by linearizing around the stable trajectory <inline-formula><inline-graphic xlink:href="566396v2_inline24.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, we get
<disp-formula id="eqn18">
<graphic xlink:href="566396v2_eqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we have ignored the zeroth- and higher-order terms. The drift velocity <italic>µ</italic>(<italic>θ</italic>) is estimated by projecting the noise-free dynamics along the normalized right eigenvector <italic>u</italic> of <italic>K</italic> with the largest real part of the eigenvalue
<disp-formula id="eqn19">
<graphic xlink:href="566396v2_eqn19.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The coefficient of diffusion can be obtained in the same way
<disp-formula id="eqn20">
<graphic xlink:href="566396v2_eqn20.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The noise coefficient is given as <inline-formula><inline-graphic xlink:href="566396v2_inline25.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Hence, we have reduced the high-dimensional dynamics to a simple one-dimensional stochastic differential equation as in <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> as
<disp-formula id="ueqn1">
<graphic xlink:href="566396v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and the potential <italic>U</italic>(<italic>θ</italic>) is obtained by the relation <inline-formula><inline-graphic xlink:href="566396v2_inline26.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
</sec>
<sec id="s4g">
<title>Network parameters and simulations</title>
<p>Unless otherwise specified, <italic>N</italic><sub>s</sub> = <italic>N</italic><sub>m</sub> = 300, <italic>τ</italic> = 10 ms. The connectivity parameters are <italic>J</italic><sub>E,s</sub> = 0.35, <italic>J</italic><sub>I,s</sub> = 0.6, <italic>J</italic><sub>E,m</sub> = 1, <italic>J</italic><sub>I,m</sub> = 0.17, <italic>J</italic><sub>f</sub> = 0.1, <italic>J</italic><sub>b</sub> = 0.25,<italic>λ</italic><sub>E,s</sub> = 0.36<italic>π,λ</italic><sub>I,s</sub> = 1.1<italic>π,λ</italic><sub><italic>E</italic>,m</sub> = 0.2<italic>π, λ</italic><sub>I,m</sub> = 0.6<italic>π,λ</italic><sub>f</sub> = <italic>λ</italic><sub>b</sub> = 0.17<italic>π</italic>. For the external input, we set C = 4,ε = 0.2, and <italic>λ</italic><sub>ext,s</sub> = 0.3<italic>π</italic>. For the modulation of the sensory network, unless otherwise specified, we set <italic>α</italic> = 0.04 when only the excitatory plasticity is considered, and <italic>α</italic> = 0.03,<italic>β</italic> = 0.08 when the inhibitory plasticity is added. As for the modulation of the single-layer memory network, we set <italic>α</italic> = 5×10<sup>−4</sup>,<italic>β</italic> = 2.4×10<sup>−3</sup>. For the transfer function, <italic>f</italic><sub>max</sub> = 100, <italic>T</italic> = 0.1, <italic>q</italic> = 2, <italic>w</italic> = 6 for sensory <italic>f</italic><sub>s</sub>, and <italic>f</italic><sub>max</sub> = 100,<italic>T</italic> = 0.1, <italic>q</italic> = 1.5, <italic>w</italic> = 6.6 for memory <italic>f</italic><sub>m</sub>.</p>
<p>We uniformly sampled 50 cue orientations in [0<sup>°</sup>, 180<sup>°</sup>]. The visual cue lasts for 0.5 seconds except for the estimation of the PFs. In the grid parameter search figures, the delay epochs last for 1 s. In <bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold>, we set <italic>α</italic> = 0.07. In <bold><xref rid="fig5" ref-type="fig">Figure 5A</xref></bold>, the manifold corresponds to the synaptic variables at 4s into the delay with <italic>α</italic> = 0.05. We uniformly sampled 100 cue orientations for the manifold.</p>
<p>All simulations of ordinary or stochastic differential equations of the network models were done using the Euler method with d<italic>t</italic> = 1 ms. We checked that similar results hold for smaller d<italic>t</italic>. Example bias and standard deviation patterns were estimated from 1000 independent realizations. The Fisher information patterns were estimated from 3000 independent realizations. The grid search of maximum bias at <italic>θ</italic> = 22.5<sup>°</sup> and standard deviation index were computed from 3000realizations.</p>
<p>All simulations were run in Matlab. The code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/KYang-N/Cardinal-Repulsion.git">https://github.com/KYang-N/Cardinal-Repulsion.git</ext-link>.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We appreciate X. Wei for sharing the code for Bayesian inference models. J. Y. was supported by the NYU Shanghai Summer Undergraduate Research Program (SURP). S. L. received STI2030-Major Projects, No.2021ZD0203700/2021ZD0203705. H. Z. and S. L. also acknowledge the support of the Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning and the NYU-ECNU Institute of Brain and Cognitive Science at NYU Shanghai.</p>
</ack>
<sec id="s5">
<title>Competing Interests</title>
<p>The authors declare no competing financial interests.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="other"><string-name><surname>Adam</surname>, <given-names>K. C. S.</given-names></string-name>, <string-name><given-names>R. L.</given-names> <surname>Rademaker</surname></string-name> and <string-name><given-names>J. T.</given-names> <surname>Serences</surname></string-name> (<year>2022</year>). <article-title>Evidence for, and challenges to, sensory recruitment models of visual working memory</article-title>. <source>Visual Memory. T. F. Brady and W. A. Bainbridge, Routledge</source>: <fpage>5</fpage>–<lpage>25</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Bae</surname>, <given-names>G. Y.</given-names></string-name> (<year>2021</year>). <article-title>“Neural evidence for categorical biases in location and orientation representations in a working memory task</article-title>.” <source>Neuroimage</source> <volume>240</volume>: <fpage>118366</fpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><string-name><surname>Bays</surname>, <given-names>P.</given-names></string-name>, <string-name><given-names>S.</given-names> <surname>Schneegans</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Ma</surname></string-name> and <string-name><given-names>T. F.</given-names> <surname>Brady</surname></string-name> (<year>2022</year>). <article-title>Representation and computation in working memory</article-title>, <source>PsyArXiv</source>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Bays</surname>, <given-names>P. M.</given-names></string-name> (<year>2014</year>). <article-title>“Noise in neural populations accounts for errors in working memory</article-title>.” <source>J Neurosci</source> <volume>34</volume>(<issue>10</issue>): <fpage>3632</fpage>–<lpage>3645</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Bouchacourt</surname>, <given-names>F.</given-names></string-name> and <string-name><given-names>T. J.</given-names> <surname>Buschman</surname></string-name> (<year>2019</year>). <article-title>“A Flexible Model of Working Memory</article-title>.” <source>Neuron</source> <volume>103</volume>(<issue>1</issue>): <fpage>147</fpage>–<lpage>160</lpage> e148.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Burak</surname>, <given-names>Y.</given-names></string-name> and <string-name><given-names>I. R.</given-names> <surname>Fiete</surname></string-name> (<year>2009</year>). <article-title>“Accurate path integration in continuous attractor network models of grid cells</article-title>.” <source>PLoS Comput Biol</source> <volume>5</volume>(<issue>2</issue>): <fpage>e1000291</fpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Burak</surname>, <given-names>Y.</given-names></string-name> and <string-name><given-names>I. R.</given-names> <surname>Fiete</surname></string-name> (<year>2012</year>). <article-title>“Fundamental limits on persistent activity in networks of noisy neurons</article-title>.” <source>Proc Natl Acad Sci U S A</source> <volume>109</volume>(<issue>43</issue>): <fpage>17645</fpage>–<lpage>17650</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name>, <string-name><given-names>J. R.</given-names> <surname>Reynolds</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>Power</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Repovs</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Anticevic</surname></string-name> and <string-name><given-names>T. S.</given-names> <surname>Braver</surname></string-name> (<year>2013</year>). <article-title>“Multi-task connectivity reveals flexible hubs for adaptive task control</article-title>.” <source>Nat Neurosci</source> <volume>16</volume>(<issue>9</issue>): <fpage>1348</fpage>–<lpage>1355</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Compte</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>N.</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>P. S.</given-names> <surname>Goldman-Rakic</surname></string-name> and <string-name><given-names>X. J.</given-names> <surname>Wang</surname></string-name> (<year>2000</year>). <article-title>“Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model</article-title>.” <source>Cereb Cortex</source> <volume>10</volume>(<issue>9</issue>): <fpage>910</fpage>–<lpage>923</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Darshan</surname>, <given-names>R.</given-names></string-name> and <string-name><given-names>A.</given-names> <surname>Rivkind</surname></string-name> (<year>2022</year>). <article-title>“Learning to represent continuous variables in heterogeneous neural networks</article-title>.” <source>Cell Rep</source> <volume>39</volume>(<issue>1</issue>): <fpage>110612</fpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>de Gardelle</surname>, <given-names>V.</given-names></string-name>, <string-name><given-names>S.</given-names> <surname>Kouider</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Sackur</surname></string-name> (<year>2010</year>). <article-title>“An oblique illusion modulated by visibility: nonmonotonic sensory integration in orientation processing</article-title>.” <source>J Vis</source> <volume>10</volume>(<issue>10</issue>): <fpage>6</fpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="other"><string-name><surname>Driscoll</surname>, <given-names>L.</given-names></string-name>, <string-name><given-names>K.</given-names> <surname>Shenoy</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Sussillo</surname></string-name> (<year>2022</year>). <article-title>“Flexible multitask computation in recurrent networks utilizes shared dynamical motifs</article-title>.” <source>bioRxiv</source>: 2022.2008.2015.503870.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="other"><string-name><surname>Eissa</surname>, <given-names>T. L.</given-names></string-name> and <string-name><given-names>Z. P.</given-names> <surname>Kilpatrick</surname></string-name> (<year>2022</year>). <article-title>“Learning efficient representations of environmental priors in working memory</article-title>.” <source>bioRxiv</source>: 2022.2007.2005.498889.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="other"><string-name><surname>Fischer</surname>, <given-names>B. J.</given-names></string-name> (<year>2010</year>). <article-title>Bayesian estimates from heterogeneous population codes</article-title>. <source>The 2010 International Joint Conference on Neural Networks (IJCNN), Barcelona, Spain</source>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Frankland</surname>, <given-names>S. M.</given-names></string-name> and <string-name><given-names>J. D.</given-names> <surname>Greene</surname></string-name> (<year>2020</year>). <article-title>“Concepts and Compositionality: In Search of the Brain’s Language of Thought</article-title>.” <source>Annu Rev Psychol</source> <volume>71</volume>: <fpage>273</fpage>–<lpage>303</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Ganguli</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>E. P.</given-names> <surname>Simoncelli</surname></string-name> (<year>2014</year>). <article-title>“Efficient sensory encoding and Bayesian inference with heterogeneous neural populations</article-title>.” <source>Neural Comput</source> <volume>26</volume>(<issue>10</issue>): <fpage>2103</fpage>–<lpage>2134</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Geisler</surname>, <given-names>W. S.</given-names></string-name> (<year>2008</year>). <article-title>“Visual perception and the statistical properties of natural scenes</article-title>.” <source>Annu Rev Psychol</source> <volume>59</volume>: <fpage>167</fpage>–<lpage>192</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Georgopoulos</surname>, <given-names>A. P.</given-names></string-name>, <string-name><given-names>A. B.</given-names> <surname>Schwartz</surname></string-name> and <string-name><given-names>R. E.</given-names> <surname>Kettner</surname></string-name> (<year>1986</year>). <article-title>“Neuronal population coding of movement direction</article-title>.” <source>Science</source> <volume>233</volume>(<issue>4771</issue>): <fpage>1416</fpage>–<lpage>1419</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Girshick</surname>, <given-names>A. R.</given-names></string-name>, <string-name><given-names>M. S.</given-names> <surname>Landy</surname></string-name> and <string-name><given-names>E. P.</given-names> <surname>Simoncelli</surname></string-name> (<year>2011</year>). <article-title>“Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title>.” <source>Nat Neurosci</source> <volume>14</volume>(<issue>7</issue>): <fpage>926</fpage>–<lpage>932</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="other"><string-name><surname>Gu</surname>, <given-names>H.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Lim</surname></string-name>, <string-name><given-names>H.-J.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Choe</surname></string-name>, <string-name><given-names>D.-G.</given-names> <surname>Yoo</surname></string-name>, <string-name><given-names>J. H.</given-names> <surname>Ryu</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Lim</surname></string-name> and <string-name><given-names>S.-H.</given-names> <surname>Lee</surname></string-name> (<year>2023</year>). “<article-title>Decision-consistent bias mediated by drift dynamics of human visual working memory</article-title>.” <source>bioRxiv</source>: 2023.2006.2028.546818.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Gu</surname>, <given-names>J.</given-names></string-name> and <string-name><given-names>S.</given-names> <surname>Lim</surname></string-name> (<year>2022</year>). <article-title>“Unsupervised learning for robust working memory</article-title>.” <source>PLoS Comput Biol</source> <volume>18</volume>(<issue>5</issue>): <fpage>e1009083</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Hansel</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>G.</given-names> <surname>Mato</surname></string-name> (<year>2013</year>). <article-title>“Short-term plasticity explains irregular persistent activity in working memory tasks</article-title>.” <source>J Neurosci</source> <volume>33</volume>(<issue>1</issue>): <fpage>133</fpage>–<lpage>149</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="book"><string-name><surname>Hansel</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>H.</given-names> <surname>Sompolinsky</surname></string-name> (<year>1998</year>). <chapter-title>Modeling Feature Selectivity in Local Cortical Circuits</chapter-title>. <source>Methods in Neuronal Modeling: From Ions to Networks</source>. <person-group person-group-type="editor"><string-name><given-names>C.</given-names> <surname>Koch</surname></string-name> and <string-name><given-names>I.</given-names> <surname>Segev</surname></string-name></person-group>, <publisher-name>MIT Press</publisher-name>: <fpage>499</fpage>–<lpage>567</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Hardman</surname>, <given-names>K. O.</given-names></string-name>, <string-name><given-names>E.</given-names> <surname>Vergauwe</surname></string-name> and <string-name><given-names>T. J.</given-names> <surname>Ricker</surname></string-name> (<year>2017</year>). <article-title>“Categorical working memory representations are used in delayed estimation of continuous colors</article-title>.” <source>J Exp Psychol Hum Percept Perform</source> <volume>43</volume>(<issue>1</issue>): <fpage>30</fpage>–<lpage>54</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Itskov</surname>, <given-names>V.</given-names></string-name>, <string-name><given-names>D.</given-names> <surname>Hansel</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name> (<year>2011</year>). <article-title>“Short-Term Facilitation may Stabilize Parametric Working Memory Trace</article-title>.” <source>Front Comput Neurosci</source> <volume>5</volume>: <fpage>40</fpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Khan</surname>, <given-names>A. G.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Poort</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Chadwick</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Blot</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sahani</surname></string-name>, <string-name><given-names>T. D.</given-names> <surname>Mrsic-Flogel</surname></string-name> and <string-name><given-names>S. B.</given-names> <surname>Hofer</surname></string-name> (<year>2018</year>). <article-title>“Distinct learning-induced changes in stimulus selectivity and interactions of GABAergic interneuron classes in visual cortex</article-title>.” <source>Nat Neurosci</source> <volume>21</volume>(<issue>6</issue>): <fpage>851</fpage>–<lpage>859</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Khona</surname>, <given-names>M.</given-names></string-name> and <string-name><given-names>I. R.</given-names> <surname>Fiete</surname></string-name> (<year>2022</year>). <article-title>“Attractor and integrator networks in the brain</article-title>.” <source>Nat Rev Neurosci</source> <volume>23</volume>(<issue>12</issue>): <fpage>744</fpage>–<lpage>766</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Kreile</surname>, <given-names>A. K.</given-names></string-name>, <string-name><given-names>T.</given-names> <surname>Bonhoeffer</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Hubener</surname></string-name> (<year>2011</year>). <article-title>“Altered visual experience induces instructive changes of orientation preference in mouse visual cortex</article-title>.” <source>J Neurosci</source> <volume>31</volume>(<issue>39</issue>): <fpage>13911</fpage>–<lpage>13920</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Kwak</surname>, <given-names>Y.</given-names></string-name> and <string-name><given-names>C. E.</given-names> <surname>Curtis</surname></string-name> (<year>2022</year>). <article-title>“Unveiling the abstract format of mnemonic representations</article-title>.” <source>Neuron</source> <volume>110</volume>(<issue>11</issue>): <fpage>1822</fpage>–<lpage>1828</lpage> e1825.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Larisch</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>L.</given-names> <surname>Gonner</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Teichmann</surname></string-name> and <string-name><given-names>F. H.</given-names> <surname>Hamker</surname></string-name> (<year>2021</year>). <article-title>“Sensory coding and contrast invariance emerge from the control of plastic inhibition over emergent selectivity</article-title>.” <source>PLoS Comput Biol</source> <volume>17</volume>(<issue>11</issue>): <fpage>e1009566</fpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Leavitt</surname>, <given-names>M. L.</given-names></string-name>, <string-name><given-names>D.</given-names> <surname>Mendoza-Halliday</surname></string-name> and <string-name><given-names>J. C.</given-names> <surname>Martinez-Trujillo</surname></string-name> (<year>2017</year>). <article-title>“Sustained Activity Encoding Working Memories: Not Fully Distributed</article-title>.” <source>Trends Neurosci</source> <volume>40</volume>(<issue>6</issue>): <fpage>328</fpage>–<lpage>346</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>B.</given-names></string-name>, <string-name><given-names>M. R.</given-names> <surname>Peterson</surname></string-name> and <string-name><given-names>R. D.</given-names> <surname>Freeman</surname></string-name> (<year>2003</year>). <article-title>“Oblique effect: a neural basis in the visual cortex</article-title>.” <source>J Neurophysiol</source> <volume>90</volume>(<issue>1</issue>): <fpage>204</fpage>–<lpage>217</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Mejias</surname>, <given-names>J. F.</given-names></string-name> and <string-name><given-names>X. J.</given-names> <surname>Wang</surname></string-name> (<year>2022</year>). <article-title>“Mechanisms of distributed working memory in a large-scale network of macaque neocortex</article-title>.” <source>Elife</source> <volume>11</volume>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> and <string-name><given-names>D. J.</given-names> <surname>Field</surname></string-name> (<year>1996</year>). <article-title>“Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>.” <source>Nature</source> <volume>381</volume>(<issue>6583</issue>): <fpage>607</fpage>–<lpage>609</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Panichello</surname>, <given-names>M. F.</given-names></string-name>, <string-name><given-names>B.</given-names> <surname>DePasquale</surname></string-name>, <string-name><given-names>J. W.</given-names> <surname>Pillow</surname></string-name> and <string-name><given-names>T. J.</given-names> <surname>Buschman</surname></string-name> (<year>2019</year>). <article-title>“Error-correcting dynamics in visual working memory</article-title>.” <source>Nat Commun</source> <volume>10</volume>(<issue>1</issue>): <fpage>3366</fpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Pollock</surname>, <given-names>E.</given-names></string-name> and <string-name><given-names>M.</given-names> <surname>Jazayeri</surname></string-name> (<year>2020</year>). <article-title>“Engineering recurrent neural networks from task-relevant manifolds and dynamics</article-title>.” <source>PLoS Comput Biol</source> <volume>16</volume>(<issue>8</issue>): <fpage>e1008128</fpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Pratte</surname>, <given-names>M. S.</given-names></string-name>, <string-name><given-names>Y. E.</given-names> <surname>Park</surname></string-name>, <string-name><given-names>R. L.</given-names> <surname>Rademaker</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Tong</surname></string-name> (<year>2017</year>). <article-title>“Accounting for stimulus-specific variation in precision reveals a discrete capacity limit in visual working memory</article-title>.” <source>J Exp Psychol Hum Percept Perform</source> <volume>43</volume>(<issue>1</issue>): <fpage>6</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Rademaker</surname>, <given-names>R. L.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Chunharas</surname></string-name> and <string-name><given-names>J. T.</given-names> <surname>Serences</surname></string-name> (<year>2019</year>). <article-title>“Coexisting representations of sensory and mnemonic information in human visual cortex</article-title>.” <source>Nat Neurosci</source> <volume>22</volume>(<issue>8</issue>): <fpage>1336</fpage>–<lpage>1344</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Renart</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>P.</given-names> <surname>Song</surname></string-name> and <string-name><given-names>X. J.</given-names> <surname>Wang</surname></string-name> (<year>2003</year>). <article-title>“Robust spatial working memory through homeostatic synaptic scaling in heterogeneous cortical networks</article-title>.” <source>Neuron</source> <volume>38</volume>(<issue>3</issue>): <fpage>473</fpage>–<lpage>485</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Roussy</surname>, <given-names>M.</given-names></string-name>, <string-name><given-names>D.</given-names> <surname>Mendoza-Halliday</surname></string-name> and <string-name><given-names>J. C.</given-names> <surname>Martinez-Trujillo</surname></string-name> (<year>2021</year>). <article-title>“Neural Substrates of Visual Perception and Working Memory: Two Sides of the Same Coin or Two Different Coins</article-title>?” <source>Front Neural Circuits</source> <volume>15</volume>: <fpage>764177</fpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Schneegans</surname>, <given-names>S.</given-names></string-name> and <string-name><given-names>P. M.</given-names> <surname>Bays</surname></string-name> (<year>2018</year>). <article-title>“Drift in Neural Population Activity Causes Working Memory to Deteriorate Over Time</article-title>.” <source>J Neurosci</source> <volume>38</volume>(<issue>21</issue>): <fpage>4859</fpage>–<lpage>4869</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Seeholzer</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Deger</surname></string-name> and <string-name><given-names>W.</given-names> <surname>Gerstner</surname></string-name> (<year>2019</year>). <article-title>“Stability of working memory in continuous attractor networks under the control of short-term plasticity</article-title>.” <source>PLoS Comput Biol</source> <volume>15</volume>(<issue>4</issue>): <fpage>e1006928</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Shen</surname>, <given-names>G.</given-names></string-name>, <string-name><given-names>X.</given-names> <surname>Tao</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>E. L.</given-names> <surname>Smith</surname>, <suffix>3rd</suffix></string-name> and <string-name><given-names>Y. M.</given-names> <surname>Chino</surname></string-name> (<year>2014</year>). <article-title>“Oblique effect in visual area 2 of macaque monkeys</article-title>.” <source>J Vis</source> <volume>14</volume>(<issue>2</issue>).</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="book"><string-name><surname>Simon</surname>, <given-names>H. A.</given-names></string-name> (<year>1995</year>). <chapter-title>Near-decomposability and complexity: How a mind resides in a brain</chapter-title>. <source>The Mind, the Brain, and Complex Adaptive Systems</source>. <person-group person-group-type="editor"><string-name><given-names>H.</given-names> <surname>Morowitz</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Singer</surname></string-name></person-group>, <publisher-name>Addison-Wesley</publisher-name>: <fpage>25</fpage>–<lpage>43</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Taylor</surname>, <given-names>R.</given-names></string-name> and <string-name><given-names>P. M.</given-names> <surname>Bays</surname></string-name> (<year>2018</year>). <article-title>“Efficient Coding in Visual Working Memory Accounts for Stimulus-Specific Variations in Recall</article-title>.” <source>J Neurosci</source> <volume>38</volume>(<issue>32</issue>): <fpage>7132</fpage>–<lpage>7142</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="other"><string-name><surname>Tomić</surname>, <given-names>I.</given-names></string-name> and <string-name><given-names>P. M.</given-names> <surname>Bays</surname></string-name> (<year>2023</year>). <article-title>“A dynamic neural resource model bridges sensory and working memory</article-title>.” <source>bioRxiv</source>: 2023.2003.2027.534406.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>van Bergen</surname>, <given-names>R. S.</given-names></string-name>, <string-name><given-names>W. J.</given-names> <surname>Ma</surname></string-name>, <string-name><given-names>M. S.</given-names> <surname>Pratte</surname></string-name> and <string-name><given-names>J. F.</given-names> <surname>Jehee</surname></string-name> (<year>2015</year>). <article-title>“Sensory uncertainty decoded from visual cortex predicts behavior</article-title>.” <source>Nat Neurosci</source> <volume>18</volume>(<issue>12</issue>): <fpage>1728</fpage>–<lpage>1730</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>van de Ven</surname>, <given-names>V.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Jacobs</surname></string-name> and <string-name><given-names>A. T.</given-names> <surname>Sack</surname></string-name> (<year>2012</year>). <article-title>“Topographic contribution of early visual cortex to shortterm memory consolidation: a transcranial magnetic stimulation study</article-title>.” <source>J Neurosci</source> <volume>32</volume>(<issue>1</issue>): <fpage>4</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>van den Berg</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>H.</given-names> <surname>Shin</surname></string-name>, <string-name><given-names>W. C.</given-names> <surname>Chou</surname></string-name>, <string-name><given-names>R.</given-names> <surname>George</surname></string-name> and <string-name><given-names>W. J.</given-names> <surname>Ma</surname></string-name> (<year>2012</year>). <article-title>“Variability in encoding precision accounts for visual short-term memory limitations</article-title>.” <source>Proc Natl Acad Sci U S A</source> <volume>109</volume>(<issue>22</issue>): <fpage>8780</fpage>–<lpage>8785</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Vogels</surname>, <given-names>T. P.</given-names></string-name>, <string-name><given-names>R. C.</given-names> <surname>Froemke</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Doyon</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Gilson</surname></string-name>, <string-name><given-names>J. S.</given-names> <surname>Haas</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Maffei</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Miller</surname></string-name>, <string-name><given-names>C. J.</given-names> <surname>Wierenga</surname></string-name>, <string-name><given-names>M. A.</given-names> <surname>Woodin</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Zenke</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Sprekeler</surname></string-name> (<year>2013</year>). <article-title>“Inhibitory synaptic plasticity: spike timing-dependence and putative network function</article-title>.” <source>Front Neural Circuits</source> <volume>7</volume>: <fpage>119</fpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>X. J.</given-names></string-name> (<year>2001</year>). <article-title>“Synaptic reverberation underlying mnemonic persistent activity</article-title>.” <source>Trends Neurosci</source> <volume>24</volume>(<issue>8</issue>): <fpage>455</fpage>–<lpage>463</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Webster</surname>, <given-names>M. A.</given-names></string-name> (<year>2015</year>). <article-title>“Visual Adaptation</article-title>.” <source>Annu Rev Vis Sci</source> <volume>1</volume>: <fpage>547</fpage>–<lpage>567</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Wei</surname>, <given-names>X. X.</given-names></string-name> and <string-name><given-names>A. A.</given-names> <surname>Stocker</surname></string-name> (<year>2015</year>). <article-title>“A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts</article-title>.” <source>Nat Neurosci</source> <volume>18</volume>(<issue>10</issue>): <fpage>1509</fpage>–<lpage>1517</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Wei</surname>, <given-names>X. X.</given-names></string-name> and <string-name><given-names>A. A.</given-names> <surname>Stocker</surname></string-name> (<year>2017</year>). <article-title>“Lawful relation between perceptual bias and discriminability</article-title>.” <source>Proc Natl Acad Sci U S A</source> <volume>114</volume>(<issue>38</issue>): <fpage>10244</fpage>–<lpage>10249</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="book"><string-name><surname>Wilson</surname>, <given-names>H. R.</given-names></string-name> (<year>1999</year>). <source>Spikes, decisions, and actions : the dynamical foundations of neuroscience</source>. <publisher-loc>Oxford</publisher-loc>, <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Wimmer</surname>, <given-names>K.</given-names></string-name>, <string-name><given-names>D. Q.</given-names> <surname>Nykamp</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Constantinidis</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Compte</surname></string-name> (<year>2014</year>). <article-title>“Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory</article-title>.” <source>Nat Neurosci</source> <volume>17</volume>(<issue>3</issue>): <fpage>431</fpage>–<lpage>439</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name> (<year>2020</year>). <article-title>“Revisit once more the sensory storage account of visual working memory</article-title>.” <source>Vis cogn</source> <volume>28</volume>(<issue>5-8</issue>): <fpage>433</fpage>–<lpage>446</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Yang</surname>, <given-names>G. R.</given-names></string-name>, <string-name><given-names>M. R.</given-names> <surname>Joglekar</surname></string-name>, <string-name><given-names>H. F.</given-names> <surname>Song</surname></string-name>, <string-name><given-names>W. T.</given-names> <surname>Newsome</surname></string-name> and <string-name><given-names>X. J.</given-names> <surname>Wang</surname></string-name> (<year>2019</year>). <article-title>“Task representations in neural networks trained to perform many cognitive tasks</article-title>.” <source>Nat Neurosci</source> <volume>22</volume>(<issue>2</issue>): <fpage>297</fpage>–<lpage>306</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname>, <given-names>K.</given-names></string-name> (<year>1996</year>). <article-title>“Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</article-title>.” <source>J Neurosci</source> <volume>16</volume>(<issue>6</issue>): <fpage>2112</fpage>–<lpage>2126</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Zylberberg</surname>, <given-names>J.</given-names></string-name>, <string-name><given-names>J. T.</given-names> <surname>Murphy</surname></string-name> and <string-name><given-names>M. R.</given-names> <surname>DeWeese</surname></string-name> (<year>2011</year>). <article-title>“A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of V1 simple cell receptive fields</article-title>.” <source>PLoS Comput Biol</source> <volume>7</volume>(<issue>10</issue>): <fpage>e1002250</fpage>.</mixed-citation></ref>
</ref-list>
<sec id="s6">
<title>Figure legends</title>
<fig id="fig4S1" position="float" fig-type="figure">
<label>Figure 4 – Figure supplement 1.</label>
<caption><p>Dynamics of bias and tuning properties of sensory-memory interacting network models. (<bold>A</bold>) Maximum firing rate of <italic>ψ</italic> = 22.2<sup>°</sup> for all stimulus orientations. The vertical grey line represents the end of the stimulus presentation. Both sensory and memory modules show lower but sustained activities during the delay period. (<bold>B</bold>) Bias evolution to input orientation <italic>θ</italic> = 18<sup>°</sup>. The bias increases both in the stimulus and delay periods, while its increasing speed is reduced during the delay period. (<bold>C</bold>) Tuning width indices (WI) measuring the asymmetry of tuning widths at cardinal and oblique orientations (<bold>Methods</bold>). WI also increases in the whole process, indicating the tuning curves of the neural population become more heterogeneous. All parameters are the same as in <bold><xref ref-type="fig" rid="fig4">Figure 4</xref></bold>.</p></caption>
<graphic xlink:href="566396v2_fig4S1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5S1" position="float" fig-type="figure">
<label>Figure 5 – Figure supplement 1.</label>
<caption><p>Comparison between bias and standard deviation (SD) patterns of the full network model (orangish) and low-dimensional projection (bluish curves). From top to bottom, each row corresponds to sensory-memory interacting networks in <bold><xref ref-type="fig" rid="fig5">Figure 5</xref></bold> with <italic>α</italic> = 0.03 (<bold>A,D</bold>), 0.04 (<bold>B,E</bold>), and 0.05 (<bold>C,F</bold>), respectively. We projected the dynamics onto the left (<bold>A-C</bold>) and right (<bold>D-F</bold>) eigenvectors of the Jacobian matrix obtained from local dynamics along the memory states (<bold>Methods</bold>). The manifold was parameterized at 1s into the delay after a 0.5s-long stimulus to determine the drift speed and diffusivity of the low-dimensional model. The initial orientations of the low-dimensional model were set to be the orientations decoded from the full model at 1s into the delay. We compared the increase of bias from then on, i.e., at 1.2s, 1.5s, and 2s into the delay for the full model, but 0.2s, 0.5s, and 1s for the low-dimensional model. Low-dimensional projection captures characteristic patterns well despite relatively larger deviation in the SD compared to bias, and we found that projecting to the right eigenvector (<bold>D-F</bold>) generally yields better predictions than projecting to the left eigenvector (<bold>A-C</bold>). All parameters are the same as in <bold><xref ref-type="fig" rid="fig5">Figure 5</xref></bold> except for <italic>α</italic>. Shaded areas (too narrow to be seen) mark the ±s.e.m. of 3000 realizations.</p></caption>
<graphic xlink:href="566396v2_fig5S1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5S2" position="float" fig-type="figure">
<label>Figure 5 – Figure supplement 2.</label>
<caption><p>Error patterns and low-dimensional dynamics for different feedforward (<bold>A-E</bold>) and feedback (<bold>F-J</bold>) connection strengths at 4s into the delay epoch. Increasing both feedforward and feedback connection strengths enlarges the bias (<bold>A,F</bold>) and flattens the SD pattern (<bold>B,G</bold>). That can be understood through the low-dimensional projection—the drift velocity increases (<bold>D,I</bold>), but the noise coefficient corresponding to diffusion is less affected (<bold>E,J</bold>). Shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption>
<graphic xlink:href="566396v2_fig5S2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6S1" position="float" fig-type="figure">
<label>Figure 6 – Figure supplement 1.</label>
<caption><p>Relationship between drift speed and memory loss in two-module (<bold>A-C</bold>) and one-module (<bold>D-F</bold>) networks. (<bold>A,D</bold>) Drift speed for different heterogeneity degrees, <italic>α</italic> and <italic>β</italic>. (<bold>B,E</bold>) Minimum Fisher information (FI). The upper panels were obtained with a coarse parameter grid, and lower panels were obtained with a fine grid but only along parameters for the smallest bias increase (red circle) and the orthogonal direction (red triangle). When bias speed is large with unbalanced excitation and inhibition strengths (triangle), the minimum FI decreases quickly in both two-module and one-module networks, suggesting memory loss. On the other hand, along the direction with the smallest bias increase (circle), the minimum FI is relatively high. The FI was estimated at 4s into the delay epoch using 1000 realizations. (<bold>C,F</bold>) Negative correlation between minimum FI and drift speed.</p></caption>
<graphic xlink:href="566396v2_fig6S1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig7S1" position="float" fig-type="figure">
<label>Figure 7 – Figure supplement 1.</label>
<caption><p>Comparison of low-dimensional dynamics between two-module and one-module network models. (<bold>A,B</bold>) Bias and standard deviation (SD) patterns of two-module (<bold>A</bold>) and one-module (<bold>B</bold>) networks adapted from <bold><xref ref-type="fig" rid="fig6">Figure 6F</xref></bold> and <bold><xref ref-type="fig" rid="fig7">Figure 7E</xref></bold>, respectively. The averages of bias and SD over different <italic>θ</italic> at 4s into the delay are similar in the two networks. (<bold>C,D</bold>) Energy potential and noise coefficients in two-module (black) and one-module (red) networks. Despite the similar bias levels at 4s, the two-module network has a shallower potential (<bold>C</bold>) but larger heterogeneity in the noise coefficient profile (<bold>D</bold>). Such differences make it possible for the SD to become smaller around cardinal orientations in the two-module network (right in <bold>A</bold>), while drift dynamics overwhelm and the SD pattern is opposite to that of the noise coefficient in the one-module network (right in <bold>B</bold>).</p></caption>
<graphic xlink:href="566396v2_fig7S1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95160.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Wei</surname>
<given-names>Xue-Xin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>UT Austin</institution>
</institution-wrap>
<city>Austin</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>In this <bold>important</bold> paper, the authors propose a computational model for understanding how the dynamics of neural representations may lead to specific patterns of errors as observed in working memory tasks. The paper provides <bold>solid</bold> evidence showing how a two-area model of sensory-memory interactions can account for the error patterns reported in orientation estimation tasks with delays. By integrating ideas from efficient coding and attractor networks, the resulting theoretical framework is appealing, and nicely captures some basic patterns of behavior data and the distributed nature of memory representation as reported in prior neurophysiological studies. The paper can be strengthened if (i) further analyses are conducted to deepen our understanding of the circuit mechanisms underlying the behavior effects; (ii) the necessity of the two-area network model is better justified; (iii) the nuanced aspects of the behavior that are not captured by the current model are discussed in more detail.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95160.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Working memory is imperfect - memories accrue errors over time and are biased towards certain identities. For example, previous work has shown memory for orientation is more accurate near the cardinal directions (i.e., variance in responses is smaller for horizontal and vertical stimuli) while being biased towards diagonal orientations (i.e., there is a repulsive bias away from horizontal and vertical stimuli). The magnitude of errors and biases increase the longer an item is held in working memory and when more items are held in working memory (i.e., working memory load is higher). Previous work has argued that biases and errors could be explained by increased perceptual acuity at cardinal directions. However, these models are constrained to sensory perception and do not explain how biases and errors increase over time in memory. The current manuscript builds on this work to show how a two-layer neural network could integrate errors and biases over a memory delay. In brief, the model includes a 'sensory' layer with heterogenous connections that lead to the repulsive bias and decreased error in the cardinal directions. This layer is then reciprocally connected with a classic ring attractor layer. Through their reciprocal interactions, the biases in the sensory layer are constantly integrated into the representation in memory. In this way, the model captures the distribution of biases and errors for different orientations that have been seen in behavior and their increasing magnitude with time. The authors compare the two-layer network to a simpler one-network model, showing that the one-model network is harder to tune and shows an attractive bias for memories that have lower error (which is incompatible with empirical results).</p>
<p>Strengths:</p>
<p>The manuscript provides a nice review of the dynamics of items in working memory, showing how errors and biases differ across stimulus space. The two-layer neural network model is able to capture the behavioral effects as well as relate to neurophysiological observations that memory representations are distributed across the sensory cortex and prefrontal cortex.</p>
<p>The authors use multiple approaches to understand how the network produces the observed results. For example, analyzing the dynamics of memories in the low-dimensional representational space of the networks provides the reader with an intuition for the observed effects.</p>
<p>As a point of comparison with the two-layer network, the authors construct a heterogenous one-layer network (analogous to a single memory network with embedded biases). They argue that such a network is incapable of capturing the observed behavioral effects but could potentially explain biases and noise levels in other sensory domains where attractive biases have lower errors (e.g., color).</p>
<p>The authors show how changes in the strength of Hebbian learning of excitatory and inhibitory synapses can change network behavior. This argues for relatively stronger learning in inhibitory synapses, an interesting prediction.</p>
<p>The manuscript is well-written. In particular, the figures are well done and nicely schematize the model and the results.</p>
<p>Weaknesses:</p>
<p>Despite its strengths, the manuscript does have some weaknesses.</p>
<p>First, as far as we can tell, behavioral data is only presented in schematic form. This means some of the nuances of the effects are lost. It also means that the model is not directly capturing behavioral effects. Therefore, while providing insight into the general phenomenon, the current manuscript may be missing some important aspects of the data.</p>
<p>Relatedly, the models are not directly fit to behavioral data. This makes it hard for the authors to exclude the possibility that there is a single network model that could capture the behavioral effects. In other words, it is hard to support the authors' conclusion that &quot;....these evolving errors...require network interaction between two distinct modules.&quot; (from the abstract, but similar comments are made throughout the manuscript). Such a strong claim needs stronger evidence than what is presented. Fitting to behavioral data could allow the authors to explore the full parameter space for both the one-layer and two-layer network architectures.</p>
<p>In addition, directly comparing the ability of different model architectures to fit behavioral data would allow for quantitative comparison between models. Such quantitative comparisons are currently missing from the manuscript.</p>
<p>To help broaden the impact of the paper, it would be helpful if the authors provided insight into how the observed behavioral biases and/or network structures influence cognition. For example, previous work has argued that biases may counteract noise, leading to decreased variance at certain locations. Is there a similar normative explanation for why the brain would have repulsive biases away from commonly occurring stimuli? Are they simply a consequence of improved memory accuracy? Why isn't this seen for all stimulus domains?</p>
<p>Previous work has found both diffusive noise and biases increase with the number of items in working memory. It isn't clear how the current model would capture these effects. The authors do note this limitation in the Discussion, but it remains unclear how the current model can be generalized to a multi-item case.</p>
<p>The role of the ring attractor memory network isn't completely clear. There is noise added in this stage, but how is this different from the noise added at the sensory stage? Shouldn't these be additive? Is the noise necessary? Similarly, it isn't clear whether the memory network is necessary - can it be replaced by autapses (self-connections) in the sensory network to stabilize its representation? In short, it would be helpful for the authors to provide an intuition for why the addition of the memory network facilitates the repulsive bias.</p>
<p>Overall:</p>
<p>Overall, the manuscript was successful in building a model that captured the biases and noise observed in working memory. This work complements previous studies that have viewed these effects through the lens of optimal coding, extending these models to explain the effects of time in memory. In addition, the two-layer network architecture extends previous work with similar architectures, adding further support to the distributed nature of working memory representations.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95160.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this manuscript, Yang et al. present a modeling framework to understand the pattern of response biases and variance observed in delayed-response orientation estimation tasks. They combine a series of modeling approaches to show that coupled sensory-memory networks are in a better position than single-area models to support experimentally observed delay-dependent response bias and variance in cardinal compared to oblique orientations. These errors can emerge from a population-code approach that implements efficient coding and Bayesian inference principles and is coupled to a memory module that introduces random maintenance errors. A biological implementation of such operation is found when coupling two neural network modules, a sensory module with connectivity inhomogeneities that reflect environment priors, and a memory module with strong homogeneous connectivity that sustains continuous ring attractor function. Comparison with single-network solutions that combine both connectivity inhomogeneities and memory attractors shows that two-area models can more easily reproduce the patterns of errors observed experimentally. This, the authors take as evidence that a sensory-memory network is necessary, but I am not convinced about the evidence in support of this &quot;necessity&quot; condition. A more in-depth understanding of the mechanisms operating in these models would be necessary to make this point clear.</p>
<p>Strengths:</p>
<p>The model provides an integration of two modeling approaches to the computational bases of behavioral biases: one based on Bayesian and efficient coding principles, and one based on attractor dynamics. These two perspectives are not usually integrated consistently in existing studies, which this manuscript beautifully achieves. This is a conceptual advancement, especially because it brings together the perceptual and memory components of common laboratory tasks.</p>
<p>The proposed two-area model provides a biologically plausible implementation of efficient coding and Bayesian inference principles, which interact seamlessly with a memory buffer to produce a complex pattern of delay-dependent response errors. No previous model had achieved this.</p>
<p>Weaknesses:</p>
<p>The correspondence between the various computational models is not fully disclosed. It is not easy to see this correspondence because the network function is illustrated with different representations for different models and the correspondence between components of the various models is not specified. For instance, Figure 1 shows that a specific pattern of noise is required in the low-dimensional attractor model, but in the next model in Figure 2, the memory noise is uniform for all stimuli. How do these two models integrate? What element in the population-code model of Figure 2 plays the role of the inhomogeneous noise of Figure 1? Also, the Bayesian model of Figure 2 is illustrated with population responses for different stimuli and delays, while the attractor models of Figures 3 and 4 are illustrated with neuronal tuning curves but not population activity. In addition, error variance in the Bayesian model appears to be already higher for oblique orientations in the first iteration whereas it is only first shown one second into the delay for the attractor model in Figure 4. It is thus unclear whether variance inhomogeneities appear already at the perceptual stage in the attractor model, as it does in the population-code model. Of course, correspondences do not need to be perfect, but the reader does not know right now how far the correspondence between these models goes.</p>
<p>The manuscript does not identify the mechanistic origin in the model of Figure 4 of the specific noise pattern that is required for appropriate network function (with higher noise variance at oblique orientations). This mechanism appears critical, so it would be important to know what it is and how it can be regulated. In particular, it would be interesting to know if the specific choice of Poisson noise in Equation (3) is important. Tuning curves in Figure 4 indicate that population activity for oblique stimuli will have higher rates than for cardinal stimuli and thus induce a larger variance of injected noise in oblique orientations, based on this Poisson-noise assumption. If this explanation holds, one wonders if network inhomogeneities could be included (for instance in neural excitability) to induce higher firing rates in the cardinal/oblique orientations so as to change noise inhomogeneities independently of the bias and thus control more closely the specific pattern of errors observed, possibly within a single memory network.</p>
<p>The main conclusion of the manuscript, that the observed patterns of errors &quot;require network interaction between two distinct modules&quot; is not convincingly shown. The analyses show that there is a quantitative but not a qualitative difference between the dynamics of the single memory area compared to the sensory-memory two-area network, for specific implementations of these models (Figure 7 - Figure Supplement 1). There is no principled reasoning that demonstrates that the required patterns of response errors cannot be obtained from a different memory model on its own. Also, since the necessity of the two-area configuration is highlighted as the main conclusion of the manuscript, it is inconvenient that the figure that carefully compares these conditions is in the Supplementary Material.</p>
<p>The proposed model has stronger feedback than feedforward connections between the sensory and memory modules. This is not a common assumption when thinking about hierarchical processing in the brain, and it is not discussed in the manuscript.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95160.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The present study proposes a neural circuit model consisting of coupled sensory and memory networks to explain the circuit mechanism of the cardinal effect in orientation perception which is characterized by the bias towards the oblique orientation and the largest variance at the oblique orientation.</p>
<p>Strengths:</p>
<p>The authors have done numerical simulations and preliminary analysis of the neural circuit model to show the model successfully reproduces the cardinal effect. And the paper is well-written overall. As far as I know, most of the studies on the cardinal effect are at the level of statistical models, and the current study provides one possibility of how neural circuit models reproduce such an effect.</p>
<p>Weaknesses:</p>
<p>There are no major weaknesses and flaws in the present study, although I suggest the author conduct further analysis to deepen our understanding of the circuit mechanism of the cardinal effects. Please find my recommendations for concrete comments.</p>
</body>
</sub-article>
</article>