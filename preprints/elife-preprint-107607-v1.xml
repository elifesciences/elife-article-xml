<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107607</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107607</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107607.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Comprehensive Neural Representations of Naturalistic Stimuli through Multimodal Deep Learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Fu</surname>
<given-names>Mingxue</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Guoqiu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Yijie</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Mingzhe</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3444-000X</contrib-id>
<name>
<surname>Wang</surname>
<given-names>Yin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>mirrorneuronwang@gmail.com</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>State Key Laboratory of Cognitive Neuroscience and Learning, and IDG/McGovern Institute for Brain Research, Beijing Normal University</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Schapiro</surname>
<given-names>Anna C</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country country="NL">Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-08-26">
<day>26</day>
<month>08</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107607</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-05-27">
<day>27</day>
<month>05</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-04-19">
<day>19</day>
<month>04</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.04.15.646250"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Fu et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Fu et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107607-v1.pdf"/>
<abstract>
<title>Abstract</title><p>A central challenge in cognitive neuroscience is understanding how the brain represents and predicts complex, multimodal experiences in naturalistic settings. Traditional neural encoding models, often based on unimodal or static features, fall short in capturing the rich, dynamic structure of real-world cognition. Here, we address this challenge by introducing a video-text alignment encoding framework that predicts whole-brain neural responses by integrating visual and linguistic features across time. Using a state-of-the-art deep learning model (VALOR), we achieve more accurate and generalizable encoding than unimodal (AlexNet, WordNet) and static multimodal (CLIP) baselines. Beyond improving prediction, our model automatically maps cortical semantic spaces, aligning with human-annotated dimensions without requiring manual labeling. We further uncover a hierarchical predictive coding gradient, where different brain regions anticipate future events over distinct timescales—an organization that correlates with individual cognitive abilities. These findings provide new evidence that temporal multimodal integration is a core mechanism of real-world brain function. Our results demonstrate that deep learning models aligned with naturalistic stimuli can reveal ecologically valid neural mechanisms, offering a powerful, scalable approach for investigating perception, semantics, and prediction in the human brain. This framework advances naturalistic neuroimaging by bridging computational modeling and real-world cognition.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Naturalistic neuroimaging</kwd>
<kwd>Neural encoding model</kwd>
<kwd>Multimodal alignment</kwd>
<kwd>Deep neural network</kwd>
<kwd>Predictive coding</kwd>
<kwd>Neural representation</kwd>
<kwd>Brain response prediction</kwd>
</kwd-group>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>32422033</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>One of the central goals of neuroscience is to understand how the brain interprets and integrates information from the rich, dynamic, and multidimensional world we experience every day. Neural encoding models have proven valuable in this effort, offering quantitative predictions of brain activity under various conditions and providing insight into how information is represented in the brain<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref></sup>. Early work in this area focused on highly controlled, simplified stimuli to establish foundational principles. However, it is now widely recognized that such designs do not reflect the complex, continuous, and context-rich nature of real-world cognition. Rather than responding to isolated, static inputs, the brain continuously processes streams of multisensory information, integrates them over time, and interprets them within dynamic environments. This realization has motivated a growing push toward encoding models that better reflect the richness and variability of naturalistic experience <sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c5">5</xref></sup>.</p>
<p>Naturalistic paradigms—such as movie watching—have emerged as powerful tools for studying brain function in ecologically valid contexts<sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c7">7</xref></sup>. Functional MRI (fMRI) during movie viewing increases participant engagement and enables researchers to investigate how the brain processes complex, time-varying inputs. Building on this foundation, several influential studies have modeled cortical responses to naturalistic stimuli using visual and semantic features <sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>, while others have advanced applications in non-invasive neural decoding<sup><xref ref-type="bibr" rid="c9">9</xref></sup>. These efforts have deepened our understanding of brain function in real-world settings. Yet, most existing encoding models are limited to a single modality, such as vision or language <sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c12">12</xref></sup>. Visual models (e.g., AlexNet<sup><xref ref-type="bibr" rid="c11">11</xref></sup>) perform well in early visual regions but generalize poorly to high-level areas, while semantic models (e.g., WordNet<sup><xref ref-type="bibr" rid="c14">14</xref></sup>) perform the opposite. Moreover, many models depend on manual annotation of semantic content—a labor-intensive, subjective process that restricts scalability and reproducibility. These limitations highlight the need for automated, multimodal approaches that can more fully capture how the brain responds to naturalistic stimuli.</p>
<p>A second major challenge in naturalistic neuroimaging is improving model generalizability—the ability to predict brain responses to new, unseen stimuli. Many current models show sharp performance declines outside their training distribution. For example, a recent study reported that encoding models trained on one image set dropped to 20% accuracy when tested on out-of-distribution stimuli <sup><xref ref-type="bibr" rid="c15">15</xref></sup>. Unimodal models may generalize within specific cortical domains but fail in broader, whole-brain contexts. To build truly useful encoding models, we must develop approaches that generalize across diverse stimuli, subjects, and cognitive domains.</p>
<p>These challenges have spurred the growth of ‘AI for neuroscience’, a field that uses advances in deep learning to model brain function with increasing accuracy<sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>. Deep neural networks have shown promise in predicting neural responses to sensory and cognitive inputs<sup><xref ref-type="bibr" rid="c18">18</xref></sup>. However, many of these models remain modality-specific, limiting their ability to mirror how the brain integrates multisensory information <sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c12">12</xref></sup>. As a result, researchers are turning to multimodal deep learning, which learns from visual, linguistic, and auditory streams to model complex brain functions<sup><xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c21">21</xref></sup>. This trend is supported by neuroscience evidence that cortical areas encode information in shared, multimodal spaces<sup><xref ref-type="bibr" rid="c22">22</xref>–<xref ref-type="bibr" rid="c24">24</xref></sup>. Still, popular models like CLIP (Contrastive Language-Image Pretraining) <sup><xref ref-type="bibr" rid="c25">25</xref></sup>—while successful in aligning image and text features—treat perception as a series of static snapshots, falling short of capturing the temporal continuity central to real-world cognition. Because the brain processes stimuli as events unfolding over time, models that incorporate temporal structure, such as video-text alignment, may offer a more biologically plausible and cognitively meaningful framework.</p>
<p>In this study, we ask a fundamental question in cognitive neuroscience: How can we build models that accurately predict whole-brain neural responses to rich, dynamic, and naturalistic experiences? To answer this, we apply a video-text alignment encoding framework, using VALOR<sup><xref ref-type="bibr" rid="c22">22</xref></sup>—a high-performing, open-source model that aligns visual and linguistic features over time—to predict brain responses during movie watching. By analyzing naturalistic fMRI data from the Human Connectome Project (HCP) <sup><xref ref-type="bibr" rid="c26">26</xref></sup>, we conducted four experiments (<xref rid="fig1" ref-type="fig">Fig. 1</xref>) to show that our model (1) achieves superior predictive accuracy across both sensory and high-order cognitive areas, (2) generalizes robustly to out-of-distribution stimuli (ShortFunMovies dataset), (3) automatically maps semantic dimensions of cortical organization without manual labeling, and (4) reveals predictive coding gradients linked to individual differences in cognitive ability. Together, these findings demonstrate that multimodal, temporally aligned models offer a powerful and ecologically valid approach to understanding how the brain processes complex, real-world information.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1</label>
<caption><title>Overview of the Study Design.</title>
<p><bold>a</bold> Encoding model construction and generalization testing (Study 1 &amp; Study 2). Left panel: Whole-brain BOLD responses were recorded while participants passively viewed naturalistic movies from the HCP 7T dataset. We extracted visual, linguistic, and multimodal (video-text aligned) features from the movie clips and trained voxel-wise encoding models using kernel ridge regression to predict brain activity. The resulting model weights reflect each voxel’s tuning to these features, providing insight into how the brain represents dynamic, real-world stimuli. Right panel : The trained models were tested on a separate dataset (‘ShortFunMovies’) in which 20 participants watched 8 novel movie segments. Generalization performance was assessed by computing the Pearson correlation between predicted BOLD responses (from HCP-trained models) and empirically measured group-level fMRI responses in the SFM dataset. <bold>b</bold> Schematic of the video-text alignment model (VALOR). VALOR extracts multimodal features by encoding videos and their corresponding textual descriptions into separate embedding spaces, which are aligned in a 512-dimensional joint space via contrastive learning. The model minimizes the distance between matched video-text pairs and maximizes it for mismatched pairs, enabling temporally and semantically rich multimodal representations. <bold>c</bold> Using VALOR to study predictive coding (Study 4). To probe predictive coding, we extended the encoding model to include a forecast window. Features at the current timepoint (F<italic><sub>t</sub></italic>) were concatenated with features from a future timepoint (F<italic><sub>t+d</sub></italic>), allowing us to assess how well different brain regions anticipate upcoming stimuli. <italic>d</italic> is the temporal offset (prediction distance) from the current TR (repetition time). Predictive performance was compared against models using only current features, revealing regional differences in prediction horizons. All human-related images in this figure have been replaced with AI-generated illustrations to avoid including identifiable individuals. No real faces or photographs of people are shown.</p></caption>
<graphic xlink:href="646250v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Study 1: Enhanced whole-brain encoding with video-text alignment</title>
<p>In Study 1, we tested whether video-text alignment features offer superior whole-brain neural encoding compared to unimodal and image-based multimodal approaches. We analyzed high-resolution 7T fMRI data from 178 participants in the Human Connectome Project (HCP) as they passively watched one hour of movie stimuli. The movie segments were split into training and test sets, and voxel-wise encoding models were built using kernel ridge regression <sup><xref ref-type="bibr" rid="c27">27</xref></sup> to predict brain responses from four types of input features: (1) AlexNet (visual features), (2) WordNet (linguistic features), (3) CLIP (image-based multimodal features), and (4) VALOR, a video-text alignment model that encodes multimodal features from continuous video sequences (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>).</p>
<p>As shown in <xref rid="fig2" ref-type="fig">Fig. 2a</xref> (left panel), each model exhibited domain-specific strengths: AlexNet performed best in early visual regions, while WordNet, CLIP, and VALOR achieved higher accuracy in language-related and high-level association areas. However, VALOR stood out by achieving significantly higher whole-brain prediction accuracy than all other models (all ps &lt; .04, FDR corrected). To quantify these effects, we computed average prediction accuracy within three sets of regions of interest (ROIs): (1) Visual cortex including V1 and V4, (2) Language-related regions including middle temporal gyrus (MTG) and angular gyrus (AG), and (3) High-level association areas including precuneus (PCu), posterior cingulate cortex (PCC) and medial prefrontal cortex (mPFC).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2</label>
<caption><title>Comparison of encoding and generalization performance across models.</title>
<p><bold>a</bold> Whole-brain encoding performance (Study 1). Voxel-wise and ROI-level comparisons of four encoding models trained on the HCP 7T dataset: VALOR (video-text alignment, red), AlexNet (visual features, blue), WordNet (linguistic features, pink), and CLIP (image-based multimodal features, gray). Left panel: Cortical surface maps show the best-performing model at each voxel. Mean whole-brain prediction accuracy across participants, with VALOR significantly outperforming all baselines. Right panel: ROI-wise prediction accuracy across predefined regions (e.g., visual, language, high-level association areas). Bars indicate group-level means, with error bars denoting standard error of the mean (SEM). Statistical comparisons (one-sided paired t-tests, FDR-corrected) are reported only between VALOR and each of the other models. <bold>b</bold> Generalization performance on independent data (Study 2). Models trained on HCP data were tested on the independent ShortFunMovies (SFM) dataset to assess generalizability. Left: Voxel-wise surface maps highlight where VALOR outperforms other models across cortex. Right: ROI-level generalization performance, with color coding and statistical tests consistent with panel (a). For both voxel-wise and ROI analyses, VALOR demonstrates broad generalization across both sensory and high-level regions, exceeding the performance of all other models.</p></caption>
<graphic xlink:href="646250v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As shown in <xref rid="fig2" ref-type="fig">Fig. 2a</xref> (right panel), AlexNet outperformed others in visual regions, while WordNet and the multimodal models performed better in language and association areas. Importantly, VALOR matched WordNet in language and high-level regions but surpassed CLIP across multiple ROIs, particularly in the precuneus and PCC. Moreover, VALOR showed significantly higher accuracy in visual cortex than WordNet (all ps &lt; .001), demonstrating its balanced predictive coverage across both sensory and cognitive domains. These results were further supported by representational similarity analysis, which revealed stronger correspondence between VALOR features and actual brain activity across ROIs (see <xref rid="figS1" ref-type="fig">Supplementary Fig. S1</xref>).</p>
<p>Together, these findings demonstrate that integrating visual and linguistic information over time yields more accurate and comprehensive neural predictions than static or unimodal models. The video-text alignment approach captures the temporal and semantic continuity of naturalistic stimuli, offering a powerful framework for modeling whole-brain activity and advancing our understanding of how the brain processes complex, dynamic experiences.</p>
</sec>
<sec id="s2b">
<title>Study 2: Robust cross-dataset generalization through video-text alignment</title>
<p>Generalizability is a key benchmark for neural encoding models, reflecting their capacity to predict brain responses to unseen, out-of-distribution stimuli. To evaluate this, we tested whether the video-text alignment model trained on the HCP dataset could generalize to an independent, qualitatively distinct dataset. Specifically, we used our in-house ShortFunMovies (SFM) dataset, in which 20 participants viewed eight short films— six animated and two live-action—differing substantially in style and content from the HCP movies. Full dataset details and analysis procedures are provided in the Methods.</p>
<p>As shown in <xref rid="fig2" ref-type="fig">Fig. 2b</xref> (left panel), voxel-wise analysis revealed that VALOR (video-text alignment) significantly outperformed all baseline models—including AlexNet (visual), WordNet (linguistic), and CLIP (image-based multimodal)—across much of the cortex. ROI-level analysis confirmed this pattern: VALOR consistently achieved higher prediction accuracy across nearly all unimodal and multimodal brain regions (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>, right panel). These results underscore the importance of temporally integrated multimodal representations for capturing generalizable brain responses.</p>
<p>Notably, VALOR achieved this without requiring manual preprocessing steps such as principal component analysis (PCA) or semantic annotation, unlike unimodal models. This highlights its strength as a scalable, automated, and ecologically valid framework for modeling brain activity in response to diverse, real-world stimuli.</p>
</sec>
<sec id="s2c">
<title>Study 3: Automated cortical semantic mapping via video-text alignment</title>
<p>Having established the predictive and generalization advantages of the video-text alignment model, we next examined its potential to uncover the semantic structure of cortical representations. The human brain excels at categorizing a vast range of stimuli—from concrete objects and actions to abstract and social concepts—potentially organizing them in a continuous semantic space, where related categories are represented in nearby cortical regions<sup><xref ref-type="bibr" rid="c5">5</xref></sup>. Prior work has mapped this semantic space using manual annotations based on WordNet, a process that is labor-intensive, time-consuming, and prone to subjectivity⁴.</p>
<p>To test whether our model could automatically derive similar semantic structures, we applied the video-text alignment encoding model to fMRI data from Huth et al. (2012), in which five participants watched two hours of naturalistic movie clips. First, we assessed voxel-wise prediction accuracy using our multimodal features. As shown in <xref rid="fig3" ref-type="fig">Fig. 3a</xref> (and <xref rid="figS2" ref-type="fig">Supplementary Fig. S2</xref>), the video-text alignment model consistently outperformed the original WordNet-based model across most of the cortex. Next, we applied principal component analysis (PCA) to encoding model weights, following the approach of Huth et al. (2012). Projecting stimulus features onto each principal component (PC) revealed interpretable semantic dimensions, while projecting encoding weights showed how each PC was represented across the cortex. Analysis of over 20,000 video clips from VALOR’s training set revealed meaningful semantic axes. For instance, PC1 distinguished mobile vs. static content, PC2 separated social vs. non-social categories, PC3 contrasted mechanical vs. non-mechanical stimuli, and PC4 differentiated natural vs. civilizational themes (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>). These components closely mirrored those reported in the original WordNet-based model. To quantify this similarity, we computed the Jaccard index between cortical projections of the top four PCs from our model and those derived from WordNet features. As shown in <xref rid="fig3" ref-type="fig">Fig. 3c</xref>, we observed substantial spatial overlaps, indicating that our model recovered the semantic organization of human brain comparable to human-labeled ground truth—without any manual annotation.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3</label>
<caption><title>Cortical semantic mapping from video-text alignment features.</title>
<p><bold>a</bold> Voxel-wise prediction performance. Top: Encoding accuracy maps based on video-text alignment features (VALOR) versus WordNet-based semantic features from Huth et al. (2012). Bottom: Difference map showing voxel-wise performance gains (VALOR minus WordNet). Red areas indicate regions where VALOR outperforms WordNet. Results are shown for one representative subject; similar patterns were observed in the remaining four participants (see <xref rid="figS2" ref-type="fig">Supplementary Fig. S2</xref>). <bold>b</bold> Semantic dimensions revealed by video-text alignment. Principal component analysis (PCA) was applied to encoding weights derived from VALOR features across ∼20,000 video clips. Example frames illustrate the semantic meaning of the top four PCs: PC1=Mobility (e.g., movement vs. stillness; aligns with Huth’s PC1), PC2=Social content (e.g., people interaction vs. nature; aligns with Huth’s PC2), PC3=Mechanical vs. non-mechanical stimuli (aligns with Huth’s PC4), PC4=Civilization vs. natural environments (aligns with Huth’s PC3). All human-related images in this figure have been replaced with AI-generated illustrations to avoid including identifiable individuals. No real faces or photographs of people are shown. <bold>c</bold> Spatial correspondence with manually annotated semantic maps in Huth et al, (2012). Jaccard similarity matrix quantifying spatial overlap between the top four PCs from the VALOR model (rows) and those from the WordNet-based model (columns). The highest similarity scores for each column were diagonal, indicating strong alignment between automatically derived and manually annotated semantic components.</p></caption>
<graphic xlink:href="646250v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Together, these results demonstrate that the video-text alignment encoding model can automatically uncover meaningful, brain-wide semantic structures from naturalistic stimuli. This provides a powerful and scalable alternative to manual labeling, offering new opportunities to study semantic representations in the human brain with greater efficiency and ecological validity.</p>
</sec>
<sec id="s2d">
<title>Study 4 Neural predictive coding mechanisms through video-text alignment</title>
<p>Predictive processing is a core principle of brain function by which the brain interprets and interacts with the world through actively anticipating future events and continuously updating its internal model to minimize prediction errors<sup><xref ref-type="bibr" rid="c28">28</xref></sup>. While much of the existing evidence for predictive coding has come from tightly controlled experimental settings<sup><xref ref-type="bibr" rid="c29">29</xref>–<xref ref-type="bibr" rid="c31">31</xref></sup>, its manifestation during naturalistic experiences remains less understood.</p>
<p>To investigate predictive coding in an ecologically valid context, we used the HCP 7T fMRI dataset, focusing on runs where participants viewed Hollywood films— narratives that naturally engage anticipation and inference. We then tested whether incorporating representations of upcoming events—using a “forecast window”—could improve the video-text alignment encoding model’s prediction of neural responses (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>). Specifically, for each time point, we combined the current feature representation (F<italic><sub>t</sub></italic>) with that of a future segment (F<italic><sub>t</sub></italic><sub>+<italic>d</italic></sub>), where d indicates the prediction distance <sup><xref ref-type="bibr" rid="c32">32</xref></sup>. Comparing models with and without future information yielded “predictive scores”, which quantify the neural benefit of forward-looking representations.</p>
<p>As shown in <xref rid="fig4" ref-type="fig">Fig. 4a</xref>, several regions exhibited significant predictive enhancements, including the superior temporal gyrus (STG), middle temporal gyrus (MTG), superior parietal gyrus (SPG), and precuneus (PCu). These findings suggest that the brain actively anticipates upcoming content during movie viewing. To test whether different brain areas operate on distinct predictive timescales, we calculated a “prediction distance” metric for each voxel. Averaging these distances across four ROIs revealed a hierarchical gradient: the STG showed shorter-range predictions, while the MTG, SPG, and especially the PCu anticipated further into the future (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>). This pattern aligns with theories of cortical hierarchy<sup><xref ref-type="bibr" rid="c33">33</xref>–<xref ref-type="bibr" rid="c35">35</xref></sup>, suggesting that higher-order regions integrate information over longer temporal windows.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 4</label>
<caption><title>Predictive coding revealed by video-text alignment features.</title>
<p><bold>a</bold> voxel-wise predictive scores. For each voxel, a predictive score was computed as the improvement in prediction accuracy when future stimulus features (F<italic><sub>t</sub></italic><sub>+<italic>d</italic></sub>) were added to current features (F<italic><sub>t</sub></italic>) in the encoding model. Only voxels with significant predictive enhancement across participants are shown (Wilcoxon rank-sum test, FDR-corrected). <bold>b</bold> regional prediction distances. Voxels that showed highest predictive scores were STG, MTG, SPG and PCu. We calculated prediction distance for these regions on a per-voxel, per-participant basis, and then averaged the values. <bold>c</bold> brain–behavior correlation. Scatter plot showing a positive correlation between prediction distance in the SPG and individual fluid cognitive scores across HCP participants (r = 0.172, p &lt; .05, FDR-corrected). This suggests that individuals with broader predictive horizons in the parietal cortex tend to exhibit stronger fluid reasoning ability.</p></caption>
<graphic xlink:href="646250v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Finally, we examined whether prediction horizons were linked to individual differences in cognition. We correlated each participant’s fluid intelligence scores with their average prediction distance in the SPG and PCu, given these regions have been strongly linked to fluid intelligence <sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. As shown in <xref rid="fig4" ref-type="fig">Fig. 4c</xref>, participants with longer predictive distances in the SPG exhibited significantly higher fluid cognition scores (r = 0.172, p &lt; .05, FDR-corrected), suggesting that predictive coding in naturalistic contexts may reflect broader cognitive capacity.</p>
<p>In summary, these results show that the video-text alignment model not only captures real-time brain responses to ongoing stimuli but also reveals how the brain projects forward in time, supporting hierarchical prediction and linking anticipatory processing to individual cognitive abilities.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>This study introduces video-text alignment encoding as a powerful and ecologically valid framework for modeling whole-brain responses to naturalistic stimuli. By integrating visual and linguistic features over time, our approach addresses long-standing limitations in traditional encoding models and offers key advances across four dimensions: predictive accuracy (Study 1), cross-dataset generalization (Study 2), semantic space mapping (Study 3), and predictive coding mechanisms (Study 4). Collectively, these findings demonstrate that temporally aligned multimodal deep learning can uncover how the brain processes complex, dynamic information in real-world contexts.</p>
<p>In Study 1, we show that video-text alignment models outperform both unimodal and static multimodal approaches in predicting cortical activity (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). While AlexNet and WordNet capture localized visual or linguistic responses, respectively, they fail to generalize beyond their domains. In contrast, VALOR—which fuses visual and linguistic information over time—achieves high prediction accuracy across both low-level sensory and high-level integrative regions, including the precuneus (PCu), insula, and medial prefrontal cortex<sup><xref ref-type="bibr" rid="c38">38</xref>–<xref ref-type="bibr" rid="c40">40</xref></sup>. VALOR matches WordNet in language regions (e.g., angular gyrus) and significantly outperforms it in visual areas (e.g., V1, V4), underscoring that semantic models alone are insufficient to capture naturalistic perception. Additionally, VALOR exceeds the performance of CLIP, a leading static multimodal model, revealing that temporal structure is critical for modeling sequential dependencies in brain processing. Interestingly, both VALOR and CLIP showed unexpectedly high accuracy in traditionally unimodal sensory areas (e.g., certain voxels in cuneus and lingual gyrus, see <xref rid="fig2" ref-type="fig">Fig. 2a</xref>, left panel), suggesting that even early visual cortex may integrate multimodal information during dynamic, real-world experiences <sup><xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c42">42</xref></sup>. This conclusion is further supported by representational similarity analysis (<xref rid="figS1" ref-type="fig">Supplementary Fig. S1</xref>), which shows VALOR’s feature space aligns more closely with measured brain activity than all other models.</p>
<p>Beyond predictive accuracy, Study 2 highlights the ecological validity of video-text alignment models by demonstrating their robust generalization to novel stimuli (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>). Traditional unimodal models (e.g., AlexNet, WordNet) require manual preprocessing (e.g., PCA, annotation alignment) to adapt to new datasets, while static image-based multimodal models (e.g., CLIP) lack temporal structure, limiting their ability to capture dynamic neural responses. In contrast, VALOR generalizes more effectively to out-of-distribution movie datasets, suggesting that integrating visual and semantic information over time leads to more stable and flexible neural representations. This aligns with the idea that the brain continuously adapts to ever-changing environments by integrating multimodal cues across time <sup><xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c46">46</xref></sup>. By demonstrating robust performance across different datasets, our approach moves beyond static encoding models toward a framework that more closely reflects the brain’s natural adaptability and predictive processing, advancing the ‘AI for neuroscience’ field toward more biologically plausible models of real-world cognition <sup><xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>.</p>
<p>Study 3 demonstrates video-text alignment model’s utility for probing higher-order semantic representations. Traditional approaches rely on labor-intensive manual annotations to map cortical semantic space, which is time-consuming and prone to raters’ variability <sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c47">47</xref></sup>. In contrast, our model automatically extracts interpretable semantic dimensions—including mobility, sociality, and civilization—that mirror those derived from WordNet-based labeling (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). This correspondence validates video-text alignment model’s ability to uncover large-scale semantic organization without human supervision. By eliminating the need for manual labeling, our approach offers a scalable and reproducible solution for studying conceptual representation in naturalistic contexts.</p>
<p>In Study 4, we used video-text alignment model to investigate predictive coding mechanisms. By incorporating a forecast window, we found that different brain regions encode future events over distinct timescales (<xref rid="fig4" ref-type="fig">Fig. 4a–b</xref>). Short-term predictions were strongest in the superior temporal gyrus (STG), while longer-range forecasts emerged in regions such as the precuneus (PCu), consistent with theories of cortical hierarchy <sup><xref ref-type="bibr" rid="c33">33</xref>–<xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup>. Notably, we observed that individuals with longer prediction distances in the superior parietal gyrus (SPG) exhibited higher fluid cognition scores. This finding suggests a potential link between anticipatory processing and individual cognitive capacity, offering new insights into how the brain predicts and organizes unfolding information in complex environments.</p>
<p>Despite these advances, several limitations remain. First, while VALOR is a high-performing model, it represents only one of many possible architectures; future work may refine these methods further by optimizing accuracy, efficiency, and interpretability. Second, our models primarily used features extracted at the single TR level, which may undersample the full temporal dynamics of perception. Future research should explore longer and adaptive temporal windows that align more closely with brain rhythms. Third, the ShortFunMovies dataset, while offering diversity in naturalistic stimuli, has a limited sample size (n = 20), and future work should validate these findings in larger and more demographically varied populations.</p>
<p>In conclusion, this work establishes video-text alignment encoding as a robust, scalable, and biologically informed framework for studying the brain’s response to naturalistic stimuli. By capturing the temporal and semantic richness of real-world input, this approach advances the field beyond static, modality-limited models and provides new tools for investigating semantic cognition, predictive processing, and individual differences. Beyond theoretical insights, our framework holds practical promise for applications in brain-computer interfaces, clinical neuroimaging, and the development of next-generation cognitive models. As encoding models evolve, video-text alignment stands out as a crucial bridge between deep learning and naturalistic neuroscience, bringing us closer to a comprehensive understanding of the brain in action.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>HCP Naturalistic fMRI Dataset</title>
<p>We analyzed high-resolution 7T fMRI data from 178 individuals who participated in the HCP movie-watching protocol. The dataset included four audiovisual movie scans (1 hour in total) with varying content, from Hollywood film clips to independent Vimeo videos. The fMRI data underwent preprocessing using the HCP pipeline, which included correction for motion and distortion, high-pass filtering, regression of head motion effects using the Friston 24-parameter model, removal of artifactual time series identified with ICA, and registration to the MNI template space. Further details on data acquisition and preprocessing can be found in previous publications<sup><xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c49">49</xref></sup>. For our analysis, we excluded rest periods and the first 20 seconds of each movie segment, resulting in approximately 50 minutes of audiovisual stimulation data paired with the corresponding fMRI response.</p>
</sec>
<sec id="s4b">
<title>Movie Feature Extraction</title>
<list list-type="order">
<list-item><p>video-text alignment features: To extract video-based multimodal features from videos, we used the open-source video-text alignment model known as VALOR<sup><xref ref-type="bibr" rid="c22">22</xref></sup>. VALOR combines visual encoders (CLIP and VideoSwin Transformer) for extracting visual features and a text encoder (BERT) for extracting textual features<sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c50">50</xref>,<xref ref-type="bibr" rid="c51">51</xref></sup>. By aligning features in a joint embedding space through contrastive learning, VALOR enables close association between similar visual and textual features. Videos were segmented at the TR level, and each clip was processed through VALOR to obtain a 512-dimension feature representation. These features were then concatenated in chronological order to create the temporal feature representation of the videos.</p></list-item>
<list-item><p>CLIP features: To compare with image-based multimodal models, we utilized CLIP, which aligns visual and textual representations through contrastive learning but, unlike video-text alignment, processes individual frames independently without capturing temporal information. We extracted frames at the TR level and processed them through CLIP’s ViT-B/32 visual encoder to obtain a 512-dimensional feature representation for each frame. This process enables direct comparison between static image-based multimodal approaches and dynamic video-based approaches.</p></list-item>
<list-item><p>AlexNet features: Visual features were extracted by capturing frames from movies at the TR level and using AlexNet for image feature processing. AlexNet is an eight-layer neural network with five convolutional layers and three fully connected layers that processes basic visual elements in its initial layers and more complex visual representations in deeper layers. In our preliminary analysis, features were extracted and voxel-wise encoding models were applied to all five convolutional layers. The fifth convolutional layer showed the best performance and was selected for further analyses. Intra-image Z-score normalization was used to reduce activation amplitude effects. Principle component analysis (PCA) was used to reduce data dimensionality, retaining the top 512 principal components to ensure consistency with the multimodal features. This process was performed using the DNNBrain toolkit<sup><xref ref-type="bibr" rid="c52">52</xref></sup>.</p></list-item>
<list-item><p>WordNet features: We used publicly available semantic category data from the HCP (7T_movie_resources/WordNetFeatures.hdf5), and processed them with the method by Huth et al. (2012). Each second of the movie clips was manually annotated with WordNet tags by participants, according to certain guidelines: a) identifying clear categories (objects and actions) in the scenes; b) labeling categories that dominated for more than half of the segment duration; c) Use of specific category labels over general ones. A semantic representation matrix was created, with rows representing movie clips per second and columns representing categories. The presence of a category was marked as 1, while its absence was marked as 0. More specific categories from the WordNet hierarchy were added to each labeled category, increasing semantic depth. This expansion resulted in a total of 859 semantic features. For generalizability test in Study 2, we aligned the annotations of the SFM dataset with those from the HCP dataset to ensure consistency in the semantic feature space.</p></list-item>
</list>
<p>These processes described above were performed on a compute node equipped with two Intel(R) Xeon(R) Platinum 8383C CPUs and three NVIDIA GeForce RTX 4090 GPUs. To comply with the journal’s policy, we replaced all human-related video frames used for illustrative purposes (e.g., in <xref rid="fig1" ref-type="fig">Fig. 1</xref> and <xref rid="fig3" ref-type="fig">Fig. 3</xref>) with AI-generated images that do not depict real individuals. These synthetic images were created solely for explanatory visualization and were not used in any model training, testing, or analysis.</p>
</sec>
<sec id="s4c">
<title>Voxel-wise encoding models</title>
<p>Voxel-wise encoding models were created to establish connections between different stimuli and the corresponding brain responses. These models were developed for each individual using visual, linguistic, and multimodal features, and kernel ridge regression was used to map them to brain activation while preserving the interpretability of the model weights. The training set consisted of all movie segments from the HCP 7T movie dataset, excluding repeated segments, while the test set consisted of the repeated segments to improve noise reduction and reliability by averaging the brain imaging data over four runs per participant. A finite impulse response model with four delays (2, 4, 6, and 8 seconds) was used to account for the hemodynamic response. The regularization parameters for each voxel and subject were optimized through 10-fold cross-validation, exploring 30 parameters ranging logarithmically from 10 to 10<sup>30</sup>. Model performance was assessed on the test data by calculating Pearson correlation coefficients between predicted and observed voxel activation sequences.</p>
</sec>
<sec id="s4d">
<title>Novel movie dataset for generalizability test</title>
<p>To evaluate the generalizability of encoding models trained on the HCP dataset, we collected a new fMRI dataset—referred to as “ShortFunMovies”(SFM)—from 20 adult female Chinese participants (mean age = 23.76 ± 2.26 years). This dataset offers exceptional stimulus diversity through eight different movie segments (45 mins in total), including six animations and two live actions, that differ markedly in content and style from the HCP movie stimuli. Each participant watched these eight audiovisual movies while undergoing an fMRI scan using a 3 Tesla Siemens Prisma Magnetom scanner at the MRI Center of Beijing Normal University. The scanning parameters were as follows: TR = 2000 ms, TE = 30 ms, flip angle = 90°, FOV = 210 × 210 mm, spatial resolution = 2.5 mm³, and multiband factor = 6. After preprocessing the fMRI data with fMRIPrep, we denoised and smoothed the brain imaging data (fwhm = 6 mm). We then extracted visual, linguistic, image-based and video-based multimodal features from the movie stimuli and fed them into subject-specific encoding models that were pretrained on the HCP dataset. These models were used to predict each individual’s brain activation patterns in response to the new stimuli. To assess the generalizability of the models, we calculated Pearson correlation coefficients between the predicted brain activations of each HCP subject models and the actual group-level mean activation observed in participants exposed to the new movie dataset. This new data collection was approved by the Institutional Review Board of Beijing Normal University (IRB_A_0024_2021002), and informed consent was obtained from all participants. All participants received monetary compensation after completing the MRI scanning. The SFM dataset was deposited in the OSF website.</p>
</sec>
<sec id="s4e">
<title>Semantic space analysis</title>
<p>Whole-brain semantic space analysis has traditionally relied on manual stimulus annotation. Huth et al. (2012) created a semantic feature matrix by manually annotating video content and decoding semantic dimensions across the brain. To address this challenge, we wanted to verify whether our video-text alignment model could accurately and efficiently automate the analysis of whole-brain semantic space. Therefore, we repeated Huth et al.’s analysis and compared our results with theirs.</p>
<p>We created individual voxel-wise encoding models for each of the five subjects in Huth et al.’s study. To perform a semantic space analysis, all subjects’ encoding model weights were merged and a PCA was performed on the combined data, using the first four principal components according to the methods described by Huth et al. Multimodal features were extracted from over 20,000 videos in the video-text alignment model (VALOR) training set<sup><xref ref-type="bibr" rid="c22">22</xref></sup> and then mapped to these PCs, allowing for semantic content decoding for each PC. To analyze how PCs from WordNet and video-text alignment features are distributed in the cortex, we projected their weights onto the cortical surfaces of subjects. By calculating Jaccard coefficients for the PC distributions in both encoding models, we measured variations and similarities across different cortical regions. The overall Jaccard index, obtained by averaging these coefficients across all participants, provides a comprehensive view of the shared neural patterns.</p>
</sec>
<sec id="s4f">
<title>Probing Predictive Coding Mechanisms</title>
<p>To test predictive coding when watching movies, we incorporated forecast representations — so-called ‘forecast window’ — and examined whether this significantly improved the prediction accuracy of video-text alignment encoding models across different voxels<sup><xref ref-type="bibr" rid="c32">32</xref></sup>. These forecast windows, denoted as <italic>F<sub>t</sub></italic><sub>+<italic>d</italic></sub>, include features extracted by the video-text alignment model from n-second clips, where n aligns with the TR duration of one second. The end frame of each segment is aligned with a temporal offset <italic>d</italic> from the current TR. <italic>F<sub>t</sub></italic><sub>+<italic>d</italic></sub> contains information from a segment subsequent to the current TR, reflecting the brain’s prediction mechanism for potential future stimuli encounters.</p>
<p>For each subject <italic>s</italic> and voxel <italic>v</italic>, we calculated the ‘Brain score’ <italic>B</italic><sup>(<italic>s</italic>,<italic>v</italic>)</sup>, reflecting the effectiveness of using features from the current TR clip to predict brain activation:
<disp-formula id="eqn1">
<graphic xlink:href="646250v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>W</italic> as the kernel ridge regression, <italic>F<sub>t</sub></italic> as the feature of the current TR clip, <italic>corr</italic> as Pearson’s correlation and <italic>Y</italic><sup>(<italic>s</italic>,<italic>v</italic>)</sup> as the fMRI signals of one individual <italic>s</italic> at one voxel <italic>v</italic>.</p>
<p>For each prediction distance <italic>d</italic>, subject <italic>s</italic>, and voxel <italic>v</italic>, we computed the ‘Brain score’ <italic>B</italic><sub>(<italic>d</italic>,<italic>s</italic>,<italic>v</italic>)</sub>, reflecting the model’s accuracy when it includes features of the forecast window alongside the present TR clip features:
<disp-formula id="eqn2">
<graphic xlink:href="646250v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>F<sub>t</sub></italic>⊕<italic>F<sub>t</sub></italic><sub>+<italic>d</italic></sub> represents the integrated feature set combining the current and forecast clip features.</p>
<p>The ‘Predictive score’ <italic>P</italic><sub>(<italic>d</italic>,<italic>s</italic>,<italic>v</italic>)</sub> was computed as the improvement in brain score when concatenating forecast windows to present multimodal features:
<disp-formula id="eqn3">
<graphic xlink:href="646250v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To ensure dimensionality compatibility between <italic>F<sub>t</sub></italic> ⊕ <italic>F<sub>t</sub></italic><sub>+<italic>d</italic></sub>, we applied PCA for dimensionality reduction, reducing both feature types to 100 dimensions each.</p>
<p>We defined the optimal ‘prediction distance’ for each individual <italic>s</italic> and voxel <italic>v</italic> as:
<disp-formula id="eqn4">
<graphic xlink:href="646250v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This process utilized the HCP movie-watching dataset, specifically focusing on the second and fourth runs due to their Hollywood film content, which typically evokes continual plot conjectures during viewing, and regarded the last movie segment of run 4 as the test set and other segments as the training set.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data and Code availability</title>
<p>All data and analysis codes in this project were uploaded to Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/2fnr4/">https://osf.io/2fnr4/</ext-link>)</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by National Natural Science Foundation of China (32422033, 32171032, 32430041), National Science and Technology Innovation 2030 Major Program (2022ZD0211000, 2021ZD0200500), Open Research Fund of the State Key Laboratory of Cognitive Neuroscience and Learning (CNLZD2103) and the start-up funding from the State Key Laboratory of Cognitive Neuroscience and Learning, IDG/McGovern Institute for Brain Research, Beijing Normal University (to Y.W.)</p>
</ack>
<sec id="d1e1070" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Author contributions</title>
<p>M.F. and Y.W. conceived and designed the research. M.F., G.C., Y.Z., and M.Z. collected and analyzed the data. M.F., G.C., Y.Z., and M.Z. wrote the initial draft of the manuscript. M.F. and Y.W. edited and reviewed the final manuscript.</p>
</sec>
</sec>
<app-group>
<app id="s8">
<title>Supplementary Materials</title>
<sec id="s8a">
<title>Representational Similarity Analysis (RSA) in Study 1</title>
<p>To assess the relationship between video-text alignment features and neural activity prior to constructing the encoding model, we conducted a representational similarity analysis (RSA). This approach allowed us to evaluate how well different feature types—AlexNet (visual), WordNet (linguistic), CLIP (image-based multimodal), and VALOR (video-based multimodal)—aligned with multivoxel brain activation patterns across key cortical regions. We focused on three groups of functionally defined regions of interest (ROIs): (i) Visual regions including V1 and V4, (ii) Language-associated regions including middle temporal gyrus (MTG) and angular gyrus (AG), (iii) Higher-order cognitive regions including medial prefrontal cortex (mPFC), posterior cingulate cortex (PCC), and precuneus (PCu)</p>
<p>Each HCP movie run was split into 4–5 stimulus segments that exhibited consistency in visual or linguistic content. Each segment was analyzed independently using a multivariate RSA framework. We computed representational dissimilarity matrices (RDMs) based on pairwise Pearson correlations between multivoxel patterns across timepoints (TRs), yielding TR-by-TR matrices for each ROI. Corresponding feature-based RDMs were constructed for each model using the extracted features. To account for the hemodynamic delay, the first four TRs of each run were excluded. We then computed feature-brain RSA similarity by correlating each brain RDM with the feature RDM using Spearman’s rank correlation. Analyses were performed at the individual subject level and results were aggregated across participants for group-level comparisons. We used paired-sample t-tests with FDR correction to assess statistical differences in RSA values between models.</p>
<p><xref rid="fig1" ref-type="fig">Figure S1</xref> summarizes the results. In early visual cortex (V1), VALOR’s RSA performance matched that of AlexNet and exceeded that of WordNet and CLIP. In V4, VALOR outperformed all other models. In language areas (MTG and AG), VALOR showed substantially higher RSA values than both unimodal and static multimodal features. This advantage also extended to high-level cognitive regions (PCu, PCC), where VALOR consistently yielded the highest RSA scores. The mPFC was the only region where VALOR and WordNet features performed comparably.</p>
<p>These results underscore the broad representational fidelity of video-text alignment features across visual, language, and integrative networks. By capturing both semantic and temporal structure, VALOR provides a more comprehensive approximation of real neural representations, supporting its use in modeling whole-brain responses to naturalistic stimuli.</p>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S1.</label>
<caption><title>Representational similarity analysis (RSA) across feature types and brain regions.</title>
<p>Bar plots show RSA results comparing four feature extraction models—video-text alignment (VALOR), CLIP, AlexNet, and WordNet—across seven predefined regions of interest (ROIs): V1, V4 (visual), MTG, AG (language), and PCu, PCC, mPFC (higher-order cognitive regions). RSA values reflect the similarity between multivoxel neural patterns and feature-derived representational dissimilarity matrices (RDMs), aggregated across all participants and movie segments from the HCP 7T dataset. VALOR consistently outperformed or matched other models across most ROIs, particularly in V4, MTG, AG, PCu, and PCC, indicating its ability to capture both low-level perceptual and high-level semantic structure. In V1, VALOR was comparable to AlexNet. In mPFC, performance was similar between VALOR and WordNet. Statistical comparisons were conducted using paired-sample t-tests, with false discovery rate (FDR) correction applied for multiple comparisons. Asterisks indicate significance levels (*p &lt; .05, ***p &lt; .001); “n.s.” = not significant. Error bars represent ±1 standard error of the mean (SEM) across participants.</p></caption>
<graphic xlink:href="646250v1_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2</label>
<caption><title>Individual subject maps showing performance difference between feature models (related to <xref rid="fig3" ref-type="fig">Figure 3a</xref>).</title>
<p>Cortical flat maps show voxel-wise differences in prediction accuracy between the video-text alignment model (VALOR) and the WordNet-based semantic model, computed as VALOR minus WordNet. Data are shown for four individual subjects (S2–S5) from the Huth et al. (2012) dataset. Red voxels indicate regions where VALOR achieved higher prediction accuracy than WordNet; blue voxels reflect regions where WordNet outperformed VALOR. These maps mirror the analysis presented in <xref rid="fig3" ref-type="fig">Figure 3a</xref> (main text) and demonstrate that VALOR consistently outperforms WordNet across a wide range of cortical areas, including both hemispheres and across subjects. The color scale reflects the magnitude of performance differences, ranging from −0.10 to 0.10 in Pearson correlation units.</p></caption>
<graphic xlink:href="646250v1_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
</app-group>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>Encoding and decoding in fMRI</article-title>. <source>NeuroImage</source> <volume>56</volume>, <fpage>400</fpage>–<lpage>410</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Du</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Vo</surname>, <given-names>V. A.</given-names></string-name>, <string-name><surname>Lal</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Huth</surname>, <given-names>A. G</given-names></string-name></person-group>. <article-title>Brain encoding models based on multimodal transformers can transfer across language and vision</article-title>. <source>Preprint at</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2305.12248">http://arxiv.org/abs/2305.12248</ext-link> (<year>2023</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>A. Y.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Wehbe</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Better models of human high-level visual cortex emerge from natural language supervision with a large and diverse dataset</article-title>. <source>Nat. Mach. Intell.</source> (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1038/s42256-023-00753-y</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huth</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vu</surname>, <given-names>A. T.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L</given-names></string-name></person-group>. <article-title>A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain</article-title>. <source>Neuron</source> <volume>76</volume>, <fpage>1210</fpage>–<lpage>1224</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huth</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>De Heer</surname>, <given-names>W. A.</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Theunissen</surname>, <given-names>F. E.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L</given-names></string-name></person-group>. <article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title>. <source>Nature</source> <volume>532</volume>, <fpage>453</fpage>–<lpage>458</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Finn</surname>, <given-names>E. S</given-names></string-name></person-group>. <article-title>Is it time to put rest to rest?</article-title> <source>Trends Cogn. Sci</source>. <volume>25</volume>, <fpage>1021</fpage>–<lpage>1032</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simony</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Chang</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Analysis of stimulus-induced brain dynamics during naturalistic paradigms</article-title>. <source>NeuroImage</source> <volume>216</volume>, <fpage>116461</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wen</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Shi</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Liu</surname>, <given-names>Z</given-names></string-name></person-group>. <article-title>Transferring and generalizing deep-learning-based neural encoding models across subjects</article-title>. <source>NeuroImage</source> <volume>176</volume>, <fpage>152</fpage>–<lpage>163</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>LeBel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jain</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Huth</surname>, <given-names>A. G</given-names></string-name></person-group>. <article-title>Semantic reconstruction of continuous language from non-invasive brain recordings</article-title>. <source>Nat. Neurosci</source>. <volume>26</volume>, <fpage>858</fpage>–<lpage>866</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khosla</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ngo</surname>, <given-names>G. H.</given-names></string-name>, <string-name><surname>Jamison</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kuceyeski</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Sabuncu</surname>, <given-names>M. R</given-names></string-name></person-group>. <article-title>Cortical response to naturalistic stimuli is largely predictable with deep neural networks</article-title>. <source>Sci. Adv</source>. <volume>7</volume>, <fpage>eabe7547</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ratan Murty</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Bashivan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Abate</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name> &amp; <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>Computational models of category-selective brain regions enable high-throughput tests of selectivity</article-title>. <source>Nat. Commun</source>. <volume>12</volume>, <fpage>5540</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Uden</surname>, <given-names>C. E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Modeling Semantic Encoding in a Common Neural Representational Space</article-title>. <source>Front. Neurosci</source>. <volume>12</volume>, <fpage>437</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wen</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision</article-title>. <source>Cereb. Cortex</source> <volume>28</volume>, <fpage>4136</fpage>–<lpage>4160</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Beckwith</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fellbaum</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gross</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Miller</surname>, <given-names>K. J</given-names></string-name></person-group>. <article-title>Introduction to WordNet: An On-line Lexical Database*</article-title>. <source>Int. J. Lexicogr</source>. <volume>3</volume>, <fpage>235</fpage>–<lpage>244</lpage> (<year>1990</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Madan</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Benchmarking Out-of-Distribution Generalization Capabilities of DNN-based Encoding Models for the Ventral Visual Cortex</article-title>. <source>Preprint at</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2406.16935">http://arxiv.org/abs/2406.16935</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Akkus</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Multimodal Deep Learning</article-title>. <source>arXiv</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2301.04856">http://arxiv.org/abs/2301.04856</ext-link> (<year>2023</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perez-Martin</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A comprehensive review of the video-to-text problem</article-title>. <source>Artif. Intell. Rev</source>. <volume>55</volume>, <fpage>4165</fpage>–<lpage>4239</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name> &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J</given-names></string-name></person-group>. <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nat. Neurosci</source>. <volume>19</volume>, <fpage>356</fpage>–<lpage>365</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dirani</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Pylkkänen</surname>, <given-names>L</given-names></string-name></person-group>. <article-title>The time course of cross-modal representations of conceptual categories</article-title>. <source>NeuroImage</source> <volume>277</volume>, <fpage>120254</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kehl</surname>, <given-names>M. S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Single-neuron representations of odours in the human brain</article-title>. <source>Nature</source> <volume>634</volume>, <fpage>626</fpage>–<lpage>634</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Der Linden</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Van Turennout</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Fernández</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>Category Training Induces Cross-modal Object Representations in the Adult Human Brain</article-title>. <source>J. Cogn. Neurosci</source>. <volume>23</volume>, <fpage>1315</fpage>–<lpage>1331</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset</article-title>. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2304.08345">http://arxiv.org/abs/2304.08345</ext-link> (<year>2023</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Girdhar</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>ImageBind: One Embedding Space To Bind Them All</article-title>. <source>Preprint at</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2305.05665">http://arxiv.org/abs/2305.05665</ext-link> (<year>2023</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</article-title>. <source>Preprint at</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2403.15377">http://arxiv.org/abs/2403.15377</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Radford</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Learning Transferable Visual Models From Natural Language Supervision</article-title>. <source>arXiv</source> (<year>2011</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The minimal preprocessing pipelines for the Human Connectome Project</article-title>. <source>NeuroImage</source> <volume>80</volume>, <fpage>105</fpage>–<lpage>124</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DupréLa Tour</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Eickenberg</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nunez-Elizalde</surname>, <given-names>A. O.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>Feature-space selection with banded ridge regression</article-title>. <source>NeuroImage</source> <volume>264</volume>, <fpage>119728</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Millidge</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Seth</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Buckley</surname>, <given-names>C. L</given-names></string-name></person-group>. <article-title>Predictive Coding: a Theoretical and Experimental Review</article-title>. <source>Preprint at</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2107.12979">http://arxiv.org/abs/2107.12979</ext-link> (<year>2022</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meyer</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Olson</surname>, <given-names>C. R</given-names></string-name></person-group>. <article-title>Statistical learning of visual transitions in monkey inferotemporal cortex</article-title>. <source>Proc. Natl. Acad. Sci</source>. <volume>108</volume>, <fpage>19401</fpage>–<lpage>19406</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shain</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Blank</surname>, <given-names>I. A.</given-names></string-name>, <string-name><surname>Van Schijndel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schuler</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Fedorenko</surname>, <given-names>E</given-names></string-name></person-group>. <article-title>fMRI reveals language-specific predictive coding during naturalistic sentence comprehension</article-title>. <source>Neuropsychologia</source> <volume>138</volume>, <fpage>107307</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Todorovic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Van Ede</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>De Lange</surname>, <given-names>F. P</given-names></string-name></person-group>. <article-title>Prior Expectation Mediates Neural Adaptation to Repeated Sounds in the Auditory Cortex: An MEG Study</article-title>. <source>J. Neurosci</source>. <volume>31</volume>, <fpage>9118</fpage>–<lpage>9123</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caucheteux</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>King</surname>, <given-names>J.-R</given-names></string-name></person-group>. <article-title>Evidence of a predictive coding hierarchy in the human brain listening to speech. Nat</article-title>. <source>Hum. Behav</source>. <volume>7</volume>, <fpage>430</fpage>–<lpage>441</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baldassano</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Discovering Event Structure in Continuous Narrative Perception and Memory</article-title>. <source>Neuron</source> <volume>95</volume>, <fpage>709</fpage>–<lpage>721.e5</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hasson</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Vallines</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name> &amp; <string-name><surname>Rubin</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>A Hierarchy of Temporal Receptive Windows in Human Cortex</article-title>. <source>J. Neurosci</source>. <volume>28</volume>, <fpage>2539</fpage>–<lpage>2550</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raut</surname>, <given-names>R. V.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name> &amp; <string-name><surname>Raichle</surname>, <given-names>M. E</given-names></string-name></person-group>. <article-title>Hierarchical dynamics as a macroscopic organizing principle of the human brain</article-title>. <source>Proc. Natl. Acad. Sci</source>. <volume>117</volume>, <fpage>20890</fpage>–<lpage>20897</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jung</surname>, <given-names>R. E.</given-names></string-name> &amp; <string-name><surname>Haier</surname>, <given-names>R. J</given-names></string-name></person-group>. <article-title>The Parieto-Frontal Integration Theory (P-FIT) of intelligence: Converging neuroimaging evidence</article-title>. <source>Behav. Brain Sci</source>. <volume>30</volume>, <fpage>135</fpage>–<lpage>154</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Preusse</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Elke</surname>, <given-names>V. D. M.</given-names></string-name>, <string-name><surname>Deshpande</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Krueger</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Wartenburger</surname>, <given-names>I</given-names></string-name></person-group>. <article-title>Fluid Intelligence Allows Flexible Recruitment of the Parieto-Frontal Network in Analogical Reasoning</article-title>. <source>Front. Hum. Neurosci</source>. <volume>5</volume>, (<year>2011</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grall</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Equita</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Finn</surname>, <given-names>E. S</given-names></string-name></person-group>. <article-title>Neural unscrambling of temporal information during a nonlinear narrative</article-title>. <source>Cereb. Cortex</source> <volume>33</volume>, <fpage>7001</fpage>–<lpage>7014</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kauttonen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hlushchuk</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Jääskeläinen</surname>, <given-names>I. P.</given-names></string-name> &amp; <string-name><surname>Tikka</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Brain mechanisms underlying cue-based memorizing during free viewing of movie Memento</article-title>. <source>NeuroImage</source> <volume>172</volume>, <fpage>313</fpage>–<lpage>325</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richter</surname>, <given-names>F. R.</given-names></string-name>, <string-name><surname>Cooper</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Bays</surname>, <given-names>P. M.</given-names></string-name> &amp; <string-name><surname>Simons</surname>, <given-names>J. S</given-names></string-name></person-group>. <article-title>Distinct neural mechanisms underlie the success, precision, and vividness of episodic memory</article-title>. <source>eLife</source> <volume>5</volume>, <elocation-id>e18260</elocation-id> (<year>2016</year>). <pub-id pub-id-type="doi">10.7554/eLife.18260</pub-id></mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonnici</surname>, <given-names>H. M.</given-names></string-name>, <string-name><surname>Richter</surname>, <given-names>F. R.</given-names></string-name>, <string-name><surname>Yazar</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Simons</surname>, <given-names>J. S</given-names></string-name></person-group>. <article-title>Multimodal Feature Integration in the Angular Gyrus during Episodic and Semantic Retrieval</article-title>. <source>J. Neurosci</source>. <volume>36</volume>, <fpage>5462</fpage>– <lpage>5471</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sepulcre</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sabuncu</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Yeo</surname>, <given-names>T. B.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Johnson</surname>, <given-names>K. A</given-names></string-name></person-group>. <article-title>Stepwise Connectivity of the Modal Cortex Reveals the Multimodal Organization of the Human Brain</article-title>. <source>J. Neurosci</source>. <volume>32</volume>, <fpage>10649</fpage>–<lpage>10661</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saalasti</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Inferior parietal lobule and early visual areas support elicitation of individualized meanings during narrative listening</article-title>. <source>Brain Behav</source>. <volume>9</volume>, <fpage>e01288</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brodersen</surname>, <given-names>K. H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Generative Embedding for Model-Based Classification of fMRI Data</article-title>. <source>PLoS Comput. Biol</source>. <volume>7</volume>, <fpage>e1002079</fpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Douglas</surname>, <given-names>P. K.</given-names></string-name></person-group> <article-title>Interpreting encoding and decoding models</article-title>. <source>Curr. Opin. Neurobiol.</source> <volume>55</volume>, <fpage>167</fpage>–<lpage>179</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Driver</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Noesselt</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Multisensory Interplay Reveals Crossmodal Influences on ‘Sensory-Specific’ Brain Regions, Neural Responses, and Judgments</article-title>. <source>Neuron</source> <volume>57</volume>, <fpage>11</fpage>–<lpage>23</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name> &amp; <string-name><surname>Bandettini</surname>, <given-names>P. A</given-names></string-name></person-group>. <article-title>Movie-watching outperforms rest for functional connectivity-based prediction of behavior</article-title>. <source>NeuroImage</source> <volume>235</volume>, <fpage>117963</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hasson</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Honey</surname>, <given-names>C. J</given-names></string-name></person-group>. <article-title>Hierarchical process memory: memory as an integral component of information processing</article-title>. <source>Trends Cogn. Sci</source>. <volume>19</volume>, <fpage>304</fpage>–<lpage>313</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>T. Vu</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Tradeoffs in pushing the spatial resolution of fMRI for the 7T Human Connectome Project</article-title>. <source>NeuroImage</source> <volume>154</volume>, <fpage>23</fpage>–<lpage>32</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Devlin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>M.-W.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Toutanova</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</article-title>. <source>Preprint at</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</ext-link> (<year>2019</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>Z.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Video Swin Transformer</article-title>. in <year>2022</year> <conf-name>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name> <fpage>3192</fpage>–<lpage>3201</lpage> (<publisher-name>IEEE</publisher-name>, <conf-loc>New Orleans, LA, USA</conf-loc>, <conf-date><year>2022</year></conf-date>). doi:<pub-id pub-id-type="doi">10.1109/CVPR52688.2022.00320</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>X.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>DNNBrain: A Unifying Toolbox for Mapping Deep Neural Networks and Brains</article-title>. <source>Front. Comput. Neurosci</source>. <volume>14</volume>, <fpage>580632</fpage> (<year>2020</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107607.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Schapiro</surname>
<given-names>Anna C</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents a <bold>valuable</bold> application of a video-text alignment deep neural network model to improve neural encoding of naturalistic stimuli in fMRI. The authors found that models based on multimodal and dynamic embedding features of audiovisual movies predicted brain responses better than models based on unimodal or static features. The evidence supporting the claims is generally <bold>solid</bold>, with clear benchmarking against baseline models. The work will be of interest to researchers in cognitive neuroscience and AI-based brain modeling.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107607.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study compares four models - VALOR (dynamic visual-text alignment), CLIP (static visual-text alignment), AlexNet (vision-only), and WordNet (text-only) - in their ability to predict human brain responses using voxel-wise encoding modeling. The results show that VALOR not only achieves the highest accuracy in predicting neural responses but also generalizes more effectively to novel datasets. In addition, VALOR captures meaningful semantic dimensions across the cortical surface and demonstrates impressive predictive power for brain responses elicited by future events.</p>
<p>Strengths:</p>
<p>The study leverages a multimodal machine learning model to investigate how the human brain aligns visual and textual information. Overall, the manuscript is logically organized, clearly written, and easy to follow. The results well support the main conclusions of the paper.</p>
<p>Weaknesses:</p>
<p>(1) My primary concern is that the performance difference between VALOR and CLIP is not sufficiently explained. Both models are trained using contrastive learning on visual and textual inputs, yet CLIP performs significantly worse. The authors suggest that this may be due to VALOR being trained on dynamic movie data while CLIP is trained on static images. However, this explanation remains speculative. More in-depth discussion is needed on the architectural and inductive biases of the two models, and how these may contribute to their differences in modeling brain responses.</p>
<p>(2) The methods section lacks clarity regarding which layers of VALOR and CLIP were used to extract features for voxel-wise encoding modeling. A more detailed methodological description is necessary to ensure reproducibility and interpretability. Furthermore, discussion of the inductive biases inherent in these models-and their implications for brain alignment - is crucial.</p>
<p>(3) A broader question remains insufficiently addressed: what is the purpose of visual-text alignment in the human brain? One hypothesis is that it supports the formation of abstract semantic representations that rely on no specific input modality. While VALOR performs well in voxel-wise encoding, it is unclear whether this necessarily indicates the emergence of such abstract semantics. The authors are encouraged to discuss how the computational architecture of VALOR may reflect this alignment mechanism and what implications it has for understanding brain function.</p>
<p>(4) The current methods section does not provide enough details about the network architectures, parameter settings, or whether pretrained models were used. If so, please provide links to the pretrained models to facilitate reproducible science.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107607.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Fu and colleagues have shown that VALOR, a model of multimodal and dynamic stimulus features, better predicts brain responses compared to unimodal or static models such as AlexNet, WordNet, or CLIP. The authors demonstrated the robustness of their findings by generalizing encoding results to an external dataset. They demonstrated the models' practical benefit by showing that semantic mappings were comparable to another model that required labor-intensive manual annotation. Finally, the authors showed that the model reveals predictive coding mechanisms of the brain, which held a meaningful relationship with individuals' fluid intelligence measures.</p>
<p>Strengths:</p>
<p>Recent advances in neural network models that extract visual, linguistic, and semantic features from real-world stimuli have enabled neuroscientists to build encoding models that predict brain responses from these features. Higher prediction accuracy indicates greater explained variance in neural activity, and therefore a better model of brain function. Commonly used models include AlexNet for visual features, WordNet for audio-semantic features, and CLIP for visuo-semantic features; these served as comparison models in the study. Building on this line of work, the authors developed an encoding model using VALOR, which captures the multimodal and dynamic nature of real-world stimuli. VALOR outperformed the comparison models in predicting brain responses. It also recapitulated known semantic mappings and revealed evidence of predictive processing in the brain. These findings support VALOR as a strong candidate model of brain function.</p>
<p>Weaknesses:</p>
<p>The authors argue that this modeling contributes to a better understanding of how the brain works. However, upon reading, I am less convinced about how VALOR's superior performance over other models tells us more about the brain. VALOR is a better model of the audiovisual stimulus because it processes multimodal and dynamic stimuli compared to other unimodal or static models. If the model better captures real-world stimuli, then I almost feel that it has to better capture brain responses, assuming that the brain is a system that is optimized to process multimodal and dynamic inputs from the real world. The authors could strengthen the manuscript if the significance of their encoding model findings were better explained.</p>
<p>In Study 3, the authors show high alignment between WordNet and VALOR feature PCs. Upon reading the method together with Figure 3, I suspect that the alignment almost has to be high, given that the authors projected VALOR features to the Huth et al.'s PC space. Could the authors conduct non-parametric permutation tests, such as shuffling the VALOR features prior to mapping onto Huth et al.'s PC space, and then calculating the Jaccard scores? I imagine that the null distribution would be positively shifted. Still, I would be convinced if the alignment is higher than this shifted null distribution for each PC. If my understanding of this is incorrect, I suggest editing the relevant Method section (line 508) because this analysis was not easy to understand.</p>
<p>In Study 4, the authors show that individuals whose superior parietal gyrus (SPG) exhibited high prediction distance had high fluid cognitive scores (Figure 4C). I had a hard time believing that this was a hypothesis-driven analysis. The authors motivate the analysis that &quot;SPG and PCu have been strongly linked to fluid intelligence (line 304)&quot;. Did the authors conduct two analyses only-SPG-fluid intelligence and PCu-fluid intelligence-without relating other brain regions to other individual differences measures? Even if so, the authors should have reported the same r-value and p-value for PCu-fluid intelligence. If SPG-fluid intelligence indeed holds specificity in terms of statistical significance compared to all possible scenarios that were tested, is this rationally an expected result, and could the authors explain the specificity? Also, the authors should explain why they considered fluid intelligence to be the proxy of one's ability to anticipate upcoming scenes during movie watching. I would have understood the rationale better if the authors had at least aggregated predictive scores for all brain regions that held significance into one summary statistic and found a significant correlation with the fluid intelligence measure.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107607.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this work, the authors aim to improve neural encoding models for naturalistic video stimuli by integrating temporally aligned multimodal features derived from a deep learning model (VALOR) to predict fMRI responses during movie viewing.</p>
<p>Strengths:</p>
<p>The major strength of the study lies in its systematic comparison across unimodal and multimodal models using large-scale, high-resolution fMRI datasets. The VALOR model demonstrates improved predictive accuracy and cross-dataset generalization. The model also reveals inherent semantic dimensions of cortical organization and can be used to evaluate the integration timescale of predictive coding.</p>
<p>This study demonstrates the utility of modern multimodal pretrained models for improving brain encoding in naturalistic contexts. While not conceptually novel, the application is technically sound, and the data and modeling pipeline may serve as a valuable benchmark for future studies.</p>
<p>Weaknesses:</p>
<p>The overall framework of using data-driven features derived from pretrained AI models to predict neural response has been well studied and accepted by the field of neuroAI for over a decade. The demonstrated improvements in prediction accuracy, generalization, and semantic mapping are largely attributable to the richer temporal and multimodal representations provided by the VALOR model, not a novel neural modeling framework per se. As such, the work may be viewed as an incremental application of recent advances in multimodal AI to a well-established neural encoding pipeline, rather than a conceptual advance in modeling neural mechanisms.</p>
<p>Several key claims are overstated or lack sufficient justification:</p>
<p>(1) Lines 95-96: The authors claim that &quot;cortical areas share a common space,&quot; citing references [22-24]. However, these references primarily support the notion that different modalities or representations can be aligned in a common embedding space from a modeling perspective, rather than providing direct evidence that cortical areas themselves are aligned in a shared neural representational space.</p>
<p>(2) The authors discuss semantic annotation as if it is still a critical component of encoding models. However, recent advances in AI-based encoding methods rely on features derived from large-scale pretrained models (e.g., CLIP, GPT), which automatically capture semantic structure without requiring explicit annotation. While the manuscript does not systematically address this transition, it is important to clarify that the use of such pretrained models is now standard in the field and should not be positioned as an innovation of the present work. Additionally, the citation of Huth et al. (2012, Neuron) to justify the use of WordNet-based annotation omits the important methodological shift in Huth et al. (2016, Nature), which moved away from manual semantic labeling altogether.</p>
<p>Since the 2012 dataset is used primarily to enable comparison in study 3, the emphasis should not be placed on reiterating the disadvantages of semantic annotation, which have already been addressed in prior work. Instead, the manuscript's strength lies in its direct comparison between data-driven feature representations and semantic annotation based on WordNet categories. The authors should place greater emphasis on analyzing and discussing the differences revealed by these two approaches, rather than focusing mainly on the general advantage of automated semantic mapping.</p>
<p>(3) The authors use subject-specific encoding models trained on the HCP dataset to predict group-level mean responses in an independent in-house dataset. While this analysis is framed as testing model generalization, it is important to clarify that it is not assessing traditional out-of-distribution (OOD) generalization, where the same subject is tested on novel stimuli, but rather evaluating which encoding model's feature space contains more stimulus-specific and cross-subject-consistent information that can transfer across datasets.</p>
<p>Within this setup, the finding that VALOR outperforms CLIP, AlexNet, and WordNet is somewhat expected. VALOR encodes rich spatiotemporal information from videos, making it more aligned with movie-based neural responses. CLIP and AlexNet are static image-based models and thus lack temporal context, while WordNet only provides coarse categorical labels with no stimulus-specific detail. Therefore, the results primarily reflect the advantage of temporally-aware features in capturing shared neural dynamics, rather than revealing surprising model generalization. A direct comparison to pure video-based models, such as Video Swin Transformers or other more recent video models, would help strengthen the argument.</p>
<p>Moreover, while WordNet-based encoding models perform reasonably well within-subject in the HCP dataset, their generalization to group-level responses in the Short Fun Movies (SFM) dataset is markedly poorer. This could indicate that these models capture a considerable amount of subject-specific variance, which fails to translate to consistent group-level activity. This observation highlights the importance of distinguishing between encoding models that capture stimulus-driven representations and those that overfit to individual heterogeneities.</p>
</body>
</sub-article>
</article>