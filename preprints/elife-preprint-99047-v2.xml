<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99047</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99047</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99047.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>The inevitability and superfluousness of cell types in spatial cognition</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5297-2114</contrib-id>
<name>
<surname>Luo</surname>
<given-names>Xiaoliang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>xiao.luo.17@ucl.ac.uk</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7261-9257</contrib-id>
<name>
<surname>Mok</surname>
<given-names>Robert M</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n1">∗∗</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7883-7076</contrib-id>
<name>
<surname>Love</surname>
<given-names>Bradley C</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="author-notes" rid="n1">∗∗</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Department of Experimental Psychology, University College London</institution></institution-wrap>, <city>London</city> <country country="GB">United Kingdom</country>,</aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016bgq349</institution-id><institution>Center for Information and Neural Networks (CiNet), National Institute of Information and Communications Technology (NICT)</institution></institution-wrap>, <city>Osaka</city>, <country country="JP">Japan</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035t8zc32</institution-id><institution>Graduate School of Frontier Biosciences, Osaka University</institution></institution-wrap>, <city>Osaka</city>, <country country="JP">Japan</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04g2vpn86</institution-id><institution>Department of Psychology, Royal Holloway, University of London</institution></institution-wrap>, <city>London</city> <country country="GB">United Kingdom</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01e41cf67</institution-id><institution>Los Alamos National Laboratory</institution></institution-wrap>, <city>Los Alamos</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country country="GR">Greece</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country country="GR">Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1"><label>∗∗</label><p>Co-senior Authors.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-08-28">
<day>28</day>
<month>08</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-11-10">
<day>10</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99047</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-08">
<day>08</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-01-12">
<day>12</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.01.10.575026"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-08-28">
<day>28</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99047.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.99047.1.sa4">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99047.1.sa3">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99047.1.sa2">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99047.1.sa1">Reviewer #3 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.99047.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Luo et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Luo et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99047-v2.pdf"/>
<abstract><p>Discoveries of functional cell types, exemplified by the cataloging of spatial cells in the hippocampal formation, are heralded as scientific breakthroughs. We question whether the identification of cell types based on human intuitions has scientific merit and suggest that “spatial cells” may arise in non-spatial computations of sufficient complexity. We show that deep neural networks (DNNs) for object recognition, which lack spatial grounding, contain numerous units resembling place, border, and head-direction cells. Strikingly, even untrained DNNs with randomized weights contained such units and support decoding of spatial information. Moreover, when these “spatial” units are excluded, spatial information can be decoded from the remaining DNN units, which highlights the superfluousness of cell types to spatial cognition. Now that large-scale simulations are feasible, the complexity of the brain should be respected and intuitive notions of cell type, which can be misleading and arise in any complex network, should be relegated to history.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Author affiliations are updated.
Discussion is extended to make clear the significant theoretical difference to existing literature studying similar topics. Discussion is also extended to discuss more clearly the problems of spatial cell definitions to strengthen the original argument as well as better linking the contribution to broader literature regarding replay and plasticity. In the appendix, additional figures are added S12-S33 to visualise the relationship between model units' spatial properties and corresponding decoder coefficient strengths.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label><title>Introduction</title>
<p>Spatial cognition encompasses our cognitive abilities to know where we are and navigate in complex environments, which includes self-localization, comprehension of environmental layouts, and efficient navigation toward distant goal locations. The prevailing view of the field attributes these functions to neural machinery within the hippocampal formation which contain a diverse array of “spatial” cell types including place cells, head-direction cells, border cells, and grid cells (e.g., <xref ref-type="bibr" rid="c22">Hartley et al. 2014</xref>; <xref ref-type="bibr" rid="c41">Moser et al. 2008</xref>; <xref ref-type="bibr" rid="c16">Grieves and Jeffery 2017</xref>), and that these cells collectively form the foundation of an internal cognitive map of space that underlie our spatial abilities (<xref ref-type="bibr" rid="c43">O’Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="c56">Taube et al., 1990</xref>; <xref ref-type="bibr" rid="c33">Lever et al., 2009</xref>; <xref ref-type="bibr" rid="c20">Hafting et al., 2005</xref>). As such, a substantial proportion of the field focuses on characterizing particular functional “cell types” based on neural activity patterns in these regions. However, these “spatial cells” were identified due to their intriguing firing patterns that piqued the interest of neuroscientists and later defined based on subjective criteria – place cells appear to encode particular locations, border cells respond to borders, head-direction cells are sensitive to a heading direction, grid cells exhibit firing fields in a grid-like pattern. In practice, the field’s initial fascination with these cells has led to a stubborn tendency to focus on “cell type” classification and overlook the fact that the criteria for distinct cell types seldom align seamlessly with empirical data and often exhibit mixed selectivity across task and environmental variables (e.g., <xref ref-type="bibr" rid="c25">Hollup et al. 2001</xref>; <xref ref-type="bibr" rid="c36">McKenzie et al. 2013</xref>; <xref ref-type="bibr" rid="c8">Dupret et al. 2010</xref>; <xref ref-type="bibr" rid="c10">Eichenbaum 2015</xref>; <xref ref-type="bibr" rid="c16">Grieves and Jeffery 2017</xref>; <xref ref-type="bibr" rid="c32">Latuske et al. 2015</xref>; <xref ref-type="bibr" rid="c54">Tang et al. 2015</xref>), raising questions about their precise roles, if any.</p>
<p>Could these cells simply arise from any systems with complex computations, unrelated to processes specific to spatial or navigation? There are strong hints that “spatial” cells may arise simply from domain-general learning algorithms that can lead to both concept and spatial neural representations (<xref ref-type="bibr" rid="c39">Mok and Love, 2019</xref>, <xref ref-type="bibr" rid="c40">2023</xref>; <xref ref-type="bibr" rid="c52">Stachenfeld et al., 2017</xref>; <xref ref-type="bibr" rid="c61">Whittington et al., 2020</xref>). Indeed, models show that spatial firing patterns can arise from factors entirely unrelated to spatial cognition, such as sparseness constraints (<xref ref-type="bibr" rid="c31">Kropff and Treves, 2008</xref>; <xref ref-type="bibr" rid="c13">Franzius et al., 2007a</xref>,<xref ref-type="bibr" rid="c14">b</xref>).</p>
<p>Another question is whether these cells are particularly important for spatial cognition. Research has shown that “spatial” cells may not be functionally more critical than those with other cells with multiplexed or hard-to-interpret firing patterns for spatial localization (<xref ref-type="bibr" rid="c5">Diehl et al., 2017</xref>) or for explaining neuronal response profiles in the medial entorhinal cortex (<xref ref-type="bibr" rid="c42">Nayebi et al., 2021</xref>). Even lesion studies in the hippocampal formation do not consistently result in specific navigation impairments (<xref ref-type="bibr" rid="c21">Hales et al., 2014</xref>; <xref ref-type="bibr" rid="c59">Whishaw and Tomie, 1997</xref>; <xref ref-type="bibr" rid="c60">Whishaw et al., 1997</xref>).</p>
<p>Considering that both biological and artificial “cells” rarely perfectly align with human-defined criteria with many cells exhibit “uninterpretable” firing patterns, one unsettling possibility is that the neuroscience may have inadvertently constrained its pursuit by fixating on the search for interpretable “cell types”. By adopting this top-down, perhaps näıve approach, the field may be investing a large proportion of its resources in a potentially fruitless quest, ignoring other explanations for the emergence of spatial cells and the foundations of spatial cognition.</p>
<p>A harsh appraisal of the field is that it looks for whatever cell type superficially reflects the answer sought. For example, how do animals localize themselves? The naive answer is that there must be place cells. Setting aside this answer provides zero insight into how such a cell came to be, there is no reason for the brain to perfectly align with our intuitions and, equally problematic, cells with these properties might arise but not serve the top-down function neuroscientist ascribe to them. Ascribing interpretable functions to cells and naming them may be good for neuroscience careers, but is it good for neuroscience?</p>
<p>In this contribution, we offer an alternative to the prevailing notion that cells with readily interpretable receptive fields in regions like hippocampus underlie spatial systems. We propose that what we commonly refer to as “spatial cells” might be inevitable derivatives of general computational mechanisms, and are in no way intrinsically spatial. Our results demonstrate that these cells could manifest within any sufficiently rich information processing system, such as perception. Remarkably, not only do we find spatial signals within a non-spatial system but also demonstrate that these purported signals, ostensibly representing spatial knowledge, do not appear to have a privileged role for tangible downstream tasks related to spatial cognition as previously claimed.</p>
<p>To evaluate these possibilities, we turned to deep neural networks (DNNs) that were information processing systems devoid of components dedicated to spatial processing. Specifically, we analyzed deep learning models of perception with hundreds of millions of parameters. DNNs trained on natural images have achieved human-level performance in recognizing real-world objects in photographs (<xref ref-type="bibr" rid="c30">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="c50">Simonyan and Zisserman, 2015</xref>). These models exhibit strong parallels with representations in the primate ventral visual stream (<xref ref-type="bibr" rid="c29">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="c63">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="c19">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="c12">Eickenberg et al., 2017</xref>; <xref ref-type="bibr" rid="c64">Zeman et al., 2020</xref>). Our investigation aims to determine whether processing egocentric visual information within such complex – yet non-spatial – models can lead to brain-like representations of allocentric spatial environments, and whether these representations are essential for spatial cognition. Additionally, we considered whether untrained DNNs (absent any experiences) can account for the emergence of spatial cells and spatial cognition.</p>
<p>To evaluate functional spatial information in perceptual DNN models, we began by creating a three-dimensional virtual environment reminiscent of a laboratory used in animal studies in neuroscience (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). In this virtual setting, an agent randomly forages in a two-dimensional square area, much like an animal would explore an enclosure. The agent “sees” images of the room within its field of view over many locations and heading directions. Images from the first-person perspective are processed by a DNN, and we assess whether the internal representations of the model contain various kinds of spatial knowledge. We adopted a decoding framework which parallels experimental techniques employed by neuroscientists. Specifically, we trained linear regression models using various levels of representations (unit activations from visual inputs) generated by DNNs of different architectures to decode navigation-relevant variables, including self-localization, heading direction, and distance to the closest wall on locations and heading directions they have not encountered before. To test whether DNN units carry information like “spatial” cells in the brain, we classified and visualized model units based on standard criteria for place, direction, and border cells (<xref ref-type="bibr" rid="c55">Tanni et al., 2022</xref>; <xref ref-type="bibr" rid="c2">Banino et al., 2018</xref>). To evaluate whether these cells play a privileged role in spatial cognition, we excluded units classified as spatial units based on traditional spatial cell criteria and assessed their spatial knowledge.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Assessing spatial knowledge in non-spatial perception systems using a linear decoding approach in a virtual environment.</title>
<p>(A) A three-dimensional virtual space is created to resemble a realistic laboratory environment with a variety of visual features. An agent moves randomly in a two-dimensional area which is within a three-dimensional space and processes first-person views of the environment. Central image shows the threedimensional environment. Surrounding images are example views taken by the agent at different locations and heading directions. (B) Top-down view of the area where the agent can explore. We define spatial knowledge of the agent with four values. The agent’s location is denoted by the Cartesian coordinates t<italic><sub>x</sub></italic>, t<italic><sub>y</sub></italic>. The agent’s heading direction is denoted by the angle t<italic><sub>r</sub></italic>. The distance between the agent and the nearest wall is t<italic><sub>b</sub></italic>. (C) Individual views are processed by perception models (deep neural networks of object recognition). We train linear regression models with various levels of internal representations from these networks to assess spatial knowledge related to self-location (t<italic><sub>x</sub></italic>, t<italic><sub>y</sub></italic>), heading direction (t<italic><sub>r</sub></italic>) and distance to the closest wall (t<italic><sub>b</sub></italic>).</p></caption>
<graphic xlink:href="575026v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To foreshadow our findings, we discovered that DNNs, regardless of their architectural variations, representation levels, or training states, possess noteworthy proficiency in allocentric spatial understanding. Spatial variables including location, heading direction and distance to boundaries can be accurately decoded from the DNN models. Additionally, a substantial number of DNN units exhibit spatial firing patterns that align with the criteria used in neuroscience to classify place, head-direction, and border cells. Notably, a significant portion of these units employs mixed-selectivity and conjunctive coding, akin to characteristics found in hippocampal neurons (<xref ref-type="bibr" rid="c10">Eichenbaum, 2015</xref>). We further revealed that excluding units meeting the classical criteria of spatial cell types has minimal impact on spatial decoding performance, reinforcing the perspective that the code underpinning spatial cognition is more distributed and diverse than previously thought, relying on neural population coding rather than individual and specialized cell types (<xref ref-type="bibr" rid="c24">Hebb, 1949</xref>; <xref ref-type="bibr" rid="c11">Eichenbaum, 2018</xref>; <xref ref-type="bibr" rid="c9">Ebitz and Hayden, 2021</xref>). In summary, our analyses suggest that “spatial” cell types may be inevitable and superfluous in complex information processing systems.</p>
</sec>
<sec id="s2">
<label>2</label><title>Results</title>
<sec id="s2a">
<label>2.1</label><title>Spatial Knowledge through Non-spatial Systems</title>
<p>We predict that spatial knowledge can arise in computational systems with sufficient complexity absent a spatial grounding. We test this hypothesis by analyzing the percepts of an agent freely moving in a virtual environment. The percepts take the form of viewpoints fed into a deep neural network with its millions of weights either randomly initialized or trained for object recognition, a non-spatial task. From various network layers, we attempted to decode spatial information such as location, from this complex computational system.</p>
<p>To assess information latent in complex networks, we trained linear regression models to decode the agent’s location, head-direction and distance to the closet border using fixed unit activity extracted at various levels of the networks. While sequential information is important for spatial systems, our decoding framework does not utilize sequential information of sampled views during training, making it a stricter test for the central claim that spatial knowledge can form out of general information processing. We expect that one’s location, heading-direction and distance from border may be decodable from visual information alone due to the inherent continuity of perception across spatial locations and/or directions. All results reported here are tested on out-of-sample locations and views using the perception model VGG-16 (<xref ref-type="bibr" rid="c51">Simonyan et al., 2014</xref>), unless noted otherwise. For details referring to training and testing procedure, see Methods. For results of other models such as Resnet-50 (<xref ref-type="bibr" rid="c23">He et al., 2016</xref>) and Vision Transformers (ViT; <xref ref-type="bibr" rid="c7">Dosovitskiy et al. 2020</xref>), see Appendix.</p>
<p>We find that spatial knowledge, namely one’s own location (spatial coordinates; <xref rid="fig2" ref-type="fig">Fig. 2B</xref>, left panel), heading direction (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, middle panel) and distance to the nearest wall (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, right panel) can be successfully decoded from unit activity in a perception model. We quantified the degree of spatial knowledge by the decoding error, that is, the deviation of the model’s prediction of its own location, heading direction, or distance to the nearest wall relative to the ground truth (i.e., the physical unit of space or angle in the virtual environment). We normalized decoding error with respect to the maximal unit length of the moving area (see Methods). For example, a normalized error of 0.05 for location prediction means the decoded location is 5% off from the true location relative to the maximal width/depth of the entire moving area. As more locations and views are sampled for training the agent (x-axis), decoding performance improves. The agent demonstrates impressive decoding performance across all spatial knowledge tests, even when trained on just 30% of randomly selected independent locations and associated viewpoints within the entire space. The moving area encompasses 289 distinct locations, each featuring 24 evenly spaced views covering the full 360-degree spectrum (for more details, please see Methods).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Perception models absent a spatial basis possess extensive spatial knowledge.</title>
<p>(A) Decoding performance across tasks and model layers. Across three tasks, mid-to-late layers exhibited lower decoding errors compared to early layers and the penultimate layer of VGG-16 (fc2). All layers outperformed chance (green and blue lines). (B) Decoding performance across a number of deep neural network architectures (penultimate layer; see Appendix for full results), including convolutional networks and vision transformers. All pre-trained and untrained models outperformed baseline measures. Error is in normalized virtual environment units (see Methods). See full results across models, layers and sampling rates in the Appendix. Shaded areas in B and error bars in C represent 95% confidence intervals of the mean decoding error (bootstrapped across locations).</p></caption>
<graphic xlink:href="575026v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Decoding performance varied across layers of the network where deeper layers typically achieved lower decoding error (apart from the penultimate layer in VGG-16, i.e., fc2). Crucially, decoders across all sampling rates and levels of representation show substantially better performance than chance (green and blue lines), determined by strategies invariant to visual inputs (i.e., random or predicting the center of possible choices; see Methods). Our results demonstrate there is sufficient spatial information to achieve remarkably low decoding error in a perception model based on a few visual snapshots of the virtual space, which could be further improved through the integration of information across views in a perception-based navigation system.</p>
<p>To test the generality our claim that spatial knowledge can arise from complex computational systems irrespective of their architectural variations or training states, we examined several computational systems including deep convolutional neural networks (DCNN) and Vision Transformers (ViT). We evaluated these models in both their pre-trained form (optimized for visual object recognition), and in an untrained state (randomly initialized parameters), following the same decoding approach described earlier. Here, we present the results for the penultimate layer representation and uniformly sampled 30% of all locations and their views for training the decoder, but similar results were observed across layers (for full results across various models, layers, and sampling rates, see Appendix.). We found that all models possessed latent spatial knowledge, exhibiting lower errors far below chance (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). Notably, this was true even in untrained networks. While one might contend that deep convolutional neural networks, with their reliance on local convolution operations, inherently possess a predisposition for extracting spatial knowledge from visual information, the revelation that an untrained ViT—comprising nothing more than a hierarchy of fully-connected layers (i.e., self-attention) and non-linear operations—can achieve superior decoding performance illustrates the inevitability of computational complexity in facilitating the extraction of spatial information necessary for cognitive spatial processes, even in non-spatial systems.</p>
</sec>
<sec id="s2b">
<label>2.2</label><title>“Spatial Cells” Inevitably Arise in Complex Computational Systems</title>
<p>How was it possible to decode spatial information from non-spatial networks? The field’s common conception is that cells exhibiting spatial firing profiles play a pivotal role in supporting spatial cognition, as these profiles appear intuitively useful to spatial tasks such as navigation. Could spatial cells be responsible for the impressive decoding of spatial information within a perception system like we have shown? If a perception system devoid of a spatial component demonstrates classically spatially-tuned unit representations, such as place, head-direction, and border cells, can “spatial cells” truly be regarded as “spatial”? Might they be inevitable derivatives of any complex system? If so, do they drive spatial knowledge, or are they in fact superfluous in spatial cognition?</p>
<p>To determine if the model’s spatial knowledge is primarily supported by spatially-tuned units like those found in the brain, we classified every hidden unit in each model based on criteria used to identify spatial cells in neuroscience for place cells (<xref ref-type="bibr" rid="c55">Tanni et al., 2022</xref>), head-direction cells (<xref ref-type="bibr" rid="c2">Banino et al., 2018</xref>), and border cells (<xref ref-type="bibr" rid="c2">Banino et al., 2018</xref>), respectively (see Methods). The composition of cell types across layers is shown in <xref rid="fig3" ref-type="fig">Fig. 3A</xref>. Contrary to the predominant view that “spatial cells” are unique properties of spatial systems and are grounded in space-related tasks, we found many units in the non-spatial model VGG-16 that satisfied the criteria of place, head-direction, and border cells (see Methods for criteria; for other models, see Appendix). The majority of units show mixed selectivity irrespective of layer depth. Notably, a significant proportion of units show both place and directional tuning (P+D), matching a common observation in the literature (e.g., <xref ref-type="bibr" rid="c38">McNaughton et al. 1991</xref>; <xref ref-type="bibr" rid="c54">Tang et al. 2015</xref>; <xref ref-type="bibr" rid="c16">Grieves and Jeffery 2017</xref>). We selected example units and plotted their spatial activation patterns in the two-dimensional virtual space (irrespective of direction), and their direction selectivity in polar plots (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>-<xref rid="fig3" ref-type="fig">3E</xref>; for more examples, see Appendix). These examples show spatially-tuned units without directional tuning that match hippocampal place cells (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>), direction-selective units with strong direction selectivity accompanied by minor spatial selectivity matching head-directional cells (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>), and units with boundary cell-like tuning (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>). There were also many units that exhibited both strong place and directional tuning (<xref rid="fig3" ref-type="fig">Fig. 3E</xref>). These results support our theory that “spatial cells” might arise in any computational systems, even in systems designed for nonspatial tasks (e.g., object recognition). Consistent with our view, we found no clear relationship between cell type distribution and spatial information in each layer. This raises the possibility that “spatial cells” do not play a pivotal role in spatial tasks as is broadly assumed.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Representations of perception models developed for object recognition exhibit typical spatial cell-like firing profiles.</title>
<p>Active units were classified across model layers based on standard criteria used to identify place cells (P), head-direction cells (D), and border cells (B). Example units from VGG-16 (see Appendix for more examples from other models). (A) Pie charts illustrating the proportion of “spatial” cell types identified in DNNs, including units that are inactive. Many units satisfied the criteria for place, head-direction and border cells irrespective of layer depth. Many units exhibited mixed selectivity, with a significant amount of units displaying strong place and directional tuning. (B-E) Spatial firing profiles of model units in spatial activation maps and polar plots . For activation maps, each unit’s activation was plotted at each location in the two-dimensional area irrespective of heading direction. For direction selectivity, polar plots show the average activity of the activity map across location at a given angle, which reflects the tuning magnitude to each heading direction). (B) Example place cell units that show strong spatial selecitivity with little direction selectivity. (C) Examples of head-direction cell units that show strong direction selectivity but weak location selectivity. (D) Examples of border cell units that respond strongly to boundaries of the environment. (E) Examples of mixed-selective place and head-direction cell units with strong spatial and directional tuning. Examples presented are from VGG-16. See Appendix for more examples.</p></caption>
<graphic xlink:href="575026v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<label>2.3</label><title>“Spatial Cells” Are Superfluous in Spatial Cognition</title>
<p>We have establishd that“spatial cells” can arise in non-spatial systems. Here, we consider whether units with spatial properties are necessary to decode spatial information. It may very well be that spatial units, akin to place cells, might not only arise in complex (including random) networks but are also superfluous to spatial cognition. To test this possibility, we performed a systematic exclusion analysis to assess whether model units that exhibit the strongest spatial firing properties contribute to spatial knowledge. First, we scored and ranked each unit classified as place, head-direction, and border cell units (see Methods), and then re-trained linear decoders without the top n units of a specific cell type and evaluated the model’s corresponding spatial knowledge of the environment. That is, we test model’s location, heading direction and distance to closet border decoding ability by excluding place, head-direction and border cell units, respectively. We repeated this procedure with a progressively higher exclusion ratio (see Methods).</p>
<p>In line with our hypothesis, excluding spatial units in the models that scored highest on each of the corresponding criteria (place field activity, number of place fields, directional tuning, and border tuning), had minimal effect on decoding performance even with a large proportion of the highest ranked spatial units being excluded (<xref rid="fig4" ref-type="fig">Fig 4B</xref>, top panel). To assess whether any detriment was observed was significant and specific to excluding highly-ranked spatial units, we randomly excluded the same number of units without regard to the spatial score (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>, bottom panel) and observed a similar pattern across four exclusion scenarios, meaning that highly-tuned spatial units contributed no more to spatial cognition than a random selection of units.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>“Spatial” cells do not play a privileged role in spatial cognition.</title>
<p>Exclusion analyses showed that units exhibiting traditional spatial firing profiles do not form the basis of the model’s spatial knowledge. (A) Units in each layer were ranked based on standard criteria for place (maximum place field activity, number of place fields), head-direction (strength of directional tuning), and border (strength of border tuning) cells. As more highly-ranked spatial units were excluded, the overall decoding performance remained relatively stable across all tasks (top). Excluding an equivalent number of units randomly yielded similar performance (bottom). (B) Model units’ contribution to spatial knowledge based on their contribution to decoding performance (magnitude of regression coefficients). More highly-ranked task-relevant units excluded results in a marked deterioration of decoding performance by decoders trained on the remaining units (top). Randomly excluding an equivalent number of units had minimal impact on performance (bottom). Results here are from VGG-16. For other models, please refer to the Appendix.</p></caption>
<graphic xlink:href="575026v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Finally, we performed an additional analysis where we excluded model units that contributed most to each of the tasks. Specifically, we identified each unit’s importance for each task, based on the magnitudes of the coefficients learned by the linear decoders (trained in <xref rid="s2a" ref-type="sec">Section 2.1</xref>), excluded the top n units, and re-trained linear decoders for each respective task using the remaining units. This approach had a greater impact on decoding error compared to spatial unit criteria or random exclusion, with performance suffering more as the exclusion ratio rose (see <xref rid="fig4" ref-type="fig">Figure 4C</xref>). Nevertheless, decoding was still possible at high exclusion ratios, suggesting that spatial information is widely distributed across network units.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label><title>Discussion</title>
<p>The prevailing perspective in neuroscience conflates spatial knowledge and the variety of spatial cognitive abilities in animals with “spatial cells” in the hippocampal formation. In this work, we proposed an alternative perspective where “spatial cells” can simply arise in non-spatial computational systems with sufficient complexity, and that these units may, in fact, be by-products of such systems absent any leading role in spatial cognition.</p>
<p>Deep neural networks (DNNs) for object recognition served as an ideal testbed for our hypotheses. Object recognition DNNs, designed to generate translation-invariant visual features for successful categorization, lack inherent spatial elements. Unlike spatial navigation systems (e.g., <xref ref-type="bibr" rid="c2">Banino et al. 2018</xref>; <xref ref-type="bibr" rid="c4">Cueva and Wei 2018</xref>), DNNs in object recognition do not purposefully incorporate information about velocity, heading direction, or event sequences. Exploring various DNN architectures for object recognition allows a comprehensive examination of the connection between spatial representations and non-spatial complex systems.</p>
<p>We applied DNNs to a three-dimensional virtual environment to simulate an agent foraging in a two-dimensional space, akin to how an animal explores an enclosure. Detailed analyses revealed that DNN’s representations contained sufficient spatial information to decode key variables, despite these DNNs being non-spatial perceptual systems. We found that many DNN units passed established criteria for classification as place, head-direction, and border cells, suggesting such units can be easily found in various non-spatial computational systems. Furthermore, excluding the “spatial” units had minimal impact on spatial decoding tasks, suggesting a non-essential role for spatial cognition. These results call into question utility of labelling these cell “types”.</p>
<p>We considered object recognition models from two prominent DNN families: deep convolutional networks, drawing inspiration from the mammalian visual system (<xref ref-type="bibr" rid="c15">Fukushima, 1980</xref>), and visual transformers, adapted from the Transformer architecture originally designed for natural language processing (<xref ref-type="bibr" rid="c58">Vaswani et al., 2017</xref>). Spatial information was readily decoded from models from both families, including untrained models with randomly initialized weights. The decoding results from the untrained visual transformer are particularly striking because transformers incorporate minimal inductive biases, such as constraints that respect spatial proximity (e.g., convolutions). In summary, complex networks that are not spatial systems, coupled with environmental input, appear sufficient to decode spatial information.</p>
<p>While previous modeling efforts have highlighted the importance of environmental sensory input in driving the emergence of spatial cells (e.g., <xref ref-type="bibr" rid="c13">Franzius et al. 2007a</xref>), it is worth noting the substantial theoretical and modeling differences to our contributions despite perceived similarity. Franzius et al. (2007) propose a sensory-driven model where spatial cells, assumed to be inherently special, emerge from visual input statistics tied to motion and slow-feature analysis, yet their model does not simultaneously produce place and head-direction cells nor assess their functional utility. In contrast, our work challenges the specialness of spatial cells, demonstrating their emergence in a non-spatial, nonsequential null model with fixed weights, while showing that these cells, though present, offer little benefit for spatial decoding tasks.</p>
<p>Indeed, our work raises the question of whether focusing on single cells and their “type” is the correct approach to understanding spatial cognition, or indeed any cognitive function. The field has maintained a persistent inclination towards identifying cell types based on firing profiles that are subjectively interpreted to be useful for the behavior of interest (<xref ref-type="bibr" rid="c22">Hartley et al., 2014</xref>; <xref ref-type="bibr" rid="c41">Moser et al., 2008</xref>; <xref ref-type="bibr" rid="c48">Sarel et al., 2017</xref>; <xref ref-type="bibr" rid="c57">Tsao et al., 2018</xref>; <xref ref-type="bibr" rid="c27">Høydal et al., 2019</xref>; <xref ref-type="bibr" rid="c17">Grieves et al., 2020</xref>; <xref ref-type="bibr" rid="c45">Ormond and O’Keefe, 2022</xref>). This is likely due to the historical backdrop in neuroscientific research where pioneers in visual neuroscience discovered and named individual cells in V1 by observing their firing profiles (<xref ref-type="bibr" rid="c26">Hubel and Wiesel, 1959</xref>) which won the Nobel Prize (1981) and marked a foundational era for the field. This paradigm, led to discoveries of place and grid cells (Nobel Prize in Physiology or Medicine, 2014), and the “Jennifer Aniston” neuron or “concept cells” (<xref ref-type="bibr" rid="c46">Quiroga et al., 2005</xref>), which were intuitively compelling and attractive as an apparent explanation. These cells may in fact play a role, but our work suggests that this must be critically assessed rather than assumed, as they may not play a privileged role compared to other cells in the population, and could even be superfluous for the context at hand. The general assumption that the simplicity and interpretability of firing profiles somehow lends support to a crucial role of these cells should be questioned. Unlike structurally or genetically defined cell types (e.g., pyramidal neurons, interneurons, dopamingeric neurons, c-Fos expressing neurons), it is unclear whether “cell types” in the spatial or even conceptual domain should be considered cell types in the same way. Indeed, the identification criteria for cell types themselves are often problematic and inconsistent across studies. For instance, place field selection thresholds vary from 20% to 25% of peak firing rate (<xref ref-type="bibr" rid="c53">Tanaka et al., 2018</xref>; <xref ref-type="bibr" rid="c6">Dombeck et al., 2010</xref>), speed thresholds range dramatically from 2 cm/s to 8.3 cm/s (<xref ref-type="bibr" rid="c53">Tanaka et al., 2018</xref>; <xref ref-type="bibr" rid="c6">Dombeck et al., 2010</xref>), and spatial bin requirements differ across leading studies (<xref ref-type="bibr" rid="c55">Tanni et al., 2022</xref>; <xref ref-type="bibr" rid="c53">Tanaka et al., 2018</xref>). As <xref ref-type="bibr" rid="c18">Grijseels et al. (2021)</xref> demonstrated, different detection methods produce vastly different place cell counts with minimal overlap between identified populations. This inconsistency raises critical questions about whether researchers are studying the same phenomena or entirely different neural populations.</p>
<p>With a growing recognition of the imperative to move beyond mere cell type categorization, as underscored by <xref ref-type="bibr" rid="c44">Olshausen and Field (2006)</xref> on the insufficient characterization of a majority of V1 cells, there is a new shift to focus on a broader perspective where neural assemblies or populations form the fundamental computational unit (<xref ref-type="bibr" rid="c24">Hebb, 1949</xref>; <xref ref-type="bibr" rid="c11">Eichenbaum, 2018</xref>; <xref ref-type="bibr" rid="c9">Ebitz and Hayden, 2021</xref>).</p>
<p>We pose a broader question: do our preconceptions of how complex systems <italic>should</italic> work hinder our aim to understand the brain’s inner workings? Notably, spatial representations conventionally linked to the hippocampal formation have been found in sensory areas (<xref ref-type="bibr" rid="c47">Saleem et al., 2018</xref>; <xref ref-type="bibr" rid="c35">Long et al., 2021</xref>; <xref ref-type="bibr" rid="c34">Long and Zhang, 2021</xref>). While prior attributions pointed to modulatory signals from the hippocampus, our study raises concerns about potential biases stemming from preconceived notions and an underestimation of the complexity inherent in the sensory system—it may be shouldering more extensive responsibilities than initially presumed. Work in other domains are reaching similar conclusions (cf. <xref ref-type="bibr" rid="c37">McMahon and Isik 2023</xref>).</p>
<p>It is also worth noting that the null model perspective extends beyond static representations to dynamic neural phenomena. Consider replay–the reactivation of neural firing patterns during rest or sleep, widely interpreted as evidence of memory consolidation and plasticity-dependent learning processes (e.g., <xref ref-type="bibr" rid="c62">Wilson and McNaughton 1994</xref>; <xref ref-type="bibr" rid="c49">Schapiro et al. 2018</xref>; <xref ref-type="bibr" rid="c3">Barry and Love 2022</xref>. However, our framework suggests that such sequential reactivation patterns could emerge as byproducts of complex system dynamics, independent of learning mechanisms. Just as spatial cell types manifest in untrained networks, replay-like patterns may arise from the inherent complexity of neural circuits without necessitating plasticity-driven explanations. Indeed, the key idea behind reservoir computing is that a pool of units with complex dynamics can provide the basis for useful computations in the absence of any plasticity (<xref ref-type="bibr" rid="c28">Jaeger and Haas, 2004</xref>). The mere observation of repeated activity patterns should not be conflated with evidence of learning-dependent processes.</p>
<p>One upshot of our contribution is that the pursuit of identifying and cataloging cell types aligned with subjective notions of how the brain works might impede scientific progress. While searching for cell types was a reasonable strategy in an era before it was straightforward to conduct large-scale simulations, this quest should now be relegated to history. Otherwise, apparent discoveries of cell types are likely to reflect the activity of broad classes of complex networks that implement a variety of functions or none at all in the case of the random networks we considered. It is far too easy for neuroscientists, including computational neuroscientists who design their models to manifest a variety of cell types, to fool themselves when the underlying complexity of the systems considered is not taking into account.</p>
</sec>
<sec id="s4">
<label>4</label><title>Methods</title>
<sec id="s4a">
<label>4.1</label><title>Virtual environment</title>
<p>We setup our virtual environment using the Unity3D Engine (editor version: 2021.3.16f1; Silicon). The three-dimensional laboratory environment was adapted from an existing template purchased on the Unity Store (<xref ref-type="bibr" rid="c1">3DEverything, 2022</xref>). Our virtual agent’s movement is constrained in a two-dimensional square placed inside the three-dimensional space, much like a moving area for an animal in real-world experiments. The agent randomly moves around the square area and captures first-person pictures of the environment across locations and heading directions.</p>
</sec>
<sec id="s4b">
<title>Environment specifications</title>
<p>The specifications for the environment, measured in Unity units, are as follows: the lab room has a height of 3.17 units, a width of 6.29 units, and a depth of 10.81 units. The moving area, which is a distinct part of the environment, has a height of 0.25 units, a width of 2 units, and a depth of 2 units. Additionally, the moving area is located at a specific location whose relative distances from the center to the lab room boundaries are: 3.59 units to the left wall, 5.35 units to the right wall, 5.55 units to the bottom wall, and 2.80 units to the front wall.</p>
<p>The agent is only allowed to move within the squared moving area. For simplicity, we discretize reachable locations by the agent in the area as a 2D grid where the agent can only move between points on the grid. We use a 17×17 grid (289 unique locations).</p>
</sec>
<sec id="s4c">
<title>Visual input</title>
<p>To collect visual inputs from the virtual environment, the agent randomly moves along the specified n-by-n grid in the squared area. We denote l as the total number of unique locations on the grid. The grid is evenly spaced. At each location, the agent turns around and captures m number of frames, each separated by a fixed angle (e.g., 15 degrees per frame). Each frame obtained by the agent is processed by a DNN (up to a given layer). Resulting outputs at the same location are considered independent data-points, each forming a long 1D vector. Repeating this procedure for all locations yields a M-by-F data matrix <bold>X</bold> where M = l × m and F is the number of feature variables. Each feature variable represents a neuron on the output layer of the DNN.</p>
</sec>
<sec id="s4d">
<label>4.2</label><title>Spatial decoding</title>
<p>To evaluate whether representations in the deep neural networks (DNN) optimized for object recognition on natural images encode spatial information, we set up a series of spatial decoding tasks where we use fixed network representations across various hidden layers to decode spatial location (i.e., coordinates), head-direction (i.e., rotation degrees) and distance (i.e., Euclidean) to the nearest border using visual inputs of views from unvisited locations and/or directions. We formulate spatial decoding as a prediction problem where we fit a set of linear regression models on representations of visual views to predict location, direction and distance to nearest borders. We define the nearest border to the agent as the border that has the shortest Euclidean distance to the agent’s location regardless of the agent’s heading direction.</p>
</sec>
<sec id="s4e">
<title>Decoder training and testing</title>
<p>To fit and test spatial decoders, we randomly sample locations in the environment (subject to a sampling rate). When a location is sampled for training, we consider views of different rotations as independent data-points. For example, a location decoder is fit to predict a vector <bold>t<sub>i</sub></bold> = [t<italic><sub>x</sub></italic>, t<italic><sub>y</sub></italic>, t<italic><sub>r</sub></italic>, t<italic><sub>b</sub></italic>] where t<italic><sub>x</sub></italic>, t<italic><sub>y</sub></italic> are the x, y coordinates, t<italic><sub>r</sub></italic> is the direction, t<italic><sub>b</sub></italic> is the shortest distance to a border, given a view respectively. For the entire training set (<bold>X<sub>i</sub></bold>, <bold>T<sub>i</sub></bold>) where <bold>T<sub>i</sub></bold> = {<bold>t<sub>1</sub></bold>, <bold>t<sub>2</sub></bold>, …, <bold>t<sub>i</sub></bold>}, we can write the decoder as
<disp-formula id="eqn1">
<graphic xlink:href="575026v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold>W<sub>i</sub></bold> are the coefficients and <bold>b<sub>i</sub></bold> are the intercepts. Decoder weights (both coefficients and intercepts) are optimized to minimize the mean squared error between the true and predicted targets.</p>
</sec>
<sec id="s4f">
<title>Feature representation and selection</title>
<p>We consider a number of representations for visual inputs. Given a DNN model, we train separate decoders using representations across different layers. We apply L<sub>2</sub> regularization on the decoder weights to reduce overfitting. Additionally, we consider different exclusion strategies (see <xref rid="s4m" ref-type="sec">Sec. 4.4</xref>) where unit representations are selectively removed based on their firing profiles (e.g., the number of place fields a unit contains).</p>
</sec>
<sec id="s4g">
<title>Performance measure</title>
<p>We report the normalized decoding error on the heldout set of location, head-direction and nearest border decodings for each representation at a given sampling rate. The normalized decoding error reflects the deviation of the model’s prediction on location, heading direction or distance to the nearest wall relative to the ground truth (i.e., in terms of physical unit of space or angle in the virtual environment) and normalized with respect to the maximal length of the moving area (2 units in both depth and width). To reflect variance of the average error, we compute a two-sided 95% confidence interval of the average using a bootstrapping approach over locations.</p>
</sec>
<sec id="s4h">
<title>Baselines</title>
<p>We compare decoding performance achieved by our agent to baselines where the agent decodes location, head-direction and distance to border based on fixed policies independent to visual inputs. Specifically, we establish two baselines where the agent simply either predict randomly within the legal bound or predict the centre position of the room (when predicting location), predict 90 degrees (when predicting rotation) and predict the distance from the centre of the room (when predicting nearest border distance).</p>
</sec>
<sec id="s4i">
<label>4.3</label><title>Profiling spatial properties of model units</title>
<p>To gain a more intuitive understanding of the kinds of model units that are most useful for spatial decoding tasks, we profile all model units from the layers we consider based on their spatial patterns of firing. We consider the most common cell types that are found to encode spatial information as defined in the neuroscience literature, which include place cells, border cells and head-direction cells. For each cell type, we track a number of measures defined below.</p>
</sec>
<sec id="s4j">
<title>Place cells</title>
<p>In our investigation, the identification of place-cell-like model units hinges on several key characteristics of unit firing. Firstly, we examine the number of place fields exhibited by each unit, defining a qualified field as one that spans 2D space ranging from 10 pixels to half the size of the environment, as outlined in <xref ref-type="bibr" rid="c55">Tanni et al. (2022)</xref>. Additionally, we consider the maximal activation of each field, providing insights into the intensity and significance of unit firing.</p>
</sec>
<sec id="s4k">
<title>Head-direction cells</title>
<p>Following <xref ref-type="bibr" rid="c2">Banino et al. (2018)</xref>, the degree of directional tuning exhibited by each unit was assessed using the length of the resultant vector of the directional activity map. In our case, there is an activity map spanning entire 2D space for each direction. Vectors corresponding to each direction of an activity map were created:
<disp-formula id="eqn2">
<graphic xlink:href="575026v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where α and β are, respectively, the angle and average intensity (irrespective of spatial locations) of direction i in the activity map. These vectors were averaged to generate a mean resultant vector:
<disp-formula id="eqn3">
<graphic xlink:href="575026v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and the length of the resultant vector calculated as the magnitude of r⃗. We used 24 angular directions uniformly spaced.</p>
</sec>
<sec id="s4l">
<title>Border cells</title>
<p>Border score definition is based on <xref ref-type="bibr" rid="c2">Banino et al. (2018)</xref> where for each of the four walls in the square enclosure, the average activation for that wall, b<italic><sub>i</sub></italic>, was compared to the average centre activity c obtaining a border score for that wall, and the maximum was used as the border-score for the unit:
<disp-formula id="eqn4">
<graphic xlink:href="575026v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where b<italic><sub>i</sub></italic> is the mean activation for bins within d<italic><sub>b</sub></italic> distance from the i-th wall and c the average activity for bins further than d<italic><sub>b</sub></italic> bins from any wall (d<italic><sub>b</sub></italic> = 3). Units with border score &gt; 0.5 are considered border-like.</p>
</sec>
<sec id="s4m">
<label>4.4</label><title>Spatial decoding with unit exclusion</title>
<p>To delve deeper into how different units within the DNN models contribute to the linear decoding of spatial information, we employ two complementary analyses. Model units are selectively excluded subject to criteria detailed below. Spatial decoders are re-trained on the rest of the model units following the same procedure as before (<xref rid="s4d" ref-type="sec">sec. 4.2</xref>).</p>
<p><bold>Excluding units by spatial profile</bold> In this first analysis, we selectively exclude units based on their firing profiles, which fall into the various spatial unit categories mentioned earlier. We initially rank all units in descending order according to their specific spatial measures (e.g., field count) and then systematically increase the exclusion rate, assessing the impact on the corresponding downstream tasks as we go. For example, we progressively exclude the top n% of units with the most place-like characteristics for varying values of n, examining the relationship between exclusion and performance of location decoding. This process is carried out separately for each unit type and their associated tasks we have defined, and we also include a control group where an equivalent number of units are excluded at random.</p>
<p><bold>Excluding units by task-relevance</bold> Our second exclusion analysis focuses on identifying the task-relevance of each model unit by utilizing previously learned decoders and their coefficient magnitudes. Essentially, units with larger coefficients are deemed more crucial for decoding, while those with smaller or zero coefficients are considered less relevant. Similar to the exclusion by spatial profile, we gradually increase the exclusion rate by selectively removing units based on the magnitude of their decoder coefficients. Additionally, we conduct a control analysis where an equal number of units are randomly chosen for exclusion. We then retrain linear decoders with the remaining units and assess their performance on downstream spatial tasks.</p>
</sec>
<sec id="s5" sec-type="data-availability">
<title>Data and code availability</title>
<p>The code for simulations and analyses are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/dontpanic/Space">https://github.com/dontpanic/Space</ext-link>.</p>
</sec>
</sec>
</body>
<back>
<sec id="s7">
<title><bold>A</bold> Spatial decoding performance across various percetion models</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1:</label>
<caption><title>VGG-16 (untrained).</title>
<p>Linear decoders trained with representations from various layers of the untrained VGG-16 achieve low errors across sampling rates; though not as good as its trained version. Mid-to-advanced layers show superior performance than early layers. As more locations are sampled for training the linear decoders, overall decoding performance improves. All model layers can decode better than two visual-invariant baselines.</p></caption>
<graphic xlink:href="575026v2_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2:</label>
<caption><title>ResNet-50 (trained).</title>
<p>Linear decoders trained with representations from various layers of the ResNet-50 pretrained on object recognition achieve low errors across sampling rates. Mid-to-advanced layers show superior performance than early layers. The penultimate layer decoding performance did not improve as much as the intermediate layers as more locations are sampled for training the linear decoders. All model layers can decode better than two visual-invariant baselines.</p></caption>
<graphic xlink:href="575026v2_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3:</label>
<caption><title>ResNet-50 (untrained).</title>
<p>Similar to ResNet-50 pretrained on images, the untrained counterpart can effectively decode spatial knowledge related to location, heading direction and distance to borders. All layers decoder better than the baseline decoders which do not rely on visual signals of the environment.</p></caption>
<graphic xlink:href="575026v2_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4:</label>
<caption><title>ViT-B/16 (trained).</title>
<p>Linear decoders trained on different layers of the pretrained ViT model show very similar decoding performance. Overall the decoding performance is much better than the two baseline decoders which do not incorporate visual signals.</p></caption>
<graphic xlink:href="575026v2_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5:</label>
<caption><title>ViT-B/16 (untrained).</title>
<p>Linear decoders trained on the untrained ViT model also achieve accurate decoding performance on location, heading direction and distance to the nearest border. Similar to the trained version, all layers considered in our analysis achieve comparable performance and outperform the baselines.</p></caption>
<graphic xlink:href="575026v2_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s8">
<title><bold>B</bold> Distribution of cell types of spatial units across models and layers</title>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S6:</label>
<caption><title>Distribution of different spatial unit types across layers of perceptual models of object recognition.</title></caption>
<graphic xlink:href="575026v2_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s9">
<title><bold>C</bold> Model units with spatial firing profiles</title>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Figure S7:</label>
<caption><title>Examples of model units exhibiting spatial characteristics.</title></caption>
<graphic xlink:href="575026v2_figs7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s10">
<title><bold>D</bold> Spatial decoding performance under unit exclusion across various perception models</title>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Figure S8:</label>
<caption><title>Excluding units with the strongest spatial profiles had minimal impact on spatial knowledge (ResNet-50).</title></caption>
<graphic xlink:href="575026v2_figs8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Figure S9:</label>
<caption><title>Excluding units by task-relevance affects spatial decoding performance (ResNet-50).</title></caption>
<graphic xlink:href="575026v2_figs9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs10" position="float" orientation="portrait" fig-type="figure">
<label>Figure S10:</label>
<caption><title>Excluding units with the strongest spatial profiles had minimal impact on spatial knowledge (ViT-B/16).</title></caption>
<graphic xlink:href="575026v2_figs10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs11" position="float" orientation="portrait" fig-type="figure">
<label>Figure S11:</label>
<caption><title>Excluding units by task-relevance affects spatial decoding performance (ViT-B/16).</title></caption>
<graphic xlink:href="575026v2_figs11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s11">
<title><bold>E</bold> Relationship between model units’ spatial properties and corresponding decoder coefficient strengths</title>
<fig id="figs12" position="float" orientation="portrait" fig-type="figure">
<label>Figure S12:</label>
<caption><title>VGG-16 (pre-trained), late layer (block5 pool) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs13" position="float" orientation="portrait" fig-type="figure">
<label>Figure S13:</label>
<caption><title>VGG-16 (pre-trained), mid layer (block4 pool) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs13.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs14" position="float" orientation="portrait" fig-type="figure">
<label>Figure S14:</label>
<caption><title>VGG-16 (pre-trained), early layer (block2 pool) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs14.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs15" position="float" orientation="portrait" fig-type="figure">
<label>Figure S15:</label>
<caption><title>VGG-16 (untrained), late layer (block5 pool) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs15.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs16" position="float" orientation="portrait" fig-type="figure">
<label>Figure S16:</label>
<caption><title>VGG-16 (untrained), mid layer (block4 pool) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs16.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs17" position="float" orientation="portrait" fig-type="figure">
<label>Figure S17:</label>
<caption><title>VGG-16 (untrained), early layer (block2 pool) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs17.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs18" position="float" orientation="portrait" fig-type="figure">
<label>Figure S18:</label>
<caption><title>ResNet-50 (pre-trained), penultimate layer (avg pool) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs18.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs19" position="float" orientation="portrait" fig-type="figure">
<label>Figure S19:</label>
<caption><title>ResNet-50 (pre-trained), late layer (conv5 block2) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs19.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs20" position="float" orientation="portrait" fig-type="figure">
<label>Figure S20:</label>
<caption><title>ResNet-50 (pre-trained), mid layer (conv4 block6) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs20.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs21" position="float" orientation="portrait" fig-type="figure">
<label>Figure S21:</label>
<caption><title>ResNet-50 (pre-trained), early layer (conv2 block3) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs21.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs22" position="float" orientation="portrait" fig-type="figure">
<label>Figure S22:</label>
<caption><title>ResNet-50 (untrained), penultimate layer (avg pool) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs22.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs23" position="float" orientation="portrait" fig-type="figure">
<label>Figure S23:</label>
<caption><title>ResNet-50 (untrained), late layer (conv5 block2) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs23.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs24" position="float" orientation="portrait" fig-type="figure">
<label>Figure S24:</label>
<caption><title>ResNet-50 (untrained), mid layer (conv4 block6) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs24.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs25" position="float" orientation="portrait" fig-type="figure">
<label>Figure S25:</label>
<caption><title>ResNet-50 (untrained), early layer (conv2 block3) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs25.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs26" position="float" orientation="portrait" fig-type="figure">
<label>Figure S26:</label>
<caption><title>ViT-B/16 (pre-trained), penultimate layer (layer 12) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs26.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs27" position="float" orientation="portrait" fig-type="figure">
<label>Figure S27:</label>
<caption><title>ViT-B/16 (pre-trained), late layer (layer 9) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs27.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs28" position="float" orientation="portrait" fig-type="figure">
<label>Figure S28:</label>
<caption><title>ViT-B/16 (pre-trained), mid layer (layer 6) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs28.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs29" position="float" orientation="portrait" fig-type="figure">
<label>Figure S29:</label>
<caption><title>ViT-B/16 (pre-trained), early layer (layer 3) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs29.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs30" position="float" orientation="portrait" fig-type="figure">
<label>Figure S30:</label>
<caption><title>ViT-B/16 (untrained), penultimate layer (layer 12) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs30.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs31" position="float" orientation="portrait" fig-type="figure">
<label>Figure S31:</label>
<caption><title>ViT-B/16 (untrained), late layer (layer 9) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs31.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs32" position="float" orientation="portrait" fig-type="figure">
<label>Figure S32:</label>
<caption><title>ViT-B/16 (untrained), mid layer (layer 6) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs32.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs33" position="float" orientation="portrait" fig-type="figure">
<label>Figure S33:</label>
<caption><title>ViT-B/16 (untrained), early layer (layer 3) units, show no apparent relationship between spatial properties and their decoder weight strengths.</title></caption>
<graphic xlink:href="575026v2_figs33.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by ESRC (ES/W007347/1), Wellcome Trust (WT106931MA), and a Royal Society Wolfson Fellowship (18302) to B.C.L., and the Medical Research Council UK (MC UU 00030/7) and a Leverhulme Trust Early Career Fellowship (Leverhulme Trust, Isaac Newton Trust: SUAI/053 G100773, SUAI/056 G105620, ECF-2019-110) to R.M.M.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>3DEverything</collab></person-group> (<year>2022</year>) <article-title>Hospital Laboratory | 3D Interior | Unity Asset Store</article-title>. URL <ext-link ext-link-type="uri" xlink:href="https://assetstore.unity.com/packages/3d/propsinterior/hospital-laboratory-54382">https://assetstore.unity.com/packages/3d/propsinterior/hospital-laboratory-54382</ext-link></mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Banino</surname> <given-names>A</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>C</given-names></string-name>, <string-name><surname>Uria</surname> <given-names>B</given-names></string-name>, <string-name><surname>Blundell</surname> <given-names>C</given-names></string-name>, <string-name><surname>Lillicrap</surname> <given-names>T</given-names></string-name>, <string-name><surname>Mirowski</surname> <given-names>P</given-names></string-name>, <string-name><surname>Pritzel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chadwick</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Degris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Modayil</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wayne</surname> <given-names>G</given-names></string-name>, <string-name><surname>Soyer</surname> <given-names>H</given-names></string-name>, <string-name><surname>Viola</surname> <given-names>F</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>B</given-names></string-name>, <string-name><surname>Goroshin</surname> <given-names>R</given-names></string-name>, <string-name><surname>Rabinowitz</surname> <given-names>N</given-names></string-name>, <string-name><surname>Pascanu</surname> <given-names>R</given-names></string-name>, <string-name><surname>Beattie</surname> <given-names>C</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sadik</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gaffney</surname> <given-names>S</given-names></string-name>, <string-name><surname>King</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kavukcuoglu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hassabis</surname> <given-names>D</given-names></string-name>, <string-name><surname>Hadsell</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kumaran</surname> <given-names>D</given-names></string-name></person-group> (<year>2018</year>) <article-title>Vector-based navigation using grid-like representations in artificial agents</article-title>. <source>Nature</source> <volume>557</volume>(<issue>7705</issue>):<fpage>429</fpage>–<lpage>433</lpage>, <pub-id pub-id-type="doi">10.1038/s41586-018-0102-6</pub-id></mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barry</surname> <given-names>DN</given-names></string-name>, <string-name><surname>Love</surname> <given-names>BC</given-names></string-name></person-group> (<year>2022</year>) <article-title>A neural network account of memory replay and knowledge consolidation</article-title>. <source>Cerebral Cortex</source> <volume>33</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>95</lpage>, <pub-id pub-id-type="doi">10.1093/cercor/bhac054</pub-id></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Cueva</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Wei</surname> <given-names>XX</given-names></string-name></person-group> (<year>2018</year>) <article-title>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization</article-title>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1803.07770">http://arxiv.org/abs/1803.07770</ext-link></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diehl</surname> <given-names>GW</given-names></string-name>, <string-name><surname>Hon</surname> <given-names>OJ</given-names></string-name>, <string-name><surname>Leutgeb</surname> <given-names>S</given-names></string-name>, <string-name><surname>Leutgeb</surname> <given-names>JK</given-names></string-name></person-group> (<year>2017</year>) <article-title>Grid and Nongrid Cells in Medial Entorhinal Cortex Represent Spatial Location and Environmental Features with Complementary Coding Schemes</article-title>. <source>Neuron</source> <volume>94</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>92.e6,</lpage>, <pub-id pub-id-type="doi">10.1016/j.neuron.2017.03.004</pub-id></mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dombeck</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Harvey</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Tian</surname> <given-names>L</given-names></string-name>, <string-name><surname>Looger</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Tank</surname> <given-names>DW</given-names></string-name></person-group> (<year>2010</year>) <article-title>Functional imaging of hippocampal place cells at cellular resolution during virtual navigation</article-title>. <source>Nature Neuroscience</source> <volume>13</volume>(<issue>11</issue>):<fpage>1433</fpage>–<lpage>1440</lpage>, <pub-id pub-id-type="doi">10.1038/nn.2648</pub-id></mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Dosovitskiy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Beyer</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kolesnikov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Weissenborn</surname> <given-names>D</given-names></string-name>, <string-name><surname>Zhai</surname> <given-names>X</given-names></string-name>, <string-name><surname>Unterthiner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Dehghani</surname> <given-names>M</given-names></string-name>, <string-name><surname>Minderer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Heigold</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gelly</surname> <given-names>S</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Houlsby</surname> <given-names>N</given-names></string-name></person-group> (<year>2020</year>) <article-title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</article-title>. <source>arXiv</source>, <pub-id pub-id-type="doi">10.48550/arxiv.2010.11929</pub-id></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dupret</surname> <given-names>D</given-names></string-name>, <string-name><surname>O’Neill</surname> <given-names>J</given-names></string-name>, <string-name><surname>Pleydell-Bouverie</surname> <given-names>B</given-names></string-name>, <string-name><surname>Csicsvari</surname> <given-names>J</given-names></string-name></person-group> (<year>2010</year>) <article-title>The reorganization and reactivation of hippocampal maps predict spatial memory performance</article-title>. <source>Nature Neuroscience</source> <volume>13</volume>(<issue>8</issue>):<fpage>995</fpage>–<lpage>1002</lpage>, DOI <pub-id pub-id-type="doi">10.1038/nn.2599</pub-id></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ebitz</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name></person-group> (<year>2021</year>) <article-title>The population doctrine in cognitive neuroscience</article-title>. <source>Neuron</source> <volume>109</volume>(<issue>19</issue>):<fpage>3055</fpage>–<lpage>3068</lpage>, <pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.011</pub-id></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eichenbaum</surname> <given-names>H</given-names></string-name></person-group> (<year>2015</year>) <article-title>Perspectives on 2014 Nobel Prize</article-title>. <source>Hippocampus</source> <volume>25</volume>:<fpage>679</fpage>–<lpage>681</lpage>, <pub-id pub-id-type="doi">10.1002/hipo.22445</pub-id></mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eichenbaum</surname> <given-names>H</given-names></string-name></person-group> (<year>2018</year>) <article-title>Barlow versus Hebb: When is it time to abandon the notion of feature detectors and adopt the cell assembly as the unit of cognition?</article-title> <source>Neuroscience Letters</source> <volume>680</volume>:<fpage>88</fpage>–<lpage>93</lpage>, DOI <pub-id pub-id-type="doi">10.1016/j.neulet.2017.04.006</pub-id></mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eickenberg</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Varoquaux</surname> <given-names>G</given-names></string-name>, <string-name><surname>Thirion</surname> <given-names>B</given-names></string-name></person-group> (<year>2017</year>) <article-title>Seeing it all: Convolutional network layers map the function of the human visual system</article-title>. <source>NeuroImage</source> <volume>152</volume>:<fpage>184</fpage>–<lpage>194</lpage>, <pub-id pub-id-type="doi">10.1016/J.NEUROIMAGE.2016.10.001</pub-id></mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franzius</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sprekeler</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wiskott</surname> <given-names>L</given-names></string-name></person-group> (<year>2007a</year>) <article-title>Slowness and Sparseness Lead to Place, Head-Direction, and Spatial-View Cells</article-title>. <source>PLoS Computational Biology</source> <volume>3</volume>(<issue>8</issue>):<fpage>e166</fpage>,  <pub-id pub-id-type="doi">10.1371/journal.pcbi.0030166</pub-id></mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franzius</surname> <given-names>M</given-names></string-name>, <string-name><surname>Vollgraf</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wiskott</surname> <given-names>L</given-names></string-name></person-group> (<year>2007b</year>) <article-title>From grids to places</article-title>. <source>Journal of Computational Neuroscience</source> <volume>22</volume>(<issue>3</issue>):<fpage>297</fpage>–<lpage>299</lpage>, DOI <pub-id pub-id-type="doi">10.1007/s10827-006-0013-7</pub-id></mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fukushima</surname> <given-names>K</given-names></string-name></person-group> (<year>1980</year>) <article-title>Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title>. <source>Biological Cybernetics</source> <volume>36</volume>(<issue>4</issue>):<fpage>193</fpage>–<lpage>202</lpage>, DOI <pub-id pub-id-type="doi">10.1007/BF00344251</pub-id></mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grieves</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Jeffery</surname> <given-names>KJ</given-names></string-name></person-group> (<year>2017</year>) <article-title>The representation of space in the brain</article-title>. <source>Behavioural Processes</source> <volume>135</volume>:<fpage>113</fpage>–<lpage>131</lpage>, DOI <pub-id pub-id-type="doi">10.1016/j.beproc.2016.12.012</pub-id></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grieves</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Jedidi-Ayoub</surname> <given-names>S</given-names></string-name>, <string-name><surname>Mishchanchuk</surname> <given-names>K</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>A</given-names></string-name>, <string-name><surname>Renaudineau</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jeffery</surname> <given-names>KJ</given-names></string-name></person-group> (<year>2020</year>) <article-title>The place-cell representation of volumetric space in rats</article-title>. <source>Nature Communications</source> <volume>11</volume>(<issue>1</issue>):<fpage>789</fpage>, DOI <pub-id pub-id-type="doi">10.1038/s41467-020-14611-7</pub-id></mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grijseels</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Shaw</surname> <given-names>K</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hall</surname> <given-names>CN</given-names></string-name></person-group> (<year>2021</year>) <article-title>Choice of method of place cell classification determines the population of cells identified</article-title>. <source>PLOS Computational Biology</source> <volume>17</volume>(<issue>7</issue>):<fpage>e1008835</fpage>, DOI <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008835</pub-id></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Güclü</surname> <given-names>U</given-names></string-name>, <string-name><surname>van Gerven</surname> <given-names>MAJ</given-names></string-name></person-group> (<year>2015</year>) <article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>(<issue>27</issue>):<fpage>10005</fpage>–<lpage>10014</lpage>, DOI <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id></mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hafting</surname> <given-names>T</given-names></string-name>, <string-name><surname>Fyhn</surname> <given-names>M</given-names></string-name>, <string-name><surname>Molden</surname> <given-names>S</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name></person-group> (<year>2005</year>) <article-title>Microstructure of a spatial map in the entorhinal cortex</article-title>. <source>Nature</source> <volume>436</volume>(<issue>7052</issue>):<fpage>801</fpage>–<lpage>806</lpage>, <pub-id pub-id-type="doi">10.1038/nature03721</pub-id></mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hales</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schlesiger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Leutgeb</surname> <given-names>J</given-names></string-name>, <string-name><surname>Squire</surname> <given-names>L</given-names></string-name>, <string-name><surname>Leutgeb</surname> <given-names>S</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>R</given-names></string-name></person-group> (<year>2014</year>) <article-title>Medial Entorhinal Cortex Lesions Only Partially Disrupt Hippocampal Place Cells and Hippocampus-Dependent Place Memory</article-title>. <source>Cell Reports</source> <volume>9</volume>(<issue>3</issue>):<fpage>893</fpage>–<lpage>901</lpage>, <pub-id pub-id-type="doi">10.1016/j.celrep.2014.10.009</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hartley</surname> <given-names>T</given-names></string-name>, <string-name><surname>Lever</surname> <given-names>C</given-names></string-name>, <string-name><surname>Burgess</surname> <given-names>N</given-names></string-name>, <string-name><surname>O’Keefe</surname> <given-names>J</given-names></string-name></person-group> (<year>2014</year>) <article-title>Space in the brain: how the hippocampal formation supports spatial cognition</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>369</volume>(<issue>1635</issue>):<fpage>20120510</fpage>, <pub-id pub-id-type="doi">10.1098/rstb.2012.0510</pub-id></mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J</given-names></string-name></person-group> (<year>2016</year>) <article-title>Deep residual learning for image recognition</article-title>, <conf-name>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name>, <fpage>770</fpage>–<lpage>778</lpage>, DOI <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hebb</surname> <given-names>D</given-names></string-name></person-group> (<year>1949</year>) <source>The Organizationof Behavior: A Neuropsychological Theory, 2002nd edn</source>. <publisher-name>Psychology Press</publisher-name>, <publisher-loc>New York</publisher-loc></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hollup</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Molden</surname> <given-names>S</given-names></string-name>, <string-name><surname>Donnett</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name></person-group> (<year>2001</year>) <article-title>Accumulation of Hippocampal Place Fields at the Goal Location in an Annular Watermaze Task</article-title>. <source>The Journal of Neuroscience</source> <volume>21</volume>(<issue>5</issue>):<fpage>1635</fpage>–<lpage>1644</lpage>, <pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-05-01635.2001</pub-id>, </mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hubel</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Wiesel</surname> <given-names>TN</given-names></string-name></person-group> (<year>1959</year>) <article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title>. <source>The Journal of Physiology</source> <volume>148</volume>(<issue>3</issue>):<fpage>574</fpage>–<lpage>591</lpage>, <pub-id pub-id-type="doi">10.1113/JPHYSIOL.1959.SP006308</pub-id></mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Høydal</surname> <given-names>ØA</given-names></string-name>, <string-name><surname>Skytøen</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Andersson</surname> <given-names>SO</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name></person-group> (<year>2019</year>) <article-title>Objectvector coding in the medial entorhinal cortex</article-title>. <source>Nature</source> <volume>568</volume>(<issue>7752</issue>):<fpage>400</fpage>– <lpage>404</lpage>, <pub-id pub-id-type="doi">10.1038/s41586-019-1077-7</pub-id></mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jaeger</surname> <given-names>H</given-names></string-name>, <string-name><surname>Haas</surname> <given-names>H</given-names></string-name></person-group> (<year>2004</year>) <article-title>Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication</article-title>. <source>Science</source> <volume>304</volume>(<issue>5667</issue>):<fpage>78</fpage>– <lpage>80</lpage>, <pub-id pub-id-type="doi">10.1126/science.1091277</pub-id></mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name></person-group> (<year>2014</year>) <article-title>Deep Supervised, but Not Unsupervised Models May Explain IT Cortical Representation</article-title>, <source>PLoS Computational Biology</source> <volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>, <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id></mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>GE</given-names></string-name></person-group> (<year>2012</year>) <source>ImageNet classification with deep convolutional neural networks</source>. <publisher-loc>In</publisher-loc>: <publisher-name>Advances in Neural Information Processing Systems</publisher-name>, vol 2, pp 1097–1105</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kropff</surname> <given-names>E</given-names></string-name>, <string-name><surname>Treves</surname> <given-names>A</given-names></string-name></person-group> (<year>2008</year>) <article-title>The emergence of grid cells: Intelligent design or just adaptation?</article-title> <source>Hippocampus</source> <volume>18</volume>(<issue>12</issue>):<fpage>1256</fpage>–<lpage>1269</lpage>, <pub-id pub-id-type="doi">10.1002/hipo.20520</pub-id></mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Latuske</surname> <given-names>P</given-names></string-name>, <string-name><surname>Toader</surname> <given-names>O</given-names></string-name>, <string-name><surname>Allen</surname> <given-names>K</given-names></string-name></person-group> (<year>2015</year>) <article-title>Interspike Intervals Reveal Functionally Distinct Cell Populations in the Medial Entorhinal Cortex</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>(<issue>31</issue>):<fpage>10963</fpage>–<lpage>10976</lpage>, <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0276-15.2015</pub-id></mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lever</surname> <given-names>C</given-names></string-name>, <string-name><surname>Burton</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jeewajee</surname> <given-names>A</given-names></string-name>, <string-name><surname>O’Keefe</surname> <given-names>J</given-names></string-name>, <string-name><surname>Burgess</surname> <given-names>N</given-names></string-name></person-group> (<year>2009</year>) <article-title>Boundary Vector Cells in the Subiculum of the Hippocampal Formation</article-title>. <source>The Journal of Neuroscience</source> <volume>29</volume>(<issue>31</issue>):<fpage>9771</fpage>–<lpage>9777</lpage>, <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1319-09.2009</pub-id></mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Long</surname> <given-names>X</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>SJ</given-names></string-name></person-group> (<year>2021</year>) <article-title>A novel somatosensory spatial navigation system outside the hippocampal formation</article-title>. <source>Cell Research</source> <volume>31</volume>(<issue>6</issue>):<fpage>649</fpage>–<lpage>663</lpage>, <pub-id pub-id-type="doi">10.1038/s41422-020-00448-8</pub-id></mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Long</surname> <given-names>X</given-names></string-name>, <string-name><surname>Deng</surname> <given-names>B</given-names></string-name>, <string-name><surname>Cai</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>ZS</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>SJ</given-names></string-name></person-group> (<year>2021</year>) <article-title>A compact spatial map in V2 visual cortex. preprint</article-title>, <source>Neuroscience</source>, <pub-id pub-id-type="doi">10.1101/2021.02.11.430687</pub-id></mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McKenzie</surname> <given-names>S</given-names></string-name>, <string-name><surname>Robinson</surname> <given-names>NTM</given-names></string-name>, <string-name><surname>Herrera</surname> <given-names>L</given-names></string-name>, <string-name><surname>Churchill</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Eichenbaum</surname> <given-names>H</given-names></string-name></person-group> (<year>2013</year>) <article-title>Learning Causes Reorganization of Neuronal Firing Patterns to Represent Related Experiences within a Hippocampal Schema</article-title>. <source>Journal of Neuroscience</source> <volume>33</volume>(<issue>25</issue>):<fpage>10243</fpage>–<lpage>10256</lpage>, <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0879-13.2013</pub-id></mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McMahon</surname> <given-names>E</given-names></string-name>, <string-name><surname>Isik</surname> <given-names>L</given-names></string-name></person-group> (<year>2023</year>) <article-title>Seeing social interactions</article-title>. <source>Trends in Cognitive Sciences</source> <volume>27</volume>(<issue>12</issue>):<fpage>1165</fpage>–<lpage>1179</lpage>, DOI <pub-id pub-id-type="doi">10.1016/j.tics.2023.09.001</pub-id></mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McNaughton</surname> <given-names>L</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Markus</surname> <given-names>J</given-names></string-name></person-group> (<year>1991</year>) “<article-title>DeadReckoning,”kmdmark Learning, and the Sense of Direction: A Neurophysiological and Computational Hypothesis</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>3</volume>:<fpage>190</fpage>–<lpage>202</lpage></mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mok</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Love</surname> <given-names>BC</given-names></string-name></person-group> (<year>2019</year>) <article-title>A non-spatial account of place and grid cells based on clustering models of concept learning</article-title>. <source>Nature Communications</source> <volume>10</volume>(<issue>1</issue>), <pub-id pub-id-type="doi">10.1038/S41467-019-13760-8</pub-id></mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mok</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Love</surname> <given-names>BC</given-names></string-name></person-group> (<year>2023</year>) <article-title>A multilevel account of hippocampal function in spatial and concept learning: Bridging models of behavior and neural assemblies</article-title>. <source>Science Advances</source> <volume>9</volume>(<issue>29</issue>), <pub-id pub-id-type="doi">10.1126/sciadv.ade6903</pub-id></mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moser</surname> <given-names>EI</given-names></string-name>, <string-name><surname>Kropff</surname> <given-names>E</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name></person-group> (<year>2008</year>) <article-title>Place Cells, Grid Cells, and the Brain’s Spatial Representation System</article-title>. <source>Annual Review of Neuroscience</source> <volume>31</volume>(<issue>1</issue>):<fpage>69</fpage>– <lpage>89</lpage>, <pub-id pub-id-type="doi">10.1146/annurev.neuro.31.061307.090723</pub-id></mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nayebi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Attinger</surname> <given-names>A</given-names></string-name>, <string-name><surname>Campbell</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Hardcastle</surname> <given-names>K</given-names></string-name>, <string-name><surname>Low</surname> <given-names>II</given-names></string-name>, <string-name><surname>Mallory</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Mel</surname> <given-names>GC</given-names></string-name>, <string-name><surname>Sorscher</surname> <given-names>B</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>AH</given-names></string-name>, <string-name><surname>Ganguli</surname> <given-names>S</given-names></string-name>, <string-name><surname>Giocomo</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Yamins</surname> <given-names>DL</given-names></string-name></person-group> (<year>2021</year>) <article-title>Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks. In: Advances in Neural Information Processing Systems</article-title>, <source>Neuroscience</source>,  <pub-id pub-id-type="doi">10.1101/2021.10.30.466617</pub-id></mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Keefe</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dostrovsky</surname> <given-names>J</given-names></string-name></person-group> (<year>1971</year>) <article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title>. <source>Brain Research</source> <volume>34</volume>(<issue>1</issue>):<fpage>171</fpage>–<lpage>175</lpage>, <pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Olshausen</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Field</surname> <given-names>DJ</given-names></string-name></person-group> (<year>2006</year>) In: <person-group person-group-type="editor"><string-name><surname>Van Hemmen</surname>, <given-names>JL</given-names></string-name>, <string-name><surname>Sejnowski</surname>, <given-names>TJ</given-names></string-name></person-group>, <chapter-title>What Is the Other 85 Percent of V1 Doing?</chapter-title>, <source>23 Problems in Systems Neuroscience</source>, <publisher-name>Oxford University Press</publisher-name>, <fpage>182</fpage>-<lpage>212</lpage> <pub-id pub-id-type="doi">10.1093/acprof:oso/9780195148220.003.0010</pub-id></mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ormond</surname> <given-names>J</given-names></string-name>, <string-name><surname>O’Keefe</surname> <given-names>J</given-names></string-name></person-group> (<year>2022</year>) <article-title>Hippocampal place cells have goaloriented vector fields during navigation</article-title>. <source>Nature</source> <volume>607</volume>(<issue>7920</issue>):<fpage>741</fpage>–<lpage>746</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41586-022-04913-9</pub-id></mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quiroga</surname> <given-names>RQ</given-names></string-name>, <string-name><surname>Reddy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kreiman</surname> <given-names>G</given-names></string-name>, <string-name><surname>Koch</surname> <given-names>C</given-names></string-name>, <string-name><surname>Fried</surname> <given-names>I</given-names></string-name></person-group> (<year>2005</year>) <article-title>Invariant visual representation by single neurons in the human brain</article-title>. <source>Nature</source> <volume>435</volume>(<issue>7045</issue>):<fpage>1102</fpage>– <lpage>1107</lpage>, DOI <pub-id pub-id-type="doi">10.1038/nature03687</pub-id></mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saleem</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Diamanti</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Fournier</surname> <given-names>J</given-names></string-name>, <string-name><surname>Harris</surname> <given-names>KD</given-names></string-name>, <string-name><surname>Carandini</surname> <given-names>M</given-names></string-name></person-group> (<year>2018</year>) <article-title>Coherent encoding of subjective spatial position in visual cortex and hippocampus</article-title>. <source>Nature</source> <volume>562</volume>:<fpage>124</fpage>–<lpage>127</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41586-018-0516-1</pub-id></mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sarel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Finkelstein</surname> <given-names>A</given-names></string-name>, <string-name><surname>Las</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ulanovsky</surname> <given-names>N</given-names></string-name></person-group> (<year>2017</year>) <article-title>Vectorial representation of spatial goals in the hippocampus of bats</article-title>. <source>Science</source> <volume>355</volume>(<issue>6321</issue>):<fpage>176</fpage>– <lpage>180</lpage>, DOI <pub-id pub-id-type="doi">10.1126/science.aak9589</pub-id></mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schapiro</surname> <given-names>AC</given-names></string-name>, <string-name><surname>McDevitt</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>TT</given-names></string-name>, <string-name><surname>Mednick</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Norman</surname> <given-names>KA</given-names></string-name></person-group> (<year>2018</year>) <article-title>Human hippocampal replay during rest prioritizes weakly learned information and predicts memory performance</article-title>. <source>Nature Communications</source> <volume>9</volume>(<issue>1</issue>):<fpage>3920</fpage>, DOI <pub-id pub-id-type="doi">10.1038/s41467-018-06213-1</pub-id></mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Simonyan</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zisserman</surname> <given-names>A</given-names></string-name></person-group> (<year>2015</year>) <article-title>Very Deep Convolutional Networks for LargeScale Image Recognition</article-title>, <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.1409.1556</pub-id></mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Simonyan</surname> <given-names>K</given-names></string-name>, <string-name><surname>Vedaldi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zisserman</surname> <given-names>A</given-names></string-name></person-group> (<year>2014</year>) <article-title>Deep inside convolutional networks: Visualising image classification models and saliency maps.</article-title>, <conf-name>2nd International Conference on Learning Representations, ICLR 2014 Workshop Track Proceedings</conf-name> pp <fpage>1</fpage>–<lpage>8</lpage></mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stachenfeld</surname> <given-names>KL</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name></person-group>, (<year>2017</year>) <article-title>The hippocampus as a predictive map</article-title>. <source>Nature Neuroscience</source> <volume>20</volume>(<issue>11</issue>):<fpage>1643</fpage>–<lpage>1653</lpage>, DOI <pub-id pub-id-type="doi">10.1038/nn.4650</pub-id></mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanaka</surname> <given-names>KZ</given-names></string-name>, <string-name><surname>He</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tomar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Niisato</surname> <given-names>K</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>AJY</given-names></string-name>, <string-name><surname>McHugh</surname> <given-names>TJ</given-names></string-name></person-group> (<year>2018</year>) <article-title>The hippocampal engram maps experience but not place</article-title>. <source>Science</source> <volume>361</volume>(<issue>6400</issue>):<fpage>392</fpage>– <lpage>397</lpage>, DOI <pub-id pub-id-type="doi">10.1126/science.aat5397</pub-id></mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Ebbesen</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Sanguinetti-Scheck</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Preston-Ferrer</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gundlfinger</surname> <given-names>A</given-names></string-name>, <string-name><surname>Winterer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Beed</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ray</surname> <given-names>S</given-names></string-name>, <string-name><surname>Naumann</surname> <given-names>R</given-names></string-name>, <string-name><surname>Schmitz</surname> <given-names>D</given-names></string-name>, <string-name><surname>Brecht</surname> <given-names>M</given-names></string-name>, <string-name><surname>Burgalossi</surname> <given-names>A</given-names></string-name></person-group> (<year>2015</year>) <article-title>Anatomical Organization and Spatiotemporal Firing Patterns of Layer 3 Neurons in the Rat Medial Entorhinal Cortex</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>(<issue>36</issue>):<fpage>12346</fpage>–<lpage>12354</lpage>, DOI <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0696-15.2015</pub-id></mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanni</surname> <given-names>S</given-names></string-name>, <string-name><surname>De Cothi</surname> <given-names>W</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>C</given-names></string-name></person-group> (<year>2022</year>) <article-title>State transitions in the statistically stable place cell population correspond to rate of perceptual change</article-title>. <source>Current Biology</source> <volume>32</volume>(<issue>16</issue>):<fpage>3505</fpage>–<lpage>3514.e7,</lpage> DOI <pub-id pub-id-type="doi">10.1016/j.cub.2022.06.046</pub-id></mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taube</surname> <given-names>J</given-names></string-name>, <string-name><surname>Muller</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ranck</surname> <given-names>J</given-names></string-name></person-group> (<year>1990</year>) <article-title>Head-direction cells recorded from the postsubiculum in freely moving rats II. Effects of environmental manipulations.</article-title>, <source>The Journal of Neuroscience</source> <volume>10</volume>(<issue>2</issue>):<fpage>436</fpage>–<lpage>447</lpage>, <pub-id pub-id-type="doi">10.1523/JNEUROSCI.10-02-00436.1990</pub-id></mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsao</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sugar</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Knierim</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name></person-group> (<year>2018</year>) <article-title>Integrating time from experience in the lateral entorhinal cortex</article-title>. <source>Nature</source> <volume>561</volume>(<issue>7721</issue>):<fpage>57</fpage>–<lpage>62</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41586-018-0459-6</pub-id></mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Vaswani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname> <given-names>N</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gomez</surname> <given-names>AN</given-names></string-name>, <string-name><surname>Kaiser</surname> <given-names>L</given-names></string-name>, <string-name><surname>Polosukhin</surname> <given-names>I</given-names></string-name></person-group> (<year>2017</year>) <article-title>Attention is all you need</article-title>. <conf-name>Advances in Neural Information Processing Systems</conf-name>, pp <fpage>5999</fpage>–<lpage>6009</lpage></mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whishaw</surname> <given-names>IQ</given-names></string-name>, <string-name><surname>Tomie</surname> <given-names>JA</given-names></string-name></person-group> (<year>1997</year>) <article-title>Perseveration on place reversals in spatial swimming pool tasks: Further evidence for place learning in hippocampal rats</article-title>. <source>Hippocampus</source> <volume>7</volume>(<issue>4</issue>):<fpage>361</fpage>–<lpage>370</lpage>, DOI <pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1997)7:4&lt;361::AID-HIPO2&gt;3.0.CO;2-M</pub-id></mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whishaw</surname> <given-names>IQ</given-names></string-name>, <string-name><surname>McKenna</surname> <given-names>JE</given-names></string-name>, <string-name><surname>Maaswinkel</surname> <given-names>H</given-names></string-name></person-group> (<year>1997</year>) <article-title>Hippocampal lesions and path integration</article-title>. <source>Current Opinion in Neurobiology</source> <volume>7</volume>(<issue>2</issue>):<fpage>228</fpage>–<lpage>234</lpage>, <pub-id pub-id-type="doi">10.1016/S0959-4388(97)80011-6</pub-id></mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whittington</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Muller</surname> <given-names>TH</given-names></string-name>, <string-name><surname>Mark</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>G</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>C</given-names></string-name>, <string-name><surname>Burgess</surname> <given-names>N</given-names></string-name>, <string-name><surname>Behrens</surname> <given-names>TE</given-names></string-name></person-group> (<year>2020</year>) <article-title>The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation</article-title>. <source>Cell</source> <volume>183</volume>(<issue>5</issue>):<fpage>1249</fpage>–<lpage>1263</lpage>, DOI <pub-id pub-id-type="doi">10.1016/J.CELL.2020.10.024</pub-id></mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname> <given-names>MA</given-names></string-name>, <string-name><surname>McNaughton</surname> <given-names>BL</given-names></string-name></person-group> (<year>1994</year>) <article-title>Reactivation of Hippocampal Ensemble Memories During Sleep</article-title>. <source>Science</source> <volume>265</volume>(<issue>5172</issue>):<fpage>676</fpage>–<lpage>679</lpage>, DOI <pub-id pub-id-type="doi">10.1126/science.8036517</pub-id></mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname> <given-names>DLK</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>H</given-names></string-name>, <string-name><surname>Cadieu</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Solomon</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Seibert</surname> <given-names>D</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name></person-group> (<year>2014</year>) <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>8624</lpage>, DOI <pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeman</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Ritchie</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Bracci</surname> <given-names>S</given-names></string-name>, <string-name><given-names>Op</given-names> <surname>de Beeck H</surname></string-name></person-group> (<year>2020</year>) <article-title>Orthogonal Representations of Object Shape and Category in Deep Convolutional Neural Networks and Human Visual Cortex</article-title>. <source>Scientific Reports</source> <volume>2020</volume> <issue>10</issue>:<fpage>1</fpage>–<lpage>12</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41598-020-59175-0</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99047.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study demonstrates that some degree of spatial tuning (e.g., place cells) and ability to decode spatial location emerges in sufficiently complex systems trained to process visual information. This intriguing observation challenges existing approaches and findings used in the study of spatial navigation. However, the strength of evidence regarding the nature and quality of spatial tuning, its compatibility with experimental data, and the overall interpretation of the study remains <bold>incomplete</bold>. This work will be of interest to the research community of spatial navigation.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99047.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study investigated spatial representations in deep feedforward neural network models (DDNs) that were often used in solving vision tasks. The authors create a three-dimensional virtual environment, and let a simulated agent randomly forage in a smaller two-dimensional square area. The agent &quot;sees&quot; images of the room within its field of view from different locations and heading directions. These images were processed by DDNs. Analyzing model neurons in DDNs, they found response properties similar to those of place cells, border cells and head direction cells in various layers of deep nets. A linear readout of network activity can decode key spatial variables. In addition, after removing neurons with strong place/border/head direction selectivity, one can still decode these spatial variables from remaining neurons in the DNNs. Based on these results, the authors argue that that the notion of functional cell types in spatial cognition is misleading.</p>
<p>Comments on the revision:</p>
<p>In the revision, the authors proposed that their model should be interpreted as a null model, rather than the actual model of the spatial navigation system in the brain. In the revision, the authors also argued that the criterion used in the place cell literature was arbitrary. However, the strength of the present work still depends on how well the null model can explain the experimental findings. It seems that currently the null model failed to explain important aspects of the response properties of different functional cell types in the hippocampus.</p>
<p>Strengths:</p>
<p>This paper contains interesting and original ideas, and I enjoy reading it. Most previous studies (e.g., Banino, Nature, 2018; Cueva &amp; Wei, ICLR, 2018; Whittington et al, Cell, 2020) using deep network models to investigate spatial cognition mainly relied on velocity/head rotation inputs, rather than vision (but see Franzius, Sprekeler, Wiskott, PLoS Computational Biology, 2007). Here, the authors find that, under certain settings, visual inputs alone may contain enough information about the agent's location, head direction and distance to the boundary, and such information can be extracted by DNNs. This is an interesting observation from these models.</p>
<p>Weaknesses:</p>
<p>While the findings reported here are interesting, it is unclear whether they are the consequence of the specific model setting and how well they would generalize. Furthermore, I feel the results are over-interpreted. There are major gaps between the results actually shown and the claim about the &quot;superfluousness of cell types in spatial cognition&quot;. Evidence directly supporting the overall conclusion seems to be weak at the moment.</p>
<p>Comments on the revision:</p>
<p>The authors showed that the results generalized to different types of networks. The results were generally robust to different types of deep network architectures. This partially addressed my concern. It remains unclear whether the findings would generalize across different types of environment. Regarding this point, the authors argued that the way how they constructed the environment was consistent with the typical experimental setting in studying spatial navigation system in rodents. After the revision, it remains unclear what the implications of the work is for the spatial navigation system in the brain, given that the null model neurons failed to reproduce certain key properties of place cells (although I agreed with the authors that examining such null models are useful and would encourage one to rethink about the approach used to study these neural systems).</p>
<p>Major concerns:</p>
<p>(1) The authors reported that, in their model setting, most neurons throughout the different layers of CNNs show strong spatial selectivity. This is interesting and perhaps also surprising. It would be useful to test/assess this prediction directly based on existing experimental results. It is possible that the particular 2-d virtual environment used is special. The results will be strengthened if similar results hold for other testing environments.</p>
<p>In particular, examining the pictures shown in Fig. 1A, it seems that local walls of the 'box' contain strong oriented features that are distinct across different views. Perhaps the response of oriented visual filters can leverage these features to uniquely determine the spatial variable. This is concerning because this is is a very specific setting that is unlikely to generalize.</p>
<p>[Updated after revision]: This concern is partially addressed in the revision. The authors argued that the way how they constructed the environment is consistent with the typical experimental setting in studying spatial navigation system in rodents.</p>
<p>(2) Previous experimental results suggest that various function cell types discovered in rodent navigation circuits persist in dark environments. If we take the modeling framework presented in this paper literally, the prediction would be that place cells/head direction cells should go away in darkness. This implies that key aspects of functional cell types in the spatial cognition are missing in the current modeling framework. This limitation needs to be addressed or explicitly discussed.</p>
<p>[Updated after revision]: The authors proposed that their model should be treated as a null model, instead of a candidate model for the brain's spatial navigation system. This clarification helps to better position this work. I would like to thank the authors for making this point explicit. However, this doesn't fully address the issues raised. The significance of the reported results still depend on how well the null model can explain the experimental findings. If the null model failed to explain important aspects of the firing properties of functional cell types, that would speak in favor of the usefulness of the concept of functional cell types.</p>
<p>(3) Place cells/border cell/ head direction cells are mostly studied in the rodent's brain. For rodents, it is not clear whether standard DNNs would be good models of their visual systems. It is likely that rodent visual system would not be as powerful in processing visual inputs as the DNNs used in this study.</p>
<p>[Updated after revision]: The authors didn't specifically address this. But clarifying their work as a null model partially addresses this concern.</p>
<p>(4) The overall claim that the functional cell types defined in spatial cognition are superfluous seems to be too strong based on the results reported here. The paper only studied a particular class of models, and arguably, the properties of these models have a major gap to those of real brains. Even though that, in the DNN models simulated in this particular virtual environment, (i) most model neurons have strong spatial selectivity; (ii) removing model neurons with the strongest spatial selectivity still retain substantial spatial information, why is this relevant to the brain? The neural circuits may operate in a very different regime. Perhaps a more reasonable interpretation of the results would be: these results raise the possibility that those strongly selective neurons observed in the brain may not be essential for encoding certain features, as something like this is observed in certain models. It is difficult to draw definitive conclusions about the brain based on the results reported.</p>
<p>[Updated after revision]: The authors clarified that their model should be interpreted as a null model. This partially addresses the concern raised here. However, some concerns remain- it remains unclear what new insights the current work offers in terms of understanding the spatial navigation systems. It seems that this work concerns more about the approach to studying the neural systems. Perhaps this point could be made even more clear.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99047.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this paper, the authors demonstrate the inevitability of the emergence of spatial information in sufficiently complex systems, even those that are only trained on object recognition (i.e. not a &quot;spatial&quot; system). As such, they present an important null hypothesis that should be taken into consideration for experimental design and data analysis of spatial tuning and its relevance for behavior.</p>
<p>Strengths:</p>
<p>The paper's strengths include the use of a large multi-layer network trained in a detailed visual environment. This illustrates an important message for the field: that spatial tuning can be a result of sensory processing. While this is a historically recognized and often-studied fact in experimental neuroscience, it is made more concrete with the use of a complex sensory network. Indeed, the manuscript is a cautionary tale for experimentalists and computational researchers alike against blindly applying and interpreting metrics without adequate controls. The addition of the deep network, i.e. the argument that sufficient processing increases the likelihood of such a confound, is a novel and important contribution.</p>
<p>Weaknesses:</p>
<p>However, the work has a number of significant weaknesses. Most notably: the spatial tuning that emerges is precisely that we would expect from visually-tuned neurons, and they do not engage with literature that controls for these confounds or compare the quality or degree of spatial tuning with neural data; the ability to linearly decode position from a large number of units is not a strong test of spatial cognition; and the authors make strong but unjustified claims as to the implications of their results in opposition to, as opposed to contributing to, work being done in the field.</p>
<p>The first weakness is that the degree and quality of spatial tuning that emerges in the network is not analyzed to the standards of evidence that have been used in well-controlled studies of spatial tuning in the brain. Specifically, the authors identify place cells, head direction cells, and border cells in their network, and their conjunctive combinations. However, these forms of tuning are the most easily confounded by visual responses, and it's unclear if their results will extend to observed forms of spatial tuning that are not.</p>
<p>For example, consider the head direction cells in Figure 3C. In addition to increased activity in some directions, these cells also have a high degree of spatial nonuniformity, suggesting they are responding to specific visual features of the environment. In contrast, the majority of HD cells in the brain are only very weakly spatially selective, if at all, once an animal's spatial occupancy is accounted for (Taube et al 1990, JNeurosci). While the preferred orientation of these cells are anchored to prominent visual cues, when they rotate with changing visual cues the entire head direction system rotates together (cells' relative orientation relationships are maintained, including those that encode directions facing AWAY from the moved cue), and thus these responses cannot be simply independent sensory-tuned cells responding to the sensory change) (Taube et al 1990 JNeurosci, Zugaro et al 2003 JNeurosci, Ajbi et al 2023).</p>
<p>As another example, the joint selectivity of detected border cells with head direction in Figure 3D suggests that they are &quot;view of a wall from a specific angle&quot; cells. In contrast, experimental work on border cells in the brain has demonstrated that these are robust to changes in the sensory input from the wall (e.g. van Wijngaarden et al 2020), or that many of them are are not directionally selective (Solstad et al 2008).</p>
<p>The most convincing evidence of &quot;spurious&quot; spatial tuning would be the emergence of HD-independent place cells in the network, however, these cells are a very small minority (in contrast to hippocampal data, Thompson and Best 1984 JNeurosci, Rich et al 2014 Science), the examples provided in Figure 3 are significantly more weakly tuned than those observed in the brain.</p>
<p>Indeed, the vast majority of tuned cells in the network are conjunctively selective for HD (Figure 3A). While this conjunctive tuning has been reported, many units in the hippocampus/entorhinal system are not strongly hd selective (Muller et al 1994 JNeurosci, Sangoli et al 2006 Science, Carpenter et al 2023 bioRxiv). Further, many studies have been done to test and understand the nature of sensory influence (e.g. Acharya et al 2016 Cell), and they tend to have a complex relationship with a variety of sensory cues, which cannot readily be explained by straightforward sensory processing (rev: Poucet et al 2000 Rev Neurosci, Plitt and Giocomo 2021 Nat Neuro). E.g. while some place cells are sometimes reported to be directionally selective, this directional selectivity is dependent on behavioral context (Markus et al 1995, JNeurosci), and emerges over time with familiarity to the environment (Navratiloua et al 2012 Front. Neural Circuits). Thus, the question is not whether spatially tuned cells are influenced by sensory information, but whether feed-forward sensory processing alone is sufficient to account for their observed turning properties and responses to sensory manipulations.</p>
<p>These issues indicate a more significant underlying issue of scientific methodology relating to the interpretation of their result and its impact on neuroscientific research. Specifically, in order to make strong claims about experimental data, it is not enough to show that a control (i.e. a null hypothesis) exists, one needs to demonstrate that experimental observations are quantitatively no better than that control.</p>
<p>Where the authors state that &quot;In summary, complex networks that are not spatial systems, coupled with environmental input, appear sufficient to decode spatial information.&quot; what they have really shown is that it is possible to decode some degree of spatial information. This is a null hypothesis (that observations of spatial tuning do not reflect a &quot;spatial system&quot;), and the comparison must be made to experimental data to test if the so-called &quot;spatial&quot; networks in the brain have more cells with more reliable spatial info than a complex-visual control.</p>
<p>Further, the authors state that &quot;Consistent with our view, we found no clear relationship between cell type distribution and spatial information in each layer. This raises the possibility that &quot;spatial cells&quot; do not play a pivotal role in spatial tasks as is broadly assumed.&quot; Indeed, this would raise such a possibility, if 1) the observations of their network were indeed quantitatively similar to the brain, and 2) the presence of these cells in the brain were the only evidence for their role in spatial tasks. However, 1) the authors have not shown this result in neural data, they've only noticed it in a network and mentioned the POSSIBILITY of a similar thing in the brain, and 2) the &quot;assumption&quot; of the role of spatially tuned cells in spatial tasks is not just from the observation of a few spatially tuned cells. But from many other experiments including causal manipulations (e.g. Robinson et al 2020 Cell, DeLauilleon et al 2015 Nat Neuro), which the authors conveniently ignore. Thus, I do not find their argument, as strongly stated as it is, to be well-supported.</p>
<p>An additional weakness is that linear decoding of position is not a measure of spatial cognition. The ability to decode position from a large number of weakly tuned cells is not surprising. However, based on this ability to decode, the authors claim that &quot;'spatial' cells do not play a privileged role in spatial cognition&quot;. To justify this claim, the authors would need to use the network to perform e.g. spatial navigation tasks, then investigate the networks' ability to perform these tasks when tuned cells were lesioned.</p>
<p>Finally, I find a major weakness of the paper to be the framing of the results in opposition to, as opposed to contributing to, the study of spatially tuned cells. For example, the authors state that &quot;If a perception system devoid of a spatial component demonstrates classically spatially-tuned unit representations, such as place, head-direction, and border cells, can &quot;spatial cells&quot; truly be regarded as 'spatial'?&quot; Setting aside the issue of whether the perception system in question does indeed demonstrate spatially-tuned unit representations comparable to those in the brain, I ask &quot;Why not?&quot; This seems to be a semantic game of reading more into a name than is necessarily there. The names (place cells, grid cells, border cells, etc) describe an observation (that cells are observed to fire in certain areas of an animal's environment). They need not be a mechanistic claim (that space &quot;causes&quot; these cells to fire) or even, necessarily, a normative one (these cells are &quot;for&quot; spatial computation). This is evidenced by the fact that even within e.g. the place cell community, there is debate as to these cells' mechanisms and function (eg memory, navigation, etc), or if they can even be said to only serve a single one function. However, they are still referred to as place cells, not as a statement of their function but as a history-dependent label that refers to their observed correlates with experimental variables. Thus, the observation that spatially tuned cells are &quot;inevitable derivatives of any complex system&quot; is itself an interesting finding which contributes to, rather than contradicts, the study of these cells. It seems that the authors have a specific definition in mind when they say that a cell is &quot;truly&quot; &quot;spatial&quot; or that a biological or artificial neural network is a &quot;spatial system&quot;, but this definition is not stated, and it is not clear that the terminology used in the field presupposes their definition.</p>
<p>In sum, the authors have demonstrated the existence of a control/null hypothesis for observations of spatially-tuned cells. However, 1) It is not enough to show that a control (null hypothesis) exists, one needs to test if experimental observations are no better than control, in order to make strong claims about experimental data, 2) the authors do not acknowledge the work that has been done in many cases specifically to control for this null hypothesis in experimental work or to test the sensory influences on these cells, and 3) the authors do not rigorously test the degree or source of spatial tuning of their units.</p>
<p>Comments on revisions:</p>
<p>While I'm happy to admit that standards of spatial tuning are not unified or consistent across the field, I do not believe the authors have addressed my primary concern: they have pointed out a null model, and then have constructed a strong opinion around that null model without actually testing if it's sufficient to account for neural data. I've slightly modified my review to that effect.</p>
<p>I do think it would be good for the authors to state in the manuscript what they mean when they say that a cell is &quot;truly&quot; &quot;spatial&quot; or that a biological or artificial neural network is a &quot;spatial system&quot;. This is implied throughout, but I was unable to find what would distinguish a &quot;truly&quot; spatial system from a &quot;superfluous&quot; one.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99047.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Luo</surname>
<given-names>Xiaoliang</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5297-2114</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mok</surname>
<given-names>Robert M</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7261-9257</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Love</surname>
<given-names>Bradley C</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7883-7076</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>but see Franzius, Sprekeler, Wiskott, PLoS Computational Biology, 2007</p>
</disp-quote>
<p>We have discussed the differences with this work in the response to Editor recommendations above.</p>
<disp-quote content-type="editor-comment">
<p>While the findings reported here are interesting, it is unclear whether they are the consequence of the specific model setting, and how well they would generalize.</p>
</disp-quote>
<p>We have considered deep vision models across different architectures in our paper, which include traditional feedforward convolutional neural networks (VGG-16), convolutional neural networks with skip connections (ResNet-50) and the Vision Transformer (VIT) which employs self-attention instead of convolution as its core information processing unit.</p>
<disp-quote content-type="editor-comment">
<p>In particular, examining the pictures shown in Fig. 1A, it seems that local walls of the ’box’ contain strong oriented features that are distinct across different views. Perhaps the response of oriented visual filters can leverage these features to uniquely determine the spatial variable. This is concerning because this is a very specific setting that is unlikely to generalize.</p>
</disp-quote>
<p>The experimental set up is based on experimental studies of spatial cognition in rodents. They are typically foraging in square or circular environments. Indeed, square environments will have more borders and corners that will provide information about the spatial environment, which is true in both empirical studies and our simulations. In any navigation task, and especially more realistic environments, visual information such as borders or landmarks likely play a major role in spatial information available to the agent. In fact, studies that do not consider sensory information to contribute to spatial information are likely missing a major part of how animals navigate.</p>
<disp-quote content-type="editor-comment">
<p>The prediction would be that place cells/head direction cells should go away in darkness. This implies that key aspects of functional cell types in the spatial cognition are missing in the current modeling framework.</p>
</disp-quote>
<p>We addressed this comment in our response to the editor’s highlight. To briefly recap, we do not intend to propose a comprehensive model of the brain that captures all spatial phenomena, as we would not expect this from an object recognition network. Instead, we show that such a simple and nonspatial model can reproduce key signatures of spatial cells, raising important questions about how we interpret spatial cell types that dominate current research.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>The network used in the paper is still guided by a spatial error signal [...] one could say that the authors are in some way hacking this architecture and turning it into a spatial navigation one through learning.</p>
</disp-quote>
<p>To be clear, the base networks we use do not undergo spatial error training. They have either been pre-trained on image classification tasks or are untrained. We used a standard neuroscience approach: training linear decoders on representations to assess the spatial information present in the network layers. The higher decoding errors in early layer representations (Fig. 2A) indicate that spatial information differs across layers—an effect that cannot be attributed to the linear decoder alone.</p>
<disp-quote content-type="editor-comment">
<p>My question is whether the paper is fighting an already won battle.</p>
</disp-quote>
<p>Intuitive cell type discovery are still being celebrated. Concentrating on this kind of cell type discovery has broader implications that could be deleterious to the future of science. One point to note is that this issue depends on the area or subfield of neuroscience. In some subfields, papers that claim to find cell types with a strong claim of specific functions are relatively rare, and population coding is common (e.g., cognitive control in primate prefrontal cortex, neural dynamics of motor control). Although rodent neuroscience as a field is increasingly adopting population approaches, influential researchers and labs are still publishing “cell types” and in top journals (here are a few from 2017-2024: Goal cells (Sarel et al., 2017), Object-vector cells (Høydal et al., 2019), 3D place cells (Grieves et al., 2020), Lap cells (Sun et al., 2020), Goal-vector cells (Ormond and O’Keefe, 2022), Predictive grid cells (Ouchi and Fujisawa, 2024).</p>
<p>In some cases, identification of cell types is only considered a part of the story, and there are analyses on behavior, neural populations, and inactivationbased studies. However, our view (and suggest this is shared amongst most researchers) is that a major reason these papers are reviewed and accepted to top journals is because they have a simple, intuitive “cell type” discovery headline, even if it is not the key finding or analysis that supports the insightful aspects of the work. This is unnecessary and misleading to students of neuroscience, related fields, and the public, it affects private and public funding priorities and in turn the future of science. Worse, it could lead the field down the wrong path, or at the least distribute attention and resources to methods and papers that could be providing deeper insights. Consistent with the central message of our work, we believe the field should prioritize theoretical and functional insights over the discovery of new “cell types”.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>The ability to linearly decode position from a large number of units is not a strong test of spatial information, nor is it a measure of spatial cognition</p>
</disp-quote>
<p>Using a linear decoder to test what information is contained in a population of neurons available for downstream areas is a common technique in neuroscience (Tong and Pratte, 2012; DiCarlo et al., 2012) including spatial cells (e.g., Diehl et al. 2017; Horrocks et al. 2024). A linear decoder is used because it is a direct mapping from neurons to potential output behavior. In other words, it only needs to learn some mapping to link one set of neurons to another set which can “read out” the information. As such, it is a measure of the information contained in the population, and it is a lower bound of the information contained - as both biological and artificial neurons can do more complex nonlinear operations (as the activation function is nonlinear).</p>
<p>We understand the reviewer may understand this concept but we explain it here to justify our position and for completeness of this public review.</p>
<disp-quote content-type="editor-comment">
<p>For example, consider the head direction cells in Figure 3C. In addition to increased activity in some directions, these cells also have a high degree of spatial nonuniformity, suggesting they are responding to specific visual features of the environment. In contrast, the majority of HD cells in the brain are only very weakly spatially selective, if at all, once an animal’s spatial occupancy is accounted for (Taube et al 1990, JNeurosci). While the preferred orientation of these cells are anchored to prominent visual cues, when they rotate with changing visual cues the entire head direction system rotates together (cells’ relative orientation relationships are maintained, including those that encode directions facing AWAY from the moved cue), and thus these responses cannot be simply independent sensory-tuned cells responding to the sensory change) (Taube et al 1990 JNeurosci, Zugaro et al 2003 JNeurosci, Ajbi et al 2023).</p>
</disp-quote>
<p>As we have noted in our response to the editor, one of the main issues is how the criteria to assess what they are interested in is created in a subjective, and biased way, in a circular fashion (seeing spatial-like responses, developing criteria to determine a spatial response, select a threshold).</p>
<p>All the examples the reviewer provides concentrate on strict criteria developed after finding such cells. What is the purpose of these cells for function, for behavior? Just finding a cell that looks like it is tuned to something does not explain its function. Neuroscience began with tuning curves in part due to methodological constraints, which was a promising start, but we propose that this is not the way forward.</p>
<disp-quote content-type="editor-comment">
<p>The metrics used by the authors to quantify place cell tuning are not clearly defined in the methods, but do not seem to be as stringent as those commonly used in real data. (e.g. spatial information, Skaggs et al 1992 NeurIPS).</p>
</disp-quote>
<p>We identified place cells following the definition from Tanni et al. (2022), by one of the leading labs in the field. Since neurons in DNNs lack spikes, we adapted their criteria by focusing on the number of spatial bins in the ratemap rather than spike-based measures. However, our central argument is that the very act of defining spatial cells is problematic. Researchers set out to find place cells to study spatial representations, find spatially selective cells with subjective, qualitative criteria (sometimes combined with prior quantitative criteria, also subjectively defined), then try to fine-tune the criteria to more “stringent” criteria, depending on the experimental data at hand. It is not uncommon to see methodological sections that use qualitative judgments, such as: “To avoid bias ... we applied a loose criteria for place cells” Tanaka et al. (2018) , which reflects the lack of clarity for and subjectivity of place cell selection criteria.</p>
<p>A simple literature survey reveals inconsistent criteria across studies. For place field selection, Dombeck et al. (2010) required mean firing rates exceeding 25% of peak rate, while Tanaka et al. (2018) used a 20% threshold. Speed thresholds also vary dramatically: Dombeck et al. (2010) calculated firing rates only when mice moved faster than 8.3 cm/s, whereas Tanaka et al. (2018) used 2 cm/s. Additional criteria differ further: Tanaka et al. (2018) required firing rates between 1-10 Hz and excluded cells with place fields larger than 1/3 of the area, while Dombeck et al. (2010) selected fields above 1.5 Hz, and Tanni et al. (2022) used a 10 spatial bins to 1/2 area threshold. As Dombeck et al. (2010) noted, differences in recording methods and place field definitions lead to varying numbers of identified place cells. Moreover, Grijseels et al. (2021) demonstrated that different detection methods produce vastly different place cell counts with minimal overlap between identified populations.</p>
<p>This reflects a deeper issue. Unlike structurally and genetically defined cell types (e.g., pyramidal neurons, interneurons, dopamingeric neurons, cFos expressing neurons), spatial cells lack such clarity in terms of structural or functional specialization and it is unclear whether such “cell types” should be considered cell types in the same way. While scientific progress requires standardized definitions, the question remains whether defining spatial cells through myriad different criteria advances our understanding of spatial cognition. Are researchers finding the same cells? Could they be targeting different populations? Are they missing cells crucial for spatial cognition that they exclude due to the criteria used? We think this is likely. The inconsistency matters because different criteria may capture genuinely different neural populations or computational processes.</p>
<p>Variability in definitions and criteria is an issue in any field. However, as we have stated, the deeper issue is whether we should be defining and selecting these cells at all before commencing analysis. By defining and restricting to spatial “cell types”, we risk comparing fundamentally different phenomena across studies, and worse, missing the fundamental unit of spatial cognition (e.g., the population).</p>
<p>We have added a paragraph in Discussion (lines 357-366) noting the inconsistency in place cell selection criteria in the literature and the consequences of using varying criteria.</p>
<p>We have also added a sentence (lines 354-356) raising the comparison of functionally defined spatial cell types with structurally and genetically defined cell types in the Discussion.</p>
<disp-quote content-type="editor-comment">
<p>Thus, the question is not whether spatially tuned cells are influenced by sensory information, but whether feed-forward sensory processing alone is sufficient to account for their observed turning properties and responses to sensory manipulations.</p>
<p>These issues indicate a more significant underlying issue of scientific methodology relating to the interpretation of their result and its impact on neuroscientific research. Specifically, in order to make strong claims about experimental data, it is not enough to show that a control (i.e. a null hypothesis) exists, one needs to demonstrate that experimental observations are quantitatively no better than that control.</p>
<p>Where the authors state that ”In summary, complex networks that are not spatial systems, coupled with environmental input, appear sufficient to decode spatial information.” what they have really shown is that it is possible to decode *some degree* of spatial information. This is a null hypothesis (that observations of spatial tuning do not reflect a ”spatial system”), and the comparison must be made to experimental data to test if the so-called ”spatial” networks in the brain have more cells with more reliable spatial info than a complex-visual control.</p>
</disp-quote>
<p>We agree that good null hypotheses with quantitative comparisons are important. However, it is not clear that researchers in the field have not been using a null hypothesis, rather they make the assumption that these cell types exist and are functional in the way they assume. We provide one null hypothesis. The field can and should develop more and stronger null hypotheses.</p>
<p>In our work, we are mainly focusing on criteria of finding spatial cells, and making the argument that simply doing this is misleading. Researcher develop criteria and find such cells, but often do not go further to assess whether they are real cell “types”, especially if they exclude other cells which can be misleading if other cells also play a role in the function of interest.</p>
<disp-quote content-type="editor-comment">
<p>But from many other experiments including causal manipulations (e.g. Robinson et al 2020 Cell, DeLauilleon et al 2015 Nat Neuro), which the authors conveniently ignore. Thus, I do not find their argument, as strongly stated as it is, to be well-supported.</p>
</disp-quote>
<p>We acknowledge that there are several studies that have performed inactivation studies that suggest a strong role for place cells in spatial behavior. Most studies do not conduct comprehensive analyses to confirm that their place cells are in fact crucial for the behavior at hand.</p>
<p>One question is how the criteria were determined. Did the researchers make their criteria based on what “worked”, so they did not exclude cells relevant to the behavior? What if their criteria were different, then the argument could have been that non-place cells also contribute to behavior.</p>
<p>Another question is whether these cells are the same kinds of cells across studies and animals, given the varied criteria across studies? As most studies do not follow the same procedures, it is unclear whether we can generalize these results across cells and indeed, across task and spatial environments.</p>
<p>Finally, does the fact that the place cells – the strongly selective cells with a place field – have a strong role in navigation provide any insight into the mechanism? Identifying cells by itself does not contribute to our understanding of how they work. Consistent with our main message, we argue that performing analyses and building computational models that uncover how the function of interest works is more valuable than simply naming cells.</p>
<disp-quote content-type="editor-comment">
<p>Finally, I find a major weakness of the paper to be the framing of the results in opposition to, as opposed to contributing to, the study of spatially tuned cells. For example, the authors state that ”If a perception system devoid of a spatial component demonstrates classically spatially-tuned unit representations, such as place, head-direction, and border cells, can ”spatial cells” truly be regarded as ’spatial’?” Setting aside the issue of whether the perception system in question does indeed demonstrate spatiallytuned unit representations comparable to those in the brain, I ask ”Why not?” This seems to be a semantic game of reading more into a name then is necessarily there. The names (place cells, grid cells, border cells, etc) describe an observation (that cells are observed to fire in certain areas of an animal’s environment). They need not be a mechanistic claim... This is evidenced by the fact that even within e.g. the place cell community, there is debate about these cells’ mechanisms and function (eg memory, navigation, etc), or if they can even be said to serve only a single function. However, they are still referred to as place cells, not as a statement of their function but as a history-dependent label that refers to their observed correlates with experimental variables. Thus, the observation that spatially tuned cells are ”inevitable derivatives of any complex system” is itself an interesting finding which *contributes to*, rather than contradicts, the study of these cells. It seems that the authors have a specific definition in mind when they say that a cell is ”truly” ”spatial” or that a biological or artificial neural network is a ”spatial system”, but this definition is not stated, and it is not clear that the terminology used in the field presupposes their definition.</p>
</disp-quote>
<p>We have to agree to disagree with the reviewer on this point. Although researchers may reflect on their work and discuss what the mechanistic role of these cells are, it is widely perceived that cell type discovery is perceived as important to journals and funders due to its intuitive appeal and easy-tounderstand impact – even if there is no finding of interest to be reported. As noted in the comment above, papers claiming cell type discovery continue to be published in top journals and is continued to be funded.</p>
<p>Our argument is that maybe “cell type” discovery research should not celebrated in the way it is, and in fact they shouldn’t be discovered when they are not genuine cell types like structural or genetic cell types. By using this term it make it appear like they are something they are not, which is misleading. They may be important cells, but providing a name like a “place” cell also suggests other cells are not encoding space - which is very unlikely to be true.</p>
<p>In sum, our view is that finding and naming cells through a flawed theoretical lens that may not actually function as their names suggests can lead us down the wrong path and be detrimental to science.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>The novelty of the current study relative to the work by Franzius, Sprekeler, Wiskott (PLoS Computational Biology, 2007) needs to be carefully addressed. That study also modeled the spatial correlates based on visual inputs.</p>
</disp-quote>
<p>Our work differs from Franzius et al. (2007) on both theoretical and experimental fronts. While both studies challenge the mechanisms underlying spatial cell formation, our theoretical contributions diverge. Franzius et al. (2007) assume spatial cells are inherently important for spatial cognition and propose a sensory-driven computational mechanism as an alternative to mainstream path integration frameworks for how spatial cells arise and support spatial cognition. In contrast, we challenge the notion that spatial cells are special at all. Using a model with no spatial grounding, we demonstrate that 1) spatial cells as naturally emerge from complex non-linear processing and 2) are not particularly useful for spatial decoding tasks, suggesting they are not crucial for spatial cognition.</p>
<p>Our approach employs null models with fixed weights—either pretrained on classification tasks or entirely random—that process visual information non-sequentially. These models serve as general-purpose information processors without spatial grounding. In contrast, Franzius et al. (2007)’s model learns directly from environmental visual information, and the emergence of spatial cells (place or head-direction cells) in their framework depends on input statistics, such as rotation and translation speeds. Notably, their model does not simultaneously generate both place and head-direction cells; the outcome varies with the relative speed of rotation versus translation. Their sensory-driven model indirectly incorporates motion information through learning, exhibiting a time-dependence influenced by slow-feature analysis.</p>
<p>Conversely, our model simultaneously produces units with place and headdirection cell profiles by processing visual inputs sampled randomly across locations and angles, independent of temporal or motion-related factors. This positions our model as a more general and fundamental null hypothesis, ideal for challenging prevailing theories on spatial cells due to its complete lack of spatial or motion grounding.</p>
<p>Finally, unlike Franzius et al. (2007), who do not evaluate the functional utility of their spatial representations, we test whether the emergent spatial cells are useful for spatial decoding. We find that not only do spatial cells emerge in our non-spatial model, but they also fail to significantly aid in location or head-direction decoding. This is the central contribution of our work: spatial cells can arise without spatial or sensory grounding, and their functional relevance is limited. We have updated the manuscript to clarify the novelty of the current contribution to previous work (lines 324-335).</p>
<disp-quote content-type="editor-comment">
<p>In Fig. 2, it may be useful to plot the error in absolute units, rather than the normalized error. The direction decoding can be quantified in terms of degree Also, it would be helpful to compare the accuracy of spatial localization to that of the actual place cells in rodents.</p>
</disp-quote>
<p>We argue it makes more sense and put comparison in perspective when we normalize the error by dividing the maximal error possible under each task. For transparency, we plot the errors in absolute physical units used by the Unity game engine in the updated Appendix (Fig. 1).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>Regarding the involvement of ’classified cells’ in decoding, I think a useful way to present the results would be to show the relationship between ’placeness’, ’directioness’ and ’borderness’ and the strength of the decoder weights. Either as a correlation or as a full scatter plot.</p>
</disp-quote>
<p>We appreciate your suggestion to visualize the relationship between units’ spatial properties and their corresponding decoder weights. We believe it would be an important addition to our existing results. Based on the exclusion analyses, we anticipated the correlation to be low, and the additional results support this expectation.</p>
<p>As an example, we present unit plots below for VGG-16 (pre-trained and untrained, at its penultimate layer with sampling rate equals 0.3; Author response image 1 and 2). Additional plots for various layers and across models are included in the supplementary materials (Fig. S12-S28). Consistently across conditions, we observed no significant correlations between units’ spatial properties (e.g., placeness) and their decoding weight strengths. These results further corroborate the conclusions drawn from our exclusion analyses.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>My main suggestions are that the authors: -perform manipulations to the sensory environment similar to those done in experimental work, and report if their tuned cells respond in similar ways -quantitatively compare the degree of spatial tuning in their networks to that seen in publicly available data -re-frame the discussion of their results to critically engage with and contribute to the field and its past work on sensory influences to these cells</p>
</disp-quote>
<p>As we noted in our opening section, our model is not intended as a model of the brain. It is a non-spatial null model, and we present the surprising finding that even such a model contains spatial cell-like units if identified using criteria typically used in the field. This raises the question whether simply finding cells that show spatial properties is sufficient to grant the special status of “cell type” that is involved in the brain function of interest.</p>
<fig id="sa3fig1">
<label>Author response image 1.</label>
<caption>
<title>VGG-16 (pre-trained), penultimate layer units, show no apparent relationship between spatial properties and their decoder weight strengths.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99047-sa3-fig1.jpg" mimetype="image"/>
</fig>
<fig id="sa3fig2">
<label>Author response image 2.</label>
<caption>
<title>VGG-16 (untrained), penultimate layer units, show no apparent relationship between spatial properties and their decoder weight strengths.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99047-sa3-fig2.jpg" mimetype="image"/>
</fig>
<p>Furthermore, our main simulations were designed to be compared to experimental work where rodents foraged around square environments in the lab. We did not do an extensive set of simulations as the purpose of our study is not to show that we capture exactly every single experimental finding, but rather raise the issues with the functional cell type definition and identification approach for progressing neuroscientific knowledge.</p>
<p>Finally, as we note in more detail below, different labs use different criteria for identifying spatial cells, which depend both on the lab and the experimental design. Our point is that we can identify such cells using criteria set by neuroscientists, and that such cell types may not reflect any special status in spatial processing. Additional simulations that show less alignment with certain datasets will not provide support for or against our general message.</p>
<p>References</p>
<p>Banino A, Barry C, Uria B, Blundell C, Lillicrap T, Mirowski P, Pritzel A, Chadwick MJ, Degris T, Modayil J, Wayne G, Soyer H, Viola F, Zhang B, Goroshin R, Rabinowitz N, Pascanu R, Beattie C, Petersen S, Sadik A, Gaffney S, King H, Kavukcuoglu K, Hassabis D, Hadsell R, Kumaran D (2018) Vector-based navigation using grid-like representations in artificial agents. Nature 557(7705):429–433, DOI 10.1038/s41586-018-0102-6, URL <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/articles/s41586-018-0102-6">http://www.nature.com/articles/s41586-018-0102-6</ext-link></p>
<p>DiCarlo JJ, Zoccolan D, Rust NC (2012) How Does the Brain Solve Visual Object Recognition? Neuron 73(3):415–434, DOI 10.1016/J.NEURON.2012.01.010, URL <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/neuron/fulltext/S0896-6273(12)00092-X">https://www.cell.com/neuron/fulltext/S0896-6273(12)00092-X</ext-link></p>
<p>Diehl GW, Hon OJ, Leutgeb S, Leutgeb JK (2017) Grid and Nongrid Cells in Medial Entorhinal Cortex Represent Spatial Location and Environmental Features with Complementary Coding Schemes. Neuron 94(1):83– 92.e6, DOI 10.1016/j.neuron.2017.03.004, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627317301873">https://linkinghub.elsevier.com/retrieve/pii/S0896627317301873</ext-link></p>
<p>Dombeck DA, Harvey CD, Tian L, Looger LL, Tank DW (2010) Functional imaging of hippocampal place cells at cellular resolution during virtual navigation. Nature Neuroscience 13(11):1433–1440, DOI 10.1038/nn.2648, URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nn.2648">https://www.nature.com/articles/nn.2648</ext-link></p>
<p>Ebitz RB, Hayden BY (2021) The population doctrine in cognitive neuroscience. Neuron 109(19):3055–3068, DOI 10.1016/j.neuron. 2021.07.011, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627321005213">https://linkinghub.elsevier.com/retrieve/pii/S0896627321005213</ext-link></p>
<p>Grieves RM, Jedidi-Ayoub S, Mishchanchuk K, Liu A, Renaudineau S, Jeffery KJ (2020) The place-cell representation of volumetric space in rats. Nature Communications 11(1):789, DOI 10.1038/s41467-020-14611-7, URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-020-14611-7">https://www.nature.com/articles/s41467-020-14611-7</ext-link></p>
<p>Grijseels DM, Shaw K, Barry C, Hall CN (2021) Choice of method of place cell classification determines the population of cells identified. PLOS Computational Biology 17(7):e1008835, DOI 10.1371/journal.pcbi.1008835, URL <ext-link ext-link-type="uri" xlink:href="https://dx.plos.org/10.1371/journal.pcbi.1008835">https://dx.plos.org/10.1371/journal.pcbi.1008835</ext-link></p>
<p>Horrocks EAB, Rodrigues FR, Saleem AB (2024) Flexible neural population dynamics govern the speed and stability of sensory encoding in mouse visual cortex. Nature Communications 15(1):6415, DOI 10.1038/s41467-024-50563-y, URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-024-50563-y">https://www.nature.com/articles/s41467-024-50563-y</ext-link></p>
<p>Høydal , Skytøen ER, Andersson SO, Moser MB, Moser EI (2019) Objectvector coding in the medial entorhinal cortex. Nature 568(7752):400– 404, DOI 10.1038/s41586-019-1077-7, URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41586-019-1077-7">https://www.nature.com/articles/s41586-019-1077-7</ext-link></p>
<p>Ormond J, O’Keefe J (2022) Hippocampal place cells have goal-oriented vector fields during navigation. Nature 607(7920):741–746, DOI 10.1038/s41586-022-04913-9, URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41586-022-04913-9">https://www.nature.com/articles/s41586-022-04913-9</ext-link></p>
<p>Ouchi A, Fujisawa S (2024) Predictive grid coding in the medial entorhinal cortex. Science 385(6710):776–784, DOI 10.1126/science.ado4166, URL <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/10.1126/science.ado4166">https://www.science.org/doi/10.1126/science.ado4166</ext-link></p>
<p>Sarel A, Finkelstein A, Las L, Ulanovsky N (2017) Vectorial representation of spatial goals in the hippocampus of bats. Science 355(6321):176–180, DOI 10.1126/science.aak9589, URL <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/10.1126/science.aak9589">https://www.science.org/doi/10.1126/science.aak9589</ext-link></p>
<p>Sun C, Yang W, Martin J, Tonegawa S (2020) Hippocampal neurons represent events as transferable units of experience. Nature Neuroscience 23(5):651–663, DOI 10.1038/s41593-020-0614-x, URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-020-0614-x">https://www.nature.com/articles/s41593-020-0614-x</ext-link></p>
<p>Tanaka KZ, He H, Tomar A, Niisato K, Huang AJY, McHugh TJ (2018) The hippocampal engram maps experience but not place. Science 361(6400):392–397, DOI 10.1126/science.aat5397, URL <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/10.1126/science.aat5397">https://www.science.org/doi/10.1126/science.aat5397</ext-link></p>
<p>Tanni S, De Cothi W, Barry C (2022) State transitions in the statistically stable place cell population correspond to rate of perceptual change. Current Biology 32(16):3505–3514.e7, DOI 10.1016/j.cub. 2022.06.046, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0960982222010089">https://linkinghub.elsevier.com/retrieve/pii/S0960982222010089</ext-link></p>
<p>Tong F, Pratte MS (2012) Decoding Patterns of Human Brain Activity. Annual Review of Psychology 63(1):483–509, DOI 10.1146/annurev-psych-120710-100412, URL <ext-link ext-link-type="uri" xlink:href="https://www.annualreviews.org/doi/10.1146/annurev-psych-120710-100412">https://www.annualreviews.org/doi/10.1146/annurev-psych-120710-100412</ext-link></p>
</body>
</sub-article>
</article>