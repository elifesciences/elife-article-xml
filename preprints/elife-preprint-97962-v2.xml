<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">97962</article-id>
<article-id pub-id-type="doi">10.7554/eLife.97962</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97962.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>ASBAR: an Animal Skeleton-Based Action Recognition framework. Recognizing great ape behaviors in the wild using pose estimation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8937-1427</contrib-id>
<name>
<surname>Fuchs</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>michael.fuchs@unine.ch</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2461-2442</contrib-id>
<name>
<surname>Genty</surname>
<given-names>Emilie</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8378-088X</contrib-id>
<name>
<surname>Zuberbühler</surname>
<given-names>Klaus</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4103-5467</contrib-id>
<name>
<surname>Cotofrei</surname>
<given-names>Paul</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00vasag41</institution-id><institution>Institute of Information Management, University of Neuchâtel</institution></institution-wrap>, <city>Neuchâtel</city>, <country country="CH">Switzerland</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00vasag41</institution-id><institution>Institute of Biology, University of Neuchâtel</institution></institution-wrap>, <city>Neuchâtel</city>, <country country="CH">Switzerland</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02wn5qz54</institution-id><institution>Institute of Behavioural and Neural Sciences, University of St Andrews</institution></institution-wrap>, <city>St Andrews</city>, <country country="GB">United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Tung</surname>
<given-names>Jenny</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Evolutionary Anthropology</institution>
</institution-wrap>
<city>Leipzig</city>
<country country="DE">Germany</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Perry</surname>
<given-names>George H</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Pennsylvania State University</institution>
</institution-wrap>
<city>University Park</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="con"><p>Contributing authors: <email>emilie.genty@unine.ch</email>; <email>klaus.zuberbuehler@unine.ch</email>; <email>paul.cotofrei@unine.ch</email>;</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-08-02">
<day>02</day>
<month>08</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-07-21">
<day>21</day>
<month>07</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP97962</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-05">
<day>05</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-09-25">
<day>25</day>
<month>09</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.09.24.559236"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-08-02">
<day>02</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.97962.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.97962.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.97962.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.97962.1.sa0">Reviewer #2 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Fuchs et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Fuchs et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-97962-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>The study and classification of animal behaviors have traditionally relied on direct human observation or video analysis, processes that are labor-intensive, time-consuming, and prone to human bias. Advances in machine learning for computer vision, particularly in pose estimation and action recognition, offer transformative potential to enhance the understanding of animal behaviors. However, the integration of these technologies for behavior recognition remains underexplored, particularly in natural settings.</p>
<p>We introduce <italic>ASBAR</italic> (<italic>Animal Skeleton-Based Action Recognition</italic>), a novel framework that integrates pose estimation and behavior recognition into a cohesive pipeline. To demonstrate its utility, we tackled the challenging task of classifying natural behaviors of great apes in the wild.</p>
<p>Our approach leverages the OpenMonkeyChallenge dataset, one of the largest open-source primate pose datasets, to train a robust pose estimation model using DeepLabCut. Subsequently, we extracted skeletal motion data from the PanAf500 dataset, a collection of in-the-wild videos of gorillas and chimpanzees annotated with nine behavior categories. Using PoseConv3D from MMAction2, we trained a skeleton-based action recognition model, achieving a Top-1 accuracy of 75.3%. This performance is comparable to previous video-based methods while reducing input data size by approximately 20-fold, offering significant advantages in computational efficiency and storage.</p>
<p>To support further research, we provide an open-source, terminal-based GUI for training and evaluation, along with a dataset of 5,440 annotated keypoints for replication and extension to other species and behaviors.</p>
<p>All models, code, and data are publicly available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/MitchFuchs/asbar">https://github.com/MitchFuchs/asbar</ext-link></p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>non-human primates</kwd>
<kwd>great apes</kwd>
<kwd>chimpanzees</kwd>
<kwd>animal behavior recognition</kwd>
<kwd>skeleton-based action recognition</kwd>
<kwd>pose estimation</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>The manuscript has been revised to address the reviewers' comments and suggestions from eLife. While most experiments remain unchanged, the behavior classification results have been updated to reflect two key modifications: (1) the loss function was changed from cross-entropy to a class-balanced focal loss, following the approach used in the vision-based baseline by Sakib et al. (2021); and (2) the PoseConv3D weights were initialized using pretrained models from FineGym (Shao et al., 2020) instead of being trained from scratch. These changes led to a substantial improvement in the recognition of tail classes, increasing the Mean Class Accuracy from 33.6% to 47%. The manuscript was also streamlined to better emphasize the main contributions and findings.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Direct observation and manual annotation of animal behaviors are labor-intensive, time-consuming, and prone to human error [<xref ref-type="bibr" rid="c1">1</xref>]. These methods also face significant limitations, such as information loss in low-visibility settings or during complex, fast-paced social interactions involving multiple individuals. Video recording and post-hoc annotation have thus become the preferred methods for studying animal behavior. They enable detailed identification and interpretation of behaviors, while also facilitating reliability testing and replication of coding. However, the manual annotation of videos remains a significant bottleneck, underscoring the need for automated systems that can streamline animal behavior analysis. Machine learning tools have the potential to identify relevant video sections containing social interactions and automatically classify behaviors, significantly expanding the scope and robustness of observational studies and enhancing our understanding of animal behaviors [<xref ref-type="bibr" rid="c2">2</xref>].</p>
<p>Recent advancements in machine learning and computer vision offer innovative avenues for building such systems. In particular, action recognition models can learn deep representations of video features and classify these features into behavior categories. Within deep learning, two primary approaches to action recognition have emerged: video-based methods and skeleton-based methods.</p>
<p>On one hand, video-based action recognition involves analyzing RGB video data to identify spatio-temporal patterns that characterize actions. This approach often relies on Convolutional Neural Networks (CNNs) [<xref ref-type="bibr" rid="c3">3</xref>] adapted to the temporal domain. Notable models include Two-Stream CNNs [<xref ref-type="bibr" rid="c4">4</xref>], C3D [<xref ref-type="bibr" rid="c5">5</xref>], I3D [<xref ref-type="bibr" rid="c6">6</xref>], (2+1)D ResNet [<xref ref-type="bibr" rid="c7">7</xref>], and SlowFast [<xref ref-type="bibr" rid="c8">8</xref>]. These methods have been extended to classify animal behaviors [<xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c14">14</xref>] and multimodal audio-visual data [<xref ref-type="bibr" rid="c15">15</xref>].</p>
<p>On the other hand, skeleton-based action recognition predicts behavior classes based on the skeletal structure and motion of the body [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c17">17</xref>]. This approach relies on an additional preprocessing step, <italic>pose estimation</italic>, which detects body parts, such as joints and bones, and extracts their coordinates from video frames [<xref ref-type="bibr" rid="c18">18</xref>]. While skeleton-based methods require the added step of pose estimation, they offer several advantages for computational ethology [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c19">19</xref>]:</p>
<list list-type="simple">
<list-item><label>(i)</label><p><italic>Cross-subject behavior recognition</italic>: These models focus on skeletal motion rather than external appearance, allowing them to generalize across individuals within the same species (e.g., [<xref ref-type="bibr" rid="c20">20</xref>] for humans, [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>] for non-human animals).</p></list-item>
<list-item><label>(ii)</label><p><italic>Robustness to visual setting changes</italic>: Video-based models are sensitive to lighting conditions, background variations, and other subtle changes in input data [<xref ref-type="bibr" rid="c23">23</xref>–<xref ref-type="bibr" rid="c25">25</xref>]. Comparatively, skeleton-based methods are less affected by these variations [<xref ref-type="bibr" rid="c26">26</xref>].</p></list-item>
<list-item><label>(iii)</label><p><italic>Reduced computational complexity</italic>: Extracting pose coordinates reduces the dimensionality of video data, lowering computational costs and power consumption [<xref ref-type="bibr" rid="c16">16</xref>]. This is particularly beneficial for field researchers with limited resources.</p></list-item>
<list-item><label>(iv)</label><p><italic>Geometric quantification</italic>: Pose estimation provides a pre-computed geometric representation of body motion and behavior [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c27">27</xref>].</p></list-item>
</list>
<p>A major challenge for skeleton-based methods in animal behavior analysis lies in obtaining accurate pose-estimated data. While human pose estimation benefits from extensive open-source datasets and state-of-the-art detectors, achieving similar performance for animals remains challenging. Fortunately, there has been a surge in annotated animal pose datasets (e.g., Animal Kingdom [<xref ref-type="bibr" rid="c28">28</xref>], Animal Pose [<xref ref-type="bibr" rid="c29">29</xref>], AP-10K [<xref ref-type="bibr" rid="c30">30</xref>], OpenMonkeyChallenge [<xref ref-type="bibr" rid="c31">31</xref>], OpenApePose [<xref ref-type="bibr" rid="c32">32</xref>], MacaquePose [<xref ref-type="bibr" rid="c33">33</xref>], Horse-30 [<xref ref-type="bibr" rid="c34">34</xref>]) and open-source pose estimation frameworks (e.g., DeepLabCut [<xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c37">37</xref>], SLEAP [<xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c39">39</xref>], AniPose [<xref ref-type="bibr" rid="c40">40</xref>]). Despite these advancements, the use of pose estimation for behavior recognition remains underexplored, particularly in natural settings. Challenges include the lack of datasets with both keypoint coordinates and behavior annotations, and the tendency to treat pose estimation and behavior recognition as separate tasks rather than parts of an integrated approach.</p>
<p>To address these challenges, we introduce ASBAR, an innovative framework for animal skeleton-based action recognition. Our contributions include:</p>
<list list-type="bullet">
<list-item><p>An integrated pipeline, which combines a DeepLabCut-based pose estimation module with a behavior recognition module from MMAction2 [<xref ref-type="bibr" rid="c41">41</xref>]. The pipeline is encapsulated in a terminal-based GUI, allowing researchers to train and evaluate models without programming knowledge, even in remote or cloud-based environments.</p></list-item>
<list-item><p>A robust primate keypoint detector, by leveraging the OpenMonkeyChallenge dataset [<xref ref-type="bibr" rid="c31">31</xref>], which spans 26 primate species. Additionally, we provide detailed performance metrics for species and individual body parts and release 5,440 high-quality keypoint annotations for great apes in their natural habitats.</p></list-item>
<list-item><p>A methodology for wild behavior analysis using the PanAf500 dataset [<xref ref-type="bibr" rid="c11">11</xref>], a twohour collection of camera trap videos annotated with nine locomotive behaviors. We demonstrate that our skeleton-based pipeline achieves performance comparable to existing video-based methods.</p></list-item>
</list>
</sec>
<sec id="s2">
<label>2</label>
<title>The ASBAR Framework</title>
<p>ASBAR is an integrated data and model pipeline (marked in red in <xref rid="fig1" ref-type="fig">Fig 1</xref>) designed to address two sequential machine learning tasks: <italic>pose estimation</italic> and <italic>action recognition</italic>.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1:</label>
<caption><title>The ASBAR Framework.</title>
<p>The ASBAR framework’s data and model pipeline (red) comprises two modules: a <italic>pose estimation</italic> module (green) based on DeepLabCut and an <italic>action recognition</italic> module (blue) integrating models from MMAc-tion2.</p></caption>
<graphic xlink:href="559236v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The first module, responsible for animal pose estimation (marked in green in <xref rid="fig1" ref-type="fig">Fig 1</xref>), incorporates key features of DeepLabCut (DLC), a widely used framework for multianimal, markerless pose estimation [<xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c37">37</xref>]. This module includes functionality for project creation, dataset preparation, model training and evaluation, configuration editing, and video analysis.</p>
<p>The second module, focused on behavior recognition (marked in blue in <xref rid="fig1" ref-type="fig">Fig 1</xref>), integrates APIs from MMAction2 [<xref ref-type="bibr" rid="c41">41</xref>], a comprehensive platform for action recognition and video understanding. Specifically, this module employs PoseConv3D [<xref ref-type="bibr" rid="c17">17</xref>], a convolutional neural network (CNN) model tailored for skeleton-based action recognition.</p>
<p>To demonstrate the capabilities of the ASBAR framework for animal skeletonbased behavior recognition, we selected a particularly challenging task for our experiments: classifying natural great ape behaviors in the wild.</p>
<sec id="s2a">
<label>2.1</label>
<title>Pose and Behavior Datasets</title>
<p>ASBAR operates on two distinct datasets: a <italic>pose</italic> dataset and a <italic>behavior</italic> dataset. The pose dataset contains images annotated with 2D keypoint coordinates and is used to train the pose estimator model. The behavior dataset comprises video clips annotated with specific behaviors.</p>
<p>Ideally, both datasets originate from the same visual distribution—for example, pose images being a subset of video frames from the behavior dataset. However, in practice, annotating a dataset with both pose and behavior labels is time-consuming and costly. A pragmatic approach involves combining pose and behavior datasets from different visual distributions. For instance, Internet images annotated with keypoints can complement video data labeled with behaviors recorded in the wild. In such cases, the pose dataset is considered <italic>within-domain</italic>, while the behavior dataset is referred to as <italic>out-of-domain</italic>.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Pose Estimation Module</title>
<p>The task of estimating the coordinates of anatomical keypoints, denoted as <italic>pose estimation</italic>, is a crucial prerequisite for skeleton-based action recognition. In most cases, each RGB frame of a video clip of an action is preprocessed to extract the set of (<italic>x, y</italic>) coordinates in the image plane of each keypoint and its relative confidence <italic>c</italic>. In practice, this transformation is often performed via supervised CNN-based machine learning models trained for keypoint detection (<xref rid="fig2" ref-type="fig">Fig 2</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2:</label>
<caption><title>From RGB image to pseudo-heatmaps.</title>
<p>The transformation of an RGB image into a 3D heatmap volume. An input image is passed through a Conv-Deconv architecture to output a probabilistic scoremap of the keypoint location (e.g., the right elbow). By finding a local maximum in the scoremap, the location coordinates and confidence can be extracted. Using a Gaussian transformation, a pseudo heatmap is generated for each keypoint and used as input of the subsequent behavior recognition model.</p></caption>
<graphic xlink:href="559236v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The goal of the pose estimation module is to extract pose information from the behavior dataset using a trained pose estimator. This module encompasses four key functionalities: <italic>data preprocessing, model benchmarking, model selection</italic>, and <italic>pose extraction</italic>.</p>
<sec id="s2b1">
<title>Data Preprocessing</title>
<p>The module provides four preprocessing steps:</p>
<list list-type="bullet">
<list-item><p><italic>Data formatting</italic>: Ensures the pose dataset meets DeepLabCut’s structural requirements.</p></list-item>
<list-item><p><italic>Data selection</italic>: Allows users to customize the dataset by selecting specific species, annotated keypoints, or excluding invisible keypoints. For example, users can limit the dataset to chimpanzees and bonobos, focusing on three visible keypoints (e.g., eyes and nose).</p></list-item>
<list-item><p><italic>Dataset splitting</italic>: Supports options for no cross-validation, 5-fold cross-validation, or 10-fold cross-validation to enable statistical validation of the model’s performance.</p></list-item>
<list-item><p><italic>Configuration setup</italic>: Enables customization of DeepLabCut’s configuration files, including training hyperparameters.</p></list-item>
</list>
</sec>
<sec id="s2b2">
<title>Model Benchmarking</title>
<p>Given the importance of high pose prediction performance for behavior recognition accuracy, the framework facilitates benchmarking various pose estimation models. Users can evaluate models with different backbones (e.g., ResNet or EfficientNet) and depths (e.g., ResNet50, ResNet101, EfficientNet-B0).</p>
</sec>
<sec id="s2b3">
<title>Model Selection</title>
<p>When the pose and behavior datasets share the same visual distribution, benchmarking results suffice for model selection. However, for out-of-domain behavior datasets (Sect.2.1), additional evaluation is necessary, as within-domain performance does not guarantee robustness to visual domain shifts. Models with EfficientNet backbones, for example, have demonstrated superior generalization to out-of-distribution scenarios compared to ResNet models [<xref ref-type="bibr" rid="c34">34</xref>]. Evaluating out-of-domain performance involves comparing model predictions with manually labeled video frames from the behavior dataset. More details are provided in Sect.3.3.</p>
</sec>
<sec id="s2b4">
<title>Pose Extraction</title>
<p>The selected model extracts pose information from the behavior dataset. Users can specify a particular model snapshot or allow the framework to choose the snapshot with the lowest test set error.</p>
</sec>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Action Recognition Module</title>
<p>Skeleton-based action recognition involves the classification of a specific action (performed by human or non-human individuals) from a sequence of skeletal joint data (i.e., coordinate lists), captured by sensors or extracted by markerless pose estimators (<xref rid="fig3" ref-type="fig">Fig. 3</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3:</label>
<caption><title>From extracted poses to behavior classification.</title>
<p>From a set of consecutive RGB frames (e.g., 20 in our experiments), the animal pose is extracted, transformed into pseudo-heatmaps, and stacked as input of the behavior recognition model. A 3D-CNN is trained to classify the represented action into the correct behavior category (e.g., here ‘walking’)</p></caption>
<graphic xlink:href="559236v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The action recognition module classifies behaviors in the behavior dataset using pose data extracted from the first module. This module includes three functionalities: <italic>data preprocessing, model training</italic>, and <italic>model evaluation</italic>.</p>
<sec id="s2c1">
<title>Data Preprocessing</title>
<p>To enable behavior recognition, the module implements four preprocessing steps:</p>
<list list-type="bullet">
<list-item><p><italic>Prediction filtering</italic>: Retains only the highest-confidence keypoint predictions for each frame. For each keypoint, the most confident coordinates within the labeled bounding box are kept, while others are discarded.</p></list-item>
<list-item><p><italic>Data sampling</italic>: Extracts sequences of consecutive frames that meet specific time and behavior label constraints (see Sect. 3.5 for details).</p></list-item>
<list-item><p><italic>Data formatting</italic>: Converts skeleton data into a structure compatible with PoseC-onv3D.</p></list-item>
<list-item><p><italic>Configuration setup</italic>: Allows customization of PoseConv3D’s configuration, including hyperparameter settings.</p></list-item>
</list>
</sec>
<sec id="s2c2">
<title>Model Training</title>
<p>Users can train several variations of PoseConv3D available in the MMAction2 toolbox [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c41">41</xref>]. These variations include different 3D-CNN backbones (e.g., SlowOnly [<xref ref-type="bibr" rid="c42">42</xref>], C3D [<xref ref-type="bibr" rid="c5">5</xref>], X3D [<xref ref-type="bibr" rid="c8">8</xref>]) with an I3D classification head [<xref ref-type="bibr" rid="c6">6</xref>]. Training can be distributed across multiple GPUs to reduce computation time.</p>
</sec>
<sec id="s2c3">
<title>Model Evaluation</title>
<p>The module produces probabilistic classifications, returning a ranked list of behavior candidates with associated confidence probabilities. The behavior with the highest confidence is used to calculate Top-1 Accuracy, the percentage of correctly predicted samples. Other metrics, such as Top-<italic>k</italic> Accuracy (percentage of ground-truth behaviors within the top-<italic>k</italic> predictions) and Mean Class Accuracy (average Top-1 accuracy across behavior classes), are also supported.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Materials and Methods</title>
<sec id="s3a">
<label>3.1</label>
<title>Datasets and Data Annotation</title>
<p>For the classification of great ape behaviors in their natural habitat, we utilized two primary datasets: OpenMonkeyChallenge and PanAf500. Additionally, we manually labeled a subset of keypoint coordinates from PanAf500, referred to as PanAf500-Pose.</p>
<sec id="s3a1">
<title>OpenMonkeyChallenge</title>
<p>OpenMonkeyChallenge (OMC) [<xref ref-type="bibr" rid="c31">31</xref>] is a benchmark dataset containing 111,529 images of 26 primate species, designed for non-human primate pose estimation challenges. The dataset includes images sourced from the web, three U.S. National Primate Research Centers, and multiview cameras at the Minnesota Zoo. Each image is annotated with species, bounding box coordinates, and 2D pose information for 17 keypoints, including the nose, eyes, head, neck, shoulders, elbows, wrists, hips, tail, knees, and ankles. For occluded keypoints, annotators were instructed to provide the most likely location and specify visibility.</p>
<p>The dataset is divided into training (60%), validation (20%), and testing (20%) subsets. While the testing annotations are withheld for competition purposes, we combined the training and validation sets to create a comprehensive <italic>pose</italic> dataset containing 89,223 images. Examples of these images are shown in <xref rid="fig4" ref-type="fig">Fig 4</xref> (left).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4:</label>
<caption><title>Examples from the <italic>pose</italic> and <italic>behavior</italic> datasets.</title>
<p>(<italic>Left</italic>) Sample images from the <italic>OpenMonkeyChallenge</italic> dataset, one of the largest collections of primate images annotated with 2D poses. This dataset contains over 100,000 images from 26 primate species. (<italic>Right</italic>) Sample video frames from the <italic>PanAf500</italic> dataset, comprising 500 videos of gorillas and chimpanzees recorded in African forests using camera traps. The dataset includes annotations for bounding boxes and behaviors. Visual challenges include small individual sizes due to camera distance, abundant vegetation, nocturnal imaging, and varying backgrounds.</p></caption>
<graphic xlink:href="559236v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3a2">
<title>PanAf500</title>
<p>The Pan African Programme “The Cultured Chimpanzee” [<xref ref-type="bibr" rid="c43">43</xref>] aims to enhance understanding of the ecological and evolutionary factors influencing chimpanzee behavioral diversity. This program has amassed thousands of hours of footage from camera traps deployed in Central African forests. The PanAf500 dataset consists of 500 15-second videos (180,000 frames at 24 FPS) of chimpanzees and gorillas, annotated with bounding box coordinates for ape detection [<xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>] and behaviors for action recognition [<xref ref-type="bibr" rid="c11">11</xref>]</p>
<p>The dataset includes nine annotated behaviors: ‘walking,’ ‘standing,’ ‘sitting,’ ‘running,’ ‘hanging,’ ‘climbing up,’ ‘climbing down,’ ‘sitting on back,’ and ‘camera interaction.’ The class distribution exhibits a long-tail pattern [<xref ref-type="bibr" rid="c46">46</xref>], with three <italic>head</italic> classes (‘walking,’ ‘standing,’ and ‘sitting’) each containing over 1,000 samples. In contrast, <italic>tail</italic> classes such as ‘running,’ ‘climbing up,’ ‘climbing down,’ ‘sitting on back,’ and ‘camera interaction’ have fewer than 100 samples each. Examples from this dataset are displayed in <xref rid="fig4" ref-type="fig">Fig 4</xref> (right).</p>
</sec>
<sec id="s3a3">
<title>PanAf500-Pose</title>
<p>To supplement the PanAf500 dataset, we manually annotated 5,440 keypoints across 320 images, using the same keypoints as in OMC. The annotation process involved three steps:</p>
<list list-type="bullet">
<list-item><p><italic>Image selection</italic>: We first shortlisted 4,000 images using predictions from ResNet152 and EfficientNet-B6 models based on high overall prediction confidence (Section 3.3). From this shortlist, 320 frames were manually selected to represent diverse scenes, lighting conditions, postures, sizes, and species, while minimizing consecutive frames;</p></list-item>
<list-item><p><italic>Mini-clip generation</italic>: For each selected frame, we generated a 34-frame mini-clip (24 frames before and 10 frames after) to capture motion and aid in labeling occluded keypoints;</p></list-item>
<list-item><p><italic>Keypoint annotation</italic>: We employed a semi-automated annotation process, initially leveraging predictions from the ResNet152 model. These predictions were refined using DeepLabCut’s Napari plugin [<xref ref-type="bibr" rid="c47">47</xref>]. In the first phase, a non-trained annotator (MF) adjusted the predictions. The annotations were then finalized by a great ape behavior and signaling expert (EG) [<xref ref-type="bibr" rid="c48">48</xref>], ensuring high-quality labels.</p></list-item>
</list>
</sec>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Evaluation Metrics</title>
<sec id="s3b1">
<label>3.2.1</label>
<title>Evaluation Metrics for Pose Estimation</title>
<sec id="s3b1a">
<title>Mean Average Euclidean Error (MAE)</title>
<p>MAE is the primary evaluation metric in DeepLabCut and measures the average Euclidean distance between the ground-truth labels <inline-formula><inline-graphic xlink:href="559236v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and the model predictions (<italic>y</italic> ∈ ℝ<sup>2</sup>):
<disp-formula id="eqn1">
<graphic xlink:href="559236v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<p>Here, <italic>J</italic> is the number of images (e.g., 89,223 in OMC) and <italic>K</italic> is the number of keypoints (e.g., 17 in OMC). Refer to <xref rid="figa1" ref-type="fig">Fig. A1</xref> in <xref rid="app1" ref-type="app">Appendix A</xref> for a visual comparison of predictions and MAE examples.</p>
</sec>
<sec id="s3b1b">
<title>Percentage of Correct Keypoint - nasal dorsum (PCK)</title>
<p>PCK measures the percentage of keypoints that fall within a specified distance of the ground-truth. PCK is computed as:
<disp-formula id="eqn2">
<graphic xlink:href="559236v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, <italic>δ</italic>(·) is an indicator function that outputs 1 when the condition is met and 0 otherwise. The threshold distance <italic>ϵ</italic> is equal to the nasal dorsum length, defined as the distance between the midpoint of the eyes and the tip of the nose calculated for each frame as:
<disp-formula id="eqn3">
<graphic xlink:href="559236v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<p>See <xref rid="figb2" ref-type="fig">Fig. B2</xref> in <xref rid="app2" ref-type="app">Appendix B</xref> for an example of nasal dorsum calculation.</p>
</sec>
<sec id="s3b1c">
<title>Normalized Error Rate (NMER)</title>
<p>NMER quantifies the mean normalized error by dividing the raw pixel distance between the predicted and ground-truth keypoints by the square root of the bounding box area [<xref ref-type="bibr" rid="c34">34</xref>]:
<disp-formula id="eqn4">
<graphic xlink:href="559236v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, <italic>w</italic> and <italic>h</italic> denote the width and height of the bounding box, respectively.</p>
<p>Both PCK and NMER are size- and distance-normalized metrics, unlike MAE, making them more robust for evaluating diverse scenarios.</p>
</sec>
</sec>
<sec id="s3b2">
<label>3.2.2</label>
<title>Evaluation Metrics for Action Recognition</title>
<p>Similar to [<xref ref-type="bibr" rid="c11">11</xref>], we use the three following action recognition metrics:</p>
<sec id="s3b2a">
<title>Top-1 Accuracy</title>
<p>Top-1 Accuracy measures the percentage of samples where the model’s highest-confidence prediction matches the ground-truth label.
<disp-formula id="eqn5">
<graphic xlink:href="559236v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Where,</p>
<list list-type="bullet">
<list-item><p><italic>N</italic> : Total number of predictions/instances</p></list-item>
<list-item><p><italic>y</italic><sub><italic>i</italic></sub>: True class for the <italic>i</italic>-th instance</p></list-item>
<list-item><p><inline-formula><inline-graphic xlink:href="559236v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> : Predicted class for the <italic>i</italic>-th instance (the class with the highest confidence score)</p></list-item>
<list-item><p><italic>δ</italic>(·): Indicator function, returns 1 if the condition is true, 0 otherwise</p></list-item>
</list>
</sec>
<sec id="s3b2b">
<title>Top-3 Accuracy</title>
<p>Top-3 Accuracy measures the percentage of samples where the ground-truth label appears within the top three predictions of the model.
<disp-formula id="eqn6">
<graphic xlink:href="559236v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Where,</p>
<list list-type="bullet">
<list-item><p><inline-formula><inline-graphic xlink:href="559236v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> : set of top-3 predicted classes for the <italic>i</italic>-th instance, ranked by confidence scores</p></list-item>
</list>
</sec>
<sec id="s3b2c">
<title>Mean Class Accuracy (MCA)</title>
<p>MCA calculates the average accuracy across all classes, giving equal weight to each class irrespective of sample size. See <xref rid="fig10" ref-type="fig">Fig. 10</xref> for details.
<disp-formula id="eqn7">
<graphic xlink:href="559236v2_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Where,</p>
<list list-type="bullet">
<list-item><p><italic>C</italic>: Total number of classes</p></list-item>
<list-item><p>TP<sub><italic>i</italic></sub>: True positives for class <italic>i</italic> (number of instances from class <italic>i</italic> correctly predicted as class <italic>i</italic>)</p></list-item>
<list-item><p>FN<sub><italic>i</italic></sub>: False negatives for class <italic>i</italic> (number of instances from class <italic>i</italic> not predicted as such)</p></list-item>
</list>
</sec>
</sec>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Methods for Pose Estimation</title>
<sec id="s3c1">
<title>Data Preprocessin</title>
<p>For our experiment, we utilized all annotated data from the OpenMonkeyChallenge (OMC) dataset, encompassing 26 species and 17 keypoints, including invisible ones. Due to OMC’s large size, a 5-fold cross-validation approach was chosen for model benchmarking.</p>
</sec>
<sec id="s3c2">
<title>Within-domain Models Benchmarking</title>
<p>We evaluated the within-domain performance of nine pose estimation models, including three ResNet architectures (RN-50, RN-101, RN-152) and six EfficientNet variants (B0, B1, B2, B3, B5, B6) [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>]. All models were pretrained on ImageNet [<xref ref-type="bibr" rid="c51">51</xref>] and then trained on OMC for 40,000 iterations, a duration estimated as sufficient for loss convergence in preliminary tests using the largest network (EfficientNet-B6).</p>
<p>Default training hyperparameters and augmentation settings were used except for the batch size, which was set to 16 (the maximum fitting into the GPU memory for EfficientNet-B6 on an NVIDIA A100 40GB). The learning rate schedule followed the defaults: 1.0e-04 until iteration 7,500, 5.0e-05 until iteration 12,000, and 1.0e-05 for the remainder. We performed 5-fold cross-validation, splitting the dataset into 80% training and 20% testing subsets, ensuring that all 89,223 images were included in the test set once. All models were trained remotely on the Google Cloud platform with NVIDIA A100 (40GB) or V100 (16GB) GPUs, using ASBAR’s GUI (see <xref rid="figd4" ref-type="fig">Fig. D4</xref> in <xref rid="app4" ref-type="app">Appendix D</xref> for examples of elements).</p>
<p>Model snapshots were saved every 5,000 iterations and evaluated on the test set. Each model’s eight snapshots were evaluated across all five folds (8× 5 = 40 evaluations per model). To handle the computational load, we customized DLC’s evaluation pipeline to batch data processing and limit evaluations to the test set. While this modification was not integrated into the released framework, the default DLC evaluation method remains available in ASBAR’s GUI for reproducibility.</p>
</sec>
<sec id="s3c3">
<title>Model Shortlisting</title>
<p>Since the ultimate goal was to predict keypoints on the out-of-domain <italic>behavior</italic> dataset, we shortlisted four models based on their robustness and generalization potential: (i) ResNet-152: Best-performing within-domain model; (ii) ResNet-50: Widely used and popular among researchers; (iii) EfficientNet-B6: Demonstrated strong generalization to out-of-domain data in prior studies [<xref ref-type="bibr" rid="c34">34</xref>]; (iv) EfficientNet-B3: Despite lower within-domain accuracy, it balances strong out-of-domain generalization [<xref ref-type="bibr" rid="c34">34</xref>] with low computational cost (1.8G FLOPs versus 4.1G for ResNet-50 and 11G for ResNet-152).</p>
<p>The shortlisted models were retrained on the full OMC dataset (89,223 images) with no test set. Training was extended to 100,000 iterations to accommodate the larger training dataset. All other hyperparameters were unchanged. Model snapshots were saved every 5,000 iterations, producing 20 snapshots per model for evaluation.</p>
</sec>
</sec>
<sec id="s3d">
<label>3.4</label>
<title>Methods for Pose Extraction</title>
<p>After evaluating pose estimation performance (see Sect. 4.1 for results), ResNet-152 was selected for pose extraction. This model was applied to all frames in the <italic>behavior</italic> dataset to predict keypoint candidates.</p>
<p>Given the visual differences between the <italic>pose</italic> and <italic>behavior</italic> datasets, the pose estimation model’s confidence threshold was lowered to 10<sup><italic>−</italic>6</sup> to maximize keypoint candidate generation and minimize cases of “no prediction.” Skeletal poses were extracted by filtering these candidates to retain only the 17 keypoints with the highest confidence scores within the annotated bounding boxes.</p>
</sec>
<sec id="s3e">
<label>3.5</label>
<title>Methods for Behavior Recognition</title>
<sec id="s3e1">
<title>Data Preprocessing</title>
<p>The methodology proposed by [<xref ref-type="bibr" rid="c11">11</xref>] was followed for data sampling. A minimum threshold of 72 consecutive frames (equivalent to 3 seconds) exhibiting the same behavior was set to ensure the inclusion of prolonged and meaningful behavioral patterns. Selected video clips were divided into samples of 20 consecutive frames, with no gaps or overlaps between samples.</p>
<p>The dataset was randomly split at the video level into training, validation, and testing subsets, using a 70-15-15 distribution. The pose extraction output was formatted and stored as triplets of (<italic>x, y</italic>, confidence) coordinates for each keypoint.</p>
</sec>
<sec id="s3e2">
<title>Model Training</title>
<p>A PoseConv3D model [<xref ref-type="bibr" rid="c17">17</xref>] with a ResNet3dSlowOnly backbone and an I3D classification head was selected for behavior recognition. This architecture was chosen for its strong performance on NTU60-XSub [<xref ref-type="bibr" rid="c20">20</xref>], a benchmark dataset for human action recognition, as reported by [<xref ref-type="bibr" rid="c17">17</xref>].</p>
<p>To adhere strictly to a skeleton-based approach, the model was trained exclusively on pose-estimated data, without incorporating the multimodal RGB+Pose capability. Only joint keypoints (excluding limbs) were used, with a sigma value of 0.6. Keypoint confidence scores were not considered, given the low confidence threshold during pose extraction.</p>
<p>The model weights were initialized from pretraining on the FineGym dataset [<xref ref-type="bibr" rid="c52">52</xref>]. Training was conducted for 50 epochs using two NVIDIA RTX 2080 Ti GPUs (2 × 11GB). A class-balanced focal loss [<xref ref-type="bibr" rid="c46">46</xref>] was employed to address the imbalanced class distribution (<italic>β</italic> = 0.992, <italic>γ</italic> = 2). Other hyperparameter choices included: batch size of 32; initial learning rate of 0.005 (with momentum of 0.9 and cosine annealing); weight decay of 0.01 and a dropout ratio of 0.8 (i.e. strong regularization to avoid overfitting). Other hyperparameters and augmentation settings followed those used in [<xref ref-type="bibr" rid="c17">17</xref>].</p>
</sec>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Results</title>
<p>This section details the experimental results. First, we present the evaluation of pose estimation models, including both within-domain and out-of-domain results, leading to the selection of an optimal model for pose extraction (Section 4.1). Additional insights into model performance at keypoint and species levels are provided in Section 4.2. Finally, skeleton-based behavior classification results are reported and compared to existing studies (Section 4.3).</p>
<sec id="s4a">
<label>4.1</label>
<title>Results of Pose Estimation</title>
<sec id="s4a1">
<title>Within-domain evaluation</title>
<p>We compared the performance of all nine models after 40,000 iterations by constructing 95% confidence intervals for MAE using a t-distribution (<italic>α</italic> = 0.025, <italic>ν</italic>= 4), given the small sample size from cross-validation. The results (as seen in <xref rid="fig5" ref-type="fig">Figure 5</xref>) show that: (i) ResNet-152 achieved the best performance (14.05 ± 0.199), statistically outperforming other models; (ii) ResNet-101 ranked second (14.334 ± 0.080); (iii) EfficientNet-B5 (14.958±0.299), EfficientNet-B6 (14.981 ± 0.288), and ResNet-50 (15.098 ± 0.12) had overlapping confidence intervals, making their performances statistically indistinguishable; (iv) EfficientNet-B3 (15.455 ± 0.097) and EfficientNet-B2 (15.519 ± 0.25) performed slightly worse; (v) EfficientNet-B1 (16.031 ± 0.167) and EfficientNet-B0 (16.631 ± 0.546) exhibited the highest error rates.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5:</label>
<caption><title>Final within-domain model performance.</title>
<p>Mean and 95% confidence intervals of the MAE (in pixels) after 40,000 iterations (end of training). Disjoint confidence intervals indicate statistically significant differences. ResNet-152 demonstrates significantly better performance compared to all other models in this task.</p></caption>
<graphic xlink:href="559236v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In addition, the performance of each model during training is visualized through deviation charts of their snapshot variants, showing the mean and standard deviation of MAE and PCK in <xref rid="fig6" ref-type="fig">Figure 6</xref>.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig. 6:</label>
<caption><title>Model’s relative performance throughout ‘within-domain’ training.</title>
<p>The mean ± std of the Mean Average Euclidean Error (MAE) in pixels (<italic>left</italic>, lower is better) and percentage of correct keypoint (PCK nasal dorsum) (<italic>right</italic>, higher is better) for all nine model variations. Evaluation results of 5-fold cross-validation on test set data, at every 5,000 iterations.</p></caption>
<graphic xlink:href="559236v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4a2">
<title>Out-of-domain Evaluation</title>
<p>Each saved model snapshot was tested on the PanAf500-Pose ground-truth annotations. To reduce the influence of noisy predictions, the minimum confidence threshold for pose prediction was maintained at the default value of 0.1.</p>
<p>Our results indicate that ResNet-152 generalizes best to out-of-domain data (<xref rid="fig7" ref-type="fig">Fig 7</xref>), achieving the highest overall PCK-nasal dorsum of 54.17% across all keypoints (<italic>n</italic> = 5, 440) and the lowest normalized error rate (NMER) of 10.19%.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Fig. 7:</label>
<caption><title>Out-of-Domain performance on PanAf500-Pose.</title>
<p>Models are evaluated using two metrics that account for the animal’s relative size and distance: PCK nasal dorsum (<italic>left</italic>, higher is better) and normalized error rate (<italic>right</italic>, lower is better). ResNet-152 demonstrates superior performance in predicting great ape poses in their natural habitat. Vertical and horizontal dashed lines indicate the maximum and minimum values, along with the corresponding number of iterations. ResNet-152 at 60,000 iterations is selected for pose extraction.</p></caption>
<graphic xlink:href="559236v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4a3">
<title>Pose extraction</title>
<p>Based on ResNet-152’s performance at 60,000 iterations—achieving a detection rate of 53.9% (very close to the highest observed value) and a minimal NMER of 10.19%—this snapshot was selected as the final keypoint detector for pose extraction.</p>
</sec>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Alternative Performance Evaluation</title>
<p>To gain deeper insights into the final pose estimation model’s performance, we evaluated it across keypoints and species using both OMC and PanAf500-Pose datasets. A confidence threshold of 0.1 was applied throughout.</p>
<sec id="s4b1">
<label>4.2.1</label>
<title>Keypoint Detection Rate</title>
<p>Our analysis revealed that not all keypoints are equally detectable for non-human primates. Detection rates, computed as the cumulative distribution of predicted distances in pixels (<xref rid="fig8" ref-type="fig">Fig 8</xref>), highlight the following trends at test time for OMC (<italic>n</italic> = 89, 223 × 17 = 1, 516, 791):</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Fig. 8:</label>
<caption><title>Keypoint detection rate on within-domain vs. out-of-domain test data.</title>
<p>The keypoint detection rate, defined as the percentage of keypoints detected within a given pixel distance, is shown for OMC (<italic>left</italic>) and PanAf500-Pose (<italic>right</italic>). For example, within a distance of 10 pixels or less, the nose is detected in approximately 95% of the 89,223 images in OMC. In contrast, the tail is detected within the same distance in only about 38% of cases.</p></caption>
<graphic xlink:href="559236v2_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<list list-type="bullet">
<list-item><p>Facial features (e.g., nose, eyes) are the easiest to detect.</p></list-item>
<list-item><p>Keypoints on the head are more accurately predicted than those below the neck.</p></list-item>
<list-item><p>Upper body limbs (e.g., wrists, elbows, shoulders) are detected more reliably than lower body limbs (e.g., ankles, knees).</p></list-item>
<list-item><p>Limb extremities (e.g., wrists, ankles) are predicted more accurately than proximal keypoints (e.g., elbows, knees).</p></list-item>
<list-item><p>Hip and tail positions are the most challenging to predict accurately.</p></list-item>
</list>
<p>These trends can be attributed to (i) the distinct visual features of facial keypoints, (ii) the prominence of heads and limb extremities, and (iii) the occlusion and ambiguity of lower body parts and tails.</p>
<p>Comparing results from PanAf500-Pose (<italic>n</italic> = 320 × 17 = 5, 440) shows a similar S-shaped distribution, indicating the model’s robustness to domain shifts. However, lower detection rates for specific keypoints may result from the precise annotations in PanAf500-Pose compared to OMC, where annotations are occasionally inconsistent (e.g., labeling fingers instead of wrists or toes instead of ankles).</p>
</sec>
<sec id="s4b2">
<label>4.2.2</label>
<title>Per-Species Accuracy</title>
<p>To evaluate species-specific performance, we analyzed chimpanzees (<italic>n</italic> = 6, 190) and gorillas (<italic>n</italic> = 1, 777) in OMC using the normalized error rate (NMER) and 95% confidence intervals (<xref rid="fig9" ref-type="fig">Fig 9</xref>).</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Fig. 9:</label>
<caption><title>Normalized error rate for chimpanzees and gorillas in OMC.</title>
<p>Mean and 95% confidence intervals for the normalized error rate (NMER). Disjoint confidence intervals indicate statistical significance. The model demonstrates lower error rates for all gorilla keypoints, suggesting higher prediction accuracy for this species.</p></caption>
<graphic xlink:href="559236v2_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig10" position="float" fig-type="figure">
<label>Fig. 10:</label>
<caption><title>Normalized confusion matrix of behavior recognition.</title>
<p>For each true behavior label (rows), the percentage of predictions across all predicted behaviors (columns) is shown. For instance, 51% of samples labeled as ‘standing’ were correctly classified, while 16% were misclassified as ‘walking’ and 33% as ‘sitting.’ The diagonal cells represent the per-class accuracy, and their average corresponds to the Mean Class Accuracy (MCA) metric. A perfect classification model would yield a normalized confusion matrix with values of 1 on the diagonal and 0 elsewhere.</p></caption>
<graphic xlink:href="559236v2_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Results indicate a statistically significant dependence on species, with gorillas consistently showing lower error rates than chimpanzees across all keypoints. This suggests that the model detects keypoints more accurately for gorillas. Additional species-level analysis is provided in <xref rid="figc3" ref-type="fig">Fig C3</xref> in <xref rid="app3" ref-type="app">Appendix C</xref>.</p>
</sec>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Results of Behavior Recognition</title>
<p>The results of behavior classification, summarized in <xref rid="tbl1" ref-type="table">Table 1</xref>, demonstrate the successful application of our skeleton-based action recognition pipeline for animals. In the context of automating the recognition of great ape behaviors in the wild—a highly relevant yet challenging task—our approach achieves accuracy comparable to other video-based techniques, such as those reported in [<xref ref-type="bibr" rid="c11">11</xref>].</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Performance comparison with previous studies.</title>
<p>Comparison of Top-1 Accuracy, Top-3 Accuracy, and Mean Class Accuracy (MCA) between ASBAR and previous video-based methods. ASBAR achieves comparable performance to video-based approaches across all metrics.</p></caption>
<graphic xlink:href="559236v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>To the best of our knowledge, this is the first use of a skeleton-based method for classifying great ape behaviors. Notably, the entire behavior dataset after pose extraction (i.e., the input features for the behavior classifier) requires less than 60 MB of storage in text format—approximately 20 times smaller than the storage requirements of the same dataset using a video-based approach. For ethologists working in the field, where computational, storage, and transfer resources are often limited, this represents a significant improvement without sacrificing performance in behavior recognition.</p>
<p>The normalized confusion matrix of the final behavior recognition model is shown in Fig.10. The model tends to overfit on <italic>head</italic> behavior classes, which have more samples in the dataset (see Sect.2.1). For instance, the model frequently overpredicts ‘walking,’ the second most represented class, at the expense of <italic>tail</italic> classes. The false positive rates (i.e., misclassification rates) for ‘walking’ on ‘sitting on back,’ ‘climbing up,’ ‘climbing down,’ ‘running,’ and ‘camera interaction’ are 0.74, 0.42, 0.50, 0.89, and 0.40, respectively.</p>
<p>Interestingly, the other two <italic>head</italic> classes—’standing’ and ‘sitting’—show near-zero false positive rates for the same <italic>tail</italic> classes. This discrepancy may be explained by the static nature of ‘standing’ and ‘sitting,’ which involve stationary poses, compared to the dynamic movements in ‘walking’ and most <italic>tail</italic> classes, where pose estimation accuracy may be lower.</p>
<p>Additionally, the true positive rates for ‘sitting on back’ and ‘running’ (i.e., their per-class accuracy) are extremely low, at 0.11 each. Both are predominantly misclassified as ‘walking.’ This likely stems from the similarity in skeletal poses across these behaviors, making it challenging for the model to differentiate between them using only skeleton data, particularly given the limited sample sizes for these classes.</p>
</sec>
</sec>
<sec id="s5">
<label>5</label>
<title>Discussion</title>
<p>Despite the growing availability of open-source resources, such as large-scale animal pose datasets and machine learning toolboxes for pose estimation and human skeleton-based action recognition, their integration for animal behavior recognition—particularly in natural settings—remains largely unexplored. With ASBAR, a framework combining animal pose estimation and skeleton-based action recognition, we provide a comprehensive data and model pipeline, methodology, and GUI to assist researchers in automatically classifying animal behaviors via pose estimation. We hope these resources will become valuable tools for advancing the understanding of animal behavior within the research community.</p>
<p>To illustrate ASBAR’s capabilities, we applied it to the challenging task of classifying great ape behaviors in their natural habitat. Our skeleton-based approach achieved accuracy comparable to previous video-based studies for Top-K and Mean Class Accuracies. Additionally, by reducing the input size of the action recognition model by a factor of approximately 20 compared to video-based methods, our approach requires significantly less computational power, storage space, and data transfer resources. These qualities make ASBAR particularly suitable for field researchers working in resource-constrained environments.</p>
<p>Our framework and results are built on the foundation of shared and opensource materials, including tools like DeepLabCut [<xref ref-type="bibr" rid="c35">35</xref>], MMAction2 [<xref ref-type="bibr" rid="c41">41</xref>], and datasets such as OpenMonkeyChallenge [<xref ref-type="bibr" rid="c31">31</xref>] and PanAf500 [<xref ref-type="bibr" rid="c11">11</xref>]. This underscores the importance of making resources publicly available, especially in primatology, where data scarcity often impedes progress in AI-assisted methodologies. We strongly encourage researchers with large annotated video datasets to make them publicly accessible to foster interdisciplinary collaboration and further advancements in animal behavior research.</p>
<sec id="s5a">
<label>5.1</label>
<title>Challenges and Future Directions</title>
<p>While our results are promising, there are areas for improvement in both pose estimation and action recognition tasks.</p>
<sec id="s5a1">
<title>Pose Estimation</title>
<p>Out-of-domain PCK metrics for pose estimation hovered just above 0.5, indicating that nearly half of the predicted keypoints were outside the acceptable range of the ground-truth coordinates. Accurate pose estimation is critical for downstream behavior classification. Future work could address this by finetuning the pose estimation model on the <italic>behavior</italic> dataset before pose extraction. Additionally, training on more specific datasets, such as OpenApePose [<xref ref-type="bibr" rid="c32">32</xref>], could improve performance. Techniques to reduce the domain gap between <italic>pose</italic> and <italic>behavior</italic> datasets [<xref ref-type="bibr" rid="c53">53</xref>] or leveraging pseudo-labels for semi-supervised learning [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c54">54</xref>, <xref ref-type="bibr" rid="c55">55</xref>] could also enhance generalization.</p>
<p>Interestingly, EfficientNet architectures performed worse than ResNet-152 in both within-domain and out-of-domain evaluations, contrary to prior results in animal pose estimation [<xref ref-type="bibr" rid="c34">34</xref>]. This discrepancy may stem from suboptimal hyperparameter tuning (e.g., fixed learning rate schedules instead of cosine decay) for EfficientNet models. Future studies should optimize hyperparameters individually for each architecture to fully explore their potential.</p>
</sec>
<sec id="s5a2">
<title>Behavior Recognition</title>
<p>While our skeleton-based pipeline achieved comparable results to previous studies, the overall accuracy remains relatively low, which could limit its practical deployment in the field. Comparisons to human-centric studies, where abundant datasets for both pose estimation and action recognition have led to higher performance [<xref ref-type="bibr" rid="c17">17</xref>], highlight the need for additional public datasets [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c56">56</xref>] to drive progress in AI-assisted animal behavior research.</p>
<p>From an algorithmic perspective, using keypoint detection as pose scoremaps rather than compressing them into (<italic>x, y, c</italic>) triplets could improve performance, particularly when pose predictions are less accurate [<xref ref-type="bibr" rid="c17">17</xref>]. Incorporating RGB-Pose dual-modality could further enhance classification accuracy, especially for behaviors with similar skeletal motion, such as ‘walking,’ ‘running,’ and ‘sitting on back.’</p>
</sec>
</sec>
</sec>
<sec id="s6">
<label>6</label>
<title>Conclusion</title>
<p>This study demonstrates the practical utility and relevance of skeleton-based action recognition approaches in animal behavior research. We hope the tools, methodologies, and insights presented here will inspire further applications of skeleton-based techniques to study a broader range of behaviors and animal species. Future advancements in pose estimation, action recognition, and dataset availability will undoubtedly enhance the impact of such approaches in ethology and beyond.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We extend our sincere gratitude to the team behind the Pan African Programme: ‘The Cultured Chimpanzee’, along with their partners, for granting us permission to use their data for this study. For access to the videos from the dataset, please reach out directly with the copyright holder Pan African Programme at <ext-link ext-link-type="uri" xlink:href="http://panafrican.eva.mpg.de">http://panafrican.eva.mpg.de</ext-link>. In particular, we would like to thank H. Kuehl, C. Boesch, M. Arandjelovic, and P. Dieguez. Further acknowledgments go to: K. Corogenes, E. Normand, V. Vergnes, A. Meier, J. Lapuente, D. Dowd, S. Jones, V. Leinert, E. Wessling, H. Eshuis, K. Langergraber, S. Angedakin, S. Marrocoli, K. Dierks, T. C. Hicks, J-Hart, K. Lee, M. Murai and the team at Chimp&amp;See.</p>
<p>The work that allowed for the collection of the PanAf500 dataset was made possible due to the generous support from the Max Planck Society, Max Planck Society Innovation Fund, and Heinz L. Krekeler. By extension, we also wish to thank: Foundation Ministre de la Recherche Scientifique, and Ministre des Eaux et Forêts in Cote d’Ivoire; Institut Congolais pour la Conservation de la Nature and Ministre de la Recherche Scientifique in DR Congo; Forestry Development Authority in Liberia; Direction des Eaux, Forêts Chasses et de la Conservation des Sols in Senegal; and Uganda National Council for Science and Technology, Uganda Wildlife Authority, and National Forestry Authority in Uganda.</p>
<p>In addition, we would like to thank the team at NCCR Evolving Language and in particular Guanghao You, for allowing us to use their computational platform.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ziegler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sturman</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Bohacek</surname>, <given-names>J.</given-names></string-name></person-group>: <article-title>Big behavior: challenges and opportunities in a new era of deep behavior profiling</article-title>. <source>Neuropsychopharmacology</source> <volume>46</volume>(<issue>1</issue>), <fpage>33</fpage>–<lpage>44</lpage> (<year>2021</year>)</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>D.J.</given-names></string-name>, <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name></person-group>: <article-title>Toward a Science of Computational Ethology</article-title>. <source>Neuron</source> <volume>64</volume>(<issue>1</issue>), <fpage>18</fpage>–<lpage>31</lpage> (<year>2014</year>) <pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.005</pub-id></mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Hinton</surname>, <given-names>G.E.</given-names></string-name></person-group>: <article-title>ImageNet classification with deep convolutional neural networks</article-title>. <source>Communications of the ACM</source> <volume>60</volume>(<issue>6</issue>), <fpage>84</fpage>–<lpage>90</lpage> (<year>2017</year>) <pub-id pub-id-type="doi">10.1145/3065386</pub-id></mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Simonyan</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zisserman</surname>, <given-names>A.</given-names></string-name></person-group>: <article-title>Two-Stream Convolutional Networks for Action Recognition in Videos</article-title>. In: <conf-name>Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1. NIPS’14</conf-name>, pp. <fpage>568</fpage>–<lpage>576</lpage>. <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA, USA</publisher-loc> (<year>2014</year>)</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Tran</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bourdev</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Fergus</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Torresani</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Paluri</surname>, <given-names>M.</given-names></string-name></person-group>: <article-title>Learning Spatiotemporal Features with 3D Convolutional Networks</article-title>. In: <conf-name>2015 IEEE International Conference on Computer Vision (ICCV)</conf-name>. <publisher-name>IEEE</publisher-name> (<year>2015</year>). <pub-id pub-id-type="doi">10.1109/iccv.2015.510</pub-id>. 10.1109/iccv.2015.510</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Carreira</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zisserman</surname>, <given-names>A.</given-names></string-name></person-group>: <article-title>Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</article-title>. In: <conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>. <publisher-name>IEEE</publisher-name> (<year>2017</year>). <pub-id pub-id-type="doi">10.1109/cvpr.2017.502</pub-id>. 10.1109/cvpr.2017.502</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Tran</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Torresani</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ray</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Paluri</surname>, <given-names>M.</given-names></string-name></person-group>: <article-title>A closer look at spatiotemporal convolutions for action recognition</article-title>. In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>6450</fpage>–<lpage>6459</lpage> (<year>2018</year>)</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Feichtenhofer</surname>, <given-names>C.</given-names></string-name></person-group>: <article-title>X3D: Expanding Architectures for Efficient Video Recognition</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name> (<year>2020</year>)</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stern</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>C.-H.</given-names></string-name></person-group>: <article-title>Analyzing animal behavior via classifying each video frame using convolutional neural networks</article-title>. <source>Scientific reports</source> <volume>5</volume>(<issue>1</issue>), <fpage>14351</fpage> (<year>2015</year>)</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="report"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Swetha</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Shah</surname>, <given-names>M.</given-names></string-name></person-group>: <source>Wildlife Action Recognition Using Deep Learning</source>. <publisher-name>University of Central Florida</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://www.crcv.ucf.edu/wp-content/uploads/2018/11/Weining_L_Report.pdf">https://www.crcv.ucf.edu/wp-content/uploads/2018/11/Weining_L_Report.pdf</ext-link> <year>2018</year></mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Sakib</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Burghardt</surname>, <given-names>T.</given-names></string-name></person-group>: <article-title>Visual Recognition of Great Ape Behaviours in the Wild</article-title>. In: <conf-name>Proc. ICPR Workshop on VAIB</conf-name> (<year>2021</year>)</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>J.</given-names></string-name></person-group>: <article-title>Action Recognition Using a Spatial-Temporal Network for Wild Felines</article-title>. <source>Animals</source> <volume>11</volume>(<issue>2</issue>), <fpage>485</fpage> (<year>2021</year>) <pub-id pub-id-type="doi">10.3390/ani11020485</pub-id></mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bohnslav</surname>, <given-names>J.P.</given-names></string-name>, <string-name><surname>Wimalasena</surname>, <given-names>N.K.</given-names></string-name>, <string-name><surname>Clausing</surname>, <given-names>K.J.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>Y.Y.</given-names></string-name>, <string-name><surname>Yarmolinsky</surname>, <given-names>D.A.</given-names></string-name>, <string-name><surname>Cruz</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kashlan</surname>, <given-names>A.D.</given-names></string-name>, <string-name><surname>Chiappe</surname>, <given-names>M.E.</given-names></string-name>, <string-name><surname>Orefice</surname>, <given-names>L.L.</given-names></string-name>, <string-name><surname>Woolf</surname>, <given-names>C.J.</given-names></string-name>, <string-name><surname>Harvey</surname>, <given-names>C.D.</given-names></string-name></person-group>: <article-title>DeepEthogram, a machine learning pipeline for supervised behavior classification from raw pixels</article-title>. <source>eLife</source> <volume>10</volume>, <elocation-id>63377</elocation-id> (<year>2021</year>)</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marks</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Jin</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Sturman</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Ziegler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kollmorgen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Mante</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Bohacek</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yanik</surname>, <given-names>M.F.</given-names></string-name></person-group>: <article-title>Deep-learning-based identification, tracking, pose estimation and behaviour classification of interacting primates and mice in complex environments</article-title>. <source>Nature machine intelligence</source> <volume>4</volume>(<issue>4</issue>), <fpage>331</fpage>–<lpage>340</lpage> (<year>2022</year>)</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bain</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nagrani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schofield</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Berdugo</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bessa</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Owen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hockings</surname>, <given-names>K.J.</given-names></string-name>, <string-name><surname>Matsuzawa</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hayashi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Biro</surname>, <given-names>D.</given-names></string-name>, <etal>et al.</etal></person-group>: <article-title>Automated audiovisual behavior recognition in wild primates</article-title>. <source>Science advances</source> <volume>7</volume>(<issue>46</issue>) (<year>2021</year>)</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>J.</given-names></string-name></person-group>: <article-title>A comparative review of graph convolutional networks for human skeleton-based action recognition</article-title>. <source>Artificial Intelligence Review</source>, <fpage>1</fpage>–<lpage>31</lpage> (<year>2022</year>)</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Duan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>B.</given-names></string-name></person-group>: <article-title>Revisiting skeleton-based action recognition</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>2969</fpage>–<lpage>2978</lpage> (<year>2022</year>)</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Cao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Simon</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wei</surname>, <given-names>S.-E.</given-names></string-name>, <string-name><surname>Sheikh</surname>, <given-names>Y.</given-names></string-name></person-group>: <article-title>Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields</article-title>. In: <conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>. <publisher-name>IEEE</publisher-name> (<year>2017</year>). <pub-id pub-id-type="doi">10.1109/cvpr.2017.143</pub-id>. 10.1109/cvpr.2017.143</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hayden</surname>, <given-names>B.Y.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>H.S.</given-names></string-name>, <string-name><surname>Zimmermann</surname>, <given-names>J.</given-names></string-name></person-group>: <article-title>Automated pose estimation in primates</article-title>. <source>American journal of primatology</source> <volume>64</volume>(<issue>10</issue>), <fpage>23348</fpage> (<year>2022</year>)</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shahroudy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Perez</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Duan</surname>, <given-names>L.-Y.</given-names></string-name>, <string-name><surname>Kot</surname>, <given-names>A.C.</given-names></string-name></person-group>: <article-title>NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <volume>42</volume>(<issue>10</issue>), <fpage>2684</fpage>–<lpage>2701</lpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bala</surname>, <given-names>P.C.</given-names></string-name>, <string-name><surname>Eisenreich</surname>, <given-names>B.R.</given-names></string-name>, <string-name><surname>Yoo</surname>, <given-names>S.B.M.</given-names></string-name>, <string-name><surname>Hayden</surname>, <given-names>B.Y.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>H.S.</given-names></string-name>, <string-name><surname>Zimmermann</surname>, <given-names>J.</given-names></string-name></person-group>: <article-title>Automated markerless pose estimation in freely moving macaques with OpenMonkeyStudio</article-title>. <source>Nature communications</source> <volume>11</volume>(<issue>1</issue>), <fpage>4560</fpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sturman</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Ziegler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Schläppi</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Akyol</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Privitera</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Slominski</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Grimm</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Thieren</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zerbi</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Grewe</surname>, <given-names>B.</given-names></string-name>, <etal>et al.</etal></person-group>: <article-title>Deep learning-based behavioral analysis reaches human accuracy and is capable of outperforming commercial solutions</article-title>. <source>Neuropsychopharmacology</source> <volume>45</volume>(<issue>11</issue>), <fpage>1942</fpage>–<lpage>1952</lpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Jafari</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Kehtarnavaz</surname>, <given-names>N.</given-names></string-name></person-group>: <article-title>A survey of depth and inertial sensor fusion for human action recognition</article-title>. <source>Multimedia Tools and Applications</source> <volume>76</volume>, <fpage>4405</fpage>–<lpage>4425</lpage> (<year>2017</year>)</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Szegedy</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zaremba</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Bruna</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Erhan</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Goodfellow</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Fergus</surname>, <given-names>R.</given-names></string-name></person-group>: <article-title>Intriguing properties of neural networks</article-title>. <source>arXiv</source> preprint arXiv:<pub-id pub-id-type="arxiv">1312.6199</pub-id> (<year>2013</year>)</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Madry</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Makelov</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Tsipras</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Vladu</surname>, <given-names>A.</given-names></string-name></person-group>: <article-title>Towards deep learning models resistant to adversarial attacks</article-title>. <source>arXiv</source> preprint arXiv:<pub-id pub-id-type="arxiv">1706.06083</pub-id> (<year>2017</year>)</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Han</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Reily</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Hoff</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name></person-group>: <article-title>Space-time representation of people based on 3D skeletal data: A review</article-title>. <source>Computer Vision and Image Understanding</source> <volume>156</volume>, <fpage>85</fpage>–<lpage>105</lpage> (<year>2017</year>)</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T.D.</given-names></string-name>, <string-name><surname>Shaevitz</surname>, <given-names>J.W.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group>: <article-title>Quantifying behavior to understand the brain</article-title>. <source>Nature Neuroscience</source> <volume>23</volume>(<issue>12</issue>), <fpage>1537</fpage>–<lpage>1549</lpage> (<year>2020</year>) <pub-id pub-id-type="doi">10.1038/s41593-020-00734-z</pub-id></mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Ng</surname>, <given-names>X.L.</given-names></string-name>, <string-name><surname>Ong</surname>, <given-names>K.E.</given-names></string-name>, <string-name><surname>Zheng</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Ni</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Yeo</surname>, <given-names>S.Y.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name></person-group>: <article-title>Animal kingdom: A large and diverse dataset for animal behavior understanding</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>19023</fpage>–<lpage>19034</lpage> (<year>2022</year>)</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Cao</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Fang</surname>, <given-names>H.-S.</given-names></string-name>, <string-name><surname>Shen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Tai</surname>, <given-names>Y.-W.</given-names></string-name></person-group>: <article-title>Cross-domain adaptation for animal pose estimation</article-title>. In: <conf-name>Proceedings of the IEEE/CVF International Conference on Computer Vision</conf-name>, pp. <fpage>9498</fpage>–<lpage>9507</lpage> (<year>2019</year>)</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Guan</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Tao</surname>, <given-names>D.</given-names></string-name></person-group>: <article-title>AP-10K: A benchmark for animal pose estimation in the wild</article-title>. <source>arXiv</source> preprint arXiv:<pub-id pub-id-type="arxiv">2108.12617</pub-id> (<year>2021</year>)</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bala</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mohan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bliss-Moreau</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Coleman</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Freeman</surname>, <given-names>S.M.</given-names></string-name>, <string-name><surname>Machado</surname>, <given-names>C.J.</given-names></string-name>, <string-name><surname>Raper</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zimmermann</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hayden</surname>, <given-names>B.Y.</given-names></string-name>, <etal>et al.</etal></person-group>: <article-title>OpenMonkey-Challenge: dataset and benchmark challenges for pose estimation of non-human primates</article-title>. <source>International Journal of Computer Vision</source> <volume>131</volume>(<issue>1</issue>), <fpage>243</fpage>–<lpage>258</lpage> (<year>2023</year>)</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desai</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Bala</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Richardson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Raper</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zimmermann</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hayden</surname>, <given-names>B.</given-names></string-name></person-group>: <article-title>OpenApePose, a database of annotated ape photographs for pose estimation</article-title>. <source>eLife</source> <volume>12</volume>, <elocation-id>86873</elocation-id> (<year>2023</year>)</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Labuguen</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Matsumoto</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Negrete</surname>, <given-names>S.B.</given-names></string-name>, <string-name><surname>Nishimaru</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Nishijo</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Takada</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Go</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Inoue</surname>, <given-names>K.-i.</given-names></string-name>, <string-name><surname>Shibata</surname>, <given-names>T.</given-names></string-name></person-group>: <article-title>MacaquePose: a novel “in the wild” macaque monkey pose dataset for markerless motion capture</article-title>. <source>Frontiers in behavioral neuroscience</source> <volume>14</volume>, <fpage>581154</fpage> (<year>2021</year>)</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Biasi</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Yuksekgonul</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rogers</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>M.W.</given-names></string-name></person-group>: <article-title>Pretraining boosts out-of-domain robustness for pose estimation</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</conf-name>, pp. <fpage>1859</fpage>–<lpage>1868</lpage> (<year>2021</year>)</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mamidanna</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cury</surname>, <given-names>K.M.</given-names></string-name>, <string-name><surname>Abe</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>V.N.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>M.W.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name></person-group>: <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature neuroscience</source> <volume>21</volume>(<issue>9</issue>), <fpage>1281</fpage>–<lpage>1289</lpage> (<year>2018</year>)</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>M.W.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name></person-group>: <article-title>Deep learning tools for the measurement of animal behavior in neuroscience</article-title>. <source>Current opinion in neurobiology</source> <volume>60</volume>, <fpage>1</fpage>–<lpage>11</lpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lauer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ye</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Menegas</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nath</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Rahman</surname>, <given-names>M.M.</given-names></string-name>, <string-name><surname>Santo</surname>, <given-names>V.D.</given-names></string-name>, <string-name><surname>Soberanes</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>V.N.</given-names></string-name>, <string-name><surname>Lauder</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Dulac</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name></person-group>: <article-title>Multi-animal pose estimation, identification and tracking with DeepLabCut</article-title>. <source>Nature Methods</source> <volume>19</volume>, <fpage>496</fpage>–<lpage>504</lpage> (<year>2022</year>)</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T.D.</given-names></string-name>, <string-name><surname>Aldarondo</surname>, <given-names>D.E.</given-names></string-name>, <string-name><surname>Willmore</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kislin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S.S.-H.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Shaevitz</surname>, <given-names>J.W.</given-names></string-name></person-group>: <article-title>Fast animal pose estimation using deep neural networks</article-title>. <source>Nature Methods</source> <volume>16</volume>(<issue>1</issue>), <fpage>117</fpage>–<lpage>125</lpage> (<year>2018</year>) <pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id></mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T.D.</given-names></string-name>, <string-name><surname>Tabris</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Matsliah</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>D.M.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ravindranath</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Papadoyannis</surname>, <given-names>E.S.</given-names></string-name>, <string-name><surname>Normand</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Deutsch</surname>, <given-names>D.S.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Z.Y.</given-names></string-name>, <etal>et al.</etal></person-group>: <article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title>. <source>Nature methods</source> <volume>19</volume>(<issue>4</issue>), <fpage>486</fpage>–<lpage>495</lpage> (<year>2022</year>)</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Karashchuk</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rupp</surname>, <given-names>K.L.</given-names></string-name>, <string-name><surname>Dickinson</surname>, <given-names>E.S.</given-names></string-name>, <string-name><surname>Walling-Bell</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sanders</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Azim</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Brunton</surname>, <given-names>B.W.</given-names></string-name>, <string-name><surname>Tuthill</surname>, <given-names>J.C.</given-names></string-name></person-group>: <article-title>Anipose: a toolkit for robust markerless 3D pose estimation</article-title>. <source>Cell reports</source> <volume>36</volume>(<issue>13</issue>) (<year>2021</year>)</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>MMAction2 Contributors: OpenMMLab’s Next Generation Video Understanding Toolbox and Benchmark</collab></person-group>. <ext-link ext-link-type="uri" xlink:href="https://github.com/open-mmlab/mmaction2">https://github.com/open-mmlab/mmaction2</ext-link> (<year>2020</year>)</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Feichtenhofer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Fan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Malik</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>K.</given-names></string-name></person-group>: <article-title>SlowFast Networks for Video Recognition</article-title>. In: <conf-name>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</conf-name>, pp. <fpage>6201</fpage>–<lpage>6210</lpage> (<year>2019</year>). <pub-id pub-id-type="doi">10.1109/ICCV.2019.00630</pub-id></mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>Max Planck Institute for Evolutionary Anthropology</collab></person-group>: <article-title>Pan African programme: The Cultured Chimpanzee</article-title>. <ext-link ext-link-type="uri" xlink:href="http://panafrican.eva.mpg.de/index.php">http://panafrican.eva.mpg.de/index.php</ext-link> (<year>2024</year>)</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Yang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Mirmehdi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Burghardt</surname>, <given-names>T.</given-names></string-name></person-group>: <article-title>Great ape detection in challenging jungle camera trap footage via attention-based spatial and temporal feature blending</article-title>. In: <conf-name>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</conf-name> (<year>2019</year>)</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Burghardt</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Mirmehdi</surname>, <given-names>M.</given-names></string-name></person-group>: <article-title>Dynamic curriculum learning for great ape detection in the wild</article-title>. <source>International Journal of Computer Vision</source> <volume>131</volume>(<issue>5</issue>), <fpage>1163</fpage>–<lpage>1181</lpage> (<year>2023</year>)</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Cui</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>T.-Y.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Belongie</surname>, <given-names>S.</given-names></string-name></person-group>: <article-title>Class-balanced loss based on effective number of samples</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>9268</fpage>–<lpage>9277</lpage> (<year>2019</year>)</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Sofroniew</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Lambert</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Evans</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Nunez-Iglesias</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bokota</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Winston</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Peña-Castellanos</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Yamauchi</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bussonnier</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Doncila Pop</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Can Solak</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Wadhwa</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Burt</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Buckley</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sweet</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Migas</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hilsenstein</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Gaifas</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bragantini</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rodríguez-Guerra</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Muñoz</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Freeman</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Boone</surname>, <given-names>P.</given-names></string-name>, R <string-name><surname>Lowe</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gohlke</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Royer</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Pierré</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Har-Gil</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>McGovern</surname>, <given-names>A.</given-names></string-name></person-group>: <article-title>napari: a multi-dimensional image viewer for Python</article-title>. <source>Zenodo</source> (<year>2024</year>) <pub-id pub-id-type="doi">10.5281/zenodo.3555620</pub-id></mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Genty</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Fuchs</surname>, <given-names>M.</given-names></string-name></person-group>: <article-title>GApS: A Coding Scheme for Great Apes Signals in ELAN</article-title>. <source>Zenodo</source>. (<year>2023</year>). <pub-id pub-id-type="doi">10.5281/zenodo.7385461</pub-id></mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name></person-group>: <article-title>Deep residual learning for image recognition</article-title>. In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>770</fpage>–<lpage>778</lpage> (<year>2016</year>)</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Tan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Le</surname>, <given-names>Q.</given-names></string-name></person-group>: <article-title>Efficientnet: Rethinking model scaling for convolutional neural networks</article-title>. In: <conf-name>International Conference on Machine Learning</conf-name>, pp. <fpage>6105</fpage>–<lpage>6114</lpage> (<year>2019</year>). <publisher-name>PMLR</publisher-name></mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Russakovsky</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Deng</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Su</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Krause</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Satheesh</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Karpathy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Khosla</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bernstein</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Fei-Fei</surname>, <given-names>L.</given-names></string-name></person-group>: <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>International Journal of Computer Vision (IJCV)</source> <volume>115</volume>(<issue>3</issue>), <fpage>211</fpage>–<lpage>252</lpage> (<year>2015</year>)</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Shao</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>D.</given-names></string-name></person-group>: <article-title>FineGym: A hierarchical video dataset for fine-grained action understanding</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>2616</fpage>–<lpage>2625</lpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Deng</surname>, <given-names>W.</given-names></string-name></person-group>: <article-title>Deep visual domain adaptation: A survey</article-title>. <source>Neurocomputing</source> <volume>312</volume>, <fpage>135</fpage>–<lpage>153</lpage> (<year>2018</year>)</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>G.H.</given-names></string-name></person-group>: <article-title>From synthetic to real: Unsupervised domain adaptation for animal pose estimation</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>1482</fpage>–<lpage>1491</lpage> (<year>2021</year>)</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Mu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Qiu</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Hager</surname>, <given-names>G.D.</given-names></string-name>, <string-name><surname>Yuille</surname>, <given-names>A.L.</given-names></string-name></person-group>: <article-title>Learning from synthetic animals</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>12386</fpage>–<lpage>12395</lpage> (<year>2020</year>)</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Coker</surname>, <given-names>D.J.</given-names></string-name>, <string-name><surname>Berumen</surname>, <given-names>M.L.</given-names></string-name>, <string-name><surname>Costelloe</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Beery</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Rohrbach</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Elhoseiny</surname>, <given-names>M.</given-names></string-name></person-group>: <article-title>MammalNet: A large-scale video benchmark for mammal recognition and behavior understanding</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>13052</fpage>–<lpage>13061</lpage> (<year>2023</year>)</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<label>Appendix A</label>
<p>Prediction Comparison of Pose Estimation Models</p>
<fig id="figa1" position="float" fig-type="figure">
<label>Fig. A1:</label>
<caption><title>Prediction comparison of the nine models at test time.</title>
<p>After 40,000 training iterations, the models’ test predictions are visually compared to one example of the test set. Note for example that i) ResNet-50 (<italic>center</italic>) wrongly predicts the top of the head as the tail’s position, ii) only three models can predict the left ankle’s position accurately (ResNet-50 (<italic>center</italic>), ResNet-101 (<italic>center right</italic>), and EfficientNet-B1 (<italic>bottom left</italic>)) and iii) no model correctly detects the left knee’s location.</p></caption>
<graphic xlink:href="559236v2_figa1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</app>
<app id="app2">
<label>Appendix B</label>
<p>PCK Nasal Dorsum</p>
<fig id="figb2" position="float" fig-type="figure">
<label>Fig. B2:</label>
<caption><title>PCK nasal dorsum.</title>
<p>The turquoise segment represents the length between the center of the eyes and the tip of the nose, i.e., the nasal dorsum. Any model prediction (represented in green) that falls within this distance of the ground-truth location (indicated in red) is considered as detected. In this case, all keypoints are detected except for the shoulders, neck, left wrist, and the hip (circled in purple). Hence, for this image, the detection rate would be 12<italic>/</italic>17 = 0.706 = 70.56%.</p></caption>
<graphic xlink:href="559236v2_figb2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</app>
<app id="app3">
<label>Appendix C</label>
<p>NMER by Families, Species and Keypoints</p>
<fig id="figc3" position="float" fig-type="figure">
<label>Fig. C3:</label>
<caption><title>Normalized error rate by families, species and keypoints.</title>
<p>For all OMC images at test time, we visualize the normalized error rate (NMER) for each species.</p></caption>
<graphic xlink:href="559236v2_figc3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</app>
<app id="app4">
<label>Appendix D</label>
<p>Examples of Elements of the ASBAR GUI</p>
<fig id="figd4" position="float" fig-type="figure">
<label>Fig. D4:</label>
<caption><title>Examples of UI elements of the ASBAR graphical user interface.</title>
<p>The GUI is terminal-based and therefore can be rendered even when accessed on a distant machine, such as a cloud-based platform or a remote high-performance computer.</p></caption>
<graphic xlink:href="559236v2_figd4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97962.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Tung</surname>
<given-names>Jenny</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Evolutionary Anthropology</institution>
</institution-wrap>
<city>Leipzig</city>
<country>Germany</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study presents a new framework (ASBAR) that combines open-source toolboxes for pose estimation and behavior recognition to automate the process of categorizing behaviors in wild apes from video data. The authors present <bold>compelling</bold> evidence that this pipeline can categorize simple wild ape behaviors from out-of-context video at a similar level of accuracy as previous models, while simultaneously vastly reducing the size of the model. The study's results should be of particular interest to primatologists and other behavioral biologists working with natural populations.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97962.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Advances in machine vision and computer learning have meant that there are now state-of-the-art and open-source toolboxes that allow for animal pose estimation and action recognition. These technologies have the potential to revolutionize behavioral observations of wild primates but are often held back by labor intensive model training and the need for some programming knowledge to effectively leverage such tools. The study presented here by Fuchs et al unveils a new framework (ASBAR) that aims to automate behavioral recognition in wild apes from video data. This framework combines robustly trained and well tested pose estimate and behavioral action recognition models. The framework performs admirably at the task of automatically identifying simple behaviors of wild apes from camera trap videos of variable quality and contexts. These results indicate that skeletal-based action recognition offers a reliable and lightweight methodology for studying ape behavior in the wild and the presented framework and GUI offer an accessible route for other researchers to utilize such tools.</p>
<p>Given that automated behavior recognition in wild primates will likely be a major future direction within many subfields of primatology, open-source frameworks, like the one presented here, will present a significant impact on the field and will provide a strong foundation for others to build future research upon.</p>
<p>Strengths:</p>
<p>Clearly articulated the argument as to why the framework was needed and what advantages it could convey to the wider field.</p>
<p>For a very technical paper it was very well written. Every aspect of the framework the authors clearly explained why it was chosen and how it was trained and tested. This information was broken down in a clear and easily digestible way that will be appreciated by technical and non-technical audiences alike.</p>
<p>The study demonstrates which pose estimation architectures produce the most robust models for both within context and out of context pose estimates. This is invaluable knowledge for those wanting to produce their own robust models.</p>
<p>The comparison of skeletal-based action recognition with other methodologies for action recognition are helpful in contextualizing the results.</p>
<p>Weaknesses:</p>
<p>While I note that this is a paper most likely aimed at the more technical reader, it will also be of interest to a wider primatological readership, including those who work extensively in the field. When outlining the need for future work I felt the paper offered almost exclusively very technical directions. This may have been a missed opportunity to engage the wider readership and suggest some practical ways those in the field could collect more ASBAR friendly video data to further improve accuracy.</p>
<p>Comments on latest version:</p>
<p>I think the new version is an improvement and applaud the authors on a well-written article that conveys some very technical details excellently. The authors have addressed my initial comments about reaching out to a wider, sometimes less technical, primatological audience by encouraging researchers to create large annotated datasets and make these publicly accessible. I also agree that fostering interdisciplinary collaboration is the best way to progress this field of research. These additions have certainly strengthened the paper but I still think some more practical advice for the actual collection of high-quality training data used to improve the pose estimates and behavioral classification in tough out-of-context environments could have been added. This doesn't detract from the quality of the paper though.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97962.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Fuchs et al. propose a framework for action recognition based on pose estimation. They integrate functions from DeepLabCut and MMAction2, two popular machine learning frameworks for behavioral analysis, in a new package called ASBAR.</p>
<p>They test their framework by:</p>
<p>Running pose estimation experiments on the OpenMonkeyChallenge (OMC) dataset (the public train + val parts) with DeepLabCut</p>
<p>Also annotating around 320 images pose data in the PanAf dataset (which contains behavioral annotations). They show that the ResNet-152 model generalizes best from the OMC data to this out-of-domain dataset.</p>
<p>They then train a skeleton-based action recognition model on PanAf and show that the top-1/3 accuracy is slightly higher than video-based methods</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97962.2.sa0</article-id>
<title-group>
<article-title>Author Response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Fuchs</surname>
<given-names>Michael</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8937-1427</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Genty</surname>
<given-names>Emilie</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2461-2442</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Zuberbühler</surname>
<given-names>Klaus</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8378-088X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Cotofrei</surname>
<given-names>Paul</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4103-5467</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review)</bold></p>
<p>Summary:</p>
<p>Advances in machine vision and computer learning have meant that there are now state-of-the-art and open-source toolboxes that allow for animal pose estimation and action recognition. These technologies have the potential to revolutionize behavioral observations of wild primates but are often held back by labor-intensive model training and the need for some programming knowledge to effectively leverage such tools. The study presented here by Fuchs et al unveils a new framework (ASBAR) that aims to automate behavioral recognition in wild apes from video data. This framework combines robustly trained and well-tested pose estimate and behavioral action recognition models. The framework performs admirably at the task of automatically identifying simple behaviors of wild apes from camera trap videos of variable quality and contexts. These results indicate that skeletal-based action recognition offers a reliable and lightweight methodology for studying ape behavior in the wild and the presented framework and GUI offer an accessible route for other researchers to utilize such tools.</p>
<p>Given that automated behavior recognition in wild primates will likely be a major future direction within many subfields of primatology, open-source frameworks, like the one presented here, will present a significant impact on the field and will provide a strong foundation for others to build future research upon.</p>
<p>Strengths:</p>
<p>Clearly articulated the argument as to why the framework was needed and what advantages it could convey to the wider field.</p>
<p>For a very technical paper it was very well written. Every aspect of the framework the authors clearly explained why it was chosen and how it was trained and tested. This information was broken down in a clear and easily digestible way that will be appreciated by technical and non-technical audiences alike.</p>
<p>The study demonstrates which pose estimation architectures produce the most robust models for both within-context and out-of-context pose estimates. This is invaluable knowledge for those wanting to produce their own robust models.</p>
<p>The comparison of skeletal-based action recognition with other methodologies for action recognition helps contextualize the results.</p>
</disp-quote>
<p>We thank Reviewer #1 for their thoughtful and constructive review of our manuscript. We are especially grateful for your recognition of the clarity of the manuscript, the strength of the technical framework, and its accessibility to both technical and non-technical audiences. Your feedback highlights exactly the kind of interdisciplinary engagement we hope to foster with this work.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses</p>
<p>While I note that this is a paper most likely aimed at the more technical reader, it will also be of interest to a wider primatological readership, including those who work extensively in the field. When outlining the need for future work I felt the paper offered almost exclusively very technical directions. This may have been a missed opportunity to engage the wider readership and suggest some practical ways those in the field could collect more ASBAR-friendly video data to further improve accuracy.</p>
</disp-quote>
<p>We appreciate this insightful suggestion and fully agree that emphasizing practical relevance is important for engaging a broader readership. In response, we have reformulated the opening of the Discussion section to place stronger emphasis on the value of shared, open-source resources and the real-world accessibility of the ASBAR framework. The revised text explicitly highlights the practical benefits of ASBAR for field researchers working in resource-constrained environments, and underscores the importance of community-driven data sharing to advance behavioral research in natural settings.</p>
<p>This section now reads: Despite the growing availability of open-source resources, such as large-scale animal pose datasets and machine learning toolboxes for pose estimation and human skeleton-based action recognition, their integration for animal behavior recognition—particularly in natural settings—remains largely unexplored. With ASBAR, a framework combining animal pose estimation and skeleton-based action recognition, we provide a comprehensive data and model pipeline, methodology, and GUI to assist researchers in automatically classifying animal behaviors via pose estimation. We hope these resources will become valuable tools for advancing the understanding of animal behavior within the research community.</p>
<p>To illustrate ASBAR’s capabilities, we applied it to the challenging task of classifying great ape behaviors in their natural habitat. Our skeletonbased approach achieved accuracy comparable to previous video-based studies for Top-K and Mean Class Accuracies. Additionally, by reducing the input size of the action recognition model by a factor of approximately 20 compared to video-based methods, our approach requires significantly less computational power, storage space, and data transfer resources. These qualities make ASBAR particularly suitable for field researchers working in resource-constrained environments.</p>
<p>Our framework and results are built on the foundation of shared and open-source materials, including tools like DeepLabCut, MMAction2, and datasets such as OpenMonkeyChallenge and PanAf500. This underscores the importance of making resources publicly available, especially in primatology, where data scarcity often impedes progress in AI-assisted methodologies. We strongly encourage researchers with large annotated video datasets to make them publicly accessible to foster interdisciplinary collaboration and further advancements in animal behavior research.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review)</bold></p>
<p>Fuchs et al. propose a framework for action recognition based on pose estimation. They integrate functions from DeepLabCut and MMAction2, two popular machine-learning frameworks for behavioral analysis, in a new package called ASBAR.</p>
<p>They test their framework by</p>
<p>Running pose estimation experiments on the OpenMonkeyChallenge (OMC) dataset (the public train + val parts) with DeepLabCut.</p>
<p>Annotating around 320 image pose data in the PanAf dataset (which contains behavioral annotations). They show that the ResNet-152 model generalizes best from the OMC data to this out-of-domain dataset.</p>
<p>They then train a skeleton-based action recognition model on PanAf and show that the top-1/3 accuracy is slightly higher than video-based methods (and strong), but that the mean class accuracy is lower - 33% vs 42%. Likely due to the imbalanced class frequencies. This should be clarified. For Table 1, confidence intervals would also be good (just like for the pose estimation results, where this is done very well).</p>
</disp-quote>
<p>We thank Reviewer #2 for their clear and helpful summary of our work, and for the thoughtful suggestions to improve the manuscript. We appreciate this observation. In the revised manuscript, we now clarify that the lower Mean Class Accuracy (MCA) in the initial version was indeed driven by significant class imbalance in the PanAf dataset, which contains highly uneven representation across behavior categories. To address this, we made two key improvements to the action recognition model:</p>
<p>(1) We replaced the standard cross-entropy loss with a class-balanced focal loss, following the approach of Sakib et al. (2021), to better account for rare behaviors during training.</p>
<p>(2) We initialized the PoseConv3D model with pretrained weights from FineGym (Shao et al., 2020) rather than training from scratch, which increased performance across underrepresented classes.</p>
<p>Together, these changes substantially improved model performance on tail classes, increasing the Mean Class Accuracy from 33.6% to 47%, now exceeding that of the videobased baseline.</p>
<p>Moreover, we sincerely thank Reviewer #2 for the thorough and constructive private feedback. Your comments have greatly helped us improve both the structure and clarity of the manuscript, and we have implemented several key revisions based on your recommendations to streamline the text and sharpen its focus on the core contributions. In particular, we have revised the tone of both the Introduction and Discussion sections to more modestly and accurately reflect the scope of our findings. We removed unnecessary implementation details—such as the description of graph-based models that were not part of the final pipeline—to avoid distracting tangents. The Methods section has been clarified and consolidated to include all evaluation metrics, a description of the data augmentation, and other methodological elements that were previously scattered across the Results section. Additionally, the Discussion now explicitly addresses the limitations of our EfficientNet results, including a dedicated paragraph that acknowledges the use of suboptimal hyperparameters and highlights the need for architecture-specific tuning, particularly with respect to learning rate schedules.</p>
</body>
</sub-article>
</article>