<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106827</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106827</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106827.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.7</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>Biologically informed cortical models predict optogenetic perturbations</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0007-0519-1116</contrib-id>
<name>
<surname>Sourmpis</surname>
<given-names>Christos</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3344-4495</contrib-id>
<name>
<surname>Petersen</surname>
<given-names>Carl CH</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4344-2189</contrib-id>
<name>
<surname>Gerstner</surname>
<given-names>Wulfram</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7568-4994</contrib-id>
<name>
<surname>Bellec</surname>
<given-names>Guillaume</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>guillweb@gmail.com</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Laboratory of Computational Neuroscience, Brain Mind Institute, School of Computer and Communication Sciences and School of Life Sciences, École Polytechnique Fédérale de Lausanne (EPFL)</institution></institution-wrap>, <city>Lausanne</city>, <country country="CH">Switzerland</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Laboratory of Sensory Processing, Brain Mind Institute, School of Life Sciences, École Polytechnique Fédérale de Lausanne (EPFL)</institution></institution-wrap>, <city>Lausanne</city>, <country country="CH">Switzerland</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04d836q62</institution-id><institution>Machine Learning Research Unit, Technical University of Vienna (TU Wien)</institution></institution-wrap>, <city>Vienna</city>, <country country="AT">Austria</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country country="FR">France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country country="GR">Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-06-16">
<day>16</day>
<month>06</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-12-18">
<day>18</day>
<month>12</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106827</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-03-21">
<day>21</day>
<month>03</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-03-15">
<day>15</day>
<month>03</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.27.615361"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-06-16">
<day>16</day>
<month>06</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106827.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.106827.1.sa2">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.106827.1.sa1">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.106827.1.sa0">Reviewer #2 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Sourmpis et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Sourmpis et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106827-v2.pdf"/>
<abstract>
<p>A recurrent neural network fitted to large electrophysiological datasets may help us understand the chain of cortical information transmission. In particular, successful network reconstruction methods should enable a model to predict the response to optogenetic perturbations. We test recurrent neural networks (RNNs) fitted to electrophysiological datasets on unseen optogenetic interventions, and measure that generic RNNs used predominantly in the field generalize poorly on these perturbations. Our alternative RNN model adds biologically informed inductive biases like structured connectivity of excitatory and inhibitory neurons, and spiking neuron dynamics. We measure that some biological inductive biases improve the model prediction on perturbed trials in a simulated dataset, and a dataset recorded in mice in vivo. Furthermore, we show in theory and simulations that gradients of the fitted RNN can be used to target micro-perturbations in the recorded circuits, and discuss the potential utility to bias an animal’s behavior and study cortical circuit mechanisms.</p>
</abstract>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id>
<institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung (SNF)</institution>
</institution-wrap>
</funding-source>
<award-id>CR-SII5_198612</award-id>
<principal-award-recipient>
<name>
<surname>Gerstner</surname>
<given-names>Wulfram</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id>
<institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung (SNF)</institution>
</institution-wrap>
</funding-source>
<award-id>00020_207426</award-id>
<principal-award-recipient>
<name>
<surname>Gerstner</surname>
<given-names>Wulfram</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id>
<institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung (SNF)</institution>
</institution-wrap>
</funding-source>
<award-id>TMAG-3_209271</award-id>
<principal-award-recipient>
<name>
<surname>Petersen</surname>
<given-names>Carl CH</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-4">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id>
<institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung (SNF)</institution>
</institution-wrap>
</funding-source>
<award-id>31003A_182010</award-id>
<principal-award-recipient>
<name>
<surname>Petersen</surname>
<given-names>Carl CH</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-5">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001821</institution-id>
<institution>Vienna Science and Technology Fund (WWTF)</institution>
</institution-wrap>
</funding-source>
<award-id>VRG24-018</award-id>
<principal-award-recipient>
<name>
<surname>Bellec</surname>
<given-names>Guillaume</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Updated text and figures following the eLife revision process.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>A fundamental question in neuroscience is how cortical circuit mechanisms drive perception and behavior. To tackle this question, experimental neuroscientists have been collecting large-scale electrophysiology datasets under reproducible experimental settings (<xref ref-type="bibr" rid="c50">Siegle et al., 2021</xref>; <xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="c59">Urai et al., 2022</xref>; <xref ref-type="bibr" rid="c26">International Brain Laboratory et al., 2023</xref>). However, neuroscience lacks data-grounded modeling approaches to generate and test hypotheses on the causal role of neuronal and circuit-level mechanisms. To leverage the high information density of contemporary recordings, we need both (1) modeling approaches that scale well with data, and (2) metrics to quantify when the models provide a plausible mechanism for the observed phenomena.</p>
<p>Biophysical simulations have been crucial for our understanding of single-cell mechanisms (<xref ref-type="bibr" rid="c25">Hodgkin, 1958</xref>), and have been used to describe interactions across cortical layers, columns, and areas (<xref ref-type="bibr" rid="c34">Markram et al., 2015</xref>; <xref ref-type="bibr" rid="c8">Billeh et al., 2020</xref>; <xref ref-type="bibr" rid="c27">Isbister et al., 2023</xref>; <xref ref-type="bibr" rid="c10">Chen et al., 2022</xref>; <xref ref-type="bibr" rid="c47">Rimehaug et al., 2023</xref>; <xref ref-type="bibr" rid="c20">Fraile et al., 2023</xref>; <xref ref-type="bibr" rid="c52">Spieler et al., 2023</xref>). A promising approach to constrain models to electrophysiological data lies in the optimization of the simulation parameters by gradient descent. These methods were successful in quantitatively classifying functional cell types (<xref ref-type="bibr" rid="c44">Pozzorini et al., 2015</xref>; <xref ref-type="bibr" rid="c57">Teeter et al., 2018</xref>), and modeling micro-circuit interactions (<xref ref-type="bibr" rid="c43">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="c14">Deny et al., 2017</xref>; <xref ref-type="bibr" rid="c33">Mahuas et al., 2020</xref>). To bridge the gap from single neurons or small retinal networks to cortical recordings in vivo, recent studies made substantial progress towards data-constrained recurrent neural network (RNN) models (<xref ref-type="bibr" rid="c42">Perich et al., 2020</xref>; <xref ref-type="bibr" rid="c7">Bellec et al., 2021</xref>; <xref ref-type="bibr" rid="c3">Arthur et al., 2023</xref>; <xref ref-type="bibr" rid="c60">Valente et al., 2022</xref>; <xref ref-type="bibr" rid="c28">Kim et al., 2023</xref>; <xref ref-type="bibr" rid="c15">Dinc et al., 2023</xref>; <xref ref-type="bibr" rid="c51">Sourmpis et al., 2023</xref>; <xref ref-type="bibr" rid="c39">Pals et al., 2024</xref>). In this line of work, neurons in the RNN are mapped one-to-one to recorded cells and optimized by gradient descent to predict recorded activity at large scale.</p>
<p>An important question is whether these data-constrained RNNs can reveal a truthful mechanism of neuronal activity and behavior. By construction, the RNNs can generate brain-like network activity, but how can we measure whether the reconstructed network faithfully represents the biophysical mechanism? To answer this question, we submit a range of RNN reconstruction methods to a difficult <italic>perturbation test</italic> : we measure the similarity of the network response to unseen perturbations in the RNN and the recorded biological circuit.</p>
<p>Optogenetics is a powerful tool to induce precise causal perturbations in vivo (<xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="c22">Guo et al., 2014</xref>). It involves the expression of light-sensitive ion channels (<xref ref-type="bibr" rid="c9">Boyden et al., 2005</xref>), such as channelrhodopsins, in specific populations of neurons (e.g., excitatory/pyramidal or inhibitory/parvalbumin-expressing). In this paper, we use datasets including both dense electrophysiological recordings and optogenetic perturbations to evaluate RNN reconstruction methods. Since the neurons in our RNNs are mapped one-to-one to the recorded cells, we can model optogenetic perturbations targeting the same cell-types and areas as done in vivo. Yet, we observe that the similarity between the simulated and recorded perturbations varies greatly depending on the reconstruction methods.</p>
<p>Most prominently, we study two opposite types of RNN specifications. First, as a control model, we consider a traditional sigmoidal RNN (<italic>σ</italic>RNN) which is arguably the most common choice for contemporary data-constrained RNNs (<xref ref-type="bibr" rid="c42">Perich et al., 2020</xref>; <xref ref-type="bibr" rid="c3">Arthur et al., 2023</xref>; <xref ref-type="bibr" rid="c39">Pals et al., 2024</xref>); and second, we develop a model with biologically informed inductive biases (bioRNN): (1) neuronal dynamics follow a simplified spiking neuron model, and (2) neurons associated with fast-spiking inhibitory cells have short-distance inhibitory projections (other neurons are excitatory with both local and long-range interareal connectivity). Following <xref ref-type="bibr" rid="c37">Neftci et al. (2019)</xref>; <xref ref-type="bibr" rid="c6">Bellec et al. (2018b</xref>, <xref ref-type="bibr" rid="c7">2021</xref>); <xref ref-type="bibr" rid="c51">Sourmpis et al. (2023)</xref>, we adapt gradient descent techniques to optimize the bioRNN parameters of neurons and synapses to explain the recorded neural activity and behavior.</p>
<p>Strikingly, we find that the bioRNN is more robust to perturbations than the <italic>σ</italic>RNN. This is nontrivial because it is in direct contradiction with other metrics often used in the field: the <italic>σ</italic>RNN simulation achieves higher similarity with unseen recorded trials before perturbation, but lower than the bioRNN on perturbed trials. This contradiction is confirmed both on synthetic and in vivo datasets. To analyze this result, we submit a spectrum of in-termediate bioRNN models to the same <italic>perturbation tests</italic>, and identify two bioRNN model features that are most important to improve robustness to perturbation: (1) Dale’s law (the cell type constrains the sign of the connections (<xref ref-type="bibr" rid="c17">Eccles, 1976</xref>)), and (2) local-only inhibition (inhibitory neurons do not project to other cortical areas). In contrast, other model features are penalizing, or do not improve significantly the prediction of the optogenetically perturbed response in this out-of-distribution fashion. It indicates that perturbation tests can validate biophysical modeling strategies in data-constrained deep learning models of neural mechanisms.</p>
<p>Beyond the optogenetic area inactivation available in the in vivo dataset, we investigate how perturbation-robust RNNs could enable targeted optogenetic protocols for the discovery of detailed neuronal circuit mechanisms in future experiments. Targeted causal interventions will become decisive in studying smaller circuit mechanisms. Acute optogenetic inactivations of genetically defined laminar sub-populations were used to characterize the causal role of specific neurons in the sensory motor pathways (<xref ref-type="bibr" rid="c56">Tamura et al., 2025</xref>; <xref ref-type="bibr" rid="c61">Wyart et al., 2025</xref>), and upcoming technology will make these experiments easier (<xref ref-type="bibr" rid="c29">Lakunina et al., 2025</xref>). To illustrate how RNN reconstruction can help to target neuronal stimulation, we consider micro-perturbations (<italic>µ</italic>-perturbation) targeting dozens of neurons in a small time window. Inspired by recent read-write all-optical setups (<xref ref-type="bibr" rid="c38">Packer et al., 2015</xref>), we imagine a modelinformed <italic>µ</italic>-perturbation protocol, where neurons are targeted based on their functional rather than genetic properties. While previous work has used linear models to produce targeted stimulations (<xref ref-type="bibr" rid="c61">Wyart et al., 2025</xref>; <xref ref-type="bibr" rid="c36">Minai et al., 2024</xref>), we show that back-propagated gradients of perturbation-robust RNNs provide a sensitivity map to predict the effect of <italic>µ</italic>-perturbations. Concretely, in a closed-loop experimental setup in silicon, we can use RNN gradients to target a <italic>µ</italic>-perturbation and change the movement in a simulated mouse. The gradients are used to identify the few neurons having the strongest causal effect on behavior. Conceptually, it means that our RNN reconstructions enable an estimation of “circuit gradients”, bringing numerical and theoretical concepts from deep learning (<xref ref-type="bibr" rid="c31">LeCun et al., 2015</xref>; <xref ref-type="bibr" rid="c46">Richards and Kording, 2023</xref>) to study biological network computation.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>Reconstructed Networks: biological inductive biases strengthen robustness to perturbations</title>
<sec id="s2a1">
<title>Synthetic dataset for challenging causal inference</title>
<p>We build a toy synthetic dataset to formalize how we intend to reverse engineer the mechanism of a recorded circuit using optogenetic perturbations and RNN reconstruction methods. It also serves as the first dataset to evaluate our network reconstruction methods. This toy example represents a simplified version of large-scale cortical recordings from multiple brain areas during a low-dimensional instructed behavior (<xref ref-type="bibr" rid="c54">Steinmetz et al., 2019</xref>; <xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="c26">International Brain Laboratory et al., 2023</xref>), similarly to the in vivo dataset of a GO/No-Go task <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref> analyzed in the next section. Let’s consider two areas <italic>A</italic> and <italic>B</italic> which are either transiently active together (“hit trial” occurring with frequency <italic>p</italic>) or quiescent together (“miss trial” occurring with probability 1 − <italic>p</italic>). Since the two areas are active or in-active together, it is hard to infer if they are connected in a feedforward or recurrent fashion. In Methods 4.1, we describe a theoretical example where it is impossible to decide between opposing mechanistic hypothesis (feedforward or recurrent) when recording only the macroscopic activations of areas <italic>A</italic> and <italic>B</italic>. In this case, performing optogenetic inactivation of one area is decisive to distinguish between the feedforward or recurrent hypothesis.</p>
<p>To generate artificial spike train recordings that capture this problem, we design two reference circuits (RefCircs) from which we can record the spike trains. Each RefCirc consist of two populations of 250 spiking neurons (80% are excitatory) representing areas <italic>A</italic> and <italic>B</italic>. To highlight the importance of optogenetic perturbations as in the Methods 4.1, the first circuit RefCirc1 is feedforward and the second Refcirc2 is recurrent: RefCirc1 (and not RefCirc2) has strictly zero feedback connections from <italic>B</italic> to <italic>A</italic>. Yet, the two RefCircs are almost identical without optogenetic perturbations: each neuron in RefCirc1 has been constructed to have an almost identical trial-averaged activity as the corresponding neuron in RefCirc2; and in response to a stimulus, the circuits display a similar bi-modal hit-or-miss response with a hit trial frequency <italic>p</italic> ≈ 50%. We consider that a trial is a hit if area <italic>A</italic> is active (averaged firing rate above 8Hz; defining a “hit” trial based on area <italic>A</italic> is equivalent to saying that both areas need to be active during unperturbed trials with this dataset. But excluding <italic>B</italic> in this definition avoids that the hit rate is trivially impacted when manipulating the activity of <italic>B</italic> with optogenetic perturbations). To simulate optogenetic inactivations of an area in the RefCircs, we inject a transient current into the inhibitory neurons, modeling the opening of light-sensitive ion channels (symmetrically, an optogenetic activation is simulated as a positive current injected into excitatory cells). <xref rid="figS1" ref-type="fig">Figure S1</xref> shows that optogenetic perturbations in area B reflect the presence or absence of feedback connections which differs in RefCirc 1 and 2. Methods 4.4 provides more details on the construction of the artificial circuits. Our <italic>perturbation test</italic> will consist of the comparison of optogenetic perturbations in the reconstructed RNN and in their references RefCirc1 and 2, without retraining the RNN on these perturbations.</p>
</sec>
<sec id="s2a2">
<title>Network reconstruction methodology (synthetic dataset)</title>
<p>To reconstruct the recorded circuits with an RNN, we record activity from the spiking RefCircs, and optimize the parameters of an RNN to generate highly similar network activity. The whole reconstruction method is summarized graphically in panel A of <xref rid="fig1" ref-type="fig">Figure 1</xref>. In the simplest cases, the RNN is specified as a sigmoidal network model (<xref ref-type="bibr" rid="c48">Rosenblatt, 1960</xref>; <xref ref-type="bibr" rid="c18">Elman, 1990</xref>): <italic>σ</italic>RNN1 and <italic>σ</italic>RNN2 are optimized to reproduce the recording from RefCirc1 and RefCirc2 respectively. In this synthetic dataset, the reconstructed <italic>σ</italic>RNNs have the same size as the RefCircs (500 neurons) and sigmoidal neurons are mapped one-to-one with RefCirc neurons (20% are mapped to inhibitory RefCirc neurons). They are initialized with all-to-all connectivity and are therefore blind to the structural difference of the RefCirc1 and 2 (feed-forward or recurrent). From each of the two RefCircs, we store 2,000 trials of simultaneous spike train recordings of all 500 neurons (step 1 in <xref rid="fig1" ref-type="fig">Fig. 1A</xref>). Half of the trials are used as training set and will be the basis for our data-driven RNN optimization. The second half of the recorded trials form the testing set and are used to evaluate the quality of the reconstruction before perturbations.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Network reconstruction and perturbation tests.</title>
<p><bold>A</bold>. The three steps to reconstruct the reference circuit (RefCirc) using a biologically informed RNN (bioRNN) or a sigmoidal RNN (<italic>σ</italic>RNN) and evaluate the reconstruction based on perturbation tests. <bold>B</bold>. Summary of the differences between a bioRNN and a <italic>σ</italic>RNN. <bold>C</bold>. Trial-averaged activity of area <italic>A</italic> of the two circuits during hit (black-dashed: RefCirc1; blue: bioRNN1; pink: <italic>σ</italic>RNN1) and miss (grey-dashed: RefCirc1; light blue: bioRNN1; light pink: <italic>σ</italic>RNN1) trials. All models display a hit rate of <italic>p</italic> ≈ 50%. <bold>D</bold>. Same as <bold>C</bold> during inactivation of area <italic>B</italic>. Δ<italic>p</italic><sup><italic>D</italic></sup> = 0 is the recorded change of hit rate for the feedforward circuit RefCirc1, so a successful reconstruction achieves <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="615361v7_inline58.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. <bold>E</bold>. Quantitative results on perturbation tests showing that <italic>σ</italic>RNN achieves the lowest loss function on the unperturbed test trials, but only the bioRNN retains an accurate fit to the perturbed trials.</p></caption>
<graphic xlink:href="615361v7_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We optimize the synaptic “weights” of the <italic>σ</italic>RNN to minimize the difference between its activity and that of the RefCirc (step 2 in <xref rid="fig1" ref-type="fig">Fig 1A</xref>, see Methods). The optimization combines three loss functions defined mathematically in Methods 4.5: (i) the neuron-specific loss function ℒ<sub>neuron</sub> is the mean-square error of the <italic>trial-averaged</italic> neural activity (e.g. the PSTH) between <italic>σ</italic>RNN and RefCirc neurons. (ii) To account for fluctuations of the single-trial network activity, we use a trial-specific loss function ℒ<sub>trial</sub>, which is the distance between the distribution of single trial population-averaged activity of <italic>σ</italic>RNN and RefCirc (see <xref ref-type="bibr" rid="c51">Sourmpis et al. (2023)</xref>). (iii) Finally, we add a regularization loss function ℒ<sub>reg</sub> to penalize unnecessarily large weights.</p>
<p>We also developed a biologically informed RNN model (bioRNN) for which we have designed a successful optimization technique. The main differences between <italic>σ</italic>RNNs and bioRNNs consist in the following biological inductive biases. Firstly, the bioRNN neuron model follows a simplified leaky integrate and fire dynamics (see Methods 4.2) yielding strictly binary spiking activity. Secondly, we constrain the recurrent weight matrix to describe cell-type specific connectivity constraints: following Dale’s law, neurons have either non-negative or non-positive outgoing connections; moreover, since cortical inhibitory neurons rarely project across areas, we assume that inhibitory neurons project only locally within the same area. Thirdly, we add a term to the regularization loss ℒ<sub>reg</sub> to implement the prior knowledge that cross-area connections are more sparse than within an area. Adding these biological features into the model requires an adapted gradient descent algorithm and matrix initialization strategies (Methods 4.5). The reconstruction method with <italic>σ</italic>RNNs and bioRNNs is otherwise identical: the models have the same size, and are optimized on the same data, for the same number of steps and using the same loss functions. The two models bioRNN1 and bioRNN2 are optimized to explain recordings from RefCirc1 and Refcirc2, respectively. Importantly, the structural difference between RefCirc1 (feedforward) and RefCirc2 (feedback) is assumed to be unknown during parameter optimization: at initialization, excitatory neurons in bioRNN1 or bioRNN2 project to any neuron in the network with transmission efficacies (aka as synaptic weights) initialized randomly.</p>
<p>After parameter optimization, we have four models, <italic>σ</italic>RNN1, <italic>σ</italic>RNN2, bioRNN1 and bioRNN2, that we call “reconstructed” models. To validate the reconstructed models, we verify that the network trajectories closely match the data on the test set in terms of (i) the “behavioral” hit-trial frequency, (ii) the peristimulus time histogram (PSTH) mean-square error of single neurons as evaluated by ℒ<italic>L</italic><sub>neuron</sub>, and (iii) the distance between single-trial network dynamics as evaluated by ℒ<sub>trial</sub> (see <xref rid="figS2" ref-type="fig">Suppl. Fig. S2</xref> and <xref rid="tbl1" ref-type="table">Table 1</xref>). At first sight, the <italic>σ</italic>RNN displays a better data fitting when comparing with the non-perturbed trials of the testing set: ℒ<sub>trial</sub> is for instance lower with <italic>σ</italic>RNN (see <xref rid="tbl1" ref-type="table">Table 1</xref>). This is expected considering that the optimization of bioRNNs is less flexible and numerically efficient because of the sign-constrained weight matrix and the imperfect surrogate gradient approximation through spiking activity. However, the two bioRNNs are drastically more robust when evaluating the models with <italic>perturbation tests</italic>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>BioRNN is more robust to optogenetic perturbations than <italic>σ</italic>RNN.</title>
<p>The table reports the trial matching (TM) loss ℒ<sub>trial</sub> on test trials, it measures the distance between the distributions of single trial network dynamics <xref ref-type="bibr" rid="c51">Sourmpis et al. (2023)</xref> in area A when stimulating area <italic>B</italic>. Column “no light” indicates values on the unperturbed test trials, and “light” the perturbation trials. <italic>±</italic> indicates the 95% confidence interval, best values are shown in bold and major failure with distance above 0.5 is in red.</p></caption>
<graphic xlink:href="615361v7_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2a3">
<title>Perturbation test</title>
<p>To test which of the reconstructed RNNs capture the causal mechanisms of the RefCircs, we simulate optogenetic activations and inactivations of area <italic>B</italic> (step 3 in <xref rid="fig1" ref-type="fig">Fig. 1A</xref>). We first compare the change of hit probability after perturbations in the reconstructed RNN <inline-formula id="inline-eqn-2"><inline-graphic xlink:href="615361v7_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and recorded in RefCirc (Δ<italic>p</italic><sup><italic>D</italic></sup>) in <xref rid="fig2" ref-type="fig">Figure 2</xref>. For the <italic>σ</italic>RNN the activation or inactivation of area <italic>B</italic> changes drastically the peak firing rate in area <italic>A</italic>: all trials become a hit during inactivation of area <italic>B</italic>. This drastic increase of hit rate is not consistent the reference where the effect of the optogenetic inactivations is mild: the distribution of network responses remains bi-modal (hit versus miss) with only a moderate change of hit frequency for RefCirc2 Δ<italic>p</italic><sup><italic>D</italic></sup> = −3%. For RefCirc1 we even expect Δ<italic>p</italic><sup><italic>D</italic></sup> = 0% by design because of the absence of feedback connections from <italic>B</italic> to <italic>A</italic>. In contrast, the bioRNN models capture these changes more accurately (see <xref rid="fig1" ref-type="fig">Fig. 1</xref> and <xref rid="fig2" ref-type="fig">2</xref>). Quantitative results are summarized in <xref rid="fig2" ref-type="fig">Fig. 2E</xref>, the error of hit probability changes <inline-formula id="inline-eqn-3"><inline-graphic xlink:href="615361v7_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is 7% with bioRNNs when averaged over all conditions (bioRNN1 and bioRNN2, with optogenetic inactivations and activations). The corresponding error is 48.5% on average for <italic>σ</italic>RNNs. In this sense, we argue that the bioRNN provides a better prediction of the perturbed hit frequency than the <italic>σ</italic>RNN. We also performed spike train recordings in the area that is not directly targeted by the light to compare the perturbed network dynamics in the fitted RNNs and the RefCirc. The perturbed dynamics are displayed in <xref rid="fig2" ref-type="fig">Fig. 2D</xref>. The quantity <inline-formula id="inline-eqn-4"><inline-graphic xlink:href="615361v7_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is a distance between the network dynamics (RNN versus reference) and is reported in <xref rid="fig2" ref-type="fig">Fig. 2D-E</xref> and <xref rid="tbl1" ref-type="table">Table 1</xref>. Again, the perturbed dynamics of the bioRNN are more similar to those of the reference circuits <inline-formula id="inline-eqn-5"><inline-graphic xlink:href="615361v7_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, than with the <inline-formula id="inline-eqn-6"><inline-graphic xlink:href="615361v7_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (t-test p-value is 0.0003).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Reconstruction of network mechanisms.</title>
<p><bold>A</bold>. RefCirc1 is feedforward and RefCirc2 is recurrent. <bold>B</bold>. The fitted RNNs are blind to the structural difference of RefCirc1 and 2 and must infer this from the spiking data. <bold>C</bold>. Raster plot showing an example trial of the bioRNN and <italic>σ</italic>RNN models, neurons in red are mapped to inhibitory neurons. <bold>D</bold>. To study which model feature matters, bioRNN variants are defined by removing one of the features, for instance “No Dale’s law” refers to a bioRNN without weight sign constraints. Trial-averaged activity in area <italic>A</italic> under activation/inactivation of area <italic>B</italic>. All the RNNs are tested with the same reference circuit and training data (No spike and No Sparsity models are shown in <xref rid="figS4" ref-type="fig">Suppl. Fig. S4</xref>). <bold>E</bold>. Error between the change of hit probability after perturbations in the RNN <inline-formula id="inline-eqn-7"><inline-graphic xlink:href="615361v7_inline59.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and in the RefCirc Δ<italic>p</italic><sup><italic>D</italic></sup>. <bold>F</bold>. The distance of network dynamics <inline-formula id="inline-eqn-8"><inline-graphic xlink:href="615361v7_inline60.gif" mimetype="image" mime-subtype="gif"/></inline-formula> between each RNN and RefCirc (horizontal axis: light power in arbitrary units). <bold>G</bold>. Same quantity as <italic>D</italic> but averaged for each RNN under the strongest light power condition (averaging activations and inactivations of area <italic>B</italic>). Statistical significance in comparison with bioRNN is computed using the mean over multiple network initializations and is indicated with 0 to 4 stars corresponding to p-values thresholds: 0.05, 10<sup>−2</sup>, 10<sup>−3</sup> and 10<sup>−4</sup>.</p></caption>
<graphic xlink:href="615361v7_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To analyze which features of bioRNN explain this robustness to perturbation, we then derive a family of models where only one feature of the reconstruction is omitted. Namely, the “No Dale’s law” model does not have excitatory and inhibitory weight constraints, the “Non-local inhibition” model allows inhibitory neurons to project outside of their areas, the “No Spike” model replaces the spiking dynamics with a sigmoidal neuron model, and the “No Sparsity” model omits the cross-area sparsity penalty in ℒ<sub>reg</sub>. Omitting all these features in bioRNN would be equivalent to using a <italic>σ</italic>RNN. The accuracy metrics on the testing sets before perturbation are reported for all RNN variants <xref rid="fig2" ref-type="fig">Fig. 2E</xref> and <xref rid="fig2" ref-type="fig">G</xref>. For reference, we also include the model “No TM” (trial-matching), which omits the loss function ℒ<sub>trial</sub> during training.</p>
<p>The strongest effect measured with this analysis is that the Dale’s law and local inhibition explain most of the improved robustness of bioRNNs. This is visible in <xref rid="fig2" ref-type="fig">Fig. 2</xref> as the perturbed trajectories of “No Dale’s law” and “Non-local inhibition” are most distant from the reference in <xref rid="fig2" ref-type="fig">Fig. 2D</xref>. This is confirmed numerically where both the hit-rate error and the distance of network dynamics increase the most when lifting these constraints (<xref rid="fig2" ref-type="fig">Fig. 2E-G</xref> and <xref rid="tbl1" ref-type="table">Table 1</xref>. We explain this result as follows: the mono-synaptic effect of a cell stimulated by the light are always correct in bioRNN (according to Dale’s law, and inhibition locality), but often wrong in the alternative models (see <xref rid="fig2" ref-type="fig">Fig. 2A</xref>). For instance, a simple explanation may justify the failure of the “Non-local inhibition” model: the stimulation of inhibitory neurons in <italic>B</italic> induces (via the erroneous mono-synaptic inhibition) a reduction in the baseline activity in area <italic>A</italic> (see the green trace during inactivation in <xref rid="fig2" ref-type="fig">Fig. 2D</xref>). More generally for <italic>perturbation testing</italic>, we speculate that these features are measured to be important here because they are central to the biophysical nature of the perturbation considered: optoge-netic perturbation targets specific cell types, and these features incorporate a biophysical connectivity priors which is hard to infer entirely from the unperturbed data.</p>
<p>Not all the biological features that we implemented in bioRNN made comparable improvements in the prediction of optogenetic perturbations. We implemented simple spiking neuron dynamics and fitted the spiking network as any other RNN using surrogate gradients <xref ref-type="bibr" rid="c37">Neftci et al. (2019)</xref> as in <xref ref-type="bibr" rid="c7">Bellec et al. (2021)</xref>; <xref ref-type="bibr" rid="c51">Sourmpis et al. (2023)</xref>. On perturbed data, the spiking bioRNN achieves slightly better performance than its “No spike” variant, but without significant margins, t-test p-value is 0.31 for <inline-formula id="inline-eqn-9"><inline-graphic xlink:href="615361v7_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (see <xref rid="tbl1" ref-type="table">Table 1</xref> and <xref rid="fig2" ref-type="fig">Fig 2E-G</xref>). We speculate that simulating spikes is not advantageous here, because optogenetic perturbations are relatively broad in space and time, and it might become more relevant for other perturbation experiments where precise timing matters or at a microcircuit level. The quantitative of sparse connectivity regularization also did not make a significant improvement in all cases (see <xref rid="tbl1" ref-type="table">Table 1</xref>).</p>
<p>Besides predicting the response to optogenetic perturbations, we wondered if we could recover the connectivity structure of the recorded circuit. Our method would not be appropriate to recover individual synaptic connections, but we tested whether the fitted RNNs reflected the optogenetic signature of the structural difference between the “feedforward” RefCirc1 and the “recurrent” RefCirc2. Our criteria is here qualitative: the early increase in the PSTH response in area <italic>A</italic> characteristic of mono-synaptic feedback from area <italic>B</italic> should not exist for RefCirc1 (see <xref rid="fig2" ref-type="fig">Figure 2</xref> and <xref rid="figS4" ref-type="fig">Suppl. Fig. S4A</xref>). To reveal this difference in the fitted bioRNN1 and bioRNN2 models, not only Dale’s law and local-inhibition are necessary, but also spiking dynamics and sparsity appear to be helpful. For instance, the erroneous early onset on the perturbed trial in area A for the “No Sparsity” model is corrected with the sparsity prior (<xref rid="figS4" ref-type="fig">Suppl. Fig. S4</xref>, red versus blue curves). Yet, these are subtle qualitative results that are likely to be less impactful and reproducible than the clear qualitative improvement obtained with perturbation testing when modeling cell-type connectivity. Indeed, we will see in the next section that we obtain consistent qualitative perturbation testing results on the larger in vivo dataset.</p>
</sec>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Predicting perturbations on in vivo electrophysiology data</title>
<p>To test whether our reconstruction method with biological inductive biases can predict optogenetic perturbations in large-scale recordings, we used the dataset from <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref>. In this study, water-deprived mice were trained to perform a whisker tactile detection task. In 50% of the trials (Go trials), a whisker is deflected, followed by a 1-second delay, after which an auditory cue signals that the mice can lick a water spout to receive a water reward. In the other 50% of trials (No-Go trials), no whisker deflection occurs, and licking after the auditory cue results in a penalty with an extended time-out period. While the mice performed the task, experimenters recorded 6, 182 units from 12 areas across 18 mice. Using this dataset, we focused on the 6 most relevant areas for executing this task (<xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>). From each area, we randomly selected 250 neurons (200 putative excitatory and 50 putative inhibitory), which correspond to 1500 neurons in total. These areas, all shown to be causally involved in the task (<xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>), include the primary and secondary whisker sensory cortex (wS1, wS2), the primary and secondary whisker motor cortex (wM1, wM2), the anterior lateral motor cortex (ALM), and the primary tongue-jaw motor cortex (tjM1). We fit the neuronal activity using the same reconstruction method as used for the synthetic dataset. In the model, we simulate the jaw movement of the mouse as a linear readout driven by the model’s neural activity. This readout is regressed with the real jaw movement extracted from video footage. The parameter optimization of the behavioral readout is performed jointly with fitting the synaptic weights to the neuronal recordings, see Methods 4.5. After training, our reconstructed model can generate neural activity with firing rate distribution, trial-averaged activity, single-trial network dynamics and behavioral outcome which are all consistent with the recordings (see <xref rid="figS6" ref-type="fig">Suppl. Fig S6</xref>). Before perturbations, we observe again that the <italic>σ</italic>RNN model fits the testing set data better than the bioRNN model (see <xref rid="tbl2" ref-type="table">Table 2</xref> and <xref rid="figS6" ref-type="fig">Fig. S6</xref>).</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><title>Trial-matching loss test loss <italic>L</italic><sub>trial</sub> of the different reconstruction methods with the real recordings from (<xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>) <italic>±</italic> indicates the 95% confidence interval.</title></caption>
<graphic xlink:href="615361v7_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>We then submit the reconstructed <italic>σ</italic>RNNs and bioRNNs models to <italic>perturbation tests</italic>. For the sessions of the in vivo dataset with optogenetic perturbation that we considered, only the behavior of an animal is recorded during inactivation of an area at a given time window (stimulus, delay, or choice periods). For each of the six areas and time windows, we extract the averaged hit frequency under optogenetic inactivation, and attempt to predict this perturbed behavior by inducing the same inactivations to the fitted RNNs. These perturbations are acute spatiotemporal optogenetic inactivations of each area during different time periods (see <xref rid="fig3" ref-type="fig">Figure 3B</xref>). As an example, we show the effect of an inactivation of wS1 during the whisker period in the model in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. In panel C, we display the simulated trial of a fitted bioRNN with and without perturbations side by side. The two trials are simulated with the same random seed, and this example shows that an early perturbation in wS1 can change a lick decision from hit to miss in the model (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Predicting optogenetic perturbations for in vivo electrophysiology data</title>
<p><bold>A</bold>. During a delayed whisker detection task, the mouse reports a whisker stimulation by licking to obtain a water reward. Jaw movements are recorded by a camera. Our model simulates the jaw movements and the neural activity from six areas. <bold>B</bold>. The experimentalists performed optogenetic inactivations of cortical areas (one area at a time) in three temporal windows. <bold>C</bold>. Example hit trial of a reconstructed network (left). Using the same random seed, the trial turns into a miss trial if we inactivate area wS1 (right, light stimulus indicated by blue shading) during the whisker period by stimulation of inhibitory neurons (red dots). <bold>D</bold>. Error of the change in lick frequency caused by the perturbation, <inline-formula id="inline-eqn-10"><inline-graphic xlink:href="615361v7_inline61.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is predicted by the model, and Δ<italic>p</italic><sup><italic>D</italic></sup> is recorded in mice. Light-shaded circles show individual reconstructed networks with different initializations. The whiskers are the standard error of means. <bold>E</bold>. Examples of <inline-formula id="inline-eqn-11"><inline-graphic xlink:href="615361v7_inline62.gif" mimetype="image" mime-subtype="gif"/></inline-formula> hit rate changes under perturbation for wS1 (Top) and tjM1 (Bottom). The black circles refer to the hit rate change from the recordings, Δ<italic>p</italic><sup><italic>D</italic></sup>. See <xref rid="figS7" ref-type="fig">Suppl. Fig. S7</xref> for the other areas.</p></caption>
<graphic xlink:href="615361v7_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Consistent with the synthetic dataset, we now find with this in vivo dataset that modeling cell-type connectivity yields better prediction of the causal effect of optogenetic perturbation. We denote by Δ<italic>p</italic><sup><italic>D</italic></sup> the in vivo change in lick probability across Go trials in response to optogenetic perturbations. The perturbations were performed in different periods for each area in <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref> (stimulation, delay, or choice periods). For all areas and time windows, we measure the corresponding <inline-formula id="inline-eqn-12"><inline-graphic xlink:href="615361v7_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the model. On average, the error change probability obtained with the <italic>σ</italic>RNN model is <inline-formula id="inline-eqn-13"><inline-graphic xlink:href="615361v7_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> which is significantly worse than the bioRNN model’s 16% (t-test p-value is 0.014, see <xref rid="fig3" ref-type="fig">Figure 3D</xref>). As in the synthetic dataset, we find this to be consistent over multiple bioRNN model variants, and we find that imposing Dale’s law and local inhibition best explain the improvement in perturbation-robustness. We also measure that the spiking bioRNN predicts the change in lick probability slightly better than the “No Spike” bioRNN model. Conversely, adding the sparsity prior does not seem to improve the perturbed hit-rate prediction on the real data as seen in the recurrent artificial dataset (RefCirc2) and not in the feedforward case (RefCirc1) as shown in <xref rid="figS5" ref-type="fig">Suppl. Fig. S5</xref>. In this sense, in vivo perturbation testing emerges as a hard test to evaluate modeling strategies combining deep learning and biophysical modeling.</p>
<p>To further analyze the consistency of the perturbations in the model, we can compare the perturbation map showing changes in lick probability obtained from acute inactivation in the data and the model. The <xref rid="figS7" ref-type="fig">Suppl. Fig. S7</xref> summarizes visually which area has a critical role at specific time points. The changes of lick probability in area wS1, ALM and tjM1 are accurately predicted by the bioRNN. In contrast, our model tends to underestimate the causal effect induced by the inactivations of wS2, wM1 and wM2 (<xref rid="figS7" ref-type="fig">Suppl Fig S7</xref>). Overall, our model is consistent with a causal chain of interaction from wS1 to ALM and continuing to tjM1.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Applications for experimental electrophysiology</title>
<p>With future progress in recording technology and reconstruction methods, network reconstruction may soon predict the effect of optogenetic perturbation with even higher accuracy. In this section, we explore possible consequences and applications for experimental electrophysiology. We demonstrate in the following that (1) perturbation-robust bioRNNs enable us to estimate gradients of the recorded circuits, (2) which in turn enable us to target <italic>µ</italic>-perturbations in the recorded circuit and optimally increase (or decrease) induced movements in our simulated mouse. The so-called “recorded circuit” is a bioRNN trained on the in vivo dataset that we use as a proxy experimental preparation. Its mathematical underpinnings enable us to make rigorous theoretical considerations and the design of forward-looking in silico experiments.</p>
<sec id="s2c1">
<title><italic>µ</italic>-perturbations measure brain gradients</title>
<p>We first prove a mathematical relationship between gradients in the recorded circuit and <italic>µ</italic>-perturbations. We define the integrated movement as <italic>Y</italic> = ∑<sub><italic>t</italic></sub> <italic>y</italic><sub><italic>t</italic></sub> where <italic>y</italic><sub><italic>t</italic></sub> is the movement of the jaw at time <italic>t</italic> generated by the model, and we denote Δ<italic>Y</italic> <sup>↯</sup> as the change of movement caused by the <italic>µ</italic>-perturbation. If the circuit has well-defined gradients (e.g. say a “No spike” bioRNN model trained on the in vivo recordings in the previous section), using a Taylor expansion, we find that:
<disp-formula id="eqn1">
<graphic xlink:href="615361v7_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where ℐ are the neuron and time indices selected for the optogenetic intervention. The error term <italic>ϵ</italic> is negligible when the current <inline-formula id="inline-eqn-14"><inline-graphic xlink:href="615361v7_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> induced by the light is small. We first confirm this approximation with numerical visualization in <xref rid="fig4" ref-type="fig">Fig. 4A</xref>: we display movement perturbations ⟨Δ<italic>Y</italic> <sup>↯</sup> ⟩ in the circuit with time windows of decreasing sizes (⟨·⟩ indicates a trial average). When the time window is small, and the perturbation is only applied to excitatory or inhibitory cells in <xref rid="fig4" ref-type="fig">Fig. 4A</xref>, one can appreciate visually the similarity with the binned gradient <inline-formula id="inline-eqn-15"><inline-graphic xlink:href="615361v7_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in <xref rid="fig4" ref-type="fig">Fig. 4B</xref>. Proceeding to a quantitative verification of equation (1), we now compare the effect of small perturbations targeting only 20 neurons on a singletrial. We use the gradient <inline-formula id="inline-eqn-16"><inline-graphic xlink:href="615361v7_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (see <xref rid="fig4" ref-type="fig">Fig. 4C</xref>) to predict the outcome of <italic>µ</italic>-perturbation as follows: for each trial, and each 100ms time window, we identify 20 neurons in the model with highest (or lowest) gradients <inline-formula id="inline-eqn-17"><inline-graphic xlink:href="615361v7_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We then re-simulate the exact same trial with identical random seed, but induce a <italic>µ</italic>-perturbation on selected neurons (see rectangles in <xref rid="fig4" ref-type="fig">Figure 4</xref>). If we target neurons with strongly positive gradients, the perturbed jaw movements are strongly amplified Δ<italic>Y</italic> <sup>↯</sup> <italic>&gt;</italic> 0; conversely, if we target neurons with negative gradients the jaw movements are suppressed Δ<italic>Y</italic> <sup>↯</sup> <italic>&lt;</italic> 0. Although the equation (1) is only rigorously valid for models with well-defined gradients like the “No Spike” model, we also confirm in <xref rid="fig4" ref-type="fig">Fig. 4D</xref> that this numerical verification also holds in a spiking circuit model where the gradients are replaced with surrogate gradients (<xref ref-type="bibr" rid="c37">Neftci et al., 2019</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Measuring circuit gradients with <italic>µ</italic>-perturbations</title>
<p><bold>A-B.</bold> Numerical verification for equation (1). <bold>A</bold> shows the change of jaw movement Δ<italic>Y</italic> following inactivations in a “No Spike” bioRNN. From left to right, we reduce the size of the spatiotemporal window for the optogenetic stimulation. <bold>B</bold>. Gradients values <inline-formula id="inline-eqn-18"><inline-graphic xlink:href="615361v7_inline63.gif" mimetype="image" mime-subtype="gif"/></inline-formula> that approximate Δ<italic>Y</italic> from <bold>A</bold> using equation (1). <bold>C-D</bold>. Verification that gradients predict the change of movement on single trials. In <bold>C</bold>, we display the gradients and jaw movement for three different trials, the neurons targeted by the <italic>µ</italic>-perturbation are boxed and the perturbed jaw movement is blue. Results averaged for every 100ms stimulation windows are shown in <bold>D</bold>: positive (resp. negative) modulated means that the 20 neurons with highest (resp. lowest) gradients are targeted, random neurons are selected for the shuffled case.</p></caption>
<graphic xlink:href="615361v7_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>An implication of equation (1) i s that the measurements ⟨Δ<italic>Y</italic> <sup>↯</sup>⟩ that can be recorded in vivo are estimates of the gradients <inline-formula id="inline-eqn-19"><inline-graphic xlink:href="615361v7_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the recorded circuit. Yet, measuring detailed gradient maps (or perturbation maps) as displayed in <xref rid="fig4" ref-type="fig">Fig. 4</xref> would be costly in vivo as it requires to average Δ<italic>Y</italic> <sup>↯</sup> over dozens of trials for each spatio-temporal window. Instead, gradient calculation in a bioRNN model (that was fitted to the experimental preparation) is a rapid mathematical exercise. If the extracted model is valid, then the gradients <inline-formula id="inline-eqn-20"><inline-graphic xlink:href="615361v7_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the bioRNN approximate (1) the effect of <italic>µ</italic>-perturbations Δ<italic>Y</italic> <sup>↯</sup> in the experimental preparation; (2) the gradient <inline-formula id="inline-eqn-21"><inline-graphic xlink:href="615361v7_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the recorded circuit.</p>
</sec>
<sec id="s2c2">
<title>Targeting in vivo <italic>µ</italic>-perturbations with bioRNN gradients</title>
<p>Building on this theoretical finding, we build a speculative experimental setup where the bioRNN gradients are used to target a <italic>µ</italic>-perturbation and increase (or decrease) the movements <italic>Y</italic> in the experimental preparation in real time. We show a schematic of this speculative closed-loop experiment in <xref rid="fig5" ref-type="fig">Fig. 5C</xref> extending contemporary read-write elecotrophysiology setups (<xref ref-type="bibr" rid="c38">Packer et al., 2015</xref>; <xref ref-type="bibr" rid="c1">Adesnik and Abdeladim, 2021</xref>; <xref ref-type="bibr" rid="c21">Grosenick et al., 2015</xref>; <xref ref-type="bibr" rid="c41">Papagiakoumou et al., 2020</xref>). We demonstrate in silico in <xref rid="fig5" ref-type="fig">Fig. 5A-B</xref> how this experiment could use bioRNN gradients to bias the simulated mouse movement <italic>Y</italic>. As a preparation step, and before applying perturbations, we assume that the bioRNN is well fitted to the recorded circuit and we collect a large databank of ℬ simulated trials from the fitted bioRNN. Then in real-time, we record the activity from the experimental preparation until the time <italic>t</italic><sup>*</sup> at which the stimulation will be delivered (Step 1 in <xref rid="fig5" ref-type="fig">Fig. 5A</xref>, <italic>t</italic><sup>*</sup> is 100ms before the decision period). Rapidly, we find the trial with the closest spike trains in the databank of simulated trials (Step 2) and use the corresponding gradient maps to target neurons with the highest gra-dient <inline-formula id="inline-eqn-22"><inline-graphic xlink:href="615361v7_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the model (Step 3). The targeted stimulation is then delivered immediately at <italic>t</italic><sup>*</sup> to the experimental preparation (Step 4). When testing this in silico on our artificial experimental preparation, we show in <xref rid="fig5" ref-type="fig">Fig. 5C</xref> that this approach can bias the quantity of jaw movement <italic>Y</italic> driven by the circuit in a predictable way. The amount of movement is consistently augmented if we target neurons with the highest <inline-formula id="inline-eqn-23"><inline-graphic xlink:href="615361v7_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (or diminished if we target neurons with the lowest <inline-formula id="inline-eqn-24"><inline-graphic xlink:href="615361v7_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Gradient targeted <italic>µ</italic>-perturbations could precisely bias an animal behavior</title>
<p><bold>A</bold>. Protocol to deliver an optimal <italic>µ</italic>-perturbation on the experimental preparation based on jaw gradients. (Step 1) The circuit is recorded until stimulation time <italic>t</italic><sup>*</sup>. (Step 2) The closest bioRNN trial to the ongoing recorded trial is retrieved from the databank ℬ. (Step 3) We select the neurons with the highest (or lowest) gradient value for the <italic>µ</italic>-perturbation. (Step 4) The <italic>µ</italic>-perturbation is delivered at <italic>t</italic><sup>*</sup>. <bold>B</bold>. Effect of the <italic>µ</italic>-perturbation using the artificial setup <bold>A</bold> under different light protocols. Practically, for “High gradient”, we keep step 3 as it is, for “Low gradient”, we change the sign of the gradient, and for “Zero gradient”, we pick the 40 neurons with lowest gradient norm. <bold>C</bold>. Speculative schematic of a close-up setup implementing the protocol <bold>A</bold> inspired by the all optical “read-write” setup from <xref ref-type="bibr" rid="c2">Aravanis et al. (2007)</xref>; <xref ref-type="bibr" rid="c38">Packer et al. (2015)</xref>.</p></caption>
<graphic xlink:href="615361v7_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>Finding the right level of detail to model recorded phenomena has sparked intensive debates in computational neuroscience. When the goal is to achieve the strongest predictive power, generalist deep learning models have proven successful across many scientific disciplines, questioning how biophysical modeling plays a role in this context. Our results show that <italic>perturbation testing</italic> is a new approach to evaluate the implementation of biophysical features in a deep learning system. Our key finding about <italic>perturbation testing</italic> relies on the difficulty for deep learning models to predict the effect of optogenetic perturbations out-of-distribution (meaning, the perturbed trials are not available in the training set of the data-constrained model). We see that standard deep learning RNNs generalize poorly to perturbed trials, even when they achieved the best fit on the unperturbed test set. In contrast, this is alleviated with our bioRNN, which implements biophysical constraints that are relevant to the nature of the perturbation. In our case, modeling cell type connectivity is crucial because the optogenetic perturbations are targeted to these genetically encoded cell types. In this sense, we believe that these features were successful on the perturbation tests because they are central to modeling the perturbation of the deep learning system. Perturbation testing emerges as a quantitative tool to search for data-constrained models beyond two standard types of incomplete brain models in computational neuroscience: (1) physiologically detailed models intended to explain brain mechanisms but do not enable powerful quantitative predictions; (2) deep learning models with high predictive power but capturing a wrong biophysical mechanism, causing erroneous generalizations. We view our work as a simple and reasonable way to combine deep learning and biophysical modeling, while rigorously evaluating the combined models.</p>
<p>Our reconstruction method and modeling choices when building the data-constrained bioRNN are innovative and are validated on <italic>perturbation tests</italic>. We achieve a reconstruction of the sensory-motor pathway in the mouse cortex during a sensory detection task from electrophysiology data. The model is optimized to explain electrophysiological recordings and generalizes better than standard models to in vivo optogenetic interventions. We found unambiguously that anatomically informed sign and connectivity constraints for dominant excitatory and inhibitory cell types improve the model robustness to optogenetic perturba-tions. We also find that assuming that inhibitory connections are short and do not project to other areas is crucial to pass our optogenetic <italic>Perturbation test</italic>. Modeling spiking neuron dynamics and adding a sparsity prior yielded more nuanced results and was not decisive, showing that making a difference on <italic>Perturbation testing</italic> is challenging. In hindsight, we conclude that adding biological constraints becomes beneficial when (1) they model the interaction between the circuit and the perturbation mechanism; (2) their implementation should not impair the efficiency of the optimization process.</p>
<p>Broadly speaking, this hindsight is also supported by other results elsewhere in neuroscience. For instance, biologically inspired topological networks having higher correlation for neighboring neurons are more consistent with comparable causal interventions in the Monkey’s visual system <xref ref-type="bibr" rid="c49">Schrimpf et al. (2024)</xref>, and detailed cell-type distribution and connectome improve models of vision in the fly brain <xref ref-type="bibr" rid="c30">Lappalainen et al. (2023)</xref>; <xref ref-type="bibr" rid="c12">Cowley et al. (2024)</xref>. For future work, there is a dense knowledge of unexploited physiological data at the connectivity, laminar or cell-type level that could be added to improve a cortical model like ours (<xref ref-type="bibr" rid="c23">Harris et al., 2019</xref>; <xref ref-type="bibr" rid="c32">Liu et al., 2022</xref>; <xref ref-type="bibr" rid="c58">Udvary et al., 2022</xref>; <xref ref-type="bibr" rid="c53">Staiger and Petersen, 2021</xref>; <xref ref-type="bibr" rid="c47">Rimehaug et al., 2023</xref>). By submitting the extended models to the relevant <italic>perturbation tests</italic>, it becomes possible to measure quantitatively the goodness of their biological mechanism implementations. We do not rule out, that significant improvements on <italic>perturbation tests</italic> can also be achieved with other means (e.g. by training deep learning architectures <xref ref-type="bibr" rid="c4">Azabou et al. (2024)</xref>; <xref ref-type="bibr" rid="c40">Pandarinath et al. (2018)</xref>; <xref ref-type="bibr" rid="c63">Ye et al. (2023)</xref> on larger datasets to enable generalization, or with generic regularization techniques like low-rank connectivity <xref ref-type="bibr" rid="c16">Dubreuil et al. (2022)</xref>; <xref ref-type="bibr" rid="c60">Valente et al. (2022)</xref>). However, in a similar way as the <italic>σ</italic>RNN was apriori an excellent predictor on our initial test-set, any powerful brain model will likely have fail-ure modes that can be well characterized and measured with an appropriate perturbation test. So <italic>perturbation tests</italic> could become a central component of an iterative loop to identify needed data collection or model improvements towards robust brain models.</p>
<p>To highlight the importance of perturbation-robust circuit models, we have discussed possible implications for experimental neuroscience in section 2.3. We build the RNN twin of the biological circuit from unperturbed electrode recordings. By implementing the correct biophysical constraints, the RNN becomes perturbation robust (i.e. it predicts the effect of causal perturbation) even without including perturbation data in the RNN training. We then demonstrated in silico that gradients of this RNN produce sensitivity maps to target micro-stimulation of the biological circuit. As a result, we could design a hypothetical closedloop setup combining read-write electrophysiology with a brain model to influence the brain activity or behavior, having potentially important practical and ethical consequences. More conceptually, we have shown theoretically that the gradients of a perturbation robust RNN are also consistent with the gradients of the recorded biological circuits. In perspective with the foundational role of gradients in machine learning theory <xref ref-type="bibr" rid="c31">LeCun et al. (2015)</xref>; <xref ref-type="bibr" rid="c46">Richards and Kording (2023)</xref>, it enables the measurement of “brain gradients” and lays a computational link between in vivo experimental research and decades of theoretical results on artificial learning and cognition.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Mathematical toy model of the difficult causal inference between H1 and H2</title>
<p>Let’s consider two simplistic mathematical models that both depend on two binary random variables <italic>A</italic> and <italic>B</italic> which represent that putative area A is active as <italic>A</italic> = 1 and area B as <italic>B</italic> = 1. With this notation, we can construct two hypothetical causal mechanisms <italic>H</italic>1 (“a feedforward hypothesis”) and <italic>H</italic>2 (“a recurrent hypothesis”), which are radically different. The empirical frequency <italic>p</italic>(<italic>A, B</italic>) of the outcome does not allow us to differentiate whether the system was generated by a feedforward mechanism <italic>H</italic>1 or a recurrent mechanism <italic>H</italic>2. Schematically, we can represent the two mechanism hypotheses as follows:
<disp-formula id="eqn2">
<graphic xlink:href="615361v7_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="615361v7_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For hypothesis <italic>H</italic>1: we assume that external inputs are driving the activity of area A such that <italic>A</italic> = 1 is active with probability <italic>p</italic><sub>0</sub>, and there are strong feed-forward connections from <italic>A</italic> to <italic>B</italic> causing systemically <italic>B</italic> = 1 as soon as <italic>A</italic> = 1. Alternatively, in <italic>H</italic>2, we assume that areas A and B receive independent external inputs with probability <inline-formula id="inline-eqn-25"><inline-graphic xlink:href="615361v7_inline19.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Each of these two inputs is sufficient to cause <italic>A</italic> = 1 or <italic>B</italic> = 1, and the two areas are also strongly connected, so <italic>A</italic> = 1 always causes <italic>B</italic> = 1 and vice versa. Under these hypothetical mechanisms <italic>H</italic>1 and <italic>H</italic>2, one finds that the empirical probability table <italic>p</italic>(<italic>A, B</italic>) is identical: <inline-formula id="inline-eqn-26"><inline-graphic xlink:href="615361v7_inline20.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (“Hit trial”, both areas are active), <italic>p</italic>(<italic>A</italic> = 0, <italic>B</italic> = 0) = 1 − <italic>p</italic><sub>0</sub> (“Miss trial”, the areas are quiescent). To prove this: <italic>a</italic> and <italic>b</italic> to denote the binary external inputs into <italic>A</italic> and <italic>B</italic>, so we have: <italic>p</italic><sub><italic>H</italic>2</sub>(<italic>A</italic> = 1, <italic>B</italic> = 1) = ∑ <sub><italic>a,b</italic></sub> <italic>p</italic>(<italic>A</italic> = 1, <italic>B</italic> = 1|<italic>a, b</italic>)<italic>p</italic>(<italic>a, b</italic>) = <italic>p</italic>(<italic>a</italic> = 1, <italic>b</italic> = 1) + <italic>p</italic>(<italic>b</italic> = 0, <italic>a</italic> = 1) + <italic>p</italic>(<italic>b</italic> = 1, <italic>a</italic> = 0) where we used that <italic>p</italic>(<italic>A</italic> = 1, <italic>B</italic> = 1|<italic>a, b</italic>) is 0 or 1, then using <italic>p</italic>(<italic>a</italic> = 1) = <italic>p</italic>(<italic>b</italic> = 1) = <italic>p</italic>1 and the independence between <italic>a</italic> and <italic>b</italic> we find: <inline-formula id="inline-eqn-66"><inline-graphic xlink:href="615361v7_inline57.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. In both cases, the possibility that only one area is active is excluded by construction. So for any <italic>A</italic> and <italic>B p</italic><sub><italic>H</italic>1</sub>(<italic>A, B</italic>) = <italic>p</italic><sub><italic>H</italic>2</sub>(<italic>A, B</italic>) and in other words, even if we observe an infinite number of trials and compute any statistics of the binary activations <italic>A</italic> and <italic>B</italic>, discriminating the two possible causal interactions (H1 versus H2) is impossible.</p>
<p>A solution to discriminate between hypotheses <italic>H</italic>1 and <italic>H</italic>2 is to induce a causal perturbation. We can discriminate between our two hypotheses if we can impose a perturbation that forces the inactivation of area <italic>B</italic> in both mathematical models. In mathematical terms we refer to the <italic>do</italic> operator from causality theory. Under the feedforward mechanism <italic>H</italic>1 and inactivation of <italic>B, A</italic> is not affected <italic>p</italic><sub><italic>H</italic>1</sub> (<italic>A</italic> = 1 | <italic>do</italic> (<italic>B</italic> = 0)) = <italic>p</italic><sub>0</sub>. Under the recurrent hypothesis, <italic>H</italic>2, and inactivation of <italic>B, A</italic> is activated only by its external input such that <italic>p</italic><sub><italic>H</italic>2</sub> (<italic>A</italic> = 1 <italic>do</italic> (<italic>B</italic> = 0)) = <italic>p</italic>1 ≠ <italic>p</italic><sub>0</sub>. So the measurement of the frequency of activation of area <italic>A</italic> under inactivation of <italic>B</italic> can discriminate between <italic>H</italic>1 and <italic>H</italic>2 which illustrates mathematically how a causal perturbation can be decisive to discriminate between those two hypothetical mechanisms.</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Neuron and jaw movement model</title>
<p>We model neurons as leaky-integrate and fire (LIF) neurons. The output of every neuron <italic>j</italic> at time <italic>t</italic> is a binary outcome <inline-formula id="inline-eqn-27"><inline-graphic xlink:href="615361v7_inline21.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (spike if <inline-formula id="inline-eqn-28"><inline-graphic xlink:href="615361v7_inline22.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, no spike if <inline-formula id="inline-eqn-29"><inline-graphic xlink:href="615361v7_inline23.gif" mimetype="image" mime-subtype="gif"/></inline-formula>) generated from its membrane voltage <inline-formula id="inline-eqn-30"><inline-graphic xlink:href="615361v7_inline24.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The following equations give the dynamics of the membrane voltage <inline-formula id="inline-eqn-31"><inline-graphic xlink:href="615361v7_inline25.gif" mimetype="image" mime-subtype="gif"/></inline-formula>:
<disp-formula id="eqn4">
<graphic xlink:href="615361v7_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="615361v7_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula id="inline-eqn-32"><inline-graphic xlink:href="615361v7_inline26.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and <inline-formula id="inline-eqn-33"><inline-graphic xlink:href="615361v7_inline27.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are the recurrent and input weight matrices. The timestep of the simulation <italic>δt</italic> is 2 ms when we simulate the real dataset and 1 ms otherwise. The superscript <italic>d</italic> denotes the synaptic delay; every synapse has one synaptic delay of either 2 or 3 ms. with <inline-formula id="inline-eqn-34"><inline-graphic xlink:href="615361v7_inline28.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, we define the integration speed of the membrane voltage, where <italic>τ</italic><sub><italic>m</italic></sub> = 30 ms for excitatory and <italic>τ</italic><sub><italic>m</italic></sub> = 10 ms for inhibitory neurons. The noise source <inline-formula id="inline-eqn-35"><inline-graphic xlink:href="615361v7_inline29.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is a Gaussian random variable with zero mean and standard deviation <inline-formula id="inline-eqn-36"><inline-graphic xlink:href="615361v7_inline30.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<italic>β</italic><sub><italic>j</italic></sub> is typically initialized at 0.14). The input <inline-formula id="inline-eqn-37"><inline-graphic xlink:href="615361v7_inline30a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is a binary pulse signal with a duration of 10 ms. For the real dataset, we have two binary pulse input signals, one for the whisker deflection and one for the auditory cue. The spikes are sampled with a Bernoulli distribution <inline-formula id="inline-eqn-38"><inline-graphic xlink:href="615361v7_inline31.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>v</italic><sub>0</sub> is the temperature of the exponential function and <italic>v</italic><sub><italic>thr,j</italic></sub> is the effective membrane threshold. After each spike, the neuron receives a reset current with an amplitude of <italic>v</italic><sub><italic>trh,j</italic></sub> and enters an absolute refractory period of 4 ms, during which it cannot fire.</p>
<p>For networks fitted to the real dataset, we also simulate the jaw movement. The jaw movement trace <italic>y</italic> is controlled by a linear readout from the spiking activity of all excitatory neurons. Specifically, <italic>y</italic> is computed as <inline-formula id="inline-eqn-39"><inline-graphic xlink:href="615361v7_inline32.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>b</italic> is a scaling parameter and <inline-formula id="inline-eqn-40"><inline-graphic xlink:href="615361v7_inline33.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is given by <inline-formula id="inline-eqn-41"><inline-graphic xlink:href="615361v7_inline34.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Here, <inline-formula id="inline-eqn-42"><inline-graphic xlink:href="615361v7_inline35.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the output weight matrix (linear readout) for the jaw, and <italic>τ</italic><sub><italic>jaw</italic></sub> = 5ms defines <inline-formula id="inline-eqn-43"><inline-graphic xlink:href="615361v7_inline36.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which controls the integration velocity of the jaw trace.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Session-stitching and network structure</title>
<p>As in (<xref ref-type="bibr" rid="c51">Sourmpis et al., 2023</xref>), we simulate multi-area cortical neuronal activity fitted to electrophysiology neural recordings. Before we start the optimization, we define and fix each neuron’s area and cell type in the model by uniquely assigning them to a neuron from the recordings. For the real dataset from <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref>, the cell type is inferred from the cell’s action potential waveform (with fast-spiking neurons classified as inhibitory and regular-spiking neurons as excitatory). Most electrophysiology datasets include recordings from multiple sessions, and our method would typically require simultaneous recordings of all neurons. To address this challenge, similarly to (<xref ref-type="bibr" rid="c51">Sourmpis et al., 2023</xref>) we use the technique called “session-stitching” which allows neighboring modeled neurons to be mapped with neurons recorded across multiple sessions. This effectively creates a “collage” that integrates data from multiple sessions within our model. This approach has practical implications for our optimization process. Specifically, the trial-matching loss includes a term for each session, with the overall loss calculated as the average across all sessions (see 4.5).</p>
<p>For both the real and the synthetic datasets, we simulate each area with 250 LIF neurons and impose that each area has 200 excitatory neurons and 50 inhibitory. Respecting the observation that inhibitory neurons mostly project in the area that they belong to (<xref ref-type="bibr" rid="c55">Tamamaki and Tomioka, 2010</xref>; <xref ref-type="bibr" rid="c35">Markram et al., 2004</xref>), we don’t allow for across-area inhibitory connections. The “thalamic” input is available to every neuron of the circuit, and the “motor” output for the real dataset, i.e., jaw movement, is extracted with a trained linear readout from all the excitatory neurons of the network, see 4.2.</p>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Reference circuits for hypotheses 1 and 2</title>
<p>To build a synthetic dataset that illustrates the difficulty of separating the feedforward (H1) and recurrent hypotheses (H2), we construct two reference spiking circuit models RefCirc1 and RefCirc2. The two networks consists of two areas A and B, and their activity follows the hard causal inference problem from method 4.1, making it hard to distinguish A1 and A2 when recording the co-activation of A and B. Moreover, to make the problem even harder, the two networks are constructed to make it almost impossible to distinguish between H1 and H2 with dense recordings: the two circuits are designed to have the same PSTH and single-trial network dynamics despite their structural difference, one is feedforward and the other is recurrent.</p>
<p>To do so, RefCirc1 and 2 are circuit models that start from random network initializations following the specifications described in Methods 4.2 and 4.3. The only difference is that we do not allow feedback connections from A to B in RefCirc1, the construction below is otherwise identical. The synaptic weights of the two circuits are optimized with the losses described in Methods 4.5 to fit the identical target statistics in all areas: the same PSTH activity for each neuron and the same distribution of single-trial network dynamics. The target statistics are chosen so the activity in RefCirc1 and 2 resemble kinematics and statistics from a primary and a secondary sensory area. The baseline firing rates of the neurons is dictated by the target PSTH distribution and it follows a log-normal distribution, with excitatory neurons having a mean of 2.9 Hz and a standard deviation of 1.25 Hz and inhibitory neurons having a mean of 4.47 Hz and a standard deviation of 1.31 Hz. The distribution of single-trial activity is given by the targeted single-trial dynamics: in RefCirc1 and 2, the areas A and B respond to input 50% of the time with a transient population average response following a double exponential kernel characterized by <italic>τ</italic><sub><italic>rise</italic></sub> = 5 ms and <italic>τ</italic><sub><italic>fall</italic></sub> = 20 ms. Mimicking a short signal propagation between areas, these transients have a 4 ms delay in area A and 12 ms delay in B (relative to the onset time of the stimulus). To impose a “behavioral” hit versus miss distribution that could emerge from a feedforward and recurrent hypothesis (see method 4.1), the targeted population-averaged response of each trial is either a double-exponential transient in both area A and B (“Hit trials”), or remains at a baseline level in both areas (“Miss trials”) in the remaining trials. At the end of the training, we verified that RefCirc1 and RefCirc2 generate very similar network activity in the absence of perturbation (see <xref rid="figS1" ref-type="fig">Figure S1</xref>). The circuits are then frozen and used to generate the synthetic dataset. We generate 2000 trials from these RefCircs, 1000 of which are used for the training set and 1000 for the testing set.</p>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Optimization and loss function</title>
<p>The optimization method we use to fit our models is back-propagation through time (BPTT). To overcome the non-differentiability of the spiking function, we use surrogate gradients (<xref ref-type="bibr" rid="c37">Neftci et al., 2019</xref>). In particular, we use the piece-wise linear surrogate deriva-tive from <xref ref-type="bibr" rid="c6">Bellec et al. (2018b)</xref>. For the derivative calculations, we use <inline-formula id="inline-eqn-44"><inline-graphic xlink:href="615361v7_inline37.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and not <inline-formula id="inline-eqn-45"><inline-graphic xlink:href="615361v7_inline38.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We use sample-and-measure loss functions that rely on summary statistics, as in (<xref ref-type="bibr" rid="c7">Bellec et al., 2021</xref>; <xref ref-type="bibr" rid="c51">Sourmpis et al., 2023</xref>), to fit the networks to the data. Our loss function has two main terms: one to fit the trial-averaged activity of every neuron (ℒ<sub>neuron</sub>), and one to fit the single trial population average activity (ℒ <sub>trial</sub>), = ℒ<sub>neuron</sub> + ℒ<sub>trial</sub>. The two terms of the loss function are reweighted with a parameter-free multi-task method (<xref ref-type="bibr" rid="c13">Défossez et al., 2023</xref>) that enables the gradients to have comparable scales.</p>
<p>As in <xref ref-type="bibr" rid="c51">Sourmpis et al. (2023)</xref>: (1) To calculate the <bold>trial-averaged loss</bold>, we first filter the trial-averaged spiking activity <inline-formula id="inline-eqn-46"><inline-graphic xlink:href="615361v7_inline39.gif" mimetype="image" mime-subtype="gif"/></inline-formula> using a rolling average window (<italic>f</italic>) of 8 ms. We then normalize it by the trial-averaged filtered data activity, (<bold><italic>z</italic></bold><sup><italic>D</italic></sup> are recorded spike trains)
<disp-formula id="eqn6">
<graphic xlink:href="615361v7_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where ⟨.⟩ <sub><italic>t</italic></sub> is the time average, and <italic>σ</italic><sub><italic>t</italic></sub> the standard deviation over time. The trial-averaged loss function is defined as:
<disp-formula id="eqn7">
<graphic xlink:href="615361v7_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>T</italic> is the number of time points in a trial and <italic>N</italic> is the number of neurons. For the real dataset, where we want to fit also the jaw movement, we have an additional term for the trial-averaged filtered and normalized jaw, <inline-formula id="inline-eqn-47"><inline-graphic xlink:href="615361v7_inline40.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>y</italic> is the simulated jaw movement and <italic>y</italic><sup><italic>D</italic></sup> the recorded jaw movement.</p>
<p>(2) To calculate the <bold>trial-matching loss</bold>, we first filter the population-average activity of each area <inline-formula id="inline-eqn-48"><inline-graphic xlink:href="615361v7_inline41.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, using a rolling average window of 32 ms. We then normalize it by the population-averaged filtered activity of the same area from the recordings, <inline-formula id="inline-eqn-49"><inline-graphic xlink:href="615361v7_inline42.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and concatenate all the areas that were simultaneously recorded, <inline-formula id="inline-eqn-50"><inline-graphic xlink:href="615361v7_inline43.gif" mimetype="image" mime-subtype="gif"/></inline-formula>), where ⟨.⟩<sub><italic>k</italic></sub> is the trial average, and <italic>σ</italic><sub><italic>k</italic></sub> the standard deviation over trials. The trial-matching loss is defined as:
<disp-formula id="eqn8">
<graphic xlink:href="615361v7_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>π</italic> is an assignment between pairs of <italic>K</italic> recorded and generated trials <italic>π</italic> : {1, … <italic>K</italic>} → {1, … <italic>K</italic>}. Note that the minimum over <italic>π</italic> is a combinatorial optimization that needs to be calculated for every evaluation of the loss function. For the real dataset, we consider the jaw movement as an additional area, and we concatenate it to the <inline-formula id="inline-eqn-51"><inline-graphic xlink:href="615361v7_inline44.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
<p>Based on this loss function, we optimize the following parameters: <inline-formula id="inline-eqn-52"><inline-graphic xlink:href="615361v7_inline44a.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and <italic>β</italic> for the RefCircs. For the RNNs, we optimize only the recurrent connectivity <inline-formula id="inline-eqn-53"><inline-graphic xlink:href="615361v7_inline45.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and the rest are fixed from the RefCircs. For the real dataset, additionally to the parameters optimized in the RefCircs, we also optimize the jaw’s linear readout <inline-formula id="inline-eqn-54"><inline-graphic xlink:href="615361v7_inline46.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and its scaling parameter <italic>b</italic>.</p>
<sec id="s4e1">
<title>Implementing Dale’s law and local inhibition</title>
<p>In our network, the recurrent weights <inline-formula id="inline-eqn-55"><inline-graphic xlink:href="615361v7_inline47.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are computed as the elementwise product of two matrices: <inline-formula id="inline-eqn-56"><inline-graphic xlink:href="615361v7_inline48.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which encodes the strength of synaptic efficacies and is always positive, and <inline-formula id="inline-eqn-57"><inline-graphic xlink:href="615361v7_inline49.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which has a fixed sign determined by the neurotransmitter type of the presynaptic neuron and <inline-formula id="inline-eqn-58"><inline-graphic xlink:href="615361v7_inline50.gif" mimetype="image" mime-subtype="gif"/></inline-formula>:
<disp-formula id="eqn9">
<graphic xlink:href="615361v7_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To enforce Dale’s law during optimization, we set any negative values of <inline-formula id="inline-eqn-59"><inline-graphic xlink:href="615361v7_inline51.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to zero at each iteration as in <xref ref-type="bibr" rid="c5">Bellec et al. (2018a)</xref>. Similarly, to constrain only local inhibitory connections during optimization, we zero out any changes in the synaptic efficacies of acrossareas inhibitory connections at each iteration. In simplified models, Dale’s law or the local inhibition constraint can be disrupted by omitting this correction step.</p>
<p>The success of the network optimization highly depends on the initialization of the recurrent weight matrices. To initialize signed matrices we follow the theoretical <xref ref-type="bibr" rid="c45">Rajan and Abbott (2006)</xref> and practical insights <xref ref-type="bibr" rid="c6">Bellec et al. (2018b)</xref>; <xref ref-type="bibr" rid="c11">Cornford et al. (2020)</xref> developed previously. After defining the constraints on the weight signs <inline-formula id="inline-eqn-60"><inline-graphic xlink:href="615361v7_inline52.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the initialization amplitude <inline-formula id="inline-eqn-61"><inline-graphic xlink:href="615361v7_inline53.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for each target neuron is adjusted to a zero-summed input weights (the sum of incoming excitatory inputs is equal to the sum of inhibitory inputs). Then the weight amplitude is re-normalized by the modulus of its largest eigenvalue of <italic>W</italic> <sup><italic>rec</italic></sup>, so all the eigenvalues of this matrix <italic>W</italic> <sup><italic>rec</italic></sup> have modulus 1 or smaller.</p>
</sec>
<sec id="s4e2">
<title>Stopping criterion for the optimization</title>
<p>For the synthetic dataset, we train the models for 4000 gradient descent steps. For the real dataset, due to limited data and a noisy test set, we select the final model based on the optimization step that yields the best trial-type accuracy (closest to the trial-type accuracy from the data), derived from the jaw trace and whisker stimulus, along with the highest trial-matched Pearson correlation between the model and the recordings.</p>
</sec>
<sec id="s4e3">
<title>Sparsity regularization</title>
<p>There is a plethora of ways to enforce sparsity. In this work, we use weight regularization. In particular, we use the <inline-formula id="inline-eqn-62"><inline-graphic xlink:href="615361v7_inline54.gif" mimetype="image" mime-subtype="gif"/></inline-formula> norm of the recurrent and input weights that promote a high level of sparsity (<xref ref-type="bibr" rid="c62">Xu et al., 2012</xref>). To avoid numerical instabil-ities, we apply this regularization only for synaptic weights above <italic>α</italic> and prune all synapses below <italic>α</italic>. (we set <italic>α</italic> = 1<italic>e</italic><sup>−7</sup>). The regularized loss function becomes:
<disp-formula id="eqn10">
<graphic xlink:href="615361v7_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>W</italic> <sup>across,<italic>d</italic></sup> are the connections from one area to the other.</p>
<p>For the synthetic dataset, we choose the level of across-area sparsity by performing a small grid search for <italic>λ</italic><sub>3</sub>. In particular, the sparsity level <italic>λ</italic><sub>3</sub> is the maximum value <italic>λ</italic><sub>3</sub> where the performance remains as good as without sparsity, see <xref rid="figS3" ref-type="fig">Suppl. Fig S3</xref>. For the real dataset, we use the same value <italic>λ</italic><sub>3</sub> as the one we found for the full reconstruction method of bioRNN1.</p>
</sec>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>Perturbation test of in silico optogenetics</title>
<p>In systems neuroscience, a method to test causal interactions between brain regions uses spatially and temporally precise optogenetic activations or inactivations (<xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="c22">Guo et al., 2014</xref>). Usually, inactivations refer to the strong activation of inhibitory neurons for cortical areas. These inhibitory neurons have strong intra-area connections that effectively “silence” their local-neighborhood (<xref ref-type="bibr" rid="c24">Helmstaedter et al., 2009</xref>).</p>
<p>Our model can simulate these perturbations and allow us to compare the causal mechanisms of two networks based on their responses to optogenetic perturbations. We implement activations and inactivations as a strong input current to all the neurons in one area’s excitatory or inhibitory population. For the RefCircs and reconstructed RNNs, we use a transient current that lasts 40 ms, from 20 ms before to 20 ms after the input stimulus. The strength of the current (light power) varies until there is an effect in the full reconstruction method bioRNN1. For the synthetic dataset in <xref rid="fig2" ref-type="fig">Figure 2</xref> (except for panel D), we inject a current of<inline-formula id="inline-eqn-63"><inline-graphic xlink:href="615361v7_inline55.gif" mimetype="image" mime-subtype="gif"/></inline-formula> into excitatory neurons for activations and <inline-formula id="inline-eqn-64"><inline-graphic xlink:href="615361v7_inline56.gif" mimetype="image" mime-subtype="gif"/></inline-formula> into inhibitory neurons for inactivations. For the real dataset, we perform optogenetics inactivations in three different periods. As in <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref>, we silence the cortical circuit during the whisker presentation, the time between the whisker and auditory stimulus, or when the animal was licking for the reward. In particular, we use transient currents to the inhibitory neurons during (i.) 100 ms before and after the whisker presentation, (ii.) 100 ms after the whisker presentation till 100ms before the onset of the auditory cue, and (iii.) after the auditory cue till the end of our simulation. For cases (i.) and (ii.), we linearly decreased the strength of the current to avoid rebound excitation. The light power is chosen so that our model has the best results in reproducing the lick probability of the recordings. It is important to mention that the perturbation data are not used to optimize the network but to test whether the resulting network has the same causal interactions with the recordings.</p>
<p>For the RefCircs and bioRNNs, we evaluate the effect of the perturbations directly from the neural activity. We use the distance of network dynamics ℒ<sub>trial</sub> to compare the two perturbed networks. For the real dataset, we compare the effect of the inactivations on the behavior; as behavior here, we mean whether the mouse/model licked. We classify the licking action using a multilayer perceptron with two hidden layers with 128 neurons each. The classifier is trained with the jaw movement of the real dataset, which was extracted from video filming using custom software, to predict the lick action, which was extracted from a piezo sensor placed in the spout. This classifier predicted lick correctly 94% of the time. We then used the same classifier on the jaw movement from the model to determine whether there was a “lick” or not. For the comparisons in both the artificial and real datasets, we trained multiple models with different random seeds for each variant and aggregated the results. The different random seeds affect both the weight initialization and the noise of our model. In particular, we used from 3 to 6 different random seeds for each different model variant.</p>
</sec>
</sec>
</body>
<back>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>The code for this project is open-sourced and published at <ext-link ext-link-type="uri" xlink:href="https://github.com/Sourmpis/BiologicallyInformed">https://github.com/Sourmpis/BiologicallyInformed</ext-link> . The dataset for the artificial dataset can be downloaded/generated on our code repository. The in vivo dataset was published openly for the previous publication <xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>. The dataset is accessible at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4720013">https://doi.org/10.5281/zenodo.4720013</ext-link> .</p>
</sec>
<sec id="s6" sec-type="supplementary">
<title>Supplemental information</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1:</label>
<caption><title>Modeling “optogenetic” perturbations.</title>
<p><bold>A</bold>. Two different network hypotheses for implementing a detection task. In RefCirc1, area A projects to area B but not vice versa. In RefCirc2, the areas are recurrently connected. <bold>B</bold>. Raster plots of all neurons in RefCirc1 during a single hit trial under normal conditions (control, left) and under optogenetic perturbation of excitatory (middle) and inhibitory (right) neurons. The duration of the light stimulus is shown with a blue shading. <bold>C</bold>. Same for RefCirc2 <bold>D</bold>. Trial-averaged activity of the two circuits during Hit (blue: RefCirc1; green: RefCirc2) and Miss (yellow: RefCirc 1; red: RefCirc2) trials. A trial is classified as “Hit” if area <italic>A</italic> reaches a transient firing rate above 8Hz; and otherwise as “Miss”. For the control case, the maximal difference between the trial average activity of the two networks is below 0.51 Hz (zoom inset).</p></caption>
<graphic xlink:href="615361v7_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2:</label>
<caption><title>Fitting Reconstructed networks to the synthetic dataset.</title>
<p><bold>A</bold>. Schematic representation of the RefCirc1 and bioRNN1. and probability of hit trials. <bold>B</bold>. Histogram of the firing rate distribution of the RefCirc1 and all the RNN1 versions. We observe that all RNN1 versions fit well with the RefCirc1. <bold>C</bold>. Left: Neuron loss of the different RNN1 variants. Right: Trial-matching loss of the different RNN1 variants. We observe that the model without the trial-matching loss function behaves considerably worse. The whiskers show the 95% confidence interval of the mean across trials. <bold>D-F</bold>. Same as <bold>A-B</bold> for RefCirc2 and RNNs2.</p></caption>
<graphic xlink:href="615361v7_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3:</label>
<caption><title>Picking the sparsity level.</title>
<p><bold>A</bold>. Grid search for the optimal maximum regularization strength (<italic>λ</italic><sub>3</sub>) without a drop in performance. As a performance measure, we used the trial-matching loss, ℒ<sub>trial</sub>.</p></caption>
<graphic xlink:href="615361v7_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4:</label>
<caption><title>Trial averaged traces across RNN variants.</title>
<p>Trial-averaged activity in area <italic>A</italic> under activation/inactivation of area <italic>B</italic>. Dashed black lines indicate the activity of RefCirc1 (thick dashed) and RefCirc2 (thin dashed). All the RNNs are tested with the same reference circuit and training data, each bioRNN model variant is shown with a different color.</p></caption>
<graphic xlink:href="615361v7_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5:</label>
<caption><title><bold>A</bold> Hit frequency prediction error <inline-formula id="inline-eqn-65"><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="615361v7_inline64.gif"/></inline-formula> as in <xref ref-type="fig" rid="fig2">Figure 2E</xref>.</title>
<p>In contrast to <xref ref-type="fig" rid="fig2">Figure 2E</xref>, here we show separately the change of hit probability for RefCirc1 (left) and RefCirc2 (right).</p></caption>
<graphic xlink:href="615361v7_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6:</label>
<caption><title>Reconstruction of the real recordings.</title>
<p><bold>A</bold>. Probability of hit trials of the different variant models. <bold>B</bold>. Histogram of the firing rate distribution from the real recordings and all the variants. <bold>C</bold>. Top: Neuron loss of the different RNN1 variants. All RNN versions have a similar loss value. Bottom: Trial-matching loss of the different model variants. We observe that the model without the trial-matching loss function behaves considerably worse. The whiskers show the 95% confidence interval.</p></caption>
<graphic xlink:href="615361v7_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Figure S7:</label>
<caption><title><bold>A</bold> Change of lick probability under inactivation of all areas in all the different temporal windows.</title>
<p>We show the Δ<italic>p</italic> from the data and reconstruction model variants.</p></caption>
<graphic xlink:href="615361v7_figS7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Alireza Modirshanechi, Shuqi Wang, and Tâm Nguyen for their valuable feedback on the manuscript. We are grateful to Vahid Esmaeili for collecting the dataset and ongoing support throughout this project. This research is supported by the Sinergia project CR-SII5 198612, the Swiss National Science Foundation (SNSF) project 200020 207426 awarded to WG, SNSF projects TMAG-3 209271 and 31003A 182010 awarded to CP, and the Vienna Science and Technology Fund (WWTF) project VRG24-018 awarded to GB.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adesnik</surname>, <given-names>H.</given-names></string-name> and <string-name><surname>Abdeladim</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Probing neural codes with two-photon holographic optogenetics</article-title>. <source>Nature Neuroscience</source>, <volume>24</volume>(<issue>10</issue>):<fpage>1356</fpage>–<lpage>1366</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aravanis</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>L.-P.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Meltzer</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Mogri</surname>, <given-names>M. Z.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>M. B.</given-names></string-name>, and <string-name><surname>Deisseroth</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2007</year>). <article-title>An optical neural interface: in vivo control of rodent motor cortex with integrated fiberoptic and optogenetic technology</article-title>. <source>Journal of Neural Engineering</source>, <volume>4</volume>(<issue>3</issue>):<fpage>S143</fpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arthur</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Preibisch</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Darshan</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2023</year>). <article-title>A scalable implementation of the recursive least-squares algorithm for training spiking neural networks</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>17</volume>:<fpage>1099510</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Azabou</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Arora</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Ganesh</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Mao</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Nachimuthu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Mendelson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Perich</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lajoie</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Dyer</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2024</year>). <article-title>A unified, scalable framework for neural population decoding</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bellec</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kappel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Maass</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Legenstein</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2018a</year>). <article-title>Deep rewiring: Training very sparse deep networks</article-title>. <conf-name>Iclr</conf-name>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bellec</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Salaj</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Subramoney</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Legenstein</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Maass</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2018b</year>). <article-title>Long shortterm memory and learning-to-learn in networks of spiking neurons</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>31</volume>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bellec</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Modirshanechi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Brea</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Fitting summary statistics of neural data with a differentiable spiking network simulator</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>34</volume>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Billeh</surname>, <given-names>Y. N.</given-names></string-name>, <string-name><surname>Cai</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Gratiy</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Iyer</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gouwens</surname>, <given-names>N. W.</given-names></string-name>, <string-name><surname>Abbasi-Asl</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Olsen</surname>, <given-names>S. R.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2020</year>). <article-title>Systematic integration of structural and functional data into multi-scale models of mouse primary visual cortex</article-title>. <source>Neuron</source>, <volume>106</volume>(<issue>3</issue>):<fpage>388</fpage>–<lpage>403</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boyden</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bamberg</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Nagel</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Deisseroth</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Millisecondtimescale, genetically targeted optical control of neural activity</article-title>. <source>Nature neuroscience</source>, <volume>8</volume>(<issue>9</issue>):<fpage>1263</fpage>–<lpage>1268</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Scherr</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Maass</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A data-based large-scale model for primary visual cortex enables brain-like robust and versatile visual processing</article-title>. <source>Science Advances</source>, <volume>8</volume>(<issue>44</issue>):<fpage>eabq7592</fpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Cornford</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kalajdzievski</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Leite</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lamarquette</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kullmann</surname>, <given-names>D. M.</given-names></string-name>, and <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Learning to live with dale’s principle: Anns with separate excitatory and inhibitory units</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cowley</surname>, <given-names>B. R.</given-names></string-name>, <string-name><surname>Calhoun</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Rangarajan</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Ireland</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name>, and <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Mapping model units to visual neurons reveals population code for social behaviour</article-title>. <source>Nature</source>, <volume>629</volume>(<issue>8014</issue>):<fpage>1100</fpage>–<lpage>1108</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Défossez</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Copet</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Synnaeve</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Adi</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2023</year>). <article-title>High fidelity neural audio compression</article-title>. <conf-name>Transactions on Machine Learning Research</conf-name>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deny</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ferrari</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Mace</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Yger</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Caplette</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Picaud</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tkacik</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Marre</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Multiplexed computations in retinal ganglion cells of a single type</article-title>. <source>Nature Communications</source>, <volume>8</volume>(<issue>1</issue>):<fpage>1964</fpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dinc</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Shai</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schnitzer</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Tanaka</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Cornn: Convex optimization of recurrent neural networks for rapid inference of neural dynamics</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dubreuil</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Valente</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Beiran</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mastrogiuseppe</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Ostojic</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The role of population structure in computations through neural dynamics</article-title>. <source>Nature Neuroscience</source>, <volume>25</volume>(<issue>6</issue>):<fpage>783</fpage>–<lpage>794</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Eccles</surname>, <given-names>J. C.</given-names></string-name></person-group> (<year>1976</year>). <source>From Electrical to Chemical Transmission in the Central Nervous System: The Closing Address of the Sir Henry Dale Centennial Symposium</source>. <publisher-name>Springer</publisher-name>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elman</surname>, <given-names>J. L.</given-names></string-name></person-group> (<year>1990</year>). <article-title>Finding structure in time</article-title>. <source>Cognitive Science</source>, <volume>14</volume>(<issue>2</issue>):<fpage>179</fpage>–<lpage>211</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esmaeili</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Tamura</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Muscinelli</surname>, <given-names>S. P.</given-names></string-name>, <string-name><surname>Modirshanechi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Boscaglia</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Oryshchuk</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Foustoukos</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Crochet</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2021</year>). <article-title>Rapid suppression and sustained activation of distinct cortical regions for a delayed sensory-triggered motor response</article-title>. <source>Neuron</source>, <volume>109</volume>(<issue>13</issue>):<fpage>2183</fpage>–<lpage>2201</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Fraile</surname>, <given-names>J. G.</given-names></string-name>, <string-name><surname>Scherr</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ramasco</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Arkhipov</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Maass</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Mirasso</surname>, <given-names>C. R.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Competition between bottom-up visual input and internal inhibition generates error neurons in a model of the mouse primary visual cortex</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grosenick</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Marshel</surname>, <given-names>J. H.</given-names></string-name>, and <string-name><surname>Deisseroth</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Closed-loop and activity-guided optogenetic control</article-title>. <source>Neuron</source>, <volume>86</volume>(<issue>1</issue>):<fpage>106</fpage>–<lpage>139</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname>, <given-names>Z. V.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Huber</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ophir</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Gutnisky</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ting</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Flow of cortical activity underlying a tactile decision in mice</article-title>. <source>Neuron</source>, <volume>81</volume>(<issue>1</issue>):<fpage>179</fpage>–<lpage>194</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Mihalas</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hirokawa</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Whitesell</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Bernard</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bohn</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Caldejon</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Casal</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Cho</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2019</year>). <article-title>Hierarchical organization of cortical and thalamic connectivity</article-title>. <source>Nature</source>, <volume>575</volume>(<issue>7781</issue>):<fpage>195</fpage>–<lpage>202</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Helmstaedter</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sakmann</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Feldmeyer</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2009</year>). <article-title>L2/3 interneuron groups defined by multiparameter analysis of axonal projection, dendritic geometry, and electrical excitability</article-title>. <source>Cerebral Cortex</source>, <volume>19</volume>(<issue>4</issue>):<fpage>951</fpage>–<lpage>962</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hodgkin</surname>, <given-names>A. L.</given-names></string-name></person-group> (<year>1958</year>). <article-title>The croonian lecture-ionic movements and electrical activity in giant nerve fibres</article-title>. <source>Proceedings of the Royal Society of London. Series B-Biological Sciences</source>, <volume>148</volume>(<issue>930</issue>):<fpage>1</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><collab>International Brain Laboratory</collab>, <string-name><surname>Benson</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Benson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Birman</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bonacchi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Catarino</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Chapuis</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Dan</surname>, <given-names>Y.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2023</year>). <article-title>A brain-wide map of neural activity during complex behaviour</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Isbister</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pokorny</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bolanos-Puchet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Santander</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Arnaudon</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Awile</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Barros-Zulaica</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Alonso</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Boci</surname>, <given-names>E.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2023</year>). <article-title>Modeling and simulation of neocortical micro-and mesocircuitry. part ii: Physiology and experimentation</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Finkelstein</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chow</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Darshan</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Distributing task-related neural activity across a cortical network through task-independent connections</article-title>. <source>Nature Communications</source>, <volume>14</volume>(<issue>1</issue>):<fpage>2851</fpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lakunina</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Socha</surname>, <given-names>K. Z.</given-names></string-name>, <string-name><surname>Ladd</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bowen</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Colonell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Doshi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Karsh</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Krumin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kulik</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Neutens</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>O’Callaghan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Olsen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Putzeys</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Reddy</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Tilmans</surname>, <given-names>H. A.</given-names></string-name>, <string-name><surname>Vargas</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Welkenhuysen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ye</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Häusser</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ting</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Consortium</surname>, <given-names>N. O.</given-names></string-name>, <string-name><surname>Dutta</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, and <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2025</year>). <article-title>Neuropixels opto: Combining high-resolution electrophysiology and optogenetics</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lappalainen</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Tschopp</surname>, <given-names>F. D.</given-names></string-name>, <string-name><surname>Prakhya</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>McGill</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nern</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Shinomiya</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Takemura</surname>, <given-names>S.-y.</given-names></string-name>, <string-name><surname>Gruntman</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Macke</surname>, <given-names>J. H.</given-names></string-name>, and <string-name><surname>Turaga</surname>, <given-names>S. C.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Connectomeconstrained deep mechanistic networks predict neural responses across the fly visual sys-tem at single-neuron resolution</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Deep learning</article-title>. <source>Nature</source>, <volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>444</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Foustoukos</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Crochet</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Petersen</surname>, <given-names>C. C.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Axonal and dendritic morphology of excitatory neurons in layer 2/3 mouse barrel cortex imaged through whole-brain two-photon tomography and registered to a digital brain atlas</article-title>. <source>Frontiers in Neuroanatomy</source>, <volume>15</volume>:<fpage>791015</fpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahuas</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Isacchini</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Marre</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Ferrari</surname>, <given-names>U.</given-names></string-name>, and <string-name><surname>Mora</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2020</year>). <article-title>A new inference approach for training shallow and deep generalized linear models of noisy interacting neurons</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>33</volume>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markram</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Muller</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ramaswamy</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Reimann</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Abdellah</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sanchez</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Ailamaki</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Alonso-Nanclares</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Antille</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Arsever</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2015</year>). <article-title>Reconstruction and simulation of neocortical microcircuitry</article-title>. <source>Cell</source>, <volume>163</volume>(<issue>2</issue>):<fpage>456</fpage>–<lpage>492</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markram</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Toledo-Rodriguez</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Gupta</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Silberberg</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Wu</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Interneurons of the neocortical inhibitory system</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>5</volume>(<issue>10</issue>):<fpage>793</fpage>–<lpage>807</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Minai</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Soldado-Magraner</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Yu</surname>, <given-names>B. M.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Miso: Optimizing brain stimulation to create neural activity states</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>37</volume>:<fpage>24126</fpage>–<lpage>24149</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neftci</surname>, <given-names>E. O.</given-names></string-name>, <string-name><surname>Mostafa</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Zenke</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks</article-title>. <source>IEEE Signal Processing Magazine</source>, <volume>36</volume>(<issue>6</issue>):<fpage>51</fpage>–<lpage>63</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Packer</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Russell</surname>, <given-names>L. E.</given-names></string-name>, <string-name><surname>Dalgleish</surname>, <given-names>H. W.</given-names></string-name>, and <string-name><surname>Häusser</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Simultaneous all-optical manipulation and recording of neural circuit activity with cellular resolution in vivo</article-title>. <source>Nature Methods</source>, <volume>12</volume>(<issue>2</issue>):<fpage>140</fpage>–<lpage>146</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Pals</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sagtekin</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Pei</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Gloeckler</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Macke</surname>, <given-names>J. H.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Inferring stochastic low-rank recurrent neural networks from neural data</article-title>. <source>arXiv</source>:<pub-id pub-id-type="arxiv">2406.16749</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pandarinath</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>O’Shea</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Collins</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jozefowicz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Stavisky</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Kao</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Trautmann</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name>, <string-name><surname>Hochberg</surname>, <given-names>L. R.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2018</year>). <article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title>. <source>Nature Methods</source>, <volume>15</volume>(<issue>10</issue>):<fpage>805</fpage>–<lpage>815</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Papagiakoumou</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ronzitti</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Emiliani</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Scanless two-photon excitation with temporal focusing</article-title>. <source>Nature Methods</source>, <volume>17</volume>(<issue>6</issue>):<fpage>571</fpage>–<lpage>581</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Perich</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Arlt</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Soares</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Young</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Mosher</surname>, <given-names>C. P.</given-names></string-name>, <string-name><surname>Minxha</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Rutishauser</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Rudebeck</surname>, <given-names>P. H.</given-names></string-name>, <string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2020</year>). <article-title>Inferring brainwide interactions using data-constrained recurrent neural network models</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Shlens</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Paninski</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sher</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Litke</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Chichilnisky</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source>, <volume>454</volume>(<issue>7207</issue>):<fpage>995</fpage>–<lpage>999</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pozzorini</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Mensi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hagens</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Naud</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Automated high-throughput characterization of single neurons by means of simplified spiking models</article-title>. <source>PLoS Computational Biology</source>, <volume>11</volume>(<issue>6</issue>):<fpage>e1004275</fpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rajan</surname>, <given-names>K.</given-names></string-name> and <string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Eigenvalue spectra of random matrices for neural networks</article-title>. <source>Physical Review Letters</source>, <volume>97</volume>(<issue>18</issue>):<fpage>188104</fpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richards</surname>, <given-names>B. A.</given-names></string-name> and <string-name><surname>Kording</surname>, <given-names>K. P.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The study of plasticity has always been about gradients</article-title>. <source>The Journal of Physiology</source>, <volume>601</volume>(<issue>15</issue>):<fpage>3141</fpage>–<lpage>3149</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rimehaug</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Stasik</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Hagen</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Billeh</surname>, <given-names>Y. N.</given-names></string-name>, <string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Olsen</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Einevoll</surname>, <given-names>G. T.</given-names></string-name>, and <string-name><surname>Arkhipov</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Uncovering circuit mechanisms of current sinks and sources with biophysical simulations of primary visual cortex</article-title>. <source>eLife</source>, <volume>12</volume>:<elocation-id>e87169</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.87169</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rosenblatt</surname>, <given-names>F.</given-names></string-name></person-group> (<year>1960</year>). <chapter-title>Perceptual generalization over transformation groups</chapter-title>. <source>Self Organizing Systems</source>, <publisher-name>Pergamon Press</publisher-name>, <publisher-loc>NY</publisher-loc>. pages <fpage>63</fpage>–<lpage>96</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Schrimpf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>McGrath</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Margalit</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Do topographic deep ann models of the primate ventral stream predict the perceptual effects of direct it cortical interventions?</article-title> <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Durand</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gale</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Graddis</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Heller</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ramirez</surname>, <given-names>T. K.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Luviano</surname>, <given-names>J. A.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2021</year>). <article-title>Survey of spiking in the mouse visual system reveals functional hierarchy</article-title>. <source>Nature</source>, <volume>592</volume>(<issue>7852</issue>):<fpage>86</fpage>–<lpage>92</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sourmpis</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Bellec</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Trial matching: capturing variability with data-constrained spiking neural networks</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Spieler</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rahaman</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Martius</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Schölkopf</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Levina</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The expressive leaky memory neuron: an efficient and expressive phenomenological neuron model can solve long-horizon tasks</article-title>. In <conf-name>The Twelfth International Conference on Learning Rep-resentations</conf-name>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Staiger</surname>, <given-names>J. F.</given-names></string-name> and <string-name><surname>Petersen</surname>, <given-names>C. C.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Neuronal circuits in barrel cortex for whisker sensory perception</article-title>. <source>Physiological Reviews</source>, <volume>101</volume>(<issue>1</issue>):<fpage>353</fpage>–<lpage>415</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Zatka-Haas</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Distributed coding of choice, action and engagement across the mouse brain</article-title>. <source>Nature</source>, <volume>576</volume>(<issue>7786</issue>):<fpage>266</fpage>– <lpage>273</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tamamaki</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Tomioka</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Long-range gabaergic connections distributed throughout the neocortex and their possible function</article-title>. <source>Frontiers in Neuroscience</source>, <volume>4</volume>:<fpage>202</fpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tamura</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bech</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mizuno</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Veaute</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Crochet</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Petersen</surname>, <given-names>C. C.</given-names></string-name></person-group> (<year>2025</year>). <article-title>Cell-class-specific orofacial motor maps in mouse neocortex</article-title>. <source>Current Biology</source>, <volume>35</volume>(<issue>6</issue>):<fpage>1382</fpage>–<lpage>1390</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teeter</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Iyer</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Menon</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Gouwens</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Szafer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cain</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Zeng</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Hawrylycz</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2018</year>). <article-title>Generalized leaky integrate-and-fire models classify multiple neuron types</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>):<fpage>709</fpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Udvary</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Harth</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Macke</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Hege</surname>, <given-names>H.-C.</given-names></string-name>, <string-name><surname>de Kock</surname>, <given-names>C. P.</given-names></string-name>, <string-name><surname>Sakmann</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Oberlaender</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The impact of neuron morphology on cortical network architecture</article-title>. <source>Cell Reports</source>, <volume>39</volume>(<issue>2</issue>).</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Urai</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Doiron</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Leifer</surname>, <given-names>A. M.</given-names></string-name>, and <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Large-scale neural recordings call for new insights to link brain and behavior</article-title>. <source>Nature Neuroscience</source>, <volume>25</volume>(<issue>1</issue>):<fpage>11</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Valente</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name>, and <string-name><surname>Ostojic</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Extracting computational mechanisms from neural data using low-rank rnns</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>35</volume>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Wyart</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Kumar</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Peysson</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Carbo-Tano</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Fidelin</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Didelot</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lejeune</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Grosse-Wentrup</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2025</year>). <article-title>All-optical investigation reveals a hierarchical organization of vsx2+ reticulospinal neurons coordinating steering and forward locomotion</article-title>. <source>Research Square</source></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2012</year>). <article-title>L_{1/2} regularization: A thresholding representation theory and a fast solver</article-title>. <source>IEEE Transactions on neural networks and learning systems</source>, <volume>23</volume>(<issue>7</issue>):<fpage>1013</fpage>–<lpage>1027</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ye</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Collinger</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wehbe</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Gaunt</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Neural data transformer 2: multicontext pretraining for neural spiking activity</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>:<fpage>80352</fpage>–<lpage>80374</lpage>.</mixed-citation></ref>
<ref id="dataref1"><mixed-citation publication-type="data" specific-use="analyzed"><person-group person-group-type="author"><string-name><surname>Esmaeili</surname></string-name></person-group> (<year iso-8601-date="2021">2021</year>) <article-title>Data set for &quot;Rapid suppression and sustained activation of distinct cortical regions for a delayed sensory-triggered motor response&quot;</article-title>. <source>Zenodo</source>. <pub-id pub-id-type="doi">10.5281/zenodo.4720013</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106827.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study demonstrates the significance of incorporating biological constraints in training neural networks to develop models that make accurate predictions under novel conditions. By comparing standard sigmoid recurrent neural networks (RNNs) with biologically constrained RNNs, the manuscript offers <bold>compelling</bold> evidence that biologically grounded inductive biases enhance generalization to perturbed conditions. This manuscript will appeal to a wide audience in systems and computational neuroscience.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106827.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This manuscript introduces a biologically informed RNN (bioRNN) that predicts the effects of optogenetic perturbations in both synthetic and in vivo datasets. By comparing standard sigmoid RNNs (σRNNs) and bioRNNs, the authors make a compelling case that biologically grounded inductive biases improve generalization to perturbed conditions. This work is innovative, technically strong, and grounded in relevant neuroscience, particularly the pressing need for data-constrained models that generalize causally.</p>
<p>Comments on revisions:</p>
<p>The authors have addressed all my concerns.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106827.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Sourmpis et al. present a study in which the importance of including certain inductive biases in the fitting of recurrent networks is evaluated with respect to the generalization ability of the networks when exposed to untrained perturbations.</p>
<p>The work proceeds in three stages:</p>
<p>(i) a simple illustration of the problem is made. Two reference (ground-truth) networks with qualitatively different connectivity, but similar observable network dynamics, are constructed, and recurrent networks with varying aspects of design similarity to the reference networks are trained to reproduce the reference dynamics. The activity of these trained networks during untrained perturbations is then compared to the activity of the perturbed reference networks. It is shown that, of the design characteristics that were varied, the enforced sign (Dale's law) and locality (spatial extent) of efference were especially important.</p>
<p>(ii) The intuition from the constructed example is then extended to networks that have been trained to reproduce certain aspects of multi-region neural activity recorded from mice during a detection task with a working-memory component. A similar pattern is demonstrated, in which enforcing the sign and locality of efference in the fitted networks has an influence on the ability of the trained networks to predict aspects of neural activity during unseen (untrained) perturbations.</p>
<p>(iii) The authors then illustrate the relationship between the gradient of the motor readout of trained networks with respect to the net inputs to the network units, and the sensitivity of the motor readout to small perturbations of the input currents to the units, which (in vivo) could be controlled optogenetically. The paper is concluded with a proposed use for trained networks, in which the models could be analyzed to determine the most sensitive directions of the network and, during online monitoring, inform a targeted optogenetic perturbation to bias behavior.</p>
<p>The authors do not overstate their claims, and in general, I find that I agree with their conclusions.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106827.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sourmpis</surname>
<given-names>Christos</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0007-0519-1116</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Petersen</surname>
<given-names>Carl CH</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3344-4495</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Gerstner</surname>
<given-names>Wulfram</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4344-2189</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Bellec</surname>
<given-names>Guillaume</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7568-4994</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review)</bold></p>
<p>Major:</p>
<p>(1) In line 76, the authors make a very powerful statement: 'σRNN simulation achieves higher similarity with unseen recorded trials before perturbation, but lower than the bioRNN on perturbed trials.' I couldn't find a figure showing this. This might be buried somewhere and, in my opinion, deserves some spotlight - maybe a figure or even inclusion in the abstract.</p>
</disp-quote>
<p>We agree with the reviewer that these results are important. The failure of σRNN on perturbed data could be inferred from the former Figures 1E, 2C-E, and 3D. Following the reviewers' comments, we have tried to make this the most prominent message of Figure 1, in particular with the addition of the new panel E. We also moved Table 1 from the  Supplementary to the main text to highlight this quantitatively.</p>
<disp-quote content-type="editor-comment">
<p>(2) It's mentioned in the introduction (line 84) and elsewhere (e.g., line 259) that spiking has some advantage, but I don't see any figure supporting this claim. In fact, spiking seems not to matter (Figure 2C, E). Please clarify how spiking improves performance, and if it does not, acknowledge that. Relatedly, in line 246, the authors state that 'spiking is a better metric but not significant' when discussing simulations. Either remove this statement and assume spiking is not relevant, or increase the number of simulations.</p>
</disp-quote>
<p>We could not find the exact quote from the reviewer, and we believe that he intended to quote “spiking is better on all metrics, but without significant margins”. Indeed, spiking did not improve the fit significantly on perturbed trials, this is particularly true in comparison with the benefits of Dale’s law and local inhibition. As suggested by the reviewer, we rephrased the sentence from this quote and more generally the corresponding paragraphs in the intro (lines 83-87) and in the results (lines 245-271). Our corrections in the results sections are also intended to address the minor point (4) raised by the same reviewer.</p>
<disp-quote content-type="editor-comment">
<p>(3) The authors prefer the metric of predicting hits over MSE, especially when looking at real data (Figure 3). I would bring the supplementary results into the main figures, as both metrics are very nicely complementary. Relatedly, why not add Pearson correlation or R2, and not just focus on MSE Loss?</p>
</disp-quote>
<p>In Figure 3 for the in-vivo data, we do not have simultaneous electrophysiological recordings and optogenetic stimulation in this dataset.  The two are performed on different recording sessions. Therefore, we can only compare the effect of optogenetics on the behavior, and we cannot compute Pearson correlation or R2 of the perturbed network activity. To avoid ambiguity, we wrote “For the sessions of the in vivo dataset with optogenetic perturbation that we considered, only the behavior of an animal is recorded” on line 294.</p>
<disp-quote content-type="editor-comment">
<p>(4) I really like the 'forward-looking' experiment in closed loop! But I felt that the relevance of micro perturbations is very unclear in the intro and results. This could be better motivated: why should an experimentalist care about this forward-looking experiment? Why exactly do we care about micro perturbation (e.g., in contrast to non-micro perturbation)? Relatedly, I would try to explain this in the intro without resorting to technical jargon like 'gradients'.</p>
</disp-quote>
<p>As suggested, we updated the last paragraph of the introduction (lines 88 - 95) to give better motivation for why algorithmically targeted acute spatio-temporal perturbations can be important to dissect the function of neural circuits. We also added citations to recent studies with targeted in vivo optogenetic stimulation. As far as we know the existing previous work targeted network stimulation mostly using linear models, while we used non-linear RNNs and their gradients.</p>
<disp-quote content-type="editor-comment">
<p>Minor:</p>
<p>(1) In the intro, the authors refer to 'the field' twice. Personally, I find this term odd. I would opt for something like 'in neuroscience'.</p>
</disp-quote>
<p>We implemented the suggested change: l.27 and l.30</p>
<disp-quote content-type="editor-comment">
<p>(2) Line 45: When referring to previous work using data-constrained RNN models, Valente et al. is missing (though it is well cited later when discussing regularization through low-rank constraints)</p>
</disp-quote>
<p>We added the citation: l.45</p>
<disp-quote content-type="editor-comment">
<p>(3) Line 11: Method should be methods (missing an 's').</p>
</disp-quote>
<p>We fixed the typo.</p>
<disp-quote content-type="editor-comment">
<p>(4) In line 250, starting with 'So far', is a strange choice of presentation order. After interpreting the results for other biological ingredients, the authors introduce a new one. I would first introduce all ingredients and then interpret. It's telling that the authors jump back to 2B after discussing 2C.</p>
</disp-quote>
<p>We restructured the last two paragraphs of section 2.1, and we hope that the presentation order is now more logical.</p>
<disp-quote content-type="editor-comment">
<p>(5) The black dots in Figure 3E are not explained, or at least I couldn't find an explanation.</p>
</disp-quote>
<p>We added an explanation in the caption of Figure 3E.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>(1) Some aspects of the methods are unclear. For comparisons between recurrent networks trained from randomly initialized weights, I would expect that many initializations were made for each model variant to be compared, and that the performance characteristics are constructed by aggregating over networks trained from multiple random initializations. I could not tell from the methods whether this was done or how many models were aggregated.</p>
</disp-quote>
<p>The expectation of the reviewer is correct, we trained multiple models with different random seeds (affecting both the weight initialization and the noise of our model) for each variant and aggregated the results. We have now clarified this in Methods 4.6. lines 658-662.</p>
<disp-quote content-type="editor-comment">
<p>(2) It is possible that including perturbation trials in the training sets would improve model performance across conditions, including held-out (untrained) perturbations (for instance, to units that had not been perturbed during training). It could be noted that if perturbations are available, their use may alleviate some of the design decisions that are evaluated here.</p>
</disp-quote>
<p>In general, we agree with the reviewer that including perturbation trials in the training set would likely improve model performance across conditions. One practical limitation explaining partially why we did not do it with our dataset is the small quantity of perturbed trials for each targeted cortical area: the number of trials with light perturbations is too scarce to robustly train and test our models.</p>
<p>More profoundly, to test hard generalizations to perturbations (aka perturbation testing), it will always be necessary that the perturbations are not trivially represented in the training data. Including perturbation trials during training would compromise our main finding: some biological model constraints improve the generalization to perturbation. To test this claim, it was necessary to keep the perturbations out of the training data.</p>
<p>We agree that including all available data of perturbed and non-perturbed recordings would be useful to build the best generalist predictive system. It could help, for instance, for closed-loop circuit control as we studied in Figure 5. Yet, there too, it will be important for the scientific validation process to always keep some causal perturbations of interest out of the training set. This is necessary to fairly measure the real generalization capability of any model. Importantly, this is why we think out-of-distribution “perturbation testing” is likely to have a recurring impact in the years to come, even beyond the case of optogenetic inactivation studied in detail in our paper.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendation for the authors:</bold></p>
<p><bold>Reviewer #1 (Recommendation for the authors):</bold></p>
<p>The code is not very easy to follow. I know this is a lot to ask, but maybe make clear where the code is to train the different models, which I think is a great contribution of this work? I predict that many readers will want to use the code and so this will improve the impact of this work.</p>
</disp-quote>
<p>We updated the code to make it easier to train a model from scratch.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendation for the authors):</bold></p>
<p>The figures are really tough to read. Some of that small font should be sized up, and it's tough to tell in the posted paper what's happening in Figure 2B.</p>
</disp-quote>
<p>We updated Figures 1 and 2 significantly, in part to increase their readability. We also implemented the &quot;Superficialities&quot; suggestions.</p>
</body>
</sub-article>
</article>