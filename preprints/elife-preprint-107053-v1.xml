<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107053</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107053</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107053.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Feedback of peripheral saccade targets to early foveal cortex</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-8046-5724</contrib-id>
<name>
<surname>Kämmer</surname>
<given-names>Luca</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>kaemmer@cbs.mpg.de</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kroell</surname>
<given-names>Lisa M</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Knapen</surname>
<given-names>Tomas</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rolfs</surname>
<given-names>Martin</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hebart</surname>
<given-names>Martin N</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a10">10</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Vision and Computational Cognition Group, Max Planck Institute of Human Cognitive and Brain Sciences</institution></institution-wrap>, <city>Leipzig</city>, <country country="DE">Germany</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Department of Medicine, Justus Liebig University Giessen</institution></institution-wrap>, <city>Giessen</city>, <country country="DE">Germany</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Department of Psychology, Humboldt University Berlin</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05kgbsy64</institution-id><institution>Spinoza Center for Neuroimaging,</institution></institution-wrap>, <city>Amsterdam</city>, <country country="NL">Netherlands</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05csn2x06</institution-id><institution>Computational Cognitive Neuroscience and Neuroimaging, Netherlands Institute for Neuroscience</institution></institution-wrap>, <city>Amsterdam</city>, <country country="NL">Netherlands</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008xxew50</institution-id><institution>Experimental and Applied Psychology, Vrije University Amsterdam</institution></institution-wrap>, <city>Amsterdam</city> <country country="NL">Netherlands</country></aff>
<aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Berlin School of Mind and Brain, Humboldt University Berlin</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff>
<aff id="a8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v4gjf40</institution-id><institution>Exzellenzcluster Science of Intelligence, Technical University Berlin</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff>
<aff id="a9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ewdps05</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin</institution></institution-wrap>, <city>Berlin</city> <country country="DE">Germany</country></aff>
<aff id="a10"><label>10</label><institution>Center for Mind, Brain and Behavior, Universities of Marburg, Giessen, and Darmstadt</institution>, <city>Marburg</city> <country country="DE">Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Kok</surname>
<given-names>Peter</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-08-20">
<day>20</day>
<month>08</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107053</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-04-22">
<day>22</day>
<month>04</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-04-22">
<day>22</day>
<month>04</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.02.20.639262"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Kämmer et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Kämmer et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107053-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Human vision is characterized by frequent eye movements and constant shifts in visual input, yet our perception of the world remains remarkably stable. Here we directly demonstrate the existence of an image-specific foveal prediction signal that reaches all the way back to primary visual cortex. To this end, we used a gaze-contingent fMRI paradigm, in which peripheral saccade targets disappeared before they could be fixated. Despite no direct foveal stimulation, we were able to decode peripheral saccade targets from foveal retinotopic areas, demonstrating that image-specific feedback during saccade preparation must underlie this effect. Decoding was sensitive to shape but not semantic category of natural images, indicating feedback of only low-to-mid-level information. Cross-decoding to a control condition with foveal stimulus presentation indicated a shared representational format between foveal feedback and direct stimulation. Moreover, eccentricity-dependent analyses showed a u-shaped decoding curve, confirming that these results are not explained by spillover of peripheral activity or large receptive fields. Finally, fluctuations in foveal decodability covaried with activity in the intraparietal sulcus, thus providing a candidate region for driving foveal feedback. These findings reveal that foveal cortex predicts the features of incoming stimuli through feedback from higher cortical areas, which offers a candidate mechanism underlying stable perception and may facilitate object recognition across saccades.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05xwwfy96</institution-id>
<institution>German National Academic Foundation</institution>
</institution-wrap>
</funding-source>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id>
<institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution>
</institution-wrap>
</funding-source>
</award-group>
<award-group id="funding-3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/0472cxd90</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id award-id-type="doi">10.3030/101039712</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Update of general structure of the manuscript.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Human vision relies heavily on foveal processing. Even though this region of the retina covers only a fraction of the visual field, it occupies a disproportionately large portion of neurons in the early visual cortex (<xref ref-type="bibr" rid="c14">Curcio et al., 1990</xref>; <xref ref-type="bibr" rid="c13">Curcio and Allen, 1990</xref>; <xref ref-type="bibr" rid="c31">Hendrickson, 2005</xref>; <xref ref-type="bibr" rid="c47">Schira et al., 2009</xref>). To take advantage of the fovea’s high resolution, humans perform several rapid eye movements per second, with each saccade bringing objects of interest into the fovea (<xref ref-type="bibr" rid="c40">O’Regan, 1992</xref>; <xref ref-type="bibr" rid="c7">Carpenter, 2000</xref>). Despite these frequent disruptions and dramatic shifts in retinal input, our perception of the visual environment remains surprisingly stable, coherent, and continuous (<xref ref-type="bibr" rid="c26">Golomb and Mazer, 2021</xref>; <xref ref-type="bibr" rid="c37">Melcher and Colby, 2008</xref>; <xref ref-type="bibr" rid="c6">Burr et al., 1994</xref>). This perceptual stability, while often taken for granted, points toward sophisticated neural mechanisms that integrate visual information across separate gaze fixations (<xref ref-type="bibr" rid="c52">Wurtz, 2008</xref>; <xref ref-type="bibr" rid="c8">Cavanagh et al., 2010</xref>; <xref ref-type="bibr" rid="c38">Merriam et al., 2007</xref>; <xref ref-type="bibr" rid="c17">Denagamage et al., 2024</xref>).</p>
<p>In the domain of visual perception, predictions may critically contribute to maintaining perceptual stability across rapid shifts in gaze (<xref ref-type="bibr" rid="c45">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="c16">De Lange et al., 2018</xref>; <xref ref-type="bibr" rid="c9">Clark, 2013</xref>). One compelling hypothesis suggests that perceptual continuity arises through predictive feedback of peripheral information from higher cortical areas to foveal retinotopic regions. This feedback informs foveal regions about the expected visual features of a stimulus prior to its direct fixation to prepare for the shift in visual input (<xref ref-type="bibr" rid="c36">Kroell and Rolfs, 2022</xref>). Support for this hypothesis comes from both psychophysical and neurophysiological studies, which suggest foveal processing of peripheral stimuli during fixation (<xref ref-type="bibr" rid="c51">Williams et al., 2008</xref>; <xref ref-type="bibr" rid="c23">Fan et al., 2016</xref>) as well as during saccadic eye movements (<xref ref-type="bibr" rid="c36">Kroell and Rolfs, 2022</xref>; <xref ref-type="bibr" rid="c35">Knapen et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Edwards et al., 2018</xref>). However, current evidence for predictive foveal feedback in visual cortex is indirect, relying mostly on the interpretation of behavioral reports. Specifically, it remains unknown 1) whether information is fed back all the way to early visual areas, 2) whether feedback is specific to stimulus features and which features are fed back, and 3) which brain regions mediate this effect.</p>
<p>To systematically address these open questions, we developed a gaze-contingent functional magnetic resonance imaging (fMRI) paradigm that allowed us to disentangle neural activity attributable to direct visual input from activity exclusively related to predictive feedback. By removing peripheral saccade targets before participants could fixate them, we ensured that observed neural activation within foveal retinotopic regions must originate from predictive rather than direct visual processes. Further, by employing naturalistic stimuli whooe visual shape and semantic content were independently manipulated, we could explicitly assess the specificity and content of the predictive feedback signals fed back to early foveal cortex.</p>
<p>Our findings robustly demonstrate the presence of predictive feedback in primary visual cortex, indicated by reliable decoding of peripheral saccade targets from foveal retinotopic areas, despite the absence of direct foveal stimulation. Critically, this decoding was selective for stimulus shape and not influenced by semantic category, indicating a predominantly low-to-mid-level visual representation. Eccentricity-dependent analyses showed a u-shaped decoding curve, demonstrating that these results cannot be explained by spillover of peripheral activity or large receptive fields. Cross-decoding analyses further confirmed the similarity between predictive feedback and direct visual representations in the fovea, reinforcing the shared nature of neural codes. Finally, exploratory analyses identified the intraparietal sulcus (IPS), a brain area integral to visuomotor coordination and eye movement planning, as a likely candidate involved in driving predictive feedback. These findings reveal a plausible neural implementation for perceptual continuity and may also facilitate object recognition across saccades (<xref ref-type="bibr" rid="c32">Herwig and Schneider, 2014</xref>; <xref ref-type="bibr" rid="c5">Blom et al., 2020</xref>).</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>Given the fast time-scale of saccade-related processes, it is challenging to investigate foveal feedback using fMRI, which is based mostly on a sluggish hemodynamic response. To address this challenge and dissociate neural processes elicited by direct visual input from those related to foveal feedback, we designed a gaze-contingent functional MRI study where the saccade target was removed before it could reach the central 2 degrees of visual angle (dva) of the fovea (1 dva radius), from which we decoded. To maximise statistical power, we implemented the paradigm using a block-design (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). During a block, participants fixated a fixation point until a stimulus appeared in the periphery, cueing them to execute a saccade toward the stimulus. As soon as their gaze came within 6.5 dva of the saccade target, the stimulus disappeared, ensuring that no part of the stimulus ever appeared in the central 2 dva of the fovea. On subsequent trials of the block, the same stimulus appeared in the participant’s periphery to cue another saccade, a process which was repeated until the end of a block. While the fovea is commonly defined as the central 5 dva of the visual field (<xref ref-type="bibr" rid="c13">Curcio and Allen, 1990</xref>; <xref ref-type="bibr" rid="c30">Hendrickson, 2009</xref>, <xref ref-type="bibr" rid="c31">2005</xref>), we focused here on the central part of the fovea, commonly referred to as the foveola (<xref ref-type="bibr" rid="c44">Poletti et al., 2017</xref>; <xref ref-type="bibr" rid="c29">Heckenlively and Arden, 2006</xref>), within 2 dva (1 dva radius), to avoid any overlap of the stimulus and the area from which we decoded. This narrow definition also allowed for enough space to execute a saccade without the stimulus breaching this region. <xref rid="fig1" ref-type="fig">Figure 1C</xref> shows that this paradigm was successful in preventing the target from reaching the central 2 dva of the fovea in 99.27% of saccades. All blocks in which the target could have appeared in this region were removed from further analysis (see methods section quantification and statistical analysis).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Experimental setup.</title>
<p>(A) Illustration of one block of the experimental and control conditions, respectively. During each block in the experimental condition, a peripheral saccade target was shown, which participants were instructed to fixate. The target disappeared before it could be foveated, and once fixation was achieved, a new target appeared, until the block was over (duration: 11 seconds). The timing of target appearance and disappearance from the experimental condition was recorded and used in the control condition, where targets appeared at fixation. (B) Depiction of the 4 stimuli used in the experiment, which were matched in either visual shape (horizontal / vertical) or semantic category (animal / instrument). Each stimulus appeared equally often in both conditions. (C) Histogram of the gaze distance from the stimulus right after stimulus disappearance including all trials. The blocks in which the stimulus edge may have appeared in the participants’ fovea during at least one saccade were excluded.</p></caption>
<graphic xlink:href="639262v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To compare the experimental condition to activation elicited by direct foveal input, we included a control condition in which participants were instructed to fixate a point at the center of the screen, with the stimulus appearing directly in the center of the fovea (<xref rid="fig1" ref-type="fig">Figure 1A</xref>, bottom). To keep visual stimulation frequency comparable, the timing of stimulus appearance and disappearance was recorded for each participant in the experimental condition and replayed in the control condition. To elucidate the content of the stimulus information fed back to foveal retinotopic areas, we used four different natural stimuli (one stimulus per block) allowing us to disentangle the nature of the representation in the foveal retinotopic cortex (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). The stimuli were manipulated to match in either shape (horizontal vs. vertical) or semantic category (animals vs. instruments).</p>
<sec id="s2a">
<title>Decoding foveal feedback</title>
<p>To test for the presence of stimulus-specific activation in foveal regions of early visual cortex, we used cross-validated multivariate decoding (<xref ref-type="bibr" rid="c28">Hebart and Baker, 2018</xref>), which reveals information that allows discriminating between different stimuli. We compared all pairs of stimuli in the experimental and control condition, respectively (chance level: 50%). In the experimental condition, we found above chance decoding in central foveal V1 (t(27) = 8.81, p &lt; 0.001, mean = 57.43%) (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). This result demonstrates that information about the peripheral saccade targets is present in central foveal regions of V1, despite never appearing in the corresponding part of the fovea. To compare this finding to direct foveal stimulus presentation, we repeated the same analysis for the control condition, where we also found strong significant decoding (t(27) = 19.92, p &lt; 0.001, mean = 84.06%) (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). To further examine the nature of the neural representation elicited by foveal feedback, we cross-decoded from experimental to control trials by training a classifier on data from central foveal V1 in the experimental condition and testing it on the control condition. Decoding was significantly above chance (t(27) = 5.22, p &lt; 0.001, mean = 57.2%), indicating a similar representational format between the neural representation elicited by direct presentation of the stimulus in the fovea and that elicited by foveal feedback (<xref rid="fig2" ref-type="fig">Figure 2A</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Foveal feedback can be decoded from V1.</title>
<p>(A) Average decoding accuracy over all pairwise comparisons for control and experimental conditions and for cross-decoding from experimental to control condition. (B) Average decoding accuracies for all early visual areas as a function of eccentricity for both experimental and control conditions. Error bars represent standard error of the mean. Note that the graphs have different scales of decoding accuracy. The central eccentricities (1-5 dva) were measured using a retinotopic localizer, the outer ones (6-10 dva) were inferred from structural data using Neuropythy <xref ref-type="bibr" rid="c37">Melcher and Colby (2008)</xref>.</p></caption>
<graphic xlink:href="639262v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>This pattern of results may alternatively be explained by spillover from peripheral regions or large receptive fields in the fovea reaching into the periphery (<xref ref-type="bibr" rid="c51">Williams et al., 2008</xref>). To address this issue, we investigated decoding as a function of eccentricity in early visual regions (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Foveal decoding due to peripheral spillover would predict a monotonic relationship between peripheral and foveal decoding in the experimental condition. Instead, we found a u-shaped relationship, with stronger decoding from peripheral and foveal regions compared to parafoveal regions. These results highlight the spatial pattern of foveal feedback, separating decoding due to direct stimulus presentation in the periphery and decoding due to feedback to foveal regions. As expected, in the control condition, decoding was the highest in the center of gaze and dropped off towards the periphery.</p>
</sec>
<sec id="s2b">
<title>Foveal feedback is sensitive to stimulus shape, not semantic category</title>
<p>Our use of natural stimuli allowed us to test the effects of shape and category on decoding accuracy to better understand the nature of the information fed back to foveal areas (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). Decoding was assessed between visually similar yet semantically dissimilar stimuli (across category), between semantically similar yet visually dissimilar stimuli (across shape), and between visually and semantically dissimilar stimuli (across both). The latter comparison served as a baseline, assessing how good decoding is for maximally different stimuli.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Foveal feedback is sensitive to stimulus shape, not semantic category.</title>
<p>(A) Schematic depiction of comparisons to assess the information content of the neural representations. Comparisons across category assess similarity of representations in terms of visual stimulus properties, with lower decoding accuracies indicating coding for visual information. Similarly, comparing across shape assesses categorical information. Comparing across both serves as a baseline, showing how high decoding accuracy is between maximally different stimuli. (B) Decoding accuracies for all comparisons using data from foveal regions of V1 and from the lateral occipital area (LO). Note that the graphs have different y-axes of decoding accuracy.</p></caption>
<graphic xlink:href="639262v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>A similar pattern of decoding accuracies emerged in both experimental and control conditions: Decoding from foveal V1 across category dropped significantly relative to baseline (experimental condition: t(27) = 2.25, p = 0.033, difference = 3.03%; control condition: t(27) = 14.74, p &lt; 0.001, difference = 16.64%), while decoding across shape remained high (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). This pattern indicates that the nature of the feedback signal in the experimental condition was related to stimulus shape information and not semantic category. The fact that the overall pattern of results across conditions looks similar in the experimental and control conditions is in line with the notion that direct stimulus presentation and foveal feedback elicit similar neural representation, as suggested by the cross-decoding results described above (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). To test the degree to which the classifier was able to pick up on category information, we repeated the same analysis in the lateral occipital area (LO), which has been shown to capture higher level information about object category (<xref ref-type="bibr" rid="c27">Grill-Spector et al., 2001</xref>). In this area, the pattern was reversed: Decoding dropped across shape relative to baseline (experimental condition: t(27) = 3.41, p = 0.002, difference = 5.31%; control condition: t(27) = 7.25, p &lt; 0.001, difference = 6.7%), while it remained high across category, which suggests that this activation more strongly reflects information about the semantic category than stimulus shape.</p>
</sec>
<sec id="s2c">
<title>The role of IPS in mediating foveal feedback</title>
<p>While the previous analyses revealed the pattern of foveal feedback in early visual regions, they left open which neural regions might be involved in driving or mediating this effect. To this end, we conducted an exploratory analysis looking at the block-by-block fluctuations in foveal decodability in the experimental condition. As a measure of decodability, we used the continuous decision value of the classifier, which signifies the distance to the classifier’s hyperplane on a given trial. Using a parametric modulation analysis, we explored which brain region’s activity increased or decreased as a function of foveal decodability. To control for the effects of the direct peripheral presentation of the stimulus, we used the block-by-block fluctuations of peripheral decoding in the experimental condition as a baseline.</p>
<p>Since foveal feedback is a process tightly linked to saccadic eye movements (<xref ref-type="bibr" rid="c36">Kroell and Rolfs, 2022</xref>), we hypothesized that regions associated with eye movements are most likely involved in driving this effect. We focused on three regions that have consistently been associated with eye movements and object representations: Frontal eye fields (FEF) (<xref ref-type="bibr" rid="c41">Paus, 1996</xref>; <xref ref-type="bibr" rid="c49">Vernet et al., 2014</xref>), intraparietal sulcus (IPS) (<xref ref-type="bibr" rid="c2">Andersen et al., 1992</xref>; <xref ref-type="bibr" rid="c1">Andersen, 1989</xref>), and lateral occipital area (LO) (<xref ref-type="bibr" rid="c34">Kawawaki et al., 2006</xref>) (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). Not all voxels in these regions were expected to be functionally active. Therefore, to focus analyses on the most relevant voxels, we selected the 100 voxels in each of these regions that activated most strongly in the experimental condition in general. The region of interest (ROI) analyses showed that all of these areas were significantly related to both foveal and peripheral decoding in the experimental condition (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). Specifically, IPS showed significantly larger activation related to foveal decoding compared to peripheral decoding (t(27) = 2.53, p = 0.009, difference = 4.22), suggesting that IPS may be involved in foveal feedback. While effects in the other regions went in the expected direction, they remained non-significant after correcting for multiple comparisons (LO: t(27) = 0.67, p = 0.256, difference = 0.53; FEF: t(27) = 2.07, p = 0.024, difference = 1.98).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Correlation of foveal decoding with ROI activation.</title>
<p>(A) Cortical masks used in the ROI analyses. These masks were generated using Neurosynth (<xref ref-type="bibr" rid="c54">Yarkoni et al., 2011</xref>) with the keyword “eye movements”. For later analyses, only the 100 most activating voxels were selected in each area. (B) Results of the ROI analyses comparing neural activation as a function of foveal and peripheral decoding in three key areas related to eye movements. Activation in the IPS was significantly higher in association with foveal decoding compared to peripheral decoding.</p></caption>
<graphic xlink:href="639262v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>The present preregistered study provides evidence that early foveal retinotopic areas are involved in the processing of peripheral saccade targets, even if the stimuli are never presented in the central part of the fovea. Using a combination of fMRI and eye-tracking, we were able to decode the identity of saccade targets from foveal regions of early visual cortex (<xref rid="fig2" ref-type="fig">Figure 2A</xref>), suggesting that shape-specific information about peripheral targets is fed back to foveal areas during saccade preparation. We also showed that this foveal feedback is unlikely to be caused by spillover from peripheral regions (<xref rid="fig2" ref-type="fig">Figure 2B</xref>) and that it has a similar neural representation as direct stimulus presentation, as shown by the above chance cross-decoding (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). This cross-decoding alleviates concerns about the type of information picked up by the decoder in the experimental condition. That is, it cannot be explained by the decoder picking up small changes in eye movements between different stimuli (<xref ref-type="bibr" rid="c51">Williams et al., 2008</xref>), since the representation -at least in part -transfers to the control condition, where participants did not move their eyes. Furthermore, we showed that foveal feedback is sensitive to the shape, but not the semantic category of the stimulus (<xref rid="fig3" ref-type="fig">Figure 3B</xref>), which suggests that foveal feedback in early visual cortex is rather rudimentary and does not convey a category-invariant stimulus representation. These findings are in line with recent work showing that feedback to primary visual cortex may primarily carry low-level perceptual information (<xref ref-type="bibr" rid="c11">Costantino et al., 2025</xref>). Lastly, in an exploratory analysis, we identified the intraparietal sulcus (IPS) as a candidate region for driving foveal feedback (<xref rid="fig4" ref-type="fig">Figure 4B</xref>).</p>
<sec id="s3a">
<title>Saccadic remapping or foveal prediction</title>
<p>Saccadic remapping, that is, the increase of activity of neurons in anticipation of a stimulus entering their receptive field, has been observed all over the visual cortex (<xref ref-type="bibr" rid="c26">Golomb and Mazer, 2021</xref>; <xref ref-type="bibr" rid="c18">Duhamel et al., 1992</xref>; <xref ref-type="bibr" rid="c39">Mirpour and Bisley, 2012</xref>). These shifts are commonly viewed as a predictive remapping of attention pointers, which do not themselves contain information about the content of the stimulus (<xref ref-type="bibr" rid="c45">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="c46">Rolfs et al., 2011</xref>; <xref ref-type="bibr" rid="c22">Fabius et al., 2020</xref>; <xref ref-type="bibr" rid="c53">Xiao et al., 2024</xref>). Foveal prediction, on the other hand, goes beyond a purely attentional effect and instead contains content-specific information about the saccade targets. <xref ref-type="bibr" rid="c36">Kroell and Rolfs (2022)</xref> in their psychophysical experiments found that during saccade preparation, peripheral saccade target representations were enhanced in the pre-saccadic fovea. These findings led the researchers to conclude that the fovea plays a unique role in maintaining perceptual continuity by predicting future inputs during saccade preparation. Our results support this view by showing shape-sensitive decoding, as well as the cross-decoding results from the experimental to control condition, indicating that foveal feedback may lead early foveal regions to share features with the peripheral target stimulus in anticipation of an upcoming saccade. These findings provide another indication for the distinction between saccadic remapping and foveal prediction, although more research is needed to differentiate these effects across the visual field.</p>
</sec>
<sec id="s3b">
<title>Comparing foveal feedback to direct presentation</title>
<p>The present study supports a predictive account of foveal processing (<xref ref-type="bibr" rid="c36">Kroell and Rolfs, 2022</xref>) and further characterizes the nature of this effect. The decoding patterns revealed that the information about the saccade targets that is fed back to the foveal cortex may reflect shape information, but does not contain higher-level categorical information. These decoding patterns resemble the ones we found in the control condition, where the stimuli were presented directly in the fovea (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). Lastly, it was possible to cross-decode by training a decoder on foveal V1 data from the experimental condition and decoding stimulus identity from the same regions in the control condition at above chance-level (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). These findings suggest shared representational formats in early visual areas between foveal prediction and direct stimulus presentation, which indicates that foveal feedback reflects low-to-mid-level features of the target similarly to the direct presentation of the stimulus.</p>
</sec>
<sec id="s3c">
<title>IPS as a candidate modulator of foveal prediction</title>
<p>In a parametric modulation analysis, we found that the intraparietal sulcus (IPS) was significantly more active in association with foveal decoding compared to peripheral decoding in the experimental condition (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). This area has been described as neither a strictly visual nor motor area but instead as performing visuomotor integration functions, such as determining the spatial location of saccade targets and forming plans to make eye movements (<xref ref-type="bibr" rid="c2">Andersen et al., 1992</xref>; <xref rid="c1" ref-type="bibr">Andersen, 1989</xref>). Further research has shown that this region represents salient stimuli, relative to the center of gaze (<xref ref-type="bibr" rid="c10">Colby and Duhamel, 1996</xref>). This integrative function makes the IPS an ideal candidate for modulating feature-specific feedback to foveal areas during saccade preparation. While this analysis is exploratory, it offers yet another indication that foveal feedback is inherently linked to saccadic eye movements, and that IPS could play an important role in driving this effect. Future hypothesis-driven research could specifically target this region to more clearly determine its role in foveal feedback.</p>
</sec>
<sec id="s3d">
<title>Limitations and future directions</title>
<p>Despite the insights gained in the present study, several open questions remain. We did not specifically test if we can find foveal feedback from peripheral targets without any stimulus-specific task (e.g., no eye movements), which is relevant to showing that foveal feedback is task-dependent. However, similar control conditions have been run by <xref ref-type="bibr" rid="c51">Williams et al. (2008)</xref> and <xref ref-type="bibr" rid="c35">Knapen et al. (2016)</xref>. Neither found any stimulus-specific foveal activation with peripheral target presentation in the absence of a specific task. To further separate the results laid out in this study from the process of saccadic remapping, future studies could specifically test periphery-to-periphery feedback for feature-specific activation. Additionally, while the stimulus-specific effects reported in the present study were robust, the results were limited to 4 different stimuli, since the addition of further conditions would have led to a reduction in statistical power. Future studies could expand upon the present approach by increasing the number of stimuli, possibly collecting data across multiple sessions to achieve sufficiently large effects. While these questions offer exciting research avenues for future studies, our results demonstrate the importance of foveal feedback during saccadic eye movements, offering a plausible candidate mechanism for our ability to integrate visual information across saccades. They also pave the way for future research about the influence of foveal prediction on object recognition.</p>
</sec>
</sec>
<sec id="s4">
<title>Methods and Materials</title>
<p>This study was pre-registered on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/rxacd/">https://osf.io/rxacd/</ext-link>), detailing the hypotheses, methodology, and planned analyses prior to data collection, with the exception of the exploratory parametric modulation analysis.</p>
<p>The dataset collected for this study is available on OpenNeuro: <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds005933">https://openneuro.org/datasets/ds005933</ext-link>. The experimental and analysis code are available on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/Lucakaemmer/FovealFeedback.git">https://github.com/Lucakaemmer/FovealFeedback.git</ext-link>.</p>
<sec id="s4a">
<title>Experimental model and study participant details</title>
<p>The experiment was performed with human participants recruited by the recruitment system of the Max Planck Institute for Cognitive and Brain Sciences in Leipzig, where the experiment was conducted. 30 participants were tested, which is similar to comparable studies. Two participants were excluded. One person had poor eye-tracking performance, which impaired the functioning of the gaze-contingent aspect of the experiment (fewer than 100 valid saccades in the experiment). The other was excluded based on poor fMRI data, likely due to drowsiness, which made it impossible to generate retinotopic masks. The remaining 28 participants (14 male and 14 female) were 18–40 years old (average 27.54 years), healthy, right-handed, and had normal or corrected-to-normal vision. Participants were compensated with €12 per hour. The experiment was conducted in accordance with the declaration of Helsinki, and the experimental procedure was approved by the ethics review board of the University of Leipzig (reference number: 421/23-ek). The participants gave written informed consent before taking part in the study.</p>
</sec>
</sec>
<sec id="s5">
<title>Method details</title>
<sec id="s5a">
<title>Stimuli</title>
<p>The stimuli were four different object images taken from the Hemera object dataset (H. Technologies. Hemera photo objects). These stimuli were chosen to match either regarding their visual shape (horn and tiger, guitar and kangaroo) or their semantic category (horn and guitar, tiger and kangaroo). The stimuli that were visually similar were also semantically dissimilar, and vice versa (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). All stimuli were normalized regarding their overall luminance and root mean square contrast and were presented at a size of 3 dva.</p>
</sec>
<sec id="s5b">
<title>Behavioral task</title>
<p>In the experimental condition, objects appeared in eight different locations arranged in a circle around the screen, each of them 4º away from the center of the screen (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). Experimental blocks started with a white fixation point appearing in one of these eight locations. After 4 seconds, the fixation point turned black, and one of the four stimuli appeared in one of the three locations on the opposite side of the screen, 7.4º or 8º away from the fixation point. The stimulus only appeared if participants were fixating on the fixation point. Stimulus appearance was the cue for the participant to perform a saccade. As soon as their gaze came within 6º of the stimulus during the saccade, the stimulus disappeared to prevent it from appearing in the participant’s fovea. After an inter-trial interval of 0.5 seconds, the same stimulus appeared again at the next location. This pattern repeated for 11 seconds, concluding one experimental block. One run consisted of 20 blocks, switching to a different stimulus in each block. The experiment included 5 experimental runs and 5 control runs.</p>
<p>In the control condition, stimuli were presented at the center of the fovea, with fixation at the center of the screen (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). The overall timing of the stimulus presentation was the same as in the experimental condition. To this end, the timing of stimulus appearance and disappearance in the experimental condition, which depended on the participants’ eye movements, was recorded and used in the control condition to provide the same exposure duration to the stimulus across conditions.</p>
<p>Lastly, an eccentricity retinotopic localizer and standard object localizer task were used to create retinotopic mapping and to identify object-processing areas in each participant. The retinotopic localizer consisted of six iterations of contracting or expanding rings to map each participant’s eccentricity in early visual areas. The object localizer consisted of blocks of either object images or scrambled images to separate object-processing areas from lower-level visual areas.</p>
</sec>
<sec id="s5c">
<title>Imaging</title>
<p>fMRI scanning was performed at the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany, using a Siemens Magnetom Skyra 3T scanner equipped with a 32-channel head coil (Siemens, Erlangen). Whole-brain T2*-weighted echo-planar images (EPI) were collected with the following parameters: repetition time (TR) = 2000 ms, echo time (TE) = 23.6 ms, flip angle = 80°, field of view (FOV) = 204mm<sup>2</sup>, voxel size = 2 × 2 × 2 mm<sup>3</sup>, and 69 transverse slices with no gap. An interleaved slice acquisition was used with a multiband acceleration factor of 3 and no in-plane acceleration, and the phase encoding direction was anterior to posterior. High-resolution T1-weighted anatomical images were acquired at the end of the scanning session using a standard magnetization-prepared rapid gradient echo (MPRAGE) sequence. Parallel imaging with GRAPPA was utilized for the T1-weighted MPRAGE sequence with an acceleration factor of 2.</p>
<p>For eye tracking, we used an EyeLink 1000+ Eyetracker, sampling the right eye at 1000 Hz. The eye tracker was calibrated using a 13-point setup to ensure high eye tracking quality. The stimuli were presented using a VPixx PROPixx projector, set to a real-time refresh rate of 180 Hz to allow for the gaze-contingent disappearance of the stimulus before participants could fixate it. The presentation was programmed in Python using PsychoPy (<xref ref-type="bibr" rid="c43">Peirce et al., 2019</xref>).</p>
<p>Each participant completed one scanning session, starting with 10 experimental runs (5 gaze-contingent and 5 fixation runs), which took 320 s each. The retinotopic localizer run took 206 s, and the object localizer run took 426 s, bringing the overall time inside the MRI scanner to around 80 minutes, including structural scans, breaks, and the calibration of the eye tracker.</p>
<sec id="s5c1">
<title>Quantification and statistical analysis</title>
<p>The fMRI data were preprocessed using fMRIprep version 20.2.0 (<xref ref-type="bibr" rid="c21">Esteban et al., 2019</xref>). Brain surfaces were reconstructed using recon-all from FreeSurfer v6.0.1 (<xref ref-type="bibr" rid="c15">Dale et al., 1999</xref>). Functional data were slice-time corrected using 3dTshift from AFNI v16.2.07 (<xref ref-type="bibr" rid="c12">Cox, 1996</xref>) and motion-corrected using mcflirt (<xref ref-type="bibr" rid="c33">Jenkinson et al., 2012</xref>). The functional data used for the decoding analysis remained in each participant’s native space, without registering to a brain template.</p>
<p>Univariate analyses were conducted on all experimental and control runs using FSL’s FEAT (v6.00) to perform a general linear model analysis (<xref ref-type="bibr" rid="c33">Jenkinson et al., 2012</xref>). The resulting beta values for each block were then used in the subsequent multivariate decoding analysis. We chose pairwise classification over multiclass classification 1) since our intent was not to build a classifier for real-world applications but to infer from the presence of above chance accuracies the presence of discriminative information (<xref ref-type="bibr" rid="c28">Hebart and Baker, 2018</xref>) and 2) since most common multiclass classification approaches implicitly rely on the comparison of multiple pairwise classifiers anyway. For the decoding analyses and statistical tests, a custom-written Python pipeline was used; a linear support vector machine with 5-fold cross-validation was used for decoding, provided by the Python package sklearn (<xref ref-type="bibr" rid="c42">Pedregosa et al., 2011</xref>). Decoding accuracies were compared with t-tests using the Python package SciPy (<xref ref-type="bibr" rid="c50">Virtanen et al., 2020</xref>). One-sample t-tests were used to assess if decoding accuracies were significantly different from chance. Since a within-subjects design was used in which all participants went through all the conditions, paired, two-sided t-tests were used to compare decoding accuracies (across shape, across category, across both). The significance threshold was set at p &lt; 0.05. Significance levels in figures are indicated as follows: *p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001.</p>
<p>Anatomical masks for V1, V2, and V3 were generated for each participant using Neuropythy (<xref ref-type="bibr" rid="c4">Benson and Winawer, 2018</xref>). The same package was used to estimate the outer retinotopic eccentricities from 6 to 10 dva. The inner eccentricities from 1 to 5 degrees were estimated using the functional retinotopic localizer. A univariate analysis was used to estimate the voxels responding to a certain range of eccentricity. Similarly, anatomical masks for LO were generated for each participant using the functional object localizer.</p>
<p>The parametric modulation analysis was conducted with FSL’s FEAT. Using the decoders’ block-by-block fluctuations in decision scores as regressors, the parametric modulation analysis was run on the preprocessed data from the experimental runs, which was converted to MNI space to allow for second-level analyses across participants. Masks for the eye-tracking ROIs were generated using the website Neurosynth (<xref ref-type="bibr" rid="c54">Yarkoni et al., 2011</xref>), which synthesizes data from thousands of fMRI studies to create functional cortical masks in MNI space. Using the keyword “eye movements”, we generated masks of regions known to be associated with eye movements: Frontal eye fields (FEF) (<xref ref-type="bibr" rid="c41">Paus, 1996</xref>; <xref ref-type="bibr" rid="c49">Vernet et al., 2014</xref>), intraparietal sulcus (IPS)(<xref ref-type="bibr" rid="c2">Andersen et al., 1992</xref>; <xref ref-type="bibr" rid="c1">Andersen, 1989</xref>), and lateral occipital area (LO) (<xref ref-type="bibr" rid="c34">Kawawaki et al., 2006</xref>) (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). We then isolated the 100 voxels per area that activated most strongly in the experimental condition overall, independent of which stimulus was shown. Beta estimates for these 100 voxels in each area were then compared with paired, one-sided t-tests using SciPy to test our hypothesis that foveal decoding was associated with larger neural activation in our ROIs. These tests were corrected for multiple comparisons using Bonferroni correction.</p>
<p>To assure that there was no direct foveal stimulation in the experimental condition, we excluded all the experimental blocks in which any part of the stimulus may have appeared in the participants’ fovea during at least one saccade, which happened in 0.73% of all saccades. Since entire blocks were removed when they contained a single such saccade, this led to an exclusion of 8.49% of experimental blocks. To keep the balance between experimental and control conditions, we also excluded the corresponding blocks from the control condition.</p>
<p>The processing of offline eye movement data was performed in Matlab 2018b (Mathworks, Nat-ick, MA, USA). Within an experimental block a trial only started (characterized by the appearance of a stimulus) once participants fixated within 1 dva of the initial fixation point, and only ended once they fixated within 1 dva of the target fixation point. In the offline analysis, a saccade was detected when 2D velocity exceeded 5 standard deviations from the median for a minimum of 5 ms (<xref ref-type="bibr" rid="c20">Engbert and Mergenthaler, 2006</xref>). Saccade candidates that were less than 10 ms apart (usually resulting from post-saccadic oscillations (<xref ref-type="bibr" rid="c48">Schweitzer and Rolfs, 2022</xref>)) were merged into a single saccade. On average, participants performed 1084 (± 206.17) trials over the course of the experiment, which amounts to 10.84 trials per experimental block. Overall, participants reached the target area (within 2 dva of stimulus center) in a single saccade in 76.07% of all stimulus appearances, and in 16.94% in more than one saccade. Note that humans can plan two saccades in advance and still allocate attention to the final goal ahead of the first saccade (<xref ref-type="bibr" rid="c46">Rolfs et al., 2011</xref>; <xref ref-type="bibr" rid="c3">Baldauf and Deubel, 2008</xref>; <xref ref-type="bibr" rid="c24">Gersch et al., 2004</xref>; <xref ref-type="bibr" rid="c25">Godijn and Theeuwes, 2003</xref>). In 6.99% of trials, no saccade was detected in the offline analysis, probably due to tracking noise in the MRI scanner. In the worst case, these trials would only have added noise. Average saccade latency was 240.70 ms, and average saccade amplitude was 6.76 dva. All the figures were prepared using Python and PowerPoint.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s6">
<title>Figure supplements</title>
<fig id="fig3_S1" position="float" fig-type="figure">
<label>Figure 3. figure supplement 1.</label>
<caption><title>Decoding stimulus content from all early foveal areas.</title>
<p>Decoding accuracies for all comparisons using data from foveal regions of V1, V2, and V3. These graphs show that the results from V1 outlined in this publication generalize to other regions of the early visual cortex.</p></caption>
<graphic xlink:href="639262v2_fig3_S1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We would like to thank the members of the Vision and Computational Cognition Group at the Max Planck Institute for Human Cognitive and Brain Sciences for their valuable feedback and contributions to this study. We would also like to thank the staff in the MRI department at the MPI-CBS that supported the data collection. This work was supported by a doctoral student scholarship awarded to L.K. by the German Academic Scholarship Foundation (“Studienstiftung des Deutschen Volkes”), a research group grant by the Max Planck Society awarded to M.N.H., the ERC Starting Grant project COREDIM (ERC-StG-2021-101039712) and the Hessian Ministry of Higher Education, Science, Research and Art (LOEWE Start Professorship to M.N.H. and Excellence Program ‘The Adaptive Mind’). M.R. was supported by the ERC Consolidator Grant project VIS-A-VIS (grant agreement 835715). The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersen</surname> <given-names>RA</given-names></string-name></person-group>. <article-title>Visual and eye movement functions of the posterior parietal cortex</article-title>. <source>Annual review of neuro-science</source>. <year>1989</year>; <volume>12</volume>(<issue>1</issue>):<fpage>377</fpage>–<lpage>403</lpage>. doi: <pub-id pub-id-type="doi">10.1146/annurev.ne.12.030189.002113</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersen</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Brotchie</surname> <given-names>PR</given-names></string-name>, <string-name><surname>Mazzoni</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Evidence for the lateral intraparietal area as the parietal eye field</article-title>. <source>Current Opinion in Neurobiology</source>. <year>1992</year> <month>Dec</month>; <volume>2</volume>(<issue>6</issue>):<fpage>840</fpage>–<lpage>846</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/0959438892901439">https://linkinghub.elsevier.com/retrieve/pii/0959438892901439</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/0959-4388(92)90143-9</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baldauf</surname> <given-names>D</given-names></string-name>, <string-name><surname>Deubel</surname> <given-names>H.</given-names></string-name></person-group> <article-title>Properties of attentional selection during the preparation of sequential saccades</article-title>. <source>Exper-imental Brain Research</source>. <year>2008</year> <month>Jan</month>; <volume>184</volume>(<issue>3</issue>):<fpage>411</fpage>–<lpage>425</lpage>. <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/s00221-007-1114-x">http://link.springer.com/10.1007/s00221-007-1114-x</ext-link>, doi: <pub-id pub-id-type="doi">10.1007/s00221-007-1114-x</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benson</surname> <given-names>NC</given-names></string-name>, <string-name><surname>Winawer</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Bayesian analysis of retinotopic maps</article-title>. <source>eLife</source>. <year>2018</year> <month>Dec</month>; <volume>7</volume>:<elocation-id>e40224</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/40224">https://elifesciences.org/articles/40224</ext-link>, doi: <pub-id pub-id-type="doi">10.7554/eLife.40224</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blom</surname> <given-names>T</given-names></string-name>, <string-name><surname>Feuerriegel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bode</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hogendoorn</surname> <given-names>H.</given-names></string-name></person-group> <article-title>Predictions drive neural representations of visual events ahead of incoming sensory information</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2020</year>; <volume>117</volume>(<issue>13</issue>):<fpage>7510</fpage>–<lpage>7515</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1917777117</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burr</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Morrone</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Ross</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Selective suppression of the magnocellular visual pathway during saccadic eye movements</article-title>. <source>Nature</source>. <year>1994</year> <month>Oct</month>; <volume>371</volume>(<issue>6497</issue>):<fpage>511</fpage>–<lpage>513</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/371511a0">https://www.nature.com/articles/371511a0</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/371511a0</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carpenter</surname> <given-names>RHS</given-names></string-name></person-group>. <article-title>The neural control of looking</article-title>. <source>Current Biology</source>. <year>2000</year> <month>Apr</month>; <volume>10</volume>(<issue>8</issue>):<fpage>R291</fpage>–<lpage>R293</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0960982200004309">https://linkinghub.elsevier.com/retrieve/pii/S0960982200004309</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/S0960-9822(00)00430-9</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cavanagh</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hunt</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Afraz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rolfs</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Visual stability based on remapping of attention pointers</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2010</year> <month>Apr</month>; <volume>14</volume>(<issue>4</issue>):<fpage>147</fpage>–<lpage>153</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1364661310000288">https://linkinghub.elsevier.com/retrieve/pii/S1364661310000288</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.tics.2010.01.007</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clark</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title>. <source>Behavioral and Brain Sciences</source>. <year>2013</year> <month>Jun</month>; <volume>36</volume>(<issue>3</issue>):<fpage>181</fpage>–<lpage>204</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.cambridge.org/core/product/identifier/S0140525X12000477/type/journal_article">https://www.cambridge.org/core/product/identifier/S0140525X12000477/type/journal_article</ext-link>, doi: <pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Colby</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Duhamel</surname> <given-names>JR</given-names></string-name></person-group>. <article-title>Spatial representations for action in parietal cortex</article-title>. <source>Cognitive Brain Research</source>. <year>1996</year> <month>Dec</month>; <volume>5</volume>(<issue>1-2</issue>):<fpage>105</fpage>–<lpage>115</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0926641096000468">https://linkinghub.elsevier.com/retrieve/pii/S0926641096000468</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/S0926-6410(96)00046-8</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Costantino</surname> <given-names>AI</given-names></string-name>, <string-name><surname>Turner</surname> <given-names>BO</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Crossley</surname> <given-names>MJ</given-names></string-name></person-group>. <article-title>Partial information transfer from peripheral visual streams to foveal visual streams may be mediated through local primary visual circuits</article-title>. <source>NeuroImage</source>. <year>2025</year>; p. <fpage>121147</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2025.121147</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cox</surname> <given-names>RW</given-names></string-name></person-group>. <article-title>AFNI: Software for Analysis and Visualization of Functional Magnetic Resonance Neuroimages</article-title>. <source>Computers and Biomedical Research</source>. <year>1996</year> <month>Jun</month>; <volume>29</volume>(<issue>3</issue>):<fpage>162</fpage>–<lpage>173</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0010480996900142">https://linkinghub.elsevier.com/retrieve/pii/S0010480996900142</ext-link>, doi: <pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Curcio</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Allen</surname> <given-names>KA</given-names></string-name></person-group>. <article-title>Topography of ganglion cells in human retina</article-title>. <source>Journal of Comparative Neurology</source>. <year>1990</year> <month>Oct</month>; <volume>300</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>25</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/10.1002/cne.903000103">https://onlinelibrary.wiley.com/doi/10.1002/cne.903000103</ext-link>, doi: <pub-id pub-id-type="doi">10.1002/cne.903000103</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Curcio</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Sloan</surname> <given-names>KR</given-names></string-name>, <string-name><surname>Kalina</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Hendrickson</surname> <given-names>AE</given-names></string-name></person-group>. <article-title>Human photoreceptor topography</article-title>. <source>Journal of Comparative Neurology</source>. <year>1990</year> <month>Feb</month>; <volume>292</volume>(<issue>4</issue>):<fpage>497</fpage>–<lpage>523</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/10.1002/cne.902920402">https://onlinelibrary.wiley.com/doi/10.1002/cne.902920402</ext-link>, doi: <pub-id pub-id-type="doi">10.1002/cne.902920402</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dale</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name></person-group>. <article-title>Cortical Surface-Based Analysis</article-title>. <source>NeuroImage</source>. <year>1999</year> <month>Feb</month>; <volume>9</volume>(<issue>2</issue>):<fpage>179</fpage>–<lpage>194</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1053811998903950">https://linkinghub.elsevier.com/retrieve/pii/S1053811998903950</ext-link>, doi: <pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Lange</surname> <given-names>FP</given-names></string-name>, <string-name><surname>Heilbron</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kok</surname> <given-names>P.</given-names></string-name></person-group> <article-title>How Do Expectations Shape Perception?</article-title> <source>Trends in Cognitive Sciences</source>. <year>2018</year> <month>Sep</month>; <volume>22</volume>(<issue>9</issue>):<fpage>764</fpage>–<lpage>779</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1364661318301396">https://linkinghub.elsevier.com/retrieve/pii/S1364661318301396</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Denagamage</surname> <given-names>S</given-names></string-name>, <string-name><surname>Morton</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Hudson</surname> <given-names>NV</given-names></string-name>, <string-name><surname>Nandy</surname> <given-names>AS</given-names></string-name></person-group>. <article-title>Widespread receptive field remapping in early primate visual cortex</article-title>. <source>Cell Reports</source>. <year>2024</year> <month>Aug</month>; <volume>43</volume>(<issue>8</issue>):<fpage>114557</fpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S2211124724008866">https://linkinghub.elsevier.com/retrieve/pii/S2211124724008866</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.celrep.2024.114557</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duhamel</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Colby</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Goldberg</surname> <given-names>ME</given-names></string-name></person-group>. <article-title>The Updating of the Representation of Visual Space in Parietal Cortex by Intended Eye Movements</article-title>. <source>Science</source>. <year>1992</year> <month>Jan</month>; <volume>255</volume>(<issue>5040</issue>):<fpage>90</fpage>–<lpage>92</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/10.1126/science.1553535">https://www.science.org/doi/10.1126/science.1553535</ext-link>, doi: <pub-id pub-id-type="doi">10.1126/science.1553535</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edwards</surname> <given-names>G</given-names></string-name>, <string-name><surname>VanRullen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Cavanagh</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Decoding Trans-Saccadic Memory</article-title>. <source>Annual Review of Neuroscience</source>. <year>2018</year> <month>Jan</month>; <volume>38</volume>(<issue>5</issue>):<fpage>1114</fpage>–<lpage>1123</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.0854-17.2017">https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.0854-17.2017</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0854-17.2017</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engbert</surname> <given-names>R</given-names></string-name>, <string-name><surname>Mergenthaler</surname> <given-names>K.</given-names></string-name></person-group> <article-title>Microsaccades are triggered by low retinal image slip</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2006</year> <month>May</month>; <volume>103</volume>(<issue>18</issue>):<fpage>7192</fpage>–<lpage>7197</lpage>. <ext-link ext-link-type="uri" xlink:href="https://pnas.org/doi/full/10.1073/pnas.0509557103">https://pnas.org/doi/full/10.1073/pnas.0509557103</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.0509557103</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esteban</surname> <given-names>O</given-names></string-name>, <string-name><surname>Markiewicz</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Blair</surname> <given-names>RW</given-names></string-name>, <string-name><surname>Moodie</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Isik</surname> <given-names>AI</given-names></string-name>, <string-name><surname>Erramuzpe</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kent</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Goncalves</surname> <given-names>M</given-names></string-name>, <string-name><surname>DuPre</surname> <given-names>E</given-names></string-name>, <string-name><surname>Snyder</surname> <given-names>M</given-names></string-name>, <string-name><surname>Oya</surname> <given-names>H</given-names></string-name>, <string-name><surname>Ghosh</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Wright</surname> <given-names>J</given-names></string-name>, <string-name><surname>Durnez</surname> <given-names>J</given-names></string-name>, <string-name><surname>Poldrack</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Gorgolewski</surname> <given-names>KJ</given-names></string-name></person-group>. <article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title>. <source>Nature Methods</source>. <year>2019</year> <month>Jan</month>; <volume>16</volume>(<issue>1</issue>):<fpage>111</fpage>–<lpage>116</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41592-018-0235-4">https://www.nature.com/articles/s41592-018-0235-4</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fabius</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Fracasso</surname> <given-names>A</given-names></string-name>, <string-name><surname>Acunzo</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Van Der Stigchel</surname> <given-names>S</given-names></string-name>, <string-name><surname>Melcher</surname> <given-names>D.</given-names></string-name></person-group> <article-title>Low-Level Visual Information Is Maintained across Saccades, Allowing for a Postsaccadic Handoff between Visual Areas</article-title>. <source>The Journal of Neuroscience</source>. <year>2020</year> <month>Dec</month>; <volume>40</volume>(<issue>49</issue>):<fpage>9476</fpage>–<lpage>9486</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1169-20.2020">https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1169-20.2020</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1169-20.2020</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname> <given-names>X</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Shao</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kersten</surname> <given-names>D</given-names></string-name>, <string-name><surname>He</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Temporally flexible feedback signal to foveal cortex for peripheral object recognition</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2016</year> <month>Oct</month>; <volume>113</volume>(<issue>41</issue>):<fpage>11627</fpage>–<lpage>11632</lpage>. <ext-link ext-link-type="uri" xlink:href="https://pnas.org/doi/full/10.1073/pnas.1606137113">https://pnas.org/doi/full/10.1073/pnas.1606137113</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1606137113</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gersch</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Kowler</surname> <given-names>E</given-names></string-name>, <string-name><surname>Dosher</surname> <given-names>B.</given-names></string-name></person-group> <article-title>Dynamic allocation of visual attention during the execution of sequences of saccades</article-title>. <source>Vision Research</source>. <year>2004</year> <month>Jun</month>; <volume>44</volume>(<issue>12</issue>):<fpage>1469</fpage>–<lpage>1483</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0042698903008009">https://linkinghub.elsevier.com/retrieve/pii/S0042698903008009</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.visres.2003.12.014</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Godijn</surname> <given-names>R</given-names></string-name>, <string-name><surname>Theeuwes</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Parallel allocation of attention prior to the execution of saccade sequences</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>2003</year>; <volume>29</volume>(<issue>5</issue>):<fpage>882</fpage>–<lpage>896</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.apa.org/doi/10.1037/0096-1523.29.5.882">https://doi.apa.org/doi/10.1037/0096-1523.29.5.882</ext-link>, doi: <pub-id pub-id-type="doi">10.1037/0096-1523.29.5.882</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Golomb</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Mazer</surname> <given-names>JA</given-names></string-name></person-group>. <article-title>Visual Remapping</article-title>. <source>Annual Review of Vision Science</source>. <year>2021</year> <month>Sep</month>; <volume>7</volume>(<issue>1</issue>):<fpage>257</fpage>–<lpage>277</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.annualreviews.org/doi/10.1146/annurev-vision-032321-100012">https://www.annualreviews.org/doi/10.1146/annurev-vision-032321-100012</ext-link>, doi: <pub-id pub-id-type="doi">10.1146/annurev-vision-032321-100012</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kourtzi</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N.</given-names></string-name></person-group> <article-title>The lateral occipital complex and its role in object recognition</article-title>. <source>Vision Research</source>. <year>2001</year> <month>May</month>; <volume>41</volume>(<issue>10-11</issue>):<fpage>1409</fpage>–<lpage>1422</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0042698901000736">https://linkinghub.elsevier.com/retrieve/pii/S0042698901000736</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/S0042-6989(01)00073-6</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hebart</surname> <given-names>MN</given-names></string-name>, <string-name><surname>Baker</surname> <given-names>CI</given-names></string-name></person-group>. <article-title>Deconstructing multivariate decoding for the study of brain function</article-title>. <source>NeuroImage</source>. <year>2018</year> <month>Oct</month>; <volume>180</volume>:<fpage>4</fpage>–<lpage>18</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1053811917306523">https://linkinghub.elsevier.com/retrieve/pii/S1053811917306523</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.005</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Heckenlively</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Arden</surname> <given-names>GB</given-names></string-name></person-group>. <source>Principles and Practice of Clinical Electrophysiology of Vision</source>. 2 ed.; <year>2006</year>. doi: <pub-id pub-id-type="doi">10.7551/mitpress/5557.001.0001</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hendrickson</surname> <given-names>A.</given-names></string-name></person-group>, <person-group person-group-type="editor"><string-name><surname>Squire</surname> <given-names>L.R.</given-names></string-name></person-group>, <chapter-title>Fovea: Primate</chapter-title>. <source>Encyclopedia of Neuroscience Elsevier</source>; <year>2009</year>.p. <fpage>327</fpage>–<lpage>334</lpage>, doi: <pub-id pub-id-type="doi">10.1016/B978-008045046-9.00920-7</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hendrickson</surname> <given-names>A.</given-names></string-name></person-group> <chapter-title>Organization of the Adult Primate Fovea</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Penfold</surname> <given-names>PL</given-names></string-name>, <string-name><surname>Provis</surname> <given-names>JM</given-names></string-name></person-group>, editors. <source>Macular Degeneration</source> <publisher-loc>Berlin/Heidelberg</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>2005</year>.p. <fpage>1</fpage>–<lpage>23</lpage>. <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/3-540-26977-0_1">http://link.springer.com/10.1007/3-540-26977-0_1</ext-link>, doi: <pub-id pub-id-type="doi">10.1007/3-540-26977-0_1</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herwig</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schneider</surname> <given-names>WX</given-names></string-name></person-group>. <article-title>Predicting object features across saccades: evidence from object recognition and visual search</article-title>. <source>Journal of Experimental Psychology: General</source>. <year>2014</year>; <volume>143</volume>(<issue>5</issue>):<fpage>1903</fpage>. doi: <pub-id pub-id-type="doi">10.1037/a0036781</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jenkinson</surname> <given-names>M</given-names></string-name>, <string-name><surname>Beckmann</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Behrens</surname> <given-names>TEJ</given-names></string-name>, <string-name><surname>Woolrich</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>SM</given-names></string-name></person-group>. <article-title>FSL</article-title>. <source>NeuroImage</source>. <year>2012</year> <month>Aug</month>; <volume>62</volume>(<issue>2</issue>):<fpage>782</fpage>–<lpage>790</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1053811911010603">https://linkinghub.elsevier.com/retrieve/pii/S1053811911010603</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kawawaki</surname> <given-names>D</given-names></string-name>, <string-name><surname>Shibata</surname> <given-names>T</given-names></string-name>, <string-name><surname>Goda</surname> <given-names>N</given-names></string-name>, <string-name><surname>Doya</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kawato</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Anterior and superior lateral occipito-temporal cortex responsible for target motion prediction during overt and covert visual pursuit</article-title>. <source>Neuroscience Research</source>. <year>2006</year> <month>Feb</month>; <volume>54</volume>(<issue>2</issue>):<fpage>112</fpage>–<lpage>123</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0168010205002804">https://linkinghub.elsevier.com/retrieve/pii/S0168010205002804</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neures.2005.10.015</pub-id>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knapen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Swisher</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Tong</surname> <given-names>F</given-names></string-name>, <string-name><surname>Cavanagh</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Oculomotor Remapping of Visual Information to Foveal Retinotopic Cortex</article-title>. <source>Frontiers in Systems Neuroscience</source>. <year>2016</year> <month>Jun</month>; 10. <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/Article/10.3389/fnsys.2016.00054/abstract">http://journal.frontiersin.org/Article/10.3389/fnsys.2016.00054/abstract</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fnsys.2016.00054</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kroell</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Rolfs</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Foveal vision anticipates defining features of eye movement targets</article-title>. <source>eLife</source>. <year>2022</year> <month>Sep</month>; <volume>11</volume>:<elocation-id>e78106</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/78106">https://elifesciences.org/articles/78106</ext-link>, doi: <pub-id pub-id-type="doi">10.7554/eLife.78106</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Melcher</surname> <given-names>D</given-names></string-name>, <string-name><surname>Colby</surname> <given-names>CL</given-names></string-name></person-group>. <article-title>Trans-saccadic perception</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2008</year> <month>Dec</month>; <volume>12</volume>(<issue>12</issue>):<fpage>466</fpage>–<lpage>473</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1364661308002325">https://linkinghub.elsevier.com/retrieve/pii/S1364661308002325</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.tics.2008.09.003</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Merriam</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Genovese</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Colby</surname> <given-names>CL</given-names></string-name></person-group>. <article-title>Remapping in Human Visual Cortex</article-title>. <source>Journal of Neurophysiology</source>. <year>2007</year> <month>Feb</month>; <volume>97</volume>(<issue>2</issue>):<fpage>1738</fpage>–<lpage>1755</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.physiology.org/doi/10.1152/jn.00189.2006">https://www.physiology.org/doi/10.1152/jn.00189.2006</ext-link>, doi: <pub-id pub-id-type="doi">10.1152/jn.00189.2006</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mirpour</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bisley</surname> <given-names>JW</given-names></string-name></person-group>. <article-title>Anticipatory Remapping of Attentional Priority across the Entire Visual Field</article-title>. <source>The Journal of Neuroscience</source>. <year>2012</year> <month>Nov</month>; <volume>32</volume>(<issue>46</issue>):<fpage>16449</fpage>–<lpage>16457</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.2008-12.2012">https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.2008-12.2012</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2008-12.2012</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Regan</surname> <given-names>JK</given-names></string-name></person-group>. <article-title>Solving the “real” mysteries of visual perception: The world as an outside memory</article-title>. <source>Canadian Journal of Psychology /Revue canadienne de psychologie</source>. <year>1992</year> <month>Sep</month>; <volume>46</volume>(<issue>3</issue>):<fpage>461</fpage>–<lpage>488</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.apa.org/doi/10.1037/h0084327">https://doi.apa.org/doi/10.1037/h0084327</ext-link>, doi: <pub-id pub-id-type="doi">10.1037/h0084327</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paus</surname> <given-names>T.</given-names></string-name></person-group> <article-title>Location and function of the human frontal eye-field: A selective review</article-title>. <source>Neuropsychologia</source>. <year>1996</year> <month>Jun</month>; <volume>34</volume>(<issue>6</issue>):<fpage>475</fpage>–<lpage>483</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/0028393295001344">https://linkinghub.elsevier.com/retrieve/pii/0028393295001344</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/0028-3932(95)00134-4</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Varoquaux</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Michel</surname> <given-names>V</given-names></string-name>, <string-name><surname>Thirion</surname> <given-names>B</given-names></string-name>, <string-name><surname>Grisel</surname> <given-names>O</given-names></string-name>, <string-name><surname>Blondel</surname> <given-names>M</given-names></string-name>, <string-name><surname>Prettenhofer</surname> <given-names>P</given-names></string-name>, <string-name><surname>Weiss</surname> <given-names>R</given-names></string-name>, <string-name><surname>Dubourg</surname> <given-names>V</given-names></string-name>, <string-name><surname>Vanderplas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Passos</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cournapeau</surname> <given-names>D.</given-names></string-name></person-group> <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>the Journal of machine Learning research</source>. <year>2011</year>; <volume>12</volume>:<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peirce</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gray</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Simpson</surname> <given-names>S</given-names></string-name>, <string-name><surname>MacAskill</surname> <given-names>M</given-names></string-name>, <string-name><surname>Höchenberger</surname> <given-names>R</given-names></string-name>, <string-name><surname>Sogo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kastman</surname> <given-names>E</given-names></string-name>, <string-name><surname>Lindeløv</surname> <given-names>JK.</given-names></string-name></person-group> <article-title>PsychoPy2: Experiments in behavior made easy</article-title>. <source>Behavior Research Methods</source>. <year>2019</year> <month>Feb</month>; <volume>51</volume>(<issue>1</issue>):<fpage>195</fpage>–<lpage>203</lpage>. <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.3758/s13428-018-01193-y">http://link.springer.com/10.3758/s13428-018-01193-y</ext-link>, doi: <pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poletti</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rucci</surname> <given-names>M</given-names></string-name>, <string-name><surname>Carrasco</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Selective attention within the foveola</article-title>. <source>Nature Neuroscience</source>. <year>2017</year> <month>Oct</month>; <volume>20</volume>(<issue>10</issue>):<fpage>1413</fpage>–<lpage>1417</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nn.4622">https://www.nature.com/articles/nn.4622</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/nn.4622</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname> <given-names>RPN</given-names></string-name>, <string-name><surname>Ballard</surname> <given-names>DH</given-names></string-name></person-group>. <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature Neuroscience</source>. <year>1999</year> <month>Jan</month>; <volume>2</volume>(<issue>1</issue>):<fpage>79</fpage>–<lpage>87</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nn0199_79">https://www.nature.com/articles/nn0199_79</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/4580</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rolfs</surname> <given-names>M</given-names></string-name>, <string-name><surname>Jonikaitis</surname> <given-names>D</given-names></string-name>, <string-name><surname>Deubel</surname> <given-names>H</given-names></string-name>, <string-name><surname>Cavanagh</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Predictive remapping of attention across eye movements</article-title>. <source>Nature Neuroscience</source>. <year>2011</year> <month>Feb</month>; <volume>14</volume>(<issue>2</issue>):<fpage>252</fpage>–<lpage>256</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nn.2711">https://www.nature.com/articles/nn.2711</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/nn.2711</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schira</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>CW</given-names></string-name>, <string-name><surname>Breakspear</surname> <given-names>M</given-names></string-name>, <string-name><surname>Spehar</surname> <given-names>B.</given-names></string-name></person-group> <article-title>The Foveal Confluence in Human Visual Cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year> <month>Jul</month>; <volume>29</volume>(<issue>28</issue>):<fpage>9050</fpage>–<lpage>9058</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1760-09.2009">https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1760-09.2009</ext-link>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1760-09.2009</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Schweitzer</surname> <given-names>R</given-names></string-name>, <string-name><surname>Rolfs</surname> <given-names>M.</given-names></string-name></person-group> <chapter-title>Definition, Modeling, and Detection of Saccades in the Face of Post-saccadic Oscillations</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Stuart</surname> <given-names>S</given-names></string-name></person-group>, editor. <source>Eye Tracking</source>, vol. <volume>183</volume> <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer US</publisher-name>; <year>2022</year>.p. <fpage>69</fpage>–<lpage>95</lpage>. <ext-link ext-link-type="uri" xlink:href="https://link.springer.com/10.1007/978-1-0716-2391-6_5">https://link.springer.com/10.1007/978-1-0716-2391-6_5</ext-link>, doi: <pub-id pub-id-type="doi">10.1007/978-1-0716-2391-6_5</pub-id>, series Title: Neuromethods.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vernet</surname> <given-names>M</given-names></string-name>, <string-name><surname>Quentin</surname> <given-names>R</given-names></string-name>, <string-name><surname>Chanes</surname> <given-names>L</given-names></string-name>, <string-name><surname>Mitsumasu</surname> <given-names>A</given-names></string-name>, <string-name><surname>Valero-Cabré</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Frontal eye field, where art thou? Anatomy, function, and non-invasive manipulation of frontal regions involved in eye movements and associated cognitive operations</article-title>. <source>Frontiers in Integrative Neuroscience</source>. <year>2014</year> <month>Aug</month>; <volume>8</volume>. <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fnint.2014.00066/abstract">http://journal.frontiersin.org/article/10.3389/fnint.2014.00066/abstract</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fnint.2014.00066</pub-id>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Virtanen</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gommers</surname> <given-names>R</given-names></string-name>, <string-name><surname>Oliphant</surname> <given-names>TE</given-names></string-name>, <string-name><surname>Haberland</surname> <given-names>M</given-names></string-name>, <string-name><surname>Reddy</surname> <given-names>T</given-names></string-name>, <string-name><surname>Cournapeau</surname> <given-names>D</given-names></string-name>, <string-name><surname>Burovski</surname> <given-names>E</given-names></string-name>, <string-name><surname>Peterson</surname> <given-names>P</given-names></string-name>, <string-name><surname>Weckesser</surname> <given-names>W</given-names></string-name>, <string-name><surname>Bright</surname> <given-names>J</given-names></string-name>, <string-name><surname>Van Der Walt</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Brett</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Millman</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Mayorov</surname> <given-names>N</given-names></string-name>, <string-name><surname>Nelson</surname> <given-names>ARJ</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kern</surname> <given-names>R</given-names></string-name>, <string-name><surname>Larson</surname> <given-names>E</given-names></string-name>, <string-name><surname>Carey</surname> <given-names>CJ</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title>. <source>Nature Methods</source>. <year>2020</year> <month>Mar</month>; <volume>17</volume>(<issue>3</issue>):<fpage>261</fpage>–<lpage>272</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41592-019-0686-2">https://www.nature.com/articles/s41592-019-0686-2</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Williams</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Baker</surname> <given-names>CI</given-names></string-name>, <string-name><surname>Op De Beeck</surname> <given-names>HP</given-names></string-name>, <string-name><surname>Mok Shim</surname> <given-names>W</given-names></string-name>, <string-name><surname>Dang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Triantafyllou</surname> <given-names>C</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N.</given-names></string-name></person-group> <article-title>Feedback of visual object information to foveal retinotopic cortex</article-title>. <source>Nature Neuroscience</source>. <year>2008</year> <month>Dec</month>; <volume>11</volume>(<issue>12</issue>):<fpage>1439</fpage>–<lpage>1445</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nn.2218">https://www.nature.com/articles/nn.2218</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/nn.2218</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wurtz</surname> <given-names>RH</given-names></string-name></person-group>. <article-title>Neuronal mechanisms of visual stability</article-title>. <source>Vision Research</source>. <year>2008</year> <month>Sep</month>; <volume>48</volume>(<issue>20</issue>):<fpage>2070</fpage>–<lpage>2089</lpage>. <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0042698908001727">https://linkinghub.elsevier.com/retrieve/pii/S0042698908001727</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.visres.2008.03.021</pub-id>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xiao</surname> <given-names>W</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kreiman</surname> <given-names>G</given-names></string-name>, <string-name><surname>Livingstone</surname> <given-names>MS</given-names></string-name></person-group>. <article-title>Feature-selective responses in macaque visual cortex follow eye movements during natural vision</article-title>. <source>Nature Neuroscience</source>. <year>2024</year> <month>Jun</month>; <volume>27</volume>(<issue>6</issue>):<fpage>1157</fpage>–<lpage>1166</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-024-01631-5">https://www.nature.com/articles/s41593-024-01631-5</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41593-024-01631-5</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yarkoni</surname> <given-names>T</given-names></string-name>, <string-name><surname>Poldrack</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Nichols</surname> <given-names>TE</given-names></string-name>, <string-name><surname>Van Essen</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Wager</surname> <given-names>TD</given-names></string-name></person-group>. <article-title>Large-scale automated synthesis of human functional neuroimaging data</article-title>. <source>Nature Methods</source>. <year>2011</year> <month>Aug</month>; <volume>8</volume>(<issue>8</issue>):<fpage>665</fpage>–<lpage>670</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nmeth.1635">https://www.nature.com/articles/nmeth.1635</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/nmeth.1635</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107053.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kok</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>useful</bold> study replicates a previous finding that information about peripherally presented visual stimuli is represented in the foveal visual cortex, and extends it by demonstrating that these representations are similar to those evoked by foveally presented stimuli. The authors' gaze-contingent fMRI design provides <bold>solid</bold> evidence for these findings. Some of the stronger theoretical claims, such as that the effects are due to predictive pre-saccadic remapping, are not fully supported by the current results.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107053.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The main contributions of this paper are: (1) a replication of the surprising prior finding that information about peripherally-presented stimuli can be decoded from foveal V1 (Williams et al 2008), (2) a new demonstration of cross-decoding between stimuli presented in the periphery and stimuli presented at the fovea, (3) a demonstration that the information present in the fovea is based on shape not semantic category, and (4) a demonstration that the strength of foveal information about peripheral targets is correlated with the univariate response in the same block in IPS.</p>
<p>Strengths:</p>
<p>The design and methods appear sound, and finding (2) above is new, and importantly constrains our understanding of this surprising phenomenon. The basic effect investigated here is so surprising that even though it has been replicated several times since it was first reported in 2008, it is useful to replicate it again.</p>
<p>Weaknesses:</p>
<p>(1) The paper, including in the title (&quot;Feedback of peripheral saccade targets to early foveal cortex&quot;) seems to assume that the feedback to foveal cortex occurs in conjunction with saccade preparation. However, participants in the original Williams et al (2008) paper never made saccades to the peripheral stimuli. So, saccade preparation is not necessary for this effect to occur. Some acknowledgement and discussion of this prior evidence against the interpretation of the effect as due to saccade preparation would be useful. (e.g., one might argue that saccade preparation is automatic when attending to peripheral stimuli.)</p>
<p>(2) The most important new finding from this paper is the cross-decodability between stimuli presented in the fovea and stimuli presented in the periphery. This finding should be related to the prior behavioral finding (Yu &amp; Shim, 2016) that when a foveal foil stimulus identical to a peripheral target is presented 150 ms after the onset of the peripheral target, visual discrimination of the peripheral target is improved, and this congruency effect occurred even though participants did not consciously perceive the foveal stimulus (Yu, Q., &amp; Shim, W. M., 2016). Modulating foveal representation can influence visual discrimination in the periphery (Journal of Vision, 16(3), 15-15).</p>
<p>(3) The prior literature should be laid out more clearly. For example, most readers will not realize that the basic effect of decodability of peripherally-presented stimuli in the fovea was first reported in 2008, and that that original paper already showed that the effect cannot arise from spillover effects from peripheral retinotopic cortex because it was not present in a retinotopic location between the cortical locus corresponding to the peripheral target and the fovea. (For example, this claim on lines 56-57 is not correct: &quot;it remains unknown 1) whether information is fed back all the way to early visual areas&quot;.) What is needed is a clear presentation of the prior findings in one place in the introduction to the paper, followed by an articulation and motivation of the new questions addressed in this paper. If I were writing the paper, I would focus on the cross-decodability between foveal and peripheral stimuli, as I think that is the most revealing finding.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107053.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study investigated whether the identity of a peripheral saccade target object is predictively fed back to the foveal retinotopic cortex during saccade preparation, a critical prediction of the foveal prediction hypothesis proposed by Kroell &amp; Rolfs (2022). To achieve this, the authors leveraged a gaze-contingent fMRI paradigm, where the peripheral saccade target was removed before the eyes landed near it, and used multivariate decoding analysis to quantify identity information in the foveal cortex. The results showed that the identity of the saccade target object can be decoded based on foveal cortex activity, despite the fovea never directly viewing the object, and that the foveal feedback representation was similar to passive viewing and not explained by spillover effects. Additionally, exploratory analysis suggested IPS as a candidate region mediating such foveal decodability. Overall, these findings provide neural evidence for the foveal cortex processing the features of the saccade target object, potentially supporting the maintenance of perceptual stability across saccadic eye movements.</p>
<p>Strengths:</p>
<p>This study is well-motivated by previous theoretical findings (Kroell &amp; Rolfs, 2022), aiming to provide neural evidence for a potential neural mechanism of trans-saccadic perceptual stability. The question is important, and the gaze-contingent fMRI paradigm is a solid methodological choice for the research goal. The use of stimuli allowing orthogonal decoding of stimulus category vs stimulus shape is a nice strength, and the resulting distinctions in decoded information by brain region are clean. The results will be of interest to readers in the field, and they fill in some untested questions regarding pre-saccadic remapping and foveal feedback.</p>
<p>Weaknesses:</p>
<p>The conclusions feel a bit over-reaching; some strong theoretical claims are not fully supported, and the framing of prior literature is currently too narrow. A critical weakness lies in the inability to test a distinction between these findings (claiming to demonstrate that &quot;feedback during saccade preparation must underlie this effect&quot;) and foveal feedback previously found during passive fixation (Williams et al., 2008). Discussions (and perhaps control analysis/experiments) about how these findings are specific to the saccade target and the temporal constraints on these effects are lacking. The relationship between the concepts of foveal prediction, foveal feedback, and predictive remapping needs more thorough treatment. The choice to use only 4 stimuli is justified in the manuscript, but remains an important limitation. The IPS results are intriguing but could be strengthened by additional control analysis. Finally, the manuscript claims the study was pre-registered (&quot;detailing the hypotheses, methodology, and planned analyses prior to data collection&quot;), but on the OSF link provided, there is just a brief summary paragraph, and the website says &quot;there have been no completed registrations of this project&quot;.</p>
<p>Specifics:</p>
<p>(1) In the eccentricity-dependent decoding results (Figure 2B), are there any statistical tests to support the results being a U-shaped curve? The dip isn't especially pronounced. Is 4 degrees lower than the further ones? Are there alternative methods of quantifying this (e.g., fitting it to a linear and quadratic function)?</p>
<p>(2) In the parametric modulation analysis, the evidence for IPS being the only region showing stronger fovea vs peripheral beta values was weak, especially given the exploratory nature of this analysis. The raw beta value can reflect other things, such as global brain fluctuations or signal-to-noise ratio. I would also want to see the results of the same analysis performed on the control condition decoding results.</p>
<p>(3) Many of the claims feel overstated. There is an emphasis throughout the manuscript (including claims in the abstract) that these findings demonstrate foveal prediction, specifically that &quot;image-specific feedback during saccade preparation must underlie this effect.&quot; To my understanding, one of the key aspects of the foveal prediction phenomenon that ties it closely to trans-saccadic stability is its specificity to the saccade target but not to other objects in the environment. However, it is not clear to what degree the observed findings are specific to saccade preparation and the peripheral saccade target. Should the observers be asked to make a saccade to another fixation location, or simply maintain passive fixation, will foveal retinotopic cortex similarly contain the object's identity information? Without these control conditions, the results are consistent with foveal prediction, but do not definitively demonstrate that as the cause, so claims need to be toned down.</p>
<p>(4) Another critical aspect is the temporal locus of the feedback signal. In the paradigm, the authors ensured that the saccade target object was never foveated via the gaze-contingent procedure and a conservative data exclusion criterion, thus enabling the test of feedback signals to foveal retinotopic cortex. However, due to the temporal sluggishness of fMRI BOLD signals, it is unclear when the feedback signal arrives at the foveal retinotopic cortex. In other words, it is possible that the feedback signal arrives after the eyes land at the saccade target location. This possibility is also bolstered by Chambers et al. (2013)'s TMS study, where they found that TMS to the foveal cortex at 350-400 ms SOA interrupts the peripheral discrimination task. The authors should qualify their claims of the results occurring &quot;during saccade preparation&quot; (e.g., pg 1 ln 22) throughout the manuscript, and discuss the importance of temporal dynamics of the effect in supporting stability across saccades.</p>
<p>(5) Relatedly, the claims that result in this paradigm reflect &quot;activity exclusively related to predictive feedback&quot; and &quot;must originate from predictive rather than direct visual processes&quot; (e.g., lines 60-65 and throughout) need to be toned down. The experimental design nicely rules out direct visual foveal stimulation, but predictive feedback is not the only alternative to that. The activation could also reflect mental imagery, visual working memory, attention, etc. Importantly, the experiment uses a block design, where the same exact image is presented multiple times over the block, and the activation is taken for the block as a whole. Thus, while at no point was the image presented at the fovea, there could still be more going on than temporally-specific and saccade-specific predictive feedback.</p>
<p>(6) The authors should avoid using the terms foveal feedback and foveal prediction interchangeably. To me, foveal feedback refers to the findings of Williams et al. (2008), where participants maintained passive fixation and discriminated objects in the periphery (see also Fan et al., 2016), whereas foveal prediction refers to the neural mechanism hypothesized by Kroell &amp; Rolfs (2022), occurring before a saccade to the target object and contains task irrelevant feature information.</p>
<p>(7) More broadly, the treatment of how foveal prediction relates to saccadic remapping is overly simplistic. The authors seem to be taking the perspective that remapping is an attentional phenomenon marked by remapping of only attentional/spatial pointers, but this is not the classic or widely accepted definition of remapping. Within the field of saccadic remapping, it is an ongoing debate whether (/how/where/when) information about stimulus content is remapped alongside spatial location (and also whether the attentional pointer concept is even neurophysiologically viable). This relationship between saccadic remapping and foveal prediction needs clarification and deeper treatment, in both the introduction and discussion.</p>
<p>(8) As part of this enhanced discussion, the findings should be better integrated with prior studies. E.g., there is some evidence for predictive remapping inducing integration of non-spatial features (some by the authors themselves; Harrison et al., 2013; Szinte et al., 2015). How do these findings relate to the observed results? Can the results simply be a special case of non-spatial feature integration between the currently attended and remapped location (fovea)? How are the results different from neurophysiological evidence for facilitation of the saccade target object's feature across the visual field (Burrow et al., 2014)? How might the results be reconciled with a prior fMRI study that failed to find decoding of stimulus content in remapped responses (Lescroart et al, 2016)? Might this reflect a difference between peripheral-to-peripheral vs peripheral-to-foveal remapping? A recent study by Chiu &amp; Golomb (2025) provided supporting evidence for peripheral-to-fovea remapping (but not peripheral-to-peripheral remapping) of object-location binding (though in the post-saccadic time window), and suggested foveal prediction as the underlying mechanism.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107053.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this paper, the authors used fMRI to determine whether peripherally viewed objects could be decoded from the foveal cortex, even when the objects themselves were never viewed foveally. Specifically, they investigated whether pre-saccadic target attributes (shape, semantic category) could be decoded from the foveal cortex. They found that object shape, but not semantic category, could be decoded, providing evidence that foveal feedback relies on low-mid-level information. The authors claim that this provides evidence for a mechanism underlying visual stability and object recognition across saccades.</p>
<p>Strengths:</p>
<p>I think this is another nice demonstration that peripheral information can be decoded from / is processed in the foveal cortex - the methods seem appropriate, and the experiments and analyses are carefully conducted, and the main results seem convincing. The paper itself was very clear and well-written.</p>
<p>Weaknesses:</p>
<p>There are a couple of reasons why I think the main theoretical conclusions drawn from the study might not be supported, and why a more thorough investigation might be needed to draw these conclusions.</p>
<p>(1) The authors used a blocked design, with each object being shown repeatedly in the same block. This meant that the stimulus was entirely predictable on each block, which weakens the authors' claims about this being a predictive mechanism that facilitates object recognition - if the stimulus is 100% predictable, there is no aspect of recognition or discrimination actually being tested. I think to strengthen these claims, an experiment would need to have unpredictable stimuli, and potentially combine behavioural reports with decoding to see whether this mechanism can be linked to facilitating object recognition across saccades.</p>
<p>(2) Given that foveal feedback has been found in previous studies that don't incorporate saccades, how is this a mechanism that might specifically contribute to stability across saccades, rather than just being a general mechanism that aids the processing/discrimination of peripherally-viewed stimuli? I don't think this paper addresses this point, which would seem to be crucial to differentiate the results from those of previous studies.</p>
</body>
</sub-article>
</article>