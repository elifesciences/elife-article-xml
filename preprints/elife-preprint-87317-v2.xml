<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">87317</article-id>
<article-id pub-id-type="doi">10.7554/eLife.87317</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.87317.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Intuitive movement-based prosthesis control enables arm amputees to reach naturally in virtual reality</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1266-0570</contrib-id>
<name>
<surname>Segas</surname>
<given-names>Effie</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mick</surname>
<given-names>Sébastien</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Leconte</surname>
<given-names>Vincent</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dubois</surname>
<given-names>Océane</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Klotz</surname>
<given-names>Rémi</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4471-0525</contrib-id>
<name>
<surname>Cattaert</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5645-3680</contrib-id>
<name>
<surname>de Rugy</surname>
<given-names>Aymar</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Univ. Bordeaux, CNRS, INCIA</institution>, UMR 5287, F-33000 Bordeaux, <country>France</country></aff>
<aff id="a2"><label>2</label><institution>ISIR UMR 7222, Sorbonne Université, CNRS</institution>, Inserm, F-75005, <country>France</country></aff>
<aff id="a3"><label>3</label><institution>CMPR Tour de Gassies</institution>, F-33520 Bruges, <country>France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Nazarpour</surname>
<given-names>Kianoush</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Edinburgh</institution>
</institution-wrap>
<city>Edinburgh</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Makin</surname>
<given-names>Tamar R</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Corresponding author : <email>aymar.derugy@u-bordeaux.fr</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-06-02">
<day>02</day>
<month>06</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2023-10-13">
<day>13</day>
<month>10</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP87317</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-03-02">
<day>02</day>
<month>03</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-03-03">
<day>03</day>
<month>03</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.10.15.22281053"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-06-02">
<day>02</day>
<month>06</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87317.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.87317.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.87317.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.87317.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.87317.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Segas et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Segas et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-87317-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Impressive progress is being made in bionic limbs design and control. Yet, controlling the numerous joints of a prosthetic arm necessary to place the hand at a correct position and orientation to grasp objects remains challenging. Here, we designed an intuitive, movement-based prosthesis control that leverages natural arm coordination to predict distal joints missing to people with transhumeral limb loss based on proximal residual limb motion and knowledge of the movement goal. This control was validated on 29 participants, including 7 with above-elbow limb loss, who picked and placed bottles in a wide range of locations in virtual reality, with median success rates over 99% and movement times identical to those of natural movements. This control also enabled 15 participants, including 3 with limb difference, to reach and grasp real objects with a robotic arm operated according to the same principle. Remarkably, this was achieved without any prior training, indicating that this control is intuitive and instantaneously usable. It could be used for phantom limb pain management in virtual reality, or to augment reaching capabilities of invasive neural interfaces usually more focused on hand and grasp control.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<notes notes-type="financial-disclosure">
<title>Funding Statement</title><p>This work was supported by the CNRS interdisciplinary project RoBioVis, and the ANR-DGA-ASTRID grant CoBioPro (ANR-20-ASTR-0012-1).</p></notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>Ethics committee CPP Est II of CHU Besancon gave ethical approval for this work (CCP Est II: 2019-A02890-57).</p><p>I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.</p><p>Yes</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>The manuscript has been revised to address issues and follow recommendations made by 3 reviewers from the editorial process at eLife.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The field of bionic limbs has seen great progress over the last few years, including Targeted Muscle Reinnervation (TMR)<sup><xref ref-type="bibr" rid="c1">1</xref></sup>, osseointegration<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, chronically implanted sensors and stimulators for bidirectional communication with the nervous system<sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c7">7</xref></sup>, and advanced signal processing to encode and decode sensorimotor signals<sup><xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c10">10</xref></sup>. Yet, simultaneous control of the multiple Degrees of Freedom (DoFs) of a prosthetic arm remains challenging, especially to bring a prosthetic hand to the correct location and orientation to efficiently grasp objects. Indeed, although simultaneous and proportional real-time myoelectric control has been achieved for two to three DoFs<sup><xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c17">17</xref></sup>, difficulties appear<sup><xref ref-type="bibr" rid="c13">13</xref></sup> and performance deteriorates as the number of DoFs increases (e.g., success rate dropped from 96 to 37% from one to three DoFs in<sup><xref ref-type="bibr" rid="c17">17</xref></sup>). Furthermore, this was achieved in lab settings, mostly on participants with valid arms, sometimes including few participants with limb difference at transradial level (either amputation or congenital limb difference)<sup><xref ref-type="bibr" rid="c12">12</xref>–<xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup>, using native muscles remaining that normally actuate the DoFs under myoelectric control (forearm, wrist, hand).</p>
<p>In the case of an amputation at humeral level, none of the forearm wrist and hand muscles would remain, and the important additional DoF of the elbow would need to be controlled. Although TMR could be used to recover valid control signals by transferring residual arm nerves controlling missing distal muscles and joints to compartments of remaining muscles<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c19">19</xref></sup>, this is usually geared toward recovering forearm supination-pronation and hand opening-closing<sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>, arguably more important and available in commercial prosthesis, rather than wrist DoFs (flexion-extension and radial-ulnar deviation) which are nevertheless critical to orient the hand in space<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>. In the end, no myoelectric solution exists to simultaneously and intuitively control the four arm DoFs (from elbow to wrist included) that are necessary to people with transhumeral limb loss in order to correctly position and orient their prosthetic hand to grasp objects in a large reachable space.</p>
<p>Here, we provide a solution with an alternative movement-based approach which leverages natural coordination between arm segments and knowledge of the movement goal. Control strategies exploiting natural synergies in arm coordination<sup><xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c24">24</xref></sup> have already been used to predict distal joints from the motion of proximal ones<sup><xref ref-type="bibr" rid="c25">25</xref>–<xref ref-type="bibr" rid="c29">29</xref></sup>. However, these have been mostly confined to the control of one DoF (i.e., either the elbow<sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup> or the wrist supination-pronation<sup><xref ref-type="bibr" rid="c29">29</xref></sup>, reconstructed from shoulder movement), or relying on additional unnatural movements to increase functionality<sup><xref ref-type="bibr" rid="c26">26</xref></sup>. Here, we unleash this movement-based approach by adding knowledge of movement goals, which could be made available through computer vision<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup> combined with gaze information<sup><xref ref-type="bibr" rid="c32">32</xref>–<xref ref-type="bibr" rid="c34">34</xref></sup>. We showed recently that adding target position and orientation to an Artificial Neural Network (ANN) trained to predict four arm distal DoFs (elbow to wrist) from proximal (shoulder) motion greatly improves these predictions, as well as human-in-the-loop control using them<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. Yet, performance remained lower than natural movements, with increased compensatory movements from trunk and shoulder, a limited workspace, and a control design not directly applicable to people with transhumeral limb loss<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. Critical changes were brought about here to overcome all those limitations, and enabled 29 participants (including 7 with transhumeral limb loss) to perform as well as natural, without any prior training, at picking and placing a bottle in a wide reachable space in virtual reality. A physical proof of concept is also provided whereby 15 participants, including 3 with limb deference, were able to reach and grasp real objects at different positions and orientations with a robotic arm operated according to the same control principle.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Natural arm movement in virtual reality</title>
<p>After linking movement trackers placed on participants to that of a virtual arm, subjects were engaged in repeatedly picking a bottle standing on a platform and placing it on another platform (see <xref rid="fig1" ref-type="fig">Fig. 1b</xref>, Methods, Virtual Arm Calibration, and Task). To maximize the workspace of this task, 300 “plausible” target locations (i.e., position and orientation) were determined randomly within the full range of motion established individually for each joint of each participant (see <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, Methods). Natural arm movements recorded to pick and place bottles at those 300 targets were then used to train the ANN involved in our movement-based control (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>, <italic>cf</italic> next section), but also to establish a new set of 200 “possible” targets within the workspace actually covered by the participants’ arm during the initial acquisition (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>). Although “plausible” targets are all supposed to be reachable, in practice, anatomical joints’ limits are interdependent in a way that makes it uncomfortable or impossible to produce arm configurations with maximal excursion simultaneously at multiple joints. To circumvent this, we used an unsupervised self-organizing neural network<sup><xref ref-type="bibr" rid="c36">36</xref></sup> to identify 200 nodes that best represent the arm postures actually produced by participants, and used forward kinematics to turn these nodes into target locations, thereby obtaining a set of 200 possible targets guaranteed to be reachable with natural arm movements (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>, see Methods). While “plausible” targets were used in the initial acquisition to collect natural arm movements from which the movement-based prosthesis control was designed, “possible” targets were used in all experimental test phases involved to compare the different controls. To illustrate the wide resulting workspace, <xref rid="fig2" ref-type="fig">Fig. 2a</xref> shows the sets of plausible and possible targets of a representative participant, and <xref rid="fig2" ref-type="fig">Fig. 2b</xref> the sets of possible targets for all subjects of Exp1 and Exp3. SupplementaryVideo1 illustrates a participant completing the task at a comfortable yet sustained pace, representative of the overall performance observed in our experiments (i.e., with typical movement times of approximately 1.3 seconds between pick and place).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1</label>
<caption><title>Overview of the task and control strategy.</title>
<p><bold>a</bold>. Wide initial workspace. Three hundred 7-DoF arm configurations (grey dots, only 3 angles displayed for convenience) within the joint operating range of a given participant (materialized by the parallelepiped) are transformed into 300 plausible target locations (grey arrows) using forward kinematics. <bold>b</bold>. Natural arm movements are recorded while participants equipped with movement trackers on arm and torso are involved in picking and placing a bottle at the 300 target locations in virtual reality. <bold>c</bold>. The ANN is trained on recorded natural arm movement to reconstruct distal DoFs (orange) from proximal ones (green) plus target information (position and orientation). <bold>d</bold>. Wide space covered during recorded natural arm movements. Two hundred nodes (red dots) that best represent the arm angular configurations actually produced (grey circles) by a participant during her/his recorded natural arm movements were identified using an unsupervised self-organizing neural network, and transformed into a set of 200 possible targets (red arrows) using forward kinematics. <bold>e</bold>. Movement-based prosthesis control. The participant performs the pick and place task at the 200 possible targets using a hybrid arm reproducing in real-time her/his own shoulder movements (green angles), and using the ANN predictions for the 5 remaining distal DoFs (orange angles).</p></caption>
<graphic xlink:href="22281053v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2</label>
<caption><title>Wide workspace covered in experiments.</title>
<p><bold>a</bold>. All targets used for a representative participant of Exp1 are displayed, together with 5 arm postures (4 at extended positions and 1 flexed in the middle) to provide perspectives. Grey arrows represent plausible targets (n=300), and red arrows represent possible targets (n=200). <bold>b</bold>. Possible targets of all participants of Exp. 1 (n=2000, red arrows) and Exp. 3 (n=1400, in blue arrows), remapped for an average arm, and regrouped on the same graph. Note that for Exp3, possible targets corresponding to participants with left sided limb loss were mirrored to be represented in relation to a right arm. This figure illustrates the comparably large workspaces obtained for the 10 participants with intact limbs of Exp1 (used for the Generic ANN) and the 7 participants with transhumeral limb loss of Exp3 (using the Generic ANN).</p></caption>
<graphic xlink:href="22281053v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Intuitive movement-based prosthesis control</title>
<p>From natural movements recorded in the initial acquisition, the control was developed based on an ANN trained to reconstruct distal joint angles from shoulder kinematics plus contextual target information. An initial version of this control was proposed and tested in<sup><xref ref-type="bibr" rid="c35">35</xref></sup>, and critical changes were designed here to improve its quality and applicability to people with transhumeral limb loss. After being trained to reconstruct five distal arm angles from proximal shoulder kinematics plus target location (3D position and 2D orientation, see Methods), the ANN illustrated in <xref rid="fig1" ref-type="fig">Fig. 1c</xref> was used to control a Hybrid Arm emulating the behavior of a residual upper arm fitted with a transhumeral prosthesis (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>). To this end, shoulder flexion-extension and abduction-adduction of the Hybrid Arm were operated from real shoulder movements produced by the operator, whereas the five remaining joint angles were driven by predictions from the ANN. Importantly, the ANN could be trained either solely from the operator’s own natural movements (Own ANN, see Methods) or from those of multiple other participants (Generic ANN). <xref rid="fig3" ref-type="fig">Fig. 3a</xref> summarizes the protocols of the three experiments gradually leading to a functional control solution for prosthesis users. SupplementaryVideo2 to 4 illustrates the ultimate goal reached in this study, that is, to enable participants with transhumeral limb loss to pick and place objects with movement time and performance similar to that with a natural arm.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>Protocols and results.</title>
<p><bold>a</bold>. Protocols of the three experiments. Each box contains a phase name and the name of the control used. Fam. stand for Familiarization phase, and Initial Acq. for Initial Acquisition phase. The order of test phases conducted with the Own and the Generic ANNs were counterbalanced in Exp2. <bold>b-d</bold>. Results for success rate (<bold>b</bold>), movement time (<bold>c</bold>) and shoulder volume (<bold>d</bold>). Each grey line corresponds to a participant. In Exp2, dashed lines indicate participants who began by the control with the Generic ANN and plain lines those who began by the control with the Own ANN. Boxes limits show first and third quartiles whereas inside line shows the median value. Whiskers show min and max values. Own, Gen, Nat represent phases in which the control was performed with the Own ANN, the Generic ANN, and the Natural Virtual Arm, respectively. In Exp3, Gen1 and Gen2 refer to the first and second block performed with the Generic ANN. Stars represent significant differences, with * for p &lt; 0.05, ** for p &lt; 0.01, and *** for p &lt; 0.001. The dashed red line represents a volume of 1 dm3 ( = 1 L).</p></caption>
<graphic xlink:href="22281053v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<title>Specific control based on participants’ own movements</title>
<p>The first experiment aimed at (i) emulating a prosthesis control as intuitive as possible in a wide workspace for participants based on their own natural movements, and (ii) collecting natural movements from several subjects to build a generic model for prosthesis control to be tested on other participants with intact limbs or with limb difference (in Exp2 and 3, respectively). After an initial acquisition in which they picked-and-placed 300 plausible targets with their right arm in a wide workspace, 10 right-handed subjects were tested on 200 possible target locations with our intuitive prosthesis control trained on their own movements (Exp1, TestOwn, <xref rid="fig3" ref-type="fig">Fig. 3a</xref>) before being tested again on the same targets with their natural arm movements (TestNat).</p>
<p>Results indicate high success rates for both conditions (median success rate of 100% and 99.7% for TestNat and TestOwn, respectively), with a minor (&lt;1%) albeit significant difference between them (TestOwn vs TestNat; n = 1886; McNemar test, p = 0.023, df = 1; <xref rid="fig3" ref-type="fig">Fig. 3b</xref>). Regarding movement times (i.e., time taken to reach and validate each target from the previous one), results were also closely similar and not significantly different between conditions (TestOwn vs TestNat; n = 10; medians of 1.25 s vs 1.17 s respectively; two-tailed paired t-test, p = 0.378, t = -0.927, df = 9). Visual inspection of individual distributions of movement times provided <xref rid="figs2" ref-type="fig">supplementary Fig. 2</xref> indicates that despite slight differences observed for some participants, no specific pattern emerges, and distributions looks very similar between conditions when data from all participants are pooled together. The volume spread by the shoulder’s trajectory throughout a phase, which includes compensatory movements of the body that might be elicited to compensate for imperfect control (see<sup><xref ref-type="bibr" rid="c35">35</xref></sup>), was also comparable and not significantly different between conditions (TestOwn vs TestNat; n = 10; medians of 0.22 dm3 vs 0.18 dm3 respectively; two-tailed paired t-test, p = 0.058, t = - 2.168, df = 9).</p>
<p>Overall, our movement-based control trained on the own natural movements of each participant enabled almost perfect success rate (99.7%), and movement times similar to that with their natural arm to pick and place the bottle in the wide workspace tested. Yet, this control is inapplicable “as is” to individuals with limb difference, for whom recording natural movements is obviously not possible from their missing arm. The next step was therefore to establish an equivalent control from data collected on the multiple participants of Exp1, and to be used on new naive participants in Exp2.</p>
</sec>
<sec id="s2d">
<title>Generic control based on movements from other participants</title>
<p>After building a generic model from data recorded in Exp1 (cf Methods), the second experiment aimed at (i) assessing performance with this generic model (as compared to that with natural movements or with a control based on each participant’s own natural movements) and (ii) validating the use of this generic model on left-handed participants using their dominant arm. As the relationship between hand locations and arm configurations depends on segments dimensions that differ between participants, a critical step toward building an efficient generic model for a new user was to remap hand position data from previous participants to the arm dimensions of that new user (<xref rid="figs1" ref-type="fig">Supplementary Fig. 1b</xref>). This was done using forward kinematics to remap data from all participants of Exp1 before using them to train the generic ANN to be used for intuitive control in TestGeneric by each participant of Exp2 (see Methods). After an initial acquisition phase equivalent to that of Exp1, 12 participants (6 left-handed) were tested on our movement-based control using either the generic model (TestGeneric) or the model based on their own movements (TestOwn). The order between TestGeneric and TestOwn was counterbalanced amongst subjects, with 3 left-handed participants in each group (see <xref rid="fig3" ref-type="fig">Fig. 3a</xref>). The protocol ended with each participant being tested again with their natural arm movements (TestNat).</p>
<p>Participants achieved high success rates in all conditions (median success rates of 99% or higher, cf <xref rid="fig3" ref-type="fig">Fig. 3b</xref>), although minor differences between conditions (&lt;1%) were found significant (TestOwn vs TestGeneric vs TestNat; n = 2268; medians of 99.50 % vs 99.24 % vs 100 % respectively, Cochran’s Q test, p = 5.19.10<sup>-12</sup>, Q = 51.97, df = 2; post-hoc McNemar test with Bonferroni adjustment; TestOwn vs TestGeneric, p = 6.42.10-5, chi.sq = 18.1, df = 1; TestOwn vs TestNat, p = 8.58.10<sup>-3</sup>, chi.sq = 8.89, df = 1; TestGeneric vs TestNat, p = 3.24.10<sup>-10</sup>, chi.sq = 41.7, df = 1). Median movement times remained below 1.4s in all conditions (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>), with no significant difference found between conditions (TestOwn vs TestGeneric vs TestNat; n = 12; RM ANOVA test, p = 0.181, DFn = 2, DFd = 22, F = 1.848). Visual inspection of individual distributions of movement times provided <xref rid="figs3" ref-type="fig">Supplementary Fig. 3</xref> indicates that despite slight differences observed for some participants, no specific pattern emerges, and distributions looks very similar between conditions when data from all participants are pooled together. With respect to the volume spread by the shoulder during the Test phases (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>), the statistical analysis revealed a significant effect of condition, (TestOwn vs TestGeneric vs TestNat; medians of 0.23dm3 vs 0.43 dm3 vs 0.14 dm3 respectively, n = 12; RM ANOVA test, p = 0.003, DFn = 2, DFd = 22, F = 7.806), with post-hoc tests indicating a higher volume for TestGeneric than for Test Nat (Tukey test with Bonferroni correction; TestOwn vs TestNat, p = 0.385; TestGeneric vs TestNat, p = 0.025; TestOwn vs TestGeneric, p = 0.347). Despite this difference, the spread volumes remained contained, never exceeding 1 liter (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>), which appears reasonable given the high compensations that could have occurred with less efficient controls over the wide workspace spanned by the targets.</p>
<p>Overall, our control based on a generic model trained on data from other participants enabled therefore new participants to reach almost all targets (&gt;99%) as well as with their natural arm (or an intuitive control trained on their own natural movements) regardless of handedness, with a moderate increase in compensatory movements. The only remaining step was then to validate this control directly on individuals with limb difference.</p>
</sec>
<sec id="s2e">
<title>Successful validation on individuals with limb loss</title>
<p>As we now have an intuitive control applicable to participants with an arm amputation on either side of their body, we tested it on 7 participants with unilateral transhumeral limb loss, 3 of them disabled on their left side. As no initial acquisition phase was conducted on these participants, the set of 200 possible targets was determined for each of them using a self-organizing map to extract representative postures from a large number of postures generated by movements produced by all participants of Exp1 filtered to the range of motion of their residual limb (see Methods). Given the lack of initial acquisition, participants had much less practice of the task before they were tested. Therefore, two blocks of intuitive control with the Generic ANN were conducted on their amputated side (TestGeneric 1 and 2, <xref rid="fig3" ref-type="fig">Fig. 3</xref>), before they were tested on the same mirrored targets with their valid arm (TestNat).</p>
<p>As for other experiments, success rates were very high (all medians above 99%), and did not differ between conditions (TestGeneric1 vs TestGeneric2 vs TestNat; medians of 99.24 % vs 99.50 % vs 99.50 % respectively, n = 1128; Cochran’s Q test, p = 0.1146, Q = 4.33, df = 2). Movement times with our movement-based control were also in the same range as in previous experiments, and were even smaller by the second block of intuitive control (TestGeneric2) than with their valid arm (TestGeneric1 vs TestGeneric2 vs TestNat; medians of 1.27 s vs 1.16 s vs 1.40 s; n = 7; Friedman test, p = 0.002, chi-squared = 12.286, df = 2; post-hoc Conover test; TestGeneric1 vs TestNat, p = 0.619; TestGeneric2 vs TestNat, p = 0.014; TestGeneric1 vs TestGeneric2, p = 0.161). Visual inspection of individual distributions of movement times provided <xref rid="figs4" ref-type="fig">Supplementary Fig. 4</xref> confirms that despite slight difference between participants, movement times tend to be smaller for TestGeneric phases (and even more so for TestGeneric2) than for TestNat. The volumes spread by the shoulder were comparable to that measured with the generic model on Exp2 (median 0.43 dm3) and did not differ between conditions (TestGeneric1 vs TestGeneric2 vs TestNat; medians of 0.36 dm3 vs 0.37 dm3 vs 0.33 dm3; n = 7; Friedman test, p = 0.867, chi-squared = 0.28571, df = 2).</p>
<p>Overall, without any prior experience at the task nor with the apparatus, our intuitive control allowed disabled participants to achieve performance as good as with their valid arm, and to achieve comparable levels of performance as for other subjects in all conditions tested (see <xref rid="fig3" ref-type="fig">Fig. 3b-d</xref>). Furthermore, beyond objective performance measures, our intuitive control elicited high enthusiasm from participants with limb loss. This is particularly well illustrated by the following feedback they provided during or after the experiment: “<italic>X</italic> years since I was able to do that, this is moving”; “doing a real movement, this is enjoyable”; “a prosthesis that would be controlled like that? I take it straight away”. See SupplementaryNote1 for more feedback from participants with limb loss.</p>
</sec>
<sec id="s2f">
<title>Physical Proof of Concept on a tele-operated robotic platform</title>
<p>To demonstrate the feasibility of our approach in the physical world, we conducted a Proof of Concept (POC) whereby 15 participants, including 2 with acquired and 1 with congenital transhumeral limb difference, were involved in reaching and grasping real objects in various positions and orientations with our novel control applied on a humanoid robotic platform specifically designed to explore human-robot control strategies<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. As with the Hybrid Arm used to emulate the behavior of an upper arm fitted with a tranhumeral prosthesis in virtual reality (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>), the shoulder flexion-extension and abduction-adduction of the robotic arm were operated from real shoulder movement produced by the operator, whereas the five remaining distal joint angles were driven by prediction from the generic ANN (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). In contrast to virtual reality, however, the torso of the humanoid robotic platform was fixed and independent of the operator, such that compensatory movements from the trunk and from shoulder translations were not transmitted to the device. This is important because this implies that the task could only be achieved with the designed control. As in virtual reality, our novel control solution (TestGeneric) was tested against a control based on natural arm movements (TestNat), whereby all joints of the robotic arm were operated directly from arm movements produced by intact-limbs participants (POC protocols <xref rid="fig4" ref-type="fig">Fig. 4c</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 4</label>
<caption><title>Physical Proof of Concept on a tele-operated robotic platform.</title>
<p><bold>a</bold>. Task and setup. The participant stands still setback from the humanoid robotic platform that faces a board on which 5 sponges are placed at different positions and orientations. The participant tele-operates the robotic arm so as to reach and grasp each of the 5 sponges of a block, one trial after another, according to order indicated by numbers written on sponges. <bold>b</bold>. Three types of blocks define three spatial arrangements of sponges on the board. <bold>c</bold> Protocols of the Proof of Concept (POC) experiments. Each box contains a phase name and the name of the control used, either based on natural arm movements (TestNat) or on predictions from the Generic ANN (TestGeneric). Fam. stand for Familiarization phase. The order of test phases was counterbalanced in POCa and POCb. <bold>d-e</bold>. Results for success rate (<bold>d</bold>) and movement time (<bold>e</bold>). Each grey line corresponds to a participant. In POCa-b, dashed lines indicate participants who began by TestGeneric and plain lines those who began by TestNat. Boxes limits show first and third quartiles whereas inside line shows the median value. Whiskers show min and max values. Stars represent significant differences, with * for p &lt; 0.05, ** for p &lt; 0.01, and *** for p &lt; 0.001. Triangles represent performances obtained for the Block 1 by the two participants with transhumeral limb loss whereas the square represents performances of the congenital limb different participant on all 3 Blocks.</p></caption>
<graphic xlink:href="22281053v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Despite the functionality of compensatory movement being withdrawn by design, high success rates were achieved by all participants in both conditions (<xref rid="fig4" ref-type="fig">Fig. 4d</xref>). A significant difference was nevertheless found between the proportion of validated targets using our movement-base control and natural control (TestGeneric vs TestNat; medians 93% vs 100% respectively; n = 180; McNemar test, p = 0.0003, chi.sq = 13.1, df = 1; <xref rid="fig4" ref-type="fig">Fig. 4d</xref>). The congenital limb different participant was able to reach and grasp 93.3% of the targets (i.e., 14/15), and the two participants with transhumeral limb loss were both able to grasp 80% of the first five targets which means that only one target was failed. Furthermore, the validated targets were reached and grasped with movement times similar to those obtained using natural control applied to the robotic arm (POCa-b TestGeneric vs TestNat; medians 6.08s vs 5.98s respectively; two-tailed paired t-test, p = 0.7026, t = -0.39198, df = 11; <xref rid="fig4" ref-type="fig">Fig. 4e</xref>). Although no statistical analysis was conducted, the movement times recorded for the congenital limb different participant and for the 2 participants with transhumeral limb loss were similar to those obtained by the intact-limbs participants with the natural control (Medians: Congenital = 5.95s; Amp1 = 5.81s; Amp2= 4.23s).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Stunning progress has been made in the field of bionic limbs to restore important hand grasping and object manipulation functions, with invasive surgery<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup> and implants enabling bidirectional communication with the nervous system<sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>. Yet, controlling the numerous joints of a prosthetic arm necessary to place the hand at a correct position and orientation to grasp objects remains challenging, and is essentially unresolved. Here we provide a non-invasive, movement-based solution to this problem, and a clear demonstration of its effectiveness on individuals with limb difference. Indeed, 29 participants including 7 with above-elbow limb loss were able to pick and place bottles in a wide workspace with almost perfect scores (median success rates above 99%) and movement times identical to those of natural movements, while five distal joints of their arm were controlled with our novel solution. The same control principles applied on a robotic platform<sup><xref ref-type="bibr" rid="c37">37</xref></sup> also enabled 15 participants, including 3 with limb difference, to reach and grasp real objects at different positions and orientations, with good success rates and movement times similar to those obtained when the robot was controlled with natural arm movements. This is ahead of other control solutions that have been proposed so far to solve this problem, and whose performances are rarely compared to that of natural movements (as unfavorable as this comparison would be to them). We now place this in perspective of recent related works, discussing critical features that enabled such performance, as well as remaining gaps and perspectives for daily-life applications.</p>
<sec id="s3a">
<title>Critical features that unleashed movement-based prosthesis control</title>
<p>The first feature that enabled our movement-based control to work so well compared to previous attempts was to introduce movement goal as an input to the trained ANN. Without it, the ANN would predict the most likely configuration of the distal joints for a given proximal (shoulder) posture, but irrespective of the location of the object to reach. Although this was found to provide an average distal configuration which was nevertheless suitable to reach in a limited workspace, at the expanse of compensatory movements from the trunk and shoulder<sup><xref ref-type="bibr" rid="c35">35</xref></sup>, we also showed that adding movement goal as an input greatly improves the performance of this control strategy<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. Indeed, this enabled to control simultaneously four distal DOFs from elbow to wrist with close to natural coordination<sup><xref ref-type="bibr" rid="c35">35</xref></sup>, where previous attempts were only able to control one DOF (either the elbow<sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup> or the wrist supination-pronation<sup><xref ref-type="bibr" rid="c29">29</xref></sup>), or rested upon additional unnatural movements to increase functionality<sup><xref ref-type="bibr" rid="c26">26</xref></sup>. Yet, performance levels were still lower than natural movements, with increased movement times and compensatory movements despite the somewhat limited workspace used, and a control design that was not directly applicable on people with limb loss<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. Here, we overcame those three limitations.</p>
<p>First, we greatly expanded the applicable workspace of this control. This was achieved by starting from the widest possible workspace, limited by the maximal range of motion of participants, and subsequently using a self-organizing network<sup><xref ref-type="bibr" rid="c36">36</xref></sup> to best represent the space covered by participants while producing natural arm movements within this space. Second, we substantially increased the amount of relevant training data, by using the entire trajectories of the recorded natural arm movements, instead of using only arm postures placing the hand sufficiently close to the target<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. Critically, the use of the entire trajectories was made efficient by artificially placing the target in the hand (see Methods), such that the ANN trained on those data performs a form of natural inverse kinematic solving, i.e., one that provides a solution that is representative of natural arm postures rather than a mere optimization for an arbitrary cost function. Third, we made the control applicable to people with limb loss. Indeed, this was not the case in<sup><xref ref-type="bibr" rid="c35">35</xref></sup> as we used the forearm sensor (not available in people with limb loss) to better assess humeral rotation, which could not be reliably measured from the sole upper arm sensor due to muscles and soft tissues around the humerus. Here, the humeral rotation was transferred as an output of the ANN, being a predicted output rather than a necessary input for the control system. In addition, instead of training the control on natural arm movements specifically produced by the intended user (also not applicable in people with limb loss), we designed a generic control based on natural movements from multiple other individuals, but specifically tuned to the morphology of the user (see Methods). This specific tuning was essential, since a given target position and orientation could call for markedly different arm postures depending on the particular arm morphology of an intended user. Finally, mirror symmetry with respect to the medial plane was applied to accommodate for either side of an amputation. Taken together, all those features enabled our participants, including those with an above-elbow amputation, to reach as well as with their natural arm with our movement-based distal joint prosthesis control.</p>
</sec>
<sec id="s3b">
<title>Perspectives for daily-life applications</title>
<p>Despite the clear benefits mentioned above, including movement goal as an input of the control system could be seen as a weakness, as it might be difficult to determine in real life settings. Yet, impressive progress in artificial intelligence and computer vision is such that what would have been difficult to imagine a decade ago appears now well within grasp<sup><xref ref-type="bibr" rid="c38">38</xref></sup>. For instance, we showed recently that deep learning combined with gaze information enables identifying an object that is about to be grasped from an egocentric view on glasses<sup><xref ref-type="bibr" rid="c33">33</xref></sup>, and this even in complex cluttered natural environments<sup><xref ref-type="bibr" rid="c34">34</xref></sup>. Six-dimensional object pose estimation is also a very active area of computer vision<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>, and prosthesis control strategies based on computer vision combined with gaze and/or myoelectric control for movement intention detection are quickly developing<sup><xref ref-type="bibr" rid="c39">39</xref>–<xref ref-type="bibr" rid="c44">44</xref></sup>, illustrating the promises of this approach. It remains that generalizing our approach to multiple tasks including more constrained reaches will require future work. For instance, once an intended object has been successfully reached or grasped, what to do with it will still require more than computer vision and gaze information to be efficiently controlled. One approach is to complement the control scheme with subsidiary movements, such as shoulder elevation to bring the hand closer to the body or sternoclavicular protraction to control hand closing<sup><xref ref-type="bibr" rid="c26">26</xref></sup>, or even movement of a different limb (e.g., a foot<sup><xref ref-type="bibr" rid="c45">45</xref></sup>). Another approach is to control the prosthesis with body movements naturally occurring when compensating for an improperly controlled prosthesis configuration<sup><xref ref-type="bibr" rid="c46">46</xref></sup>. In both cases, particular attention should be paid to ensure that subsidiary movements do not contaminate natural arm coordination, which is essential to the current movement-based control.</p>
<p>Although our approach enabled participants to converge to the correct position and orientation to grasp simple objects with movement times similar to those of natural movements, it is important to note that further developments are needed to produce natural trajectories compatible with real-world applications. As easily visible on supplementary videos 2 to 4, the distal joints predicted by the ANN are realized instantaneously such that a discontinuity occurs at each target change, whereby the distal part of the arm jumps to the novel prediction associated with the new target location. We circumvented problems associated with this discontinuity on our physical proof of concept by introducing a period before the beginning of each trial for the robotic arm to smoothly reach the first prediction from the ANN. This issue, however, needs to be better handled for real-life scenarios where a user will perform sequences of movements toward different objects.</p>
<p>Another requirement for our control to be functional on prosthesis is to have actuated wrist joints available, as those are essential to orient the hand in space<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>. This is not the case for most commercial prostheses, which sometimes include wrist flexion-extension (mostly passive), and very rarely wrist radial-ulnar deviation<sup><xref ref-type="bibr" rid="c47">47</xref></sup>. Notable exceptions includes the LUKE/DEKA arm<sup><xref ref-type="bibr" rid="c48">48</xref></sup> and the RIC arm<sup><xref ref-type="bibr" rid="c49">49</xref></sup>, which both include those two degrees of freedom as actuated joints, but with a fixed linear relationship between them. Hopefully, the type of control proposed here will highlight the need for, and foster mechatronic developments of, a suitable actuated wrist with human-like motion capabilities<sup><xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c50">50</xref></sup>.</p>
<p>As already mentioned, the solution proposed here is suitable to control distal arm joints to place the hand at a correct position and orientation to grasp objects in a wide workspace, but not for fine hand and grasp control involved in object manipulations, which relies heavily on tactile and somatosensory feedback information<sup><xref ref-type="bibr" rid="c51">51</xref>,<xref ref-type="bibr" rid="c52">52</xref></sup>. In this context, our movement-based approach appears complementary to more invasive ones, which specifically target those latter functions through bi-directional interactions with the nervous system for both motor control and sensory processing<sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c7">7</xref></sup>. Combining those with osseointegration at humeral level<sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref></sup> would be particularly relevant as this would also restore amplitude and control over shoulder movements, which are essential for our control but greatly affected with conventional residual limb fitting harness and sockets. Yet, testing with a physical prosthesis will need to ensure that the full desired workspace can be obtained with the weight of the attached device, and if not, a procedure to scale inputs will need to be refined. Finally, our movement-based approach could also be combined with semi-autonomous grasp control to accommodate for multiple grasp functions<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c44">44</xref></sup>.</p>
<p>Besides developments needed for application to a real-life setting, the control proposed here could be used as is in virtual reality for the management of Phantom-Limb Pain (PLP), a painful sensation perceived in the missing limb that often occurs after an amputation. Although the precise mechanisms behind PLP and its proposed treatments are still debated and unresolved<sup><xref ref-type="bibr" rid="c53">53</xref>,<xref ref-type="bibr" rid="c54">54</xref></sup>, reduction of pain has been repeatedly reported using mirror therapy, whereby the intact hand is moved while the patient views it through a mirror at the place of his/her missing limb<sup><xref ref-type="bibr" rid="c55">55</xref></sup>. Yet, mirror therapy was found ineffective on patients with distorted (telescoped) phantom limb<sup><xref ref-type="bibr" rid="c56">56</xref></sup>, and is not applicable to people with bilateral limb loss. Those two limitations can easily be overcome in virtual reality<sup><xref ref-type="bibr" rid="c57">57</xref></sup>, and our novel movement-based control provides a solution immediately available to control virtual (missing) limbs with natural coordination solely from residual limb motion.</p>
<p>Importantly, self-reported feedback from amputated participants indicates that overall, they found our prosthesis control solution intuitive and natural, and would use it should it be available on their prosthesis (see SupplementaryNote1). Given the rich perspectives associated with this movement-based control alternative, its complementarity with other quickly developing approaches, and the demonstration provided here of its effectiveness on people with limb loss, we believe that this alternative is going to positively impact the field of bionic limbs and prosthesis control.</p>
</sec>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>All participants had normal or corrected-to-normal vision and none suffered from motor disorder that could have affected their ability to perform the task (except limb difference in Exp3 and in the physical proof of concept POC). The intact-limbs participants’ handedness was assessed using the Edinburgh Handedness Inventory (EHI)<sup><xref ref-type="bibr" rid="c58">58</xref></sup>. For Exp1 and 2, EHI scores over 50 (below -50) corresponded to right-handed (left-handed) participants. For POCa-b, intact-limbs participants with positive EHI scores were included. Exp1 was conducted on 10 naive, intact-limbs, right-handed participants (5 males, EHI mean 84.0; SD 18.4), aged 24 to 43 years (mean 27.3; SD 6.0). Exp2 was conducted on 12 naive, intact-limbs participants (8 males), aged 20 to 35 years (mean 24.1; SD 4.4). Six of them were right-handed (EHI mean 96.7; SD 5.2), and the other 6 were left-handed (EHI mean -85.4; SD 13.0). Exp3 was conducted on 7 naive participants having undergone transhumeral amputation (7 males), aged 25-48 (mean 40.4; SD 8.4). Information related to each participant’s amputation are provided in <xref rid="tbl1" ref-type="table">Table 1</xref>. POCa-b were conducted on 12 naive, intact-limbs participants (6 males, EHI mean 87.1; SD 22.8), aged 19 to 69 years (mean 33.3; SD 16.6). POCc was conducted on 1 congenital limb different participant, with forearm malformation on the right side, male, aged 22, naive about the task. POCd was conducted on 2 male participants having undergone transhumeral amputation on the right side, aged 34 and 39 years. Both were also included in the Exp3 (see <xref rid="tbl1" ref-type="table">Table 1</xref> lines 4 and 5). They completed Exp3 before POCd. All participants gave their informed consent and research presented here has been conducted in accordance with the Declaration of Helsinki and with local ethics committee (CCP Est II: n°2019-A02890-57).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Exp3 participants’ amputation description.</title><p>Each line contains the time since amputation, the residual limb circumference and length, and the side of the amputation for a participant (R = right, L = left).</p></caption>
<graphic xlink:href="22281053v3_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s4b">
<title>Apparatus</title>
<p>During an experimental session, participants remained seated on a chair located at the center of the experimental room. They wore a virtual reality headset (Vive™ Pro, HTC Corporation) that was adjusted by the experimenter to fit the head firmly and comfortably. When movements of the dominant arm for intact-limbs participants or the valid arm (contralateral to the amputated side) for people with transhumeral limb loss were recorded, four motion trackers (Vive™ Tracker HTC Corporation) were attached to the body using elastic straps. Each segment of the arm (upper arm, forearm and hand) as well as the trunk had a dedicated tracker attached to it. The fingers were immobilized with hand wraps so that the hand tracker would move with wrist movements only. For people with transhumeral limb loss, when motion of the amputated side was recorded, only two trackers were attached: one on the trunk and one on the residual limb. A push-button was placed under the participant’s contralateral hand or under the participant’s dominant foot.</p>
<p>The infrared beacons and virtual environment were calibrated so that the workspace was centered on the chair, its ground plane at the same height as the room’s floor and its scale identical to real-world dimensions. For each VR device (headset and trackers), the tracking setup measured the 3D position and orientation relative to a fixed reference frame within the virtual environment, using SteamVR (Valve Corporation) as middleware. These measurements were recorded at 90 Hz and the virtual environment was displayed synchronously to the participant at a 90 Hz refresh rate through the headset’s stereoscopic display. The Unity engine (Unity Technologies) was used to run the simulation of the virtual scene’s contents and interaction with the participant.</p>
</sec>
<sec id="s4c">
<title>Virtual Arm Calibration</title>
<p>The scene displayed a virtual arm whose skeleton consisted of three rigid segments (upper arm, forearm and hand) linked to each other by spherical joints. After the participant was equipped with the VR devices, a procedure was carried out to make this virtual arm mimic the participant’s actual arm motion. This procedure included five steps:
<list list-type="order">
<list-item><p>During a ten-second recording, motion data were collected while the participant was asked to perform slow movements using all of the arm’s degrees of freedom (DoFs): shoulder flexion-extension (θ<sub>S-FE</sub>), shoulder abduction-adduction (θ<sub>S-AA</sub>) and humeral rotation (θ<sub>H-R</sub>), elbow flexion-extension (θ<sub>E-FE</sub>), forearm pronation-supination (θ<sub>F-PS</sub>), wrist flexion-extension (θ<sub>W-FE</sub>) and radial-ulnar deviation (θ<sub>W-RU</sub>) (see <xref rid="figs1" ref-type="fig">Supplementary Fig. 1a</xref>). For participants with arm amputation, when residual limb movements were recorded, only the first two DoFs were taken into account.</p></list-item>
<list-item><p>The method described in<sup><xref ref-type="bibr" rid="c59">59</xref></sup> was applied to estimate the joint centers’ locations relative to a parent tracker. The upper arm’s tracker worked as the parent for the virtual shoulder and elbow, whereas the forearm’s tracker worked as the virtual wrist’s parent. At the end of this step, the estimated joint centers were displayed as yellow spheres linked by grey lines and the trackers’ silhouettes were outlined in the virtual scene. For people with transhumeral limb loss, when residual limb movements were considered, only the shoulder’s center was estimated, with the upper arm’s tracker working as parent. Based on the tracker’s orientation, a grey line drawn in the virtual scene indicated the estimated actual arm’s humeral axis. The line’s length was estimated based on the participant’s height (see 3). A yellow sphere representing a hypothetical elbow center was placed according to these estimations of the humerus’s orientation and length as well as the shoulder center.</p></list-item>
<list-item><p>The virtual arm’s segment dimensions were adjusted to match those of the participant’s arm. These dimensions were measured as the distances between estimated joint centers. When residual limb movements were used on people with transhumeral limb loss, these dimensions were computed by scaling a standard set of segment lengths based on the participant’s height.</p></list-item>
<list-item><p>Then, the virtual shoulder was attached to the participant’s estimated shoulder center, so that the root of the virtual arm would follow the actual shoulder at all times.</p></list-item>
<list-item><p>The virtual arm was locked in a reference posture with the elbow flexed at 90° where its segment orientations and joint positions were clearly visible. The virtual arm’s segments were “linked” to the corresponding trackers one at a time, while the yellow spheres acted as anatomical landmarks. First, the participant was asked to move their arm so that the yellow sphere representing the estimated elbow center overlaid the virtual arm’s elbow. When the overlaying was deemed correct, the virtual upper arm became a child object of the corresponding tracker, so that its orientation followed that of the actual upper arm. Then, the same method was repeated to associate the virtual and actual forearms by overlaying the estimated wrist’s yellow sphere with the virtual arm’s wrist. Finally, the tracker’s silhouette was used as a landmark for the participant to orient the actual hand similarly to the virtual hand, aligned with the virtual forearm. The procedure ended with the virtual hand being made a child object of the hand tracker. For people with transhumeral limb loss, when residual limb movements were considered, only the virtual upper arm needed to be attached to the corresponding tracker. The hypothetical elbow sphere was used as a landmark for participants to align their residual limb with the virtual upper arm.</p></list-item>
</list>
As a final step, this virtual arm calibration was corrected for errors in humeral rotation, and reduced from a 9-DoF to a 7-DoF virtual arm. Indeed, soft tissues around the biceps and triceps are such that the sensor attached to the upper arm is not able to follow accurately the rotation of the humerus. To counter this, humeral rotation was computed based on the triangle formed by the centers of the three joints (shoulder, elbow and wrist), estimated using both the upper arm and the forearm sensors. Furthermore, the procedure described above considers three rigid segments linked by spherical joints offering each three DoFs in rotation. Although the resulting nine DoFs allowed the arm’s segments to be placed at all times in orientations identical to those of the actual arm despite slight variations from an ideal 7-DoF arm, the reduction to seven DoFs was necessary to match anatomical arm description, and to emulate control over the relevant prosthesis joints. This was achieved by extracting seven joint angles from the 9-DoF kinematic chain’s segment orientations, following a kinematic model comprising three DoFs at shoulder level, one DoF at elbow level and three DoFs at wrist level. Then, the same model was used to compute the segment orientations corresponding to the posture described by these seven joint angles, and the resulting virtual arm was moved accordingly in the virtual scene.</p>
</sec>
<sec id="s4d">
<title>Hybrid Arm Control</title>
<p>The Hybrid Arm was designed to emulate the behavior of a residual upper arm fitted with a transhumeral prosthesis (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>). In the case of an actual prosthesis, movements of the whole arm would combine the wearer’s residual limb motion with the prosthesis’s actuation of artificial joints, hence the term “Hybrid”. To emulate this behavior, the two most proximal DoFs (<italic>i.e.,</italic> shoulder flexion-extension and shoulder adduction-abduction) were taken from the participant’s natural shoulder motion derived from the Virtual Arm, whereas the five remaining joint angles were driven by predictions from an Artificial Neural Network (ANN) trained as indicated in the next section (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>). Following the same 7-DoF model as with the Virtual Arm, segment orientations were then computed from the whole set of seven joint angles, and the Hybrid Arm was moved accordingly in the virtual scene.</p>
</sec>
<sec id="s4e">
<title>Own and Generic ANN</title>
<p>In order to drive the Hybrid Arm’s five distal joints, an ANN was trained to predict the five corresponding joint angles from natural arm movements recorded using the Virtual Arm in the VR setup. This section presents the two ANNs used in this study: The Own ANN, trained on the data produced by the same participant as the one that is going to use the network to control the Hybrid Arm, and the Generic ANN, trained on data from 10 other participants recorded in Exp1, and tuned to the arm size of the user. ANNs inputs and outputs are presented in <xref rid="fig1" ref-type="fig">Fig. 1c and in</xref> <xref rid="figs1" ref-type="fig">Supplementary Fig. 1a</xref>. The network structure includes two densely connected layers of 256 neurons each, a dropout layer with a drop fraction of 0.5 and a dense layer of 64 neurons. The network was implemented and trained using TensorFlow in association with Keras as the programming interface.</p>
<p>The Own ANN training data was taken from the recording of an Initial Acquisition phase performed with the Virtual Arm (cf. Experimental Phases). From this recording, 7 signals were extracted and fed to the ANN as inputs: the 2 most proximal angles of the Virtual Arm, and 5 goal-related <italic>contextual information</italic> (3 Cartesian coordinates and 2 spherical angles that define the position and orientation of the hand as if a hypothetical cylindrical target was placed in it at any time, see an explanation for this choice in the next paragraph). The error between the ANN outputs (i.e., predictions of the 5 distal DoFs) and the actual 5 distal joints of the Virtual Arm produced in the same recording session were used to train the ANN. The network was trained during a pause after the Initial Acquisition phase, and was therefore specific to the corresponding participant.</p>
<p>In our experiments, the targets were sparse and scattered within a wide and continuous workspace. Mirroring this discrete distribution, the goal-related contextual information describing the target locations provided discontinuous and highly clustered signals, displaying little variability within a trial and changing abruptly to express the new target location as soon as the next trial began. Preliminary testing revealed that training on such input signals resulted in the ANN being much more subject to overfitting and less efficient for control. To avoid this issue, the training data made use of hand locations instead of target locations to provide contextual information. For each sample, the recorded arm posture was therefore treated as if it brought the virtual hand at the exact location of a hypothetical target. Accordingly, the contextual information provided as input corresponded to the position and orientation of the virtual hand, such that the training data covered the workspace more homogeneously and continuously. Thirty epochs were done with a learning rate of 1e-4, similar to that used in<sup><xref ref-type="bibr" rid="c35">35</xref></sup>.</p>
<p>In the case of a person with transhumeral limb loss, driving the prosthesis’s joints with an ANN trained on the wearer’s own motion tracking data would be impractical. To tackle this, we designed a method to create a Generic ANN, by transforming motion tracking data recorded from previous participants into a training dataset adapted to the current participant. Recordings produced in the Initial Acquisition phase of the 10 participants of Exp1 were concatenated into a single dataset. However, this dataset would not be appropriate “as is” to train a Generic ANN because the relationship between hand locations and postures depends on arm segment dimensions that differ between participants. Indeed, as illustrated <xref rid="figs1" ref-type="fig">Supplementary Fig. 1b</xref>, similar arm postures give rise to different hand locations depending on arm morphology, such that hand positions need to be “remapped” to the current participant’s arm before being fed as training data to a Generic ANN. This was achieved using forward kinematics solving with the 7-DoF model underlying the Virtual Arm calibrated for the current participant. The dataset adapted to the current participant contained the remapped hand locations as well as the original arm angular configurations from Exp1, and was used to train the Generic ANN to perform the same task as the Own ANN: predict 5 distal DoFs from 2 proximal joint angles and 5 spatial parameters expressing the hand location. For the sake of fair comparison, this network’s structure (<italic>i.e.,</italic> layer arrangement and number of neurons) was identical to that of the Own ANN. However, the training dataset included data from 10 participants instead of just 1, thus containing approximately 10 times more samples than data used to train an Own ANN. Given this, the epoch number was reduced to 10 to minimize computation time, and a momentum parameter was introduced to further prevent overfitting. An offline analysis indicated that a learning rate of 1.59e-7 combined with a momentum of 0.95 constitutes a good compromise for the Generic ANN to perform well both when the target is considered in the hand (as for the training data used) and when moving toward it (as it is mostly the case during online experimental phases).</p>
</sec>
<sec id="s4f">
<title>Task</title>
<p>All experiments relied on a pick-and-place task with a virtual cylindrical bottle (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). Participants were asked to perform the task with either the Virtual Arm or the Hybrid Arm, by moving their own arm (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>) or residual limb. Even if they were instructed not to move their trunk and to keep their back against the chair depending on the phase and protocol, participants were not physically restrained. The goal was to reach and grasp the bottle with the virtual hand, then bring it at another location indicated by a cylindrical platform. A <italic>trial</italic> refers to only one part of this process: either the bottle-picking or the bottle-placing. In either case, participants completed the trial by pressing the button while the virtual hand was inside a target zone, corresponding to a region in the five-dimensional space of hand locations (3D position × 2D orientation) centered on the target’s location and delimited by a spatial and angular tolerance. A hard constraint was defined with a spatial tolerance of 2cm and an angular tolerance of 5°, whereas a relaxed constraint was defined with a spatial tolerance of 4cm and an angular tolerance of 10°. A semi-transparent arrow was attached to the virtual hand to indicate the hand’s axis (arrow direction) and center (arrow base) to help participants bring the hand inside the target zone. Whenever the hand was inside the target zone, the bottle turned red as a sign that it could be either grasped or released. The virtual hand was limited to two states: either open and empty, or closed and holding the bottle. Participants could only toggle the hand’s state while the hand was in the target zone, by pressing the button to complete the trial. During a bottle-picking trial, the target corresponded to the bottle itself: the target’s center was placed at the middle of the bottle’s height and its axis was the bottle’s revolution axis. During a bottle-placing task, the target corresponded to the cylindrical platform on which the bottle needed to be placed: the target’s axis was perpendicular to the platform’s surface and its center was placed so that a correct hand positioning would bring the bottom of the bottle against the platform. This was made so that the instruction to “place the bottle on the platform” would remain intuitive.</p>
<p>Participants were given a short time (either 5 or 10s depending on the experimental phase) to complete each trial, and instructed to perform the task at a comfortable yet sustained pace. If the task was not completed within the allotted time, the current trial ended with a short audio cue and the hand’s state was automatically toggled. Each experiment involved four to five phases, within which trials were grouped in blocks of 50 trials (<italic>i.e.,</italic> twenty-five repetitions of the pick-and-place process) interspersed with short pauses (usually &lt; 1min, occasionally up to a few minutes if needed).</p>
</sec>
<sec id="s4g">
<title>Targets Sets Generation</title>
<p>Two sets of targets were generated for each participant: A set of Plausible Targets for the Initial Acquisition phase (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>), based on the range of motion of joint angles, and a set of Possible Targets for the Test phases, based on movements previously produced in the Initial Acquisition (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>). Each target was defined by 5 spatial parameters: 3 Cartesian coordinates of its center (in the shoulder referential of the participant), and 2 spherical coordinates describing its orientation relative to the vertical axis. Note that because both the bottle and platform are cylindrical, their rotation about their revolution axis is irrelevant to the task, such that a pair of spherical coordinates is sufficient to describe their orientation.</p>
</sec>
<sec id="s4h">
<title>Joint Angle Ranges of Motion</title>
<p>With the VR headset temporarily taken off, participants were asked to perform a few repetitions of an elementary movement for each arm DoF, travelling across its whole range of motion (<xref rid="figs1" ref-type="fig">Supplementary Fig. 1a</xref>). For each movement, the experimenter performed a demonstration that the participants were required to mimic with their own arm, and ranges of motion were estimated from extreme values reached with the Virtual Arm recorded. In addition, the range of motion of the elbow was artificially fixed at 85% of maximal extension in order to avoid postures in which the arm would be too straight. Indeed, the triangle used to compute humeral rotation (formed by the 3 joint centers, shoulder, elbow and wrist) would be too small, or even vanishing for a perfectly straight arm. For participants with transhumeral limb loss, only the first 2 elementary residual limb movements were performed and subject to range-of-motion extraction.</p>
</sec>
<sec id="s4i">
<title>Plausible Targets Set for Initial Acquisition Phase</title>
<p>Plausible targets were generated based on the estimated ranges of motion, as well as on restrictions applied on the workspace. Firstly, 7-DoF arm angular configurations were drawn at random within the ranges of motion following a multivariate uniform probability distribution. Then, forward kinematics was used to compute the target location that would be reached by the virtual hand for those postures.</p>
<p>The resulting target locations were then filtered according to three criteria:
<list list-type="bullet">
<list-item><p>The angle between the target’s axis and the vertical axis did not exceed 80°, excluding targets pointing downwards or horizontally.</p></list-item>
<list-item><p>The distance between the target’s center and the participant’s frontal plane exceeded a third of the participant’s arm length, ensuring that all targets were in front of the participants, and excluding targets too close to their trunk.</p></list-item>
<list-item><p>The distance between the target’s center and the horizontal plane passing through the participant’s shoulder did not exceed two thirds of the participant’s arm length, excluding targets too close to the legs.</p></list-item>
</list>
The remaining targets spanned a roughly hemispherical region centered on the shoulder (cf Plausible target set <xref rid="fig1" ref-type="fig">Fig. 1a</xref> and <xref rid="fig2" ref-type="fig">Fig. 2a</xref>). The random drawing went until 300 suitable targets were obtained, which were then shuffled (half of them treated as picking locations and the other half as placing locations) to form a sequence of alternating bottles and platforms.</p>
</sec>
<sec id="s4j">
<title>Possible Targets Set for Test Phases</title>
<p>A second targets set was generated to cover the participant’s reachable space more accurately. This was achieved by applying an unsupervised learning algorithm called Growing Neural Gas (GNG)<sup><xref ref-type="bibr" rid="c36">36</xref></sup> to arm angular configurations previously recorded in the Initial Acquisition phase. A GNG is a type of self-organizing map whose structure is based on a graph where each node is associated with a position in the feature space (in our case, the 7-dimensional space of joint angles describing an arm posture). It is trained through a growing process that fits the graph’s topological structure to the input data by incrementally moving existing nodes and adding new ones. This process allows the graph to “learn” the input data’s topology in terms of size and local density, and returns a set of nodes directly within the feature space by the end of the training (<italic>cf</italic> red dots <xref rid="fig1" ref-type="fig">Fig. 1d</xref>).</p>
<p>In the present case, the input data corresponded to the Virtual Arm’s postures performed by the participant during the Initial Acquisition phase, downsampled by a factor 10 for the growing process to remain time-efficient. In this way, the neural gas grew inside the region of the configuration space effectively explored when the participant moved their arm to complete the task. The training parameters were tuned to return 200 nodes, and the 7-DoF postures associated with these nodes were transformed into a set of 200 Possible targets using forward kinematics solving (cf Possible target set <xref rid="fig1" ref-type="fig">Fig. 1d</xref> and <xref rid="fig2" ref-type="fig">Fig. 2a</xref>). The generated targets were then ordered in a sequence by randomly drawing targets from the set in a way that prevented two consecutive targets from being too close (&lt; 20 cm) to each other.</p>
<p>Because participants with arm amputation did not perform the Initial Acquisition phase, the GNG was applied on data recorded from previous participants of Exp1, following a similar reasoning as behind the Generic ANN. The data from the Initial Acquisition phases of the 10 participants for Exp1 were then filtered as exposed previously for plausible targets (except that ranges of motion associated with residual limb motion were applied), and downsampled by a factor 100 to obtain an amount of input samples comparable to that used in Exp1 and 2 (where data from a single recording session was downsampled by a factor of 10). The growing process and targets generation from the resulting postures followed the same method as explain previously, except that mirror symmetry with respect to the medial plane was applied as appropriate to accommodate the amputated side and the valid arm side used for participants in Exp3.</p>
</sec>
</sec>
<sec id="s5">
<title>Experimental Phases</title>
<sec id="s5a">
<title>Familiarization</title>
<p>The first phase was designed to allow participants to familiarize themselves with the apparatus, virtual scene and experimental task. During this phase, intact-limbs participants drove the Virtual Arm to perform up to 3 blocks of 50 trials while target locations followed the first items of the Plausible Targets Set. Hard Constraints (2cm, 5°) were applied to the target zone to ensure maximal use of the range of motion. The time limit was set at 5s, with the experimenter being able to manually skip a trial upon request in the event of a participant having issues completing this trial. The Familiarization phase ended when the participant was able to reach most targets comfortably.</p>
<p>For participants with transhumeral limb loss, the 5s time limit was withdrawn so that they could freely explore the apparatus. Relaxed constraints (4cm, 10°) were applied to mimic the constraints applied during the following Test phases in Exp3. For the residual limb side, participants drove the Hybrid Arm using predictions from the Generic ANN to reach targets from the possible targets set. For the valid side, the Familiarization phase was done on the same targets after they were mirrored symmetric with respect to the medial plane, and the participant drove the Virtual Arm.</p>
</sec>
<sec id="s5b">
<title>Initial Acquisition</title>
<p>The aim of the Initial Acquisition phase was to record participants’ natural movements in order to train their Own ANN used in Exp1 and 2, and to train the Generic ANN used in Exp2 and 3. The participant controlled the Virtual Arm while the Plausible Targets Set was used to elicit 300 trials. As in the Familiarization phase, hard constraints (2cm, 5°) were applied with a 5s time limit. In order to promote arm movements only, participants were asked to keep their back against the chair and not to move their trunk.</p>
</sec>
<sec id="s5c">
<title>Test</title>
<p>The Test phases aimed at comparing performances achieved using the Hybrid Arm or the Virtual Arm. When using the Hybrid Arm, either the Own or the Generic ANN was interfaced so that at each time step, the ANN received 7 inputs and predicted 5 joint angles. As in the training data, 2 of these inputs were the proximal joint angles extracted from the actual shoulder’s motion and mimicked by the Hybrid Arm’s proximal DoFs. However, contextual information provided by the 5 remaining inputs was different from that of the training data. Indeed, instead of expressing the hand location, they expressed the target location (bottle or platform), thereby being congruent with the current goal of the task. The distal joint angles predicted by the ANN were then sent back to the simulation engine in order to update the Hybrid Arm’s posture. As in the Initial Acquisition phase, participants were required to perform the task by moving their own arm in order to bring the virtual hand on the target, with the help of the semi-transparent arrow. They were given no details regarding the operation of the Hybrid Arm, and instructed to complete the task by performing arm movements as natural as possible. For test phases of Exp2 and 3, the instruction « to keep their back against the chair and not to move their trunk » was somehow relaxed such that they were allowed to move their trunk only if deemed absolutely necessary to reach the target. One Test phase consisted of 200 trials corresponding to the Possible Targets Set, conducted with relaxed constraints (4cm, 10°) and a time limit extended to 10s.</p>
</sec>
<sec id="s5d">
<title>Protocols</title>
<p>Exp1 aimed at recording natural arm movements from multiple subjects in order to train the Generic ANN for Exp2 and 3, and to compare performances using either the Virtual Arm or the Own ANN. The push-button was placed under the participant’s left hand, and each participant performed a Familiarization phase and an Initial Acquisition phase, followed by two Test phases: one with the control of a Hybrid Arm based on the Own ANN predictions, and one with the Virtual Arm (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). During all those phases, participants were instructed not to move their trunk in order to perform only the arm movement needed to get the target.</p>
<p>Exp2 aimed at comparing performances with the Generic and Own ANNs, and to validate that a Generic ANN trained on right-handed participants could be used by left handed participants. The push-button was placed on the ground under the participant’s dominant foot, and each participant performed a Familiarization phase and an Initial Acquisition phase, followed by three Test phases: One with the Own ANN, one with the Generic ANN, and a final baseline Test phase using the Virtual Arm. The order of Test phases with the Own and Generic ANNs were counterbalanced among participants, with equal number of left-handed and right-handed participants in each group (cf <xref rid="fig3" ref-type="fig">Fig. 3a</xref>).</p>
<p>Exp3 aimed at evaluating the performance achieved by participants with transhumeral limb loss using control from the Generic ANN, and compare them to the performance with their valid arm. The push-button was placed under the participants’ dominant foot, and participants performed a Familiarization phase followed by two Test phases with the Generic ANN on their amputated side. Then, after proper calibration with their valid arm (i.e., contralateral to the amputation), participants performed a Familiarization phase followed by a baseline Test phase with the Hybrid Arm (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>).</p>
</sec>
<sec id="s5e">
<title>Data Reduction and Metrics</title>
<p>Data were first filtered to remove trials with substantial measurement errors associated with motion capture. Two filters were applied: one for « freezing » behavior and one for « jumping » behavior. The « freezing » filter removed trials where a sensor position (e.g. trunk, arm, fore-arm and hand sensors during baseline phases and only trunk and arm sensors for test phases) stayed still for at least 0.5s. The « jumping » filter removed trials with a shoulder position moving more than 0.01m between two samples (equivalent to a velocity of 0.9 m/s). Over all experiments, this process removed an average of 3.8 (± 7.6) % trials per participant and experimental phase.</p>
<p>Given the high success rate associated with all phases of all experiments (average of 99.22 ± 1.7 % trials validated per participant and experimental phase), analyses were conducted on the following metrics computed on trials validated (i.e., button pressed while being within the target zone) in all Test phases by a participant.</p>
<p>The Movement Time (MT) refers to the time taken to reach and validate a target. It is computed as the time between target appearance and the moment the target is validated using the push-button (see <xref rid="fig5" ref-type="fig">Fig. 5a</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 5</label>
<caption><title>Timing protocols of the Virtual Reality (a) and the Physical Proof of Concept (b) experiments.</title>
<p><bold>a</bold>. Upper part: Sequence of four hypothetical trials conducted in virtual reality. In each trial, the participant had to move the virtual hand to a target zone. When in the target zone, the cylindrical object turned red (as indicated by the red squares) and the trial was successful if the participant pressed the validation button while within the target zone (see trials 1, 2, and 4). A trial was failed if the participant did not validate the target within the allotted time (see Trial 3). In this case, a sound signaled the time out and the subsequent trial began. Success Rate was calculated for each experimental phase as the percentage of successful trials. Movement Time was computed for each successful trial as the time between the beginning of the trial and the target validation. Lower part: A phase was sliced into blocks of 50 trials. Between blocks, participants could rest during pauses. The Shoulder Volume was computed by pooling all the shoulder movements done during the successful trials of a phase. <bold>b</bold>. Sequence of two hypothetical trials conducted in the Physical Proof of Concept. Each trial required the participant to move their arm so that a robotic arm could reach a physical target (i.e., rectangular sponges). During the first 0.75s of a trial, the robotic arm’s distal joints reached the first ANN prediction. At this time, a “Go” signal indicated to the participant that they could start moving. A trial was successful if the participant grasped the target with the robotic gripper and removed it from the wooden sticks (see Trial 1, Target acquired). If the participant was not able to grasp the target within the allotted time, the trial was failed (see Trial 2). Success Rate was calculated for each phase (i.e., a sequence of 5 targets x 3 blocks done with the same control type) as the percentage of successful trials. During a trial, the participant was allowed to open and close the gripper as many times as necessary (see black arrowheads). The Movement Time was calculated for each successful trial as the time between the “Go” signal and the last closure of the gripper. At the end of each trial, the participant was instructed to place their arm alongside their body, while the robotic arm returned to a neutral posture alongside the robotic platform.</p></caption>
<graphic xlink:href="22281053v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The shoulder position Spread Volume (SV) is obtained by computing the ellipsoid containing 90% of shoulder position during a phase (see<sup><xref ref-type="bibr" rid="c35">35</xref></sup> for more details and <xref rid="fig5" ref-type="fig">Fig. 5a</xref> for an illustration of the period over which this was computed). Because a certain amount of shoulder movement naturally occurs during reaching, the SV at baseline should be viewed as a benchmark over which compensatory movements probably occurs.</p>
</sec>
<sec id="s5f">
<title>Physical Proof of Concept (POC)</title>
<p>The feasibility of our approach in the physical world was established through a Proof of Concept (POC) conducted using a humanoid robotic platform with human-like arm dimension and Degrees of Freedom<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. The arms of the platform are linked to a fixed robotic trunk with no shoulder translational DoF. Thus, the robotic arm is not worn by the participant, allowing the inclusion of intact-limbs individuals, and preventing the use of compensatory movements from the trunk and shoulder to perform the task. During the experiment, participants stood still, setback to the robotic platform, to avoid visual occlusion of the working space (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). Only the right arm of the robotic platform was used, and the participants could trigger the opening and the closing of the robotic gripper using a push button placed under their foot. The same virtual reality setup as in Exp1 to 3 was used for the POC (see Apparatus) to link the participant’s arm movements to those of the virtual arm. After calibrating the virtual arm (see Virtual Arm Calibration), the headset was removed. Targets were rectangular sponges fixed with wooden sticks on a 45cm X 40cm bench (length X height) and a trial was defined as an attempt to reach and grasp a target with the robotic arm. Participants were instructed to place their arm along their body at the beginning of each trial and waited for a sound signal (i.e., beginning of the trial) to reach and grasp the target. As in virtual reality, Familiarization and Test phases were conducted to familiarize the participant with the task and to compare controls applied in different Test phases. The phases were divided into blocks of 5 targets for practical reason (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>), and numbers written on the sponges indicated the order with which participants had to reach and grasp them within each block of target configurations. The position and orientation of each sponge were set at the beginning of each block using a supplementary sensor. Targets could be vertical or tilted at 45 and -45° on the frontal plane, and varied in depth by about 10 cm. Block 1 was used to familiarize the participants with the control(s) used (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>). In POCa, b and c, blocks 1 to 3 were then performed with each control tested, and a Test phase was constituted by pulling trials from all blocks performed with a given control. In POCa-b, participant performed each block of target configuration with both controls before going to the next block of target configuration. Half participants began with the control based on natural arm movements (TestNat) while the other half began with the control based on the Generic ANN (TestGeneric). Participants of POCd only performed the first block for Familiarization and Test phases. The joint configuration of the Virtual Arm (see Hybrid Arm Control) or that of the control based on the Generic ANN (see Own and Generic Arm ANN) were applied to the robotic arm depending on the control tested. The data used to train the Generic ANN were remapped according to the dimensions of the robotic arm. To prevent sharp acceleration at target change when using the control based on the Generic ANN, 0.75s was allotted before the beginning of each trial (signaled by a sound) for the robotic arm to smoothly reach the first prediction from the ANN. As in Exp1 to 3, only trials validated (i.e., trials with a sponge grasped with the gripper without falling) in all Test phases by a participant were considered for further analysis. Movement time (MT) was defined here as the time spent between the beginning of the trial and the last close of the gripper and was computed for each validated target (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). Since shoulder translations had no impact on the robotic arm movements, and since the participants’ position in the room was not restricted, the shoulder position Spread Volume (SV) was not computed.</p>
</sec>
<sec id="s5g">
<title>Statistical Analysis</title>
<p>MT was grouped by participant and test phase, and median values over trials were extracted for each of these groups. By design, the SV already gave a single value per participant and test phase. In this way, we obtained samples of one value per participant for each combination of metric and test phase. For Exp1 and POCa-b, two test phases (with the Virtual Arm and the Own ANN, or with the Virtual Arm and the Generic ANN applied to the robotic arm, respectively) were compared. After testing for normality using the Shapiro test, either a paired T-test or a Wilcoxon test was conducted. For Exp2 and 3, three test phases were compared, involving either the 7-DoF Virtual Arm, the Own ANN, or the Generic ANN. Thus, after testing for normality using the Shapiro Test and for homogeneity of variances between modes using the Maulchly’s Test, either a repeated measures ANOVA or a Friedman test was conducted. If a significant difference was found at this level, post-hoc analyses were conducted using either Tukey tests or Conover tests, respectively. The high success rates observed led to equality between several participants (e.g., at 100% success), which prevented the use of statistical tests based either on normality assumption or on ranking procedure. Thus, statistical differences reported here were assessed by comparing the differences in the achievement of each target along all the phases of each experiment. For Exp1 and POCa-b, a McNemar test for paired samples was conducted to find statistical differences between the two phases. For Exp2 and Exp3, a Cochran’s Q test for paired samples was first performed followed by a post-hoc McNemar test if needed. Data processing and statistical analysis were carried out with the R software environment, with a significance threshold set at α = 0.05 with a Bonferroni correction applied if needed. Due to the insufficient number of participants, no statistical analysis was conducted for POCc and d.</p>
</sec>
</sec>
<sec id="s6">
<title>Data and code availability</title>
<p>Raw data recorded during the three experiments, and code required for data treatment and ANNs training, are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7187850">https://doi.org/10.5281/zenodo.7187850</ext-link>. Further information and requests should be addressed to the corresponding author Aymar de Rugy (aymar.derugy@u-bordeaux.fr).</p>
</sec>
</body>
<back>
<sec id="d1e1315" sec-type="data-availability">
<title>Data Availability</title>
<p>All data recorded during the three experiments are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7187850">https://doi.org/10.5281/zenodo.7187850</ext-link></p>
<p>
<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7187850">https://doi.org/10.5281/zenodo.7187850</ext-link>
</p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>The authors would like to thank Émilie Doat and Léa Haefflinger for their help during the experiments, Gerald E Loeb for interactions on an earlier version of the control proposed here and feedback on this manuscript, and all participants who took part in this study. This work was supported by the CNRS interdisciplinary project RoBioVis, and the ANR-DGA-ASTRID grant CoBioPro (ANR-20-ASTR-0012-1).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Kuiken</surname>, <given-names>T. A.</given-names></string-name> <etal>et al.</etal> <article-title>Targeted Muscle Reinnervation for Real-Time Myoelectric Control of Multifunction Artificial Arms</article-title>. <source>JAMA J. Am. Med. Assoc</source>. <volume>301</volume>, <fpage>619</fpage>–<lpage>628</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Jönsson</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Caine-Winterberger</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Brånemark</surname>, <given-names>R</given-names></string-name>. <article-title>Osseointegration amputation prostheses on the upper limbs: methods, prosthetics and rehabilitation</article-title>. <source>Prosthet. Orthot. Int</source>. <volume>35</volume>, <fpage>190</fpage>–<lpage>200</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Ortiz-Catalan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mastinu</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Sassu</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Aszmann</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>Brånemark</surname>, <given-names>R</given-names></string-name>. <article-title>Self-Contained Neuromusculoskeletal Arm Prostheses</article-title>. <source>N. Engl. J. Med</source>. <volume>382</volume>, <fpage>1732</fpage>–<lpage>1738</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Ortiz-Catalan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Håkansson</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Brånemark</surname>, <given-names>R</given-names></string-name>. <article-title>An osseointegrated human-machine gateway for long-term sensory feedback and motor control of artificial limbs</article-title>. <source>Sci. Transl. Med</source>. <volume>6</volume>, <fpage>257r</fpage>e6 (<year>2014</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>D’Anna</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal> <article-title>A closed-loop hand prosthesis with simultaneous intraneural tactile and position feedback</article-title>. <source>Sci. Robot</source>. <volume>4</volume>, <fpage>eaau8892</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Zollo</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal> <article-title>Restoring tactile sensations via neural interfaces for real-time force-and-slippage closed-loop control of bionic hands</article-title>. <source>Sci. Robot</source>. <volume>4</volume>, <fpage>eaau9924</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Salminger</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Long-term implant of intramuscular sensors and nerve transfers for wireless control of robotic arms in above-elbow amputees</article-title>. <source>Sci. Robot</source>. <volume>4</volume>, <fpage>eaaw6306</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Jezernik</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Grill</surname>, <given-names>W. M.</given-names></string-name> &amp; <string-name><surname>Sinkjaer</surname>, <given-names>T</given-names></string-name>. <article-title>Neural network classification of nerve activity recorded in a mixed nerve</article-title>. <source>Neurol. Res</source>. <volume>23</volume>, <fpage>429</fpage>–<lpage>434</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Farina</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal> <article-title>Toward higher-performance bionic limbs for wider clinical use</article-title>. <source>Nat. Biomed. Eng</source>. <fpage>1</fpage>–<lpage>13</lpage> (<year>2021</year>) doi:<pub-id pub-id-type="doi">10.1038/s41551-021-00732-x</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Cracchiolo</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Decoding of grasping tasks from intraneural recordings in trans-radial amputee</article-title>. <source>J. Neural Eng</source>. <volume>17</volume>, <issue>026034</issue> (<year>2020</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>de Rugy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Loeb</surname>, <given-names>G. E.</given-names></string-name> &amp; <string-name><surname>Carroll</surname>, <given-names>T. J</given-names></string-name>. <article-title>Virtual biomechanics: a new method for online reconstruction of force from EMG recordings</article-title>. <source>J. Neurophysiol</source>. <volume>108</volume>, <fpage>3333</fpage>–<lpage>3341</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Jiang</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Dosen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Muller</surname>, <given-names>K.-R.</given-names></string-name> &amp; <string-name><surname>Farina</surname>, <given-names>D</given-names></string-name>. <article-title>Myoelectric Control of Artificial Limbs—Is There a Need to Change Focus? [In the Spotlight]</article-title>. <source>IEEE Signal Process. Mag</source>. <volume>29</volume>, <fpage>152</fpage>–<lpage>150</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Hahne</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Dähne</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hwang</surname>, <given-names>H.-J.</given-names></string-name>, <string-name><surname>Müller</surname>, <given-names>K.-R.</given-names></string-name> &amp; <string-name><surname>Parra</surname>, <given-names>L. C</given-names></string-name>. <article-title>Concurrent Adaptation of Human and Machine Improves Simultaneous and Proportional Myoelectric Control</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng</source>. <volume>23</volume>, <fpage>618</fpage>–<lpage>627</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Ameri</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kamavuako</surname>, <given-names>E. N.</given-names></string-name>, <string-name><surname>Scheme</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Englehart</surname>, <given-names>K. B.</given-names></string-name> &amp; <string-name><surname>Parker</surname>, <given-names>P. A</given-names></string-name>. <article-title>Support Vector Regression for Improved Real-Time, Simultaneous Myoelectric Control</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng</source>. <volume>22</volume>, <fpage>1198</fpage>–<lpage>1209</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Ortiz-Catalan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Håkansson</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Brånemark</surname>, <given-names>R</given-names></string-name>. <article-title>Real-time and simultaneous control of artificial limbs based on pattern recognition algorithms</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng. Publ. IEEE Eng. Med. Biol. Soc</source>. <volume>22</volume>, <fpage>756</fpage>–<lpage>764</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>L. H.</given-names></string-name>, <string-name><surname>Kuiken</surname>, <given-names>T. A.</given-names></string-name> &amp; <string-name><surname>Hargrove</surname>, <given-names>L. J</given-names></string-name>. <article-title>Myoelectric control system and task-specific characteristics affect voluntary use of simultaneous control</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng. Publ. IEEE Eng. Med. Biol. Soc</source>. <volume>24</volume>, <fpage>109</fpage>–<lpage>116</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Nowak</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Vujaklija</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Sturma</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Castellini</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Farina</surname>, <given-names>D</given-names></string-name>. <article-title>Simultaneous and Proportional Real-Time Myocontrol of up to Three Degrees of Freedom of the Wrist and Hand</article-title>. <source>IEEE Trans. Biomed. Eng</source>. <fpage>1</fpage>–<lpage>12</lpage> (<year>2022</year>) doi:<pub-id pub-id-type="doi">10.1109/TBME.2022.3194104</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Hahne</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Schweisfurth</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Koppe</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Farina</surname>, <given-names>D</given-names></string-name>. <article-title>Simultaneous control of multiple functions of bionic hand prostheses: Performance and robustness in end users</article-title>. <source>Sci. Robot</source>. <volume>3</volume>, <fpage>eaat3630</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Kuiken</surname>, <given-names>T. A.</given-names></string-name> <etal>et al.</etal> <article-title>Targeted reinnervation for enhanced prosthetic arm function in a woman with a proximal amputation: a case study</article-title>. <source>Lancet Lond. Engl</source>. <volume>369</volume>, <fpage>371</fpage>–<lpage>380</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Hargrove</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Kuiken</surname>, <given-names>T. A</given-names></string-name>. <article-title>Myoelectric Pattern Recognition Outperforms Direct Control for Transhumeral Amputees with Targeted Muscle Reinnervation: A Randomized Clinical Trial</article-title>. <source>Sci. Rep</source>. <volume>7</volume>, <issue>13840</issue> (<year>2017</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Montagnani</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Controzzi</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Cipriani</surname>, <given-names>C</given-names></string-name>. <article-title>Is it Finger or Wrist Dexterity That is Missing in Current Hand Prostheses?</article-title> <source>IEEE Trans. Neural Syst. Rehabil. Eng. Publ. IEEE Eng. Med. Biol. Soc</source>. <volume>23</volume>, <fpage>600</fpage>–<lpage>609</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Kanitz</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Montagnani</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Controzzi</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Cipriani</surname>, <given-names>C</given-names></string-name>. <article-title>Compliant Prosthetic Wrists Entail More Natural Use Than Stiff Wrists During Reaching, Not (Necessarily) During Manipulation</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng</source>. <volume>26</volume>, <fpage>1407</fpage>–<lpage>1413</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Soechting</surname>, <given-names>J. F.</given-names></string-name> &amp; <string-name><surname>Lacquaniti</surname>, <given-names>F</given-names></string-name>. <article-title>Invariant characteristics of a pointing movement in man</article-title>. <source>J. Neurosci. Off. J. Soc. Neurosci</source>. <volume>1</volume>, <fpage>710</fpage>–<lpage>720</lpage> (<year>1981</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Desmurget</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Postural and synergic control for three-dimensional movements of reaching and grasping</article-title>. <source>J. Neurophysiol</source>. <volume>74</volume>, <fpage>905</fpage>–<lpage>910</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Popovic</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Popovic</surname>, <given-names>D</given-names></string-name>. <article-title>Cloning biological synergies improves control of elbow neuroprosthesis</article-title>. <source>IEEE Eng. Med. Biol. Mag. Q. Mag. Eng. Med. Biol. Soc</source>. <volume>20</volume>, <fpage>74</fpage>–<lpage>81</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Kaliki</surname>, <given-names>R. R.</given-names></string-name>, <string-name><surname>Davoodi</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Loeb</surname>, <given-names>G. E</given-names></string-name>. <article-title>Evaluation of a noninvasive command scheme for upper-limb prostheses in a virtual reality reach and grasp task</article-title>. <source>IEEE Trans. Biomed. Eng</source>. <volume>60</volume>, <fpage>792</fpage>–<lpage>802</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Merad</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Assessment of an automatic prosthetic elbow control strategy using residual limb motion for transhumeral amputated individuals with socket or osseointegrated prostheses</article-title>. <source>IEEE Trans. Med. Robot. Bionics</source> <fpage>1</fpage>–<lpage>1</lpage> (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.1109/TMRB.2020.2970065</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Merad</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Can We Achieve Intuitive Prosthetic Elbow Control Based on Healthy Upper Limb Motor Strategies?</article-title> <source>Front. Neurorobotics</source> <volume>12</volume>, (<year>2018</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Montagnani</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Controzzi</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Cipriani</surname>, <given-names>C</given-names></string-name>. <article-title>Exploiting arm posture synergies in activities of daily living to control the wrist rotation in upper limb prostheses: A feasibility study</article-title>. <source>Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. IEEE Eng. Med. Biol. Soc. Annu. Int. Conf</source>. 2015, <fpage>2462</fpage>–<lpage>2465</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title>Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images. in <italic>Computer Vision – ECCV</italic> 2022 (eds. Avidan, S., Brostow, G., Cissé, M., Farinella, G. M. &amp; Hassner, T</article-title>.) <fpage>298</fpage>–<lpage>315</lpage> (<source>Springer Nature Switzerland</source>, <year>2022</year>). doi:<pub-id pub-id-type="doi">10.1007/978-3-031-19824-3_18</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Nguyen</surname>, <given-names>V. N.</given-names></string-name>, <string-name><surname>Du</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Xiao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Ramamonjisoa</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Lepetit</surname>, <given-names>V</given-names></string-name>. <source>PIZZA: A Powerful Image-only Zero-Shot Zero-CAD Approach to 6 DoF Tracking</source>. Preprint at <pub-id pub-id-type="doi">10.48550/arXiv.2209.07589</pub-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Markovic</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dosen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Cipriani</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Popovic</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Farina</surname>, <given-names>D</given-names></string-name>. <article-title>Stereovision and augmented reality for closed-loop control of grasping in hand prostheses</article-title>. <source>J. Neural Eng</source>. <volume>11</volume>, <issue>046001</issue> (<year>2014</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><given-names>Pérez</given-names> <surname>de San Roman</surname></string-name>, P., <etal>et al.</etal> <article-title>Saliency Driven Object recognition in egocentric videos with deep CNN: toward application in assistance to Neuroprostheses</article-title>. <source>Comput. Vis. Image Underst</source>. <volume>164</volume>, <fpage>82</fpage>–<lpage>91</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>González-Díaz</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Benois-Pineau</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Domenger</surname>, <given-names>J.-P.</given-names></string-name>, <string-name><surname>Cattaert</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>de Rugy</surname>, <given-names>A</given-names></string-name>. <article-title>Perceptually-guided deep neural networks for ego-action prediction: Object grasping</article-title>. <source>Pattern Recognit</source>. <volume>88</volume>, <fpage>223</fpage>–<lpage>235</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Mick</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Shoulder kinematics plus contextual target information enable control of multiple distal joints of a simulated prosthetic arm and hand</article-title>. <source>J. NeuroEngineering Rehabil</source>. <volume>18</volume>, <issue>3</issue> (<year>2021</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Fritzke</surname>, <given-names>B.</given-names></string-name> <source>A Growing Neural Gas Network Learns Topologies. in Advances in Neural Information Processing Systems</source> <volume>vol. 7</volume> (MIT Press, <year>1994</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Mick</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Reachy, a 3D-Printed Human-Like Robotic Arm as a Testbed for Human-Robot Control Strategies</article-title>. <source>Front. Neurorobotics</source> <volume>13</volume>, (<year>2019</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Hinton</surname>, <given-names>G</given-names></string-name>. <source>Deep learning. Nature</source> <volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Starke</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Weiner</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Crell</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Asfour</surname>, <given-names>T</given-names></string-name>. <article-title>Semi-autonomous control of prosthetic hands based on multimodal sensing, human grasp demonstration and user intention</article-title>. <source>Robot. Auton. Syst</source>. <volume>154</volume>, <issue>104123</issue> (<year>2022</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Markovic</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dosen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Popovic</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Graimann</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Farina</surname>, <given-names>D</given-names></string-name>. <article-title>Sensor fusion and computer vision for context-aware control of a multi degree-of-freedom prosthesis</article-title>. <source>J. Neural Eng</source>. <volume>12</volume>, <issue>066022</issue> (<year>2015</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Krausz</surname>, <given-names>N. E.</given-names></string-name> <etal>et al.</etal> <article-title>Intent Prediction Based on Biomechanical Coordination of EMG and Vision-Filtered Gaze for End-Point Control of an Arm Prosthesis</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng. Publ. IEEE Eng. Med. Biol. Soc</source>. <volume>28</volume>, <fpage>1471</fpage>–<lpage>1480</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Ghazaei</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Alameer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Degenaar</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Morgan</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Nazarpour</surname>, <given-names>K</given-names></string-name>. <article-title>Deep learning-based artificial vision for grasp classification in myoelectric hands</article-title>. <source>J. Neural Eng</source>. <volume>14</volume>, <issue>036025</issue> (<year>2017</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Mouchoux</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>Artificial Perception and Semiautonomous Control in Myoelectric Hand Prostheses Increases Performance and Decreases Effort</article-title>. <source>IEEE Trans. Robot</source>. <volume>37</volume>, <fpage>1298</fpage>–<lpage>1312</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>He</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Kubozono</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fukuda</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Yamaguchi</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Okumura</surname>, <given-names>H</given-names></string-name>. <article-title>Vision-Based Assistance for Myoelectric Hand Control</article-title>. <source>IEEE Access</source> <volume>8</volume>, <fpage>201956</fpage>–<lpage>201965</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Resnik</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Klinger</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Etter</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Fantini</surname>, <given-names>C</given-names></string-name>. <article-title>Controlling a multi-degree of freedom upper limb prosthesis using foot controls: user experience</article-title>. <source>Disabil. Rehabil. Assist. Technol</source>. <volume>9</volume>, <fpage>318</fpage>–<lpage>329</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Legrand</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Simultaneous Control of 2DOF Upper-Limb Prosthesis With Body Compensations-Based Control: A Multiple Cases Study</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng</source>. <volume>30</volume>, <fpage>1745</fpage>–<lpage>1754</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Bajaj</surname>, <given-names>N. M.</given-names></string-name>, <string-name><surname>Spiers</surname>, <given-names>A. J.</given-names></string-name> &amp; <string-name><surname>Dollar</surname>, <given-names>A. M</given-names></string-name>. <article-title>State of the Art in Artificial Wrists: A Review of Prosthetic and Robotic Wrist Design</article-title>. <source>IEEE Trans. Robot</source>. <volume>35</volume>, <fpage>261</fpage>–<lpage>277</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Resnik</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Klinger</surname>, <given-names>S. L.</given-names></string-name> &amp; <string-name><surname>Etter</surname>, <given-names>K</given-names></string-name>. <article-title>The DEKA Arm: its features, functionality, and evolution during the Veterans Affairs Study to optimize the DEKA Arm</article-title>. <source>Prosthet. Orthot. Int</source>. <volume>38</volume>, <fpage>492</fpage>–<lpage>504</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Lenzi</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lipsey</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Sensinger</surname>, <given-names>J. W</given-names></string-name>. <article-title>The RIC Arm—A Small Anthropomorphic Transhumeral Prosthesis</article-title>. <source>IEEEASME Trans. Mechatron</source>. <volume>21</volume>, <fpage>2660</fpage>–<lpage>2671</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Fan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Wei</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Ren</surname>, <given-names>L</given-names></string-name>. <article-title>Prosthetic and robotic wrists comparing with the intelligently evolved human wrist: A review</article-title>. <source>Robotica</source> <fpage>1</fpage>–<lpage>23</lpage> (<year>2022</year>) doi:<pub-id pub-id-type="doi">10.1017/S0263574722000856</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Marasco</surname>, <given-names>P. D.</given-names></string-name> <etal>et al.</etal> <article-title>Neurorobotic fusion of prosthetic touch, kinesthesia, and movement in bionic upper limbs promotes intrinsic brain behaviors</article-title>. <source>Sci. Robot</source>. <volume>6</volume>, <fpage>eabf3368</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Marasco</surname>, <given-names>P. D.</given-names></string-name> <etal>et al.</etal> <article-title>Illusory movement perception improves motor control for prosthetic hands</article-title>. <source>Sci. Transl. Med</source>. <volume>10</volume>, <fpage>eaao6990</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><given-names>DI</given-names> <surname>Pino</surname></string-name>, <string-name><given-names>G.</given-names>, <surname>Piombino</surname></string-name>, <string-name><given-names>V.</given-names>, <surname>Carassiti</surname></string-name>, <string-name><given-names>M.</given-names> <surname>&amp; Ortiz-Catalan</surname></string-name>, M. <article-title>Neurophysiological models of phantom limb pain: what can be learnt</article-title>. <source>Minerva Anestesiol</source>. <volume>87</volume>, <fpage>481</fpage>–<lpage>487</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Makin</surname>, <given-names>T. R.</given-names></string-name> &amp; <string-name><surname>Flor</surname>, <given-names>H</given-names></string-name>. <article-title>Brain (re)organisation following amputation: Implications for phantom limb pain</article-title>. <source>NeuroImage</source> <volume>218</volume>, <issue>116943</issue> (<year>2020</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Chan</surname>, <given-names>B. L.</given-names></string-name> <etal>et al.</etal> <article-title>Mirror therapy for phantom limb pain</article-title>. <source>N. Engl. J. Med</source>. <volume>357</volume>, <fpage>2206</fpage>–<lpage>2207</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Foell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bekrater-Bodmann</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Diers</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Flor</surname>, <given-names>H</given-names></string-name>. <article-title>Mirror therapy for phantom limb pain: Brain changes and the role of body representation</article-title>. <source>Eur. J. Pain</source> <volume>18</volume>, <fpage>729</fpage>–<lpage>739</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Thøgersen</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Individualized Augmented Reality Training Reduces Phantom Pain and Cortical Reorganization in Amputees: A Proof of Concept Study</article-title>. <source>J. Pain</source> <volume>21</volume>, <fpage>1257</fpage>– <lpage>1269</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><surname>Oldfield</surname>, <given-names>R. C</given-names></string-name>. <article-title>The assessment and analysis of handedness: The Edinburgh inventory</article-title>. <source>Neuropsychologia</source> <volume>9</volume>, <fpage>97</fpage>–<lpage>113</lpage> (<year>1971</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>O’Brien</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Bodenheimer</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Brostow</surname>, <given-names>G. J.</given-names></string-name> &amp; <string-name><surname>Hodgins</surname>, <given-names>J. K.</given-names></string-name> <source>Automatic Joint Parameter Estimation from Magnetic Motion Capture Data</source>. <ext-link ext-link-type="uri" xlink:href="https://smartech.gatech.edu/handle/1853/3408">https://smartech.gatech.edu/handle/1853/3408</ext-link> (<year>1999</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><string-name><surname>Dufour</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Pillu</surname>, <given-names>M</given-names></string-name>. <article-title>Biomécanique fonctionelle Membres-Tête-Tronc</article-title>. (<source>Elsevier / Masson</source>, <year>2017</year>).</mixed-citation></ref>
</ref-list>
<sec id="s7">
<title>Supplementary Information</title>
<p><bold>SupplementaryVideo1</bold>: <ext-link ext-link-type="uri" xlink:href="https://youtu.be/XPIlkrjwTtc">https://youtu.be/XPIlkrjwTtc</ext-link></p>
<p>A representative intact-limbs participant performing the pick and place task.</p>
<p><bold>SupplementaryVideo2</bold>: <ext-link ext-link-type="uri" xlink:href="https://youtu.be/Utoa9aYWRK0">https://youtu.be/Utoa9aYWRK0</ext-link></p>
<p>Participant 1 with transhumeral limb loss performing the pick and place task with residual limb movement-based control. For anonymization purpose the number reported is not the one of the <xref rid="tbl1" ref-type="table">Table 1</xref> or the Supplementary Note 1.</p>
<p><bold>SupplementaryVideo3</bold>: <ext-link ext-link-type="uri" xlink:href="https://youtu.be/RpZwwJ9-bEg">https://youtu.be/RpZwwJ9-bEg</ext-link></p>
<p>Participant 2 with transhumeral limb loss performing the pick and place task with residual limb movement-based control. For anonymization purpose the number reported is not the one of the <xref rid="tbl1" ref-type="table">Table 1</xref> or the Supplementary Note 1.</p>
<p><bold>SupplementaryVideo4</bold>: <ext-link ext-link-type="uri" xlink:href="https://youtu.be/T2NR02exeR0">https://youtu.be/T2NR02exeR0</ext-link></p>
<p>Participant 3 with transhumeral limb loss performing the pick and place task with residual limb movement-based control. For anonymization purpose the number reported is not the one of the <xref rid="tbl1" ref-type="table">Table 1</xref> or the Supplementary Note 1.</p>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Fig. 1.</label>
<caption><p><bold>a</bold>. ANNs Inputs and Outputs, displayed together with movements done to get the joints range of motion (black arrows). ANNs Inputs include shoulder flexion-extension (θ<sub>S-FE</sub>), shoulder abduction-adduction (θ<sub>S-AA</sub>), target position in relation to the shoulder (P<sub>T-X</sub>, P<sub>T-Y</sub> and P<sub>T-Z</sub>), and target orientation (as angles of rotation) with respect to the frontal and sagittal plane (θ<sub>T-F</sub> and θ<sub>T-S</sub>). ANNs outputs include all distal angles from the humeral rotation included: humeral rotation (θ<sub>H-R</sub>), elbow flexion-extension (θ<sub>E-FE</sub>), forearm pronation-supination (θ<sub>F-PS</sub>), wrist flexion-extension (θ<sub>W-FE</sub>) and radial-ulnar deviation (θ<sub>W-RU</sub>). <bold>b</bold>. Remapping target position for different arm morphologies. Two arms with the same angular configuration but different segments length lead to different positions of the target (orange arrows). Forward kinematics was used to remap target position for a subject with a different arm segments’ length (remapping for a shorter arm displayed). Note that the target orientation remains unaffected.</p></caption>
<graphic xlink:href="22281053v3_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Fig. 2.</label>
<caption><p>Distributions of movement times of each participant (and data from all participants regrouped in the last subplot) for the two experimental conditions (TestNat and TestOwn) of Exp1. Please note that the median of each individual distribution indicated by a vertical bar corresponds to the median movement time of each individual displayed as a circle <xref rid="fig3" ref-type="fig">Fig. 3c</xref>.</p></caption>
<graphic xlink:href="22281053v3_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Fig. 3.</label>
<caption><p>Distributions of movement times of each participant (and data from all participants regrouped in the last subplot) for the three experimental conditions (TestNat, TestOwn and TestGen) of Exp2. Please note that the median of each individual distribution indicated by a vertical bar corresponds to the median movement time of each individual displayed as a circle <xref rid="fig3" ref-type="fig">Fig. 3c</xref>.</p></caption>
<graphic xlink:href="22281053v3_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Fig. 4.</label>
<caption><p>Distributions of movement times of each participant (and data from all participants regrouped in the last subplot) for the three test phases (TestNat, TestGen1 and TestGen2) of Exp3. Please note that the median of each individual distribution indicated by a vertical bar corresponds to the median movement time of each individual displayed as a circle <xref rid="fig3" ref-type="fig">Fig. 3c</xref>.</p></caption>
<graphic xlink:href="22281053v3_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><bold>Supplementary Note 1: Feedback from amputated participants of Exp3 about the movement-based prosthesis control.</bold> The following are oral reports formulated either spontaneously or during informal talks with the experimenter while the experiment was in pause or finished. English translations are provided followed by the original French sentence in brackets. For anonymization purpose the numbers reported are not the ones of the <xref rid="tbl1" ref-type="table">Table 1</xref> or the Supplementary Videos 2 to 4, and the number of years since amputation has been replaced by <italic>X</italic> in the oral report of the Participant 7 with transhumeral limb loss.</p>
<sec id="s7a">
<title>Participant 1 with transhumeral limb loss</title>
<p>Participant: “A prosthesis that would be controlled like that? I take it right away.”</p>
<p><italic>(Participant : « Une prothèse qui se dirigerait comme çaJe prends de suite. »)</italic></p>
<p>Experimenter: “Do you feel as it’s a natural movement?” Participant: “Yes.”</p>
<p><italic>(Expérimentateur : « Vous avez l’impression que c’est le mouvement naturel» Participant : « Oui. »)</italic></p>
</sec>
<sec id="s7b">
<title>Participant 2 with transhumeral limb loss</title>
<p>Participant: “At the end, it is quite intuitive.”</p>
<p><italic>(</italic>Participant : <italic>« Finalement, c’est assez intuitif. »)</italic></p>
</sec>
<sec id="s7c">
<title>Participant 3 with transhumeral limb loss</title>
<p>The participant reported that he would use a prosthesis behaving like this.</p>
<p><italic>(Le participant rapporte qu’il prendrait une prothèse qui se comporterait comme cela.)</italic></p>
</sec>
<sec id="s7d">
<title>Participant 4 with transhumeral limb loss</title>
<p>Participant: “Doing a real arm movement, this is enjoyable.”</p>
<p><italic>(</italic>Participant : <italic>« Faire un vrai mouvement du bras, c’est agréable. »)</italic></p>
<p>The participant reported that he found the arm a bit too stiff from time to time.</p>
<p><italic>(Le participant a rapporté qu’il trouvait le bras un peu trop raide à certain moment.)</italic></p>
</sec>
<sec id="s7e">
<title>Participant 5 with transhumeral limb loss</title>
<p>Participant: “It’s intuitive, it’s easy.”</p>
<p><italic>(</italic>Participant : <italic>« C’est intuitif, c’est facile. »)</italic></p>
</sec>
<sec id="s7f">
<title>Participant 6 with transhumeral limb loss</title>
<p>Participant: “The movement doesn’t feel like a natural movement to me, it’s when I’m on target that the wrist is well placed.”</p>
<p><italic>(</italic>Participant : <italic>« Le mouvement ne me fait pas penser à un mouvement naturel, c’est quand je suis sur la cible que le poignet est bien placé. »)</italic></p>
</sec>
<sec id="s7g">
<title>Participant 7 with transhumeral limb loss</title>
<p>Participant: “It’s surprising.”</p>
<p><italic>(</italic>Participant : <italic>« C’est étonnant. »)</italic></p>
<p>Participant: “<italic>X</italic> years since I was able to do that, this is moving.”</p>
<p><italic>(Participant : « Il y a X ans que je ne peux plus faire ça, c’est émouvant. »)</italic></p>
</sec>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87317.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Nazarpour</surname>
<given-names>Kianoush</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Edinburgh</institution>
</institution-wrap>
<city>Edinburgh</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This paper is <bold>important</bold> because of its integration of movement and contextual information to control a virtual arm for individuals with upper-limb differences. The provided evidence <bold>convincingly</bold> demonstrates the approach's feasibility for manipulating a single object shape in different orientations within a virtual environment. However, additional improvements are needed for this proof-of-concept neuro-model to fulfil practical requirements.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87317.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Segas et al. present a novel solution to an upper-limb control problem which is often neglected by academia. The problem the authors are trying to solve is how to control the multiple degrees of freedom of the lower arm to enable grasp in people with transhumeral limb loss. The proposed solution is a neural network based approach which uses information from the position of the arm along with contextual information which defines the position and orientation of the target in space. Experimental work is presented, based on virtual simulations and a telerobotic proof of concept.</p>
<p>The strength of this paper is that it proposes a method of control for people with transhumeral limb loss which does not rely upon additional surgical intervention to enable grasping objects in the local environment. A challenge the work faces is that it can be argued that a great many problems in upper limb prosthesis control can be solved given precise knowledge of the object to be grasped, its relative position in 3D space and its orientation. It is difficult to know how directly results obtained in a virtual environment will translate to real world impact. Some of the comparisons made in the paper are to physical systems which attempt to solve the same problem. It is important to note that real world prosthesis control introduces numerous challenges which do not exist in virtual spaces or in teleoperation robotics.</p>
<p>The authors claim that the movement times obtained using their virtual system, and a teleoperation proof of concept demonstration, are comparable to natural movement times. The speed of movements obtained and presented are easier to understand by viewing the supplementary materials prior to reading the paper. The position of the upper arm and a given target are used as input to a classifier, which determines the positions of the lower arm, wrist and the end effector. The state of the virtual shoulder in the pick and place task is quite dynamic and includes humeral rotations which would be challenging to engineer in a real physical prosthesis above the elbow. Another question related to the pick and place task used is whether or not there are cases where both the pick position and the place position can be reached via the same, or very similar, shoulder positions? i.e. with the shoulder flexion-extension and abduction-adduction remaining fixed, can the ANN use the remaining five joint angles to solve the movement problem with little to no participant input, simply based on the new target position? If this was the case, movements times in the virtual space would present a very different distribution to natural movements, while the mean values could be similar. The arguments made in the paper could be supported by including individual participant data showing distributions of movement times and the distances travelled by the end effector where real movements are compared to those made by an ANN.</p>
<p>In the proposed approach users control where the hand is in space via the shoulder. The position of the upper arm and a given target are used as input to a classifier, which determines the positions of the lower arm, wrist and the effector. The supplementary materials suggest the output of the classifier occurs instantaneously, in that from the start of the trial the user can explore the 3D space associated with the shoulder in order to reach the object. When the object is reached a visual indicator appears. In a virtual space this feedback will allow rapid exploration of different end effector positions which may contribute to the movement times presented. In a real world application, movement of a distal end-effector via the shoulder is not to be as graceful and a speed accuracy trade off would be necessary to ensure objects are grasped, rather than knocked or moved.</p>
<p>Another aspect of the movement times presented which is of note, although it is not necessarily incorrect, is that the virtual prosthesis performance is close too perfect. In that, at the start of each trial period, either pick or place, the ANN appears to have already selected the position of the five joints it controls, leaving the user to position the upper arm such that the end effector reaches the target. This type of classification is achievable given a single object type to grasp and a limited number of orientations, however scaling this approach to work robustly in a real world environment will necessitate solving a number of challenges in machine learning and in particular computer vision which are not trivial in nature. On this topic, it is also important to note that, while very elegant, the teleoperation proof of concept of movement based control does not seem to feature a similar range of object distance from the user as the virtual environment. This would have been interesting to see and I look forward to seeing further real world demonstrations in the authors future work.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87317.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Segas et al motivate their work by indicating that none of the existing myoelectric solution for people with trans-humeral limb difference offer four active degrees of freedom, namely forearm flexion/extension, forearm supination/pronation, wrist flexion/extension, and wrist radial/ulnar deviation. These degrees of freedom are essential for positioning the prosthesis in the correct plan in the space before a grasp can be selected. They offer a controller based on the movement of the stump.</p>
<p>The proposed solution is elegant for what it is trying to achieve in a laboratory setting. Using a simple neural network to estimate the arm position is an interesting approach, despite the limitations/challenges that the approach suffers from, namely, the availability of prosthetic hardware that offers such functionality, information about the target and the noise in estimation if computer vision methods are used. Segas et al indicate these challenges in the manuscript, although they could also briefly discuss how they foresee the method could be expanded to enable a grasp command beyond the proximity between the end-point and the target. Indeed, it would be interesting to see how these methods can be generalise to more than one grasp.</p>
<p>One bit of the results that is missing in the paper is the results during the familiarisation block. If the methods in &quot;intuitive&quot; I would have thought no familiarisation would be needed. Do participants show any sign of motor adaptation during the familiarisation block?</p>
<p>In Supplementary Videos 3 and 4, how would the authors explain the jerky movement of the virtual arm while the stump is stationary? How would be possible to distinguish the relative importance of the target information versus body posture in the estimation of the arm position? This does not seem to be easy/clear to address beyond looking at the weights in the neural network.</p>
<p>I am intrigued by how the Generic ANN model has been trained, i.e. with the use of the forward kinematics to remap the measurement. I would have taught an easier approach would have been to create an Own model with the native arm of the person with the limb loss, as all your participants are unilateral (as per Table 1). Alternatively, one would have assumed that your common model from all participants would just need to be 'recalibrated' to a few examples of the data from people with limb difference, i.e. few shot calibration methods.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87317.2.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This work provides a new approach to simultaneously control elbow and wrist degrees of freedom using movement based inputs, and demonstrate performance in a virtual reality environment. The work is also demonstrated using a proof-of-concept physical system. This control algorithm is in contrast to prior approaches which electrophysiological signals, such as EMG, which do have limitations as described by the authors. In this work, the movements of proximal joints (eg shoulder), which generally remain under voluntary control after limb amputation, are used as input to neural networks to predict limb orientation. The results are tested by several participants within a virtual environment, and preliminary demonstrated using a physical device, albeit without it being physically attached to the user.</p>
<p>Strengths:</p>
<p>
Overall, the work has several interesting aspects. Perhaps the most interesting aspect of the work is that the approach worked well without requiring user calibration, meaning that users could use pre-trained networks to complete the tasks as requested. This could provide important benefits, and if successfully incorporated into a physical prosthesis allow the user to focus on completing functional tasks immediately. The work was also tested with a reasonable number of subjects, including those with limb-loss. Even with the limitations (see below) the approach could be used to help complete meaningful functional activities of daily living that require semi-consistent movements, such as feeding and grooming.</p>
<p>Weaknesses:</p>
<p>
While interesting, the work does have several limitations. In this reviewer's opinion, main limitations are: the number of 'movements' or tasks that would be required to train a controller that generalized across more tasks and limb-postures. The authors did a nice job spanning the workspace, but the unconstrained nature of reaches could make restoring additional activities problematic. This remains to be tested.</p>
<p>The weight of a device attached to a user will impact the shoulder movements that can be reliably generated. Testing with a physical prosthesis will need to ensure that the full desired workspace can be obtained when the limb is attached, and if not, then a procedure to scale inputs will need to be refined.</p>
<p>The reliance on target position is a complicating factor in deploying this technology. It would be interesting to see what performance may be achieved by simply using the input target positions to the controller and exclude the joint angles from the tracking devices (eg train with the target positions as input to the network to predict the desired angles).</p>
<p>Treating the humeral rotation degree of freedom is tricky, but for some subjects, such as those with OI, this would not be as large of an issue. Otherwise, the device would be constructed that allowed this movement.</p>
<p>Overall, this is an interesting preliminary study with some interesting aspects. Care must be taken to systematically evaluate the method to ensure clinical impact.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87317.2.sa4</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Segas</surname>
<given-names>Effie</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1266-0570</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mick</surname>
<given-names>Sébastien</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Leconte</surname>
<given-names>Vincent</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dubois</surname>
<given-names>Océane</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Klotz</surname>
<given-names>Rémi</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cattaert</surname>
<given-names>Daniel</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4471-0525</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>de Rugy</surname>
<given-names>Aymar</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5645-3680</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<p>We are very grateful to the reviewers for their thorough assessment of our study, and their acknowledgment of its strengths and weaknesses. We did our best below to address the weaknesses raised in their public review, and to comply with their recommendations.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Segas et al. present a novel solution to an upper-limb control problem which is often neglected by academia. The problem the authors are trying to solve is how to control the multiple degrees of freedom of the lower arm to enable grasp in people with transhumeral limb loss. The proposed solution is a neural network based approach which uses information from the position of the arm along with contextual information which defines the position and orientation of the target in space. Experimental work is presented, based on virtual simulations and a telerobotic proof of concept</p>
<p>The strength of this paper is that it proposes a method of control for people with transhumeral limb loss which does not rely upon additional surgical intervention to enable grasping objects in the local environment. A challenge the work faces is that it can be argued that a great many problems in upper limb prosthesis control can be solved given precise knowledge of the object to be grasped, its relative position in 3D space and its orientation. It is difficult to know how directly results obtained in a virtual environment will translate to real world impact. Some of the comparisons made in the paper are to physical systems which attempt to solve the same problem. It is important to note that real world prosthesis control introduces numerous challenges which do not exist in virtual spaces or in teleoperation robotics.</p>
</disp-quote>
<p>We agree that the precise knowledge of the object to grasp is an issue for real world application, and that real world prosthesis control introduces many challenges not addressed in our experiments. Those were initially discussed in a dedicated section of the discussion (‘Perspectives for daily-life applications’), and we have amended this section to integrate comments by reviewers that relate to those issues (cf below).</p>
<disp-quote content-type="editor-comment">
<p>The authors claim that the movement times obtained using their virtual system, and a teleoperation proof of concept demonstration, are comparable to natural movement times. The speed of movements obtained and presented are easier to understand by viewing the supplementary materials prior to reading the paper. The position of the upper arm and a given target are used as input to a classifier, which determines the positions of the lower arm, wrist and the end effector. The state of the virtual shoulder in the pick and place task is quite dynamic and includes humeral rotations which would be challenging to engineer in a real physical prosthesis above the elbow. Another question related to the pick and place task used is whether or not there are cases where both the pick position and the place position can be reached via the same, or very similar, shoulder positions? i.e. with the shoulder flexion-extension and abduction-adduction remaining fixed, can the ANN use the remaining five joint angles to solve the movement problem with little to no participant input, simply based on the new target position? If this was the case, movements times in the virtual space would present a very different distribution to natural movements, while the mean values could be similar. The arguments made in the paper could be supported by including individual participant data showing distributions of movement times and the distances travelled by the end effector where real movements are compared to those made by an ANN.</p>
<p>In the proposed approach users control where the hand is in space via the shoulder. The position of the upper arm and a given target are used as input to a classifier, which determines the positions of the lower arm, wrist and the effector. The supplementary materials suggest the output of the classifier occurs instantaneously, in that from the start of the trial the user can explore the 3D space associated with the shoulder in order to reach the object. When the object is reached a visual indicator appears. In a virtual space this feedback will allow rapid exploration of different end effector positions which may contribute to the movement times presented. In a real world application, movement of a distal end-effector via the shoulder is not to be as graceful and a speed accuracy trade off would be necessary to ensure objects are grasped, rather than knocked or moved.</p>
</disp-quote>
<p>As correctly noted by the reviewer and easily visible on videos, the distal joints predicted by the ANN are realized instantaneously in the virtual arm avatar, and a discontinuity occurs at each target change whereby the distal part of the arm jumps to the novel prediction associated with the new target location. As also correctly noted by the reviewer, there are indeed some instances where minimal shoulder movements are required to reach a new target, which in practice implies that on those instances, the distal part of the arm avatar jumps instantaneously close to the new target as soon as this target appears. Please note that we originally used median rather than mean movement times per participant precisely to remain unaffected by potential outliers that might come from this or other situations. We nevertheless followed the reviewer’s advice and have now also included individual distributions of movement times for each condition and participant (cf Supplementary Fig. 2 to 4 for individual distributions of movement time for Exp1 to 3, respectively). Visual inspection of those indicates that despite slight differences between participants, no specific pattern emerges, with distributions of movement times that are quite similar between conditions when data from all participants are pooled together.</p>
<p>Movement times analysis indicates therefore that the overall participants’ behavior has not been impacted by the instantaneous jump in the predicted arm positions at each of the target changes. Yet, those jumps indicate that our proposed solution does not satisfactorily reproduce movement trajectory, which has implications for application in the physical world. Although we introduced a 0.75 s period before the beginning of each trial for the robotic arm to smoothly reach the first prediction from the ANN in our POC experiment (cf Methods), this would not be practical for a real-life scenario with a sequence of movements toward different goals. Future developments are therefore needed to better account for movement trajectories. We are now addressing this explicitly in the manuscript, with the following paragraph added in the discussion (section ‘Perspectives of daily-life applications’):</p>
<p>“Although our approach enabled participants to converge to the correct position and orientation to grasp simple objects with movement times similar to those of natural movements, it is important to note that further developments are needed to produce natural trajectories compatible with real-world applications. As easily visible on supplementary videos 2 to 4, the distal joints predicted by the ANN are realized instantaneously such that a discontinuity occurs at each target change, whereby the distal part of the arm jumps to the novel prediction associated with the new target location. We circumvented problems associated with this discontinuity on our physical proof of concept by introducing a period before the beginning of each trial for the robotic arm to smoothly reach the first prediction from the ANN. This issue, however, needs to be better handled for real-life scenarios where a user will perform sequences of movements toward different objects.”</p>
<disp-quote content-type="editor-comment">
<p>Another aspect of the movement times presented which is of note, although it is not necessarily incorrect, is that the virtual prosthesis performance is close too perfect. In that, at the start of each trial period, either pick or place, the ANN appears to have already selected the position of the five joints it controls, leaving the user to position the upper arm such that the end effector reaches the target. This type of classification is achievable given a single object type to grasp and a limited number of orientations, however scaling this approach to work robustly in a real world environment will necessitate solving a number of challenges in machine learning and in particular computer vision which are not trivial in nature. On this topic, it is also important to note that, while very elegant, the teleoperation proof of concept of movement based control does not seem to feature a similar range of object distance from the user as the virtual environment. This would have been interesting to see and I look forward to seeing further real world demonstrations in the authors future work.</p>
</disp-quote>
<p>According to this comment, the reviewer has the impression that the ANN had already selected a position of the five joints it controls at the start of each trial, and maintained those fixed while the user operates the upper arm so as to reach the target. Although the jumps at target changes discussed in the previous comment might give this impression, and although this would be the case should we have used an ANN trained with contextual information only, it is important to stress that our control does take shoulder angles as inputs, and produced therefore changes in the predicted distal angles as the shoulder moves.</p>
<p>To substantiate this, we provide in Author response image 1 the range of motion (angular difference at each joint between the beginning and the end of each trial) of the five distal arm angles, regrouped for all angles and trials of Exp1 to 3 (one circle and line per participant, representing the median of all data obtained by that participant in the given experiment and condition, as in Fig. 3 of the manuscript). Please note that those ranges of motion were computed on each trial just after the target changes (i.e., after the jumps) for conditions with prosthesis control, and that the percentage noted on the figure below those conditions correspond to the proportion of the range of motion obtained in the natural movement condition. As can be seen, distal angles were solicited in all prosthesis control conditions by more than half the amount they moved in the condition of natural movements (between 54 and 75% depending on conditions).</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-87317-sa4-fig1.jpg" mimetype="image"/>
</fig>
<p>With respect to the last part of this comment, we agree that scaling this approach to work robustly in a real world environment will necessitate solving a number of challenges in machine learning and in particular computer vision. We address those in a specific section of the discussion (‘Perspectives for daily-life application’) which has been further amended in response to the reviewers’ comments. As also mentioned earlier and at the occasion of our reply to other reviewers’ comments, we also agree that our physical proof of concept is quite preliminary, and we are looking forward to conduct future work in order to solve some of the issues discussed and get closer to real world demonstrations.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Segas et al motivate their work by indicating that none of the existing myoelectric solution for people with transhumeral limb difference offer four active degrees of freedom, namely forearm flexion/extension, forearm supination/pronation, wrist flexion/extension, and wrist radial/ulnar deviation. These degrees of freedom are essential for positioning the prosthesis in the correct plan in the space before a grasp can be selected. They offer a controller based on the movement of the stump.</p>
<p>The proposed solution is elegant for what it is trying to achieve in a laboratory setting. Using a simple neural network to estimate the arm position is an interesting approach, despite the limitations/challenges that the approach suffers from, namely, the availability of prosthetic hardware that offers such functionality, information about the target and the noise in estimation if computer vision methods are used. Segas et al indicate these challenges in the manuscript, although they could also briefly discuss how they foresee the method could be expanded to enable a grasp command beyond the proximity between the end-point and the target. Indeed, it would be interesting to see how these methods can be generalise to more than one grasp.</p>
</disp-quote>
<p>Indeed, we have already indicated those challenges in the manuscript, including the limitation that our control “is suitable to place the hand at a correct position and orientation to grasp objects in a wide workspace, but not for fine hand and grasp control ...” (cf 4th paragraph of the ‘Perspectives for daily-life applications’ section of the discussion). We have nevertheless added the following sentence at the end of this paragraph to stress that our control could be combined with recently documented solutions for multiple grasp functions: “Our movement-based approach could also be combined with semi-autonomous grasp control to accommodate for multiple grasp
functions39,42,44.”</p>
<disp-quote content-type="editor-comment">
<p>One bit of the results that is missing in the paper is the results during the familiarisation block. If the methods in &quot;intuitive&quot; I would have thought no familiarisation would be needed. Do participants show any sign of motor adaptation during the familiarisation block?</p>
</disp-quote>
<p>Please note that the familiarization block indicated Fig. 3a contains approximately half of the trials of the subsequent initial acquisition block (about 150 trials, which represents about 3 minutes of practice once the task is understood and proficiently executed), and that those were designed to familiarize participants with the VR setup and the task rather than with the prosthesis controls. Indeed, it is important that participants were made familiar with the setup and the task before they started the initial acquisition used to collect their natural movements. In Exp1 and 2, there was therefore no familiarization to the prosthesis controls whatsoever (and thus no possible adaptation associated with it) before participants used them for the very first time in the blocks dedicated to test them. This is slightly different in Exp3, where participants with an amputated arm were first tested on their amputated side with our generic control. Although slight adaptation to the prosthesis control might indeed have occurred during those familiarization trials, this would be difficult in practice to separate from the intended familiarization to the task itself, which was deemed necessary for that experiment as well. In the end, we believe that this had little impact on our data since that experiment produced behavioral results comparable to those of Exp1 and 2, where no familiarization to the prosthesis controls could have occurred.</p>
<disp-quote content-type="editor-comment">
<p>In Supplementary Videos 3 and 4, how would the authors explain the jerky movement of the virtual arm while the stump is stationary? How would be possible to distinguish the relative importance of the target information versus body posture in the estimation of the arm position? This does not seem to be easy/clear to address beyond looking at the weights in the neural network.</p>
</disp-quote>
<p>As discussed in our response to Reviewer1 and now explicitly addressed in the manuscript, there is a discontinuity in our control, whereby the distal joints of the arm avatar jumps instantaneously to the new prediction at each target change at the beginning of a trial, before being updated online as a function of ongoing shoulder movements for the rest of that trial. In a sense, this discontinuity directly reflects the influence of the target information in the estimation of the distal arm posture. Yet, as also discussed in our reply to R1, the influence of proximal body posture (i.e., Shoulder movements) is made evident by substantial movements of the predicted distal joints after the initial jumps occurring at each target change. Although those features demonstrate that both target information and proximal body posture were involved in our control, they do not establish their relative importance. While offline computation could be thought to quantify their relative implication in the estimation of the distal arm posture, we believe that further human-in-the-loop experiments with selective manipulation of this implication would be necessary to establish how this might affect the system controllability.</p>
<disp-quote content-type="editor-comment">
<p>I am intrigued by how the Generic ANN model has been trained, i.e. with the use of the forward kinematics to remap the measurement. I would have taught an easier approach would have been to create an Own model with the native arm of the person with the limb loss, as all your participants are unilateral (as per Table 1). Alternatively, one would have assumed that your common model from all participants would just need to be 'recalibrated' to a few examples of the data from people with limb difference, i.e. few shot calibration methods.</p>
</disp-quote>
<p>AR: Although we could indeed have created an Own model with the native arm of each participant with a limb loss, the intention was to design a control that would involve minimal to no data acquisition at all, and more importantly, that could also accommodate bilateral limb loss. Indeed, few shot calibration methods would be a good alternative involving minimal data acquisition, but this would not work on participants with bilateral limb loss.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>This work provides a new approach to simultaneously control elbow and wrist degrees of freedom using movement based inputs, and demonstrate performance in a virtual reality environment. The work is also demonstrated using a proof-of-concept physical system. This control algorithm is in contrast to prior approaches which electrophysiological signals, such as EMG, which do have limitations as described by the authors. In this work, the movements of proximal joints (eg shoulder), which generally remain under voluntary control after limb amputation, are used as input to neural networks to predict limb orientation. The results are tested by several participants within a virtual environment, and preliminary demonstrated using a physical device, albeit without it being physically attached to the user.</p>
<p>Strengths:</p>
<p>Overall, the work has several interesting aspects. Perhaps the most interesting aspect of the work is that the approach worked well without requiring user calibration, meaning that users could use pre-trained networks to complete the tasks as requested. This could provide important benefits, and if successfully incorporated into a physical prosthesis allow the user to focus on completing functional tasks immediately. The work was also tested with a reasonable number of subjects, including those with limb-loss. Even with the limitations (see below) the approach could be used to help complete meaningful functional activities of daily living that require semi-consistent movements, such as feeding and grooming.</p>
<p>Weaknesses:</p>
<p>While interesting, the work does have several limitations. In this reviewer's opinion, main limitations are: the number of 'movements' or tasks that would be required to train a controller that generalized across more tasks and limbpostures. The authors did a nice job spanning the workspace, but the unconstrained nature of reaches could make restoring additional activities problematic. This remains to be tested.</p>
</disp-quote>
<p>We agree and have partly addressed this in the first paragraph of the ‘Perspective for daily life applications’ section of the discussion, where we expand on control options that might complement our approach in order to deal with an object after it has been reached. We have now amended this section to explicitly stress that generalization to multiple tasks including more constrained reaches will require future work: “It remains that generalizing our approach to multiple tasks including more constrained reaches will require future work. For instance, once an intended object has been successfully reached or grasped, what to do with it will still require more than computer vision and gaze information to be efficiently controlled. One approach is to complement the control scheme with subsidiary movements, such as shoulder elevation to bring the hand closer to the body or sternoclavicular protraction to control hand closing26, or even movement of a different limb (e.g., a foot45). Another approach is to control the prosthesis with body movements naturally occurring when compensating for an improperly controlled prosthesis configuration46.”</p>
<disp-quote content-type="editor-comment">
<p>The weight of a device attached to a user will impact the shoulder movements that can be reliably generated. Testing with a physical prosthesis will need to ensure that the full desired workspace can be obtained when the limb is attached, and if not, then a procedure to scale inputs will need to be refined.</p>
</disp-quote>
<p>We agree and have now explicitly included this limitation and perspective to our discussion, by adding a sentence when discussing possible combination with osseointegration: “Combining those with osseointegration at humeral level3,4 would be particularly relevant as this would also restore amplitude and control over shoulder movements, which are essential for our control but greatly affected with conventional residual limb fitting harness and sockets. Yet, testing with a physical prosthesis will need to ensure that the full desired workspace can be obtained with the weight of the attached device, and if not, a procedure to scale inputs will need to be refined.”</p>
<disp-quote content-type="editor-comment">
<p>The reliance on target position is a complicating factor in deploying this technology. It would be interesting to see what performance may be achieved by simply using the input target positions to the controller and exclude the joint angles from the tracking devices (eg train with the target positions as input to the network to predict the desired angles).</p>
</disp-quote>
<p>Indeed, the reliance on precise pose estimation from computer vision is a complicating factor in deploying this technology, despite progress in this area which we now discuss in the first paragraph of the ‘Perspective for daily life applications’ section of the discussion. Although we are unsure what precise configuration of input/output the reviewer has in mind, part of our future work along this line is indeed explicitly dedicated to explore various sets of input/output that could enable coping with availability and reliability issues associated with real-life settings.</p>
<disp-quote content-type="editor-comment">
<p>Treating the humeral rotation degree of freedom is tricky, but for some subjects, such as those with OI, this would not be as large of an issue. Otherwise, the device would be constructed that allowed this movement.</p>
</disp-quote>
<p>We partly address this when referring to osseointegration in the discussion: “Combining those with osseointegration at humeral level3,4 would be particularly relevant as this would also restore amplitude and control over shoulder movements, which are essential for our control but greatly affected with conventional residual limb fitting harness and sockets.” Yet, despite the fact that our approach proved efficient in reconstructing the required humeral angle, it is true that realizing it on a prosthesis without OI is an open issue.</p>
<disp-quote content-type="editor-comment">
<p>Overall, this is an interesting preliminary study with some interesting aspects. Care must be taken to systematically evaluate the method to ensure clinical impact.</p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>Page 2: Sentence beginning: &quot;Here, we unleash this movement-based approach by ...&quot;. The approach presented utilises 3D information of object position. Please could the authors clarify whether or not the computer vision references listed are able to provide precise 3D localisation of objects?</p>
</disp-quote>
<p>While the references initially cited in this sentence do support the view that movement goals could be made available in the context of prosthesis control through computer vision combined with gaze information, it is true that they do not provide the precise position and orientation (I.e., 6d pose estimation) necessary for our movementbased control approach. Six-dimensional object pose estimation is nevertheless a very active area of computer vision that has applications beyond prosthesis control, and we have now added to this sentence two references illustrating recent progress in this research area (cf. references 30 and 31).</p>
<disp-quote content-type="editor-comment">
<p>Page 6: Sentence beginning: &quot;The volume spread by the shoulder's trajectory ...&quot;.</p>
<list list-type="bullet">
<list-item><p>Page 7: Sentence beginning: &quot;With respect to the volume spread by the shoulder during the Test phases ...&quot;.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Page 7: Sentence beginning: &quot;Movement times with our movement-based control were also in the same range as in previous experiments, and were even smaller by the second block of intuitive control ...&quot;.</p>
</list-item></list>
<p>On the shoulder volume presented in Figure 3d. My interpretation of the increased shoulder volume in Figure 3D Expt 2 shown in the Generic ANN was that slightly more exploration of the upper arm space was necessary (as related to the point in the public review). Is this what the authors mean by the action not being as intuitive? Does the reduction in movement time between TestGeneric1 and TestGeneric 2 not suggest that some degree of exploration and learning of the solution space is taking place?</p>
</disp-quote>
<p>Indeed, the slightly increased shoulder volume with the Generic ANN in Exp2 could be interpreted as a sign that slightly more exploration of the upper arm space was necessary. At present, we do not relate this to intuitiveness in the manuscript. And yes, we agree that the reduction in movement time between TestGeneric1 and TestGeneric 2 could suggest some degree of exploration and learning.</p>
<disp-quote content-type="editor-comment">
<p>Page 7: Sentence beginning: &quot;As we now dispose of an intuitive control ...&quot;. I think dispose may be a false friend in this context!</p>
</disp-quote>
<p>This has been replaced by “As we now have an intuitive control…”.</p>
<disp-quote content-type="editor-comment">
<p>Page 8: Section beginning &quot;Physical Proof of Concept on a tele-operated robotic platform&quot;. I assume this section has been added based on suggestions from a previous review. Although an elegant PoC the task presented in the diagram appears to differ from the virtual task in that all the targets are at a relatively fixed distance from the robot. In respect to the computer vision ML requirements, this does not appear to require precise information about the distance between the user and an object. Please could this be clarified?</p>
</disp-quote>
<p>Indeed, the Physical Proof of Concept has been added after the original submission in order to comply with requests formulated at the editorial stage for the paper to be sent for review. Although preliminary and suffering from several limitations (amongst which a reduced workspace and number of trials as compared to the VR experiments), this POC is a first step toward realizing this control in the physical world. Please note that as indicated in the methods, the target varied in depth by about 10 cm, and their position and orientation were set with sensors at the beginning of each block instead of being determined from computer vision (cf section ‘Physical Proof of Concept’ in the ‘Methods’: “The position and orientation of each sponge were set at the beginning of each block using a supplementary sensor. Targets could be vertical or tilted at 45 and -45° on the frontal plane, and varied in depth by about 10 cm.”).</p>
<disp-quote content-type="editor-comment">
<p>Page 10: Sentence beginning: &quot;This is ahead of other control solutions that have been proposed ...&quot;. I am not sure what this sentence is supposed to convey and no references are provided. While the methods presented appear to be a viable solution for a group of upper-limb amputees who are often ignored by academic research, I am not sure it is appropriate for the authors to compare the results obtained in VR and via teleoperation to existing physical systems (without references it is difficult to understand what comparison is being made here).</p>
</disp-quote>
<p>The primary purpose of this sentence is to convey that our approach is ahead of other control solutions proposed so far to solve the particular problem as defined earlier in this paragraph (“Yet, controlling the numerous joints of a prosthetic arm necessary to place the hand at a correct position and orientation to grasp objects remains challenging, and is essentially unresolved”), and as documented to the best we could in the introduction. We believe this to be true and to be the main justification for this publication. The reviewer’s comment is probably directed toward the second part of this sentence, which states that performances of previously proposed control solutions (whether physical or in VR) are rarely compared to that of natural movements, as this comparison would be quite unfavorable to them. We soften that statement by removing the last reference to unfavorable comparison, but maintained it as we believe it is reflecting a reality that is worth mentioning. Please note that after this initial paragraph, and an exposition of the critical features of our control, most of the discussion (about 2/3) is dedicated to limitations and perspectives for daily-life application.</p>
<disp-quote content-type="editor-comment">
<p>Page 10: Sentence: &quot;Here, we overcame all those limitations.&quot; Again, the language here appears to directly compare success in a virtual environment with the current state of the art of physical systems. Although the limitations were realised in a virtual environment and a teleoperation PoC, a physical implementation of the proposed system would depend on advances in machine vision to include movement goal. It could be argued that limitations have been traded, rather immediately overcome.</p>
</disp-quote>
<p>In this sentence, “all those limitations” refers to all three limitations mentioned in the previous sentences in relation to our previous study which we cited in that sentence (Mick et al., JNER 2021), rather than to limitations of the current state of the art of physical systems. To make this more explicit, we have now changed this sentence to “Here, we overcome those three limitations”.</p>
<disp-quote content-type="editor-comment">
<p>Page 11: Sentence beginning: &quot;Yet, impressive progresses in artificial intelligence and computer vision ...&quot;.</p>
<list list-type="bullet">
<list-item><p>Page 11: Sentence beginning: &quot;Prosthesis control strategies based on computer vision ...&quot;</p>
</list-item></list>
<p>The science behind self-driving cars is arguably of comparable computational complexity to the real-world object detection and with concurrent real-time grasp selection. The market for self-driving cars is huge and a great deal of R&amp;D has been funded, yet they are not yet available. The market for advanced upper-limb prosthetics is very small, it is difficult to understand who would deliver this work.</p>
</disp-quote>
<p>We agree that the market for self-driving cars is much higher than that for advanced upper-limb prosthetics. Yet, as mentioned in our reply to a previous comment, 6D object pose estimation is a very active area of computer vision that has applications far beyond prosthesis control (cf. in robotics and augmented reality). We have added two references reflecting recent progress in this area in the introduction, and have amended the discussion accordingly: “Yet, impressive progress in artificial intelligence and computer vision is such that what would have been difficult to imagine a decade ago appears now well within grasp38. For instance, we showed recently that deep learning combined with gaze information enables identifying an object that is about to be grasped from an egocentric view on glasses33, and this even in complex cluttered natural environments34. Six-dimensional object pose estimation is also a very active area of computer vision30,31, and prosthesis control strategies based on computer vision combined with gaze and/or myoelectric control for movement intention detection are quickly developing39–44, illustrating the promises of this approach.”</p>
<disp-quote content-type="editor-comment">
<p>Page 15: Sentence beginning: &quot;From this recording, 7 signals were extracted and fed to the ANN as inputs: ...&quot;.</p>
<list list-type="bullet">
<list-item><p>Page 15: Sentence beginning: &quot;Accordingly, the contextual information provided as input corresponded to the ...&quot;.</p>
</list-item></list>
<p>The two sentences appear to contradict one another and it is difficult to understand what the Own ANN was trained on. If the position and the orientation of the object were not used due to overfitting, why claim that they were used as contextual information? Training on the position and orientation of the hand when solving the problem would not normally be considered contextual information, the hand is not part of the environment or setting, it is part of the user. Please could this section be made a little bit clearer?</p>
</disp-quote>
<p>The Own ANN was trained using the position and the orientation of a hypothetic target located within the hand at any given time. This approach has been implemented to increase the amount of available data. However, when the ANN is utilized to predict the distal part of the virtual arm, the position and orientation of the current target are provided. We acknowledge that the phrasing could be misleading, so we have added the following clarification to the first sentence: &quot;… (3 Cartesian coordinates and 2 spherical angles that define the position and orientation of the hand as if a hypothetical cylindrical target was placed in it at any time, see an explanation for this choice in the next paragraph)&quot;.</p>
<disp-quote content-type="editor-comment">
<p>Page 16: Sentence beginning: &quot;A trial refers to only one part of this process: either ...&quot;. Would be possible to present these values separately?</p>
</disp-quote>
<p>Although it would be possible to present our results separately for the pick phase and for the place phase, we believe that this would overload the manuscript for little to no gain. Indeed, nothing differentiates those two phases other than the fact that the bottle is on the platform (waiting to be picked) in the pick phase, and in the hand (waiting to be placed) in the place phase. We therefore expect to have very similar results for the pick phase and for the place phase, which we verified as follows on Movement Time: Author response image 2 shows movement time results separated for the pick phase (a) and for the place phase (b), together with the median (red dotted line) obtained when results from both phases are polled together. As illustrated, results are very similar for both phases, and similar to those currently presented in the manuscript with both phases pooled (Fig3C).</p>
<fig id="sa4fig2">
<label>Author response image 2.</label>
<graphic mime-subtype="jpg" xlink:href="elife-87317-sa4-fig2.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Page 19: Sentence beginning &quot;The remaining targets spanned a roughly ...&quot;. Figure 2 is a very nice diagram but it could be enhanced with a simple visual representation of this hemispherical region on the vertical and horizontal planes.</p>
</disp-quote>
<p>We made a few attempts at enhancing this figure as suggested. However, the resulting figures tended to be overloaded and were not conclusive, so we opted to keep the original.</p>
<disp-quote content-type="editor-comment">
<p>Page 19: Sentence beginning &quot;The Movement Time (MT) ...&quot;</p>
<list list-type="bullet">
<list-item><p>Page 19: Sentence beginning &quot;The shoulder position Spread Volume (SV) ...&quot;
Would it be possible to include a traditional timing protocol somewhere in the manuscript so that readers can see the periods over which these measures calculated?</p>
</list-item></list>
</disp-quote>
<p>We have now included Fig. 5 to illustrate the timing protocol and the periods over which MT and SV were computed.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>Minor comments</p>
<p>Page 6: &quot;Yet, this control is inapplicable &quot;as is&quot; to amputees, for which recording ...&quot; -&gt; &quot;Yet, this control is inapplicable &quot;as is&quot; to amputees, for WHOM recording ... &quot;</p>
</disp-quote>
<p>This has been modified as indicated.</p>
<disp-quote content-type="editor-comment">
<p>Throughout: &quot;amputee&quot; -&gt; &quot;people with limb loss&quot; also &quot;individual with limb deficiency&quot; -&gt; &quot;individual with limb difference&quot;</p>
</disp-quote>
<p>We have modified throughout as indicated.</p>
<disp-quote content-type="editor-comment">
<p>It would have been great to see a few videos from the tele-operation as well. Please could you supply these videos?</p>
</disp-quote>
<p>Although we agree that videos of our Physical Proof of Concept would have been useful, we unfortunately did not collect videos that would be suitable for this purpose during those experimental phases. Please note that this Physical Proof of Concept was not meant to be published originally, but has been added after the original submission in order to comply with requests formulated at the editorial stage for the paper to be sent for review.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>Consider using the terms: intact-limb rather than able-bodied, residual limb rather than stump, congenital limb different rather than congenital limb deficiency.</p>
</disp-quote>
<p>We have modified throughout as indicated.</p>
</body>
</sub-article>
</article>