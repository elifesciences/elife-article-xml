<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">105213</article-id>
<article-id pub-id-type="doi">10.7554/eLife.105213</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.105213.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Mesoscale functional architecture in medial posterior parietal cortex</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Hira</surname>
<given-names>Riichiro</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<email>rhira.phy2@tmd.ac.jp</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Townsend</surname>
<given-names>Leah B</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Smith</surname>
<given-names>Ikuko T</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yu</surname>
<given-names>Che-Hang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Stirman</surname>
<given-names>Jeffrey N</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yu</surname>
<given-names>Yiyi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Smith</surname>
<given-names>Spencer LaVere</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>sls@ucsb.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>Department of Electrical and Computer Engineering, University of California Santa Barbara</institution></institution-wrap>, <city>Santa Barbara</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>Department of Molecular, Cellular, and Developmental Biology, Department of Psychology and Brain Sciences, University of California Santa Barbara</institution></institution-wrap>, <city>Santa Barbara</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0130frc33</institution-id><institution>Neuroscience Center, University of North Carolina Chapel Hill</institution></institution-wrap>, <city>Chapel Hill</city>, <country country="US">United States</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/051k3eh31</institution-id><institution>Department of Physiology and Cell Biology, Tokyo Medical and Dental University</institution></institution-wrap>, <city>Tokyo</city>, <country country="JP">Japan</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Petreanu</surname>
<given-names>Leopoldo</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Champalimaud Center for the Unknown</institution>
</institution-wrap>
<city>Lisbon</city>
<country>Portugal</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing Interest Statement: S.L.S. is a paid consultant for companies that sell optics and multiphoton microscopes. C.-H.Y. and S.L.S. have interests in the company Pacific Optica. The other authors declare no competing interests.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-02-07">
<day>07</day>
<month>02</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP105213</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-12-02">
<day>02</day>
<month>12</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-12-02">
<day>02</day>
<month>12</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.08.27.555017"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Hira et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Hira et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-105213-v1.pdf"/>
<abstract>
<title>Summary</title>
<p>The posterior parietal cortex (<bold>PPC</bold>) in mice has various functions including multisensory integration<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref></sup>, vision-guided behaviors<sup><xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>, working memory<sup><xref ref-type="bibr" rid="c7">7</xref>–<xref ref-type="bibr" rid="c13">13</xref></sup>, and posture control<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c15">15</xref></sup>. However, an integrated understanding of these functions and their cortical localizations in and around the PPC and higher visual areas (<bold>HVAs</bold>), has not been completely elucidated. Here we simultaneously imaged the activity of thousands of neurons within a 3 x 3 mm<sup>2</sup> field-of-view, including eight cortical areas around the PPC, during behavior with a two-photon mesoscope<sup><xref ref-type="bibr" rid="c16">16</xref></sup>. Mice performed both a vision-guided task and a choice history-dependent task, and the imaging results revealed distinct, localized, behavior-related functions of two medial PPC areas. Neurons in the anteromedial (<bold>AM</bold>) HVA responded to both vision and choice information, and thus AM is a locus of association between these channels. By contrast, the anterior (<bold>A</bold>) HVA stores choice history with sequential dynamics and represents posture. Mesoscale correlation analysis on the intertrial variability of neuronal activity demonstrated that neurons in area A shared fluctuations with the primary somatosensory area, while neurons in AM exhibited diverse, area-dependent interactions. Pairwise interarea interactions among neurons were precisely predicted by the anatomical input correlations, with the exception of some global interactions. Thus, the medial PPC has two distinct modules, areas A and AM, which each have distinctive modes of cortical communication. These medial PPC modules can serve separate higher-order functions: area A for transmission of information including posture, movement, and working memory; and area AM for multisensory and cognitive integration with locally processed signals.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Updated version with new analysis and figures.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The posterior parietal cortex (PPC) is an association area located between the visual and somatosensory cortices and is widely shared by mammals. The PPC has distinct regions and each one of these has unique functions and corticocortical projection patterns in primates <sup><xref ref-type="bibr" rid="c17">17</xref>–<xref ref-type="bibr" rid="c19">19</xref></sup>. In mice, the PPC overlaps with HVAs <sup><xref ref-type="bibr" rid="c20">20</xref></sup>, has characteristic cytoarchitectures <sup><xref ref-type="bibr" rid="c21">21</xref>–<xref ref-type="bibr" rid="c24">24</xref></sup> and unique corticocortical recurrent projection patterns <sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>, which are similar to those of rats <sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c26">26</xref></sup>. Despite its relatively small area compared to that of primates, mouse PPC has various functions like primates including navigation <sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>, multisensory integration <sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref></sup>, memory-guided decision making <sup><xref ref-type="bibr" rid="c6">6</xref>–<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>, visually <sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c5">5</xref></sup> and auditory <sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup> guided decision making. In rats, PPC is also important in attention <sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>, sensory history <sup><xref ref-type="bibr" rid="c13">13</xref></sup>, and posture coordination <sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c15">15</xref></sup>. Thus, neurons in rodent PPC can represent various sensorimotor and integrative functions. However, prior studies typically focused on a single cortical location and often a single function, and thus it is unclear whether neurons with particular functions are distributed in overlapping populations or localized.</p>
<p>The function of an individual cortical area is influenced by the information that area receives from other brain regions. Connectivity among cortical areas is highly specific in mice <sup><xref ref-type="bibr" rid="c33">33</xref></sup>, and includes frequent cases of neurons sending bifurcated axons to multiple target regions <sup><xref ref-type="bibr" rid="c34">34</xref></sup>. This axonal divergence indicates that to understand association inter-area interactions, it is insufficient to consider only pairwise connections. Instead, such analysis should consider the shared connectivity patterns among three or more areas. When weak connections are included, the mouse cerebrum is estimated to have a 97% connectivity density <sup><xref ref-type="bibr" rid="c35">35</xref></sup>. In addition, cortical activity is a complex interplay of diverse information due to interactions with the thalamus, basal ganglia, and cerebellum. Moreover, the mode of these interactions can vary depending on behavior <sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. Thus, to understand the nature of the PPC, it is necessary to examine not only the pairwise interaction between cortical areas, but also investigate multiple interrelated areas in an integrated manner under a variety of environmental constraints.</p>
<p>Here, we examine the functional architecture of the mouse PPC and surrounding cortical areas using calcium imaging with a large field-of-view two-photon microscope <sup><xref ref-type="bibr" rid="c16">16</xref></sup> during head-fixed behavior in multiple tasks. We mapped vision, choice, choice history, and posture-related neurons within and surrounding the PPC. We found clear functional localization between two areas in medial PPC: HVAs A and AM. Furthermore, the mesoscale correlation structure of trial-to-trial variability indicated that A and AM had distinct relationships with surrounding cortical areas. The local interarea interaction aligned with the anatomical structure, but the global interaction did not, and thus the functional data revealed an aspect of the circuit organization. Our findings indicate that various modes of interarea communication constitute diverse functions of PPC.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Demarcation of PPC as A, AM, and RL</title>
<p>To functionally determine the precise anatomical location of PPC, we first identified retinotopic and somatotopic organizations with intrinsic signal optical imaging (ISOI) in anesthetized mice <sup><xref ref-type="bibr" rid="c38">38</xref>–<xref ref-type="bibr" rid="c42">42</xref></sup>. Retinotopic mapping revealed locations of each HVA including AM (<xref rid="fig1" ref-type="fig">Figure 1A</xref>), which has been considered to be the core of mice PPC in recent studies <sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>. To map the boundary of the primary somatosensory (<bold>S1</bold>) area anterior to the HVAs, we used tactile vibration of the body parts including a trunk as trunk S1 is known to be located at the most posterior portion of the S1 <sup><xref ref-type="bibr" rid="c33">33</xref></sup>. As expected, the trunk S1 (<bold>S1t</bold>) was located at a posterior region within S1 (<xref rid="fig1" ref-type="fig">Figure 1B</xref>; <xref ref-type="fig" rid="figs1">Figure S1A</xref>,<xref ref-type="fig" rid="figs1">B</xref>). The distance from the center of the AM to the center of the S1t was ∼900 μm (894 ± 18.3 (mean ± s.d.), n = 8 mice). Assuming that the diameter of AM and S1t are 600 μm and 400 μm, respectively, we could estimate that there would be a space 400 μm wide along the rostrocaudal axis between these two areas. Here, we defined this space as area A (<xref rid="fig1" ref-type="fig">Figure 1C</xref>), because it roughly overlaps with the HVA identified as area A in a landmark anatomical study of mouse HVAs <sup><xref ref-type="bibr" rid="c20">20</xref></sup>. Our definition of area A likely includes a larger area than this original definition.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Definition of cortical area A and redefinition of PPC based on visual and somatosensory responses.</title>
<p><bold>A.</bold> Elevation <italic>(left)</italic> and azimuth (<italic>right</italic>) maps showing retinotopy that form the borders of V1 (primary visual cortex) and AM. The cortical surface was color-coded according to the location of the bar on the monitor at the time of the evoked visual response. <bold>B.</bold> Tactile stimuli of a tail (left, red) and a trunk (<italic>right</italic>, green) resulted in clear spots in S1. <bold>C.</bold> The center positions of S1 tail and S1 trunk regions relative to the center position of AM from 8 mice. The gap space between the S1 trunk region and AM was defined as A. <bold>D.</bold> A location of the cranial window and FOV of two-photon imaging was overlaid on the Allen CCF version 3 <sup><xref ref-type="bibr" rid="c33">33</xref></sup>. <bold>E.</bold> The average image of calcium imaging and ROI positions for panel <bold>F</bold>. <bold>F</bold>. Fluorescence traces (mean and s.e.m.) of ROI indicated in panel <bold>E</bold> when the mouse received visual stimulation (azimuth) and tactile stimuli (tail, trunk, and whisker). <bold>G,H.</bold> Single neuron resolution mapping of somatotopy (<bold>G</bold>) and retinotopy (<bold>H</bold>) revealed that A had neither retinotopy nor somatotopy, but had neurons activated by tactile stimuli and a small number of neurons activated by visual stimulation. On the other hand, AM had retinotopy and a small number of neurons activated by tactile stimuli. A was defined after the S1 and AM were determined by somatotopic and retinotopic organizations, respectively. Scale bar: 1mm. <bold>I</bold>. Redefined PPC in this study (A, AM and RL).</p></caption>
<graphic xlink:href="555017v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Often, PPC is determined using stereotaxic coordinates, and we considered this approach. However, systematic mapping of visual areas relative to the lambda and midline showed that each HVA location based on the stereotaxic coordinates was unreliable. The mean distance from the center position of each HVA in each mouse to the average center position across mice was 244 μm, (range: 198 – 316 μm, n = 12 mice) (<xref ref-type="fig" rid="figs1">Figure S1C</xref>,<xref ref-type="fig" rid="figs1">D</xref>) when the positions were measured relative to the lambda landmark. We confirmed that the variability in locations was not explained by the age or sex (<xref ref-type="fig" rid="figs1">Figure S1E</xref>). Instead, the variation of HVAs substantially decreased after the registration of the areas by translation based on the location of V1 and RL. After this registration, the mean distance from the center position of each HVA to the average center position across mice was ∼91.5 μm (range: 38.6 - 141 μm), n = 12 mice): a reduction in variability by more than half (<xref ref-type="fig" rid="figs1">Figure S1F-I</xref>). This improvement indicates that relative locations of HVAs are similar across mice <sup><xref ref-type="bibr" rid="c43">43</xref></sup>, but HVA locations relative to skull landmarks are more than twice as variable. Thus, localizing these small HVAs and components of PPC by stereotaxic coordinates can introduce variability and decrease the fidelity of localization. We concluded that functional imaging of retinotopic and somatotopic organization must be obtained in each individual mouse to precisely and rigorously map HVAs and PPC, and to ascribe functions to neurons therein.</p>
<p>ISOI was supplemented with two-photon calcium imaging to observe the activity of neurons within and surrounding PPC including area A (<xref rid="fig1" ref-type="fig">Figure 1D-F</xref>). In awake mice, somatosensory stimuli including tail, trunk, neck, and whiskers resulted in responses in PPC areas. In S1, somatosensory stimulation evoked localized neuronal responses that exhibited somatotopic organization. In contrast, area A neurons activated by somatosensory stimuli did not exhibit clear somatotopy (<xref rid="fig1" ref-type="fig">Figure 1G</xref>). Likewise, visual responses of neurons in area A did not exhibit clear retinotopy (<xref rid="fig1" ref-type="fig">Figure 1H</xref>), in contrast to other HVAs <sup><xref ref-type="bibr" rid="c20">20</xref></sup>. Some area A neurons showed activity by stimulation of multiple body parts such as trunk and tail, or trunk and whiskers, which is similar to area 5 of macaque <sup><xref ref-type="bibr" rid="c44">44</xref></sup> (<xref rid="fig1" ref-type="fig">Figure 1F</xref>). Thus, area A integrates multiple somatosensory inputs, potentially contributing to body coordination (e.g., posture) <sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c44">44</xref></sup>. Some neurons in AM were activated by tactile stimuli, and again this activity lacked somatotopy. Based on the multisensory responsiveness of areas A, AM, and RL <sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c45">45</xref>,<xref ref-type="bibr" rid="c46">46</xref></sup>, we demarcate the PPC as a three-module structure, consisting of AM, RL, and A (as defined above) (<xref rid="fig1" ref-type="fig">Figure 1I</xref>). Note that these areas overlap with HVAs.</p>
</sec>
<sec id="s2b">
<title>Visually guided task</title>
<p>After precisely mapping PPC and adjacent cortical areas, we examined layer 2/3 neuronal activity during behavior with single neuron precision across multiple cortical areas simultaneously with large field-of-view, two-photon calcium imaging (<xref ref-type="supplementary-material" rid="d1e3143">Movie S1</xref>). We used transgenic mice that express a genetically encoded calcium sensor, GCaMP6s, in cortical neurons <sup><xref ref-type="bibr" rid="c47">47</xref></sup>. The field-of-view (<xref ref-type="fig" rid="figs2">Figure S2</xref>) typically covered PPC and surrounding areas including HVAs, V1, S1, and a portion of retrosplenial cortex (RSC).</p>
<p>PPC has been reported to have functions including visually guided decision-making and working memory. Do these functions differ among RL, A, and AM? To address this question, we trained the mice in head-fixed conditions to perform vision-guided and history-guided two-alternative forced-choice tasks (<xref ref-type="fig" rid="figs3">Figure S3A-D</xref>) and imaged the activity of neurons for each task. First, we investigated the distribution of task-related neurons during a visually-guided task (“<bold>vision task</bold>”; <xref rid="fig2" ref-type="fig">Figure 2A-D</xref>). In this task, mice had to lick one of two waterspouts (left or right) based on the visual stimuli, naturalistic video (<xref ref-type="supplementary-material" rid="d1e3150">Movie S2</xref>) or black screen, presented to the left visual field of the mice (imaging in the right hemisphere) (<xref ref-type="fig" rid="figs3">Figure S3A</xref>,<xref ref-type="fig" rid="figs3">B</xref>). This simple task was designed to clearly map sparse responses of neurons to natural video <sup><xref ref-type="bibr" rid="c48">48</xref></sup>, and to map the choice-related activity guided by the visual stimuli at the same time to see segregation or overlapping of the vision and choice-related neurons around PPC.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Vision and choice information are merged in AM.</title>
<p><bold>A.</bold> Schematic illustration of a vision-guided choice task. <bold>B.</bold> The lick-ports were approached after a 4-s sampling period (SP). The mouse was given a reward (water) or a punishment (air-puff) based on the licking direction during response period (RP). ITI, Inter-trial-interval. <bold>C.</bold> Rules of vision and history task. <bold>D.</bold> Four example raw traces (black) and inferred spikes (green) during the task. <bold>E.</bold> Representative vision neurons (ROI 1-4 in <bold>I</bold>). <bold>F.</bold> Representative choice neuron (ROI 5-8 in <bold>i</bold>) and a non-selective neuron (ROI 9). <bold>G.</bold> Schematic of our encoding model. <bold>H.</bold> <italic>t</italic>-SNE result of single neuron activities (left). Task-related neurons were mapped onto the <italic>t</italic>-SNE (right). <bold>I</bold>. All active neurons in the vision task (left) and the <italic>t</italic>-values of video (red), blank (orange), left choice (cyan), and right choice (purple) are overlaid (middle). The ratio of vision and choice neurons are also shown (right). <bold>J.</bold> Map of Association index (harmonic mean of vision and choice neurons.) <bold>K</bold>. Decoding accuracy of choice by activity of 50 neurons from eight areas before and after the choice were plotted (left). The decoding accuracies 1s before (right bottom, gray shade; p=0.13; one-way ANOVA) and just after (right top, blue shade; p=3.6 × 10<sup>-</sup> <sup><xref ref-type="bibr" rid="c9">9</xref></sup>; one-way ANOVA) the choice was shown. <bold>L.</bold> The activity of vision neurons was averaged and aligned by the order. <bold>M.</bold> The time after the onset of the video was precisely decoded with a population of vision neurons in panel <bold>L. N.</bold> Decoding accuracy of time during video presentation (top; p=1.1 × 10<sup>-<xref ref-type="bibr" rid="c15">15</xref></sup>; one-way ANOVA) and black screen presentation (bottom; p=0.013; one-way ANOVA) were plotted. <bold>O.</bold> Decoding accuracies of time in video presentation and choice direction indicate that AM would be the best position for associating these two signals. Error bars, mean ± s.e.m. in <bold>E</bold> and <bold>F</bold>, 95% confidence interval in <bold>K</bold> and <bold>N</bold>. Scale bar: 1mm.</p></caption>
<graphic xlink:href="555017v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Large field-of-view calcium imaging during the task enabled us to simultaneously observe ∼10<sup>4</sup> neurons (n = 5 mice, total of 10 sessions, 47162 neurons). Neurons were robustly activated by specific frames in the naturalistic video, by the black screen, or by left or right choice (<xref rid="fig2" ref-type="fig">Figure 2E</xref>,<xref ref-type="fig" rid="fig2"><bold>F</bold></xref>; <xref ref-type="supplementary-material" rid="d1e3157">Movie S3</xref>). Notably, some PPC neurons showed complex dynamics rather than simply encoding specific visual or choice information, indicating integrative roles. To quantitatively identify and map the functional properties of each neuron, we constructed a simple linear regression model (<bold>“encoding model”,</bold> <xref rid="fig2" ref-type="fig">Figure 2G</xref>). In the encoding model, the activity of each neuron was fitted by a weighted sum of the external control parameters such as a video frame and behavior parameters such as choice and reward directions. The linear model was sufficient to estimate the correlation with neural activity, since there is little correlation between task parameters such as visual stimuli, choice, reward, and inter-trial-intervals (<bold>materials and methods</bold>). The statistical significance (p&lt;0.001) of each parameter corresponding to visual stimulus and chosen direction was used to identify the vision and choice-related neurons, respectively. The functional labels revealed by this encoding model were consistent with the <italic>t</italic>-SNE clusters, indicating the validity of the encoding model (<xref rid="fig2" ref-type="fig">Figure 2H</xref>; <xref ref-type="fig" rid="figs4">Figure S4B</xref><bold>; material and methods</bold>). The neurons which were selective to video or black screen (<bold>“vision neurons”</bold>) were found in V1 and HVAs including AM, while the neurons which were selective to the chosen direction (<bold>“choice neurons”</bold>) were distributed in S1t and A (<xref rid="fig2" ref-type="fig">Figure 2I</xref>). Interestingly, the neurons that activated during the choice period irrespective of the choice directions were distributed broadly (<xref ref-type="fig" rid="figs4">Figure S4A</xref>). The vision and choice neurons were distributed mostly exclusively, but some overlapping was found around AM as revealed by mapping of the harmonic mean of the percentage of vision and choice neurons (<bold>“association index”</bold>) (<xref rid="fig2" ref-type="fig">Figure 2J</xref>; <xref ref-type="fig" rid="figs4">Figure S4A</xref>; <xref ref-type="supplementary-material" rid="d1e3157">Movie S3</xref>). This indicates that the AM would be suitable for integrating vision and choice information rather than the A, RL or the other areas around PPC.</p>
<p>The PPC neurons had been thought to be important for decision making, but recently it has also been suggested that the PPC neurons store the history information rather than deciding the upcoming choice. To analyze whether the neurons around PPC represent the upcoming choice or the past choice in our visually guided task, we decoded the choice direction with the activity of 50 neurons before and after the choice (<xref rid="fig2" ref-type="fig">Figure 2K</xref>). The decoding accuracies of A and S1 were higher than the other areas at the choice timing and they stored the choice information even 5 s after the choice. On the other hand, the decoding accuracy of the choice 1 s before the choice was very low in any areas recorded. Thus, neurons in areas A and S1t reflect the choice direction immediately after the choice and store it over 5 s. In addition, vision neurons preferred a specific time of the video frame resulting in the vision neurons sequentially firing during the sampling period (<xref rid="fig2" ref-type="fig">Figure 2L</xref>). Decoding analysis showed that the vision neurons precisely possessed the temporal information of the video frame (<xref rid="fig2" ref-type="fig">Figure 2M</xref>). Such precise decoding of time during the sampling period was not observed in black screen trials (<xref rid="fig2" ref-type="fig">Figure 2N</xref>). From the two decoding performances, we divided the imaged areas into four groups (<xref rid="fig2" ref-type="fig">Figure 2O</xref>). Note that area AM is the most suitable area for the association of these two parameters, as it possesses a substantial amount of decodable information about both parameters.</p>
</sec>
<sec id="s2c">
<title>Choice history guided task</title>
<p>Next, we asked whether and where choice history information can be stored in PPC and surrounding areas. We designed a choice history-dependent task (“<bold>history task</bold>”) with the same set up as the vision task (<xref rid="fig2" ref-type="fig">Figure 2A-C</xref>). In this task, the mice had to choose the opposite side of the previous choice. Mice had to store the direction of the choice for 6-10 seconds from the previous choice to the next choice including an inter-trial-interval (<bold>ITI</bold>) and a sampling period. Since the visual stimuli were randomly presented during the sampling period, the mice had to ignore the visual stimuli. This task was designed to map the history neurons in contrast to the choice neurons and the vision neurons around PPC.</p>
<p>Large FOV calcium imaging during the history task (n=4 mice, 10 sessions, 27059 neurons) revealed that neurons in A and S1t maintained their activity during the ITI in a past-choice-specific manner, indicating that these neurons stored the choice history for making the next choice (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). We used the same encoding model as in the vision task to identify the neurons representing the history of the previous choice (<bold>“history neurons”</bold>) as well as choice and vision neurons. Again, the functional labels revealed by this encoding model were consistent with the <italic>t</italic>-SNE clusters, indicating the validity of the encoding model (<xref rid="fig3" ref-type="fig">Figure 3B</xref>; <xref ref-type="fig" rid="figs4">Figure S4C</xref>,<xref ref-type="fig" rid="figs4">D</xref>). We found that neurons in area A had a significantly larger ratio of history neurons than all the other areas including S1t that had more choice neurons, and that neurons in AM exhibit large association index (<xref rid="fig3" ref-type="fig">Figure 3C, D</xref>). We also found more history neurons in the history task in A compared to those in the vision task (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). Thus, A exhibited a task-specific functional change, preferentially representing a parameter when it is relevant, despite being available in both tasks.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Robust representation of choice history in A.</title>
<p><bold>A.</bold> The representative history neurons. Numbers correspond to that of panel <bold>B</bold> and <bold>C. B.</bold> The <italic>t</italic>-SNE result for the single neuron activity (left). The right panel shows that each cluster of <italic>t</italic>-SNE has the distinct functional types defined by the encoding model. <bold>C.</bold> Position of the single neurons imaged in history task (left) and the percentages of history, choice, and vision neurons. <bold>D.</bold> The percentage of history neuron and the association index were overlaid. Note that these two roughly corresponded to A and AM, respectively. <bold>E.</bold> The percentage of history neurons in A was higher in the history task than in the vision task (top; p=4.1 × 10<sup>-<xref ref-type="bibr" rid="c4">4</xref></sup>; chi-squared test). The percentage of vision neurons in A was not different (bottom p=0.61; chi-squared test). <bold>F.</bold> Decoding accuracy of choice, outcome, and visual stimuli by the activity of 20 neurons from each area before and after the choice onset, reward delivery, and the end of the visual stimuli, respectively. Line colors corresponded to the areas shown in panel <bold>G. G.</bold> Decoding accuracy of the choice (p=2.2 × 10<sup>-</sup><sup><xref ref-type="bibr" rid="c13">13</xref></sup>; one-way ANOVA), outcome (p=0.13; one-way ANOVA), and visual stimuli (p=0.094; one-way ANOVA) at the shaded time in panel <bold>F</bold> were compared for each area. <bold>h.</bold> Decoding accuracy of the previous choice (thick lines) and the next choice (thin lines) as a function of time relative to the next choice. Note that both A and S1t had larger information on the previous choice until 1 s before starting the next choice than the next choice. <bold>i.</bold> The decoding accuracy of the next choice from A activity was significantly larger when the next choice was correct than when the next choice was incorrect (Wilcoxon signed-rank test with Bonferroni correction). <bold>J.</bold> The activity of simultaneously imaged history neurons was aligned by their peak time. The continuous sequential activity can be observed in the two consecutive trials. <bold>K.</bold> jPCA plot of the history neurons shown in panel <bold>J</bold> (right), that of A neurons (middle), and that of AM neurons (right). <bold>L.</bold> The R<sup><xref ref-type="bibr" rid="c2">2</xref></sup> of Mskew indicating how rotational dynamics was dominant was compared (p=5.1 × 10<sup>-</sup><sup><xref ref-type="bibr" rid="c33">33</xref></sup>; one-way ANOVA). <bold>M.</bold> The R<sup><xref ref-type="bibr" rid="c2">2</xref></sup> of Mskew was significantly positively correlated in A and S1t but not in AM (*** p&lt;10<sup>-</sup><sup><xref ref-type="bibr" rid="c6">6</xref></sup>; ** p&lt;0.01; <italic>t</italic>-test). <bold>N.</bold> The decoded rate of time after the choice using neurons in A (top) and AM (bottom). Note that cross-choice mistakes (*) were rarer in A than in AM. <bold>O.</bold> Decoding accuracy of time with the correct choice (top; p=5.6 × 10<sup>-</sup><sup><xref ref-type="bibr" rid="c14">14</xref></sup>; one-way ANOVA) and decoding rate of time with an incorrect choice (bottom; p=8.5 × 10<sup>-</sup><sup><xref ref-type="bibr" rid="c10">10</xref></sup>; one-way ANOVA). Error bars, mean ± s.e.m. in <bold>i</bold>, 95% confidence interval in <bold>G. M,</bold> and <bold>O</bold>. Scale bar: 1mm.</p></caption>
<graphic xlink:href="555017v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>PPC neurons are known to store the information of not only the choice history but also the outcome and sensory histories <sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c13">13</xref></sup>. We compared these candidates by decoding analysis (<xref rid="fig3" ref-type="fig">Figure 3F</xref>,<xref ref-type="fig" rid="fig3"><bold>G</bold></xref>). Although the outcome and visual information rapidly decreased in all areas after a trial, the choice information was strongly represented in A and S1t, and stored during the ITI, especially in A. We confirmed that the choice history information was about the past rather than about the future in both A and S1t (<xref rid="fig3" ref-type="fig">Figure 3H</xref>). We also asked whether the history neurons in A contributed to the working memory. When the next trials were correct, the decoding accuracy from history neurons in A during the ITI stayed higher than when the next trials were incorrect (<xref rid="fig3" ref-type="fig">Figure 3I</xref>). Thus, the choice history information possessed by history neurons led the animal to the correct choice in following trials. In other words, the history neurons exhibited properties of working memory. What kind of dynamics did the history neurons use to store this choice history information? The population activity of the history neurons indicated that each history neurons had a specific time of peak activity, resulting in the sequential activity after the choice (<xref rid="fig3" ref-type="fig">Figure 3J</xref>). In this task, left and right selection are alternated, so the activity of the history neuron is a sequence that repeats in two consecutive trials. We used jPCA<sup><xref ref-type="bibr" rid="c49">49</xref></sup> to visualize and quantify this activity pattern (<xref rid="fig3" ref-type="fig">Figure 3K</xref>). As expected, the population of history neurons showed one-round rotational dynamics in two trials, and the same was true for the A population, but the AM population did not show rotational dynamics. We quantified the extent to which population activity can be approximated by the rotational component (R<sup><xref ref-type="bibr" rid="c2">2</xref></sup> of M<sub>skew</sub>; <bold>materials and methods</bold>) and found that the value in A and S1t was not only higher than that in the other areas (<xref rid="fig3" ref-type="fig">Figure 3L</xref>) but also significantly correlated with the correct rate of the session (<xref rid="fig3" ref-type="fig">Figure 3M</xref>). Interestingly, the sequential activity was also observed in AM, but it was less specific to the choice (<xref rid="fig3" ref-type="fig">Figure 3N</xref>), resulting in the decoding accuracy of time after a specific choice being than in A (<xref rid="fig3" ref-type="fig">Figure 3O</xref>). These results indicate that the history-related activities of A and S1t encode past behavior histories by creating long sequences, and that these rotational dynamics lead to behaviors that reflect past histories.</p>
</sec>
<sec id="s2d">
<title>Representation of posture</title>
<p>Neuronal activity in S1 typically represents simple somatosensory information, and thus the history neurons in S1t may be a consequence of habitual changes of the posture of the animal after making a choice in a trial. In this case, history neurons may not store the choice history internally, but they may simply reflect the posture of the mouse <sup><xref ref-type="bibr" rid="c14">14</xref></sup>. To test this possibility, we analyzed the posture of the mice performing the tasks using a deep-learning based tracking method (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). We found that the angle of the tail was significantly different from the baseline values several seconds after the RP (<xref ref-type="fig" rid="figs5">Figure S5A</xref>). Therefore, the position of the tail was useful information for the next choice in the history task. Overall, although the body was tightly restricted in the body-holder, mice still moved the tail and the paws in relation to the choice, the choice history, and the choice timing. Some parameters could potentially explain apparent task-related activities of neurons, which had been largely ignored in previous studies.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Posture related neurons and robust representation of the history neurons.</title>
<p><bold>A.</bold> Snapshots from the cameras. The green, magenta, and orange circles indicate that the estimated positions of the left forepaw, the right forepaw, and the tail, respectively, when each part crossed the edge of the body holder. Each line was drawn between each estimated body part and the center of the body holder. Blue circles indicate the edge of the body holder. <bold>B</bold>. Schematic of the regression model for estimation of posture/movement related activity. <bold>C</bold>. Pseudo-R2 values were mapped for the left forepaw, right forepaw, the tail and the pupil. <bold>D.</bold> The body holder was randomly rotated during the history task to prevent the mice from keeping choice history as posture. <bold>E</bold>. Representative holder angle-related neurons. Numbers correspond to their location in the left panel. Neuron 1 was holder-related but not to reward direction, whereas neuron 2 was related to both the holder and the reward. <bold>F</bold>. The <italic>t</italic>-values of history neurons and holder-related neurons were mapped. <bold>G</bold>. Decoding accuracy for the holder direction and the choice history in each area was compared. Note that the A and S1t had precise information for both. <bold>H</bold>. The <italic>t</italic>-values for the history-related activity and for the holder angle in each history neuron were plotted. No significant relationship was observed. <bold>I</bold>. The percentage of the history neurons in A (green) and AM (magenta) was not different between the history task and the task with random holder rotation (p=0.15 in A; p=0.38 in AM; chi-squared test). <bold>J</bold>. The decoding accuracy of choice history in A (green) and AM (magenta) was not significantly different regardless of the holder movements (p=0.87 in A; p=0.95 in AM; Wilcoxon’s rank sum test). Scale bar: 1mm.</p></caption>
<graphic xlink:href="555017v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We searched for relationships between the activity of neurons and these posture-related parameters during the history task. We constructed a linear model based on these parameters to fit the activity of each neuron during the history task in a similar way to the encoding model for defining task-related neurons (<xref rid="fig4" ref-type="fig">Figure 4B</xref>, <bold>materials and methods</bold>). We found that the neurons related to the tail and forepaws were similarly distributed around the parietal cortex including S1 and A, while the pupil-size related neurons were mapped around visual areas (<xref rid="fig4" ref-type="fig">Figure 4C</xref>), although pupil size and pupil size-related neurons would just respond to luminance changes. A and S1 had a comparable ratio of neurons that were related to the tail and the paws. However, it remained unclear whether the activity of history neurons represented an internally stored choice history variable, or just reflected the sensory information that happened to correlate with the choice on the next trial.</p>
<p>If neurons in A retain history information by receiving posture information, perturbing the posture information should decrease A’s history information. To test this, we prevented the mouse from maintaining its posture during the ITI by randomly rotating the body holder (+24 or -24 degrees), forcibly introducing a physical disturbance making it difficult for the mouse to retain information of the previous choice by posture (<xref rid="fig4" ref-type="fig">Figure 4D</xref>). Three mice successfully performed the task in this new condition. We conducted a regression analysis and mapped the neurons reflecting the angle of the holder (“<bold>holder neurons</bold>”; <xref ref-type="fig" rid="fig4">Figure 4F</xref>) and found that they had similar distributions to history neurons (n=3 mice). Decoding analysis also showed the close relationship between history and posture information at the area level (<xref rid="fig4" ref-type="fig">Figure 4G</xref>). By contrast, the representation of the angle and representation of the choice history were not correlated at the single neuron level (<xref rid="fig4" ref-type="fig">Figure 4H</xref>). This result is evidence that the neurons regarding two different spatial features, the direction of choice and angle of the body, may share some neuronal resources in A and S1t, but these neurons process the information independently. Consistently, the percentage of history neurons and decoding accuracy of choice history were almost the same regardless of the holder movement in both A and AM (<xref rid="fig4" ref-type="fig">Figure 4I</xref>,<xref ref-type="fig" rid="fig4"><bold>J</bold></xref>). Thus, history neurons represent an internally stored choice history variable, independent of the posture. Taken together, the history task and posture-perturbation experiments demonstrated that the most specific function of neurons in area A is storing choice history that contributes to the next choice.</p>
</sec>
<sec id="s2e">
<title>Mesoscale correlation structure</title>
<p>PPC has anatomical connections with a variety of surrounding and remote areas <sup><xref ref-type="bibr" rid="c22">22</xref></sup>, but the functional correlation between those areas has not been extensively examined. In particular, based on the critical functional differences between A and AM that we found, A and AM may belong to distinct cortical networks that consist of different sets of densely interacting cortical areas. To address this question, we focused on the correlation in trial-to-trial variability, <italic>r<sub>t</sub></italic>. In the following, we analyzed <italic>r<sub>t</sub></italic> on two different levels: pairs of single-neuron activities and pairs of population activities.</p>
<p>First, to obtain <italic>r<sub>t</sub></italic> between a pair of single neurons, <italic>r<sub>t_single</sub>,</italic> we averaged the activity of individual neurons over the sampling period by trial type, and subtracted this from the trial-wise activity. The trial types consisted of four sets of pairs of stimuli and responses, that is, the video stimulation and left choice, the video stimulation and right choice, the black screen and left choice, and the black screen and right choice. This operation extracted the fluctuation components of single-neuron activity that are independent of the trial types. Correlation of these fluctuations was defined as <italic>r<sub>t_single.</sub></italic> The average value of <italic>r<sub>t_single</sub></italic> was small (∼0.01) but larger than 0 even when the distance between neurons was more than 2 mm (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). Notably, a small number of pairs had a large positive correlation irrespective of distance (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). In fact, kurtosis (0 in the normal distribution) of correlation after Fisher’s z-transformation was a very large value of 3.77 (p &lt; 10<sup>-<xref ref-type="bibr" rid="c6">6</xref></sup>). Thus, we focused on neuron pairs with <italic>r<sub>t_single</sub></italic> &gt; 0.3 (shaded areas in <xref rid="fig5" ref-type="fig">Figure 5B</xref>) that were very rare in shuffled pairs in the following analysis to capture the dominant correlation structures.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Mesoscale correlation of PPC and surrounding areas.</title><p><bold>A</bold>. Average of trial-to-trial correlation (<italic>rt</italic>_single) between a PPC neuron and a neuron in the other areas was plotted against the distance between the neuron pair. The correlation was small but significantly larger than zero regardless of the distances. <bold>B.</bold> Log-scaled histogram of the <italic>rt</italic>_single. The shaded area indicates a large positive correlation (&gt; 0.3). <bold>C</bold>. The coupling neurons whose <italic>rt</italic>_single with seed neurons (large circle) in A (left) and AM (right) was larger than 0.3 were shown with the same color as the seed neuron. Most neurons coupling to the A neurons were in the S1t, while the neurons coupling with the AM neurons were spreading but the spreading area was specific to each seed neuron. <bold>D.</bold> Schematic of the K-means (k=6) clustering of neurons in A based on the ratio vector, <bold><italic>R</italic></bold>. This vector, given for each neuron, consists of the ratio of the number of coupling neurons (<italic>rt_single</italic> &gt;0.3) in the seven areas other than A. In this case, for example, <italic>R<sup>i</sup>A</italic>, the vector of the <italic>i</italic>-th neuron in A, had the largest value of 0.5 in the fifth element, which indicates this neuron had half the number of coupling neurons in the RSP. K-means clustering of the ratio vectors can identify the clusters of A neurons sharing the distribution of the highly correlated neurons. The analysis was done for all neurons in AM instead of A as well. <bold>E.</bold> Polar plots of the preference vectors of A (left) and AM (right). The preference vector is mean ratio vector of each cluster. <bold>F.</bold> Angles between any pair of the preference vectors which were obtained 1,000 times with randomly selected neurons. <bold>G.</bold> The relationship between the preferred area of single neurons in A or AM and their task-relatedness was compared. For example, significantly large number of A neurons preferentially coupling with the S1t neurons were choice neurons or history neurons whereas those coupling with the AM neurons were vision neurons. <bold>H.</bold> <italic>CCt</italic> between pairs of eight areas during vision task and history task was highly correlated (r=0.95). <bold>I.</bold> The <italic>CCt</italic> between an area to the other areas were compared. A and AM had the two largest <italic>CCt</italic> (top; p=1.7 ×10<sup>-<xref ref-type="bibr" rid="c6">6</xref></sup>; one-way ANOVA), which was not the case when coupling neurons were eliminated (bottom; p=6.7 × 10<sup>-<xref ref-type="bibr" rid="c7">7</xref></sup>; one-way ANOVA). <bold>J.</bold> <italic>CCt</italic> as a function of its dimension. <italic>CCt</italic> from AM were uniform regardless of the interacting areas whereas A had large <italic>CCt</italic> with S1t. Error bars, 95% confidence interval.</p></caption>
<graphic xlink:href="555017v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In which regions are neurons tightly functionally coupled to neurons in AM and A? When mapping the “coupling” neurons in surrounding areas possessing <italic>r<sub>t_single</sub></italic> values &gt; 0.3 with randomly chosen “seed” AM or A neurons, consistent spatial patterns emerged (<bold>“coupling neurons”, </bold><xref ref-type="fig" rid="figs6">Figure S6A</xref>). The coupling neurons were often localized in specific cortical regions. The localized region of the coupling neurons was S1t in most cases when the seed neurons were in A, while the region was variable when the seed neurons were in AM (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). To quantify this property, we used the normalized spatial distribution of coupling neurons for each individual seed neuron of A or AM (<xref rid="fig5" ref-type="fig">Figure 5D</xref>). Then we clustered these vectors using K-means (k=6) clustering and displayed the six mean vectors (“<bold>preference vectors</bold>”) for each cluster. The result showed that interaction with most clusters from seed area A was largely dominated by S1t, while each cluster in AM preferred a specific area such as V1, RSC, A, and S1t (<xref rid="fig5" ref-type="fig">Figure 5E</xref>). These relationships were observed in both vision and history tasks. Thus, AM had multiple interaction channels with surrounding areas, while A interacted with S1t, regardless of tasks. We quantified the similarities of preference vectors by averaging the angles between all pairs of the two vectors, which we referred to as “<bold>coupling diversity</bold>”. We confirmed that the difference in the coupling diversity between A and AM was statistically significant by repeating K-means clustering with randomly chosen neurons in A or AM (<xref rid="fig5" ref-type="fig">Figure 5F</xref>; <bold>materials and methods</bold>).</p>
<p>Is there a relationship between the interarea interaction and functional types in the task? We termed the area where the largest number of coupling neurons are located the “<bold>preferred area</bold>” and examined the relationship between the preferred area and task relevance. We found that both A and AM neurons with S1t in their preferred areas were significantly more likely to be choice and history neurons. Neurons in area A with visual areas (RL, V1, PM, or RSC) in their preferred areas were significantly more likely to be vision neurons (<xref rid="fig5" ref-type="fig">Figure 5G</xref>). Moreover, AM neurons preferring A had significant task relevance for all vision, choice, and history. Thus, A and AM neurons determined their functional types by interarea coupling at least in part.</p>
<p>Next, to investigate <italic>r<sub>t</sub></italic> of the population activity (<italic>r<sub>t_population</sub></italic>), we first reduced the dimension of population activity in each area into 10 by using PCA (principal component analysis) (<xref ref-type="fig" rid="figs6">Figure S6B</xref>,<xref ref-type="fig" rid="figs6">C</xref>). Then, “fluctuation activity” was again calculated for each dimension by trial type similar to the single neuron case above, this time representing noise in population activation patterns. We applied CCA (canonical correlation analysis) to each pair of areas and obtained an average of top 10 canonical correlation (<italic>CC<sub>t</sub></italic>) as <italic>r<sub>t_population</sub></italic> The <italic>CC<sub>t</sub></italic> structure between areas was similar across task types (<xref rid="fig5" ref-type="fig">Figure 5H</xref>) indicating that this structure reflects the underlying functional connectivity independent of the task. The <italic>CC<sub>t</sub></italic> between A and S1t was the largest among all the pairs (<xref rid="fig5" ref-type="fig">Figure 5H</xref>), whereas when the <italic>CC<sub>t</sub></italic> was averaged across all connections for each area, A and AM had the largest and second largest <italic>CC<sub>t</sub></italic>, respectively (<xref rid="fig5" ref-type="fig">Figure 5I</xref>). The dominance in <italic>CC<sub>t</sub></italic> in A and AM disappeared when the neurons with <italic>r<sub>t,single</sub></italic> &gt;0.3 were removed. Notably, the <italic>CC<sub>t</sub></italic> of AM and the other areas was uniform regardless of the paired areas across all 10 dimensions (<xref rid="fig5" ref-type="fig">Figure 5J</xref>). Thus, area AM is an integration hub of interareal communication, whereas A simply coupled with S1t, and such correlation structure at the population level critically depends on this subset of neurons.</p>
</sec>
<sec id="s2f">
<title>Relationship between mesoscale functional correlation and anatomical connections</title>
<p>Next, we asked whether distinct network level interaction of A and AM can be seen in the neuropil activity level observed in the same two-photon calcium imaging data. We examined the correlation of trial-to-trial “fluctuations” in neuropil activity (<italic>r<sub>t_neuropil</sub></italic>). The fluctuation of the neuropil activity was often localized in a specific area or a set of areas (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). Consistently, K-means clustering revealed that the cluster boundaries of the neuropil activity approximately corresponded to the area boundaries (<xref rid="fig6" ref-type="fig">Figure 6B</xref>). <italic>r<sub>t_neuropil</sub></italic> was similar regardless of the task types (<xref rid="fig6" ref-type="fig">Figure 6C</xref>), so we combined these data for the following analysis. The neuropil activity in AM well correlated with PM, RSC, V1, RL, and A, whereas that of A highly correlated with S1t (<xref rid="fig6" ref-type="fig">Figure 6D</xref>). The neuropil activity is known to reflect the local average of input in the imaged location <sup><xref ref-type="bibr" rid="c50">50</xref></sup>. Indeed, the trial-to-trial variation of a neuropil activity could be approximated by the average of 1,000 - 10,000 neurons within several hundred micrometers from the center (<xref ref-type="fig" rid="figs7">Figure S7</xref>). These data indicate that the correlation structures of average activity of neurons differs between A and AM.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Comparison of functional and anatomical correlation structures.</title>
<p><bold>A.</bold> Trial-to-trial fluctuation of neuropil activity of the mouse 2. Sixteen trials presenting the natural video and when the left choice was made were shown. <bold>B.</bold> K-means clustering of each background pixels (k=8) from two mice resembling the functionally defined areas was shown. <bold>C.</bold> Correlation between background trial-to-trial activity (<italic>rt,neuropil</italic>) in pairs of areas was very similar regardless of the task types (<italic>r</italic>=0.95). <bold>D.</bold> Seed correlation analysis of neuropil activity for A and AM. <bold>E.</bold> Injection sites for cortico-cortical projections (blue) and thalamo-cortical projections (magenta). <bold>F.</bold> Two examples of projection sources (black circles) for a target site (red circle). <bold>G.</bold> A binary matrix Y (<italic>Npix × Nin</italic>j). Each column consists of the projection targets of each injection experiment. Two rows indicated by rectangles correspond to the sets of projection sources for the two target sites shown in panel <bold>F. H.</bold> <italic>ranatomy</italic> between each one of the pairs of 8 areas. <bold>I.</bold> <italic>ranatomy</italic> precisely predict <italic>rt,neuropil</italic>. <bold>J.</bold> Correlation between <italic>rt,neuropil</italic> and <italic>ranatomy</italic> with corico-cortical or thalamo-cortical projection was plotted against the number of injection sites used for the analysis. <bold>K.</bold> <italic>ranatomy</italic> map when the seed is in AI and in AM. Note that these maps resemble panel <bold>D</bold>. <bold>L.</bold> Moderate correlation between <italic>ranatomy</italic> and <italic>CCt</italic> (<italic>r</italic>=0.60). <bold>M.</bold> The first (largest) <italic>CCt</italic> component was the most inconsistent with the anatomical data. <bold>N</bold>. Information sharing index was plotted against the dimension of <italic>CCt</italic>. The first component had the largest information sharing index for all areas. <bold>O.</bold> Global information sharing index was plotted against the dimension of <italic>CCt</italic>. The first component had the largest global information sharing index for all areas. Error bars, mean ± s.e.m.</p></caption>
<graphic xlink:href="555017v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Next, we utilized Allen connectivity atlas where the mesoscale connectivity between cortical and subcortical areas are comprehensively described. We used results of the anterograde tracer experiments where the adeno associated virus that delivers the fluorescent protein was injected in cortex or thalamus (<xref rid="fig6" ref-type="fig">Figure 6E</xref>). We found that among cortico-cortical interactions only S1t clearly projected to A more than AM, whereas visual cortices (V1, AL, PM) and frontal cortices (orbitofrontal cortex, dorsal and ventral anterior cingulate cortex) preferentially projected to AM rather than to A (<xref ref-type="fig" rid="figs8">Figure S8A</xref>). These data are consistent with our results that A represented sensorimotor parameters together with the S1t, and that AM is an interaction hub which was revealed by the mesoscale correlation analysis.</p>
<p>How does information about anatomical connections relate to functional activity correlations at mesoscale? In general, the activity correlation between two neurons is influenced by the common input <sup><xref ref-type="bibr" rid="c51">51</xref></sup>. In the case of inter-area correlations, we hypothesized that the more common input two areas receive, the higher the activity correlation between the two areas. To address this question, we used the Allen connectivity data to quantify the degree of anatomical input similarity between any two regions as a correlation (<italic>r<sub>anatomy</sub></italic>, <bold>materials and methods;</bold> <xref rid="fig6" ref-type="fig">Figure 6E-G</xref>). Interarea correlation between the anatomical input correlation and the neuropil correlation was surprisingly high (<italic>r</italic> = 0.94, <xref rid="fig6" ref-type="fig">Figure 6H,I</xref>).This correspondence suggests that the mesoscale interarea correlation is determined by the cortico-cortical common input at mesoscale. The high correlation was more evident when both the cortico-cortical and thalamo-cortical projection data were combined (<xref rid="fig6" ref-type="fig">Fig 6J</xref>). Thus, <italic>r<sub>t_neuropil</sub></italic>, which reflects correlation of the average activity of neurons is strictly determined by <italic>r<sub>anatomy</sub></italic>.</p>
<p>Does this explain the difference in correlation structure between A and AM? The seed analysis reveals that A and AM have distinct patterns of anatomical input (<xref rid="fig6" ref-type="fig">Figure 6K</xref>) and the patterns highly resembled the neuropil correlations seen earlier (<xref ref-type="fig" rid="fig6">Figure6D</xref>). This surprising similarity between anatomical and functional seed correlation was the case regardless of the seed areas (<xref ref-type="fig" rid="figs8">Figure S8B</xref>). Taken together, these data suggest that the distinction of interarea communication in A and AM is led by the cortico-cortical and thalamo-cortical inputs.</p>
<p>Can the anatomical input correlation predict the correlation between population activities? The correlation between <italic>CC<sub>t</sub></italic> and <italic>r<sub>anatomy</sub></italic> was moderate (<italic>r</italic> = 0.60, <xref rid="fig6" ref-type="fig">Figure 6L</xref>). This moderate correlation did not change when the coupling neurons were eliminated (<italic>r</italic> = 0.61). Interestingly, the largest canonical component was the most unpredictable from the anatomical data (<xref rid="fig6" ref-type="fig">Figure 6M</xref>). Thus, the canonical correlation analysis adds important information about population dynamics of neurons that is unpredictable by our analysis on the anatomical input correlation.</p>
<p>The correlation between cortical areas can be affected by globally shared activity reflecting brain states or neuromodulator levels. To evaluate the contribution of global activity on the canonical correlation between areas, we first compared the canonical coefficient vectors (CCV). We found that the first CCV had a similar orientation, regardless of the paired areas (<xref ref-type="fig" rid="fig6">Figure6N</xref>). This indicates that the largest components of correlated activity in the CCA analysis are globally shared fluctuation. We also directly evaluated the correlated activity components across all 8 areas with generalized canonical correlation analysis. The first CCV also had a similar orientation to the first generalized canonical coefficient vector (GCCV) (<xref rid="fig6" ref-type="fig">Figure 6O</xref>). These results indicate that the 1st canonical component reflects a global correlation across all cortical areas imaged. Such global correlation may be caused by factors other than corticocortical or thalamocortical inputs. We also confirmed the robustness of these results by repeating analyses using only the 40% highly active neurons after denoising with non-negative deconvolution (36828 out of 91397 neurons; <xref ref-type="fig" rid="figs9">Figure S9</xref>).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study, we functionally characterized neurons in areas A and AM, two components of mouse PPC. Imaging during a vision-guided task revealed that neurons in area A showed choice-related activity, while neurons in area AM represented both choice and visual information. Imaging during a history-guided task demonstrated that area A robustly represented choice history with choice-specific activity dynamics. Area A also represented the body posture of the mouse, cooperatively with area S1t. Experiments with perturbation of posture suggested that the choice history-related neurons in A and S1t stored a working memory for the upcoming choice. Analysis of the correlation of trial-to-trial variability indicated that, regardless of the tasks, the interactions of neurons in areas A and S1t were always large and stable, while the interactions of neurons in area AM and the surrounding areas were widespread and shared information specific to each area. These results were consistent with the patterns of corticocortical connections, revealed by both functional and anatomical data. Overall, we determined the functional differences between two PPC regions, areas A and AM, and characterized their distinct mesoscale interactions.</p>
<p>It is generally believed that the parietal association cortex evolved largely after the emergence of primates<sup><xref ref-type="bibr" rid="c52">52</xref></sup>. It has been unclear whether PPC in mice or rats is useful for understanding parietal association cortex in primates including humans. By contrast, there are commonly observed distinctions within the PPC: Area 5 and 7 in cats <sup><xref ref-type="bibr" rid="c53">53</xref></sup>, rostral and caudal PPC of ferrets<sup><xref ref-type="bibr" rid="c54">54</xref></sup>, galagoes <sup><xref ref-type="bibr" rid="c52">52</xref></sup>, squirrel monkeys <sup><xref ref-type="bibr" rid="c55">55</xref></sup>, and titi monkeys <sup><xref ref-type="bibr" rid="c56">56</xref></sup> are similar in the sense that visual-to-somatosensory gradient can be observed along the posterior to anterior axis. Macaque PPC also has a similar gradient along rostrocaudal direction <sup><xref ref-type="bibr" rid="c17">17</xref>–<xref ref-type="bibr" rid="c19">19</xref></sup>. In addition, the somatotopic localization clearly observed in the somatosensory cortex is less clear in area 5 (most anterior part of PPC), where neurons responsive to multiple joints and anterior-posterior limb-tail interactions appear <sup><xref ref-type="bibr" rid="c44">44</xref></sup>, similar to responses of area A neurons (<xref rid="fig1" ref-type="fig">Figure 1G</xref>). Thus, functional differentiation within PPC commonly observed in carnivores, prosimians, New-World monkeys and Old-World monkey was also evident in a rodent. This homology increases the significance of examining the functional differentiation in the mouse PPC in understanding the circuit principle of mammalian parietal association cortex.</p>
<p>The functional differences along the rostrocaudal axis within the medial PPC suggest that various other PPC functions reported previously in mice would be mapped somewhere in this region. Neurons representing decision making <sup><xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup>, action history <sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c57">57</xref></sup>, and the posture <sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c15">15</xref></sup> would likely to be observed primarily within area A rather than area AM, because area A neurons were found to be activated by multiple somatosensory stimuli, choice, choice history, and body posture. On the other hand, in vision-guided tasks, the neurons in AM would have more fundamental roles in the integration of visual information for coordinating behavior, compared to neurons in A<sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>. In visual navigation tasks, not only A or AM but also S1t would be highly activated <sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>. Note that RSC had few neurons with selectivity relevant to the tasks in this study, which suggests that selective activity in RSC may emerge only when mice engage in navigation <sup><xref ref-type="bibr" rid="c28">28</xref></sup> or when the mice need to assess the value of an action <sup><xref ref-type="bibr" rid="c58">58</xref></sup>. When vision is critical for choice during navigation and/or determining the value of an action, the neurons in A, AM, and RSC may cooperatively interact <sup><xref ref-type="bibr" rid="c28">28</xref></sup>. Fine functional segregation could also depend on task difficulty and learning stage <sup><xref ref-type="bibr" rid="c8">8</xref></sup>. However, even when the apparent functional representations are mixed, it is essential to precisely map the PPC subregions to see whether the mixed representation can be decomposed into several groups, each of which is mapped in a discrete subregion.</p>
<p>The differences in forms of interareal interaction among neurons in A and AM imply that AM has a more central role in integrating multimodal information, while A is more specific to controlling and/or monitoring behavior. In fact, area AM interacted with many areas through multiple independent channels, as shown in the correlation analysis, at both the single neuron level and the population level. Neurons in area AM may represent multiple types of information in a category-free manner, and thereby its microcircuit, or downstream circuit, could associate them when needed <sup><xref ref-type="bibr" rid="c59">59</xref></sup>. The long-range projections from various cortical areas including frontal cortex, such as the orbitofrontal cortex and anterior cingulate cortex, converge on the AM (<xref ref-type="fig" rid="figs8">Figure S8A</xref>), supporting the view that AM is a key component in integrative, cognitive roles. Neurons in area A had relatively limited variation in long-range anatomical input, but they had much larger behavior-related activity cooperating with area S1. Thus, our results provide a new framework regarding PPC functions: first, a hub of widespread cortico-cortical interactions (area AM) and, second, a core for behavior monitoring and control (area A) in PPC.</p>
<p>There is another new aspect of PPC function in mice that we present here: the representation of posterior S1 next to the anterior border of PPC. We found that clear somatotopic organization was a critical difference between these two areas. By precisely mapping the borders of PPC and S1, we found that S1t and the anterior part of PPC, area A, possessed almost the same rate of neurons representing choice, choice history and posture, even when posture perturbation was added during the task. Direct functional interaction between S1t and A, and/or top-down input from other areas may support the robust representation of history information in these parietal areas. Area A may send behavior-relevant information to S1 to modulate sensory processing as indicated in the top-down effects from secondary motor and somatosensory areas to S1 <sup><xref ref-type="bibr" rid="c60">60</xref>,<xref ref-type="bibr" rid="c61">61</xref></sup>. On the other hand, representation of the task-independent posture signals suggests that S1 sends the sensory signals to A. Thus, A and S1 likely send information bidirectionally. These results significantly impact the interpretation of the activity of neurons in the parietal cortex especially during head-fixed navigation tasks <sup><xref ref-type="bibr" rid="c7">7</xref>–<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup> since the mice always need to coordinate the somatosensory information to maintain body balance during the tasks.</p>
<p>Our data also provided neuropil activity, which may be comparable to the data obtained with widefield, 1-photon calcium imaging that lacks single neuron resolution <sup><xref ref-type="bibr" rid="c62">62</xref></sup>. The correlation of trial-to-trial fluctuations in the neuropil activity revealed a clear correspondence to mesoscale anatomical structures. This correspondence was independent of the tasks and was also observed during passive viewing trials (<xref ref-type="fig" rid="figs8">Figure S8B</xref>). These results imply that shared input from distant areas shapes the spatial structure of cortical activity in general. In other words, the shared input through mesoscale connectivity could predict the brain-wide correlation structures of the functional fluctuations. Since anatomical input correlations can be useful in predicting functional correlations in any cortical area, we developed software that allows for a simple search for seed correlations (<xref ref-type="fig" rid="figs8">Figure S8C</xref>). We also estimated that neuropil activity approximates the activity of thousands of neurons around the region, suggesting that mesoscale anatomical connectivity constrains the relationship between the activities of two populations of neurons in connected cortical areas. Thus, anatomical structures determine mesoscale functional interactions, and thereby constrain overall activity of local individual neurons. This multiscale interaction would serve as an effective constraint when simulating large-scale neural network models <sup><xref ref-type="bibr" rid="c63">63</xref></sup>.</p>
<p>We found that the largest component of canonical correlation reflects globally shared information, which is not well explained by anatomical data. The significant influence of the global component implies that even when understanding the relationship between two areas, it is necessary to consider their relationship to other areas or the entire brain. This global component may involve neuromodulators reflecting states of arousal or motivation <sup><xref ref-type="bibr" rid="c64">64</xref></sup>, slow rhythms that propagate throughout the brain <sup><xref ref-type="bibr" rid="c65">65</xref></sup>, neuronal activity related to movement <sup><xref ref-type="bibr" rid="c66">66</xref></sup>, glial cell activity <sup><xref ref-type="bibr" rid="c67">67</xref></sup>, or combinations thereof.</p>
<p>Why are there two different functional modules in the medial PPC, areas AM and A? One possibility is that AM explores potential relationships between various signals from a number of cortical areas and their outcomes. When AM identifies information important to the current situation, neurons in area A store that information for a few seconds and contribute to the action, or behavior. Thus, area AM must constantly compare various pieces of information without bias, whereas area A must focus on specific information. Such a fundamental difference in the required information processing may separate areas AM and A into different functional networks. The coordination of the two would then create a unique and essential role for animals to survive in an ever-changing world.</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Subjects</title>
<p>All procedures conducted at the University of North Carolina at Chapel Hill were reviewed and approved by the Institutional Animal Care and Use Committee (IACUC) of the University of North Carolina at Chapel Hill, which ensures compliance with the standards of the American Association for Laboratory Animal Science (AALAS). All procedures conducted in the University of California, Santa Barbara were carried out in accordance with the guidelines and regulations of the US Department of Health and Human Services and approved by the Institutional Animal Care and Use Committee at the University of California, Santa Barbara. Mice were imaged at the age of 8-40 weeks. Adult C57BL/6J mice (Jackson labs) or GCaMP6s <sup><xref ref-type="bibr" rid="c47">47</xref></sup> expressing transgenic adult mice of both sex were used in this study. GCaMP6s expressing were induced by the triple crossing of TITL-GCaMP6s line (Allen Institute Ai94), Emx1-Cre line (Jackson Labs #005628) and ROSA:LNL:tTA line (Jackson Labs #011008) <sup><xref ref-type="bibr" rid="c68">68</xref></sup>. Intrinsic signal of equal numbers of males and females C57BL/6J mice were imaged for the P60 and P100 groups (6 mice per time point, 3 of each sex for each time point) when the variability of the visual areas was examined.</p>
</sec>
<sec id="s4b">
<title>Surgical procedure and experimental steps for intrinsic imaging</title>
<p>On the day of imaging, anesthesia was induced with 5% isoflurane. After initial anesthesia induction, 2.5 mg/kg chlorprothixene was administered via intraperitoneal injection. Isoflurane was reduced and maintained between 1-2.5% during surgery. Ophthalmic ointment (Lacri-lube, Allergan) was applied to both eyes and a heating-pad maintained body temperature. The scalp was then resected, exposing the skull over visual cortex. To register functional maps of visual cortical areas to stereotaxic landmarks, three reference points were labeled with a marker near primary visual cortex (0.8 mm anterior to lambdoid suture, 2.3 mm lateral to the midline). Using digital calipers, three separate measurements were taken from the center of each dot to the midline (mediolateral coordinates) and from the center of each dot to the lambdoid suture (rostrocaudal coordinates). For each reference point, the three measurements were averaged. We used the lambdoid suture instead of bregma as a landmark since the position of the bregma is less reliable to define the coordinates of the brain <sup><xref ref-type="bibr" rid="c69">69</xref>–<xref ref-type="bibr" rid="c71">71</xref></sup>. Although it lacks accuracy, one may be able to use the approximate distance between the lambdoid structure and bregma (∼4.0 mm) to convert the coordinates from lambda-based to bregma-based one.</p>
<p>A head-plate was affixed to the skull. A 3.5-4 mm craniotomy was performed over the visual areas when needed. The glass window with a 3 mm diameter was put onto the cortex, and a tissue adhesive was used to seal the space between the glass and the skull. Then, the mouse was transferred to the intrinsic signal optical imaging (ISOI) system. The ophthalmic ointment was removed from the contralateral eye prior to imaging by manually blinking the eye gently. Isoflurane was maintained between 1-1.5% during imaging. The camera was focused on the skull to acquire an image of the dots relative to the head-plate and major vessels visible through the skull. The camera was then adjusted for ISOI imaging and an initial retinotopy set was acquired through the skull <sup><xref ref-type="bibr" rid="c41">41</xref></sup>. After imaging experiments, the mouse was removed from the ISOI rig and given a second dose of 2.5 mg/kg chlorprothixene. The ophthalmic ointment was reapplied to the contralateral eye.</p>
</sec>
<sec id="s4c">
<title>Intrinsic imaging and sensory stimuli</title>
<p>The brain was illuminated with 700 nm light and imaged with a tandem lens macroscope focused 600 μm into the brain from the vasculature. Images were acquired with a 12-bit CCD camera (Dalsa 1M30), frame grabber and custom software (David Ferster, with in-house modifications by Jeffrey Stirman &amp; R. H.) at 30 frames per second. Visual stimuli were presented to the left eye relative to the imaged hemisphere using a Dell LCD monitor (Dell U2711b, 2560 × 1440 pixels, 60 Hz) sitting 20 cm from the mouse. The monitor was tilted towards the mouse 17.5° from vertical to cover the visual field (110° by 75°). All stimuli were generated and presented using MATLAB and Psychtoolbox (<ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/">http://psychtoolbox.org/</ext-link>) <sup><xref ref-type="bibr" rid="c72">72</xref>,<xref ref-type="bibr" rid="c73">73</xref></sup>. All stimuli were modified to correct for visual distortions caused by the flatness of the monitor. Retinotopy was initially mapped by showing the animal a single, square-wave white bar drifting across a black background to identify right V1 and right HVAs (horizontally for azimuth and vertically for elevation). This stimulus was presented for 30 – 50 eight-second-long cycles.</p>
<p>For somatosensory stimuli, the vibration of the membrane of a standard computer speaker was used. The speaker was controlled with the same LabVIEW software. One side of a plastic pipette was attached to the membrane with tape, and the other side of the pipette was attached to a body part (tail, trunk, neck, left ear, left whisker pad, left forelimb and left hindlimb) gently with a tape. As a result, the vibration of the speaker membrane was transferred to the body. Fifty Hz pure tone which gives a 50 Hz sinusoidal movement of the membrane for 0.8 sec vibrated the skin of the targeted body part every 8 seconds. Both cutaneous and proprioceptive responses would be obtained by these stimuli. The speaker sound of 50 Hz tone is far outside the mouse hearing range <sup><xref ref-type="bibr" rid="c74">74</xref></sup>, and we confirmed that the stimulation did not evoke activity in the auditory areas with ISOI mapping. The room light was turned off when a pinna or a whisker pad was stimulated to prevent the pipette from being seen since the vibrating pipette was just in front of the left eye. The pipette was outside the visual field from the mouse eyes when the tail, trunk, neck, left forelimb and left hind limb were stimulated.</p>
</sec>
<sec id="s4d">
<title>Analysis of ISOI data</title>
<p>The images with 12-bit pixel data were binned in software four times temporally and 2 × 2 spatially, resulting in images with 16-bit pixel data. From these binned images, Fourier analysis of each pixel’s time course was used to extract the magnitude and phase of signal modulation at the stimulus frequency for mapping of visually evoked areas. The phase of signal modulation was used to generate maps of the phase of the cortical response, mapping retinotopy of the visual cortex. To identify the areas activated by somatosensory stimuli, the mean intensity of each pixel for 2,000 ms after the stimulus onset was subtracted from the mean intensity of the pixel for 1,000 ms before the stimulus onset across all stimulus trials. The negative value of the pixel implies evoked activity in the pixel. The image with evoked activity was filtered with a Gaussian filter with a standard deviation of 10 pixels, and the pixel with the minimal value was identified as a peak location (downward signal is observed in the activated area). The median value of all pixel was set to the baseline. The mean value of the minimal value and baseline was set the threshold, and the pixels with less than the threshold were regarded as the evoked area.</p>
</sec>
<sec id="s4e">
<title>Image registration for ISOI map</title>
<p>Retinotopic maps were drawn (ImageJ/Fiji) <sup><xref ref-type="bibr" rid="c75">75</xref>–<xref ref-type="bibr" rid="c77">77</xref></sup> based on the first set of imaging data (obtained through the skull) and then modified as needed with the second set (obtained without the skull) to ensure that boundaries between cortical visual areas were accurately identified. These maps were drawn based on color reversals <sup><xref ref-type="bibr" rid="c20">20</xref></sup> and other landmarks present in the retinotopy data. For example, when identifying AL, we look for three stereotyped color reversals to identify the border between AL and LM/LI, AL and V1, and AL and RL. We also used the magnitude maps from these stimuli to ensure that the anterior boundary between AL and non-visual areas is accurately restricted to the area that is visually responsive.</p>
<p>The stereotaxic reference coordinates were first plotted, and the registered to the cortical area location by aligning the plotted coordinates with the reference image (image of the stereotaxic reference coordinates marked on the skull). The reference image was then scaled such that 1 mm in the reference image was the equivalent of 1 mm in the coordinate plot. The reference image was then rotated and moved in X-Y (rigid transform) so that the plotted stereotaxic reference coordinates were in agreement with the marks in the reference image. Using the plotted stereotaxic reference markers, the same image transformations were applied to the retinotopically identified ROIs, producing a set of ROIs registered relative to stereotaxic reference coordinates. This was done for each mouse in the data set. The center of mass for each ROI was computed, and this was then converted to millimeters by measuring the number of pixels in 1 mm and dividing the center location by the conversion factor.</p>
</sec>
<sec id="s4f">
<title>Functional definitions of cortical areas</title>
<p>Previous studies on the demarcation of HVAs based on anatomical tracers have shown that the HVAs comprised AL, RL, A, AM, PM, LM, LI and posterior areas <sup><xref ref-type="bibr" rid="c20">20</xref></sup>. With functional mapping with ISOI, we and other laboratories have identified comparable areas with the definition based on anatomical tracing, but the definition of area A and RL has been inconsistent between studies. In this paper, we simply followed our previous studies <sup><xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c78">78</xref></sup> on the definition of HVAs (AL, RL, AM, PM, LM, and LI) for HVA mapping (<xref ref-type="fig" rid="figs1">Figure S1</xref>) with ISOI.</p>
<p>In the analysis with two-photon imaging, since we found that area A largely lacked retinotopy, we redefined the areas as follows. By data from two-photon calcium imaging (tactile stimuli and visual stimuli), first, the V1, AM, S1t, and S1barrel were determined by retinotopic and somatotopic organizations. When the cortical neurons activated by visual and tactile stimuli were overlaid, we found an area where the evoked activity was almost absent in the medial space of the FOV. The area was determined as RSC. Then, the triangle behind AM between RSC and V1 was determined as PM. Then, the rectangular space between the S1 barrel and V1, and between S1t and AM was determined as RL and A, respectively. We found a borderline between S1t and S1 barrel, a borderline between V1 and AM, and a borderline between V1 and PM lined up in the straight line. We used the line for the border between RL and A. These definitions are largely consistent with the previous anatomical definitions of the Allen Mouse Brain Common Coordinate Framework (CCFv3) <sup><xref ref-type="bibr" rid="c79">79</xref></sup>. We did not discriminate the RSPagl and RSPd in this study since we did not find any functional landmarks. When we analyzed the differences or interactions between areas, we selected the neurons in each area excluding the neurons in a 50 μm-wide transition zone, to avoid contamination.</p>
</sec>
<sec id="s4g">
<title>Surgical procedure for head-fixed behavior</title>
<p>Mice were anesthetized with isoflurane (1.5 – 1.8%) and acepromazine (1.5 – 1.8 mg/kg body weight) when performing a craniotomy over the posterior parietal cortex. Carprofen (5 mg/kg body weight) was administered prior to surgery. Mouse body temperature was maintained using physically activated heat packs during surgery. The mouse’s eyes were kept moist with ointment during surgery. The scalp overlying the dorsal cortex was removed, and a custom head-fixing imaging chamber (<sup><xref ref-type="bibr" rid="c80">80</xref></sup>; <xref ref-type="fig" rid="figs3">Figure S3B</xref>) was mounted to the skull with a self-cure dental adhesive system (Super bond, L-Type Radiopaque; Sun Medical) and luting cement (Micron Luting; PD). The surface of the intact skull was subsequently coated with clear acrylic dental resin (Super bond, Polymer clear; Sun Medical) to prevent drying. Mice were then returned to their cages.</p>
<p>After the behavior training and before the two-photon imaging sessions, a second surgery was done with the same anesthetic conditions described above. A 4-mm diameter craniotomy was performed over the posterior parietal cortex (right hemisphere, centered 2.0 mm lateral and 2.50 mm posterior from bregma) and covered with two #1 thickness coverslips with 3.5 mm diameter and one #1 thickness coverslip with 5 mm diameter, glued together with optical adhesive (NOA68, Norland). The edge of the upper coverslip with 5 mm diameter was sealed with luting cement (Micron Luting; PD). Mice were then returned to their cages. The condition of the cortical surface was typically the worst in the first week but recovered after the second week. We did not image the activity of neurons when the cortical surface above the PPC was not clear.</p>
</sec>
<sec id="s4h">
<title>Head-fixed tasks</title>
<p>In all tasks, mice were kept in a body-holder with the head fixed to the stage (<xref ref-type="fig" rid="figs3">Figure S3A</xref>). A white square in the left or right position at the display was presented when the mouse licked to the left or right. In the visually guided task (vision task or “V” task, <xref rid="fig2" ref-type="fig">Figure 2A-C</xref>), mice had to lick left or right based on the visual stimuli presented in the display located at the left visual field of them. When a natural video taken in the cage (<xref ref-type="fig" rid="figs3">Figure S3B</xref>) was presented, the mouse had to lick left after a 4-s sampling period to get a drop of water as a reward (1.5 - 2.0 μl) from the left lick port. When a black screen was shown on the display, the mice had to lick right to get the reward from the right lick port. During the sampling period, the lick-ports were withdrawn. Mice learned to not lick during this sampling period (<xref ref-type="fig" rid="figs3">Figure S3E</xref>,<xref ref-type="fig" rid="figs3">F</xref>). The visual stimuli were also presented during the response period and for 2 s after the response when the response was correct. The visual stimuli stopped when the response was incorrect. During the inter-trial-interval (ITI), a gray screen was presented. In the vision task, the probability that the correct direction was left was #R / (#L + #R), where #L and #R are the total number of correct choices in left and right trials before the trial in the session, respectively. When the mice were rewarded three times in the same direction, the correct direction was switched on the next trial. These procedures were implemented to avoid biased licking in the same direction.</p>
<p>We also designed a choice history-dependent task (history task or “H” task) with the same set up as the vision task. In this task, the mice had to choose the other side of the choice from the previous trial (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). Thus, the information of direction of the previous choice had to be stored for 6-10 s from the previous choice to the next choice including ITI (2 – 6 s) and a sampling period (4 s). Since the visual stimuli were randomly presented during the sampling period, the mice had to ignore the visual stimuli.</p>
<p>In some sessions of the choice history-dependent task, the body-holder was moved randomly to prevent the mice from storing information about the previous trials based on the body-angle. In every trial, the body-holder was reset to the center position when the ITI started and then moved to either a left or right angle (± 24 degrees).</p>
<p>For training sessions before the vision task and history task, we trained the mice with the “V+H” task, where the mice could follow either the visual stimuli and history of the choice. In this task, the correct chosen direction was the opposite of the previous choice as in the history task, and the visual stimuli were consistent with the correct direction. In a few other sessions of history tasks, the display was switched off or the body-holder was randomly moved in the dark environment. We conducted but did not analyze the data of two-photon imaging during the V+H task, the history task in the dark, or the history task in the dark with body-holder movements in this study.</p>
</sec>
<sec id="s4i">
<title>Training of the tasks</title>
<p>We trained the mice with a serial schedule in custom-made training boxes or under the microscope (<xref ref-type="fig" rid="figs3">Figure S3C</xref>). After recovery from the surgery of a headplate attachment to the skull, we started water restriction (1 ml / day) until the weight was 85 – 90% of the original weight. Then, we started to train the mice with “pre-training”. At the first day of pre-training, the mice freely got a drop of water from both left and right licking ports with the head-fixed condition for 30 min. During the pre-training after the second session, the temporal pattern of the task (i.e. SP (sampling period), RP (response period) and ITI (inter-trial-interval)) was introduced as the vision task described above except for that the RP was unlimited. In other words, the incorrect licking did not result in the termination of RP so that the mice could try to lick until they lick to the correct direction and get a drop of water (1.5 - 2.0 μl). The visual stimuli were switched at every trial between a natural video and a black screen, and the correct licking directions were left and right, respectively as in the V+H task. The distance between the two licking ports was set relatively small (∼2 – 4 mm) to allow the mice to find both spouts easily and was gradually increased up to ∼5.5 mm (distance depended on the mice). The experimenter assisted the mice to find the licking port when the mice missed it for several minutes by repositioning it and manual delivery of water. Overall, the pre-training was used to make the mice learn to lick both sides of the licking ports, which took 2 – 5 sessions. When the number of reward mice got exceeded 150 per session (&lt; 2 hours), the task was switched to the V+H task. In this task, the temporal pattern of the task was the same as the vision task, but the correct direction of licking was set the opposite side of the previous direction. Therefore, the mice could either follow visual stimulus or the choice history to get the reward. The experimenter assisted the mice to find the licking port by repositioning the spouts and manual delivery of water when the mice lost motivation, especially in several initial sessions. The V+H task was trained to give the baseline condition where we can retrain the V task and H task from the task. After the acquisition of the V+H task, we switched the task to the vision task. The correct rate immediately dropped after switching from V+H task to V task but recovered after that. The transient decrease in the correct rate indicates that the mice used history information during the V+H task. After reaching the criteria, the mice were trained to perform the task under the microscope. After the V task, we retrained mice with V+H task to recover the baseline condition. After that, we switched the task to the H task. We found that the correct rate dropped immediately after switching the task from V+H task to H task and recovered again. This transient drop demonstrates that the mice also used vision information in the V+H task. In the H task, all the mice reached the criteria under the microscope during two-photon imaging (blue dots in <xref ref-type="fig" rid="figs3">Figure S3D</xref>). With two mice, we trained the H task with randomly moving body-holder, and both mice reached the criteria during the two-photon imaging (orange dots in <xref ref-type="fig" rid="figs3">Figure S3D</xref>). Throughout the training, mice kept their weight more than 85 % of the original weight and were monitored their health conditions every day for about 4 months.</p>
</sec>
<sec id="s4j">
<title>Two-photon imaging</title>
<p>Two-photon imaging was carried out using a custom Diesel2p microscope controlled by costom LabVIEW software (National instruments). The two-photon excitation laser from an 80-MHz Newport Spectra-Physics Mai Tai DeepSee was scanned by a resonant scanner (CRS 8 KHz, Cambridge Technologies) and two Galvo scanners (62010H, Cambridge Technologies). The photon signal was collected by a PMT (H7422P-40, Hamamatsu) and amplified with a variable high speed current amplifier (#59-179, Edmund Optics). Two-photon imaging of 3 × 3 mm<sup>2</sup> was collected at 5.6 Hz for imaging. Since the maximum scan angle of the resonant scanner is limited, we made two rectangles with 1.5 × 3 mm<sup>2</sup> to cover 3 × 3 mm<sup>2</sup> FOV (<xref ref-type="fig" rid="figs2">Figure S2A</xref>,<xref ref-type="fig" rid="figs2">B</xref>). With these parameters, a neuron with a 10 μm diameter circle was supposed to have 29 pixels inside the ROI (region of interest), including more than 100 laser pulses on average, which was sufficient to find the single neuron activities (<xref ref-type="fig" rid="figs2">Figure S2C</xref>,<xref ref-type="fig" rid="figs2">D</xref>). Imaging was performed with &lt;120 mW excitation 910 nm laser out of the front of the custom-made objective lens (0.55 NA). Mice were head-fixed at about 15 cm from a flat monitor, with their left eye facing the display during imaging. An 80° × 45° the left visual field was covered with the display. During imaging, we monitored a diameter and position of the left pupil, movements of whiskers, both forelimbs, a mouth and a tail with four custom-controlled CMOS cameras at 30 fps. Licking to the left or right licking port was also monitored with an electrical circuit <sup><xref ref-type="bibr" rid="c81">81</xref></sup>. The timing signals for the resonant scanner were used for synchronization of all the other devices and the data. The imaging depth below the dura was between 80 μm and 180 μm (layer 2/3). To ensure the scanning planes were parallel between different sessions, we tilted the stage of the body holder manually before the imaging sessions to make the imaging plane horizontal to the surface of the brain.</p>
<p>Visual stimulation for retinotopic mapping was displayed on a 60 Hz 7” LCD monitor (9.5 x 15 cm<sup>2</sup>). A white vertical or horizontal bar was drifted from temporal to nasal or top to bottom at a cycle of 8 s. The tactile simulation for mapping of the somatotopic organization was made with a gentle and brief air-puff (20 – 50 ms) to a specified part of the body at a cycle of 4 or 8 s. Both visual and tactile stimuli were repeated at least 20 cycles. When the mouse started to actively move during the stimuli, the imaging was stopped and the data were discarded.</p>
</sec>
<sec id="s4k">
<title>Image processing</title>
<p>Image analysis was performed using ImageJ (National Institutes of Health) and MATLAB (MathWorks, Natick, MA, USA) software. Image sequences were corrected for focal plane displacement, and region of interest (ROIs) within somata were automatically determined using suite2p (<xref ref-type="fig" rid="figs2">Figure S2E</xref>) <sup><xref ref-type="bibr" rid="c82">82</xref></sup>. The fluorescence traces and shape of ROIs were inspected visually. Elongated ROIs, which were more likely to be dendrites or axons, were excluded if the ratio of the length of the major axis to the length of the minor axis was greater than <bold>3</bold>. To eliminate the effect of background activity, we made a square-shaped mask whose pixels were within 50 μm from the edge of the ROI, excluding pixels less than 5 pixels from the edge of the ROI and pixels within the other ROIs. Then, we subtracted the mean value of this area from the mean value of the ROI. The obtained value was referred to as “activity” of the neuron. Those neurons showing activity with skewness of &gt; 0.3 <sup><xref ref-type="bibr" rid="c80">80</xref></sup> were regarded as <bold>“active neurons”</bold> and used for analysis. We also obtained active neurons with kurtosis &gt; 5 as and defined them as <bold>“highly active neurons”</bold>. Non-negative deconvolution was conducted with foopsi<sup><xref ref-type="bibr" rid="c83">83</xref></sup>. The values after non-negative deconvolution were thresholded with median + 1.5 × 1.4826 × mad and was shown as the inferred spike.</p>
<p>To analyze neuropil activity, the pixels within each ROI identified with suite2p were filled with each background activity described above. After that, the size of the image was reduced to 120 × 120 pixel<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, where the size of a single pixel is 25 × 25 μm<sup>2</sup>.</p>
</sec>
<sec id="s4l">
<title>Processing of video data using DeepLabCut</title>
<p>We recorded video data (160 x 120 pixel<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, 30 fps, ∼ 1 hour / session) from three IR cameras (Basler SCA640-70GM, GIGE, Lens: Basler C125-1218-5M, 12 mm) placed in front of the mouse, at the rear of the mouse, and in front of the left eye during task performance. We analyzed these data with DeepLabCut <sup><xref ref-type="bibr" rid="c84">84</xref></sup> (<ext-link ext-link-type="uri" xlink:href="https://github.com/AlexEMG/DeepLabCut">https://github.com/AlexEMG/DeepLabCut</ext-link>) for tracking the left and right forepaws, the tail and the pupil. We labeled each body part in randomly extracted 100 frames from each video and trained a model for each video data for each session using the 100 labeled data. About 20,000 iterations were performed in training the model. Google Colaboratory was utilized for the training of the model using GPU. We further analyzed the positions of the body parts in frame with estimated relative likelihood was greater than 0.9. The horizontal and vertical position was directly extracted from the result of the tracking, and then we obtained the direction (angle) of the left and right forepaws and the tail from the body center (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). The pupil size was calculated by identifying two edges of the left pupil (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). We visually inspected the quality of the results and confirmed that all the tracking results were as precise as human vision.</p>
</sec>
<sec id="s4m">
<title>Encoding models</title>
<p>To map the neurons encoding visual, choice and history-related information, we used a simple linear encoding model (<xref rid="fig2" ref-type="fig">Figure 2g</xref>). We confirmed that the range of maximum VIF (variance inflation factor) for parameters for visual, choice, and history was 1.7 - 3.5 (mean: 2.5) in the vision task, 1.5 - 3.1 (mean: 2.0) in the history task, and 2.1 - 4.1 (mean: 2.9) in the history task with holder rotation. In total, the range was 1.5 - 4.1 (mean: 2.4), which indicates that simple linear regression is sufficient to estimate the correlation between these parameters and single neuron activity without a serious problem of collinearity. The activity of each neuron was fitted by a set of predictor variables for each session. The 29 - 32 predictors consisted of the timing of the visual stimuli (556 ms each, 11 bins, video (+1) or dark (−1)), chosen directions (left (+1) or right (−1)), choice histories (3.7 - 7.4 s after left (+1) or right (−1) choice), reward directions (2 s after rewarded, left (+1) or right (−1)), rotation of the body holder (left (+1) or right (−1)), and an inter-trial interval. Non-selective parameters were also added for each parameter, to avoid identifying neurons activated by both types of stimuli, choice, history, reward or holder rotation as task-related neurons. For example, the non-selective parameter was +1 on the time of choice regardless of the choice directions. The Bonferroni-corrected <italic>p</italic>-values for the visual stimuli, choice, choice history, and holder rotation parameters in the encoding model were used to identify the vision, choice, choice history, and holder-related neurons, respectively. P-values (p&lt;0.001) for each parameter of the regression model were used to determine the significance. The <italic>t</italic>-values in the regression model were used to map the strength and position of each task-related neuron. To map the ratio of task-related neurons, we counted the number of active neurons and the number of task-related neurons within 150 μm of each pixel and divided the latter by the former to calculate the ratio. The association index was determined by the harmonic mean of the rates of vision neurons and choice neurons.</p>
<p>We also mapped the posture/movement parameters by estimating the posture/movement-related neurons with a regression analysis. As some of the VIFs for the predictors including task parameters and posture/movement parameters were very high (typically 300 or more), we used a linear model with L2 parameter penalization (a ridge regression). The predictors consisted of position, speed, and velocity of the left forepaw, the right forepaw, and the tail along the x-y plane and the angle (<xref ref-type="fig" rid="figs5">Figure S5</xref>), pupil size, speed and velocity of pupil size change, and task parameters described above (total 59 parameters). The neuronal activity and predictors were z-scored. We determined the lambda value for each neuron (range: 10<sup>-<xref ref-type="bibr" rid="c4">4</xref></sup> ‒ 10<sup>4</sup>) and avoided overfitting with five-fold cross-validation. To estimate the contribution of parameters for the left forelimb, the right forelimb, the tail, and the pupil, we repeated the same analysis with a reduced model where each set of predictors was eliminated from the full model (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). Then, the pseudo-R<sup><xref ref-type="bibr" rid="c2">2</xref></sup> was obtained for each set of predictors by (<italic>MSE<sub>reduced</sub></italic> ‒ <italic>MSE<sub>full</sub></italic>) <italic>/ MSE<sub>null</sub></italic>, where MSE is the mean squared error, <italic>MSE<sub>reduced</sub></italic> is MSE for the reduced model, <italic>MSE<sub>full</sub></italic> is the MSE of the full model, and <italic>MSE<sub>null</sub></italic> is the null model. For example, we constructed a regression model without the parameters regarding the left forelimb (green shade of <xref rid="fig4" ref-type="fig">Figure 4B</xref>), obtained <italic>MSE<sub>reduced</sub></italic> for the left forelimb, and the pseudo-R<sup><xref ref-type="bibr" rid="c2">2</xref></sup> was calculated as above by comparing the MSE of the full model and the null model.</p>
</sec>
<sec id="s4n">
<title>Decoding analysis</title>
<p>The direction of the choice was decoded by the population activity of neurons with a logistic regression model (Matlab built-in function, “glmfit” with binomial distribution and logit link function). For each time bin, the model was trained to predict the chosen direction using the activity of neurons in the time bin except for the trial. Then, the choice in the trial was predicted by the activity of neurons in the trial (leave-one-out). The outcome (reward or punishment) and visual stimulation (video or blank) were similarly decoded by the activity of neurons. The number of trials was matched to the smaller number of the right and left selections in order to align the number of selections between the left and right selections. For example, if the left selection had 150 trials and the right selection had 200 trials, all trials from the left selection were used and 150 trials from the right selection were randomly selected. To avoid bias due to the selection of trials, we repeated the random selection 20 times and averaged the 20 decoding results to determine the accuracy of the decoding. By this manipulation, decoding accuracy is 50% when the neuron has no information at all.</p>
<p>The time after the initiation of the videos (<xref rid="fig2" ref-type="fig">Figure 2L</xref>), or the time after the left or right choice (<xref rid="fig3" ref-type="fig">Figure 3F</xref>,<xref ref-type="fig" rid="fig3"><bold>N</bold></xref>) was independently decoded by using a support vector machine (Matlab built-in function, fitcsvm) and the activity of neurons. For decoding the time after the video starts, each imaging frame (185 ms) during the sampling period was used as a label for classification of the time. For decoding the time after the choice, 9.5 seconds after the choice were divided into 17 parts (each part being 555 ms) and each part was used as a label for the classification. We trained multiple support vector machines, each of which classifies whether a given set of neural activities belongs to the <italic>i</italic>th interval or not (<italic>i</italic> = 1, 2, …, the total number of parts). A ten-fold cross-validation procedure was used to avoid over-fitting.</p>
</sec>
<sec id="s4o">
<title>Dimensional reduction</title>
<p>To confirm whether the encoding model explains the diversity of single neuron activity, we visualized each set of the task-related neurons with <italic>t</italic>-SNE. We averaged single neuron activity over trials for each of four trial types: video + left choice, video + right choice, blank + left choice, and blank + right choice. Then, we defined a vector for each neuron by combining the four averaged activities. The Matlab function “tsne” was applied to this set of vectors.</p>
<p>jPCA is a dimensional reduction method for visualizing neural dynamics developed by Mark Churchland (<ext-link ext-link-type="uri" xlink:href="https://churchland.zuckermaninstitute.columbia.edu/content/code">https://churchland.zuckermaninstitute.columbia.edu/content/code</ext-link>) <sup><xref ref-type="bibr" rid="c49">49</xref></sup>. It is useful for visualizing not only rotational dynamics but also sequential dynamics because it approximates the direction of change in the population vector over time that is orthogonal to the current activity (<italic>M<sub>skew</sub></italic>). To apply jPCA analysis, the activity of neurons in the area of interest was smoothed with a Gaussian filter with σ=0.9 s and z-scored. After reducing the dimensionality to 6 using standard PCA, the activity from 1 s before to 5 s after the choice was concatenated on all trials. jPCA was applied and a linear transformation of the jPCA in the first plane was used to show the neural trajectories. To compare the neural dynamics with behavioral performance, we applied jPCA to 10-minute segments of population activity and compared R<sup><xref ref-type="bibr" rid="c2">2</xref></sup> for <italic>M<sub>skew</sub></italic> with the correct rate in those 10 minutes. R<sup><xref ref-type="bibr" rid="c2">2</xref></sup> for <italic>M<sub>skew</sub></italic> is the coefficient of determination of the regression model for approximating the temporal change of population vector by the current population vector <sup><xref ref-type="bibr" rid="c49">49</xref></sup>.</p>
</sec>
<sec id="s4p">
<title>Correlation analysis at single neuron and population level</title>
<p>Correlation of trial-to-trial variance of activity between a pair of single neurons was defined as <italic>r<sub>t_single</sub>.</italic> To calculate <italic>r<sub>t_single</sub></italic>, we averaged the activity of individual neurons over the sampling period, and the average across each trial type was subtracted from this value. The trial types consisted of four sets of pairs of stimuli and responses, that is, the video stimulation and left choice, the video stimulation and right choice, the black screen and left choice, and the black screen and right choice. By this operation, we extracted the fluctuating components of single-neuron activity that are independent of the trial types. We defined a neuron with 0.3 or more <italic>r<sub>t_single</sub></italic> with a neuron in AM or A as a coupling neuron if the neuron was not in the same area. We counted the number of coupling neurons in each area and considered the number as a vector. We normalized this vector so that the sum was 1 (<bold>ratio vector</bold>) only when the total number was 5 or more. Then we clustered the ratio vectors using K-means (k=6) clustering and defined the mean vector of each cluster as a “<bold>preference vector”</bold>. We confirmed that repeated K-means clustering with different initial values resulted in that more than 80% of the neuron pairs being classified into the same cluster. We averaged the angles between all pairs of the preference vectors, and referred to this as “<bold>coupling diversity</bold>”. For comparison of coupling diversity of AM and A, we randomly extracted half of the ratio vectors from AM and A, conducted K-means clustering, and obtained the coupling diversity 1,000 times to know the distribution of the coupling diversities. We defined area X as the <bold>“preferred area”</bold> of a neuron in A and AM if the neuron had one or more coupling neurons in the area X. We then compared the proportion of task-related neurons (i.e., vision, choice or history neurons) between the neurons whose preferred area includes area X and the other neurons by a chi-square test.</p>
<p>To calculate the trial-to-trial fluctuations in neuropil activity (<italic>r<sub>t_neuropil</sub></italic>), we averaged the neuropil images over sampling period, and the average image across each trial type was subtracted from this image. The trial types consisted of four sets of pairs of stimuli and responses, that is, the video stimulation and left choice, the video stimulation and right choice, the black screen and left choice, and the black screen and right choice. By this operation, we extracted the fluctuating components of neuropil activity that are independent of the trial types.</p>
</sec>
<sec id="s4q">
<title>CCA (canonical correlation analysis)</title>
<p>CCA was used to compare neural population activities in two areas. As a preprocessing step, the data from an area was reduced to <italic>N<sub>PCA</sub></italic> (=10) dimensions using PCA (principal component analysis). This ensured that CCA did not find dimensions of high correlation but low data variance <sup><xref ref-type="bibr" rid="c85">85</xref></sup>. To estimate the area-to-area correlation in the task-independent variables, the CCA was calculated in task-independent spaces, which is practically the same as noise correlation, as follows (<xref ref-type="fig" rid="figs6">Figure S6B</xref>,<xref ref-type="fig" rid="figs6">C</xref>).</p>
<p>The value of each principal component was averaged over a 4-s sampling period in each trial. Then, we obtained the fluctuation component of the value by subtracting the mean of each four trial-types. Here four trial-types were defined by a combination of visual stimuli and choices: natural video and left choice, natural video, and right choice, black screen and left choice, and black screen and right choice. We call these fluctuating dimensions task-independent space. The fluctuation was concatenated into one column vector per a principal component (<italic>N<sub>PCA</sub></italic> by <italic>N<sub>trial</sub></italic> matrix, <italic>N<sub>trial</sub></italic> is a total number of four trial types). CCA between any pair of the cortical areas was obtained in these task-independent spaces. The <italic>N<sub>CCA</sub></italic> (=10) CCA values were averaged and the value was referred to as <italic>CC<sub>t,X,Y</sub></italic>, where X and Y are a cortical areas.</p>
<p>We also used CCA to estimate the amount of shared information between an area and the other areas (<xref rid="fig6" ref-type="fig">Figure 6L, M</xref>; <xref ref-type="fig" rid="figs6">Figure S6D, E</xref>). Suppose we have time series of the <italic>N<sub>PCA</sub></italic> variables (principal components) in the task-independent spaces, <bold>X</bold> and <bold>Y<sub>a</sub></bold>, from an area X and another area Y<sub>a</sub>, respectively,
<disp-formula>
<graphic xlink:href="555017v2_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where X<italic><sub>i</sub></italic> and Y<sub>a</sub>,<italic><sub>i</sub></italic> (<italic>i</italic>=1,2,3,… <italic>N<sub>PCA</sub></italic>) are column vectors, whose length is equal to the number of trials (<italic>N<sub>trial</sub></italic>). To calculate <italic>N<sub>CCA</sub></italic> canonical correlation values, linear transformation of <bold>X</bold> and <bold>Y<sub>a</sub></bold> with weight matrices (canonical coefficient vectors, CCVs) <bold>A<sub>a</sub></bold> and <bold>B<sub>a</sub></bold> give canonical scores, <bold>S<sub>a</sub></bold> and <bold>T<sub>a</sub></bold>, respectively as follows:
<disp-formula>
<graphic xlink:href="555017v2_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula>
<graphic xlink:href="555017v2_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where both <bold>A<sub>a</sub></bold> and <bold>B<sub>a</sub></bold> are <italic>N<sub>PCA</sub></italic> by <italic>N<sub>CCA</sub></italic> matrices with <italic>N<sub>CCA</sub></italic> CCVs (column vectors) of length N<sub>PCA</sub>. The <italic>n</italic>-th CCV transforms the time series of principal components, <bold>X</bold> and <bold>Y<sub>a</sub>,</bold> to <italic>n</italic>-th canonical score. As a result, <bold>S<sub>a</sub></bold> and <bold>T<sub>a</sub></bold> are the matrixes with <italic>N<sub>CCA</sub></italic> by <italic>N<sub>tria</sub></italic><sub>l</sub>. This transformation was done so that the correlations between the canonical scores are maximized (with the built-in function “<italic>canoncorr</italic>” in Matlab).</p>
<p>We have seven time series of the N<sub>PCA</sub> variables (principal components) in the task-independent spaces, <bold>Y<sub>1</sub>, Y<sub>3</sub>, …, Y<sub>7</sub></bold>, from areas Y<sub>1</sub>, Y<sub>3</sub>, …, Y<sub>7</sub>. Then, we can get sets of CCVs, <bold>A<sub>2</sub>, A<sub>3</sub>, …, A<sub>7</sub></bold> and <bold>B<sub>1</sub>, B<sub>3</sub>, …, B<sub>7</sub></bold> and sets of the canonical scores, <bold>S<sub>1</sub>, S<sub>3</sub>, …, S<sub>7</sub></bold> and <bold>T<sub>1</sub>, T<sub>3</sub>, …, T<sub>7</sub>.</bold> Here, the CCVs of <bold>A<sub>1</sub>, A<sub>2</sub>, …, A<sub>7</sub></bold> defined in the 10-dimensional data space of area <bold>X</bold> correspond to the linear projections of activity of neurons in the area X that best explain the activities of neurons in each of the other areas (7 areas in total). If area X shares similar information with another seven areas, CCVs, <bold>A<sub>1</sub>, A<sub>2</sub>, …, A<sub>7</sub></bold> will be oriented in a similar direction and therefore the length of the sum of the CCVs will be larger. Accordingly, we defined the <bold>“information sharing index”</bold> (<xref ref-type="fig" rid="figs6">Figure S6D</xref>) of area X as the length of the sum of CCVs as max <inline-formula><inline-graphic xlink:href="555017v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>s<sub>i</sub></italic> can be 1 or -1, <italic>A<sub>i,d</sub></italic> is a <italic>d</italic>-th column vector (CCV) of <italic>i</italic>-th area, and || denotes the norm.</p>
<p>We also used generalized CCA (GCCA)<sup><xref ref-type="bibr" rid="c86">86</xref></sup>, which extends the notion of CCA to &gt;2 data sets. We estimated similarity between the first generalized CCV and CCVs for each area by inner product of these vectors. We defined the average of these inner products as <bold>“global information sharing index”</bold> (<xref ref-type="fig" rid="figs6">Figure S6E</xref>).</p>
</sec>
<sec id="s4r">
<title>Analysis of Allen connectivity data</title>
<p>We used the Allen Mouse Brain Connectivity Atlas (<ext-link ext-link-type="uri" xlink:href="http://connectivity.brain-map.org">http://connectivity.brain-map.org</ext-link> <sup><xref ref-type="bibr" rid="c33">33</xref></sup>) to investigate the pattern of the corticocortical and thalamocortical projections around the mouse PPC. In this database, connectivity data are available for mice from injection areas (AAV1-EGFP as an anterograde tracer) to any brain region. For estimating the relative projection density between A and AM, the corticocortical projection density was obtained in each layer at 0, 100, 200, 300, 400, 500, 600, 700 μm from pia, averaged and normalized with the injection volume, then averaged across wild-type mice. This was repeated with the major source areas to the A and AM (V1, PM, AL, vRSC, S1t, OFC, dACC, and vACC; <xref ref-type="fig" rid="figs8">Figure S8A</xref>). For seed correlation analysis, we downloaded the data whose injection sites were either isocortex (929 experiments) or thalamus (189 experiments) at 100 μm resolution (<xref rid="fig6" ref-type="fig">Figure 6E</xref>). We set a 100 μm-space grid that covered entire dorsal cortical areas including the PPC. Then, a set of injection sites that projected to each grid at the 300 μm from the surface of the brain were identified for each grid (<xref rid="fig6" ref-type="fig">Figure 6F</xref>). Thus, each grid has a set of the injection sites where the grid receives its projection from either isocortex or thalamus. Using these binary connectivity matrices, we calculated the correlation coefficient between two sets of injection sites of the two grids. Based on this correlation matrices, we got a map of the correlation coefficient for each grid (seed) (<xref ref-type="fig" rid="figs8">Figure S8B</xref>).</p>
<p>Let a binary data matrix <bold>P</bold> (<italic>N<sub>pix</sub></italic> × <italic>N<sub>inj</sub></italic>) be a set of results of the tracer experiments, where <italic>N<sub>pix</sub></italic> and <italic>N<sub>inj</sub></italic> is the number of pixel (cortical surface) and the number of injection experiments, respectively (<xref rid="fig6" ref-type="fig">Figure 6G</xref>). Let <bold>Q</bold> (<italic>N<sub>pix</sub></italic> × <italic>N<sub>inj</sub></italic>) be a set of the injected volume (area), ad let <bold>W</bold> (<italic>N<sub>pix</sub></italic> × <italic>N<sub>pix</sub></italic>) be a pixel-to-pixel connection matrix. By definition,
<disp-formula>
<graphic xlink:href="555017v2_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here, we need a correlation matrix, <bold>WW<sup>T</sup></bold>. However, there are no direct data for <bold>W</bold> or <bold>Q</bold>. Therefore, we estimated it by <bold>P</bold> as follows
<disp-formula>
<graphic xlink:href="555017v2_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
If the injection volume is small and random enough, <bold>QQ<sup>T</sup></bold> → <bold>Id</bold> as the number of experiments gets large. If so, we can approximate <bold>WW<sup>T</sup></bold> by <bold>PP<sup>T</sup></bold>. In reality, <bold>QQ<sup>T</sup></bold> should work as a spatial filter of <bold>WW<sup>T</sup></bold>. As the injection volume would be &lt;500 μm on average, the seed correlation map should capture the structure with a resolution of several hundred micrometers, which would be sufficient for our analysis. This resolution could be improved with decreasing injection volume, or kernel regression <sup><xref ref-type="bibr" rid="c87">87</xref></sup>.</p>
<p>To effectively visualize these maps, we developed a software that users can explore cortical surface point (seeds) to see the cortical areas that shared either the corticocortical input, thalamocortical input or both types of input (<xref ref-type="fig" rid="figs8">Figure S8C</xref>, <ext-link ext-link-type="uri" xlink:href="https://web.ece.ucsb.edu/~riichirohira/TopBM-2.05/index.html">https://web.ece.ucsb.edu/~riichirohira/TopBM-2.05/index.html</ext-link>).</p>
</sec>
<sec id="s4s">
<title>Statistics</title>
<p>Wilcoxon’s rank-sum test and chi-squared test were used for pairwise comparison. For multiple comparison, we used one-way ANOVA and showed confidence intervals (p=0.05) in the figures. No statistical tests were performed to predetermine sample size. No blinding or randomization was performed.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data availability</title>
<p>Data is available upon request.</p>
</sec>
<sec id="s6">
<title>Code availability</title>
<p>A web-based interactive software including its source code is available on line (<ext-link ext-link-type="uri" xlink:href="https://web.ece.ucsb.edu/~riichirohira/TopBM-2.05/index.html">https://web.ece.ucsb.edu/~riichirohira/TopBM-2.05/index.html</ext-link>). The script for the data analysis is available upon request.</p>
</sec>
<sec id="s7">
<title>Supplementary figures</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Intrinsic signal imaging and the individual variability of the position of the HVAs.</title>
<p><bold>A.</bold> Activity maps of S1 for forelimb, hindlimb, whiskers, tail, trunk, and ear were shown. <bold>B.</bold> S1 regions obtained in panel <bold>A.</bold> A “homunculus” was shown for reference. <bold>C.</bold> Left, mouse skull and the bony landmarks for reference. We used the midline and a lambda on the skull to register the V1 and HVAs across different mice. Right, overlapped maps of the V1 and HVAs for 12 mice. Note that there is large variability of HVA locations across mice. <bold>D.</bold> Distribution of the center of mass of V1 and HVAs relative to the midline and lambda. <bold>E.</bold> Comparison of the distribution of the center of mass of V1 and HVAs between different ages (left) and between different sexes (right). No significant difference was found (p&gt;0.05, Wilcoxon rank-sum test). The bar indicates ± standard deviation. <bold>F.</bold> Left, we used the midpoint of the center of V1 and the center of RL, and the angle between the midline and the straight line connecting the center of V1 and the center of RL to register the V1 and HVAs across different mice (internal registration). Right, the overlapped maps of the V1 and HVAs for 12 mice. Note that all the HVAs are better registered than the panel <bold>C</bold>, right. <bold>G.</bold> Location of V1 and HVAs before (left) and after (right) the internal registration. The gray lines connect the points from the same mice. The dot colors are the same as panel <bold>D</bold>. <bold>H</bold>. The standard deviation of the location of the center of mass along the anteroposterior and mediolateral axis before and after the internal registration. * p&lt;0.05. ** p&lt;0.01 (Two-sample <italic>F</italic>-test for equal variance). The dot colors are the same as panel <bold>D</bold>. <bold>I</bold>. The mean distance from the center position of each HVA in each mouse to the average center position across mice was compared before and after the registration.</p></caption>
<graphic xlink:href="555017v2_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
<caption><title>A large field-of-view two-photon microscope and its scanning strategy.</title>
<p><bold>A.</bold> A large field-of-view FOV) two-photon microscope, Diesel2p <sup><xref ref-type="bibr" rid="c16">16</xref></sup>. The body holder, head-plate holder, display and four cameras are also shown. <bold>B.</bold> Left, the brain areas of the right cerebral cortex of the mouse from the Allen atlas (version 3) <sup><xref ref-type="bibr" rid="c33">33</xref></sup>. Right, the scanning strategy to cover the 3 mm by 3 mm square area around the mouse PPC. Two 1.5 mm by 3 mm rectangle areas were sequentially scanned using two Galvo scanners and one resonant scanner. <bold>C.</bold> The scanning strategy in panel <bold>B</bold> resulted in 2459 x 1350 pixel<sup><xref ref-type="bibr" rid="c2">2</xref></sup> for each square frame, which yields 29 pixels (&gt;100 laser pulses) for a circle with a 10 μm diameter (the size of a single neuron). <bold>D.</bold> Representative activity of simultaneously detected neurons in the same FOV. Ten neurons in 8 areas are shown. Each color of trace corresponds to the area. <bold>E</bold>. Top. The image sequence was registered to the template within a session. Middle. The day-by-day distortion was corrected based on the blood vessels. Bottom. When overlapping the neurons across different mice, we registered images based on the boundaries of areas.</p></caption>
<graphic xlink:href="555017v2_figs2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Training schedule and learning curves of the tasks for head-fixed mice.</title><p><bold>A.</bold> A mouse was head-fixed and held inside a plastic body-holder. The mouse can enter the body-holder smoothly with its head plate into the aperture of the body holder. The display was located at a location covering the left visual field. <bold>B.</bold> A montage of 4-second natural videos (30 Hz, 24 out of 120 images) was shown. This photo was taken while moving a camera in a cage. <bold>C.</bold> The serial training schedule for multiple tasks. <bold>D.</bold> The correct rates of three example mice were plotted as a function of training sessions. Background color corresponds to the type of the task illustrated in panel <bold>C</bold>. The black, blue and orange dots indicate the imaging experiments of the vision task, history task and history task with a moving body-holder<bold>. E.</bold> The tongue extension movement during the task was monitored by the video put in front of the mouse (30 fps). Note that this does not necessarily indicate contact between the tongue and spout. We electronically measured contacts between the tongue and the spout and used the signal for the choice. <bold>F.</bold> Offline analysis captured the tongue extension to the left (left licking, magenta) and tongue extension to the right (right licking, green) during the response period. In both trials with the left choice (top panels) and the right choice (bottom panels), tongue extension movements during the sampling period were hardly observed.</p></caption>
<graphic xlink:href="555017v2_figs3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4.</label>
<caption><title>Distribution of the task-related neurons in vision and history task.</title>
<p><bold>A</bold>.The <italic>t</italic>-values for each type of task-related neurons were mapped in the vision task. <bold>B.</bold> The result of <italic>t</italic>-SNE analysis in the vision task. <bold>C</bold> and <bold>D</bold>. The same as <bold>A</bold> and <bold>B</bold> but in the history task, respectively.</p></caption>
<graphic xlink:href="555017v2_figs4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5.</label>
<caption><title>Analysis of postures and movements during the task.</title>
<p><bold>A</bold>. The position, the velocity and the speed of the three body parts were aligned by the initiation of the response period (RP) for the trials with left and right choice during the mice performing the history task (n=3 mice, total 6 sessions). Shaded areas indicate the sampling period (SP). <bold>B.</bold> Coordinates of the angle of the body parts. <bold>C</bold>. The pupil size increased and decreased after the black screen and the natural video started, respectively.</p></caption>
<graphic xlink:href="555017v2_figs5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S6.</label>
<caption><title>Canonical correlation analysis (CCA).</title><p><bold>A</bold>. An example source neuron in AM (red) and neurons with <italic>rt</italic> ≥ 0.3 (blue) were shown. <bold>B.</bold> Trial-to-trial activity fluctuation of the highly correlated neurons (<italic>rt</italic> ≥ 0.3, panel b) was color coded in each trial block. <bold>C</bold>. Illustration of CCA pre-processing. Activity of the population of neurons was extracted in each area, and the PCA (principal component analysis) reduced the data dimensions to 10. Then, “fluctuation” activity was calculated similar to the single neuron case. The data during the sampling period of each trial block (<bold>LL</bold>:video/left choice. <bold>LR</bold>:video/right choice. <bold>RR</bold>: black/right choice. <bold>RL</bold>: black/left choice) was extracted and averaged. Then, the residual of the data after subtracting the trial-mean in each trial type was concatenated as a new matrix, <bold><italic>Mt</italic></bold> (one column vector per PC dimension, that is, 10 × <italic>N<sub>trial</sub></italic> matrix. <bold>D.</bold> Illustration of the canonical correlation analysis and calculation of information sharing index. See methods for the precise definition. <bold>E.</bold> Illustration of the generalized canonical correlation analysis and calculation of global information sharing index. See methods for the precise definition.</p></caption>
<graphic xlink:href="555017v2_figs6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Figure S7.</label>
<caption><title>The trial-to-trial fluctuation of the neuropil activity and its relation to the single neuron activity.</title>
<p><bold>A.</bold> To compare the trial-to-trial fluctuation of the neuropil activity and single neuron activity, the single neurons were randomly selected (filled circle) within the distance, R, from the center (red square). Then, the trial-to-trial fluctuation of the neuropil activity at the center and the trial-to-trial fluctuation of the mean activity of selected neurons was compared. <bold>B.</bold> The z-scored neuropil activity in each trial was compared with the z-scored mean activity of <italic>N</italic> selected neurons in the corresponding trial was compared. <bold>C.</bold> The correlation coefficient obtained in the panel <bold>B</bold> was repeatedly calculated with a range of neurons averaged and the distance. The lines show the polynomial fitting curves.</p></caption>
<graphic xlink:href="555017v2_figs7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Figure S8.</label>
<caption><title>Anatomical structure analyzed with anterograde tracer database and its relation to the correlation structure of the trial-to-trial variability in neuropil activity.</title>
<p><bold>A.</bold> Using Allen connectivity atlas, the axonal density of corticocortical projection was analyzed. The nine cortical areas where A or AM receive the dense projection are shown. The density was obtained from multiple depths (0, 100, 200, 300, 400, 500, 600, 700 μm from pia), averaged and normalized with the injection volume, then averaged across the experiments. Only the projection from the S1t preferentially distributed in A rather than AM. Top-down axons from dACC, vACC, and OFC preferentially projected to AM rather than A, as well as visual cortices (AL, PM, and V1). Gray boundaries show A and AM in Allen brain atlas, and the white boundaries indicate A and AM in our definition. <bold>B.</bold> Top and second rows, the correlation between each cortical region and the seed points in each area (A, AM, RL, V1, PM, RSC, S1t, S1b) was color-mapped. The high correlation indicates that the region has highly shared input from the cortex (“CC”, top row) or the thalamus (“TC”, second row) with the seed region. A dotted circle indicates the imaging area. Second-bottom and bottom, the seed correlation of trial-to-trial activity fluctuation during the vision task (“M2, V”, second-bottom row) and during passively viewing the moving bar (“M2, P”, bottom row) of mouse 2. Note that all the correlation maps in each column are similar. <bold>C.</bold> A snapshot of the online interactive software, “BrainModules”, for exploring the anatomical seed correlation analysis. Link: <ext-link ext-link-type="uri" xlink:href="https://web.ece.ucsb.edu/~riichirohira/TopBM-2.06/index.html">https://web.ece.ucsb.edu/~riichirohira/TopBM-2.06/index.html</ext-link></p></caption>
<graphic xlink:href="555017v2_figs8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Figure S9.</label>
<caption><p><bold>A.</bold> Randomly selected 20 active neurons (top) and randomly selected 20 highly active neurons (kurtosis &gt; 5, bottom). <bold>B-J.</bold> The same analysis using only highly active neurons (<bold>B:</bold> <xref rid="fig5" ref-type="fig">Figure 5A</xref>; <bold>C:</bold> <xref rid="fig5" ref-type="fig">Figure 5B</xref>; D: <xref rid="fig5" ref-type="fig">Figure 5E</xref> (<italic>rt,single</italic> &gt;0.25); <bold>E:</bold> <xref rid="fig5" ref-type="fig">Figure 5F</xref>; F: <xref rid="fig5" ref-type="fig">Figure 5H</xref>; G: <xref rid="fig5" ref-type="fig">Figure 5I</xref>; H: <xref rid="fig5" ref-type="fig">Figure 5J</xref>, I: <xref rid="fig6" ref-type="fig">Figure 6L</xref>; J: <xref rid="fig6" ref-type="fig">Figure 6M</xref>, K:, <xref rid="fig6" ref-type="fig">Figure 6N</xref>, L: <xref rid="fig6" ref-type="fig">Figure 6O</xref>).</p></caption>
<graphic xlink:href="555017v2_figs9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by National Science Foundation (grant nos. NeuroNex 1934288 and 1707287 to S.L.S.; BRAIN EAGER 1450824 to S.L.S.) and the NIH (grant nos. NINDS R01NS091335 and NEI R01EY024294 to S.L.S.; R01NS128079 to I.T.S.; JP22wm0525007 and JP24wm0625405 from AMED, to R.H. and JP22H02731 (RH), JP21B304 (RH), JP21H05134 (RH), JP21H05135 (RH) from MEXT/JSPS to R.H.</p>
</ack>
<sec id="suppd1e3166" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e3143">
<label>Movie S1</label><caption><title>Movie S1</title><p>Two-photon calcium imaging of mouse #1 during task performance. Left, entire field of view (3 mm ×3 mm). Right, square portion of image shown in the right (0.63 mm × 0.63 mm). After registration, a gaussian filter with standard deviation of 0.36 s was applied to each pixel in the temporal direction for visualization. Time (s) is shown in the upper left. The first frame displays the average image and the location of the cortical areas. The original image was 1345 x 2500 pixels, taken at 5.5 fps. Raw data will be available in <ext-link ext-link-type="uri" xlink:href="https://figshare.com">https://figshare.com</ext-link>.</p></caption>
<media xlink:href="supplements/555017_file02.avi"/>
</supplementary-material>
<supplementary-material id="d1e3150">
<label>Movie S2</label><caption><title>Movie S2</title><p>Natural images used as visual stimuli for head-fixed mice. A small camera was placed in a mouse cage at the mouse’s eye level to mimic the mouse’s field-of-view. Moving the camera produced global motion and moving the mouse in the cage provided local motion. The luminance of the frame was averaged across the video frames. The video was presented in grayscale at 30 frames/s.</p></caption>
<media xlink:href="supplements/555017_file03.avi"/>
</supplementary-material>
<supplementary-material id="d1e3157">
<label>Movie S3</label><caption><title>Movie S3</title><p>Trial-average image (3mm x 3mm) of mouse #1 for video stimulus + left choice trials (left, magenta) and video stimulus + right choice trials (center, green). The images were aligned at the onset of visual stimulation. Time (s) was shown in the upper left. Visual stimulus onset was set to zero seconds. In the right panel, these two images were overlaid. During visual stimulation, there is an activity sequence of white cells around the visual cortices, which is related neither to left or right choices, but is the activity that was evoked similarly by the video frames. Images around 4-s included selective activities for left (magenta) and right (green) choices. Note that in the AM, white cells for vision and green and magenta cells for choice are all mixed together. After registration, the trial-average image was obtained, then each pixel was divided by the average intensity of the pixel. Then, the pixel values less than 5 % above the baseline were set to zero for visualization. No filter was used. The first frame shows the average image and the location of the areas. The original image was 1345 × 2500 pixels, taken at 5.5 fps.</p></caption>
<media xlink:href="supplements/555017_file04.avi"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname>, <given-names>Y.-H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A neural circuit for auditory dominance over visual perception</article-title>. <source>Neuron</source> <volume>93</volume>, <fpage>940</fpage>–<lpage>954</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olcese</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Iurilli</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Medini</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Cellular and synaptic architecture of multisensory integration in the mouse neocortex</article-title>. <source>Neuron</source> <volume>79</volume>, <fpage>579</fpage>–<lpage>593</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yoshitake</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Visual map shifts based on whisker-guided cues in the young mouse visual cortex</article-title>. <source>Cell reports</source> <volume>5</volume>, <fpage>1365</fpage>–<lpage>1374</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Odoemene</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Pisupati</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Churchland</surname>, <given-names>A. K</given-names></string-name></person-group>. <article-title>Visual evidence accumulation guides decision-making in unrestrained mice</article-title>. <source>Journal of Neuroscience</source> <volume>38</volume>, <fpage>10143</fpage>–<lpage>10155</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Orlandi</surname>, <given-names>J. G.</given-names></string-name>, <string-name><surname>Abdolrahmani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Aoki</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Lyamzin</surname>, <given-names>D. R.</given-names></string-name> &amp; <string-name><surname>Benucci</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Distributed context-dependent choice information in mouse posterior cortex</article-title>. <source>Nature Communications</source> <volume>14</volume>, <issue>192</issue> (<year>2023</year>).</mixed-citation></ref>
<ref id="c6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goard</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Pho</surname>, <given-names>G. N.</given-names></string-name>, <string-name><surname>Woodson</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Sur</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Distinct roles of visual, parietal, and frontal motor cortices in memory-guided sensorimotor decisions</article-title>. <source>eLife</source> <volume>5</volume>, <elocation-id>e13764</elocation-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Coen</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Tank</surname>, <given-names>D. W</given-names></string-name></person-group>. <article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title>. <source>Nature</source> <volume>484</volume>, <fpage>62</fpage>–<lpage>68</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Driscoll</surname>, <given-names>L. N.</given-names></string-name>, <string-name><surname>Pettit</surname>, <given-names>N. L.</given-names></string-name>, <string-name><surname>Minderer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chettih</surname>, <given-names>S. N.</given-names></string-name> &amp; <string-name><surname>Harvey</surname>, <given-names>C. D</given-names></string-name></person-group>. <article-title>Dynamic reorganization of neuronal activity patterns in parietal cortex</article-title>. <source>Cell</source> <volume>170</volume>, <fpage>986</fpage>–<lpage>999</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Funamizu</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kuhn</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Doya</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>Neural substrate of dynamic Bayesian inference in the cerebral cortex</article-title>. <source>Nature neuroscience</source> (<year>2016</year>).</mixed-citation></ref>
<ref id="c10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Runyan</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Piasini</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Harvey</surname>, <given-names>C. D</given-names></string-name></person-group>. <article-title>Distinct timescales of population coding across cortex</article-title>. <source>Nature</source> <volume>548</volume>, <fpage>92</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morcos</surname>, <given-names>A. S.</given-names></string-name> &amp; <string-name><surname>Harvey</surname>, <given-names>C. D</given-names></string-name></person-group>. <article-title>History-dependent variability in population dynamics during evidence accumulation in cortex</article-title>. <source>Nature neuroscience</source> <volume>19</volume>, <fpage>1672</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tseng</surname>, <given-names>S.-Y.</given-names></string-name>, <string-name><surname>Chettih</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>Arlt</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Barroso-Luque</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Harvey</surname>, <given-names>C. D</given-names></string-name></person-group>. <article-title>Shared and specialized coding across posterior cortical areas for dynamic navigation decisions</article-title>. <source>Neuron</source> <volume>110</volume>, <fpage>2484</fpage>–<lpage>2502.</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akrami</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kopec</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Diamond</surname>, <given-names>M. E.</given-names></string-name> &amp; <string-name><surname>Brody</surname>, <given-names>C. D</given-names></string-name></person-group>. <article-title>Posterior parietal cortex represents sensory history and mediates its effects on behaviour</article-title>. <source>Nature</source> <volume>554</volume>, <issue>368</issue> (<year>2018</year>).</mixed-citation></ref>
<ref id="c14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mimica</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Dunn</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Tombaz</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Bojja</surname>, <given-names>V. S.</given-names></string-name> &amp; <string-name><surname>Whitlock</surname>, <given-names>J. R</given-names></string-name></person-group>. <article-title>Efficient cortical coding of 3D posture in freely behaving rats</article-title>. <source>Science</source> <volume>362</volume>, <fpage>584</fpage>–<lpage>589</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c15"><label>15</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Krumin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name> &amp; <string-name><surname>Carandini</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Task specificity in mouse parietal cortex</article-title>. <source>Neuron</source> <volume>110</volume>, <fpage>2961</fpage>–<lpage>2969.</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>C.-H.</given-names></string-name>, <string-name><surname>Stirman</surname>, <given-names>J. N.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hira</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>S. L</given-names></string-name></person-group>. <article-title>Diesel2p mesoscope with dual independent scan engines for flexible capture of dynamics in distributed neural circuitry</article-title>. <source>Nature communications</source> <volume>12</volume>, <fpage>6639</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cavada</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Goldman-Rakic</surname>, <given-names>P. S.</given-names></string-name></person-group> <article-title>Posterior parietal cortex in rhesus monkey: I. Parcellation of areas based on distinctive limbic and sensory corticocortical connections</article-title>. <source>Journal of Comparative Neurology</source> <volume>287</volume>, <fpage>393</fpage>–<lpage>421</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c18"><label>18</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Freedman</surname>, <given-names>D. J.</given-names></string-name> &amp; <string-name><surname>Ibos</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>An integrative framework for sensory, motor, and cognitive functions of the posterior parietal cortex</article-title>. <source>Neuron</source> <volume>97</volume>, <fpage>1219</fpage>–<lpage>1234</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c19"><label>19</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rizzolatti</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Fogassi</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Gallese</surname>, <given-names>V</given-names></string-name></person-group>. <article-title>Parietal cortex: from sight to action</article-title>. <source>Current opinion in neurobiology</source> <volume>7</volume>, <fpage>562</fpage>–<lpage>567</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Q.</given-names></string-name> &amp; <string-name><surname>Burkhalter</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Area map of mouse visual cortex</article-title>. <source>The Journal of comparative neurology</source> <volume>502</volume>, <fpage>339</fpage>–<lpage>357</lpage>, doi:<pub-id pub-id-type="doi">10.1002/cne.21286</pub-id> (<year>2007</year>).</mixed-citation></ref>
<ref id="c21"><label>21</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Franklin</surname>, <given-names>K. B. J.</given-names></string-name> &amp; <string-name><surname>Paxinos</surname>, <given-names>G.</given-names></string-name></person-group> <source>Paxinos and Franklin’s The mouse brain in stereotaxic coordinates</source>. Fourth edition. edn, (<year>2001</year>).</mixed-citation></ref>
<ref id="c22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hovde</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Gianatti</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Witter</surname>, <given-names>M. P.</given-names></string-name> &amp; <string-name><surname>Whitlock</surname>, <given-names>J. R</given-names></string-name></person-group>. <article-title>Architecture and organization of mouse posterior parietal cortex relative to extrastriate areas</article-title>. <source>Eur J Neurosci</source>, doi:<pub-id pub-id-type="doi">10.1111/ejn.14280</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilissen</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Farrow</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bonin</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Arckens</surname>, <given-names>L</given-names></string-name></person-group>. <article-title>Reconsidering the border between the visual and posterior parietal cortex of mice</article-title>. <source>Cerebral Cortex</source> <volume>31</volume>, <fpage>1675</fpage>–<lpage>1692</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c24"><label>24</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lyamzin</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Benucci</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>The mouse posterior parietal cortex: anatomy and functions</article-title>. <source>Neuroscience research</source> <volume>140</volume>, <fpage>14</fpage>–<lpage>22</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whitlock</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Sutherland</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Witter</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name> &amp; <string-name><surname>Moser</surname>, <given-names>E. I</given-names></string-name></person-group>. <article-title>Navigating from hippocampus to parietal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>105</volume>, <fpage>14755</fpage>–<lpage>14762</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olsen</surname>, <given-names>G. M.</given-names></string-name> &amp; <string-name><surname>Witter</surname>, <given-names>M. P</given-names></string-name></person-group>. <article-title>Posterior parietal cortex of the rat: Architectural delineation and thalamic differentiation</article-title>. <source>Journal of Comparative Neurology</source> <volume>524</volume>, <fpage>3774</fpage>–<lpage>3809</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krumin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name> &amp; <string-name><surname>Carandini</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Decision and navigation in mouse parietal cortex</article-title>. <source>eLife</source> <volume>7</volume>, <elocation-id>e42583</elocation-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Minderer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>K. D.</given-names></string-name> &amp; <string-name><surname>Harvey</surname>, <given-names>C. D</given-names></string-name></person-group>. <article-title>The Spatial Structure of Neural Encoding in Mouse Posterior Cortex during Navigation</article-title>. <source>Neuron</source> (<year>2019</year>).</mixed-citation></ref>
<ref id="c29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pho</surname>, <given-names>G. N.</given-names></string-name>, <string-name><surname>Goard</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Woodson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Crawford</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Sur</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Task-dependent representations of stimulus and choice in mouse parietal cortex</article-title>. <source>Nature communications</source> <volume>9</volume>, <fpage>2596</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c30"><label>30</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Zhong</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Duan</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Pan</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Xu</surname>, <given-names>N.-l.</given-names></string-name></person-group> <article-title>Dynamic and causal contribution of parietal circuits to perceptual decisions during category learning</article-title>. <source>bioRxiv</source>, <volume>304071</volume> (<year>2018</year>).</mixed-citation></ref>
<ref id="c31"><label>31</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reep</surname>, <given-names>R. L.</given-names></string-name> &amp; <string-name><surname>Corwin</surname>, <given-names>J. V</given-names></string-name></person-group>. <article-title>Posterior parietal cortex as part of a neural network for directed attention in rats</article-title>. <source>Neurobiology of learning and memory</source> <volume>91</volume>, <fpage>104</fpage>–<lpage>113</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname>, <given-names>F. C.</given-names></string-name>, <string-name><surname>Jacobson</surname>, <given-names>T. K.</given-names></string-name> &amp; <string-name><surname>Burwell</surname>, <given-names>R. D</given-names></string-name></person-group>. <article-title>Single neuron activity and theta modulation in the posterior parietal cortex in a visuospatial attention task</article-title>. <source>Hippocampus</source> <volume>27</volume>, <fpage>263</fpage>–<lpage>273</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oh</surname>, <given-names>S. W.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A mesoscale connectome of the mouse brain</article-title>. <source>Nature</source> <volume>508</volume>, <issue>207</issue> (<year>2014</year>).</mixed-citation></ref>
<ref id="c34"><label>34</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Han</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The logic of single-cell projections from visual cortex</article-title>. <source>Nature</source> <volume>556</volume>, <fpage>51</fpage>–<lpage>56</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gămănuţ</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The mouse cortical connectome, characterized by an ultra-dense cortical graph, maintains specificity by distinct connectivity profiles</article-title>. <source>Neuron</source> <volume>97</volume>, <fpage>698</fpage>–<lpage>715.</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bressler</surname>, <given-names>S. L.</given-names></string-name> &amp; <string-name><surname>Kelso</surname>, <given-names>J. A.</given-names></string-name></person-group> <article-title>Cortical coordination dynamics and cognition</article-title>. <source>Trends Cogn Sci</source> <volume>5</volume>, <fpage>26</fpage>–<lpage>36</lpage>, doi:<pub-id pub-id-type="doi">10.1016/s1364-6613(00)01564-3</pub-id> (<year>2001</year>).</mixed-citation></ref>
<ref id="c37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Avena-Koenigsberger</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Misic</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Sporns</surname>, <given-names>O</given-names></string-name></person-group>. <article-title>Communication dynamics in complex brain networks</article-title>. <source>Nat Rev Neurosci</source> <volume>19</volume>, <fpage>17</fpage>–<lpage>33</lpage>, doi:<pub-id pub-id-type="doi">10.1038/nrn.2017.149</pub-id> (<year>2017</year>).</mixed-citation></ref>
<ref id="c38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>I. T.</given-names></string-name>, <string-name><surname>Townsend</surname>, <given-names>L. B.</given-names></string-name>, <string-name><surname>Huh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>S. L</given-names></string-name></person-group>. <article-title>Stream-dependent development of higher visual cortical areas</article-title>. <source>Nature neuroscience</source> <volume>20</volume>, <fpage>200</fpage>–<lpage>208</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c39"><label>39</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Stirman</surname>, <given-names>J. N.</given-names></string-name>, <string-name><surname>Dorsett</surname>, <given-names>C. R.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>S. L</given-names></string-name></person-group>. <article-title>Mesoscale correlation structure with single cell resolution during visual coding</article-title>. <source>bioRxiv</source>, <volume>469114</volume> (<year>2018</year>).</mixed-citation></ref>
<ref id="c40"><label>40</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>S. L.</given-names></string-name> &amp; <string-name><surname>Trachtenberg</surname>, <given-names>J. T</given-names></string-name></person-group>. <article-title>Experience-dependent binocular competition in the visual cortex begins at eye opening</article-title>. <source>Nature neuroscience</source> <volume>10</volume>, <issue>370</issue> (<year>2007</year>).</mixed-citation></ref>
<ref id="c41"><label>41</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kalatsky</surname>, <given-names>V. A.</given-names></string-name> &amp; <string-name><surname>Stryker</surname>, <given-names>M. P</given-names></string-name></person-group>. <article-title>New paradigm for optical imaging: temporally encoded maps of intrinsic signal</article-title>. <source>Neuron</source> <volume>38</volume>, <fpage>529</fpage>–<lpage>545</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c42"><label>42</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marshel</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Garrett</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Nauhaus</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Callaway</surname>, <given-names>E. M</given-names></string-name></person-group>. <article-title>Functional specialization of seven mouse visual cortical areas</article-title>. <source>Neuron</source> <volume>72</volume>, <fpage>1040</fpage>–<lpage>1054</lpage>, doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.004</pub-id> (<year>2011</year>).</mixed-citation></ref>
<ref id="c43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Waters</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Biological variation in the sizes, shapes and locations of visual cortical areas in the mouse</article-title>. <source>PloS one</source> <volume>14</volume>, <fpage>e0213924</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c44"><label>44</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sakata</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Takaoka</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Kawarasaki</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Shibutani</surname>, <given-names>H</given-names></string-name></person-group>. <article-title>Somatosensory properties of neurons in the superior parietal cortex (area 5) of the rhesus monkey</article-title>. <source>Brain Res</source> <volume>64</volume>, <fpage>85</fpage>–<lpage>102</lpage>, doi:<pub-id pub-id-type="doi">10.1016/0006-8993(73)90172-8</pub-id> (<year>1973</year>).</mixed-citation></ref>
<ref id="c45"><label>45</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Itokazu</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Streamlined sensory motor communication through cortical reciprocal connectivity in a visually guided eye movement task</article-title>. <source>Nature communications</source> <volume>9</volume>, <fpage>338</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c46"><label>46</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mohan</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Functional architecture and encoding of tactile sensorimotor behavior in rat posterior parietal cortex</article-title>. <source>Journal of Neuroscience</source>, 0693-0619 (<year>2019</year>).</mixed-citation></ref>
<ref id="c47"><label>47</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>T.-W.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title>. <source>Nature</source> <volume>499</volume>, <fpage>295</fpage>–<lpage>300</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c48"><label>48</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Froudarakis</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Population code in mouse V1 facilitates readout of natural scenes through increased sparseness</article-title>. <source>Nature neuroscience</source> <volume>17</volume>, <issue>851</issue> (<year>2014</year>).</mixed-citation></ref>
<ref id="c49"><label>49</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Neural population dynamics during reaching</article-title>. <source>Nature</source> <volume>487</volume>, <fpage>51</fpage>–<lpage>56</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c50"><label>50</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kerr</surname>, <given-names>J. N. D.</given-names></string-name>, <string-name><surname>Greenberg</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Helmchen</surname>, <given-names>F</given-names></string-name></person-group>. <article-title>Imaging input and output of neocortical networks in vivo</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>102</volume>, <fpage>14063</fpage>–<lpage>14068</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c51"><label>51</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de la Rocha</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Doiron</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Shea-Brown</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Josić</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Reyes</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Correlation between neural spike trains increases with firing rate</article-title>. <source>Nature</source> <volume>448</volume>, <fpage>802</fpage>–<lpage>806</lpage>, doi:<pub-id pub-id-type="doi">10.1038/nature06028</pub-id> (<year>2007</year>).</mixed-citation></ref>
<ref id="c52"><label>52</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaas</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Gharbawie</surname>, <given-names>O. A.</given-names></string-name> &amp; <string-name><surname>Stepniewska</surname>, <given-names>I</given-names></string-name></person-group>. <article-title>The organization and evolution of dorsal stream multisensory motor pathways in primates</article-title>. <source>Front Neuroanat</source> <volume>5</volume>, <issue>34</issue>, doi:<pub-id pub-id-type="doi">10.3389/fnana.2011.00034</pub-id> (<year>2011</year>).</mixed-citation></ref>
<ref id="c53"><label>53</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scannell</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Blakemore</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Young</surname>, <given-names>M. P</given-names></string-name></person-group>. <article-title>Analysis of connectivity in the cat cerebral cortex</article-title>. <source>The Journal of neuroscience : the official journal of the Society for Neuroscience</source> <volume>15</volume>, <fpage>1463</fpage>–<lpage>1483</lpage>, doi:<pub-id pub-id-type="doi">10.1523/jneurosci.15-02-01463.1995</pub-id> (<year>1995</year>).</mixed-citation></ref>
<ref id="c54"><label>54</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dell</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Innocenti</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Hilgetag</surname>, <given-names>C. C.</given-names></string-name> &amp; <string-name><surname>Manger</surname>, <given-names>P. R</given-names></string-name></person-group>. <article-title>Cortical and thalamic connectivity of posterior parietal visual cortical areas PPc and PPr of the domestic ferret (Mustela putorius furo)</article-title>. <source>The Journal of comparative neurology</source> <volume>527</volume>, <fpage>1315</fpage>–<lpage>1332</lpage>, doi:<pub-id pub-id-type="doi">10.1002/cne.24630</pub-id> (<year>2019</year>).</mixed-citation></ref>
<ref id="c55"><label>55</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gharbawie</surname>, <given-names>O. A.</given-names></string-name>, <string-name><surname>Stepniewska</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Burish</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Kaas</surname>, <given-names>J. H</given-names></string-name></person-group>. <article-title>Thalamocortical connections of functional zones in posterior parietal cortex and frontal cortex motor regions in New World monkeys</article-title>. <source>Cereb Cortex</source> <volume>20</volume>, <fpage>2391</fpage>–<lpage>2410</lpage>, doi:<pub-id pub-id-type="doi">10.1093/cercor/bhp308</pub-id> (<year>2010</year>).</mixed-citation></ref>
<ref id="c56"><label>56</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Padberg</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Disbrow</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Krubitzer</surname>, <given-names>L</given-names></string-name></person-group>. <article-title>The organization and connections of anterior and posterior parietal cortex in titi monkeys: do New World monkeys have an area 2?</article-title> <source>Cereb Cortex</source> <volume>15</volume>, <fpage>1938</fpage>–<lpage>1963</lpage>, doi:<pub-id pub-id-type="doi">10.1093/cercor/bhi071</pub-id> (<year>2005</year>).</mixed-citation></ref>
<ref id="c57"><label>57</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hwang</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Dahlen</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Mukundan</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Komiyama</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>History-based action selection bias in posterior parietal cortex</article-title>. <source>Nature communications</source> <volume>8</volume>, <fpage>1242</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c58"><label>58</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hattori</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Danskin</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Babic</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Mlynaryk</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Komiyama</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Area-Specificity and Plasticity of History-Dependent Value Coding During Learning</article-title>. <source>Cell</source> (<year>2019</year>).</mixed-citation></ref>
<ref id="c59"><label>59</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raposo</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name> &amp; <string-name><surname>Churchland</surname>, <given-names>A. K</given-names></string-name></person-group>. <article-title>A category-free neural population supports evolving demands during decision-making</article-title>. <source>Nature neuroscience</source> <volume>17</volume>, <fpage>1784</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c60"><label>60</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manita</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A top-down cortical circuit for accurate sensory perception</article-title>. <source>Neuron</source> <volume>86</volume>, <fpage>1304</fpage>–<lpage>1316</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c61"><label>61</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kwon</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Minamisawa</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>O’Connor</surname>, <given-names>D. H</given-names></string-name></person-group>. <article-title>Sensory and decision-related activity propagate in a cortical feedback loop during touch perception</article-title>. <source>Nature neuroscience</source> <volume>19</volume>, <fpage>1243</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c62"><label>62</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname>, <given-names>W. E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Global representations of goal-directed behavior in distinct cell types of mouse neocortex</article-title>. <source>Neuron</source> <volume>94</volume>, <fpage>891</fpage>–<lpage>907</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c63"><label>63</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Einevoll</surname>, <given-names>G. T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The scientific case for brain simulations</article-title>. <source>Neuron</source> <volume>102</volume>, <fpage>735</fpage>–<lpage>744</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c64"><label>64</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McGinley</surname>, <given-names>M. J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Waking State: Rapid Variations Modulate Neural and Behavioral Responses</article-title>. <source>Neuron</source> <volume>87</volume>, <fpage>1143</fpage>–<lpage>1161</lpage>, doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.012</pub-id> (<year>2015</year>).</mixed-citation></ref>
<ref id="c65"><label>65</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neske</surname>, <given-names>G. T</given-names></string-name></person-group>. <article-title>The Slow Oscillation in Cortical and Thalamic Networks: Mechanisms and Functions</article-title>. <source>Frontiers in neural circuits</source> <volume>9</volume>, <fpage>88</fpage>, doi:<pub-id pub-id-type="doi">10.3389/fncir.2015.00088</pub-id> (<year>2015</year>).</mixed-citation></ref>
<ref id="c66"><label>66</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title>. <source>Science</source> <volume>364</volume>, <issue>255</issue>, doi:<pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id> (<year>2019</year>).</mixed-citation></ref>
<ref id="c67"><label>67</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rasmussen</surname>, <given-names>R. N.</given-names></string-name>, <string-name><surname>Asiminas</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Carlsen</surname>, <given-names>E. M. M.</given-names></string-name>, <string-name><surname>Kjaerby</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>N. A</given-names></string-name></person-group>. <article-title>Astrocytes: integrators of arousal state and sensory context</article-title>. <source>Trends in neurosciences</source> <volume>46</volume>, <fpage>418</fpage>–<lpage>425</lpage>, doi:<pub-id pub-id-type="doi">10.1016/j.tins.2023.03.003</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c68"><label>68</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Madisen</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Transgenic mice for intersectional targeting of neural sensors and effectors with high specificity and performance</article-title>. <source>Neuron</source> <volume>85</volume>, <fpage>942</fpage>–<lpage>958</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c69"><label>69</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chan</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kovacevic</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Ho</surname>, <given-names>S. K. Y.</given-names></string-name>, <string-name><surname>Henkelman</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name><surname>Henderson</surname>, <given-names>J. T</given-names></string-name></person-group>. <article-title>Development of a high resolution three-dimensional surgical atlas of the murine head for strains 129S1/SvImJ and C57Bl/6J using magnetic resonance imaging and micro-computed tomography</article-title>. <source>Neuroscience</source> <volume>144</volume>, <fpage>604</fpage>–<lpage>615</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c70"><label>70</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spring</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lerch</surname>, <given-names>J. P.</given-names></string-name> &amp; <string-name><surname>Henkelman</surname>, <given-names>R. M</given-names></string-name></person-group>. <article-title>Sexual dimorphism revealed in the structure of the mouse brain using three-dimensional magnetic resonance imaging</article-title>. <source>Neuroimage</source> <volume>35</volume>, <fpage>1424</fpage>–<lpage>1433</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c71"><label>71</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barth</surname>, <given-names>A. M. I.</given-names></string-name> &amp; <string-name><surname>Mody</surname>, <given-names>I</given-names></string-name></person-group>. <article-title>Changes in hippocampal neuronal activity during and after unilateral selective hippocampal ischemia in vivo</article-title>. <source>Journal of Neuroscience</source> <volume>31</volume>, <fpage>851</fpage>–<lpage>860</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c72"><label>72</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name></person-group> <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial vision</source> <volume>10</volume>, <fpage>433</fpage>–<lpage>436</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c73"><label>73</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kleiner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name> &amp; <string-name><surname>Pelli</surname>, <given-names>D. G</given-names></string-name></person-group>. “<article-title>What’s new in Psychtoolbox-3?</article-title>”. <source>Perception</source> <volume>36</volume> (<year>2007</year>).</mixed-citation></ref>
<ref id="c74"><label>74</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engle</surname>, <given-names>J. R.</given-names></string-name> &amp; <string-name><surname>Barnes</surname>, <given-names>C. A</given-names></string-name></person-group>. <article-title>Characterizing cognitive aging of associative memory in animal models</article-title>. <source>Frontiers in aging neuroscience</source> <volume>4</volume> (<year>2012</year>).</mixed-citation></ref>
<ref id="c75"><label>75</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schindelin</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Fiji: an open-source platform for biological-image analysis</article-title>. <source>Nature methods</source> <volume>9</volume>, <fpage>676</fpage>–<lpage>682</lpage>, doi:<pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id> (<year>2012</year>).</mixed-citation></ref>
<ref id="c76"><label>76</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneider</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Rasband</surname>, <given-names>W. S.</given-names></string-name> &amp; <string-name><surname>Eliceiri</surname>, <given-names>K. W</given-names></string-name></person-group>. <article-title>NIH Image to ImageJ: 25 years of image analysis</article-title>. <source>Nature methods</source> <volume>9</volume>, <fpage>671</fpage>–<lpage>675</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c77"><label>77</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schindelin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rueden</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Hiner</surname>, <given-names>M. C.</given-names></string-name> &amp; <string-name><surname>Eliceiri</surname>, <given-names>K. W</given-names></string-name></person-group>. <article-title>The ImageJ ecosystem: An open platform for biomedical image analysis</article-title>. <source>Molecular reproduction and development</source> <volume>82</volume>, <fpage>518</fpage>–<lpage>529</lpage>, doi:<pub-id pub-id-type="doi">10.1002/mrd.22489</pub-id> (<year>2015</year>).</mixed-citation></ref>
<ref id="c78"><label>78</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stirman</surname>, <given-names>J. N.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>I. T.</given-names></string-name>, <string-name><surname>Kudenov</surname>, <given-names>M. W.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>S. L</given-names></string-name></person-group>. <article-title>Wide field-of-view, multi-region, two-photon imaging of neuronal activity in the mammalian brain</article-title>. <source>Nature Biotechnology</source> <volume>34</volume>, <fpage>857</fpage>–<lpage>862</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c79"><label>79</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Q.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The Allen mouse brain common coordinate framework: a 3D reference atlas</article-title>. <source>Cell</source> <volume>181</volume>, <fpage>936</fpage>–<lpage>953.</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c80"><label>80</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hira</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Spatiotemporal dynamics of functional clusters of neurons in the mouse motor cortex during a voluntary movement</article-title>. <source>Journal of Neuroscience</source> <volume>33</volume>, <fpage>1377</fpage>–<lpage>1390</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c81"><label>81</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Slotnick</surname>, <given-names>B</given-names></string-name></person-group>. <article-title>A simple 2-transistor touch or lick detector circuit</article-title>. <source>Journal of the experimental analysis of behavior</source> <volume>91</volume>, <issue>253</issue> (<year>2009</year>).</mixed-citation></ref>
<ref id="c82"><label>82</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</article-title>. <source>Biorxiv</source>, <volume>061507</volume> (<year>2017</year>).</mixed-citation></ref>
<ref id="c83"><label>83</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vogelstein</surname>, <given-names>J. T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Fast nonnegative deconvolution for spike train inference from population calcium imaging</article-title>. <source>Journal of neurophysiology</source> <volume>104</volume>, <fpage>3691</fpage>–<lpage>3704</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c84"><label>84</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <source>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. Report No. 1546-1726</source>, (<publisher-name>Nature Publishing Group</publisher-name>, <year>2018</year>).</mixed-citation></ref>
<ref id="c85"><label>85</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sussillo</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name> &amp; <string-name><surname>Shenoy</surname>, <given-names>K. V</given-names></string-name></person-group>. <article-title>A neural network that finds a naturalistic solution for the production of muscle activity</article-title>. <source>Nature neuroscience</source> <volume>18</volume>, <fpage>1025</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c86"><label>86</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smilde</surname>, <given-names>A. K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Common and distinct components in data fusion</article-title>. <source>Journal of Chemometrics</source> <volume>31</volume>, <fpage>e2900</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c87"><label>87</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knox</surname>, <given-names>J. E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>High-resolution data-driven model of the mouse connectome</article-title>. <source>Network Neuroscience</source> <volume>3</volume>, <fpage>217</fpage>–<lpage>236</lpage> (<year>2018</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105213.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Petreanu</surname>
<given-names>Leopoldo</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Champalimaud Center for the Unknown</institution>
</institution-wrap>
<city>Lisbon</city>
<country>Portugal</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study measures the functional specialization of distinct subregions within the mouse posterior parietal cortex (PPC) using mesoscopic two-photon calcium imaging during visual discrimination and choice history-dependent tasks. It presents <bold>compelling</bold> evidence supporting the existence of functional specialized subregions within the PPC. The work will be of interest to system and computational neuroscientists interested in decision-making, working memory, and multisensory integration.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105213.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study examined the functional organization of the mouse posterior parietal cortex (PPC) using meso-scale two-photon calcium imaging during visually-guided and history-guided tasks. The researchers found distinct functional modules within the medial PPC: area A, which integrates somatosensory and choice information, and area AM, which integrates visual and choice information. Area A also showed a robust representation of choice history and posture. The study further revealed distinct patterns of inter-area correlations for A and AM, suggesting different roles in cortical communication. These findings shed light on the functional architecture of the mouse PPC and its involvement in various sensorimotor and cognitive functions.</p>
<p>Strengths:</p>
<p>Overall, I find this manuscript excellent. It is very clearly written and built up logically. The subject is important, and the data supports the conclusions without overstating implications. Where the manuscript shines the most is the exceptionally thorough analysis of the data. The authors set a high bar for identifying the boundaries of the PPC subareas, where they combine both somatosensory and visual intrinsic imaging. There are many things to compliment the authors on, but one thing that should be applauded in particular is the analysis of the body movements of the mice in the tube. Anyone working with head-fixed mice knows that mice don't sit still but that almost invariable remains unanalyzed. Here the authors show that this indeed explained some of the variance in the data.</p>
<p>Weaknesses:</p>
<p>I see no major weaknesses and I only have minor comments.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105213.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The posterior parietal cortex (PPC) has been identified as an integrator of multiple sensory streams and guides decision-making. Hira et al observe that dissection of the functional specialization of PPC subregions requires simultaneous measurement of neuronal activity throughout these areas. To this end, they use wide-field calcium imaging to capture the activity of thousands of neurons across the PPC and surrounding areas. They begin by delineating the boundaries between the primary sensory and higher visual areas using intrinsic imaging and validate their mapping using calcium imaging. They then conduct imaging during a visually guided task to identify neurons that respond selectively to visual stimuli or choices. They find that vision and choice neurons intermingle primarily in the anterior medial (AM) area, and that AM uniquely encodes information regarding both the visual stimulus and the previous choice, positioning AM as the main site of integration of behavioral and visual information for this task.</p>
<p>Strengths:</p>
<p>There is an enormous amount of data and results reveal very interesting relationships between stimulus and choice coding across areas and how network dynamics relate to task coding.</p>
<p>Weaknesses:</p>
<p>The enormity of the data and the complexity of the analysis make the manuscript hard to follow. Sometimes it reads like a laundry list of results as opposed to a cohesive story.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105213.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work from Hira et al leverages mesoscopic 2-photon imaging to study large neural populations in different higher visual areas, in particular areas A and AM of the parietal cortex. The focus of the study is to obtain a better understanding of the representation of different task-related parameters, such as choice formation and short-term history, as well as visual responses in large neural populations across different cortical regions to obtain a better understanding of the functional specialization of neural populations in each region as well as the interaction of neural populations across regions. The authors image a large number of neurons in animals that either perform visual discrimination or a history-dependent task to test how task demands affect neural responses and population dynamics. Furthermore, by including a behavioral perturbation of animal posture they aim to dissociate the neural representation of history signals from body posture. Lastly, they relate their functional findings to anatomical data from the Allen connectivity atlas and show a strong relation between functional correlations on anatomical connectivity patterns.</p>
<p>Strengths:</p>
<p>Overall, the study is very well done and tackles a problem that should be of high interest to the field by aiming to obtain a better understanding of the function and spatial structure of different regions in the parietal cortex. The experimental approach and analyses are sound and of high quality and the main conclusions are well supported by the results. Aside from the detailed analyses, a particular strength is the additional experimental perturbation of posture to isolate history-related activity which supports the conclusion that both posture and history signals are represented in different neurons within the same region.</p>
<p>Weaknesses:</p>
<p>The main point that I found hard to understand was the fairly strong language on functional clusters of neurons while also stating that neurons encoded combinations of different types of information and leveraging the encoding model to dissociate these contributions. Do the authors find mixed selectivity or rather functional segregation of neural tuning in their data? More details on this and some other points are below.</p>
</body>
</sub-article>
</article>