<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107088</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107088</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107088.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Development of Auditory and Spontaneous Movement Responses to Music over the First Year of Life</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0420-2147</contrib-id>
<name>
<surname>Nguyen</surname>
<given-names>Trinh</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>Trinh.nguyen@iit.it</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6343-0433</contrib-id>
<name>
<surname>Bigand</surname>
<given-names>Félix</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1455-9297</contrib-id>
<name>
<surname>Reisner</surname>
<given-names>Susanne</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0917-9583</contrib-id>
<name>
<surname>Koul</surname>
<given-names>Atesh</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9613-8933</contrib-id>
<name>
<surname>Bianco</surname>
<given-names>Roberta</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1092-0137</contrib-id>
<name>
<surname>Markova</surname>
<given-names>Gabriela</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0472-0374</contrib-id>
<name>
<surname>Hoehl</surname>
<given-names>Stefanie</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4723-4344</contrib-id>
<name>
<surname>Novembre</surname>
<given-names>Giacomo</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>Giacomo.novembre@iit.it</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042t93s57</institution-id><institution>Neuroscience of Perception and Action Lab, Italian Institute of Technology</institution></institution-wrap>, <city>Rome</city>, <country country="IT">Italy</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03prydq77</institution-id><institution>Department of Developmental and Educational Psychology, University of Vienna</institution></institution-wrap>, <city>Vienna</city>, <country country="AT">Austria</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/038t36y30</institution-id><institution>Department of Developmental and Biological Psychology, Heidelberg University</institution></institution-wrap>, <city>Heidelberg</city>, <country country="DE">Germany</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03z3mg085</institution-id><institution>Institute for Early Life Care, Paracelsus Medical University</institution></institution-wrap>, <city>Salzburg</city>, <country country="AT">Austria</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Dubois</surname>
<given-names>Jessica</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Inserm Unité NeuroDiderot, Université Paris Cité</institution>
</institution-wrap>
<city>Paris</city>
<country country="FR">France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-07-14">
<day>14</day>
<month>07</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107088</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-04-20">
<day>20</day>
<month>04</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-05-05">
<day>05</day>
<month>05</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.05.04.649695"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Nguyen et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Nguyen et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107088-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Humans across cultures not only share the ability to recognise music but also respond to it through movement. While the sensory encoding of music is well-studied, when and how infants naturally start moving to music is largely unexplored. This study simultaneously investigates infants’ neural (auditory) responses and spontaneous movements to music during the first year of life. Neural activity (EEG) and body kinematics (markerless pose estimation) were recorded from 79 infants (aged 3, 6, and 12 months) listening to refrains of children’s music, along with shuffled, high-pitched, and low-pitched versions of the same songs. Neural data revealed that, across all ages, infants exhibit enhanced auditory responses to music compared to shuffled music, indicating that auditory encoding of music emerges early in development. Movement data revealed a different outcome. While coarse auditory-motor coupling is present at all ages, more complex structured movement patterns emerge in response to music only by 12 months. Notably, no age group demonstrated evidence of coordinated movements to music. Additionally, enhanced auditory responses to high vs low pitch were only evident at 6 months, while infants’ movements were better predicted by high-pitched compared to low-pitched music at all ages. This study provides initial insights into how the developing brain gradually transforms music into spontaneous movements of increasing complexity.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Music</kwd>
<kwd>neural encoding</kwd>
<kwd>movement</kwd>
<kwd>development</kwd>
<kwd>infancy</kwd>
<kwd>pitch</kwd>
</kwd-group>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/0472cxd90</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id award-id-type="doi">10.3030/948186</award-id>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/013tf3c58</institution-id>
<institution>Austrian Science Fund (FWF)</institution>
</institution-wrap>
</funding-source>
<award-id award-id-type="doi">10.55776/W1262</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Musicality – the biological predisposition to perceive, appreciate, and produce music (<xref ref-type="bibr" rid="c41">Honing, 2018</xref>; <xref ref-type="bibr" rid="c99">Trehub, 2003</xref>) – is increasingly recognised as a fundamental aspect of human nature. Numerous accounts suggest that engaging with music through movement is at the core of musicality (<xref ref-type="bibr" rid="c44">Honing et al., 2015</xref>; <xref ref-type="bibr" rid="c87">Schachner et al., 2009</xref>; <xref ref-type="bibr" rid="c100">Trehub et al., 2015</xref>). Functionally, such engagement can be broken down into two fundamental components of neurocognitive development: the ability to perceive and recognise music (<italic>sensory component</italic>), and the ability to produce movement responses that are temporally aligned with the musical structure, from coordinated vocalizations and percussive actions up to complex dance moves (<italic>motor component</italic>; <xref ref-type="bibr" rid="c6">Brown, 2022</xref>; <xref ref-type="bibr" rid="c99">Trehub, 2003</xref>a; <xref ref-type="bibr" rid="c101">Trevarthen, 1999</xref>). Despite this inherent predisposition toward music, the developmental trajectory of infants’ musicality remains largely unknown (see <xref ref-type="bibr" rid="c69">Nguyen et al., 2023</xref>, for a review). While there is increasing research on infant music perception, including controlled manipulations of select musical features, we know less about the translation of perception into action, namely the ontogenesis of infants’ spontaneous movements to music (see <xref ref-type="bibr" rid="c27">Fujii et al., 2014</xref>; <xref ref-type="bibr" rid="c70">Nguyen, Reisner, et al., 2023</xref>; <xref ref-type="bibr" rid="c108">Zentner &amp; Eerola, 2010</xref>). Furthermore, making our understanding of music-driven motor engagement even more incomplete, no studies to date have looked at both brain activity and spontaneous body movements simultaneously, especially during the first year of life. Accordingly, how the processing of music and its features is transformed into organised motor responses remains underexplored.</p>
<p>The <italic>sensory component</italic> of musicality, namely music perception, can be measured using electroencephalography (EEG), specifically by recording cortical auditory evoked potentials (event-related potentials [ERP]). One of these responses is the infantile P1, a phase-locked EEG positivity peaking around 200-300 ms after an auditory stimulus (<xref ref-type="bibr" rid="c9">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="c57">Kushnerenko et al., 2002</xref>; <xref ref-type="bibr" rid="c107">Wunderlich et al., 2006</xref>). The infantile P1 has been observed in response to both musical notes and speech segments. Auditory evoked potentials, when elicited isochronously, can also be captured using frequency domain analyses (<xref ref-type="bibr" rid="c15">Damsma et al., 2024</xref>; <xref ref-type="bibr" rid="c72">Novembre &amp; Iannetti, 2018</xref>) such as auditory steady-state responses (ASSR), which are also called steady-state evoked potentials (SSEP, e.g., <xref ref-type="bibr" rid="c12">Cirelli et al., 2016</xref>; <xref ref-type="bibr" rid="c68">Nave et al., 2022</xref>). These neural responses can provide insight into the developing auditory system and its ability to encode musical structure. Using these neurophysiological measures, prior research has shown that newborns and infants are sensitive to beat structure, pitch deviants and tone interval regularities (<xref ref-type="bibr" rid="c2">Bianco et al., 2025</xref>; <xref ref-type="bibr" rid="c19">Edalati et al., 2023</xref>; <xref ref-type="bibr" rid="c35">Háden et al., 2009</xref>, <xref ref-type="bibr" rid="c34">2015</xref>, <xref ref-type="bibr" rid="c33">2022</xref>; <xref ref-type="bibr" rid="c89">Stefanics et al., 2009</xref>; <xref ref-type="bibr" rid="c106">Winkler et al., 2009</xref>). Despite these promising results, the neurophysiology of early music processing – particularly its developmental trajectory – remains not fully understood. Here, our primary goal is to investigate infants’ neural encoding of music utilizing both ERP and ASSR approaches to characterize how such neural responses change across the first year of life.</p>
<p>Another component of musicality is the capacity to move to music (<italic>motor component;</italic> <xref ref-type="bibr" rid="c6">Brown, 2022</xref>; <xref ref-type="bibr" rid="c23">Fitch, 2015</xref>; <xref ref-type="bibr" rid="c44">Honing et al., 2015</xref>; <xref ref-type="bibr" rid="c100">Trehub et al., 2015</xref>). This capacity is linked to infants not only recognising musical structure but also moving their bodies in response to it. Even though this capacity appears to develop precociously, as evidenced by the fact that even 28-35-week-old foetuses move to music (<xref ref-type="bibr" rid="c50">Kisilevsky et al., 2004</xref>), very few studies have systematically examined music-driven spontaneous body movements in infants. An influential paper by <xref ref-type="bibr" rid="c108">Zentner and Eerola (2010)</xref> reported that infants across a large age range (from 5 to 24 months) showed more spontaneous rhythmic movements in response to classical music and children’s music compared to infant-directed speech. Importantly, their movements were not synchronized with the musical input, even though a small degree of tempo flexibility was observed (i.e., faster musical tempi evoked relatively faster movement periodicities). The lack of synchrony between music and body movements has also been reported in younger (i.e., 3-4 months old) infants listening to popular music (<xref ref-type="bibr" rid="c27">Fujii et al., 2014</xref>). Further, another study testing 7-month-old infants reported more movement in response to (sung) playsongs compared to lullabies but did not assess movement synchrony (<xref ref-type="bibr" rid="c70">Nguyen, Reisner, et al., 2023</xref>). Despite these initial investigations, it remains unclear when infants begin to move in response to music, which specific movements are evoked, and when these movements become coordinated with the music. Moreover, a critical limitation in existing research is the lack of a control condition to determine whether these movements are driven specifically by musical structure or reflect general motor activity in response to auditory input. As a second goal, this study is the first to systematically test the gradual development of music-induced movements in different age groups across the first year of life.</p>
<p>Music engages both sensory and motor systems, yet different musical features may differentially shape infants’ engagement with music. While rhythm has been widely studied in early music cognition, pitch is another salient acoustic cue that could play a role in auditory-motor engagement, particularly in infancy. High pitch is a defining feature of infant-directed speech (<xref ref-type="bibr" rid="c22">Fernald &amp; Simon, 1984</xref>), among other features such as exaggerated intonation, slower tempo, and simplified vocabulary (<xref ref-type="bibr" rid="c21">Fernald &amp; Kuhl, 1987</xref>; <xref ref-type="bibr" rid="c56">Kuhl &amp; Meltzoff, 1982</xref>). Similarly, infants most frequently listen to music characterized by high pitch (<xref ref-type="bibr" rid="c14">Costa-Giomi &amp; Sun, 2016</xref>; <xref ref-type="bibr" rid="c66">Nakata &amp; Trehub, 2011</xref>). Reflecting its prominence, high pitch is found to be one of the most prominent features thought to effectively capture (<xref ref-type="bibr" rid="c13">Conrad et al., 2011</xref>; <xref ref-type="bibr" rid="c18">Eckerdal &amp; Merker, 2009</xref>; <xref ref-type="bibr" rid="c96">Trainor, 1996</xref>; <xref ref-type="bibr" rid="c98">Trainor &amp; Zacharias, 1998</xref>) and guide infants’ attention (<xref ref-type="bibr" rid="c58">Lense et al., 2022</xref>; <xref ref-type="bibr" rid="c97">Trainor &amp; Desjardins, 2002</xref>). On the neural level, infants are also better at encoding pitch deviances in the high voice of polyphonic music, thus showing <italic>high voice superiority</italic> from 3 months of age (<xref ref-type="bibr" rid="c60">Marie &amp; Trainor, 2013</xref>, <xref ref-type="bibr" rid="c61">2014</xref>). Taken together, these findings indicate that higher-pitch music would amplify infants’ neural responses (i.e., sensory component) in comparison to lower-pitch music. On the other hand, we know that adults move more to music with greater energy in lower frequencies (<xref ref-type="bibr" rid="c7">Cameron et al., 2022</xref>; <xref ref-type="bibr" rid="c91">Stupacher et al., 2013</xref>, <xref ref-type="bibr" rid="c90">2016</xref>; <xref ref-type="bibr" rid="c103">Van Dyck et al., 2013</xref>). Yet, it remains unknown whether low-pitch music elicits increased movement in infants, as it does in adults, or whether infants’ attraction to high pitch also extends to enhance their motor responses. As a third goal, we thus investigate how musical pitch affects infants’ sensory and motor components.</p>
<p>We presented infants, aged 3, 6, and 12 months, with instrumental refrains of children’s songs (music), shuffled versions of the same songs (shuffled music), and transpositions of the songs that would either emphasize the melody (high pitch) or the bassline (low pitch). We recorded infants’ neural activity using EEG and specifically extracted ERPs and ASSR as indices of infants’ neural response to the various auditory stimuli. We also analysed spontaneous (full-body) movement kinematics using automated video-based motion tracking (DeepLabCut) and extracted <italic>principal movements</italic> using principal component analysis (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>, c.f. <xref ref-type="bibr" rid="c4">Bigand et al., 2024</xref>, <xref ref-type="bibr" rid="c95">Toiviainen et al., 2010</xref>). By adopting a cross-sectional design, we aimed to characterize the maturation of both auditory and movement responses across infancy. We hypothesized that auditory responses would be enhanced when triggered by music compared to shuffled music. This hypothesis was based on the notion that musical structure, notably eroded in the shuffled musical stimuli, is essential to attract infants’ attention towards predictable events (<xref ref-type="bibr" rid="c54">Kouider et al., 2015</xref>; <xref ref-type="bibr" rid="c58">Lense et al., 2022</xref>). Similarly, based on previous evidence comparing movement responses to music vs speech and silence (<xref ref-type="bibr" rid="c27">Fujii et al., 2014</xref>; <xref ref-type="bibr" rid="c108">Zentner &amp; Eerola, 2010</xref>), we expected the presence of musical structure to increase the likelihood of spontaneous movements in response to music compared to shuffled music, but we did not have a specific hypothesis about which particular movements would be produced. We further hypothesized that infants would show enhanced neural responses to high-compared to low-pitch music and explored co-occurring differences in spontaneous movements. Generally, we aimed to characterize the maturation of both auditory and motor responses as infants get older. By studying both sensory and motor components of musicality, we aimed to deepen our understanding of when and how infants learn to transform what they perceive into spontaneous movements, eventually leading to the emergence of synchronization to music (<xref ref-type="bibr" rid="c6">Brown, 2022</xref>; <xref ref-type="bibr" rid="c23">Fitch, 2015</xref>; <xref ref-type="bibr" rid="c44">Honing et al., 2015</xref>; <xref ref-type="bibr" rid="c76">Patel &amp; Iversen, 2014</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Overview of the procedure (A), experimental conditions (B), and participant sample (C).</title>
<p>(A) Infants sat in front of a screen with speakers on each side. The screen showed slowly blossoming flowers to attract infants’ attention. Caregivers (not shown) sat behind the infants and wore noise-cancelling headphones. (B) Infants listened to polyphonic auditory stimuli consisting of a melody and a bassline in four different conditions. The music condition included two children’s songs. The shuffled music condition included versions of the songs used in the music condition that were shuffled in pitch and randomized in inter-onset intervals (IOI). Stimuli belonging to the music and shuffled music conditions had the same pitches. In the high-pitch condition, the melody was shifted one octave higher than in the music condition. In the low-pitch condition, the bassline was shifted one octave lower than in the music condition. Hence, the two voices composing the high-pitch condition were one octave higher than those composing the low-pitch condition. (C) The sample included infants at 3 months (N=26), 6 months (N=26), 12 months (N=27), and an adult control sample (N=26). The dots overlaying the images represent the body parts whose movements were tracked using video-based kinematic analysis.</p></caption>
<graphic xlink:href="649695v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2">
<title>Results</title>
<p>This study included EEG and movement measurements taken from 79 infants in the first year of life, as well as EEG measurements taken from a control sample of 26 adults. Participants were exposed to two polyphonic children’s songs featuring a melody and a bassline and their manipulated versions. We investigated neural responses and motor responses to music vs shuffled music (a control condition in which we shuffled the melody and randomized the inter-onset intervals (IOI) of the music). Additionally, we contrasted neural and movement responses to high-vs low-pitch music. Music vs shuffle conditions (manipulation of structure but not pitch height) and high pitch vs low pitch conditions (manipulation of pitch height but not structure) are contrasted separately to avoid comparing conditions differing in more than one variable. We first present the neural and then the movement results.</p>
<sec id="s2a">
<title>EEG: Event-related potentials (ERP)</title>
<p><xref rid="fig2" ref-type="fig">Figure 2</xref> shows the average ERPs to the notes in the auditory stimuli (specifically bassline notes, see methods). Adults’ ERPs, which served as the ground truth to interpret infants’ responses, included an early positivity peaking at 37 ms post-stimulus (so-called “P50”, here reaching an amplitude of 1.05 µV), followed by a later negativity peaking at 87 ms post-stimulus (so-called “N100”, here reaching an amplitude of -0.43 µV) and a second positivity peaking at 158 ms post-stimulus (so-called “P200”, here reaching an amplitude of 0.85 µV). This triphasic EEG pattern has been widely observed in adults in response to fast-rising auditory stimuli and across different contexts (<xref ref-type="bibr" rid="c74">Novembre et al., 2018</xref>; <xref ref-type="bibr" rid="c81">Pratt et al., 2008</xref>; <xref ref-type="bibr" rid="c83">Remijn et al., 2014</xref>; <xref ref-type="bibr" rid="c88">Somervail et al., 2021</xref>). Cluster-based permutation analyses, contrasting the ERPs elicited by music vs shuffled music, revealed that the amplitude of the P50 – hereafter referred to as P1 – was larger in response to music compared to shuffled music, particularly within the time range comprised between -17 and 58 ms post-stimulus (<italic>cluster-t</italic>=366.16, <italic>p</italic>=.016). Similarly, the amplitude of the following P200 - hereafter referred to as P2 – was larger in response to music than to shuffled music, particularly within the time range of 114 to 190 ms post-stimulus (<italic>cluster-t</italic>=395.42, <italic>p</italic>=.016). Both P1 and P2 responses were observed over fronto-central electrodes, showing a medial distribution, in line with previous literature (e.g., <xref ref-type="bibr" rid="c59">Lijffijt et al., 2009</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Event-related potentials (ERPs) elicited by the notes comprised within the music (orange, left) vs shuffled music (khaki, left) as well as by the notes comprised within the high-pitch (light blue, right) vs low-pitch music (purple, right), across four groups of participants (plotted in ascending order of age, from top to bottom): 3-, 6-, 12-month-old infants (N=79) and adults (N=26).</title>
<p>Grand-average ERPs are averaged across electrodes within the significant cluster of each age group in the music condition (except for pitch condition comparison in the 6-month-olds). Shaded areas indicate the standard error. ERPs show progressively shorter latencies with increasing age. All groups exhibited a P1 response, while only older infants (12-month-olds) and adults additionally exhibited a P2. Music elicited a larger P1 (and, when present, P2) amplitude compared to shuffled music, notably across all groups (time ranges associated with a significant difference are indicated by horizontal black lines). The topography of this neural response (averaged across the time window of the P1 cluster) in the music condition shifted more medially with increasing age. Colorbars beneath topography plots index EEG amplitude values.</p></caption>
<graphic xlink:href="649695v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>All infants’ ERPs showed a P1 response, while a P2 response was observed only in 12-month-old infants, albeit with a lower amplitude than the P1. The P1 latency decreased (<italic>Χ<sup>2</sup></italic>(2)=391.25, <italic>p</italic>&lt;.001), and its amplitude increased (<italic>Χ<sup>2</sup></italic>(2)=8.59, <italic>p</italic>=.014) with age (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, left). Importantly, and in line with the adults’ data, all infant groups exhibited enhanced P1 amplitudes in response to music compared to shuffled music. Cluster-based permutation (<italic>n<sub>Perm</sub></italic>=1000) testing revealed that 3-month-old infants’ P1 amplitude was enhanced between 177 and 305 ms post-stimulus (<italic>cluster-t</italic>=1111.90, <italic>p</italic>=.002), peaking at 212 ms and reaching an amplitude of 1.8 µV. The topography included a frontocentral cluster with a slight right lateralization. In 6-month-old infants, the amplitude of the P1 was enhanced between 116 and 284 ms post-stimulus (<italic>cluster-t</italic>=1401.60, <italic>p</italic>=.002), peaking at 165 ms and reaching an amplitude of 2.8 µV. The topography included a few (centro-) parietal electrodes in addition to several frontocentral electrodes, with a bilateral activation. In 12-month-old infants, the amplitude of the neural response to music was enhanced in a two-peak cluster (<italic>cluster-t</italic>=1416.30, <italic>p</italic>=.002). The first peak, an infantile P1, occurred between 104 and 227 ms, peaked at 146 ms post-stimulus and reached an amplitude of 3.1 µV. Notably, 12-month-old infants exhibited an additional positivity, namely an infantile P2, possibly homologous to the P200 observed in adults. The P2 ranged between 307 and 325 ms post-stimulus and peaked at 316 ms, reaching an average amplitude of 1.026 µV. The topographies remained frontocentral but were more medial, similar to adults.</p>
<p>Next, we examined neural responses to the notes in the high- and low-pitch conditions (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, right). The morphology of both adults’ and infants’ ERPs was generally comparable to that elicited by the music condition. Cluster-based permutation (<italic>n<sub>Perm</sub></italic>=1000) testing revealed that the amplitude of the adults’ ERPs was comparable across high- and low-pitch conditions (<italic>ps</italic> &gt; .050). This was also the case for both 3- and 12-month-old infants, but notably not for 6-month-old infants, who exhibited an enhanced P1 in response to high-pitch vs low-pitch conditions (<italic>cluster-t</italic>=763.84, <italic>p</italic>=.002). This enhanced positivity (178 and 332 ms) peaked at 204 ms and reached an average amplitude of 2.8 µV. Similar to the neural response elicited by the music condition, the topography included few (centro-) parietal electrodes in addition to several frontocentral electrodes, with a bilateral activation.</p>
</sec>
<sec id="s2b">
<title>EEG: Auditory Steady State Responses (ASSR)</title>
<p><xref rid="fig3" ref-type="fig">Figure 3</xref> shows bar plots indexing the relative power of ASSRs elicited by the auditory stimuli (power estimates were averaged across the electrodes comprised within the ERP clusters that were common to all age groups, i.e., FP2, F7, F3, Fz, F4, F8, FC7, FC3, FCz, FC4, FC8, C3, Cz, C4; see <xref rid="fig2" ref-type="fig">Fig. 2</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Relative EEG Power (arbitrary unit [a.u.], y-axis) of the auditory steady-state responses (ASSR) elicited by music versus shuffled music (orange and khaki, left), and high-pitch versus low-pitch musical stimuli (blue and purple, right), across four groups of listeners: 3-month-olds (first row), 6-month-olds (second row), 12-month-olds (third row), and adults (fourth row).</title>
<p>ASSR power estimates at the frequency (x-axis) matching the musical beat (2.25 Hz, highlighted by vertical dashed lines and including standard error bars) were statistically higher when elicited by music compared to shuffled music across nearly all participant groups (i.e., all but 6-month-olds). High- and low-pitch stimuli evoked similar ASSR (at 2.25 Hz). These results generally align with the ERP results (<xref ref-type="fig" rid="fig2">Fig. 2</xref>) across most infant groups and adults, except for 6-month-old infants for whom differences across conditions were either trending (music vs shuffled) or not significant (high vs low pitch).</p></caption>
<graphic xlink:href="649695v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Our frequency of interest was 2.25 Hz, matching the musical beat as well as the presentation rate of the majority of the notes (15 out of 16 notes) in the auditory sequences (see methods, section “Stimuli”). We used linear mixed models, including power estimates as the dependent variable to contrast the different conditions and age groups (fixed and interaction effects). Participants were modelled as random intercepts. Power estimates were generally higher in response to music as opposed to shuffled music. Model outputs indicated that the power of the ASSR elicited by the music condition was significantly higher than that elicited by the shuffled music condition in 3-month-old infants (<italic>F</italic>(1,50)=7.82, <italic>p</italic>=.007), 12-month-old infants (<italic>F</italic>(1,52)=12.03, <italic>p</italic>=.001) and adults (<italic>F</italic>(1,50)=13.49, <italic>p</italic>&lt;.001); while it only reached a trend for significance in 6-month-old infants (<italic>F</italic>(1,50)=2.95, <italic>p</italic>=.092). ASSR power estimates were not different across high-pitch and low-pitch conditions across all age groups (<italic>ps</italic>&gt;.240). These results seem to be generally in line with the ERP results and suggest that most groups showed stronger neural responses to music than shuffled music (even though 6-month-old infants only showed a marginal difference), leading to an enhancement of power in ASSR at a frequency matching the musical beat. This enhancement, however, was not sensitive to the relative pitch of the music.</p>
</sec>
<sec id="s2c">
<title>Extraction of Principal Movements and Estimation of Quantity of Movement</title>
<p>Using principal component analysis, we decomposed full-body kinematics into 10 Principal Movements (PMs), explaining 79.7 % of the whole kinematic variance. The PMs (depicted in <xref rid="fig4" ref-type="fig">Fig. 4</xref>) were reminiscent of common infant movements such as front-back rocking (PM1), side sway (PM2), proto-clapping (PM3), leg kicking (PM4), up-down rocking (PM5), arm pedalling (PM6), feet kicking (PM7), whole body wiggling (PM8), feet shuffling (PM9) and feet pedalling (PM10). Labels were assigned qualitatively, following visual inspection of the PMs.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Infants’ principal movements (PMs).</title>
<p>PMs are illustrated by showing the two most different body postures (min and max of the PM score, in grey and black, respectively) from the frontal perspective. The reader should interpret the PM as the kinematic displacement necessary to shift from one body posture (grey) to the other (black). Circle diagrams denote the proportion (%) of kinematic variance explained by each PM. Together, the ten PMs account for 79.7% of the total kinematic variance.</p></caption>
<graphic xlink:href="649695v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The quantity of Movement (QoM) was estimated and compared across PMs, conditions, and groups (<xref rid="fig5" ref-type="fig">Fig. 5</xref>). QoM estimates were based on first-degree differentiation of the PM time series (see methods). A random intercept was included for each infant. Linear mixed effect modelling yielded a significant interaction between Condition and Age (χ²(2)=16.76, <italic>p</italic>&lt;.001) indicating that only 12-month-old infants exhibited higher QoM in response to music compared to shuffled music, which was numerically the case across all PMs (post-hoc contrasts with adjusted <italic>p</italic>-values: <italic>t(69.8)</italic>=4.86, <italic>p</italic>&lt;.001). Even though there was no interaction effect between Condition, Age and PMs, we still ran post-hoc comparisons to gain preliminary evidence about specific PMs driving the above-described effects. Results indicated that differences in 12-month-olds’ QoM in response to music vs shuffled music were mostly driven by movements of the upper body and/or upper limbs. Specifically, front-back rocking (PM1), side sway (PM2), proto-clapping (PM3), up-down rocking (PM5) and arm pedalling (PM6) were linked with significantly higher QoM in response to music as opposed to shuffled music (PMs 1,2,3,5,6; <italic>ps</italic>&lt;.050, corrected using the false-discovery rate). Contrarily, younger infants (3- and 6-month-olds) did not exhibit significantly different QoM in response to music vs shuffled music in any of the PMs (<italic>ps</italic>&gt;.123). Further, when comparing infants’ QoM to music at different pitches, we found no significant differences between conditions across PMs and age groups (<italic>ps</italic>&gt;.295).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Quantity of movement (mean, a.u.; [QoM]) elicited by music (orange) versus shuffled music (khaki) and high-pitch (blue) versus low-pitch music (purple) across different age groups (3-month-olds, 6-month-olds, 12-month-olds) and principal movements (PMs).</title>
<p>Bar plots indicate the mean and standard error of QoM across different age groups, conditions, and PMs. Only twelve-month-old infants showed significantly increased QoM in response to music compared to shuffled music, specifically in PMs involving upper body movements (front-back rocking, side sway, proto-clapping, up-down rocking, and arm pedalling). No significant differences were observed between high- and low-pitch conditions. These results were also replicated in a supplementary analysis assessing differences in variance of (as opposed to mean) QoM (see Fig. S1 and Supplements for more details). † = p&lt;.100, * = p&lt;.050, ** = p&lt;.010, *** = p&lt;.001</p></caption>
<graphic xlink:href="649695v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The model also yielded a significant interaction between PMs and Age (χ²(18)=181.575, <italic>p</italic>&lt;.001), indicating that QoM generally increased with age but differently across PMs. Specifically, from 3 to 6 months, the PMs associated with higher QoM were front-back rocking (PM1), proto-clapping (PM3) and arm pedalling (PM6) (<italic>t</italic>(95.4)=2.34-2.60, <italic>p</italic>=.016-.032). From 3 to 12 months, front-back rocking (PM1), side sway (PM2), proto-clapping (PM3), up-down rocking (PM5) and arm pedalling (PM6) became more prevalent (<italic>t</italic>(95.4)=2.69-5.06, <italic>p</italic>=.001-.025). From 6 to 12 months, proto-clapping (PM3) became even more prevalent (<italic>t</italic>(95.4)=2.71, <italic>p</italic>=.012). Across the first year of life, infants seemed to consistently move their lower body while slowly increasing their capacity for upper-body and whole-body movements while seated.</p>
</sec>
<sec id="s2d">
<title>Movement: Granger Causality Analysis</title>
<p>Beyond looking at how much infants moved, we further investigated whether the spontaneous occurrence of infant movements could be explained by preceding changes in the intensity of the auditory stimuli. To do so, we used the sound envelope of the auditory stimuli (indexing changes in intensity over time) to predict infant movement velocity (time series representing changes in movement velocity, averaged across all PMs) and vice versa, using Granger-Causality analysis. Prediction estimates (Granger F-values) were yielded across different time lags, indexing the elapsed time necessary for a change in stimulus intensity to predict a change in movement velocity (i.e., highest F-values represent optimal prediction).</p>
<p>A preliminary analysis (sanity check) showed that musical stimuli predicted subsequent movement velocity better than vice versa across all age groups (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, top-right corner; <italic>F</italic>(1)=94.62, <italic>p</italic>&lt;.001). To identify the optimal temporal lags predicting sound-driven changes in movement, we conducted bootstrapped t-tests corrected for multiple comparisons across time points. Results (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, left) showed that musical stimuli were better predictors of movement than shuffled stimuli at 3 months (lags of 160-200 ms, <italic>t</italic>(40)=3.55-3.98, <italic>p</italic>&lt;.001), 6 months (lags of 120-240 ms, <italic>t</italic>(37)=2.99-4.49, <italic>p</italic>&lt;=.001) and 12 months (lags of 160-240 ms, <italic>t</italic>(31.6)=4.95-5.03, <italic>p</italic>&lt;.001). Conversely, there was a drop in prediction at later time lags in all groups (∼320-360 ms; <italic>t</italic>(43.6)=3.88-8.25, <italic>p</italic>&lt;.001). Together, these results suggest that infants’ movement was related to intensity changes in the music, but not in the shuffled music.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Music-driven movement (Granger-Causality analysis).</title>
<p>Top-right: A sanity check analysis showed that musical stimuli predicted subsequent movement velocity (green) better than vice versa (grey; p&lt;.001). Left: Movement velocity was better predicted by music (orange) than by shuffled music (khaki), particularly with time lags of 160-200 ms (shaded areas indicate standard errors; horizontal black lines underline time ranges associated with a significant difference between conditions). Right: Movement was better predicted by high-pitch music (blue) compared to low-pitch music (purple).</p></caption>
<graphic xlink:href="649695v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Results associated with the high- and low-pitch conditions yielded similar Granger F-values to the music condition (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, right). Notably, prediction estimates were generally higher for the high-pitch condition as compared to the low-pitch condition, indicating that high-pitch music was a better predictor of movement than low-pitch music. The difference between these two conditions was significant in one time-window in the 3-month-old infants (80-200 ms, <italic>t</italic>(39.26)=2.66-3.81, <italic>p</italic>&lt;.002), while it encompassed two time windows in the 6-(120-200 ms, <italic>t</italic>(45.64)=3.14-4.21, <italic>p</italic>&lt;=.004, and 320-360 ms, <italic>t</italic>(45.7)=3.32-3.82, <italic>p</italic>&lt;=.004) and 12-month-olds (120-160ms, <italic>t</italic>(40.4)=3.20-3.73, <italic>p</italic>&lt;=.002 and 520-600 ms, <italic>t</italic>(39.9)=3.13-3.41, <italic>p</italic>&lt;.004) – perhaps suggesting that this effect grows stronger with age. Finally, Granger Causality statistics stratified by each PM and age group are detailed in the Supplements and <italic>Fig. S1</italic>.</p>
</sec>
<sec id="s2e">
<title>Movement: Phase-locked changes and periodicity</title>
<p>The Granger Causality analysis indicated that changes in music intensity drove movement in time, especially at a time lag comprised within a 200 ms delay. This result indirectly suggests that changes in music might evoke a phase-locked movement response. If so, we should be able to observe such phase-locked response when epoching movement data to peaks in the amplitude envelope of the auditory stimuli. To test this prediction, we ran supplementary event-related analyses on the movement velocity time series used for the previous analysis. Cluster-based permutation analyses revealed no significant clusters across age groups and conditions (<italic>ps</italic>&gt;.050; see Supplements, <italic>Fig. S2</italic>), even though it should be noted that 12-month-old infants exhibited a movement peak at ∼200 ms, which was slightly but not significantly higher in response to music vs shuffled conditions. These results indicated that infants did not consistently exhibit phase-locked movement responses to musical events such as peaks in the amplitude envelope of the auditory stimuli.</p>
<p>Next, we also examined to what extent spontaneous movements were periodic and how so across conditions. This analysis builds upon the distribution of the highest coefficients yielded by auto-correlation analyses (similar to <xref ref-type="bibr" rid="c108">Zentner &amp; Eerola, 2010</xref>). The results indicated that overall, spontaneous movements tended to be periodic, but such periodicity did not match the musical beat and was not different across conditions (see <italic>Supplements</italic>). In other words, movement periodicity did not result in coordination with music, and it was not modulated by whether infants were listening to music vs shuffled music or high-vs low-pitch music (<italic>Fig. S3</italic>). Together, these results indicate that while infants might generally exhibit rhythmic movements in response to sounds, the specificity of this behaviour in response to music is still to develop after 12 months of age.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>This study examined the development of infants’ neural and movement responses to music during the first year of life using a combination of neural measures, such as ERP and ASSR, alongside quantitative analyses of infants’ movement. These approaches allowed us to explore both sensory and motor components of musicality, shedding light on how infants process and respond to musical stimuli. Below, we discuss 1) the development of neural auditory responses to music, 2) the emergence of music-driven movement patterns, and 3) the sensitivity of both components to pitch across early development.</p>
<sec id="s3a">
<title>Neural Responses to Music: Sensitivity to structured music across all ages</title>
<p>Much research suggests that the infants’ auditory system is sensitive to structural features of music, such as timing and pitch regularities, from early on (<xref ref-type="bibr" rid="c86">Saffran et al., 1999</xref>; <xref ref-type="bibr" rid="c94">Thiessen &amp; Saffran, 2009</xref>; see <xref ref-type="bibr" rid="c69">Nguyen et al., 2023</xref> for a review). However, the developmental trajectory of such musical sensitivity across the first year of life is still underexplored. Here, we provide a detailed characterization of the progressive maturation of auditory evoked potentials, neurophysiological measures that generally capture rapid changes in the sensory environment, such as the onset of musical notes (<xref ref-type="bibr" rid="c57">Kushnerenko et al., 2002</xref>; <xref ref-type="bibr" rid="c88">Somervail et al., 2021</xref>). Specifically, we observed ERPs with progressively shorter latency and larger amplitude throughout the first year of life. While these findings likely reflect a general maturation of sensory systems, they also highlight how the evoked potentials elicited by musical stimuli were enhanced in amplitude compared to those elicited by shuffled (i.e., structure-free) stimuli, an effect consistently observed across all age groups. This is in line with the idea that sensitivity to musical regularities emerges early and persists throughout infancy (<xref ref-type="bibr" rid="c101">Trevarthen, 1999</xref>). Indeed, sensitivity to simple rhythmic or pitch regularities has been observed in infants at various ages (<xref ref-type="bibr" rid="c12">Cirelli et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Edalati et al., 2023</xref>; <xref ref-type="bibr" rid="c24">Flaten et al., 2022</xref>; <xref ref-type="bibr" rid="c35">Háden et al., 2009</xref>, <xref ref-type="bibr" rid="c33">2022</xref>; <xref ref-type="bibr" rid="c89">Stefanics et al., 2009</xref>; <xref ref-type="bibr" rid="c106">Winkler et al., 2009</xref>), and even in premature neonates (<xref ref-type="bibr" rid="c85">Saadatmehr et al., 2025</xref>). Such sensitivity may be rooted in general auditory predictive processing, whereby the brain extracts regularities from past observations to generate and update predictions about incoming sensory information (<xref ref-type="bibr" rid="c26">Friston, 2010</xref>; <xref ref-type="bibr" rid="c51">Köster et al., 2020</xref>; <xref ref-type="bibr" rid="c104">Vuust &amp; Witek, 2014</xref>). Ecologically valid music, such as the refrains used here, naturally incorporates rhythmic and pitch regularities that could trigger the generation of predictions – a process markedly dampened by shuffled music (<xref ref-type="bibr" rid="c3">Bianco et al., 2024</xref>, <xref ref-type="bibr" rid="c2">2025</xref>; <xref ref-type="bibr" rid="c58">Lense et al., 2022</xref>). Which specific musical regularities drove infants’ predictions in our study? Previous EEG research suggests that in newborns, probabilistic auditory predictions are primarily driven by timing rather than pitch regularities when listening to ecologically valid music (<xref ref-type="bibr" rid="c2">Bianco et al., 2025</xref>). In contrast, adults rely on both timing and pitch regularities to generate predictions (<xref ref-type="bibr" rid="c17">Di Liberto et al., 2020</xref>). Given that our study examined infants older than newborns but not yet as mature as adults, it is likely that timing regularities played a key role for the youngest participants, while pitch regularities may have had a greater influence on the older age groups. Future research should investigate when, during development, the human brain begins incorporating pitch alongside timing regularities to generate musical predictions.</p>
<p>To explore the underlying neural mechanisms driving the above-described neural process, we analysed ASSR besides ERPs. This was done under the assumption that if the described results were driven by neural entrainment to the beat of the music, then ASSR might better capture differences across conditions, as this measure can capture oscillatory activity. Instead, if the results were driven by evoked responses, then the ASSR results would generally align with ERP results or even be slightly less robust, given that frequency-domain analysis is less suited for capturing time-domain neural modulations. Results showed enhanced ASSR at a frequency matching the musical beat, with stronger power in response to music as opposed to shuffled stimuli and no differences between high- and low-pitch musical stimuli. Hence, ASSR and ERP results were very similar, a conclusion that is in line with previous accounts suggesting that on some occasions, the two measures might originate from the same signal, i.e., evoked responses (<xref ref-type="bibr" rid="c8">Capilla et al., 2011</xref>; <xref ref-type="bibr" rid="c72">Novembre &amp; Iannetti, 2018</xref>). Further, building on evidence indicating that the infantile P1 originates from the auditory cortex (<xref ref-type="bibr" rid="c9">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="c65">Musacchia et al., 2017</xref>; <xref ref-type="bibr" rid="c84">Riva et al., 2018</xref>), this result sheds light on one of several underlying neural structures that might be responsible for identifying musical structure in the auditory input.</p>
</sec>
<sec id="s3b">
<title>Motor engagement with Music: Movement Sophistication develops with Age</title>
<p>In adults, music engages not only the auditory system but also the motor system (<xref ref-type="bibr" rid="c28">Fujioka et al., 2012</xref>; <xref ref-type="bibr" rid="c30">Grahn &amp; Brett, 2007</xref>; <xref ref-type="bibr" rid="c73">Novembre &amp; Keller, 2018</xref>; <xref ref-type="bibr" rid="c80">Phillips-Silver &amp; Keller, 2012</xref>), often leading to spontaneous motor responses coordinated with the musical input (<xref ref-type="bibr" rid="c46">Hurley et al., 2014</xref>; <xref ref-type="bibr" rid="c47">Janata et al., 2012</xref>). While some evidence indicates that such spontaneous body movements are also exhibited by infants (<xref ref-type="bibr" rid="c82">Provasi et al., 2014</xref>), it remains unclear how this behaviour matures throughout development and how sophisticated it is.</p>
<p>First, as a coarse measure of auditory-motor coupling, we used Granger Causality to predict the time course of body movements using the time course of the auditory input (specifically the amplitude envelope). Taking this approach, we could predict body movements of infants from 3 to 12 months, specifically while listening to music as opposed to the shuffled conditions. This result indicates that recognising musical structures leads not only to distinct neural encoding patterns (as discussed in the previous section) but also to different levels of motor engagement. Further, as this result was observed across all ages, we might speculate that such audio-motor coupling, specifically triggered by music, emerges early in development and might be biologically predisposed. This conclusion aligns with previous findings showing that even foetuses as young as 28–35 weeks gestational age exhibit movement responses to musical sounds (<xref ref-type="bibr" rid="c50">Kisilevsky et al., 2004</xref>). However, it is important to note that these prenatal motor responses were not compared to motor responses to other auditory stimuli, as in our study, therefore raising questions about their specificity.</p>
<p>Next, taking a more complex measure of movement, we used principal component analysis to break down body kinematics into 10 independent principal movements (PMs; <xref ref-type="bibr" rid="c4">Bigand et al., 2024</xref>; <xref ref-type="bibr" rid="c95">Toiviainen et al., 2010</xref>) that explained nearly 80% of the whole kinematic variance. To the best of our knowledge, this approach has never been adopted for the analysis of infants’ datasets, which normally do not distinguish between different kinds of movements (<xref ref-type="bibr" rid="c27">Fujii et al., 2014</xref>; <xref ref-type="bibr" rid="c70">Nguyen, Reisner, et al., 2023</xref>; <xref ref-type="bibr" rid="c108">Zentner &amp; Eerola, 2010</xref>) or do so qualitatively (<xref ref-type="bibr" rid="c92">Thelen 1979</xref>, <xref ref-type="bibr" rid="c93">1981</xref>). Building on this data-driven approach, we compared to what extent different movements were exhibited by infants across different ages and conditions. We found that only 12-month-old infants exhibited more movement in response to music compared to shuffled music. This result was driven by specific upper-body movements such as front-back rocking, side swaying, proto-clapping, and arm pedalling. It is not straightforward to interpret why these (and not other) movements were triggered by music, and why only at this age. Potential explanations include the development of refined postural control, typically achieved by 9–10 months (<xref ref-type="bibr" rid="c32">Hadders-Algra, 2005</xref>), and the fact that infants were tested in a seated position (a measure taken to simplify comparability across age groups). This seating arrangement likely facilitated upper body movements as the feet were resting. Further studies are needed to systematically characterize infants’ movements to music across different contexts.</p>
<p>Finally, we examined movement coordination – a potential precursor of dance – and tested whether the periodicity of spontaneous movements matched the periodicity of the music. We did not find any evidence of movement coordination in none of the age groups. This result is in line with previous studies (<xref ref-type="bibr" rid="c27">Fujii et al., 2014</xref>; <xref ref-type="bibr" rid="c108">Zentner &amp; Eerola, 2010</xref>) and suggests that sensorimotor transformation of music, specifically the ability to preserve its periodicity, develops after the first year of life. This delayed emergence may be due to motor control skills required for such transformation, which continue to mature into toddlerhood and even middle childhood (<xref ref-type="bibr" rid="c49">Kim &amp; Schachner, 2022</xref>; <xref ref-type="bibr" rid="c79">Phillips-Silver et al., 2024</xref>). Hence, while coarse auditory-motor coupling is present at all ages, diversified movement patterns emerge only by 12 months, and spontaneous coordination with music likely continues to develop throughout infancy into childhood. We suggest that the increasing complexity of infants’ motor response to music is linked to the gradual maturation of the dorsal auditory stream, which connects posterior regions of the superior temporal gyrus with premotor cortices (e.g., <xref ref-type="bibr" rid="c10">Chen et al., 2009</xref>; <xref ref-type="bibr" rid="c30">Grahn &amp; Brett, 2007</xref>; <xref ref-type="bibr" rid="c53">Kotz et al., 2018</xref>). Although this pathway is present at birth (<xref ref-type="bibr" rid="c25">Friederici, 2011</xref>; <xref ref-type="bibr" rid="c78">Perani et al., 2010</xref>), it has been suggested to play a crucial role in rhythmic entrainment and beat perception, functions that likely develop further as the pathway matures (<xref ref-type="bibr" rid="c41">Honing, 2018</xref>; <xref ref-type="bibr" rid="c64">Merchant &amp; Honing, 2014</xref>; <xref ref-type="bibr" rid="c76">Patel &amp; Iversen, 2014</xref>). Our suggestion is indirectly supported by comparative work on non-human primates, whose dorsal auditory stream is less developed than in humans (<xref ref-type="bibr" rid="c64">Merchant &amp; Honing, 2014</xref>; <xref ref-type="bibr" rid="c76">Patel &amp; Iversen, 2014</xref>). Indeed, research suggests that adult macaques struggle to recognise musical beats (<xref ref-type="bibr" rid="c43">Honing et al., 2012</xref>, <xref ref-type="bibr" rid="c42">2018</xref>), while adult chimpanzees—though capable of adjusting their rhythmic sway periodicity in response to the beat (<xref ref-type="bibr" rid="c36">Hattori, 2021</xref>; <xref ref-type="bibr" rid="c37">Hattori &amp; Tomonaga, 2020</xref>)—are unable to match it accurately, much like infants in our study and previous literature (<xref ref-type="bibr" rid="c108">Zentner &amp; Eerola, 2010</xref>).</p>
</sec>
<sec id="s3c">
<title>Pitch Sensitivity: Auditory and Movement preferences for High-Pitched music</title>
<p>At 6 months, infants exhibited enhanced neural responses to high-pitch music compared to low-pitch music, a specificity not observed at 12 months or in adults. This transient enhancement may reflect a critical period of heightened auditory plasticity, potentially supporting the detection of socially relevant stimuli (<xref ref-type="bibr" rid="c21">Fernald &amp; Kuhl, 1987</xref>; <xref ref-type="bibr" rid="c55">Kuhl, 2010</xref>). Indeed, high-pitched sounds are prominent in infant-directed speech and singing, which are integral to caregiver-infant interactions and social communication during early infancy (<xref ref-type="bibr" rid="c21">Fernald &amp; Kuhl, 1987</xref>; <xref ref-type="bibr" rid="c38">Hilton et al.., 2022</xref>; <xref ref-type="bibr" rid="c66">Nakata &amp; Trehub, 2011</xref>; <xref ref-type="bibr" rid="c98">Trainor &amp; Zacharias, 1998</xref>; <xref ref-type="bibr" rid="c102">Tsang &amp; Conrad, 2010</xref>). At six months, caregiver-infant face-to-face interactions peak (<xref ref-type="bibr" rid="c1">Beebe et al., 2016</xref>; <xref ref-type="bibr" rid="c20">Feldman, 2007</xref>), with infants relying more on behavioural cues, often conveyed through auditory modalities (<xref ref-type="bibr" rid="c31">Gratier et al., 2015</xref>; <xref ref-type="bibr" rid="c71">Nguyen, Zimmer, et al., 2023</xref>), and less on physical objects than in later developmental stages. Thus, the enhanced sensitivity to high-pitched music at 6 months may reflect an essential phase in infants’ developing abilities to process both musical and communicative exchanges (Shenfield et al., 2003). As infants grow, their early sensitivity to high pitch, likely shaped by exposure to infant-directed speech and singing, may become integrated with broader auditory processing capabilities. As their auditory system matures, their ability to process lower-pitched sounds may develop further, leading to more balanced neural encoding of both high- and low-pitch music by 12 months and into adulthood. Additionally, an expanding attentional focus beyond the caregiver (<xref ref-type="bibr" rid="c77">Pauen et al., 2015</xref>) may contribute to this shift.</p>
<p>Why high-pitch music was generally associated with enhanced auditory-motor coupling compared to low-pitch music is more difficult to explain. High-pitch music might have led to higher arousal, akin to infant-directed songs (<xref ref-type="bibr" rid="c11">Cirelli et al., 2020</xref>; <xref ref-type="bibr" rid="c48">Juslin &amp; Laukka, 2003</xref>), which in turn strengthened the coupling between spontaneous movements and music. Conversely, low-pitch music might have led to lower arousal or generally reduced attentional processes, thereby weakening auditory-motor coupling. Either way, it is difficult to reconcile this result with the ERP results showing enhanced auditory processing of high-pitch music only at 6 months. Perhaps, as infants mature, their sensitivity to high-pitched sounds may integrate with not only broader auditory but also motor processing mechanisms. Such a developmental trajectory might explain the reduced neural specificity to high-pitch music by 12 months, alongside a shift toward more coordinated motor responses to lower-pitched stimuli in adulthood (<xref ref-type="bibr" rid="c7">Cameron et al., 2022</xref>; <xref ref-type="bibr" rid="c45">Hove et al., 2014</xref>; <xref ref-type="bibr" rid="c90">Stupacher et al., 2016</xref>). These speculations could be addressed by future research exploring how musical pitch and rhythm interact to drive movement beyond infancy into adulthood to particularly examine <italic>when</italic> neural and motor engagement with low-pitch music becomes more prominent (<xref ref-type="bibr" rid="c7">Cameron et al., 2022</xref>; <xref ref-type="bibr" rid="c91">Stupacher et al., 2013</xref>, <xref ref-type="bibr" rid="c90">2016</xref>).</p>
</sec>
<sec id="s4">
<title>Conclusion</title>
<p>Our study demonstrates that while robust auditory processing of music is present as early as 3 months, the translation of these sensory processes into organised motor behaviours unfolds gradually over and beyond the first year of life. Specifically, while coarse auditory-motor coupling is present at all ages, diversified movement patterns emerge only by 12 months, and spontaneous coordination with music likely develops throughout infancy and into childhood. Hence, our study provides evidence that, much like the auditory encoding of music, the propensity to move in response to music emerges early in development. This may reflect a biological or early-developing predisposition, eventually leading to dance-like behaviour. However, by 12 months, these motor responses remain relatively underdeveloped. Additionally, our study points to a previously unknown association between high-pitch music and auditory-motor coupling, extending the current research focus on infant-directed communication towards motor engagement and spontaneous behaviour. Together, these findings provide initial insights into how the developing brain gradually transforms music into spontaneous movements of increasing complexity. Future research should extend our characterisation of music-induced movement beyond the first year of life and further explore its (to-date mysterious) functional significance (<xref ref-type="bibr" rid="c39">Hoehl et al., 2020</xref>; <xref ref-type="bibr" rid="c62">Markova et al., 2019</xref>; <xref ref-type="bibr" rid="c69">Nguyen, Flaten, et al., 2023</xref>; <xref ref-type="bibr" rid="c105">Wass et al., 2020</xref>).</p>
</sec>
</sec>
<sec id="s5">
<title>Materials &amp; Methods</title>
<p>The study was preregistered: <ext-link ext-link-type="uri" xlink:href="https://aspredicted.org/WW3_3TF">https://aspredicted.org/WW3_3TF</ext-link>. Deviations are listed in the Supplement.</p>
<sec id="s5a">
<title>Infant Experiment</title>
<p>Ninety-eight full-term infants, divided into three different age groups (see <xref rid="fig1" ref-type="fig">Fig. 1C</xref>), participated in this experiment. Nineteen infants were excluded due to fussiness (n=9) or technical issues (n=10), leading to a 19,6 % attrition rate (in line with infant EEG studies; <xref ref-type="bibr" rid="c40">Hoehl &amp; Wahl, 2012</xref>). The remaining 79 infants were either 3 months (n=26, 14 girls, <italic>mean</italic>=113.04 days, <italic>SD</italic>=5.68 days, <italic>range</italic>=98-120 days), 6 months (n=26, 14 girls, <italic>mean</italic>=195.88 days, <italic>SD</italic>=9.46 days, <italic>range</italic>=182-211 days) or 12-13 months of age (n=27, 9 girls, <italic>mean</italic>=380.44 days, <italic>SD</italic>=14.93 days, <italic>range</italic>=361-413 days). All infants were born with a minimum gestational age of 37 weeks, weighed at least 2500 g at birth, and had no known developmental delays, neurological disorders, or hearing impairments. A primary caregiver gave written informed consent and was present during the experiment. The study was approved by the local Ethics committee of the University of Vienna (no. 00645) in line with the Declaration of Helsinki.</p>
</sec>
<sec id="s5b">
<title>Stimuli</title>
<p>Auditory stimuli were generated by rearranging the refrains of two polyphonic children’s songs (“La Vaca Lola” and “Hopp Juliska”) using Logic Pro X (Apple, Inc.). Stimuli belonged to four distinct conditions (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). The music condition entailed the presentation of the rearranged refrains. The shuffled music condition entailed a disruption of pitch order and timing regularities of the refrains presented in the music condition. Specifically, the original temporal order of the notes was shuffled (disrupting pitch order), and the IOIs were replaced by a new pool of random values uniformly distributed around the original mean IOI ± 30-50% of the distance between the mean and the minimum original IOI (disrupting timing regularities). The bass and melody notes were treated independently in the shuffling process. In addition, the new pool of IOI was not quantized, so the notes did not fall on the beat, thus disrupting the isochrony of the original playsongs. In the high-pitch condition, the melody was shifted one octave higher than in the music condition. In the low-pitch condition, the bassline was shifted one octave lower than in the music condition. Hence, the two voices composing the high-pitch condition were one octave higher than those composing the low-pitch condition. All stimuli were built using the same musical instruments: the melody was played by a flute (“VHS Flute” from Yamaha DX7, Arturia) and the bassline was played by an electric bass (“Liverpool” electric bass from Logic Pro), with almost all bassline notes (15 out of 16) falling on the beat. Importantly, all stimuli had the same duration (21 s), tempo (135 BPM), tonality (C-major) and loudness (which stayed within a range of 1.5 LUFS, a measure that considers the sensitivity of the human auditory system across frequencies).</p>
</sec>
<sec id="s5c">
<title>Procedure</title>
<sec id="s5c1">
<title>Setup and Task</title>
<p>Infants were seated in an infant highchair equipped with an age-appropriate baby seat facing a computer screen (1 m distance, see <xref rid="fig1" ref-type="fig">Fig. 1A</xref>). Parents sat next to the infants (on their right and slightly towards the back). The stimuli were presented free field (∼60 dB SPL) from two audio speakers (Q Acoustics 3020i) placed on either side of the monitor, facing the infants. During the procedure, infants passively listened to a series of auditory stimuli in the following sequence: First, there were 10 seconds of silence, followed by the shuffled music condition. Then, the music, high-pitch and low-pitch conditions were played in random order. After that, there were another 10 seconds of silence, followed by a second round of the music, high-pitch and low-pitch conditions in random order. Finally, the shuffled music condition was played one last time, followed by 10 seconds of silence. The same song was used in all the different conditions within each presentation block. The order of presentation blocks for each song was counterbalanced. Meanwhile, infants were watching a silent movie of blooming flowers, slowly fading in and out (image duration 10 s). Videos were included to keep infants calm and engaged. Only infants who remained calm for at least two presentation blocks were included in the final sample (n=3 were excluded). Stimulus presentation and the synchronization between EEG and video cameras were controlled in Presentation (Version 23.0, Neuro-behavioral System, Berkeley, CA) utilizing triggers sent at the start of the experiment and the beginning of each trial.</p>
<sec id="s5c1a">
<title>Electroencephalography (EEG)</title>
<p>We recorded EEG using a Brain Products BrainAmp DC system with 32 active Ag-AgCl electrodes, mounted on infant-sized EEG caps (Acticap) according to the international 10-10 system. The reference electrode was placed at TP9 (left mastoid), and the ground electrode was placed at Fp1. EEG data were sampled at 1000 Hz. Impedances were below 20 kΩ at the beginning of the experiment.</p>
</sec>
<sec id="s5c1b">
<title>Video Recording</title>
<p>Infant body movement was recorded using three video cameras (Axis Communication, Lund) sampling at 25 Hz with a resolution of 1920 x 1080 pixels. The three video recordings, taken from frontal (0°), diagonal (45°) and side view (90°) perspectives, were synchronized in VideoSyncPro (Mangold International). The cameras were positioned at the height of the infants’ heads to capture their whole bodies.</p>
</sec>
</sec>
</sec>
<sec id="s5d">
<title>Adult Control Experiment</title>
<p>Twenty-six healthy young adults (11 female, <italic>mean</italic> = 27.55 years of age, <italic>SD</italic>: 7.09 years, <italic>range</italic>=19-47 years) participated in the experiment. The study was approved by the Regional Ethics Committee of Liguria (no. 794/2021). Participants underwent the same experimental procedure as the infants, including the same auditory stimuli and video material. EEG was recorded with 64 Ag-AgCl electrodes, closely matched to the infant configuration (see Supplements for further details) and sampled at 1024 Hz. Data were down-sampled to 1000 Hz to match the pre-processing and data analysis procedure (detailed below) of the infant study. The experiment was conducted to provide a ground truth for the interpretation of the infants’ EEG data. Therefore, no video data was recorded to track adults’ movements.</p>
</sec>
<sec id="s5e">
<title>Data Processing</title>
<sec id="s5e1">
<title>EEG pre-processing</title>
<p>Infant EEG data are notoriously noisy as infants may not understand or comply with task instructions. It follows that their behaviour can lead to artefacts in the EEG recordings. To mitigate this issue, we used a combination of open-access denoising algorithms and adopted a fully data-driven pre-processing pipeline that we previously developed to denoise EEG data recorded from awake monkeys (<xref ref-type="bibr" rid="c3">Bianco et al., 2024</xref>) and humans dancing (Bigand, Bianco, Abalde, Nguyen, et al., 2024). The pipeline relied on a combination of MATLAB functions developed for toolboxes such as Fieldtrip (<xref ref-type="bibr" rid="c75">Oostenveld et al., 2010</xref>) and eeglab (<xref ref-type="bibr" rid="c16">Delorme &amp; Makeig, 2004</xref>). Continuous EEG data were band-pass filtered (0.3 Hz - 30 Hz, 3<sup>rd</sup> order Butterworth filter, zero-phase) and segmented into trials starting 3 seconds before song onset and ending 3 seconds post song offset. Next, noisy and faulty electrodes were identified and rejected by assessing flat-lining for over 5 s (function <italic>clean_flatlines</italic>), correlations between electrodes for <italic>r</italic>&lt;.1 or line noise above 20 SD from the mean of all electrodes (function <italic>clean_channels)</italic>. These initial steps were conducted to find a rough indication of electrodes carrying low-quality signals. In addition, mean, standard deviation, and peak-to-peak distances were calculated over time for each electrode. If any of those variables exceeded 2.5 SD from the mean of the other electrodes, that electrode was (provisionally) discarded. This process was iterated without the outlier electrode(s) until a distribution without outliers was found. Electrodes were then re-referenced to the averaged mastoids (TP9, TP10) or the left mastoid only in cases when TP10 was noisy (N=26). We then used artifact subspace reconstruction (ASR, threshold 5) to remove artefactual EEG activity (<xref ref-type="bibr" rid="c52">Kothe &amp; Jung, 2015</xref>). An automatic independent component analysis (ICA) procedure using ICLabel was implemented, and components that were classified as eye components with a minimum probability of 50 % were rejected (mean=0.76, SD=0.67, 0-2 components per infant). Next, previously excluded electrodes were interpolated by using the neighbouring electrodes (spherical spline; mean number of interpolated channels=6, SD=3.09).</p>
</sec>
<sec id="s5e2">
<title>Video data pre-processing</title>
<p>We extracted infants’ movement time series by using DeepLabCut (version 2.2.3; <xref ref-type="bibr" rid="c63">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="c67">Nath et al., 2019</xref>) in Python on videos filming the infant from the front (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>). A trained coder (T.N.) labelled 18 body parts per individual (left eye, right eye, mouth, left shoulder, right shoulder, chest, left elbow, left wrist, left hand, right elbow, right wrist, right hand, left knee, left ankle, left foot, right knee, right ankle, right foot) in 10 frames randomly taken from the videos of each infant in each age group (n=24 (3m), 25 (6m) and 24 (12m); 80% were used for training). We used a ResNet-50 neural network with default parameters for single subject detection for around 5-17 training iterations (correcting labels of 5 frames per video of age groups) until test error plateaued. The test error was 8.27 (3-month-olds), 10.19 (6-month-olds), and 11.89 (12-month-olds) pixels (image size was 1920 by 1080 pixels). The networks of each age group were used to analyse videos from the same age group. The (x and y) coordinates were then pre-processed using a custom code written in Python. Coordinates that were estimated with likelihood values 2 SD away from the mean probability per age group were set to NaN (i.e., not a number). These gaps in the coordinates of body parts were then recovered from predefined neighbouring body parts in a forward and backward approach (from head to feet, followed by feet to head). For example, if the left wrist was missing at t<sub>0</sub>, it was recovered using the position of the left elbow or the left hand at t<sub>0</sub>. Next, we assessed the correctness of the estimated body part displacements, i.e., demeaned coordinates. Specifically, we set the coordinates (either x or y) of each body part below or above 3 SD from the mean of all trials of each infant to NaN (outlier procedure). To further verify left- and right-hand displacements, we checked the distance between the wrist and hand, specifically using the same outlier procedure. The remaining NaN values were replaced using linear interpolation for each body part coordinate in time.</p>
<sec id="s5e2a">
<title>Principal movements</title>
<p>We used Principal Component Analysis (PCA) to reduce dimensionality of the pre-processed movement data and at the same time to extract a set of interpretable Principal Movements (PM) that generalize across trials, conditions, and all infants (<xref ref-type="bibr" rid="c5">Bigand, Bianco, Abalde, &amp; Novembre, 2024</xref>). To ensure that PCA captured only common movements across participants, rather than postural or anthropometric differences between them, we demeaned and standardized the body part vectors for each trial. Specifically, we subtracted the mean body part vector (averaged over time) from the body part vectors at each frame. Then, these demeaned body part vectors were divided by a global measure of standard deviation across time, which was computed by combining all body parts to preserve the variance differences between body parts. This allowed us to concatenate all trials from all participants into a single data matrix, with each trial contributing equally to the variance of the pooled matrix. PMs were derived from the eigenvectors, and their corresponding scores obtained from the PCA applied to the data matrix:
<disp-formula>
<graphic xlink:href="649695v1_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where c<sub>i</sub>(t) is the i<sup>th</sup> PM score at time t and w<bold><sub>i</sub></bold> is the i<sup>th</sup> PM weight vector, or eigenvector (1 × 36). Each of the PMs reflects the trajectory of covarying body parts. We extracted 10 PMs that explained 79.7 % of the whole kinematic variance (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). It is important to note that the PCA does not directly extract “real movements” but rather movement dimensions with large variances. PMs’ time series (score of each principal component) were low-pass filtered at 10 Hz. Next, we calculated the time series’ first-degree derivative to extract velocities and the absolute values of velocities to calculate movement quantities (e.g., <xref ref-type="bibr" rid="c4">Bigand et al., 2024</xref>; <xref ref-type="bibr" rid="c29">Górecki &amp; Łuczak, 2013</xref>). Velocities include information on movement direction (below or above 0) and are, therefore, particularly useful to analyse correlations with other streams, such as music (detailed in the section on Granger Causality). Movement quantities only consider the amount of movement and can contribute to understanding descriptive differences between conditions, regardless of the movement direction.</p>
</sec>
</sec>
</sec>
<sec id="s5f">
<title>Data analyses</title>
<sec id="s5f1">
<title>EEG: Event-related potentials (ERP) analysis</title>
<p>EEG data were first lowpass filtered at 20 Hz (FIR-type filter) to specifically retain the part of the EEG signal carrying event-related potentials. EEG data were epoched according to the onsets of the bass notes of the musical stimuli (which fell on the beat of the music in 15 out of 16 notes). Epochs included 600 ms of data (-100 ms to +500 ms relative to tone onset – note that notes occurred every 444 ms, so the last 56 ms of each epoch refers to the next tone). In preparation for further analysis, epochs were automatically examined to check if they were still contaminated by artifacts. Epochs were automatically excluded if the standard deviation of the analysed channels exceeded a voltage threshold of 100 μV within a 200 ms interval. All infants and adults contributed at least 165 clean epochs per condition (3m: <italic>M</italic>=340.09, <italic>SD</italic>=138.96; 6m: <italic>M</italic>=373.69, <italic>SD</italic>=144.07; 12m: <italic>M</italic>=361.47, <italic>SD</italic>=157.46; adults: <italic>M</italic>=399.95, <italic>SD</italic>=8.50). Epochs were baseline-corrected by subtracting the average voltage between -100 and 0 ms relative to the tone onset. Epochs belonging to the same experimental condition were averaged together. Event-related EEG amplitude modulations were compared using cluster-based permutation analyses (<italic>n<sub>Perm</sub></italic>=1000; Maris &amp; Ostenveld, 2007). Clusters were based on both temporal consecutiveness and spatial adjacency of EEG electrodes (3 cm distance). A cluster had to be composed of at least two consecutive time points with a <italic>p-value</italic> &lt;.05 on at least two neighbouring EEG electrodes. The clusters were tested using a <italic>p-value</italic> of &lt;.05 and one-tailed according to the above-detailed hypotheses.</p>
<p>In additional analyses, we extracted the amplitude and latencies of the individual peak of the P1 component. Using two linear mixed-effects models, we tested whether amplitudes (dependent variable) and latencies (dependent variable) were dependent on age and condition (fixed and interaction effect) while assuming a random intercept for each participant.</p>
</sec>
<sec id="s5f2">
<title>EEG: Auditory Steady State Response (ASSR) analysis</title>
<p>EEG data were segmented according to each condition (21 seconds long, 3 repetitions of the refrain) and averaged over segments of the same condition to enhance the ASSR (see <xref ref-type="bibr" rid="c12">Cirelli et al., 2016</xref>). Each segment was submitted to single-taper frequency transformation using a Hilbert taper to estimate frequencies between 0.5 and 5 Hz, with a frequency resolution of 0.25 Hz. Activity unrelated to the frequency of interest was removed by subtracting the average amplitude measured at neighbouring frequency bins (i.e., −5 to −3 bins and +3 to +5 bins). Performing this subtraction removes residual background noise around the frequency bin of interest, leaving only the activity directly related to the ASSR. Next, we extracted amplitudes at the beat-related (2.25 Hz) frequencies and compared these across conditions.</p>
</sec>
<sec id="s5f3">
<title>Movement: Granger Causality analysis</title>
<p>We conducted a Granger Causality analysis in R (function = <italic>causality,</italic> library = <italic>vars)</italic> to measure to what extent the envelope of the auditory stimuli predicted movement velocity over time and across conditions. To do so, we extracted the broadband amplitude envelope of the auditory stimuli using Hilbert transform. We then conducted Granger Causality analyses on amplitude envelopes predicting the PM velocity time series across different model orders (lags) ranging from 40 ms to 1000 ms. Granger Causality is based on the concept of prediction in the sense that if a time series X<sub>t</sub> &quot;Granger-causes&quot; another time series Y<sub>t</sub>, then past values of X<sub>t</sub> contain information that helps predict Y<sub>t</sub> beyond the information contained in past values of Y<sub>t</sub> alone. Granger Causality is particularly useful for capturing the temporal relationship between different time series that are characterized by sudden and transient bursts in activity (such as infant rhythmic movement; <xref ref-type="bibr" rid="c92">Thelen, 1979</xref>). As a sanity check, we also conducted a reverse analysis predicting music (i.e., the amplitude envelope of the auditory stimuli) from the PM velocity time series and verified that the resulting F-Values (averaged across all lags: 40 – 1000 ms) were statistically lower compared to those yielded by the former (music-to-movement) analysis. Indeed, the sanity check analysis reveals that predictions from audio stimuli to PM velocity time series (mean=1.08, SE=0.009) were significantly higher than from PMs to audio stimuli (mean=1.03, SE=0.009; χ²(1)=1669.2, <italic>p</italic>&lt;.001, see <xref rid="fig5" ref-type="fig">Fig. 5</xref>).</p>
</sec>
<sec id="s5f4">
<title>Movement: Event-related analysis</title>
<p>To examine whether the events (i.e., musical notes) comprised within the auditory stimuli would trigger event-related movements, we used an event-related approach (mirroring the ERP analysis discussed above). First, we segmented epochs of 700 ms (between -100 ms and +600 ms relative to peaks in the amplitude). Epochs were then averaged for each condition. Detailed results for this analysis are reported in the Supplements (see <italic>Fig. S2</italic>).</p>
</sec>
<sec id="s5f5">
<title>Movement: Periodicity analysis</title>
<p>We analysed the movement periodicity using autocorrelation on the same epochs employed in the event-related movement analysis. This means we estimated the autocorrelation of the movement time series in a running window of 3.5 s (length of 1 bar) with 25% overlap, mirroring the approach by Zenter &amp; Eerola (2010). This approach provides an estimation of the periodicity of the PMs. We extracted the local maxima of the autocorrelation values in each window and fitted a normal distribution (Kernel density estimation) to the collection of these values for each condition. We used a bin width of 0.08 and estimated values between 300 and 600 ms according to the beat-related lag in our auditory stimuli. We then compared the beat-related lag (444 ms lag) across conditions. The results are reported in the Supplements (see <italic>Fig. S3</italic>).</p>
</sec>
</sec>
<sec id="s5g">
<title>Statistical analyses</title>
<p>Analyses were conducted in R and included non-parametric tests or linear mixed-effects models (LMM; lme4 package). Each participant was modelled as a random intercept in all LMMs. Statistical significance was evaluated by likelihood-ratio tests conducted using <italic>Anova</italic> (car package), and post-hoc contrasts were run using <italic>emmeans</italic> (emmeans package). We controlled for increased Type I error from multiple comparisons, when necessary, using the false-discovery rate (Benjamini &amp; Hochberg, 1995). The statistical significance level was set to α = 0.05.</p>
</sec>
</sec>

</body>
<back>
<sec id="s8" sec-type="data-availability">
<title>Data availability</title>
<p>The data reported in this manuscript will be made available upon publication in the following repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48557/DCSCFO">https://doi.org/10.48557/DCSCFO</ext-link>. The codes used for analyses and figures are available on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/tnguyen1992/tinydancer">https://github.com/tnguyen1992/tinydancer</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We are grateful to Eluisa Nimpf, Flavia Arnese, Larissa Reitinger, Josefine Schürholz and Liesbeth Forsthuber for their support in data acquisition, processing, and video coding. Additionally, we thank all families who participated in the study and the Department of Obstetrics and Gynaecology of the Vienna General Hospital for supporting our participant recruitment. Trinh Nguyen and Roberta Bianco also acknowledge the support of Horizon Europe’s Marie Skłodowska-Curie Actions (SYNCON, 101105726; PHYLOMUSIC, 101064334).</p>
</ack>
<sec id="d1e1711" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Funding</title>
<p>This research has received funding from the European Research Council awarded to Giacomo Novembre (MUSICOM, 948186) and the Austrian Science Fund (FWF) DK Grant “Cognition &amp; Communication 2&quot;: W1262-B29 [10.55776/W1262].</p>
</sec>
<sec id="s7">
<title>Credits</title>
<p><bold>Trinh Nguyen</bold>: Conceptualization, Methodology, Software, Formal Analysis, Investigation, Data curation, Visualization, Writing - Original draft, Writing – Review &amp; Editing, Project administration; <bold>Félix Bigand</bold>: Software, Validation, Data curation, Formal Analysis, Writing – Review &amp; Editing. <bold>Susanne Reisner</bold>: Investigation, Data curation, Writing – Review &amp; Editing, Project administration. <bold>Atesh Koul</bold>: Software, Writing – Review &amp; Editing; <bold>Roberta Bianco</bold>: Validation, Software, Writing - Original draft, Writing – Review &amp; Editing; <bold>Gabriela Markova</bold>: Conceptualization, Data curation, Writing - Review &amp; Editing; <bold>Stefanie Hoehl</bold>: Conceptualization, Methodology, Resources, Writing - Review &amp; Editing, Supervision, Funding acquisition; <bold>Giacomo Novembre</bold>: Conceptualization, Methodology, Resources, Writing - Original draft, Writing - Review &amp; Editing, Supervision, Funding acquisition.</p>
</sec>
</sec>
<sec id="suppd1e1711" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1702">
<label>Supplementary Materials</label>
<media xlink:href="supplements/649695_file03.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beebe</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Messinger</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bahrick</surname>, <given-names>L. E.</given-names></string-name>, <string-name><surname>Margolis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Buck</surname>, <given-names>K. A.</given-names></string-name>, &amp; <string-name><surname>Chen</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2016</year>). <article-title>A systems view of mother-infant face-to-face communication</article-title>. <source>Developmental Psychology</source>, <volume>52</volume>(<issue>4</issue>), <fpage>556</fpage>–<lpage>571</lpage>. <pub-id pub-id-type="doi">10.1037/a0040085</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Bianco</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Tóth</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Bigand</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Sziller</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Háden</surname>, <given-names>G. P.</given-names></string-name>, <string-name><surname>Winkler</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Novembre</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2025</year>). <article-title>Human newborns form musical predictions based on rhythmic but not melodic structure</article-title> (p. <fpage>2025.02.19.639016</fpage>). <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2025.02.19.639016</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bianco</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Zuk</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Bigand</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Quarta</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Grasso</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Arnese</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ravignani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Battaglia-Mayer</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Novembre</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Neural encoding of musical expectations in a non-human primate</article-title>. <source>Current Biology</source>, <volume>34</volume>(<issue>2</issue>), <fpage>444</fpage>–<lpage>450.e5</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2023.12.019</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Bigand</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bianco</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Abalde</surname>, <given-names>S. F.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Novembre</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Imaging the dancing brain: Decoding sensory, motor and social processes during dyadic dance</article-title> (p. <fpage>2024.12.17.628913</fpage>). <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2024.12.17.628913</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bigand</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bianco</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Abalde</surname>, <given-names>S. F.</given-names></string-name>, &amp; <string-name><surname>Novembre</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2024</year>). <article-title>The geometry of interpersonal synchrony in human dance</article-title>. <source>Current Biology</source>, <volume>34</volume>(<issue>13</issue>), <fpage>3011</fpage>–<lpage>3019.e4</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2024.05.055</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brown</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Group dancing as the evolutionary origin of rhythmic entrainment in humans</article-title>. <source>New Ideas in Psychology</source>, <volume>64</volume>, <fpage>100902</fpage>. <pub-id pub-id-type="doi">10.1016/j.newideapsych.2021.100902</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cameron</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Dotov</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Flaten</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Bosnyak</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hove</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Trainor</surname>, <given-names>L. J</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Undetectable, very-low frequency sound increases dancing at a live concert</article-title>. <source>Current Biology</source>, <volume>32</volume>(<issue>21</issue>), <fpage>R1222</fpage>–<lpage>R1223</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2022.09.035</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Capilla</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pazo-Alvarez</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Darriba</surname>, <given-names>Á.</given-names></string-name>, <string-name><surname>Campo</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Gross</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Steady-State Visual Evoked Potentials Can Be Explained by Temporal Superposition of Transient Event-Related Responses</article-title>. <source>PloS One</source>, <volume>6</volume>, <fpage>e14543</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0014543</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Peter</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Burnham</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Auditory ERP response to successive stimuli in infancy</article-title>. <source>PeerJ</source>, <volume>4</volume>, <fpage>e1580</fpage>. <pub-id pub-id-type="doi">10.7717/peerj.1580</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Penhune</surname>, <given-names>V. B.</given-names></string-name>, &amp; <string-name><surname>Zatorre</surname>, <given-names>R. J</given-names></string-name></person-group>. (<year>2009</year>). <article-title>The Role of Auditory and Premotor Cortex in Sensorimotor Transformations</article-title>. <source>Annals of the New York Academy of Sciences</source>, <volume>1169</volume>(<issue>1</issue>), <fpage>15</fpage>–<lpage>34</lpage>. <pub-id pub-id-type="doi">10.1111/j.1749-6632.2009.04556.x</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cirelli</surname>, <given-names>L. K.</given-names></string-name>, <string-name><surname>Jurewicz</surname>, <given-names>Z. B.</given-names></string-name>, &amp; <string-name><surname>Trehub</surname>, <given-names>S. E</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Effects of Maternal Singing Style on Mother– Infant Arousal and Behavior</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>32</volume>(<issue>7</issue>), <fpage>1213</fpage>–<lpage>1220</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01402</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cirelli</surname>, <given-names>L. K.</given-names></string-name>, <string-name><surname>Spinelli</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Nozaradan</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Trainor</surname>, <given-names>L. J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Measuring Neural Entrainment to Beat and Meter in Infants: Effects of Music Background</article-title>. <source>Frontiers in Neuroscience</source>, <volume>10</volume>. <pub-id pub-id-type="doi">10.3389/fnins.2016.00229</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conrad</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Walsh</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Allen</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Tsang</surname>, <given-names>C. D</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Examining infants’ preferences for tempo in lullabies and playsongs</article-title>. <source>Canadian Journal of Experimental Psychology / Revue Canadienne de Psychologie Expérimentale</source>, <volume>65</volume>(<issue>3</issue>), <fpage>168</fpage>–<lpage>172</lpage>. <pub-id pub-id-type="doi">10.1037/a0023296</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Costa-Giomi</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Sun</surname>, <given-names>X</given-names></string-name></person-group>. (<year>2016</year>). <chapter-title>Infants’ Home Soundscape: A Day in the Life of a Family</chapter-title>. In <source>Contemporary Research in Music Learning Across the Lifespan</source> (pp. <fpage>87</fpage>–<lpage>96</lpage>). <publisher-name>Routledge</publisher-name>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Damsma</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Roo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Doelling</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bazin</surname>, <given-names>P.-L.</given-names></string-name>, &amp; <string-name><surname>Bouwer</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Tempo-dependent selective enhancement of neural responses at the beat frequency can be explained by both an oscillator and an evoked model</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2024.07.11.603023</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delorme</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Makeig</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2004</year>). <article-title>EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>134</volume>(<issue>1</issue>), <fpage>9</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Liberto</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Pelofi</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bianco</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Herrero</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>de Cheveigné</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Shamma</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title>. <source>eLife</source>, <volume>9</volume>, <elocation-id>e51784</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.51784</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Eckerdal</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Merker</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2009</year>). <chapter-title>‘Music’ and the ‘action song’ in infant development: An interpretation</chapter-title>. In <source>Communicative musicality: Exploring the basis of human companionship</source> (pp. <fpage>241</fpage>–<lpage>262</lpage>). <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edalati</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wallois</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Safaie</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ghostine</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kongolo</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Trainor</surname>, <given-names>L. J.</given-names></string-name>, &amp; <string-name><surname>Moghimi</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Rhythm in the premature neonate brain: Very early processing of auditory beat and meter. <italic>The Journal of Neuroscience</italic></article-title>, <source>Jn-rm</source>-<fpage>1100</fpage>–<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1100-22.2023</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feldman</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Parent–Infant Synchrony: Biological Foundations and Developmental Outcomes</article-title>. <source>Current Directions in Psychological Science</source>, <volume>16</volume>(<issue>6</issue>), <fpage>340</fpage>–<lpage>345</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-8721.2007.00532.x</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernald</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Kuhl</surname>, <given-names>P</given-names></string-name></person-group>. (<year>1987</year>). <article-title>Acoustic determinants of infant preference for motherese speech</article-title>. <source>Infant Behavior and Development</source>, <volume>10</volume>(<issue>3</issue>), <fpage>279</fpage>–<lpage>293</lpage>. <pub-id pub-id-type="doi">10.1016/0163-6383(87)90017-8</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernald</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Simon</surname>, <given-names>T</given-names></string-name></person-group>. (<year>1984</year>). <article-title>Expanded intonation contours in mothers’ speech to newborns</article-title>. <source>Developmental Psychology</source>, <volume>20</volume>(<issue>1</issue>), <fpage>104</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1037/0012-1649.20.1.104</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fitch</surname>, <given-names>W. T</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Four principles of bio-musicology</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>370</volume>(<issue>1664</issue>), <fpage>20140091</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2014.0091</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Flaten</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Marshall</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Dittrich</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Trainor</surname>, <given-names>L. J</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Evidence for top-down metre perception in infancy as shown by primed neural responses to an ambiguous rhythm</article-title>. <source>European Journal of Neuroscience</source>, <fpage>ejn.15671</fpage>. <pub-id pub-id-type="doi">10.1111/ejn.15671</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friederici</surname>, <given-names>A. D</given-names></string-name></person-group>. (<year>2011</year>). <article-title>The Brain Basis of Language Processing: From Structure to Function</article-title>. <source>Physiological Reviews</source>, <volume>91</volume>(<issue>4</issue>), <fpage>1357</fpage>–<lpage>1392</lpage>. <pub-id pub-id-type="doi">10.1152/physrev.00006.2011</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2010</year>). <article-title>The free-energy principle: A unified brain theory?</article-title> <source>Nature Reviews Neuroscience</source>, <volume>11</volume>(<issue>2</issue>), <elocation-id>Article 2</elocation-id>. <pub-id pub-id-type="doi">10.1038/nrn2787</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fujii</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Watanabe</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Oohashi</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Hirashima</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nozaki</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Taga</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Precursors of Dancing and Singing to Music in Three-to Four-Months-Old Infants</article-title>. <source>PLOS One</source>, <volume>9</volume>(<issue>5</issue>), <fpage>e97680</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0097680</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fujioka</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Trainor</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Large</surname>, <given-names>E. W.</given-names></string-name>, &amp; <string-name><surname>Ross</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Internalized Timing of Isochronous Sounds Is Represented in Neuromagnetic Beta Oscillations</article-title>. <source>The Journal of Neuroscience</source>, <volume>32</volume>(<issue>5</issue>), <fpage>1791</fpage>–<lpage>1802</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4107-11.2012</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Górecki</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Łuczak</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Using derivatives in time series classification</article-title>. <source>Data Mining and Knowledge Discovery</source>, <volume>26</volume>, <fpage>310</fpage>–<lpage>331</lpage>. <pub-id pub-id-type="doi">10.1007/s10618-012-0251-4</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grahn</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Brett</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Rhythm and Beat Perception in Motor Areas of the Brain</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>19</volume>(<issue>5</issue>), <fpage>893</fpage>–<lpage>906</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2007.19.5.893</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gratier</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Devouche</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Guellai</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Infanti</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Yilmaz</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Parlato-Oliveira</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Early development of turn-taking in vocal interaction between mothers and infants</article-title>. <source>Frontiers in Psychology</source>, <volume>6</volume>, <fpage>1167</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2015.01167</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hadders-Algra</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Development of Postural Control During the First 18 Months of Life</article-title>. <source>Neural Plasticity</source>, <volume>12</volume>(<issue>2–3</issue>), <fpage>695071</fpage>. <pub-id pub-id-type="doi">10.1155/NP.2005.99</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Háden</surname>, <given-names>G. P.</given-names></string-name>, <string-name><surname>Bouwer</surname>, <given-names>F. L.</given-names></string-name>, <string-name><surname>Honing</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Winkler</surname>, <given-names>I</given-names></string-name></person-group>. (<year>2022</year>). <article-title><italic>Beat processing in newborn infants cannot be explained by statistical learning based on transition probabilities</italic> [Preprint]</article-title>. <source>Neuroscience</source>. <pub-id pub-id-type="doi">10.1101/2022.12.20.521245</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Háden</surname>, <given-names>G. P.</given-names></string-name>, <string-name><surname>Németh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Török</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Winkler</surname>, <given-names>I</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Predictive processing of pitch trends in newborn infants</article-title>. <source>Brain Research</source>, <volume>1626</volume>, <fpage>14</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainres.2015.02.048</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Háden</surname>, <given-names>G. P.</given-names></string-name>, <string-name><surname>Stefanics</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Vestergaard</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Denham</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Sziller</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Winkler</surname>, <given-names>I</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Timbre-independent extraction of pitch in newborn infants</article-title>. <source>Psychophysiology</source>, <volume>46</volume>(<issue>1</issue>), <fpage>69</fpage>–<lpage>74</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.2008.00749.x</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hattori</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2021</year>). <chapter-title>Behavioral Coordination and Synchronization in Non-human Primates</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>J. R.</given-names> <surname>Anderson</surname></string-name> &amp; <string-name><given-names>H.</given-names> <surname>Kuroshima</surname></string-name></person-group> (Eds.), <source>Comparative Cognition: Commonalities and Diversity</source> (pp. <fpage>139</fpage>–<lpage>151</lpage>). <publisher-name>Springer</publisher-name>. <pub-id pub-id-type="doi">10.1007/978-981-16-2028-7_9</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hattori</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Tomonaga</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Rhythmic swaying induced by sound in chimpanzees (Pan troglodytes)</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>2</issue>), <fpage>936</fpage>–<lpage>942</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1910318116</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hilton</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Bertolo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lee-Rubin</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Amir</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bainbridge</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Simson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Knox</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Glowacki</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Alemu</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Galbarczyk</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jasienska</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ross</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Neff</surname>, <given-names>M. B.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cirelli</surname>, <given-names>L. K.</given-names></string-name>, <string-name><surname>Trehub</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>M.</given-names></string-name>, <etal>…</etal> <string-name><surname>Mehr</surname>, <given-names>S. A</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Acoustic regularities in infant-directed speech and song across cultures</article-title>. <source>Nature Human Behaviour</source>. <pub-id pub-id-type="doi">10.1038/s41562-022-01410-x</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoehl</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fairhurst</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Schirmer</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Interactional Synchrony: Signals, Mechanisms, and Benefits</article-title>. <source>Social Cognitive and Affective Neuroscience</source>, <fpage>nsaa024</fpage>. <pub-id pub-id-type="doi">10.1093/scan/nsaa024</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoehl</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Wahl</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Recording infant ERP data for cognitive research</article-title>. <source>Developmental Neuropsychology</source>, <volume>37</volume>(<issue>3</issue>), <fpage>187</fpage>–<lpage>209</lpage>. <pub-id pub-id-type="doi">10.1080/87565641.2011.627958</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Honing</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2018</year>). <source>The Origins of Musicality</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Honing</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Bouwer</surname>, <given-names>F. L.</given-names></string-name>, <string-name><surname>Prado</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Merchant</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Rhesus Monkeys (Macaca mulatta) Sense Isochrony in Rhythm, but Not the Beat: Additional Support for the Gradual Audiomotor Evolution Hypothesis</article-title>. <source>Frontiers in Neuroscience</source>, <volume>12</volume>. <pub-id pub-id-type="doi">10.3389/fnins.2018.00475</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Honing</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Merchant</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Háden</surname>, <given-names>G. P.</given-names></string-name>, <string-name><surname>Prado</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Bartolo</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Rhesus Monkeys (Macaca mulatta) Detect Rhythmic Groups in Music, but Not the Beat</article-title>. <source>PLoS ONE</source>, <volume>7</volume>(<issue>12</issue>), <fpage>e51369</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0051369</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Honing</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>ten Cate</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Peretz</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Trehub</surname>, <given-names>S. E</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Without it no music: Cognition, biology and evolution of musicality</article-title>. <source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source>, <volume>370</volume>(<issue>1664</issue>), <fpage>20140088</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2014.0088</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hove</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Marie</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bruce</surname>, <given-names>I. C.</given-names></string-name>, &amp; <string-name><surname>Trainor</surname>, <given-names>L. J</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Superior time perception for lower musical pitch explains why bass-ranged instruments lay down musical rhythms</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>111</volume>(<issue>28</issue>), <fpage>10383</fpage>–<lpage>10388</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1402039111</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hurley</surname>, <given-names>B. K.</given-names></string-name>, <string-name><surname>Martens</surname>, <given-names>P. A.</given-names></string-name>, &amp; <string-name><surname>Janata</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Spontaneous sensorimotor coupling with multipart music</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>40</volume>, <fpage>1679</fpage>– <lpage>1696</lpage>. <pub-id pub-id-type="doi">10.1037/a0037154</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Janata</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Tomic</surname>, <given-names>S. T.</given-names></string-name>, &amp; <string-name><surname>Haberman</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Sensorimotor coupling in music and the psychology of the groove</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>141</volume>(<issue>1</issue>), <fpage>54</fpage>–<lpage>75</lpage>. <pub-id pub-id-type="doi">10.1037/a0024208</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Juslin</surname>, <given-names>P. N.</given-names></string-name>, &amp; <string-name><surname>Laukka</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Communication of emotions in vocal expression and music performance: Different channels, same code?</article-title> <source>Psychological Bulletin</source>, <volume>129</volume>(<issue>5</issue>), <fpage>770</fpage>–<lpage>814</lpage>. <pub-id pub-id-type="doi">10.1037/0033-2909.129.5.770</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Schachner</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2022</year>). <article-title>The origins of dance: Characterizing the development of infants’ earliest dance behavior</article-title>. <source>Developmental Psychology</source>. <pub-id pub-id-type="doi">10.1037/dev0001436</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kisilevsky</surname>, <given-names>B. s.</given-names></string-name>, <string-name><surname>Hains</surname>, <given-names>S. m. j.</given-names></string-name>, <string-name><surname>Jacquet</surname>, <given-names>A.-Y.</given-names></string-name>, <string-name><surname>Granier-Deferre</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Lecanuet</surname>, <given-names>J. p.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Maturation of fetal responses to music</article-title>. <source>Developmental Science</source>, <volume>7</volume>(<issue>5</issue>), <fpage>550</fpage>–<lpage>559</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-7687.2004.00379.x</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Köster</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kayhan</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Langeloh</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Hoehl</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Making Sense of the World: Infant Learning From a Predictive Processing Perspective</article-title>. <source>Perspectives on Psychological Science</source>, <volume>15</volume>(<issue>3</issue>), <fpage>562</fpage>–<lpage>571</lpage>. <pub-id pub-id-type="doi">10.1177/1745691619895071</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="patent"><person-group person-group-type="author"><string-name><surname>Kothe</surname>, <given-names>C. A. E.</given-names></string-name>, &amp; <string-name><surname>Jung</surname>, <given-names>T.-P.</given-names></string-name></person-group> (<year>2015</year>). <source>Artifact removal techniques with signal reconstruction</source> (<publisher-name>World Intellectual Property Organization Patent 2015/047462 A9</publisher-name>). <ext-link ext-link-type="uri" xlink:href="https://patents.google.com/patent/WO2015047462A9/en">https://patents.google.com/patent/WO2015047462A9/en</ext-link></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kotz</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Ravignani</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Fitch</surname>, <given-names>W. T</given-names></string-name></person-group>. (<year>2018</year>). <article-title>The Evolution of Rhythm Processing</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>22</volume>(<issue>10</issue>), <fpage>896</fpage>–<lpage>910</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2018.08.002</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kouider</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Long</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Le Stanc</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Charron</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fievet</surname>, <given-names>A.-C.</given-names></string-name>, <string-name><surname>Barbosa</surname>, <given-names>L. S.</given-names></string-name>, &amp; <string-name><surname>Gelskov</surname>, <given-names>S. V.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Neural dynamics of prediction and surprise in infants</article-title>. <source>Nature Communications</source>, <volume>6</volume>(<issue>1</issue>), <fpage>8537</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms9537</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuhl</surname>, <given-names>P. K</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Brain Mechanisms in Early Language Acquisition</article-title>. <source>Neuron</source>, <volume>67</volume>(<issue>5</issue>), <fpage>713</fpage>–<lpage>727</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2010.08.038</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuhl</surname>, <given-names>P. K.</given-names></string-name>, &amp; <string-name><surname>Meltzoff</surname>, <given-names>A. N</given-names></string-name></person-group>. (<year>1982</year>). <article-title>The bimodal perception of speech in infancy</article-title>. <source>Science (New York, N.y.)</source>, <volume>218</volume>(<issue>4577</issue>), <fpage>1138</fpage>–<lpage>1141</lpage>. <pub-id pub-id-type="doi">10.1126/science.7146899</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kushnerenko</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ceponiene</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Balan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Fellman</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Huotilaine</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Näätäne</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Maturation of the auditory event-related potentials during the first year of life</article-title>. <source>Neuroreport</source>, <volume>13</volume>(<issue>1</issue>), <fpage>47</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-200201210-00014</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lense</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Shultz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Astésano</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Jones</surname>, <given-names>W</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Music of infant-directed singing entrains infants’ social visual behavior</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>119</volume>(<issue>45</issue>), <fpage>e2116967119</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2116967119</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lijffijt</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Moeller</surname>, <given-names>F. G.</given-names></string-name>, <string-name><surname>Boutros</surname>, <given-names>N. N.</given-names></string-name>, <string-name><surname>Burroughs</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lane</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Steinberg</surname>, <given-names>J. L.</given-names></string-name>, &amp; <string-name><surname>Swann</surname>, <given-names>A. C</given-names></string-name></person-group>. (<year>2009</year>). <article-title>The Role of Age, Gender, Education, and Intelligence in P50, N100, and P200 Auditory Sensory Gating</article-title>. <source>Journal of Psychophysiology</source>, <volume>23</volume>(<issue>2</issue>), <fpage>52</fpage>–<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1027/0269-8803.23.2.52</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marie</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Trainor</surname>, <given-names>L. J</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Development of Simultaneous Pitch Encoding: Infants Show a High Voice Superiority Effect</article-title>. <source>Cerebral Cortex</source>, <volume>23</volume>(<issue>3</issue>), <fpage>660</fpage>–<lpage>669</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhs050</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marie</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Trainor</surname>, <given-names>L. J</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Early development of polyphonic sound encoding and the high voice superiority effect</article-title>. <source>Neuropsychologia</source>, <volume>57</volume>, <fpage>50</fpage>–<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.02.023</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markova</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Hoehl</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Neurobehavioral Interpersonal Synchrony in Early Development: The Role of Interactional Rhythms</article-title>. <source>Frontiers in Psychology</source>, <volume>10</volume>, <fpage>2078</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2019.02078</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mamidanna</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cury</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Abe</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>V. N.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Bethge</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2018</year>). <article-title>DeepLabCut: Markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature Neuroscience</source>, <volume>21</volume>(<issue>9</issue>), <elocation-id>Article 9</elocation-id>. <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Merchant</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Honing</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Are non-human primates capable of rhythmic entrainment? Evidence for the gradual audiomotor evolution hypothesis</article-title>. <source>Frontiers in Neuroscience</source>, <volume>7</volume>, <fpage>274</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2013.00274</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Musacchia</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ortiz-Mantilla</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Choudhury</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Realpe-Bonilla</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Roesler</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Benasich</surname>, <given-names>A. A</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Active auditory experience in infancy promotes brain plasticity in Theta and Gamma oscillations</article-title>. <source>Developmental Cognitive Neuroscience</source>, <volume>26</volume>, <fpage>9</fpage>–<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1016/j.dcn.2017.04.004</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nakata</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Trehub</surname>, <given-names>S. E</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Expressive timing and dynamics in infant-directed and non-infant-directed singing</article-title>. <source>Psychomusicology: Music, Mind and Brain</source>, <volume>21</volume>(<issue>1–2</issue>), <fpage>45</fpage>–<lpage>53</lpage>. <pub-id pub-id-type="doi">10.1037/h0094003</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nath</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Mathis</surname>, <given-names>M. W</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Using DeepLabCut for 3D markerless pose estimation across species and behaviors</article-title>. <source>Nature Protocols</source>, <volume>14</volume>(<issue>7</issue>), <fpage>2152</fpage>–<lpage>2176</lpage>. <pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nave</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Hannon</surname>, <given-names>E. E.</given-names></string-name>, &amp; <string-name><surname>Snyder</surname>, <given-names>J. S</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Steady state-evoked potentials of subjective beat perception in musical rhythms</article-title>. <source>Psychophysiology</source>, <volume>59</volume>(<issue>2</issue>), <fpage>e13963</fpage>. <pub-id pub-id-type="doi">10.1111/psyp.13963</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Nguyen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Flaten</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Trainor</surname>, <given-names>L. J.</given-names></string-name>, &amp; <string-name><surname>Novembre</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2023</year>). <article-title><italic>Early social communication through music: State of the art and future perspectives</italic> [Preprint]</article-title>. <source>PsyArXiv</source>. <pub-id pub-id-type="doi">10.31234/osf.io/j5g69</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Nguyen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Reisner</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lueger</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wass</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Hoehl</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Markova</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Sing to me, baby: Infants show neural tracking and rhythmic movements to live and dynamic maternal singing</article-title> (p. <fpage>2023.02.28.530310</fpage>). <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2023.02.28.530310</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Zimmer</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Hoehl</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Your turn, my turn. Neural synchrony in mother–infant proto-conversation</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>378</volume>(<issue>1875</issue>), <fpage>20210488</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2021.0488</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Novembre</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Iannetti</surname>, <given-names>G. D</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Tagging the musical beat: Neural entrainment or event-related potentials?</article-title> <source>Proceedings of the National Academy of Sciences</source>, <volume>115</volume>(<issue>47</issue>), <fpage>E11002</fpage>– <lpage>E11003</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1815311115</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Novembre</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Keller</surname>, <given-names>P. E.</given-names></string-name></person-group> (<year>2018</year>). <chapter-title>Music and Action</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>R.</given-names> <surname>Bader</surname></string-name></person-group> (Ed.), <source>Springer Handbook of Systematic Musicology</source> (pp. <fpage>523</fpage>–<lpage>537</lpage>). <publisher-name>Springer</publisher-name>. <pub-id pub-id-type="doi">10.1007/978-3-662-55004-5_28</pub-id></mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Novembre</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Pawar</surname>, <given-names>V. M.</given-names></string-name>, <string-name><surname>Bufacchi</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Kilintari</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Srinivasan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rothwell</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Haggard</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Iannetti</surname>, <given-names>G. D</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Saliency Detection as a Reactive Process: Unexpected Sensory Events Evoke Corticomuscular Coupling</article-title>. <source>The Journal of Neuroscience</source>, <volume>38</volume>(<issue>9</issue>), <fpage>2385</fpage>–<lpage>2397</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2474-17.2017</pub-id></mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Schoffelen</surname>, <given-names>J.-M</given-names></string-name></person-group>. (<year>2010</year>). <article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source>Computational Intelligence and Neuroscience</source>, <volume>2011</volume>, <fpage>e156869</fpage>. <pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patel</surname>, <given-names>A. D.</given-names></string-name>, &amp; <string-name><surname>Iversen</surname>, <given-names>J. R</given-names></string-name></person-group>. (<year>2014</year>). <article-title>The evolutionary neuroscience of musical beat perception: The Action Simulation for Auditory Prediction (ASAP) hypothesis</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>8</volume>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fnsys.2014.00057">https://www.frontiersin.org/articles/10.3389/fnsys.2014.00057</ext-link></mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pauen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Birgit</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hoehl</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Bechtel</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Show Me the World: Object Categorization and Socially Guided Object Learning in Infancy</article-title>. <source>Child Development Perspectives</source>, <volume>9</volume>(<issue>2</issue>), <fpage>111</fpage>–<lpage>116</lpage>. <pub-id pub-id-type="doi">10.1111/cdep.12119</pub-id></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perani</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Saccuman</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Scifo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Spada</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Andreolli</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Rovelli</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Baldoli</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Koelsch</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Functional specializations for music processing in the human newborn brain</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>107</volume>(<issue>10</issue>), <fpage>4758</fpage>–<lpage>4763</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0909074107</pub-id></mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Phillips-Silver</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hartmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Fernández-García</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Maurno</surname>, <given-names>N. C. G.</given-names></string-name>, <string-name><surname>Toiviainen</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>González</surname>, <given-names>M. T. D</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Development of full-body rhythmic synchronization in middle childhood</article-title>. <source>Scientific Reports</source>, <volume>14</volume>(<issue>1</issue>), <fpage>15741</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-024-66438-7</pub-id></mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Phillips-Silver</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Keller</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Searching for Roots of Entrainment and Joint Action in Early Musical Interactions</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>6</volume>. <pub-id pub-id-type="doi">10.3389/fnhum.2012.00026</pub-id></mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pratt</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Starr</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Michalewski</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>Bleich</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Mittelman</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2008</year>). <article-title>The auditory P50 component to onset and offset of sound</article-title>. <source>Clinical Neurophysiology</source>, <volume>119</volume>(<issue>2</issue>), <fpage>376</fpage>–<lpage>387</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinph.2007.10.016</pub-id></mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Provasi</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>D. I.</given-names></string-name>, &amp; <string-name><surname>Barbu-Roth</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Rhythm perception, production, and synchronization during the perinatal period</article-title>. <source>Frontiers in Psychology</source>, <volume>5</volume>, <fpage>1048</fpage>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Remijn</surname>, <given-names>G. B.</given-names></string-name>, <string-name><surname>Hasuo</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Fujihira</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Morimoto</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2014</year>). <article-title>An introduction to the measurement of auditory event-related potentials (ERPs)</article-title>. <source>Acoustical Science and Technology</source>, <volume>35</volume>(<issue>5</issue>), <fpage>229</fpage>–<lpage>242</lpage>. <pub-id pub-id-type="doi">10.1250/ast.35.229</pub-id></mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riva</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Cantiani</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Mornati</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Gallo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Villa</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Mani</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Saviozzi</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Marino</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Molteni</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Distinct ERP profiles for auditory processing in infants at-risk for autism and language impairment</article-title>. <source>Scientific Reports</source>, <volume>8</volume>(<issue>1</issue>), <fpage>715</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-017-19009-y</pub-id></mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saadatmehr</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Edalati</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wallois</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ghostine</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kongolo</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Flaten</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Tillmann</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Trainor</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Moghimi</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2025</year>). <article-title>Auditory Rhythm Encoding during the Last Trimester of Human Gestation: From Tracking the Basic Beat to Tracking Hierarchical Nested Temporal Structures</article-title>. <source>Journal of Neuroscience</source>, <volume>45</volume>(<issue>4</issue>):<elocation-id>e0398242024</elocation-id>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0398-24.2024</pub-id></mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>E. K.</given-names></string-name>, <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, &amp; <string-name><surname>Newport</surname>, <given-names>E. L</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Statistical learning of tone sequences by human infants and adults</article-title>. <source>Cognition</source>, <volume>70</volume>(<issue>1</issue>), <fpage>27</fpage>–<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1016/S0010-0277(98)00075-4</pub-id></mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schachner</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>T. F.</given-names></string-name>, <string-name><surname>Pepperberg</surname>, <given-names>I. M.</given-names></string-name>, &amp; <string-name><surname>Hauser</surname>, <given-names>M. D</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Spontaneous Motor Entrainment to Music in Multiple Vocal Mimicking Species</article-title>. <source>Current Biology</source>, <volume>19</volume>(<issue>10</issue>), <fpage>831</fpage>– <lpage>836</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2009.03.061</pub-id></mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Somervail</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Novembre</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Bufacchi</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Guo</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Crepaldi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Iannetti</surname>, <given-names>G. D</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Waves of Change: Brain Sensitivity to Differential, not Absolute, Stimulus Intensity is Conserved Across Humans and Rats</article-title>. <source>Cerebral Cortex (New York, N.Y.: 1991)</source>, <volume>31</volume>(<issue>2</issue>), <fpage>949</fpage>–<lpage>960</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhaa267</pub-id></mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stefanics</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Háden</surname>, <given-names>G. P.</given-names></string-name>, <string-name><surname>Sziller</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Balázs</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Beke</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Winkler</surname>, <given-names>I</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Newborn infants process pitch intervals</article-title>. <source>Clinical Neurophysiology</source>, <volume>120</volume>(<issue>2</issue>), <fpage>304</fpage>–<lpage>308</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinph.2008.11.020</pub-id></mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stupacher</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hove</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Janata</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Audio Features Underlying Perceived Groove and Sensorimotor Synchronization in Music</article-title>. <source>Music Perception</source>, <volume>33</volume>(<issue>5</issue>), <fpage>571</fpage>–<lpage>589</lpage>. <pub-id pub-id-type="doi">10.1525/mp.2016.33.5.571</pub-id></mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stupacher</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hove</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Novembre</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Schütz-Bosbach</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Keller</surname>, <given-names>P. E</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Musical groove modulates motor cortex excitability: A TMS investigation</article-title>. <source>Brain and Cognition</source>, <volume>82</volume>(<issue>2</issue>), <fpage>127</fpage>–<lpage>136</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandc.2013.03.003</pub-id></mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thelen</surname>, <given-names>E</given-names></string-name></person-group>. (<year>1979</year>). <article-title>Rhythmical stereotypies in normal human infants</article-title>. <source>Animal Behaviour</source>, <volume>27</volume>, <fpage>699</fpage>–<lpage>715</lpage>. <pub-id pub-id-type="doi">10.1016/0003-3472(79)90006-X</pub-id></mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thelen</surname>, <given-names>E</given-names></string-name></person-group>. (<year>1981</year>). <article-title>Kicking, rocking, and waving: Contextual analysis of rhythmical stereotypies in normal human infants</article-title>. <source>Animal Behaviour</source>, <volume>29</volume>(<issue>1</issue>), <fpage>3</fpage>–<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1016/s0003-3472(81)80146-7</pub-id></mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thiessen</surname>, <given-names>E. D.</given-names></string-name>, &amp; <string-name><surname>Saffran</surname>, <given-names>J. R</given-names></string-name></person-group>. (<year>2009</year>). <article-title>How the Melody Facilitates the Message and Vice Versa in Infant Learning and Memory</article-title>. <source>Annals of the New York Academy of Sciences</source>, <volume>1169</volume>(<issue>1</issue>), <fpage>225</fpage>–<lpage>233</lpage>. <pub-id pub-id-type="doi">10.1111/j.1749-6632.2009.04547.x</pub-id></mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Toiviainen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Luck</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Thompson</surname>, <given-names>M. R</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Embodied Meter: Hierarchical Eigenmodes in Music-Induced Movement</article-title>. <source>Music Perception</source>, <volume>28</volume>(<issue>1</issue>), <fpage>59</fpage>–<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1525/mp.2010.28.1.59</pub-id></mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trainor</surname>, <given-names>L. J</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Infant preferences for infant-directed versus noninfant-directed playsongs and lullabies</article-title>. <source>Infant Behavior and Development</source>, <volume>19</volume>(<issue>1</issue>), <fpage>83</fpage>–<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1016/S0163-6383(96)90046-6</pub-id></mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trainor</surname>, <given-names>L. J.</given-names></string-name>, &amp; <string-name><surname>Desjardins</surname>, <given-names>R. N</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Pitch characteristics of infant-directed speech affect infants’ ability to discriminate vowels</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>9</volume>(<issue>2</issue>), <fpage>335</fpage>–<lpage>340</lpage>. <pub-id pub-id-type="doi">10.3758/BF03196290</pub-id></mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trainor</surname>, <given-names>L. J.</given-names></string-name>, &amp; <string-name><surname>Zacharias</surname>, <given-names>C. A</given-names></string-name></person-group>. (<year>1998</year>). <article-title>Infants prefer higher-pitched singing</article-title>. <source>Infant Behavior and Development</source>, <volume>21</volume>(<issue>4</issue>), <fpage>799</fpage>–<lpage>805</lpage>. <pub-id pub-id-type="doi">10.1016/S0163-6383(98)90047-9</pub-id></mixed-citation></ref>
<ref id="c99"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trehub</surname>, <given-names>S. E</given-names></string-name></person-group>. (<year>2003</year>). <article-title>The developmental origins of musicality</article-title>. <source>Nature Neuroscience</source>, <volume>6</volume>(<issue>7</issue>), <fpage>669</fpage>–<lpage>673</lpage>. <pub-id pub-id-type="doi">10.1038/nn1084</pub-id></mixed-citation></ref>
<ref id="c100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trehub</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Becker</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Morley</surname>, <given-names>I</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Cross-cultural perspectives on music and musicality</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>370</volume>(<issue>1664</issue>), <fpage>20140096</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2014.0096</pub-id></mixed-citation></ref>
<ref id="c101"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trevarthen</surname>, <given-names>C</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Musicality and the intrinsic motive pulse: Evidence from human psychobiology and infant communication</article-title>. <source>Musicae Scientiae</source>, <volume>3</volume>(<issue>1_suppl</issue>), <fpage>155</fpage>–<lpage>215</lpage>. <pub-id pub-id-type="doi">10.1177/10298649000030S109</pub-id></mixed-citation></ref>
<ref id="c102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsang</surname>, <given-names>C. D.</given-names></string-name>, &amp; <string-name><surname>Conrad</surname>, <given-names>N. J</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Does the message matter? The effect of song type on infants’ pitch preferences for lullabies and playsongs</article-title>. <source>Infant Behavior and Development</source>, <volume>33</volume>(<issue>1</issue>), <fpage>96</fpage>–<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1016/j.infbeh.2009.11.006</pub-id></mixed-citation></ref>
<ref id="c103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Dyck</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Moelants</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Demey</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Deweppe</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Coussement</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Leman</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2013</year>). <article-title>The Impact of the Bass Drum on Human Dance Movement</article-title>. <source>Music Perception</source>, <volume>30</volume>(<issue>4</issue>), <fpage>349</fpage>–<lpage>359</lpage>. <pub-id pub-id-type="doi">10.1525/mp.2013.30.4.349</pub-id></mixed-citation></ref>
<ref id="c104"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vuust</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Witek</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Rhythmic complexity and predictive coding: A novel approach to modeling rhythm and meter perception in music</article-title>. <source>Frontiers in Psychology</source>, <volume>5</volume>, <fpage>1111</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2014.01111</pub-id></mixed-citation></ref>
<ref id="c105"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wass</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Whitehorn</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Marriott Haresign</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Phillips</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Leong</surname>, <given-names>V</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Interpersonal Neural Entrainment during Early Social Interaction</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>24</volume>(<issue>4</issue>), <fpage>329</fpage>–<lpage>342</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2020.01.006</pub-id></mixed-citation></ref>
<ref id="c106"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Winkler</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Háden</surname>, <given-names>G. P.</given-names></string-name>, <string-name><surname>Ladinig</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Sziller</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Honing</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Newborn infants detect the beat in music</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>106</volume>(<issue>7</issue>), <fpage>2468</fpage>–<lpage>2471</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0809035106</pub-id></mixed-citation></ref>
<ref id="c107"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wunderlich</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Cone-Wesson</surname>, <given-names>B. K.</given-names></string-name>, &amp; <string-name><surname>Shepherd</surname>, <given-names>R. K</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Maturation of the cortical auditory evoked potential in infants and young children</article-title>. <source>Hearing Research</source>. <pub-id pub-id-type="doi">10.1016/j.heares.2005.11.010</pub-id></mixed-citation></ref>
<ref id="c108"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zentner</surname>, <given-names>M. R.</given-names></string-name>, &amp; <string-name><surname>Eerola</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Rhythmic engagement with music in infancy</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>107</volume>(<issue>13</issue>), <fpage>5768</fpage>–<lpage>5773</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1000121107</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107088.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Dubois</surname>
<given-names>Jessica</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Inserm Unité NeuroDiderot, Université Paris Cité</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study offers <bold>important</bold> insights into the development of infants' responses to music based on the exploration of EEG neural auditory responses and video-based movement analysis. The <bold>convincing</bold> results revealed that evoked responses emerge between 3 and 12 months of age, but data analysis requires further refinement to fully complement the findings related to movement in response to music. This study will be of significant interest to developmental psychologists and neuroscientists, as well as researchers interested in music processing and in the translation of perception into action.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107088.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study aims to investigate the development of infants' responses to music by examining neural activity via EEG and spontaneous body kinematics using video-based analysis. The authors also explore the role of musical pitch in eliciting neural and motor responses, comparing infants at 3, 6, and 12 months of age.</p>
<p>Strengths:</p>
<p>A key strength of the study lies in its analysis of body kinematics and modeling of stimulus-motor coupling, demonstrating how the amplitude envelope of music predicts infant movement, and how higher musical pitch may enhance auditory-motor synchronization.</p>
<p>Weaknesses:</p>
<p>The neural data analysis is currently limited to auditory evoked potentials aligned with beat timing. A more comprehensive approach is needed to robustly support the proposed developmental trajectory of neural responses to music.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107088.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Infants' auditory brain responses reveal processing of music (clearly different from shuffled music patterns) from the age of 3 months; however, they do not show a related increase in spontaneous movement activity to music until the age of 12 months.</p>
<p>Strengths:</p>
<p>This is a nice paper, well designed, with sophisticated analyses and presenting clear results that make a lot of sense to this reviewer. The additions of EEG recordings in response to music presentations at 3 different infant ages are interesting, and the manipulation of the music stimuli into shuffled, high, and low pitch to capture differences in brain response and spontaneous movements is good. I really enjoyed reading this work and the well-written manuscript.</p>
<p>Weaknesses:</p>
<p>I only have two comments. The first is a change to the title. Maybe the title should refer to the first &quot;postnatal&quot; year, rather than the first year of life. There are controversies about when life really starts; it could be in the womb, so using postnatal to refer to the period after birth resolves that debate.</p>
<p>The other comment relates to the 10 Principal Movements (PMs) identified. I was wondering about the rationale for identifying these different PMs and to what extent many PMs entered in the analyses may hinder more general pattern differences. Infants' spontaneous movements are very variable and poorly differentiated in early development. Maybe, instead of starting with 10 distinct PMs, a first analysis could be run using the combined Quantity of Movements (QoM) without PM distinctions to capture an overall motor response to music. Maybe only 2 PMs could be entered in the analysis, for the arms and for the legs, regardless of the patterns generated. Maybe the authors have done such an analysis already, but describing an overall motor response, before going into specific patterns of motor activation, could be useful to describe the level of motor response. Again, infants provide extremely variable patterns of response, and such variability may potentially hinder an overall effect if the QoM were treated as a cumulated measure rather than one with differentiated patterns.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107088.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study provides a detailed investigation of neural auditory responses and spontaneous movements in infants listening to music. Analyses of EEG data (event-related potentials and steady-state responses) first highlighted that infants at 3, 6, and 12 months of age and adults showed enhanced auditory responses to music than shuffled music. 6-month-olds also exhibited enhanced P1 response to high-pitch vs low-pitch stimuli, but not the other groups. Besides, whole body spontaneous movements of infants were decomposed into 10 principal components. Kinematic analyses revealed that the quantity of movement was higher in response to music than shuffled music only at 12 months of age. Although Granger causality analysis suggested that infants' movement was related to the music intensity changes, particularly in the high-pitch condition, infants did not exhibit phase-locked movement responses to musical events, and the low movement periodicity was not coordinated with music.</p>
<p>Strengths:</p>
<p>This study investigates an important topic on the development of music perception and translation to action and dance. It targets a crucial developmental period that is difficult to explore. It evaluates two modalities by measuring neural auditory responses and kinematics, while cross-modal development is rarely evaluated. Overall, the study fills a clear gap in the literature.</p>
<p>Besides, the study uses state-of-the-art analyses. All steps are clearly detailed. The manuscript is very clear, well-written, and pleasant to read. Figures are well-designed and informative.</p>
<p>Weaknesses:</p>
<p>(1) Differences in neural responses to high-pitch vs low-pitch stimuli between 6-month-olds and other infants are difficult to interpret.</p>
<p>(2) Making some links between the neural and movement responses that are described in this manuscript could be expected, given the study goal. Although kinematic analyses suggested that movement responses are not phase-locked to the music stimuli, analyses of Granger causality between motion velocity and neural responses could be relevant.</p>
<p>(3) The study considers groups of infants at different ages, but infants within each group might be at different stages of motor development. Was this assessed behaviorally? Would it be possible to explore or take into account this possible inter-individual variability?</p>
</body>
</sub-article>
</article>