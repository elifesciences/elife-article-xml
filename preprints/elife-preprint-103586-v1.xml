<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">103586</article-id>
<article-id pub-id-type="doi">10.7554/eLife.103586</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103586.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>High-throughput unsupervised quantification of patterns in the natural behavior of marmosets</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7483-994X</contrib-id>
<name>
<surname>Menegas</surname>
<given-names>William</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>menegas@mit.edu</email>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Corbett</surname>
<given-names>Erin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Beliard</surname>
<given-names>Kimberly</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5753-8264</contrib-id>
<name>
<surname>Xu</surname>
<given-names>Haoran</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Parmar</surname>
<given-names>Shivangi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5938-4227</contrib-id>
<name>
<surname>Desimone</surname>
<given-names>Robert</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8021-277X</contrib-id>
<name>
<surname>Feng</surname>
<given-names>Guoping</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>fengg@mit.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ymca674</institution-id><institution>Hock E. Tan and K. Lisa Yang Center for Autism Research, Yang Tan Collective, McGovern Institute for Brain Research, Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology</institution></institution-wrap>, <city>Cambridge</city>, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0ya142</institution-id><institution>Stanley Center for Psychiatric Research, Broad Institute of MIT and Harvard</institution></institution-wrap>, <city>Cambridge</city>, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Noel</surname>
<given-names>Jean-Paul</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Minnesota</institution>
</institution-wrap>
<city>Minneapolis</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Taffe</surname>
<given-names>Michael A</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of California, San Diego</institution>
</institution-wrap>
<city>San Diego</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>Equal contribution</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-12-18">
<day>18</day>
<month>12</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP103586</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-10-02">
<day>02</day>
<month>10</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-09-20">
<day>20</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.08.30.610159"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Menegas et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Menegas et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-103586-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Recent advances in genetic engineering have accelerated the production of nonhuman primate models for neuropsychiatric disorders. To use these models for preclinical drug testing, behavioral screening methods will be necessary to determine how the model animals deviate from controls, and whether treatments can restore typical patterns of behavior. In this study, we collected a multimodal dataset from a large cohort of marmoset monkeys and described typical patterns in their natural behavior. We found that these behavioral measurements varied substantially across days, and that behavioral state usage was highly correlated to the behavior of cagemates and to the vocalization rate of other animals in the colony. To elicit acute behavioral responses, we presented animals with a panel of stimuli including novel, appetitive, neutral, aversive, and social stimuli. By comparing these behavioral conditions, we demonstrate that outlier detection can be used to identify atypical responses to a range of stimuli. This data will help guide the study of marmosets as models for neuropsychiatric disorders.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Additional data was added to Figure 5, and more detail was added to the Methods section.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Expanding the number of species across the phylogenetic tree that can be used to test potential treatments for neuropsychiatric disorders should increase the probability that these treatments can generalize to humans<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. Marmosets have recently gained significant attention as a nonhuman primate model species<sup><xref ref-type="bibr" rid="c2">2</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup> that displays several types of social behaviors which are disrupted in human neuropsychiatric disorders such as vocal communication<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>, face processing<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>, and social learning<sup><xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c13">13</xref></sup>. We applied methods for studying natural behavior, vocalizations, responses to stimuli, and cognitive ability to a large cohort of marmosets to find patterns characteristic of typically developing animals. This data will help determine what symptoms are present in neuropsychiatric disorder model animals, and whether treatments can revert the symptoms.</p>
<p>The application of deep learning to the quantification of video data has recently expanded the potential scope of behavioral measurements. Specifically, methods for extracting animal posture from video data<sup><xref ref-type="bibr" rid="c14">14</xref>–<xref ref-type="bibr" rid="c16">16</xref></sup> have been transformative because they allow high-dimensional video data to be distilled into much lower-dimensional postural time series data that retains information about the animals’ overt movements. Furthermore, novel approaches to supervised<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup> and unsupervised<sup><xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c22">22</xref></sup> modeling have demonstrated that patterns in these large new datasets can be identified and interpreted. These computational approaches to ethology have already been applied to the study of neuropsychiatric disorder model mice to reveal similarities and differences across different mouse models of autism spectrum disorder (ASD)<sup><xref ref-type="bibr" rid="c23">23</xref></sup>. Deep learning has also been applied to the annotation and study of patterns in audio data<sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>. This has allowed for high-dimensional audio data (spectrograms) to be compressed into low-dimensional time series data, which can be modeled to understand the basic patterns underlying these complex social processes<sup><xref ref-type="bibr" rid="c26">26</xref></sup>.</p>
<p>Because the phenotypes of neuropsychiatric disorders are defined in relation to the behavioral patterns of typically developing controls, we collected a large baseline video and audio dataset from group-housed animals living in captivity comprised of 5,400 sessions from 120 animals living in 36 different family units. We found that the animals’ natural behavioral state usage was highly correlated with environmental variables such as their cagemates’ behavior and the vocalizations of other animals in the colony. To elicit acute behavioral responses, we presented the animals with a set of stimuli including novel, appetitive, neutral, aversive, and social stimuli. We found that patterns in the acute responses to these stimuli were consistent across animals, meaning that this data could be used to benchmark methods for characterizing the response to each stimulus. Human patients with ASD can display diverse symptoms<sup><xref ref-type="bibr" rid="c27">27</xref></sup> including differences in social affect, repetitive behaviors, restricted interests, lower verbal ability, and impaired cognitive ability.</p>
<p>Importantly, this diversity is observed even in monogenic forms of ASD, such as in Phelan-McDermid syndrome<sup><xref ref-type="bibr" rid="c28">28</xref>–<xref ref-type="bibr" rid="c30">30</xref></sup>, which can be studied in animal models with alterations in the Shank3 gene<sup><xref ref-type="bibr" rid="c31">31</xref></sup>. These patients have a range of symptoms, and individual patients can exhibit different combinations of symptoms. For this reason, we aimed to develop a framework for identifying outliers on an individual basis (by comparing data from each animal to the whole-population distribution) rather than on a group basis (comparing across pre-defined groups of animals).</p>
<p>To benchmark our methods, we measured the distinguishability between nine stimulus-response conditions. Surprisingly, we found that most stimuli did not cause large changes in the total usage of each behavioral state. Instead, the responses to these stimuli were better differentiated by comparing the correlation patterns between the usage of each behavioral state and the time spent interacting with the stimuli. Using these patterns, we showed that outlier detection could distinguish between conditions with similar accuracy as 2-class classification. Crucially, we found that outlier detection was also robust to heterogeneity, whereas 2-class classification required homogeneity in the training and test data. This demonstrates the importance of using outlier detection in cases where experimental groups may be heterogeneous, such as in ASD models.</p>
<p>Overall, these results will help guide studies of how marmoset models for neuropsychiatric disorders differ from typical animals, and whether treatments can revert these changes.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Extracting behavioral data from video</title>
<p>Marmosets form stable breeding pairs that cooperate to raise infants during their development, which takes over a year. Because marmosets often give birth to 2 sibling animals at a time, they usually live in a family of at least 4 total animals when raised in captivity<sup><xref ref-type="bibr" rid="c32">32</xref></sup>. Once the adult female gives birth to another set of siblings, older siblings are paired with new partners in different cages to form their own family units. To study marmosets living in this controlled semi-natural environment, we trained a neural network to label 21 postural key points of multiple animals (<xref rid="fig1" ref-type="fig">Fig. 1a-b</xref>) and developed a classifier to distinguish animals based on the color of the hair tuft near the ear, which we artificially dyed to make animals easily distinguishable to a human observer (<xref rid="fig1" ref-type="fig">Fig. 1c-d</xref>). This allowed us to extract postural time series data from 36 cages with similar family structure (two adult parents and two juvenile animals).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Pose estimation for marmosets in group housing</title>
<p><bold>a)</bold> Schematic of home cage video recording, and keypoint label accuracy (pixel-based root mean square error (RMSE) across an increasing number of labeled frames). Bars indicate standard deviation across training sessions. <bold>b)</bold> Example time series of all keypoints (Y coordinate), colored based on animal hair color. <bold>c)</bold> Example labeled image, and ID accuracy based on 8000 labeled animals drawn from 112 different camera installations. “Missing” indicates that an animal was not labeled and “Switched” indicates that an animal was labeled with the wrong identity. <bold>d)</bold> Example time series of all keypoints (X and Y coordinates), colored based on animal hair color. e) Raw data time series (z-scored) across 6 minutes (left), and PCA of raw data (right). <bold>f)</bold> Egocentric data time series (z-scored) after normalization using X/Y position and orientation (left), and PCA of egocentric data (right).</p></caption>
<graphic xlink:href="610159v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Principal component analysis of the posture time series data revealed that most of the variance was explained by the animal’s X position, Y position, and body angle (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>, <xref rid="figs1" ref-type="fig">Fig. S1</xref>). This is an expected result from video data where the animals are small compared to the field of view. To extract more detailed information about the animals’ body movements, we created an egocentric version of the data that was normalized with respect to X position, Y position, and body angle. In this egocentric data, the principal components of variance reflect differences in posture, such as head angle and limb position (<xref rid="fig1" ref-type="fig">Fig. 1f</xref>, <xref rid="figs1" ref-type="fig">Fig. S1</xref>). Because the Z position of the animal can vary dramatically as animals climb on the cage walls, we used stereo vision to estimate the animals’ depths in the cage (<xref rid="figs2" ref-type="fig">Fig. S2</xref>). Recent work has shown that is also possible to estimate 3D postures directly from 2D postures<sup><xref ref-type="bibr" rid="c33">33</xref></sup>, and this will be a promising future direction for enabling similar types of high-throughput video data collection with even fewer cameras. In this study, we used stereo vision to estimate depth since we collected video from multiple cameras above each cage. Notably, we temporarily installed smooth transparent ceilings during the weeks of recording to restrict the animals’ ability to climb on the ceiling, and temporarily placed horizontal cage dividers to reduce the total cage size during data collection.</p>
<p>Labeling natural behavior on a frame-by-frame basis is a difficult task because behaviors can have different timescales and levels of description. Additionally, an animal may simultaneously perform multiple behaviors at once. Our approach to this problem was to train a recurrent neural network to use long input sequences of features (25 seconds) to predict the following 5 seconds (<xref rid="fig2" ref-type="fig">Fig. 2a-b</xref>). After training the network, we hierarchically clustered the latent space using t-SNE while systematically varying the alpha and perplexity parameters<sup><xref ref-type="bibr" rid="c34">34</xref></sup>. We then traced cluster identities across hierarchical levels (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>, S3). We found that the lowest level of clustering (level 5) resulted in sub-second labels with very specific human descriptions (<xref rid="fig2" ref-type="fig">Fig. 2e-f</xref>, S4). The highest level of clustering (level 1) divided the data based on broad categories that humans described as “climbing”, “active”, “social”, and “alone” (<xref rid="figs4" ref-type="fig">Fig. S4</xref>). To increase the human-interpretability of these clusters, we plotted the distribution of features across clusters at every hierarchical level (<xref rid="figs4" ref-type="fig">Fig. S4</xref>). This unsupervised hierarchical approach to labeling behavioral states allowed us to study behavioral responses at multiple levels of description.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Hierarchical description of natural behavior using deep learning</title>
<p><bold>a)</bold> Schematic of network structure. Latent state at each time point used for downstream analysis. <bold>b)</bold> Example ethogram across 60 seconds with colors indicating behavioral state as defined by the cluster ID. c) Latent space clustering using t-SNE with decreasing values of a and perplexity to produce a hierarchical clustering (with colors representing different clusters). <bold>d)</bold> Hierarchical organization of behavioral states across levels of analysis. with the average usage for each state(% of total time) shown as a bar graph. <bold>e)</bold> Hierarchical organization of behavioral states, with colors representing cluster ID. <bold>f)</bold> Average state duration at each level of analysis, with each point representing the average state duration from one video session. Bars indicate quartiles (25%, 50%, 75%) and open circles indicate the median across sessions.</p></caption>
<graphic xlink:href="610159v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To understand how the input time series data was weighed in the bottleneck layer of the model, we measured the average attention allocation to previous time points (<xref rid="figs5" ref-type="fig">Fig. S5</xref>). We found that most attention was concentrated in the 1-second period prior to the current frame, but that attention was more evenly distributed across the prior 20-second period when animals’ behavioral states remained similar across those frames (<xref rid="figs5" ref-type="fig">Fig. S5</xref>). This demonstrates a key benefit of using a deep neural network model to describe the ongoing behavioral state. Compared to using features with pre-determined durations (which would specifically set the timescale of the analysis based on the kernel size of those features), this approach allowed the model to access information from different timescales.</p>
</sec>
<sec id="s2b">
<title>Comparing striatal neural data to video data</title>
<p>To determine whether the organization of the behavioral latent space was relevant to the neural representation of ongoing behavior, we recorded striatal neuron activity from 4 animals using silicon probes mounted on movable drives (<xref rid="fig3" ref-type="fig">Fig. 3a-c</xref>). Each animal was implanted with two 64 channel probes. We recorded neural activity in the striatum because it is known to contain diverse signals related to movement<sup><xref ref-type="bibr" rid="c35">35</xref></sup> and social interactions<sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. To allow animals to have a full range of movement and to have natural interactions with cagemates, we used on-head amplifiers and an on-head data logger to wirelessly record from these neurons while animals moved freely in their home cages.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Striatal neurons reflect behavioral states defined by deep learning</title>
<p><bold>a)</bold> Schematic of silicon probe implant design and on-head wireless recording. <bold>b)</bold> Example image from video of two animals living in the same cage, with one of those animals wearing a wireless logger. <bold>c)</bold> Example of spike data from 1 minute of recording from 128 channels (spikes sorted into 25 units). <bold>d)</bold> All recorded neurons clus­ tered by similarity, with colors indicating the highest-level clusters of neurons. <bold>e)</bold> For each of the 4 high level clusters: the regions in the behavioral latent space with enriched activity (left), the normalized change in firing rate in each of the highest-level behavioral clusters (mid-left), the transitions with enriched activity (mid-right), and summary of the enriched states and transitions (right). *** indicates p <sub>&lt;</sub> 10-<sup><xref ref-type="bibr" rid="c20">20</xref></sup> using a one-sample t-test to determine if neurons had a higher firing rate during each behavioral state. <bold>f)</bold> Data from four example neurons showing their firing rate enrichment (left) and the normalized change in firing rate based on the position of the other animal (right). Position is displayed as a distance relative to the head radius (h.r.) of the animal. One example neuron was selected from each of the 4 highest-level clusters.</p></caption>
<graphic xlink:href="610159v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Because the number of concurrently recorded neurons was relatively low, we did not use population-based methods to analyze the data – as is possible in studies where hundreds or thousands of neurons are recorded simultaneously<sup><xref ref-type="bibr" rid="c38">38</xref></sup>. Instead, to estimate whether individual neuronal activity was related to the behavioral states defined by our clustering method, we compared each neuron’s firing rate distribution to shuffled data based on the overall frequency of each behavior during the session. We did not track the same neurons across multiple sessions, but instead moved the probes between sessions to record new neurons in subsequent sessions. We pooled the data from 234 sessions (from 4 animals) and hierarchically clustered neurons according to their firing rate enrichment maps (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>). We found that the highest-level clusters of striatal neurons had average activity patterns that were clearly related to the behavioral state clustering. Neural cluster 1 had increased activity when animals were in the “social” state (<xref rid="fig3" ref-type="fig">Fig. 3e</xref>), cluster 2 had increased activity when animals were in the “alone” state (<xref rid="fig3" ref-type="fig">Fig. 3e</xref>) and cluster has increased activity when animals were in the “climbing” state (<xref rid="fig3" ref-type="fig">Fig. 3e</xref>). Interestingly, cluster had higher activity when neurons were “alone” or “active” (<xref rid="fig3" ref-type="fig">Fig. 3e</xref>). This class of neurons is likely related to movement, because transitions between these two states would be characteristic of an animal alternating between movement and temporary pauses between movements (<xref rid="figs3" ref-type="fig">Fig. S3</xref>). Finally, we also found that individual neurons could have similar patterns of enrichment as the group averages (<xref rid="fig3" ref-type="fig">Fig. 3f</xref>).</p>
</sec>
<sec id="s2c">
<title>Large-scale data collection reveals variability across days and correlation between cagemates</title>
<p>To treat the study of marmoset behavior as a quantitative data science problem<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c40">40</xref></sup>, we collected a large dataset of natural behavioral data. We recorded video data from 120 individual animals living in 36 families at time points corresponding to the siblings being aged 3 months, 6 months, 9 months, 12 months, and 15 months. Some parents were observed with different sets of siblings, as the data collection took several years to complete. In total, we recorded 5400 sessions (defined as the data from one animal in one day) and sorted the sessions using PCA to visualize the largest sources of variance (<xref rid="figs6" ref-type="fig">Fig. S6</xref>). We found that behavioral state usage varied dramatically across days (<xref rid="fig4" ref-type="fig">Fig. 4a-b</xref>). Notably, there was a large amount of variability across all sessions (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>), and individual animals measured longitudinally had large within-animal variability across sessions as well (<xref rid="fig4" ref-type="fig">Fig. 4d</xref>). To determine whether this variability could be caused by non-uniform sleep quality, we used motion watches to measure activity across all hours. We found that animals in our study had very consistent sleep patterns and that sleep quality was similar across days (<xref rid="figs7" ref-type="fig">Fig. S7</xref>). Because of the variability in behavioral state usage across days, it would be difficult to find stable phenotypes of individual animals without collecting weeks or months of data per animal. To determine whether animals’ stable behavioral traits could be measured more efficiently, we aimed to better understand the sources of the observed variability.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Behavioral distribution varies across days and is highly correlated to cagemates</title>
<p><bold>a)</bold> Example behavioral state usage from 4 different days sampled from two cagemates. Circle diameter indicates the frequency of each behavioral state. <bold>b)</bold> Distribution across all animals segmented into 10 quantiles with error bars representing the 25%, 50%, and 75% boundaries in the data. c) Distribution of behavioral states across 5400 sessions sampled from 112 animals, sorted based on PC1 from principal component analysis across sessions. <bold>d)</bold> Example distribution of behavioral states across 58 sessions sampled from a single animal, also sorted based on PC1. <bold>e)</bold> Correlation of time spent active across all animals (each dot represents one paired observation of an animal and one cagemate. <bold>f)</bold> Top: distribution of PC1 and PC2 across 45 sessions from an example family of 4 marmosets. Bottom: total time spent in active behavioral states across the 45 sessions, with colors representing animal ID. <bold>g)</bold> Correlation between measurements of activity normalized by cagemate activity (X-axis) or normalized by average colony activity (Y-axis). Each dot represents single animal average.</p></caption>
<graphic xlink:href="610159v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>When measuring the time spent in “active” behavioral states across 45 sessions (non-continuous days) in an example family, we found that cagemates displayed highly similar patterns of activity (<xref rid="fig4" ref-type="fig">Fig. 4e-f</xref>). We pooled data from all simultaneously measured animals and found a high correlation between the time spent active and the cagemates’ time spent active (<xref rid="fig4" ref-type="fig">Fig. 4e</xref>). To better understand the timescale of this effect, we compared the average correlation to cagemates across timescales ranging from seconds to hours (<xref rid="figs6" ref-type="fig">Fig. S6</xref>). For this analysis, we calculated correlation based on the average usage of all behavioral states. We found that both the correlations to cagemates and the correlation to shuffled data increased as a function of bin size. This is because the distribution of behavioral states becomes more similar to the mean as the sampling window size increases. However, the correlation to cagemates was higher than the correlation to shuffled data across all bin sizes (<xref rid="figs6" ref-type="fig">Fig. S6</xref>). Interestingly, measuring an animal’s average time spent active relative to cagemates gave an accurate estimate of the animal’s average time spent active relative to the colony average (<xref rid="fig4" ref-type="fig">Fig. 4g</xref>). Overall, these results indicate that measurements of animals’ behavioral traits depend heavily on their social environment.</p>
</sec>
<sec id="s2d">
<title>Correlation between total call rate in a housing room and behavioral state usage</title>
<p>Families of marmosets are housed in separate cages, but animals can make loud calls that likely influence the behavioral state of neighboring cages despite the lack of visual access. Because we found that animals’ behavioral state usage was highly correlated to their cagemates’ behaviors, we also wanted to measure the effect that these loud vocalizations from neighboring cages could have on behavioral state usage. To do this, we installed microphones outside of each cage in a room, such that the signal would be loudest when a call originated from a nearby cage (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>, S8), but loud calls in the room could still be detected by each microphone. As is standard in this colony, visual barriers between cages prevented animals from directly observing their neighbors. We trained a neural network to identify segments of audio data that contained vocalizations using human labels (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>) and trained a separate network to classify the vocalizations according to their call type (<xref rid="figs9" ref-type="fig">Fig. S9</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Room call rate varies across days and is highly correlated to behavior</title>
<p><bold>a)</bold> Schematic of microphone distribution and example of raw amplitude data converted to spectrogram data. <bold>b)</bold> Example audio data classified by a supervised neural network as time containing no calls (15 minutes total of concatenated spectrograms, top), and containing calls of any type (15 minutes total of concatenated spectro­ grams, bottom). <bold>c)</bold> Distribution of time spent near other animals from 5 minute clips (n = 3808 clips) from 36 cages_ <bold>d)</bold> Distribution of average call rates across all 5 minute clips (n = 3808 clips) from 36 cages_<bold>e)</bold> Example series of non-continuous 5 minute clips comparing the time spent near other animals (top) and total call rate in the room (bottom). <bold>f)</bold> Comparison of cages with 2 animals (top) or 4 animals (bottom). <bold>g)</bold> Correlation between behavioral state usage and call rate across n=52 pair housed animals, with * indicating p&lt;10·<sup>2</sup> and *** indicating p&lt;10-<sup><xref ref-type="bibr" rid="c10">10</xref></sup> using a one-sample t-test to determine if each behavioral state was positively/negatively correlated with call rate across animals. h) Top: Correlations between behavioral states and call rate from one example animal, colored based on behavioral state. Each point represents one 5 minute clip. Bottom: Hierarchical organization of the correlation between behavioral state usage and room call rate. Bars indicate the quartiles (25%, 50%, and 75%) and circles indicate the median across animals.</p></caption>
<graphic xlink:href="610159v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Similar to behavioral state usage (<xref rid="fig5" ref-type="fig">Fig. 5c</xref>), we found that the total call rate varied dramatically across recordings (<xref rid="fig5" ref-type="fig">Fig. 5d</xref>). We split the audio data into 5-minute (non-continuous) segments and found that the average call rate in these segments varied from 0 calls per minute to 60 calls per minute (<xref rid="fig5" ref-type="fig">Fig. 5d-e</xref>). We measured the correlation between call rate and behavioral state usage and found a strong negative correlation between call rate and the high-level “social” behavioral state (<xref rid="fig5" ref-type="fig">Fig. 5f-g</xref>). To better understand which specific behaviors were anticorrelated with high call rate, we plotted the correlation across all 80 low-level behavioral states (<xref rid="fig5" ref-type="fig">Fig. 5h</xref>) and found that call rate was specifically anti-correlated with social behavioral states in which the animals are at rest and not moving (<xref rid="fig5" ref-type="fig">Fig. 5h</xref>, S4). This implies that a high rate of calls in a room can interrupt animals during social resting states and cause them to preferentially exhibit more active/attentive states. To confirm the generality of this effect, we compared data from 52 animals living in families of 4 animals (two parents and two siblings) to data from 43 animals living in families of 2 animals (two unrelated adults). In both datasets, a similar overall pattern emerged (<xref rid="figs10" ref-type="fig">Fig. S10</xref>).</p>
</sec>
<sec id="s2e">
<title>Measuring behavioral response to a panel of stimuli</title>
<p>Because the animals’ natural behavior was highly variable and highly correlated to their cagemates’ behavior and facility conditions, we aimed to measure species-typical responses to a set of stimuli that could be robustly elicited by presenting animals with these stimuli. We selected a range of stimuli including positive, neutral, negative, novel, and social stimuli (<xref rid="fig6" ref-type="fig">Fig. 6a</xref>, <xref rid="figs11" ref-type="fig">Fig. S11</xref>). These stimuli were presented to the animals over the course of a 3-week recording session, which was repeated every 3 months based on the age of the juveniles in the cage. We found that most stimuli did not cause large changes in behavioral state usage rates (<xref rid="fig6" ref-type="fig">Fig. 6a</xref>, <xref rid="figs12" ref-type="fig">Fig. S12</xref>). Interestingly, we also found that some stimuli which did not cause a large change in behavioral state usage did still cause a significant change in the time spent near the stimulus (<xref rid="fig6" ref-type="fig">Fig. 6b</xref>). To better understand this data, we looked for correlations between behavioral state usage and the amount of time spent interacting with each stimulus (<xref rid="figs11" ref-type="fig">Fig. S11</xref>, <xref rid="figs12" ref-type="fig">Fig. S12</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Comparing across behavioral conditions induced by stimuli</title>
<p><bold>a)</bold> Change in total time spent active during the presentation of each stimulus. * indicates p&lt;1<sub>Q·</sub><sup>3</sup>, ** indicates p&lt;1Q-6, and*** indicates p&lt;10-<sup>20</sup>using a one-sided t-test to determine whether each stimulus increased or decreased the time spent active. <bold>b)</bold> Change in time spent in the region of the cage where the stimulus is placed (for each stimulus). * indicates p&lt;10-3, ** indicates p&lt;1Q-<sup>6</sup>, and *** indicates p&lt;10-<sup><xref ref-type="bibr" rid="c20">20</xref></sup> using a one-sided t-test to determine whether each stimulus increased or decreased the time spent near the stimulus. <bold>c)</bold> Left: average usage of behavioral states when Toy or Tablet are presented. <bold>d)</bold> Difference in usage between conditions and discriminability between conditions based on usage.* indicates p&lt;10 3,** indicates p&lt;10<sup>6</sup>, and*** indicates p&lt;10-<sup><xref ref-type="bibr" rid="c20">20</xref></sup> using a paired t-test to determine whether there was a difference in behavioral state usage between conditions. Paired t-tests compare data from the same animal in different conditions. e) Left: average correlation between behavioral state usage and time spent interacting with stimuli in each condition. <bold>f)</bold> Difference in correla­ tions between conditions and discriminability between conditions based on these correlations. * indicates p&lt;1Q·<sup>3</sup>, ** indicates p&lt;1Q-6, and *** indicates p&lt;10-<sup>20</sup> using a paired t-test to determine whether there was a difference in behavioral state correlations between conditions. Paired t-tests compare data from the same animal in different conditions.</p></caption>
<graphic xlink:href="610159v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>As an example of this type of analysis, we compared the “toy response” and “tablet response” conditions (<xref rid="fig6" ref-type="fig">Fig. 6c-f</xref>). We found that the usage of most behavioral states was similar between these conditions, though a few were different (<xref rid="fig6" ref-type="fig">Fig. 6c-d</xref>). This allowed the conditions to be weakly discriminated based on behavioral state usage (<xref rid="fig6" ref-type="fig">Fig. 6d</xref>). By contrast, the correlation patterns between behavioral state usage and time spent interacting with each stimulus were very different between conditions (<xref rid="fig6" ref-type="fig">Fig. 6e-f</xref>). For example, social behavioral state usage was much more highly correlated with tablet interaction time than with toy interaction time. This resulted in high discriminability between the conditions using the behavioral state correlation data (<xref rid="fig6" ref-type="fig">Fig. 6f</xref>).</p>
<p>Overall, these results suggest that some stimuli elicit large responses that can be clearly distinguished based on behavioral state usage, but some stimuli required the behavioral state correlation measurements to be discriminable from other stimulus response conditions (<xref rid="figs12" ref-type="fig">Fig. S12</xref>). For the following analyses, we use both behavioral state usage and behavioral state correlation measurements as features for studying the species-typical responses to each stimulus condition.</p>
</sec>
<sec id="s2f">
<title>Outlier detection</title>
<p>Given the heterogeneity across patients in human neuropsychiatric disorders, we aimed to develop a phenotyping method to determine whether each animal had a “typical” or “atypical” response to each stimulus. To validate this methodology, we measured the distinguishability of the behavioral responses to each stimulus based on patterns in the behavioral state usage and behavioral state correlations (<xref rid="fig7" ref-type="fig">Fig. 7</xref>).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Discriminating between conditions using outlier detection</title>
<p><bold>a)</bold> Strategy for assigning outlier score based on a probability distribution defined by “Condition 1” and then applied to either held-out data from “Condition 1” or to held-out data from “Condition 2”. <bold>b)</bold> Outlier scores for each behavioral state across 32 age-matched animals in Condition 2. <bold>c)</bold> Discriminability between example conditions (Toy and Touchscreen) based on 2-class classification. <bold>d)</bold> Discriminability between example condi­ tions (Toy and Touchscreen) based on 1-class outlier detection. Bars indicate quartiles (25%, 50%, 75%) and open circles indicate the median score for each group. <bold>e)</bold> Schematic of 2-class classification. See methods for full detail. <bold>f)</bold> Comparison of auROC scores (from held-out data) across models trained using a different number of animals (n=4 to n=32). <bold>g)</bold> Schematic of outlier detection. See methods for full detail. <bold>h)</bold> Comparison of auROC scores (from held-out data) across models trained using a different number of animals (n=4 to n=32).</p></caption>
<graphic xlink:href="610159v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We first estimated a probability distribution for each feature in each condition (<xref rid="fig7" ref-type="fig">Fig. 7a</xref>). Next, we assigned outlier scores for new data, which came either from held-out animals responding to the same stimulus (“Condition 1”), or from animals responding to another stimuli (“Condition 2”).</p>
<p>We assigned an outlier score for each feature based on the probability distributions derived from animals in the training set (<xref rid="fig7" ref-type="fig">Fig. 7a</xref>). As expected, data from different conditions had higher outlier scores (<xref rid="fig7" ref-type="fig">Fig. 7b</xref>). To benchmark the number of animals needed to robustly discriminate between conditions using outlier detection, we compared classification accuracy between 2-class classification using SVM models (<xref rid="fig7" ref-type="fig">Fig. 7c</xref>) and 1-class outlier detection (<xref rid="fig7" ref-type="fig">Fig. 7d</xref>).</p>
<p>We found that 2-class classification using SVM could accurately discriminate between conditions even with very few animals used as training data (<xref rid="fig7" ref-type="fig">Fig. 7e-f</xref>). This approach could be useful in cases where the number of animals available is small, and the phenotype is very consistent across animals because SVM models can assign greater importance to variables that differ between pre-defined classes. By contrast, 1-class outlier detection methods must weigh all variables equally. In spite of this, we found that outlier detection scores were able to accurately discriminate between conditions if the number of animals used in the training set was sufficient (<xref rid="fig7" ref-type="fig">Fig. 7g-h</xref>). For studies of nonhuman primates, where the total number of animals used per study must be minimized for ethical and logistical reasons, large datasets of typically developing animals will be valuable to use as control data for studies of disordered states that can use outlier detection to determine the type and severity of the disorders. In this study, we were able to use outlier scores to distinguish between stimulus response conditions with similar accuracy as 2-class classification in cases where a large number of age-matched typically developing animals were used in the training set (<xref rid="fig7" ref-type="fig">Fig. 7</xref>). One caveat to this approach is that outlier detection was less accurate when non-age-matched animals were used in the training set (<xref rid="figs13" ref-type="fig">Fig. S13</xref>). This is likely because of large scale differences between the behavioral state usage distributions of juvenile and adult animals (<xref rid="figs14" ref-type="fig">Fig. S14</xref>).</p>
<p>In the preceding analysis, we used data from typically developing animals that had similar responses to each stimulus. To simulate increased heterogeneity in the dataset, we artificially mixed data from multiple conditions so that we could measure the effect of heterogeneity on the discriminability of conditions (<xref rid="fig8" ref-type="fig">Fig. 8</xref>). Using 2-class classification, we found that the score given to these artificial outliers varied significantly based on the number of outliers artificially added to the training dataset (<xref rid="fig8" ref-type="fig">Fig. 8a</xref>). By contrast, outlier detection was not dependent on the number of outliers added to the training dataset (<xref rid="fig8" ref-type="fig">Fig. 8b</xref>). We measured this property across all pairs of conditions and found that classification-based scores varied based on the number of outliers artificially added to the training set across many pairs of conditions (<xref rid="fig8" ref-type="fig">Fig. 8c-d</xref>). By contrast, outlier detection was robust to changes in the heterogeneity of the training data across pairs of conditions (<xref rid="fig8" ref-type="fig">Fig. 8e-f</xref>). This demonstrates the potential usefulness of our outlier detection strategy for studying neuropsychiatric disorder model animals that could have heterogeneous phenotypes.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8:</label>
<caption><title>Outlier detection has stable performance regardless of heterogeneity in the dataset</title>
<p><bold>a)</bold> Scores assigned to inliers (blue) or outliers added to Condition 2 (red) using 2-class classification, with an increasing number of outliers artificially added to the training set. •• indicates p&lt;10<sup>12</sup> using at-test to determine whether the scores differed when the number of outliers added to the training set was O or 16. <bold>b)</bold> Scores assigned to inliers (blue) or outliers added to Condition 2 (red) using outlier detection, with an increasing number of outliers artificially added to the training set. ** indicates p&lt;10-<sup>12</sup> using at-test to determine whether the scores differed when the number of outliers added to the training set was O or 16. <bold>c)</bold> Schematic of 2-class classification with heterogeneity artificially added to Condition 2. See methods for full detail. <bold>d)</bold> Change in score assigned to outliers as a function of the number of outliers artificially added to the training data. <bold>e)</bold> Schematic of outlier detection with heterogeneity artificially added to Condition 2. See methods for full detail. <bold>f)</bold> Change in score assigned to outliers as a function of the number of outliers artificially added to the training data.</p></caption>
<graphic xlink:href="610159v2_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<sec id="s3a">
<title>Methods for quantifying natural behavior</title>
<p>Finding patterns in natural behavior is a complex problem, and several different approaches may yield meaningful descriptions of video data. Broadly, the analysis of video data can be either pixel-based<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup> or keypoint based<sup><xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c42">42</xref></sup>. We chose to use a keypoint-based method because of the large variety of cage decorations, objects, stimuli, and other animals present in the study. Pixel-based methods may be more appropriate when the conditions are more constant across video frames, such as in the quantification of facial expression in head-fixed mice<sup><xref ref-type="bibr" rid="c43">43</xref></sup>. Another key decision is whether to use supervised or unsupervised learning approaches. Supervised learning can be beneficial in cases where a small number of human-recognizable behavioral modules are the subject of the investigation, such as in studies of aggressive responses<sup><xref ref-type="bibr" rid="c44">44</xref></sup> or escape responses<sup><xref ref-type="bibr" rid="c45">45</xref></sup> in mice. Unsupervised learning can be more difficult to implement because the patterns identified by the model may not have much biological relevance. For instance, models using raw x/y/z coordinates rather than egocentric coordinates may label different positions in the cage as key behavioral states, rather than different motions. In cases where input data contains missing values or large jumps, the model may identify these features as the main sources of variance. We chose to use an unsupervised learning approach because our goal was to describe the entire distribution of the animals’ daily behaviors in a relatively unbiased manner rather than selecting human-identifiable behaviors manually and quantifying the frequency of those behaviors.</p>
<p>Aside from questions regarding methodology, the study of natural behavior is also complicated by the lack of a specific definition of what constitutes a behavioral module. Importantly, behaviors can occur on different timescales, and it is not always clear (a priori) which timescale will be relevant for each study. For instance, some neuropsychiatric disorder model animals may have motor skill deficits requiring a detailed kinematic analysis of sub-second behaviors<sup><xref ref-type="bibr" rid="c46">46</xref></sup> and others may have deficits in complex social interactions that occur across several seconds or minutes. Many studies have focused on sub-second units of behavior because they are the shortest segments that can be clearly differentiated into units, often referred to as syllables<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>. Another complication is that behavior can be described at many levels of specificity. At higher levels of specificity, more data is needed to find patterns. But at lower levels of specificity, patterns may not be apparent due to mixed effects within a single category. We chose a method where each frame would have a single label, while preserving hierarchical information. This allowed us to identify patterns at multiple levels of specificity.</p>
</sec>
<sec id="s3b">
<title>Large-scale data collection and reproducibility</title>
<p>Logistical constraints and ethical considerations limit the ability to collect large behavioral datasets from nonhuman primates, as is common in several other model organisms. NHP research is comparatively much more expensive, due to the larger living spaces required, longer generation times, and more complex veterinary care needed to maintain high-quality living conditions. We opportunistically collected data from a large colony of marmosets that was primarily used for transgenic model production. This open-source dataset will be useful for future studies that can use this data as a large normative control group to achieve strong statistical power even in cases where the number of animals in experimental groups is lower. Additionally, sharing this large dataset will allow computational neuroscientists to develop new methods for analysis without conducting their own large-scale data collection efforts. This is crucial because it will allow their models to be trained on data from a variety of animals and conditions, rather than on example datasets comprised of a small number of videos.</p>
<p>When comparing natural behavioral data across facilities, challenges can arise due to environmental variables such as the behavioral distribution of family members, number of vocalizations in the room, room configuration, and stimuli present. Although these conditions may change the exact distribution of behavioral state usage, we expect that the general principles for marmoset natural behavioral analysis identified in this study should apply across facilities.</p>
</sec>
<sec id="s3c">
<title>High dimensional phenotyping for neuropsychiatric disorders</title>
<p>Neuropsychiatric disorders can have a wide range of symptoms, and disease model animals will need to be tested in a variety of contexts to study their natural behavior, responses to stimuli, cognitive ability, and performance in other assays. People with autism can have a variety of symptoms, such as differences in social affect, vocal communication, cognitive ability, responses to novel or high-intensity sensory stimuli, restricted interests, or repetitive behavior. Some of these symptoms may only manifest in certain conditions, such as in novel environments or when meeting new people. Some symptoms may also be secondary effects, such as differences in organ health<sup><xref ref-type="bibr" rid="c47">47</xref></sup>. To test which phenotypes are present in disease model animals, high throughput and high dimensional methods will be essential. This is particularly important for testing whether treatments can rescue multiple deficits or only a subset of the deficits.</p>
</sec>
<sec id="s3d">
<title>Outlier detection for individual phenotyping</title>
<p>Several physical traits are routinely measured in developing human infants to ensure that they are within normal ranges. For instance, height and weight charts are used to determine how each infant compares to the overall population. Identifying outliers using these measurements is important, because they may need additional care. However, assessing normality in social and cognitive domains is more difficult because of the complexity of these behaviors. Model animals can serve a crucial purpose for preclinical screening in these areas, because measurements of complex behaviors can be collected more reproducibly in well-controlled lab environments.</p>
<p>Additionally, even patients with the same monogenic form of autism can exhibit substantial heterogeneity. For instance, one ASD patient could have typical behavioral patterns with respect to sensory stimuli, but atypical behavioral patterns related to social stimuli. Because of this, outlier detection in model animals should be done on an individual basis, to identify whether each individual is an outlier in each condition or assay.</p>
<p>Many studies of disease model mice have focused on group-based differences in specific assays<sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup>. Although bulk differences between heterogeneous classes can be detectible with large sample sizes (as is typical in mouse studies), this is logistically impractical for NHP research.</p>
<p>Rather than producing large cohorts of control animals for each study, a much more efficient use of animals is to collect large datasets of control data that can be used to power future studies with smaller cohorts. This could be done for data related to natural behaviors, responses to stimuli, cognitive tests, and other assays. These datasets could be standardized and shared across institutes, as is becoming more common for mouse research<sup><xref ref-type="bibr" rid="c49">49</xref></sup>. The normative models from these datasets could be used to test whether smaller cohorts of animals (controls, disease models, or treated animals) are outliers relative to the dataset. The open-source data from this study will contribute to these ongoing efforts.</p>
</sec>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Animals</title>
<p>For this study, we opportunistically collected behavioral data from 120 animals that were part of a large pre-existing colony used for transgenic animal production. No animals were obtained or bred specifically for use in this project. Additionally, we used data from 4 animals that had been implanted with silicon probes for electrophysiology. The animals were all group housed in family units consisting of 2 adult animals and juveniles up to the age of 15 months. After 15 months, the juveniles would be paired with other animals (in new cages). The lights in the facility were turned on at 7:00 am and turned off at 7:00 pm. Veterinary checks and other work in the rooms (such as cage changes) were done between 7:00 am and 10:00 am. Each session of data was collected between 10:00 am and 4:00 pm. After 5:00 pm, the animals were not disturbed to ensure that their sleep cycle would not be affected.</p>
<p>All procedures were performed in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and approved by the MIT Committee on Animal Care (IACUC). The MIT Division of Comparative Medicine (DCM) routinely monitored animal welfare and provided them with veterinary care as needed.</p>
</sec>
<sec id="s4b">
<title>Home cage video recording</title>
<p>Cages were scheduled for recording sessions based on the ages of the juvenile animals living in the cage, such that they would be recorded for 3 weeks at a time when the animals were 3 months, 6 months, 9 months, 12 months, and 15 months old. Recording sessions were scheduled for weekdays to avoid variance in environmental conditions between weekdays and weekend days. Once juveniles were 15 months old, they were transferred to new cages to begin their own families with new partners. These measurements began once the animals were 3 months old, because that was when the hair tufts near their ears was long enough to dye (to differentiate between animals) and also because the animals began to behave more independently from their parents (rather than clinging to one of their parents) at this time. One week before each recording session, cages were changed to accommodate a transparent ceiling panel to facilitate top-down video recording. The smooth transparent ceiling panels prevented animals from climbing on the ceiling during the recording sessions, and horizontal dividers were added to further restrict the total cage size during the recordings.</p>
<p>One day before the first recording session, cameras were placed above the cages, the ceiling was replaced with a transparent panel, and animals’ hair was dyed. The hair colors were planned such that each cage would have a red-haired animal, white-haired animal, blue-haired animal, and yellow-haired animal. Red, blue, and yellow were chosen as the dye colors to maximize distinguishability between animals, according to human observers. Human distinguishability is essential to this approach since we used a supervised model to classify the animals based on hair color. To apply hair dye, one experimenter caught and restrained the animal while a second experimenter applied the dye into the hair. Dye was applied liberally to the tufts of hair near the ears so that there was no white hair remaining. Animals were then left in a temporary holding cage for 20 minutes so that the dye could dry before they were transferred back to their home cage. This was done to prevent family members from rubbing the dye onto themselves and to prevent the dye from marking items/surfaces in the home cage. After animals were returned to their cages, we waited at least one day before any recordings were performed to allow the dye to dry completely and allow the other animals to habituate to the dye.</p>
<p>Video recording was done using ZED stereo cameras mounted above the cages and collected using the ZED SDK developed by the manufacturer. Video was collected from 2 cameras at 60 frames per second, and with 720p resolution. Each image had a size of 1280 pixels by 720 pixels. Camera settings (i.e. brightness/contrast/exposure/gain) were manually adjusted after each camera installation to avoid image quality issues caused by oversaturation or insufficient gain.</p>
<p>Because light sources directly above the cages would cause glare in the video, we either moved the cages or moved the light sources to ensure that cages were not directly below light bulbs. After exporting the proprietary “svo” videos generated by the cameras into the “avi” format using the ZED SDK, further processing was done using python and Matlab.</p>
</sec>
<sec id="s4c">
<title>Pose extraction</title>
<p>Postural keypoints were extracted from the raw video data using DeepLabCut (DLC)<sup><xref ref-type="bibr" rid="c14">14</xref></sup> and sorted using a two-step method we developed for this specific use case. First, tracklets were joined based on kinematic features (as in the typical DLC pipeline). Then, tracklets were sorted based on the average distance between each tracklet and the labels of each hair color. To train a network to recognize hair color, we placed one label on the center of the head of each animal, rather than on each hair tuft. This approach leveraged the kinematic data to join nearby points and used the hair color data to ensure that animal identities were not mixed after animals had close interactions or were occluded by other animals. Egocentric postures were calculated by setting the middle point on the body to (0,0) and then rotating all points based on the angle formed by the front point on the body and the back point on the body. Features were then calculated based on both the raw time series data and the egocentric time series data. All code and posture time series data will be made available online alongside the publication of this paper. Raw data will be available upon request, due to logistical issues with publicly hosting the entire unprocessed dataset.</p>
</sec>
<sec id="s4d">
<title>Behavioral state labeling</title>
<p>A recurrent neural network was trained to use 25 seconds of input data (features) to predict the next 5 seconds of data (features). This network had a bottleneck size of 256, and the data from this bottleneck was used for hierarchical t-SNE clustering. As suggested in tree-SNE<sup><xref ref-type="bibr" rid="c34">34</xref></sup>, we systematically varied the alpha (exaggeration) and perplexity (# of neighbors) in the t-SNE clustering algorithm. At high levels of alpha/perplexity, we observed fewer distinct clusters. At lower levels of alpha/perplexity, we observed more distinct clusters. Initial clustering was done using Density-Based Spatial Clustering of Applications with Noise (DBSCAN)<sup><xref ref-type="bibr" rid="c50">50</xref></sup>. Cluster labels were assigned to new data (obtained after initial model training) using openTSNE<sup><xref ref-type="bibr" rid="c51">51</xref></sup> to embed new points into the existing embedding, and then using k-nearest neighbors (KNN) to assign a state label. This was done at the lowest hierarchical level only, because higher levels could be inferred from this information. All code and model output data will be made available online alongside the publication of this paper.</p>
</sec>
<sec id="s4e">
<title>Home cage audio recording</title>
<p>Microphones were placed outside of each cage under study and also outside of each neighboring cage. Small wireless microphones were used so that they could be placed around the room without adding any wires or large equipment that could potentially draw the animals’ attention. Audio data was recorded at 48,000 samples per second (48 kHz) and logged locally on each wireless recording device. The data from each device was synchronized to the video data using a series of 5 claps from a clapper board at the beginning and end of each video. These claps were clearly visible in the video and easily identifiable in the audio data by a human observer. The clap onset time was manually annotated for each video and audio recording.</p>
</sec>
<sec id="s4f">
<title>Vocalization labeling</title>
<p>Audio data was converted into spectrograms using SciPy<sup><xref ref-type="bibr" rid="c50">50</xref></sup> with nperseg = 700, noverlap = 525, and nfft = 2000. These spectrograms were segmented using a supervised neural network trained to perform binary classification where 0 = no call and 1 = call. To determine the average call rate (<xref rid="fig5" ref-type="fig">Fig. 5</xref>), a threshold of 0.5 was applied to the output of the neural network, such that all outputs below 0.5 were set to 0 and all outputs above 0.5 were set to 1. Spectrograms were segmented into discrete calls using this binary classification. Next, the discrete calls were classified by a separate neural network trained to perform classification based on human categorization of calls in the training set into 8 classes. The types of calls labeled by this model were limited to the types of calls that were commonly observed in our training data. The audio data will be made available online alongside the publication of this paper.</p>
</sec>
<sec id="s4g">
<title>Neural data acquisition</title>
<p>Animals were implanted with silicon probes mounted on drives that could be lowered into the striatum. Data was collected using on-head amplifiers and on-head data loggers. Two silicon probes with 64 channels each were implanted into each of the four animals according to the protocol published by Cambridge Neurotech<sup><xref ref-type="bibr" rid="c52">52</xref></sup> and lowered between sessions using drives made by Cambridge Neurotech. The full description of this surgical procedure will be published in a separate paper. Data was logged on-head during each session and the logger was removed so that it could be charged, and the data could be downloaded between sessions. This reduced the total weight on the head of the animal during the non-recording time. During non-recording time, a lightweight black cover was attached in place of the data logger. Data was acquired at a rate of 25,000 samples per second (25 kHz) using data loggers made by White Matter. Spike sorting was performed using Kilosort<sup><xref ref-type="bibr" rid="c53">53</xref></sup> version 3 followed by further manual curation. Units with missing data (either due to drift or due to lapses in the connection between the amplifiers and the logger) were not included for further analysis.</p>
</sec>
<sec id="s4h">
<title>Neural data analysis</title>
<p>Neural data was aligned to video data using a flashing light on the top of the data logger that could be seen by the camera at the beginning and end of each session. Each data format (video, audio, and neural) had a different sampling rate. Video was sampled at 60 Hz, audio was sampled at 48,000 Hz, and neural data was sampled at 25,000 Hz. We converted all data into bins of 60 samples per second to match the rate of the video recording, which was the slowest of the three data modalities. We compared the observed distribution of spikes across the behavioral states to a simulated distribution based on the frequency of behavioral events in each session. We clustered neurons based on these normalized spike distributions across the behavioral latent space, and manually inspected the 4 highest-level clusters using post-hoc statistical tests to determine whether the neurons in each cluster had increased activity during each of the highest-level behavioral states.</p>
</sec>
<sec id="s4i">
<title>Stimulus presentation</title>
<p>Stimuli were presented over a course of 3 weeks, and the same series of stimuli were presented to each cage every 3 months. The first 5 sessions were “control” sessions. In this condition, a horizontal divider was placed in the middle of the cage to restrict the total cage size and simplify analysis. Ledges, branches, and hammocks were removed so that animals would not be occluded from view. Nest boxes and other cage attachments were also removed, so that animals could not leave the field of view. A bed was placed in the rear left corner of the cage so that animals had a comfortable place to sit when resting. Food was placed into two bowls that fit into slots designed to hold them so that they remained in place. Water was freely available through spouts on the side of the cages. At the beginning of each session, cameras were turned on and audio/neural data was synchronized to the cameras as described above (using a clapper board to synchronize the audio data to the video and using the lights on top of the data logger to synchronize the neural data to the video). After the completion of each session, cages were restored to their normal state.</p>
<p>After the 5 days of these control sessions, stimuli were presented once per day. During these sessions, the cages were set up as described above and then one of the 8 types of stimuli was added. Including the control sessions, this means that there were 9 total stimulus-response conditions (one control condition, and 8 stimulus conditions). Because the control condition was performed 5 times per set of recordings, and the toy conditions was performed 3 times per set of recordings, there was not an even number of each condition in the total dataset. The conditions are described in the following section.
<list list-type="order">
<list-item><p><italic>Control</italic>. The only items present in the cage were a bed that animals could rest in, two bowls of food placed in slots designed to hold the bowls, and water-dispensers. These items were also present in the other stimulus conditions.</p></list-item>
<list-item><p><italic>Novel object.</italic> A large and brightly colored plastic dog toy was placed in the cage. Importantly, this type of object is not typically placed in the cage. Marmosets cautiously approach these objects but have low overall interest in them.</p></list-item>
<list-item><p><italic>Treat</italic>. A mixture of foraging material and foods high in sugar were placed in a bowl. Foods such as banana chips, marshmallows, pudding, craisins, and dried mango were mixed into the foraging materials. Importantly, these foods were not regularly given as part of the standard diet. Marmosets quickly approach and consume these highly appetitive items.</p></list-item>
<list-item><p><italic>Noise.</italic> A movie played by a computer outside of the cage. Marmosets do not have visual access to the movie, and only hear the dialogue and soundtrack. This “background noise” stimulus is not meant to have a large impact on their behavior.</p></list-item>
<list-item><p><italic>Mask.</italic> A gorilla mask was placed outside the cage in the same position as the computer playing the “background noise” stimulus. The mask stimulus is similar to “human intruder” experiments. Marmosets have a very large negative response to this stimulus, often climbing to the furthest corner of the cage in a group, and sometimes making angry calls.</p></list-item>
<list-item><p><italic>Toys.</italic> A set of colorful wicker balls that animals frequently chew on and interact with. The toys are presented for 3 consecutive days. The wicker balls are attached to the side of the cage using zip ties so that they cannot be moved to different positions in the cage, which could complicate the interpretation of the data.</p></list-item>
<list-item><p><italic>Intruder animal.</italic> A novel animal from a different family is placed in a transfer box inside the cage. Animals were allowed to have auditory/visual access to the animal, but not able to physically interact (they were separated by a transparent plastic panel). This was done to prevent injury of either the family or the intruder animals. This is a highly salient stimulus and causes a large increase in activity.</p></list-item>
<list-item><p><italic>Tablet 1.</italic> Animals were presented with a neutral stimulus (blue square) in the center of the screen. They could touch the screen to receive a syrup reward that was delivered from a metal spout attached to the bottom of the tablet. Further cognitive tasks used the same apparatus, but had increased task demands (forced choice, following a moving target, remembering past trials, or foraging) and took place individually rather than in the group setting. This cognitive data will be fully explored in an accompanying study.</p></list-item>
<list-item><p><italic>Tablet 2.</italic> Animals were presented with a social stimulus (marmoset face) in the center of the screen. They could touch the screen to receive a syrup reward that was delivered from a metal spout attached to the bottom of the tablet.</p></list-item>
</list>
</p>
</sec>
<sec id="s4j">
<title>2-class classification</title>
<p>For 2-class classification, we used a support vector machine (SVM) approach to distinguish the behavioral responses to different stimuli. First, we normalized features using z-scoring (so that each feature would have a mean of 0 and a standard deviation of 1). Then, we trained 2-class SVM models to discriminate between each pair of stimulus-response conditions using these normalized features (derived from both behavioral state usage and behavioral state correlations). We evaluated these models by calculating scores for held-out data from each class.</p>
<p>Several other model types could be used for 2-class classification such as Gaussian Naïve Bayes (GNB) or Linear discriminant analysis (LDA). Because we obtained high discriminability between conditions using SVM models even when using a small number of animals as training data, we did not seek to further optimize our 2-class classification approach.</p>
</sec>
<sec id="s4k">
<title>Outlier detection</title>
<p>For 1-class outlier detection, we first generated a probability density estimate (PDE) for each input feature based on the distribution of that feature in the training data. This PDE for behavioral state usage features was defined in the range of (0,1) and the PDE for behavioral state correlation features was defined in the range of (−1,1). Both types of distributions were made with a kernel bandwidth of 0.2. Outlier scores were calculated by evaluating the PDE at a single point to obtain a density value. Importantly, this is a relative measurement (i.e. if x<sub>1</sub> has a higher density value than x<sub>2</sub>, then x<sub>1</sub> is relatively more likely to have been drawn from the distribution than x<sub>2</sub>). As such, these measurements are not probability measurements because we evaluated the PDE at a fixed point rather than finding the probability that the data would fall within a certain interval. We calculated this relative density measurement for each input feature (including behavioral state usage features and behavioral state correlation features), and then averaged across these values to generate a single outlier score.</p>
<p>To estimate the number of animals needed, we randomly sampled a set number of animals (i.e. 4, 8, 16, 32) from the training set and compared the scores of held-out animals from the same condition to scores obtained from other conditions. As the number of animals in the training set increases, the resulting PDE should more accurately estimate the true distribution, and outliers should be more discriminable from inliers. In line with this reasoning, we observed that when the number of observations (animals) used as training data was low, all data used in the test set had a high outlier score – regardless of whether it came from the same condition or different conditions. In these cases (with few observations in the training set) each new observation would appear to be an outlier. By contrast, when we increased the number of observations (animals) used as training data, the outlier scores could accurately discriminate between conditions (as measured by auROC). For auROC analysis, we calculated the area under the ROC curve, such that a score of 0.5 would indicate a model with no discriminative ability and a score of 1.0 would indicate perfect discrimination between the conditions.</p>
</sec>
<sec id="s4l">
<title>Creating artificial heterogeneity to test 2-class classification and outlier detection</title>
<p>To determine whether each model was robust to outliers, we artificially added “outlier” data to the training set by mixing data from different stimulus-response conditions. To do this, we pooled data from all other stimulus-response conditions and randomly selected a number of datasets (i.e. 0, 2, 4, 8, 16) to add into one of the conditions. We then calculated either 2-class classification scores or outlier scores for these artificial outliers as described above. The “change in score” metric indicates the difference between the average score given to outliers when the number of outliers added was 0 and when the number of outliers added was 2, 4, 8, or 16.</p>
</sec>
<sec id="s4m">
<title>Data and code availability</title>
<p>Data and code associated with this paper will be available online upon publication. Processed data types such as postural time series data, behavioral state data, and vocalization time series data will be publicly hosted. Large data types such as raw multi-view video data will be available upon request, due to logistical issues with publicly hosting the entire unprocessed dataset.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Additional information</title>
<ack>
<title>Acknowledgements</title>
<p>We thank Alex Mathis and Mackenzie Mathis for the helpful discussions and access to beta versions of their software. We thank MIT DCM for support with animal care. This work was funded by the Yang-Tan Center for Molecular Therapeutics in Neuroscience and the Tan-Yang Center for Autism Research of the Yang Tan Collective at MIT, the Stanley Center for Psychiatric Research at Broad Institute of MIT and Harvard, the Poitras Center for Psychiatric Disorders Research at MIT, and the Simons Center for the Social Brain at MIT.</p>
</ack>
<sec id="s5a">
<title>Author contributions</title>
<p>W.M. and E.C. designed the study and equipment. W.M., E.C., and K.B. collected the behavioral datasets. W.M., S.P., and H.X. collected the neural dataset. W.M. performed the analysis and wrote the manuscript. All authors reviewed the manuscript.</p>
</sec>
<sec id="s6">
<title>Declaration of interests</title>
<p>The authors declare no competing interests.</p>
</sec>
</sec>
<sec id="s7">
<title>Supplementary figures</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1:</label>
<caption><title>PCA in raw data compared to egocentric data</title>
<p><bold>a)</bold> Example timeseries of raw postural time series data (21 keypoints) plotted across 6 minutes. <bold>b)</bold> Example time series of egocentric data (normalized by position and body angle) plotted across 6 minutes. <bold>c)</bold> For the first four raw data principal components (PCs), the distribution within the image (left), example regression with features (middle), and average posture at different scores (right). <bold>d)</bold> For the first four egocentric data principal compo­ nents (PCs), the distribution within the image (left), example egocentric body part coordinate colored based on score (middle), and average posture at different scores (right).</p></caption>
<graphic xlink:href="610159v2_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2:</label>
<caption><title>Depth estimates for top-down home-cage video</title>
<p><bold>a)</bold> Comparison of methods for depth estimation using single view, stereo vision, or stereo vision with mesh grid extraction. <bold>b)</bold> Example single-view image, and comparison of score (pixel-based) between human-annotated frames labeled with binary O (not climbing) or 1 (climbing) labels. <bold>c)</bold> Example depth time series based on single-view (30 minute trace). <bold>d)</bold> Example stereo vision image, and comparison of score (depth-based) between human-annotated frames labeled with binary O (not climbing) or 1 (climbing) labels. <bold>e)</bold> Example time series (depth estimate) based on stereo vision. <bold>f)</bold> Example mesh grid based on stereo depth estimates, and compari­ son of score (depth-based) between human-annotated frames labeled with binary O (not climbing) or 1 (climbing) labels. <bold>g)</bold> Example mesh grid time series, with depths sorted at each time point. <bold>h)</bold> Example plots of uncorrected position (left) and depth-corrected position (right), with color corresponding to distance from the camera.</p></caption>
<graphic xlink:href="610159v2_figs2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3:</label>
<caption><title>Hierarchical behavioral state representation</title>
<p><bold>a)</bold> Latent space clustered into 4 components with a median duration of 4.0 seconds. <bold>b)</bold> Latent space clustered into 10 components with a median duration of 3.0 seconds. c) Latent space clustered into 18 components with a median duration of 1.5 seconds. <bold>d)</bold> Latent space clustered into 30 components with a median duration of 1.0 seconds. <bold>e)</bold> Latent space clustered into 80 components with a median duration of 0.5 seconds.</p></caption>
<graphic xlink:href="610159v2_figs3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4:</label>
<caption><p>Mapping human description and features to clusters in latent space</p></caption>
<graphic xlink:href="610159v2_figs4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5:</label>
<caption><title>Attention distribution varies depending on previous states</title>
<p><bold>a)</bold> Schematic of recurrent neural network model. <bold>b)</bold> Attention distribution concentrated on recent time points when previous states have high variability (top) or attention distribution distributed widely when previous states have low variability (bottom). <bold>c)</bold> Top: Example time series of the fraction of attention allocated to previous states (&gt;1 second prior) over the course of one minute of video. <bold>d)</bold> Average attention allocation across behavioral states where each row represents one behavioral state and color corresponds to average attention. <bold>e)</bold> Attention to previous states plotted as a function of the consistency of previous states. Consistency is defined where 1.0 is the case where all states in the last 20 seconds are identical. <bold>f)</bold> Attention to previous states plotted for discrete ranges of past state consistency. Data is divided into ten groups, starting with the range (0 to 0.1) and ending with the range (0.9 to 1.0).</p></caption>
<graphic xlink:href="610159v2_figs5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S6:</label>
<caption><title>Measuring correlation between cagemates across timescales</title>
<p><bold>a)</bold> All sessions plotted by PC1 and PC2. Colors correspond to the frequency of “Climbing”, “Active”, “Social”, and “Alone” behavioral states. <bold>b)</bold> Correlation measurements with different bin sizes ranging from 1 second to four hours. Gray circles represent correlations obtained using shuffled data (with the same time bin sizes). <bold>c)</bold> Distri­ bution of behavioral state usage, and PCA across sessions. <bold>d)</bold> Time spent active relative to cagemates, with animals significantly less active than cagemates (p&lt;0.001) shown in red and animals significantly more active than cagemates (p&lt;0.001) shown in blue. Paired t-tests were used to compare simultaneously collected activity between animals’ and their cagemates. Cagemate data was averaged to generate a single measurement for each session.</p></caption>
<graphic xlink:href="610159v2_figs6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Figure S7:</label>
<caption><title>Sleep cycle monitoring</title>
<p><bold>a)</bold> Activity monitoring using a motion watch. Continuous data from one week of monitoring is plotted for each animal with n = 44 animals. Blue bars indicate nighttime (lights off) and yellow bars indicate daytime (lights on). Darker areas in the plot indicate higher activity. In cases where animals removed or disabled motion watches prior to the end of the experiment, remaining values were replaced by NaN. <bold>b)</bold> Average activity trace across 44 animals. <bold>c)</bold> Average 24 hour cycle (based on data from 1 week) plotted for each animal. <bold>d)</bold> Raw motion watch data (a.u. = arbitrary units) separated into 4 time windows of 6 hours. <bold>e)</bold> Filtered motion watch data representing an estimate of time spent at rest(%) separated into 4 time windows of 6 hours.</p></caption>
<graphic xlink:href="610159v2_figs7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Figure S8:</label>
<caption><title>Call localization using array of microphones</title>
<p><bold>a)</bold> Example time series (35 seconds) of raw data from 4 microphones placed near different cages. Dotted lines indicate calls labeled by neural network. <bold>b)</bold> Amplitudes (running average of the absolute value of the spectro­ gram) used to assign each call to the microphone with the largest amplitude recording. <bold>c)</bold> Spectrograms depict­ ing the distribution of signal across frequencies. <bold>d)</bold> Four individual call spectrograms, where the brightness indicates the amplitude measurement of the call in each microphone, relative to the amplitude measurement from the other microphones.</p></caption>
<graphic xlink:href="610159v2_figs8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Figure S9:</label>
<caption><p>Supervised categorical audio classification</p></caption>
<graphic xlink:href="610159v2_figs9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs10" position="float" orientation="portrait" fig-type="figure">
<label>Figure S10:</label>
<caption><title>Comparison of 2-animal cage and 4-animal cage correlation patterns</title>
<p><bold>a)</bold> Correlation between behavioral state usage and call rate from cages with 2 adult animals (filled circles indicate positive correlation, open circles indicate negative correlation). <bold>b)</bold> Data from n = 52 animals, with bars indicating quartile boundaries (25%, 50%, and 75%) and circles indicating median across animals. <bold>c)</bold> Correlation between behavioral state usage and call rate from cages with 4 animals (filled circles indicate positive correla­ tion, open circles indicate negative correlation). <bold>d)</bold> Data from n = 43 animals, with bars indicating quartile bound­ aries (25%, 50%, and 75%) and circles indicating median across animals. <bold>e)</bold> Correlation between data from cages with 2 animals and data from cages with 4 animals. <bold>f)</bold> Correlation from three example animals that were recorded in both conditions (both in the 2-animal condition and the 4-animal condition).</p></caption>
<graphic xlink:href="610159v2_figs10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs11" position="float" orientation="portrait" fig-type="figure">
<label>Figure S11:</label>
<caption><title>Behavioral responses to a panel of stimuli</title>
<p><bold>a)</bold> Recording timeline, which was repeated every 3 months. <bold>b)</bold> Example correlations between behavioral state usage and time spent near tablet (using 1-minute bins). <bold>c)</bold> Difference in behavioral state usage, comparing periods of “high” interaction (&gt;90% time spent near stimulus per minute) and periods of “low” interaction (&lt;10% time spent near stimulus per minute). <bold>d)</bold> Schematic of stimulus delivery. <bold>e)</bold> Distribution of time spent near tablet across 91 non-age-matched animals (quantified based on% time spent near stimulus per session). <bold>f)</bold> Distribu­ tion of time spent near tablet quantified based on % time spent near stimulus per minute. <bold>g)</bold> Comparison of cage location during times of “high” interaction with tablet (&gt;90% per minute) and times of “low” interaction with tablet (&lt;10% per minute). <bold>h)</bold> Difference in behavioral state usage between times of “high” interaction with tablet (&gt;90% per minute) and “low” interaction with tablet (&lt;10% per minute).</p></caption>
<graphic xlink:href="610159v2_figs11.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs12" position="float" orientation="portrait" fig-type="figure">
<label>Figure S12:</label>
<caption><title>Comparing frequency-based metrics and correlation-based metrics</title>
<p><bold>a)</bold> Change in behavioral state usage across hierarchical levels (purple indicates an increase in state usage and grey indicates a decrease in state usage). Comparisons are relative to control (bed-only condition with no other stimuli). <bold>b)</bold> Change in behavioral state correlations across hierarchical levels (blue indicates an increase in correlation and red indicates a decrease in correlation). Comparisons are relative to control (bed-only condition with no other stimuli). <bold>c)</bold> Example comparing the usage of two behavioral states when animals were presented with toy (green) or touchscreen (purple).* indicates p <sub>&lt;</sub> 10·<sup>6</sup>, ** indicates p <sub>&lt;</sub> 10-<sup>12</sup>, and*** indicates p <sub>&lt;</sub> 10-<sup>20</sup> using paired t-tests to determine whether the same animals used each behavioral state more or less in each condition. <bold>d)</bold> Example comparing the correlation between two behavioral states with the toy (green) or touch­ screen (purple) stimuli.* indicates p <sub>&lt;</sub> 10·<sup>6</sup>, ** indicates p <sub>&lt;</sub> 10-<sup>12</sup>, and*** indicates p <sub>&lt;</sub> 10·<sup>20</sup>using paired t-tests to determine whether the same animals had higher or lower correlation measurements for each behavioral state in each condition.</p></caption>
<graphic xlink:href="610159v2_figs12.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs13" position="float" orientation="portrait" fig-type="figure">
<label>Figure S13:</label>
<caption><title>Outlier detection using data from non-age-matched animals</title>
<p><bold>a)</bold> auROC scores from 2-class classification across an increasing number of animals (non-age-matched) used in training data. Score indicates the discriminability between each pair of stimulus-response conditions. <bold>b)</bold> auROC scores from 1-class outlier detection across an increasing number of animals (non-age-matched) used in training data. Score indicates the discriminability between each pair of stimulus-response conditions.</p></caption>
<graphic xlink:href="610159v2_figs13.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs14" position="float" orientation="portrait" fig-type="figure">
<label>Figure S14:</label>
<caption><title>Differences in behavioral distribution between juvenile and adult animals</title>
<p><bold>a)</bold> Comparison of the fraction of time spent upright/climbing per session between juvenile and adult animals (left), and a paired comparison across cagemates of different ages (right). <bold>b)</bold> Comparison of the fraction of time spent active/running per session between juvenile and adult animals (left), and a paired comparison across cagemates of different ages (right). <bold>c)</bold> Comparison of the fraction of time spent social/near other per session between juvenile and adult animals (left), and a paired comparison across cagemates of different ages (right). <bold>d)</bold> Comparison of the fraction of time spent alone per session between juvenile and adult animals (left), and a paired comparison across cagemates of different ages (right). <bold>e)</bold> Normalized difference in usage of behavioral states at level 5. <bold>f)</bold> Example graphical representation. <bold>g)</bold> Normalized difference in usage of behavioral states across hierarchal levels. <bold>h)</bold> Example graphical representation. Statistical tests were unpaired (left) or paired using data simultaneously collected from cagemates (right).</p></caption>
<graphic xlink:href="610159v2_figs14.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barron</surname>, <given-names>H. C.</given-names></string-name>, <string-name><surname>Mars</surname>, <given-names>R. B.</given-names></string-name>, <string-name><surname>Dupret</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lerch</surname>, <given-names>J. P.</given-names></string-name> &amp; <string-name><surname>Sampaio-Baptista</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Cross-species neuroscience: closing the explanatory gap</article-title>. <source>Philosophical Transactions of the Royal Society B</source> <volume>376</volume>, (<year>2021</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname>, <given-names>C. T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Marmosets: A Neuroscientific Model of Human Social Behavior</article-title>. <source>Neuron</source> <volume>90</volume>, <fpage>219</fpage>– <lpage>233</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sasaki</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Generation of transgenic non-human primates with germline transmission</article-title>. <source>Nature 2009</source> <volume>459</volume>:<issue>7246</issue> 459, <fpage>523</fpage>–<lpage>527</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bendor</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Wang</surname>, <given-names>X</given-names></string-name></person-group>. <article-title>The neuronal representation of pitch in primate auditory cortex</article-title>. <source>Nature 2005</source> <volume>436</volume>:<issue>7054</issue> 436, <fpage>1161</fpage>–<lpage>1165</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Okano</surname>, <given-names>H</given-names></string-name></person-group>. <article-title>Current Status of and Perspectives on the Application of Marmosets in Neurobiology</article-title>. <source>Annu Rev Neurosci</source> <volume>44</volume>, <fpage>27</fpage>–<lpage>48</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname>, <given-names>C. T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Marmosets: A Neuroscientific Model of Human Social Behavior</article-title>. <source>Neuron</source> <volume>90</volume>, <fpage>219</fpage>– <lpage>233</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Beck</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Meade</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Wang</surname>, <given-names>X</given-names></string-name></person-group>. <article-title>Antiphonal call timing in marmosets is behaviorally significant: interactive playback experiments</article-title>. <source>J Comp Physiol A Neuroethol Sens Neural Behav Physiol</source> <volume>195</volume>, <fpage>783</fpage>–<lpage>789</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Aoi</surname>, <given-names>M. C.</given-names></string-name> &amp; <string-name><surname>Miller</surname>, <given-names>C. T.</given-names></string-name></person-group> <article-title>Representing the dynamics of natural marmoset vocal behaviors in frontal cortex</article-title>. <source>bioRxiv</source> (<year>2024</year>) doi:<pub-id pub-id-type="doi">10.1101/2024.03.17.585423</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hung</surname>, <given-names>C.-C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Functional mapping of face-selective regions in the extrastriate visual cortex of the marmoset</article-title>. <source>J Neurosci</source> <volume>35</volume>, <fpage>1160</fpage>–<lpage>1172</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mitchell</surname>, <given-names>J. F.</given-names></string-name> &amp; <string-name><surname>Leopold</surname>, <given-names>D. A</given-names></string-name></person-group>. <article-title>The marmoset monkey as a model for visual neuroscience</article-title>. <source>Neurosci Res</source> <volume>93</volume>, <fpage>20</fpage>–<lpage>46</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takahashi</surname>, <given-names>D. Y.</given-names></string-name>, <string-name><surname>Liao</surname>, <given-names>D. A.</given-names></string-name> &amp; <string-name><surname>Ghazanfar</surname>, <given-names>A. A</given-names></string-name></person-group>. <article-title>Vocal Learning via Social Reinforcement by Infant Marmoset Monkeys</article-title>. <source>Curr Biol</source> <volume>27</volume>, <fpage>1844</fpage>–<lpage>1852.e6</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Meisner</surname>, <given-names>O. C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Development of a Marmoset Apparatus for Automated Pulling (MarmoAAP) to Study Cooperative Behaviors</article-title>. <source>bioRxiv</source> (<year>2024</year>) doi:<pub-id pub-id-type="doi">10.1101/2024.02.16.579531</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tyree</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Metke</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Miller</surname>, <given-names>C. T</given-names></string-name></person-group>. <article-title>Cross-Modal Representation of Identity in Primate Hippocampus</article-title>. <source>Science</source> <volume>382</volume>, <issue>417</issue> (<year>2023</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lauer</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Multi-animal pose estimation, identification and tracking with DeepLabCut</article-title>. <source>Nat Methods</source> <volume>19</volume>, <fpage>496</fpage>–<lpage>504</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title>. <source>Nat Methods</source> <volume>19</volume>, <fpage>486</fpage>–<lpage>495</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dunn</surname>, <given-names>T. W.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Geometric deep learning enables 3D kinematic profiling across species and environments</article-title>. <source>Nat Methods</source> <volume>18</volume>, <issue>564</issue> (<year>2021</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kabra</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Robie</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Rivera-Alba</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Branson</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>JAABA: interactive machine learning for automatic annotation of animal behavior</article-title>. <source>Nat Methods</source> <volume>10</volume>, <fpage>64</fpage>–<lpage>67</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodwin</surname>, <given-names>N. L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Simple Behavioral Analysis (SimBA) as a platform for explainable machine learning in behavioral neuroscience</article-title>. <source>Nat Neurosci</source> (<year>2024</year>) doi:<pub-id pub-id-type="doi">10.1038/S41593-024-01649-9</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>J. B. G. M. C. D., William, B. &amp; W., S.J</collab></person-group>. <article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title>. <source>J R Soc Interface</source> <volume>11</volume>, <issue>20140672</issue> (<year>2014</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wiltschko</surname>, <given-names>A. B.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Mapping Sub-Second Structure in Mouse Behavior</article-title>. <source>Neuron</source> (<year>2015</year>) doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hsu</surname>, <given-names>A. I.</given-names></string-name> &amp; <string-name><surname>Yttri</surname>, <given-names>E. A</given-names></string-name></person-group>. <article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title>. <source>Nat Commun</source> <volume>12</volume>, (<year>2021</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>de Chaumont</surname>, <given-names>F.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Live Mouse Tracker: real-time behavioral analysis of groups of mice</article-title>. <source>bioRxiv</source> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.1101/345132</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klibaite</surname>, <given-names>U.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Deep phenotyping reveals movement phenotypes in mouse neurodevelopmental models</article-title>. <source>Mol Autism</source> <volume>13</volume>, <fpage>1</fpage>–<lpage>18</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Coffey</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Marx</surname>, <given-names>R. G.</given-names></string-name> &amp; <string-name><surname>Neumaier</surname>, <given-names>J. F</given-names></string-name></person-group>. <article-title>DeepSqueak: a deep learning-based system for detection and analysis of ultrasonic vocalizations</article-title>. <source>Neuropsychopharmacology</source> <volume>44</volume>, <fpage>859</fpage>–<lpage>868</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fonseca</surname>, <given-names>A. H. O.</given-names></string-name>, <string-name><surname>Santana</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Bosque Ortiz</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Bampi</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Dietrich</surname>, <given-names>M. O</given-names></string-name></person-group>. <article-title>Analysis of ultrasonic vocalizations from mice using computer vision and machine learning</article-title>. <source>Elife</source> <volume>10</volume>, (<year>2021</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grijseels</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Fairbank</surname>, <given-names>D. A.</given-names></string-name> &amp; <string-name><surname>Miller</surname>, <given-names>C. T</given-names></string-name></person-group>. <article-title>A model of marmoset monkey vocal turn-taking</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source> <volume>291</volume>, <issue>20240150</issue> (<year>2024</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buch</surname>, <given-names>A. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Molecular and network-level mechanisms explaining individual differences in autism spectrum disorder</article-title>. <source>Nat Neurosci</source> <volume>26</volume>, <fpage>650</fpage>–<lpage>663</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frank</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>The Neurological Manifestations of Phelan-McDermid Syndrome</article-title>. <source>Pediatr Neurol</source> <volume>122</volume>, <fpage>59</fpage>–<lpage>64</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schön</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Definition and clinical variability of SHANK3-related Phelan-McDermid syndrome</article-title>. <source>Eur J Med Genet</source> <volume>66</volume>, <issue>104754</issue> (<year>2023</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Phelan</surname>, <given-names>K.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Phelan-McDermid syndrome: a classification system after 30 years of experience</article-title>. <source>Orphanet J Rare Dis</source> <volume>17</volume>, (<year>2022</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peça</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Shank3 mutant mice display autistic-like behaviours and striatal dysfunction</article-title>. <source>Nature</source> <volume>472</volume>, <fpage>437</fpage>–<lpage>442</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Del Rosario</surname>, <given-names>R. C. H.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Sibling chimerism among microglia in marmosets</article-title>. <source>bioRxiv</source> (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.10.16.562516</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gosztolai</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>LiftPose3D, a deep learning-based approach for transforming two-dimensional to three-dimensional poses in laboratory animals</article-title>. <source>Nat Methods</source> <volume>18</volume>, <fpage>975</fpage>–<lpage>981</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Robinson</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Pierce-Hoffman</surname>, <given-names>E.</given-names></string-name></person-group> <article-title>Tree-SNE: Hierarchical Clustering and Visualization Using t-SNE</article-title>. <source>arXiv</source> (<year>2020</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markowitz</surname>, <given-names>J. E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The Striatum Organizes 3D Behavior via Moment-to-Moment Action Selection</article-title>. <source>Cell</source> <volume>174</volume>, <fpage>44</fpage>–<lpage>58.e17</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baéz-Mendoza</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Schultz</surname>, <given-names>W</given-names></string-name></person-group>. <article-title>Performance error-related activity in monkey striatum during social interactions</article-title>. <source>Sci Rep</source> <volume>6</volume>, (<year>2016</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klein</surname>, <given-names>J. T.</given-names></string-name> &amp; <string-name><surname>Platt</surname>, <given-names>M. L</given-names></string-name></person-group>. <article-title>Social information signaling by neurons in primate striatum</article-title>. <source>Curr Biol</source> <volume>23</volume>, <fpage>691</fpage>–<lpage>696</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name></person-group> <article-title>High-dimensional geometry of population responses in visual cortex</article-title>. <source>bioRxiv</source> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.1101/374090</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>D. J.</given-names></string-name> &amp; <string-name><surname>Perona</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Toward a Science of Computational Ethology</article-title>. <source>Neuron</source> <volume>84</volume>, <fpage>18</fpage>–<lpage>31</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mobbs</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Promises and challenges of human computational ethology</article-title>. <source>Neuron</source> <volume>109</volume>, <fpage>2224</fpage>– <lpage>2238</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Weinreb</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics</article-title>. <source>bioRxiv</source> (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.03.16.532307</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneider</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>J. H.</given-names></string-name> &amp; <string-name><surname>Mathis</surname>, <given-names>M. W</given-names></string-name></person-group>. <article-title>Learnable latent embeddings for joint behavioural and neural analysis</article-title>. <source>Nature</source> <volume>617</volume>, <fpage>360</fpage>–<lpage>368</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dolensek</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Gehrlach</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Klein</surname>, <given-names>A. S.</given-names></string-name> &amp; <string-name><surname>Gogolla</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>Facial expressions of emotion states and their neuronal correlates in mice</article-title>. <source>Science (1979)</source> <volume>368</volume>, (<year>2020</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nair</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>An approximate line attractor in the hypothalamus encodes an aggressive state</article-title>. <source>Cell</source> <volume>186</volume>, <issue>178</issue> (<year>2023</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yilmaz</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Meister</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Rapid Innate Defensive Responses of Mice to Looming Visual Stimuli</article-title>. <source>Current Biology</source> <volume>23</volume>, <fpage>2011</fpage>–<lpage>2015</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Preisig</surname>, <given-names>D. F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>High-speed video gait analysis reveals early and characteristic locomotor phenotypes in mouse models of neurodegenerative movement disorders</article-title>. <source>Behavioural Brain Research</source> <volume>311</volume>, <fpage>340</fpage>–<lpage>353</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname>, <given-names>Y. E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Evaluation of Brain-Body Health in Individuals With Common Neuropsychiatric Disorders</article-title>. <source>JAMA Psychiatry</source> <volume>80</volume>, <issue>567</issue> (<year>2023</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Welch</surname>, <given-names>J. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cortico-striatal synaptic defects and OCD-like behaviours in Sapap3-mutant mice</article-title>. <source>Nature</source> <volume>448</volume>, <fpage>894</fpage>–<lpage>900</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aguillon-Rodriguez</surname>, <given-names>V.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Standardized and reproducible measurement of decision-making in mice</article-title>. <source>Elife</source> <volume>10</volume>, (<year>2021</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Virtanen</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title>. <source>Nature Methods 2020</source> <volume>17</volume>:<issue>3</issue> 17, <fpage>261</fpage>–<lpage>272</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poličar</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Stražar</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Zupan</surname>, <given-names>B</given-names></string-name></person-group>. <article-title>openTSNE: A Modular Python Library for t-SNE Dimensionality Reduction and Embedding</article-title>. <source>J Stat Softw</source> <volume>109</volume>, <fpage>1</fpage>–<lpage>30</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Jacobs</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>De Zeeuw</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Romano</surname>, <given-names>V</given-names></string-name></person-group>. <article-title>Standard Operating Protocol: Implantation of Cambrige NeuroTech chronic silicon probe and mini-amp-64 digital headstage in mice</article-title>. <source>Protocol Exchange</source> (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.21203/RS.3.PEX-2188/V2</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sridhar</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Solving the spike sorting problem with Kilosort</article-title>. <source>bioRxiv</source> <elocation-id>2023.01.07.523036</elocation-id> (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.01.07.523036</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103586.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Noel</surname>
<given-names>Jean-Paul</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Minnesota</institution>
</institution-wrap>
<city>Minneapolis</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study demonstrates the ability for high-throughput recording and categorization of unconstrained and stimulus-based behaviors across a very large population of marmosets (n = 120 animals across 36 family units). The authors implement an analytical approach to identify &quot;outlier&quot; behavior that could be key in the development of next-generation precision psychiatry. While the strength of evidence appears <bold>solid</bold> overall, many key methodological details are <bold>incomplete</bold>.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103586.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors demonstrate a fully unsupervised, high throughput (meaning very low human interaction required) approach to quantifying marmoset behavior in unconstrained environments.</p>
<p>Strengths:</p>
<p>The authors provide an approach that is scalable, easy to implement at face value, and highly robust. Currently, most behavioral quantification approaches do not work well on marmosets, or the published examples that do look promising do not scale towards high throughput as demonstrated by the authors.</p>
<p>While marmosets can certainly be a useful translational research model devoid of free behavior quantification, the authors make a compelling point about how this approach can be useful in the study of treatments of emerging marmoset disease models.</p>
<p>Overall this is a very exhaustive manuscript that overcomes significant shortcomings in previous work and speaks highly to the use of marmosets for unconstrained behavioral and neural assessment.</p>
<p>Weaknesses:</p>
<p>Recording marmoset behavior with a 60Hz frame rate is a significant limitation to the approach which is hopefully easily alleviated in the future through better cameras/reconstruction pipelines. Marmosets (in the reviewers' experience) have a lot of motion energy above the 30Hz nyquist limit imposed by this system and are agile to a degree requiring higher frame rates.</p>
<p>The manuscript neglects recent approaches to non-human primate behavioral quantification from other groups that should be included. Simians are simians after all.</p>
<p>As a minor weakness, this reviewer would have liked to see code shared for the reviewers to evaluate, especially pertaining to the high throughput and robustness of the approach.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103586.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this manuscript, Menegas et al. classify the &quot;control&quot; behavior of captive marmosets. They combine behavioral screening from video recordings with audio and neural recordings (from the striatum) to better define what can be considered a typical behavioral repertoire for captive marmoset monkeys. A range of analyses is presented, investigating various aspects of behavior, such as social interactions and the detection of atypical individuals.</p>
<p>The manuscript is compelling in many respects, especially due to the richness of the dataset and the breadth of analyses presented. However, a significant issue with the manuscript lies in its writing: the results are conveyed in an overly succinct and superficial manner, and the &quot;Methods&quot; section is nearly absent. Key concepts are often undefined, and the mathematical details underlying the figures are not explained, leaving readers to guess the authors' approach.</p>
<p>Another issue is the vague use of the term &quot;natural behavior.&quot; All data presented here appear to have been collected in small cages with limited climbing opportunities and enrichment. Thus, the authors should refrain from using &quot;natural&quot; to describe these conditions.</p>
<p>Below, we elaborate further on the lack of methodological detail. Based on these issues, we believe the manuscript, in its current form, does not meet the scientific standards necessary for proper review. We strongly encourage the authors to undertake an extensive revision.</p>
<p>Major Revision Points:</p>
<p>The methods and results require significantly more detail. A scientific publication should provide readers with enough information to reproduce the study. Here, the detail level is far too low to fully understand, or reproduce, the study, and in many instances, readers are left to guess how the figure panels were produced. Below is a non-exhaustive list of examples illustrating these issues:</p>
<p>(1) &quot;we temporarily placed horizontal cage dividers to reduce the total cage size during data collection&quot;: What were the resulting (and initial) cage dimensions?</p>
<p>(2) &quot;After training the network, we hierarchically clustered the latent space&quot;: What is the latent space? Based on Figure 2a, it appears related to the network's recurrent layer, but this is not clarified in the text.</p>
<p>(3) Alpha and perplexity parameters: Please define these terms. Since these concepts appear fundamental, readers should not have to consult external references.</p>
<p>(4) &quot;We then traced cluster identities across hierarchical levels&quot;: What are hierarchical levels?</p>
<p>(5) &quot;To understand how the input time series data was weighed in the bottleneck layer of the model&quot;: What is the bottleneck layer?</p>
<p>(6) &quot;we measured the average attention allocation to previous time points&quot;: The authors should define &quot;attention allocation.&quot;</p>
<p>(7) &quot;we compared each neuron's firing rate distribution to shuffled data based on the overall frequency of each behavior during the session&quot;: This description is insufficient to understand the analysis.</p>
<p>(8) &quot;we hierarchically clustered neurons according to their firing rate enrichment maps&quot;: No mathematical explanation is provided for neuron clustering, nor is the concept of a &quot;firing rate enrichment map&quot; clarified.</p>
<p>(9) &quot;Cluster 4 showed higher activity when neurons were 'alone' or 'active'&quot;: This is vague and uses unclear jargon (e.g., &quot;neurons alone&quot;). Additionally, no mathematical explanation is provided for assigning neuronal activity to behavioral states.</p>
<p>(10) Figure 3f, right-side panels: The analysis seems to involve cage mate positioning, yet no description is provided.</p>
<p>(11) &quot;we used motion watches to measure activity across all hours&quot;: Are these motion-sensitive watches physically attached to the animals? The methodology should be described, including data analysis details.</p>
<p>This list could continue, but we trust the authors understand the point. There is a wealth of analyses and information in this study, but the descriptions are too superficial. We understand that fully describing each analysis may require significant rewriting, including supplementary figures, and will likely make the manuscript longer. This is entirely acceptable, as the ideas presented here are worth the added rigor.</p>
<p>&quot;Natural behavior&quot;: Typically, the term &quot;natural&quot; suggests that the dataset reflects the range of behaviors exhibited by animals in the wild. Here, however, recordings were made in a small cage with limited climbing opportunities and enrichment. Under these conditions, it's hard to justify describing the behavior as &quot;natural&quot;. In a project aimed at classifying the behavioral repertoire of marmoset monkeys and making this dataset accessible to other laboratories, it would be helpful to include more detailed information about the animals' housing conditions. This might include cage sizes, temperature, humidity, and details on food quantities, quality, and feeding times.</p>
<p>Correlation versus causation: In the section titled &quot;Large-scale data collection reveals variability across days and correlation between cagemates,&quot; the authors conclude: &quot;Overall, these results indicate that measurements of animals' behavioral traits depend heavily on their social environment.&quot; This interpretation seems incorrect. We know that animal behavior varies throughout the day, with activity peaks typically occurring in the morning and afternoon. Such factors, or other external influences, could induce correlations between animals that are not caused by social interactions.</p>
<p>Figure 4g: What are we intended to conclude from this analysis?</p>
<p>Figure 5: Please specify the type of calls analyzed. For example, did you analyze only long-distance calls (aka 'loud phees' or 'shrills')? In &quot;We split the audio data into 5-minute (non-continuous) segments and found that the average call rate in these segments varied from 0 calls per minute to 60 calls per minute (Fig. 5d-e),&quot; does the call rate refer to individual animals or the entire cage?</p>
<p>&quot;This implies that a high rate of calls in a room can interrupt animals during social resting states and cause them to preferentially exhibit more active/attentive states.&quot; Does it? This could simply indicate that more active animals produce more calls.</p>
<p>&quot;We recorded neural activity in the striatum because it is known to contain diverse signals related to movement and social interactions.&quot; While I understand that the authors intend to publish neural data separately, a brief discussion of the striatum's role here would be helpful.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103586.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Menegas</surname>
<given-names>William</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7483-994X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Corbett</surname>
<given-names>Erin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Beliard</surname>
<given-names>Kimberly</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Xu</surname>
<given-names>Haoran</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5753-8264</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Parmar</surname>
<given-names>Shivangi</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Desimone</surname>
<given-names>Robert</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5938-4227</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Feng</surname>
<given-names>Guoping</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8021-277X</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We would like to thank the editors and reviewers for taking the time to help improve our manuscript. We appreciate the feedback and will definitely increase the level of methodological detail in a revised submission.</p>
<p>Here is a brief summary of our plan to address the points raised by the reviewers. We will respond to the comments in a point-by-point manner when we resubmit a revised manuscript.</p>
<p><bold>Reviewer 1</bold></p>
<p>This reviewer raised a question about the 60 Hz frame rate for recording. We agree that increasing the number of cameras and frame rate would improve the tracking quality, but this would come at the cost of scalability. In the current study (and other concurrent studies in the lab), we recorded from 10-20 families simultaneously to try to sample the distribution of behavioral responses to stimuli observed in animals in our colony. This was only possible logistically because of the lightweight equipment design allowing us to record data from animals without large disruptions to their home-cage environment.</p>
<p>One strategy for acquiring higher-resolution data is to build a small number of enclosures that are fully surrounded by cameras, and to cycle animals through these enclosures (1). However, this strategy limits throughput by reducing the number of animals per day that can be studied. If the size and cost of cameras and computers decreases in the future, then this recording strategy will be scalable to the whole-colony level. For our current study and analysis, we are limited by the resolution of our dataset. We do believe that our data (although not a perfect 3d reconstruction or an extremely high frame rate) is sufficient to label behavioral states with high accuracy. We will add a figure to more clearly show that behavioral state data can be accurately inferred from this imperfect data, which has also been recently highlighted by other groups (2).</p>
<p>Additionally, with recent progress in the application of deep learning to animal pose tracking, new models can infer 3d pose dynamics from 2d data (3) and leverage spatiotemporal structure to clean up noisy data (4). We believe that other groups will be able to use these types of approaches to extract much more value from this dataset. So, in summary, we do understand the concern related to reconstruction quality and will 1) more clearly define the usefulness of our current models, 2) release our data and code so that others can build upon it or repurpose it, and 3) plan future experiments with higher camera count and frame rate as permitted by logistical constraints.</p>
<p><bold>Reviewer 2</bold></p>
<p>This reviewer asked for an increased level of methodological detail. We will try to address this in a few ways:</p>
<p>(1) Code and data sharing. We believe that many of the questions related to the methodology will be best answered by sharing the data and code directly. Because there is a large amount of code associated with this manuscript, it is impractical to list every step and every parameter in the paper. Along with our revised manuscript, we will make our data and code publicly available. That said, we will improve our description of key parameters in the paper as the reviewer suggested.</p>
<p>(2) More detailed Methods section. The reviewer asked us to provide more methodological detail. We understand that this is currently a weakness of our manuscript, and we will focus on addressing it. For instance, the reviewer rightly points out that we did not describe the motion watches used to generate the data in Figure S7. We will address this.</p>
<p>(3) Simplify the manuscript. The paper currently has 22 figures, and further analysis could be done based on the results shown in any of them. For instance, this reviewer asked us to add a comparison across females and males (similar to our comparison of juveniles and adults). While we plan to add that analysis, we recognize that there are several figures/panels that are not closely related to our intended goal of describing the patterns we found in our large dataset. We will simplify the manuscript by removing some excess figures/panels and focus on describing the parts of the analysis that are crucial to our conclusions in greater detail.</p>
<p>(4) More careful language. This reviewer pointed out that there were some inaccuracies with our descriptive language. For instance, we used the term &quot;natural&quot; behavior to describe the behavior of animals in captivity, which may more accurately be described as their home-cage behavior. We will be more careful to align our language to the standard for the field. For instance, several studies refer to unrestrained behavior in a laboratory setting as &quot;spontaneous&quot; behavior rather than &quot;natural&quot; behavior (5). In our case, the data consists of both spontaneously occurring behavior and responses to a set of stimuli. We will make sure that the descriptions are more precise in the revised manuscript.</p>
<p>(1) Bala, P. C. et al. Automated markerless pose estimation in freely moving macaques with OpenMonkeyStudio. Nat Commun 11, (2020).</p>
<p>(2) Weinreb, C. et al. Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics. bioRxiv (2023) doi:10.1101/2023.03.16.532307.</p>
<p>(3) Gosztolai, A. et al. LiftPose3D, a deep learning-based approach for transforming two-dimensional to three-dimensional poses in laboratory animals. Nat Methods 18, 975–981 (2021).</p>
<p>(4) Wu, A. et al. Deep Graph Pose: a semi-supervised deep graphical model for improved animal pose tracking. Adv Neural Inf Process Syst 33, 6040–6052 (2020).</p>
<p>(5) Levy, D. R. et al. Mouse spontaneous behavior reflects individual variation rather than estrous state. Curr Biol 33, 1358-1364.e4 (2023).</p>
</body>
</sub-article>
</article>