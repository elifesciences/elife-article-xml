<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">92517</article-id>
<article-id pub-id-type="doi">10.7554/eLife.92517</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.92517.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Insights in neuronal tuning: Navigating the statistical challenges of autocorrelation and missing variables</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0008-4549-4397</contrib-id>
<name>
<surname>Nevjen</surname>
<given-names>Fredrik</given-names>
</name>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Dunn</surname>
<given-names>Benjamin Adric</given-names>
</name>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Mathematical Sciences, Norwegian University of Science and Technology</institution></aff>
<aff id="a2"><label>2</label><institution>Kavli Institute for Systems Neuroscience, Norwegian University of Science and Technology</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>For correspondence:</bold> <email>fredrik.nevjen@ntnu.no</email>; <email>benjamin.dunn@ntnu.no</email></corresp>
<fn id="n1" fn-type="present-address"><p>Department of Mathematical Sciences, NTNU, Norway</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-01-26">
<day>26</day>
<month>01</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP92517</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-10-25">
<day>25</day>
<month>10</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-10-30">
<day>30</day>
<month>10</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.25.563994"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Nevjen &amp; Dunn</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Nevjen &amp; Dunn</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-92517-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Recent advances in neuroscience have improved our ability to investigate neural activity by making it possible to measure vast amounts of neurons and behavioral variables, and explore the underlying mechanisms that connect them. However, comprehensively understanding neuronal tuning poses challenges due to statistical issues such as temporal autocorrelation and missing variables, as neurons are likely driven in part by unknown factors. The field consequently needs a systematic approach to address these challenges. This study compares various methods for covariate selection using both simulated data and calcium data from the medial entorhinal cortex. We conclude that a combination of cross-validation and a cyclical shift permutation test yields higher test power than other evaluated methods while maintaining proper error rate control, albeit at a higher computational cost. This research sheds light on the quest for a systematic understanding of neuronal tuning and provides insight into covariate selection in the presence of statistical complexities.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Single neuron tuning has been an important tool in our attempts to understand the brain. Many insights have come from considering, e.g., the possible roles of head direction cells (<bold><italic><xref ref-type="bibr" rid="c40">Taube et al., 1990</xref></italic></bold>), place cells (<bold><italic><xref ref-type="bibr" rid="c29">O’Keefe and Dostrovsky, 1971</xref></italic></bold>), and grid cells (<bold><italic><xref ref-type="bibr" rid="c15">Hafting et al., 2005</xref></italic></bold>) – each of which was first characterized through careful inspection of their response properties of single neurons to aspects of the animal’s behavior. As neuroscience moves towards more population-based approaches (<bold><italic><xref ref-type="bibr" rid="c4">Cunningham and Yu, 2014</xref>; <xref ref-type="bibr" rid="c1">Bassett and Sporns, 2017</xref>; <xref ref-type="bibr" rid="c8">Datta et al., 2019</xref></italic></bold>), the classification of neurons is still just as important both to leverage traditional methods but also to isolate the relevant cell populations to further study with population-based methods such as dimensionality reduction (<bold><italic><xref ref-type="bibr" rid="c4">Cunningham and Yu, 2014</xref></italic></bold>), neural manifolds (<bold><italic><xref ref-type="bibr" rid="c11">Gallego et al., 2017</xref></italic></bold>), latent variable models (LVM) (<bold><italic><xref ref-type="bibr" rid="c23">Lawrence, 2003</xref>; <xref ref-type="bibr" rid="c51">Yu et al., 2008</xref>; <xref ref-type="bibr" rid="c49">Wu et al., 2017</xref>; <xref ref-type="bibr" rid="c31">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="c2">Bjerke et al., 2023</xref>; <xref ref-type="bibr" rid="c38">Schneider et al., 2023</xref></italic></bold>), and topological data analysis (TDA) (<bold><italic><xref ref-type="bibr" rid="c5">Curto and Itskov, 2008</xref>; <xref ref-type="bibr" rid="c36">Rybakken et al., 2019</xref>; <xref ref-type="bibr" rid="c12">Gardner et al., 2022</xref></italic></bold>). Furthermore, at the same time that technology has enabled large neural recordings, our ability to capture detailed descriptions of animals has also improved (<bold><italic><xref ref-type="bibr" rid="c24">Mathis et al., 2018</xref></italic></bold>), sparking a renewed interest in animal behavior, while dramatically expanding the number of variables considered to explain the neural activity (<bold><italic><xref ref-type="bibr" rid="c27">Mimica et al., 2023</xref></italic></bold>). This expansion in terms of neurons as well as possible explanatory variables makes it both more difficult to appropriately ask the question of what variables appear to matter and how we can interpret the answer we receive.</p>
<p>Working under the hope and assumption that neurons have some form of specialization, the need for covariate selection is clear: out of a large number of possible covariates, such as position in space, speed, and head direction, to which features does a neuron selectively respond? This is, however, a deceptively difficult question to ask in a meaningful way, and we wish to highlight important statistical challenges that commonly occur in this endeavor. At their core these challenges boil down to misspecification of statistical models or tests, causing them to yield misleading results. We focus on the generalized linear model (GLM) (<bold><italic><xref ref-type="bibr" rid="c25">McCullagh and Nelder, 1989</xref></italic></bold>), which is a versatile and popular framework to model the relationship between neural activity and external covariates (<bold><italic><xref ref-type="bibr" rid="c43">Truccolo et al., 2005</xref>; <xref ref-type="bibr" rid="c44">Weber and Pillow, 2017</xref>; <xref ref-type="bibr" rid="c16">Hardcastle et al., 2017</xref></italic></bold>), but the issues studied here are important regardless of model choice.</p>
<p>The first challenge we would like to highlight is related to the temporal autocorrelation in neural data. Neural activity and behavioral variables have a non-zero correlation between the values at time t and those preceding and following that time point, arising due to the time-dependent nature of the underlying factors driving the activity, including internal chemical and electrical processes, dynamics of animal behavior, and other external stimuli (<bold><italic><xref ref-type="bibr" rid="c48">Wilson, 1999</xref>; <xref ref-type="bibr" rid="c22">Keat et al., 2001</xref>; <xref ref-type="bibr" rid="c43">Truccolo et al., 2005</xref>; <xref ref-type="bibr" rid="c50">York et al., 2020</xref>; <xref ref-type="bibr" rid="c8">Datta et al., 2019</xref>; <xref ref-type="bibr" rid="c21">Hubel and Wiesel, 1959</xref></italic></bold>). Apparent correlations between two autocorrelated time series that appear significant when they are actually independent of each other, known as <italic>nonsense correlation</italic>, were as relevant 100 years ago (<bold><italic><xref ref-type="bibr" rid="c52">Yule, 1926</xref></italic></bold>) as they are today (<bold><italic>Harris, 2020a</italic></bold>). <xref rid="fig1" ref-type="fig">Figures 1 A</xref> and <xref rid="fig1" ref-type="fig">B</xref> illustrate how this can influence hypothesis tests concerning correlations if one applies a test assuming independent data. This problem naturally extends to the incorrect use of any statistical method or model that assumes temporal independence. The GLM for instance assumes that the response is independent when conditioned on the variables included in the model (analogous to the error terms in a linear regression model being independent), which will only hold in the unlikely case that the included variables capture all of the underlying temporal structure (autocorrelation).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>It is problematic to naively use statistics when necessary assumptions are not met. <bold>A</bold>: Empirical distribution of the Pearson correlation coefficient between two independent samples of 200 data points each, repeated 10000 times. Within sample the data are either independent or weakly or strongly autocorrelated. When the samples have temporal autocorrelation the empirical distribution of the correlation coefficient widens beyond the theoretical distribution for independent and identically distributed (IID) data. <bold>B</bold>: Corresponding distributions of <italic>p</italic>-values obtained under the assumption that the data is independent, showing that this mistake inflates the number of small <italic>p</italic>-values, yielding false positives (type I error) beyond the significance level set by the researcher. <bold>C</bold>: Empirical distribution of the deviance of a Poisson GLM comparing two nested models where one includes an irrelevant covariate (<italic>x</italic>-axis is cut at 30). 200 data points are generated of each covariate and the response variable, and this is repeated 10000 times. The covariates are either IID or autocorrelated, and we compare cases where there is or is not another relevant covariate missing from the model. <bold>D</bold>: Corresponding distributions of <italic>p</italic>-values obtained from likelihood-ratio tests comparing the nested models. When there exists a missing covariate, the number of small <italic>p</italic>-values and hence the rate of false positives are inflated. <bold>E</bold>: Type I error rates (how often does an overly complex model attain the best CV-score) for different CV schemes using different choices of block size and whether to skip blocks or not, including a scheme where folds are drawn completely at random, ignoring temporal structure. We see that failure to account properly for temporal dependencies greatly increases the probability that the wrong model is selected. Data is generated as in Results on simulated data, with 300 spike trains and 3 covariates simulated. The bars show 95% Clopper-Pearson confidence interval for the proportion of incorrect conclusions for each CV scheme. <bold>F</bold>: Same scenario as in <bold>E</bold>, except here a combination of cross-validation and the Wilcoxon signed-rank test is used. The significance level is set to <italic>α</italic> = 0.05, meaning that is the type I error rate we expect to see. With too small blocks the false positive rate is inflated, and with larger blocks the method appears overly conservative. <bold>G</bold>: Expected experimentwise type I error rates when performing multiple, independent tests, illustrating the need for correction. For a hypothesis test that on its own has type I error rate <italic>α</italic>, the experimentwise type I error rate grows as 1 − (1 − <italic>α</italic>)<sup><italic>n</italic></sup>.</p></caption>
<graphic xlink:href="563994v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Missing variables is another issue that is very likely to apply to any modeling of neural activity, since it is improbable that we are able to observe and correctly represent every underlying variable that is driving the neuron in question. Parameter estimates and <italic>p</italic>-values for the regression models are biased in the event of omitted variables (<bold><italic><xref ref-type="bibr" rid="c39">Stevenson, 2018</xref>; <xref ref-type="bibr" rid="c10">Dunn and Battistin, 2017</xref>; <xref ref-type="bibr" rid="c7">Das and Fiete, 2020</xref></italic></bold>). These omitted variables are typically also temporally autocorrelated, which further introduces temporal structure that could be misattributed to other variables. <xref rid="fig1" ref-type="fig">Figure 1 C</xref> and <xref rid="fig1" ref-type="fig">D</xref> show that missing variables substantially alter the distribution of the deviance of a GLM. It is illustrated specifically that a likelihood-ratio test based on GLM assumptions will behave as intended if all relevant variables are accounted for, but that the omission of relevant variables, autocorrelated or not, yields increased error rates.</p>
<p>The challenges mentioned so far rule out the use of the standard approaches in statistics for evaluating a given model performance, such as Akaike and Bayesian information criteria, which, similar to the likelihood-ratio test described above and illustrated in <xref rid="fig1" ref-type="fig">Figure 1 C</xref> and <xref rid="fig1" ref-type="fig">D</xref>, would yield overly optimistic results. Cross-validation (CV) is a commonly used method for model selection in such situations, estimating model performance by testing on withheld data folds (<bold><italic><xref ref-type="bibr" rid="c19">Hastie et al., 2001</xref></italic></bold>). To incorporate temporal structure, CV can be adapted with blocking (keeping blocks of consecutive time bins intact when constructing the folds) and skipping (removing portions of the data at the edges of the folds) (<bold><italic><xref ref-type="bibr" rid="c34">Racine, 2000</xref>; <xref ref-type="bibr" rid="c35">Roberts et al., 2017</xref></italic></bold>) (see <xref rid="fig9" ref-type="fig">Figure 9</xref>). If arbitrary data points or small blocks are used for constructing the folds, the error rate will be inflated (see <xref rid="fig1" ref-type="fig">Figure 1 E</xref>). The forward selection approach used by <bold><italic><xref ref-type="bibr" rid="c16">Hardcastle et al. (2017</xref></italic></bold>) combines blocked CV with the Wilcoxon signed-rank test (<bold><italic><xref ref-type="bibr" rid="c46">Wilcoxon, 1945</xref></italic></bold>), a hypothesis test more general than those based on the GLM assumptions. Using small blocks, however, in the CV scheme may still inflate error rates in this approach (see <xref rid="fig1" ref-type="fig">Figure 1 F</xref>). Furthermore, testing only the best model in each step is a form of selective inference (<bold><italic><xref ref-type="bibr" rid="c41">Taylor and Tibshirani, 2015</xref></italic></bold>), akin to a multiple comparisons problem (<bold><italic><xref ref-type="bibr" rid="c13">Goeman and Solari, 2014</xref></italic></bold>), which can result in inflated false positives and requires correction (see <xref rid="fig1" ref-type="fig">Figure 1 G</xref>). If a small number of features are considered, however, the method tends to be conservative due to the general nature of the signed-rank test. This inflates the rate of false negatives, making it more challenging to discover actual relationships. Thus this approach can be overly permissive or very conservative depending on the data and the way the model is applied.</p>
<p>We evaluate different methods for covariate selection for the GLM, based on the forward selection procedure used by <bold><italic><xref ref-type="bibr" rid="c16">Hardcastle et al. (2017</xref></italic></bold>), and explore possible adjustments to the CV scheme and hypothesis test used, in order to consolidate the inherent conservativeness of the hypothesis test and the lack of correction for multiple comparisons (note they do have a countermeasure for this, which we discuss briefly in Methods and Materials). We make modifications to their procedure by i) using skipping in the CV scheme to further separate the test and training data, ii) adjusting the number of folds, iii) considering different options for the hypothesis test used, and iv) correcting these tests for the multiple comparisons that are performed in each step of the forward selection procedure. The results we obtain with a modified Wilcoxon signed-rank test are similar to those of the unmodified one, while using a permutation test with cyclical shifts (details in Methods and Materials) yields higher test power than the signed-rank test, but with an increased computational cost. Finally, we discuss how the results of such studies, even when done as correctly as possible, can still be misleading, and suggest a way forward.</p>
<p>Our analyses are performed on simulated data and on calcium data from the medial entorhinal cortex (MEC) in freely moving mice (<bold><italic><xref ref-type="bibr" rid="c53">Zong et al., 2022</xref></italic></bold>). The methods are implemented in the statistical programming language R <bold><italic>R Core Team</italic></bold> (<bold><italic>2019</italic></bold>), and both simulated and real case studies were run on x86_64-apple-darwin15.6.0 (64-bit) with R version 3.6.0. All relevant code can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/fredrine/covariateselection">https://github.com/fredrine/covariateselection</ext-link>.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>The methods that we have evaluated are summarized in <xref rid="tbl1" ref-type="table">Table 1</xref>. We denote the unmodified forward selection procedure using the Wilcoxon signed-rank test as SR and its Bonferroni corrected (for multiple testing) counterpart as SR<sub>Bonf</sub>. For these the <italic>p</italic>-values were calculated from a theoretical distribution of the test statistic. The procedure using modifications to the cross-validation (CV) scheme, as well as a maxT correction (<bold><italic><xref ref-type="bibr" rid="c45">Westfall and Young, 1993</xref></italic></bold>) for multiplicity is denoted as mSR<sub>MaxT</sub>. A further modified version of this where the new model were compared not to the previous one, but one also including the added covariate reversed in time, is denoted as mSRR<sub>MaxT</sub>. To able to use the maxT correction we calculated the <italic>p</italic>-values by constructing a shuffled distribution by permuting the signs of the CV scores for these methods. CS<sub>Bonf</sub> is used to denote the procedure using the modified CV scheme, with a permutation test with cyclical shifts in place of the signed-rank test. Cross-validation, with blocking and skipping, used to directly select covariates without any hypothesis test is denoted as CV. Detailed descriptions of all the methods can be found in Methods and Materials. <xref rid="fig2" ref-type="fig">Figure 2</xref> illustrates how a distribution of <italic>p</italic>-values will look when the performed test is either invalid, valid or conservative.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Summary of different hypothesis tests used in step 3 of the forward selection procedure, and the corresponding CV scheme used to determine which covariate that should be added to the model.</title></caption>
<graphic xlink:href="563994v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Different hypothetical distributions of <italic>p</italic>-values resulting from a hypothesis test. For the test to be valid, the probability of obtaining a <italic>p</italic>-value smaller than a specified significance level <italic>α</italic> must be at most <italic>α</italic>, i.e. Pr(<italic>p</italic> ≤ <italic>α</italic>) ≤ <italic>α</italic>, when the null hypothesis is assumed to be true. If this is not true, and Pr(<italic>p</italic> ≤ <italic>α</italic>) &gt; <italic>α</italic>, the distribution will be skewed towards 0, the rate of false positives will be inflated (<bold>A</bold>), and the test is invalid. For a valid test we typically see Pr(<italic>p</italic> ≤ <italic>α</italic>) = <italic>α</italic> (<bold>B</bold>), giving a uniform distribution of <italic>p</italic>-values. A test might also be conservative, meaning it is still valid, but the distribution is skewed towards 1 (<bold>C</bold>). In the latter case the rate of false negatives will be inflated, and it is less likely that true positive results are discovered.</p></caption>
<graphic xlink:href="563994v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2a">
<title>Results on simulated data</title>
<p>We have conducted a simulation study to test the different methods under conditions similar to real data, but where we know the underlying truth. Variables with temporal autocorrelation are constructed by passing a vector of independently sampled uniform variables through a smoothing filter. One such variable is treated as unobserved, but driving the cell to fire. Three other variables are sampled independently from the driving variable and each other, and are treated as potential covariates for the regression model. One of these is generated as a two-dimensional variable, emulating animal position.</p>
<p>Two different scenarios are simulated. In scenario 1, the unobserved variable is all that drives the simulated cell, and thus the observed variables are all irrelevant. In this case we would like our methods to conclude that no variable should be included in the model. Furthermore, for a hypothesis test to be valid the probability of observing a <italic>p</italic>-value below the significance level <italic>α</italic> (for any choice of 0 &lt; <italic>α</italic> &lt; 1) must not exceed <italic>α</italic> (<bold><italic><xref ref-type="bibr" rid="c3">Casella and Berger, 2002</xref></italic></bold>, p.397). In practice this means that the distribution of observed <italic>p</italic>-values should be uniform or skewed to the right. The latter could however be an indication that the hypothesis test is conservative, and potentially lacking in power. In scenario 2, the cell activity is also affected by the two-dimensional variable, specifically by having heightened spike activity in two specific spatial positions. In this case our methods would ideally recognize that the two-dimensional variable should be included in the model, and that the others should not.</p>
<p><xref rid="fig3" ref-type="fig">Figure 3</xref> shows the results from the forward selection procedure in scenario 1, where none of the observed covariates are relevant to the cell activity. In part A there are histograms showing how often each of the possible covariate combinations is selected as the final model for each of the methods evaluated. The methods all conclude that few to no covariates should be included. CV alone (no hypothesis test conducted) naturally yields more complex models than the other methods, and is the only method with a type I error rate exceeding the selected significance level of <italic>α</italic> = 0.05. The remaining methods all have reasonable error rates, where mSR<sub>MaxT</sub> and SR<sub>Bonf</sub> in particular appear overly conservative with no false positives. The modified signed-rank test testing against the model with the reversed covariate (mSRR<sub>MaxT</sub>) and the unmodified signed-rank test (SR) both appear conservative, while the method using cyclic shifts (CS<sub>Bonf</sub>) has an error rate closer to the selected significance level. A summary of the error rates with corresponding 95% Clopper-Pearson confidence intervals are shown in <xref rid="tbl2" ref-type="table">Table 2</xref>.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><p>Summary of results when using simulated data with 300 simulated cells. We show type I error rates (proportion of simulations where a covariate is incorrectly included) for scenario 1, where none of the observed covariates affect the response, and power (proportion of simulations where the two-dimensional covariate is correctly included) for scenario 2, where the two-dimensional covariate has an effect on the response. We also show corresponding 95% Clopper-Pearson confidence intervals. For each hypothesis test the significance level <italic>α</italic> = 0.05 is used.</p></caption>
<graphic xlink:href="563994v1_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Testing simulated data where none of the observed covariates have any effect on the response. <bold>A</bold>: For each method (CV, mSR<sub>MaxT</sub>, mSRR<sub>MaxT</sub>, CS<sub>Bonf</sub>, SR, SR<sub>Bonf</sub>, see <xref rid="tbl1" ref-type="table">Table 1</xref>) the number of cells classified as tuned to each of the 8 combinations of the three covariates is shown, for a total of 300 simulated cells. The bars are colored according to which final model is selected by the forward selection procedure. Note that the y-axis is in log-scale to emphasize sparse results. <bold>B</bold>: Distribution of <italic>p</italic>-values for each method at the first step. For this scenario the proportion of <italic>p</italic>-values smaller than <italic>α</italic> = 0.05 corresponds to the type I error rate, and must itself be smaller than <italic>α</italic> on average for the method to be valid. CV without any added hypothesis test exceeds this threshold, whereas the other methods all give error rates below it.</p></caption>
<graphic xlink:href="563994v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The results from scenario 2, where the simulated two-dimensional covariate affects the cell activity, is shown in <xref rid="fig4" ref-type="fig">Figure 4</xref>, and summarized in <xref rid="tbl2" ref-type="table">Table 2</xref>. The correct covariate is identified by the CV scheme in every simulation. The method using the cyclical permutation test (CS<sub>Bonf</sub>) has the highest estimated test power (defined here as the proportion of simulations where the two-dimensional covariate is correctly included) of the methods which include a hypothesis test. We see smaller test power for mSR<sub>MaxT</sub> than its alternative counterpart mSRR<sub>MaxT</sub>. We can observe that the results from SR are similar to mSRR<sub>MaxT</sub>, suggesting that the increased power from comparing to the model with the reversed covariate is on par with the increased power from not accounting for multiple comparisons, in this case. Generally the latter will likely scale differently with the number of covariates considered, which in the extreme case will cause inflated type I errors (see <xref rid="fig1" ref-type="fig">Figure 1</xref>). For SR<sub>Bonf</sub> we observe low power, in line with how it appears conservative in <xref rid="fig3" ref-type="fig">Figure 3</xref>.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Testing on simulated data where the simulated position covariate has an effect on the response. For each method (CV, mSR<sub>MaxT</sub>, mSRR<sub>MaxT</sub>, CS<sub>Bonf</sub>, SR, SR<sub>Bonf</sub>, see <xref rid="tbl1" ref-type="table">Table 1</xref>) the number of cells classified as tuned to each of the 8 combinations of the three covariates is shown, for a total of 300 simulated cells. The bars are colored according to which final model is selected by the forward selection procedure. We see that the methods in general correctly favor the pure position model. Note that the y-axis is in log-scale to emphasize sparse results. Covariate A is relevant to the activity of the simulated cells and should be included in the model. Additionally, since we use a significance level of <italic>α</italic> = 0.05, we expect to see additional, irrelevant covariates for approximately 5% of the cells where the correct one is included.</p></caption>
<graphic xlink:href="563994v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Results on calcium data</title>
<p>A study using calcium data from the medial entorhinal cortex (MEC) of freely moving mice is also conducted to test the methods on real data. The data is the publicly available dataset from <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>) and <bold><italic><xref ref-type="bibr" rid="c28">Obenhaus et al. (2022</xref></italic></bold>), who classify grid cells using the cyclical shift permutation test by shifting the neural activity in time, and the grid score as the test statistic.</p>
<p>We test the methods again in two scenarios. In scenario 1, we combine cell activity from one session (approximately 27 minutes) with the behavioral covariates of another, so that the observed variables are all irrelevant to the response. Thus we expect the methods to conclude that no variable should be included in the model. In scenario 2, the cell activity is matched with the correct behavioral covariates, and thus we expect to see the methods classify the cells as having tuning. We do not however know the ground truth in this scenario, but we can assess how well our methods identify the grid cells previously classified by <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>) as position-tuned.</p>
<p>The results from scenario 1, with mismatched cell activity and covariates, are shown in <xref rid="fig5" ref-type="fig">Figure 5</xref>, tested with 249 cells. A summary of the error rates can be found in <xref rid="tbl3" ref-type="table">Table 3</xref>. All methods select few to no covariates, similarly to what we observed in scenario 1 with simulated data. CV alone yields the most complex models, and thus has the highest error rate. The unmodified signed-rank test (SR), the Bonferroni corrected version of this (SR<sub>Bonf</sub>), and the maxT corrected version where we perform the permutation of signs (mSR<sub>MaxT</sub>) all appear overly conservative, yielding no false positives. The alternative version of the latter (mSRR<sub>MaxT</sub>) and the cyclic shift based method (CS<sub>Bonf</sub>) commit slightly more errors, but not more than the selected significance level of <italic>α</italic> = 0.05.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><p>Summary of type I error rates (proportion of cells where a covariate is incorrectly included) for the scenario with mismatched data where none of the observed covariates affect the response, and proportion of the 147 grid cells classified by <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>) that are also classified as position tuned. We also show corresponding 95% Clopper-Pearson confidence intervals. For each hypothesis test the significance level <italic>α</italic> = 0.05 is used. (*) The confidence intervals are based on the binomial distribution, which assumes that the neurons are independent. In reality there are small correlations present, meaning that the intervals are slightly too optimistic, and should be somewhat wider.</p></caption>
<graphic xlink:href="563994v1_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Results from testing on calcium data where the cell activity and covariates are from different sessions. <bold>A</bold>: For each method (CV, mSR<sub>MaxT</sub>, mSRR<sub>MaxT</sub>, CS<sub>Bonf</sub>, SR, SR<sub>Bonf</sub>, see <xref rid="tbl1" ref-type="table">Table 1</xref>) the number of cells classified as tuned to each of the 8 combinations of the three covariates is shown, for a total of 249 cells. The bars are colored according to which final model is selected by the forward selection procedure. Note that the y-axis is in log-scale to emphasize sparse results. <bold>B</bold>: Distribution of <italic>p</italic>-values for each method at the first step. For this scenario the proportion of <italic>p</italic>-values smaller than <italic>α</italic> = 0.05 corresponds to the type I error rate, and must itself be smaller than <italic>α</italic> on average for the method to be valid. Cross-validation without any added hypothesis test exceeds this threshold, whereas the other methods all give error rates below it.</p></caption>
<graphic xlink:href="563994v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The confidence intervals included in <xref rid="tbl3" ref-type="table">Table 3</xref> are constructed in the same way as in <xref rid="tbl2" ref-type="table">Table 2</xref> in the previous section, using Clopper-Pearson method for a binomial proportion. However, while the simulated neurons in the previous section are generated independently, the activity of actual neurons will be correlated to some extent. Therefore the assumption of independence between the neurons, which is made when using the binomial distribution, is violated. The confidence intervals in <xref rid="tbl3" ref-type="table">Table 3</xref> are therefore likely to be overly optimistic, meaning they are too narrow. Methods that account for dependencies are likely to be overly conservative since the correlations are small and limited to clusters of neurons. A solution to this issue which accounts more specifically for the type of correlation structure that is found in neurons is to our knowledge not found in the field.</p>
<p><xref rid="fig6" ref-type="fig">Figure 6</xref> shows the results from our tests on the calcium data in scenario 2, where the cell activity and covariates are correctly matched. Most cells are classified as position-tuned by all the methods, while CV and CS<sub>Bonf</sub> both suggest that head direction or speed also might have an effect on the cell activity of a large portion of the cells. <xref rid="tbl3" ref-type="table">Table 3</xref> shows for each method evaluated the proportion of the 147 grid cells classified by <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>) that the GLM also classified as position tuned. CV and CS<sub>Bonf</sub> classify the most grid cells, followed by mSRR<sub>MaxT</sub>, while the remaining methods classify less than half of these cells.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Results from testing on calcium data where the cell activity and covariates are from the same session. For each method (CV, mSR<sub>MaxT</sub>, mSRR<sub>MaxT</sub>, CS<sub>Bonf</sub>, SR, SR<sub>Bonf</sub>, see <xref rid="tbl1" ref-type="table">Table 1</xref>) the number of cells classified as tuned to each of the 8 combinations of the three covariates is shown, for a total of 249 cells. The bars are colored according to which final model is selected by the forward selection procedure. Most methods favor models including position as a covariate, and CS often suggests a model with multiple covariates.</p></caption>
<graphic xlink:href="563994v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Ratemaps for some of the grid cells that CS<sub>Bonf</sub> classifies as position tuned are shown to the left in <xref rid="fig7" ref-type="fig">Figure 7</xref>. The middle of the same figure shows ratemaps of some of the 87 cells classified as position tuned by the GLM, but that are not classified as grid cells. For most of these cells the ratemaps convincingly indicate that there is a structural relationship between firing and position, and some even show weak grid patterns, suggesting they barely failed to be classified as grid cells. The rightmost part of the figure shows ratemaps for 9 of the 22 cells that are classified as grid cells by <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>), but not recognized as position tuned by the GLM. We can see that the firing patterns of these cells generally appear more foggy than those in the leftmost plot, and the average mean event rate for these neurons are also smaller than the ones that are classified by the GLM, which might explain why these are not. The grid score is also a more specialized way to classify neurons than the GLM, meaning weaker patterns may still be found by the former because it specifically looks for those patterns.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Ratemaps and tuning curves for a selection of cells with different classifications by the GLM approach with CS<sub>Bonf</sub>. <bold>A</bold>: Cells classified as grid cells by <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>), and as position tuned by the GLM. <bold>B</bold>: Cells not classified as grid cells, but as position tuned by the GLM. <bold>C</bold>: Cells classified as grid cells, but not classified as position tuned by the GLM. Animal position is binned using 40 bins in each direction, and the average number of events occurring in each two-dimensional bin is calculated. Then a two-dimensional Gaussian filter with standard deviation of 1 bin is applied. Each ratemap is then normalized by dividing it by its 95th percentile. The mean of the average event rates for each group the subset of cells is from is displayed in each plot title.</p></caption>
<graphic xlink:href="563994v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig8" ref-type="fig">Figure 8</xref> shows the number of cells that mSRR<sub>MaxT</sub>, SR and CS<sub>Bonf</sub> classify as tuned to the different covariates, as well as the number of grid cells classified by <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>). Most of the 147 grid cells are classified with each method, with CS<sub>Bonf</sub> classifying the most. As with the simulated data we can also here observe that the results from SR are similar to those from mSRR<sub>MaxT</sub>. When using CS<sub>Bonf</sub> we obtain results suggesting the cells have conjunctive tuning to two or even all three of the covariates considered, with 53 cells (21.6%) being classified as tuned to all three. We do not know the underlying truth of what is driving the firing of the neurons, but these results are consistent with other findings in the MEC (<bold><italic><xref ref-type="bibr" rid="c37">Sargolini et al., 2006</xref>; <xref ref-type="bibr" rid="c16">Hardcastle et al., 2017</xref></italic></bold>).</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><p>Resulting classification from the GLM using CS<sub>Bonf</sub>, mSRR<sub>MaxT</sub> and SR, compared to the grid cell classification by <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>). This means that CS<sub>Bonf</sub> classifies for example 2 + 0 + 20 + 17 + 33 + 15 + 2 + 0 = 89 cells as tuned to head direction, 17 + 20 + 16 + 31 + 37 + 40 + 33 + 15 = 209 to position, 33 + 20 = 53 as tuned to position, head direction and speed, and that 37 + 40 + 33 + 15 = 125 of the 147 grid cells are classified as position-tuned. In general CS<sub>Bonf</sub> classifies more cells than the other two methods, which yield similar results.</p></caption>
<graphic xlink:href="563994v1_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9.</label>
<caption><p>Illustration of CV scheme with blocking and skipping. Each box is a block of consecutive time bins. When a fold is used as the test set, the neighboring blocks are excluded from the training set. Note that there are 8 folds in the illustration, compared to 10 or 20 in our analyses.</p></caption>
<graphic xlink:href="563994v1_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We show through testing on both simulated data and real neural data that methods typically used for covariate selection in neuroscience can be either overly conservative or overly permissive, but they can be made statistically robust and even more powerful with a few modifications.</p>
<p>Cross-validation (CV) on its own can work well for the purpose of covariate selection if the temporal structure of the data is taken into consideration, although it typically yields false positive rates beyond desirable levels. This might be sufficient if the goal of the analysis is to explore, rather than decisively conclude which covariates are relevant to the neural response. If the analysis is performed with the intent of conducting confirmatory research, additional hypothesis testing in the forward selection scheme is desired to control the rate of false positives.</p>
<p>The Wilcoxon signed-rank test is inherently conservative when testing CV log-likelihood values of nested models, meaning that their type I error rate control is stricter than the threshold set by the user (explained further in Methods and Materials). If the method is used incorrectly, however, either by using too small blocks of data for the CV, or by failing to account for multiple testing, the false positive rate may instead be inflated (see <xref rid="fig1" ref-type="fig">Figure 1</xref>). The modification of the signed-rank test where we compare the proposed model to one where the added covariate is reversed alleviates some of the conservative tendencies of the test, because we compare models of equal complexity. We see that this results in increased test power both for simulated and real data.</p>
<p>The forward selection procedure without modifications to the CV scheme and signed-rank test (SR) yields very similar results to our adjusted method (mSRR<sub>MaxT</sub>), so for this particular case it may seem like the inherent conservativeness and lack of correction for multiple comparisons cancel each other out, coincidentally. We should however strive for statistical robustness and generality, meaning the adjusted method should be preferred, as statistics of each data set will be different. The permutation test using cyclical shifts of the covariate considered for the model attains desirable error rates and higher test power than the other methods. This does however come at computational cost.</p>
<p>An additional remark with respect to statistical robustness is that even though the methods using the signed-rank test appear valid, we should be cautious. The fact that the CV training sets overlap to some extent, potentially violating signed-rank test assumptions, means that we are unable to completely trust the generality of the results. This issue was addressed by <bold><italic>Dietterich</italic></bold> (<bold><italic>1998</italic></bold>), who proposed a solution of using a 5 x 2 CV test, where the data is split into CV folds multiple times in order to reduce the effect of overlapping training sets. However, with temporal data, such as ours, the repeated splitting of the data is not feasible unless the data length is very long.</p>
<p>The methods presented in this work serve as a proposal to the problems caused by temporal structure without having to model this structure specifically. Including past spike history using auto-regressive terms in the regression model is common in the field (<bold><italic><xref ref-type="bibr" rid="c43">Truccolo et al., 2005</xref></italic></bold>). While these terms have biophysical interpretations at smaller time scales, where they for example can represent the refractory period of the neuron, they are unlikely to remove all temporal dependency due to unobserved, temporally dependent factors also driving the activity (<bold><italic><xref ref-type="bibr" rid="c39">Stevenson, 2018</xref></italic></bold>). Hence the additional adjustments we suggest are warranted.</p>
<p>An overarching goal in neuroscience is to determine which purpose the brain area in question serves, and classifying individual neurons from the area in terms of tuning to covariates is one possible approach. If we treat the classification of each neuron as a vote, we can likely conclude that the covariates with the most votes are important to the brain region as a whole. Furthermore, an anatomical clustering of cells with similar tuning is logical in the sense that these cells rely on input from each other to function as a network. <bold><italic><xref ref-type="bibr" rid="c28">Obenhaus et al. (2022</xref></italic></bold>) argue that this is true in particular for the medial entorhinal cortex (MEC), where a mental map is constructed by different cell types all related to navigation.</p>
<p>Not all cells are specialized into cell types, however, and instead of modelling single neurons another possibility is to use methods and models that consider the population code more directly, using for example latent variable models (LVM) (<bold><italic><xref ref-type="bibr" rid="c23">Lawrence, 2003</xref>; <xref ref-type="bibr" rid="c51">Yu et al., 2008</xref>; <xref ref-type="bibr" rid="c49">Wu et al., 2017</xref>; <xref ref-type="bibr" rid="c31">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="c2">Bjerke et al., 2023</xref>; <xref ref-type="bibr" rid="c38">Schneider et al., 2023</xref></italic></bold>) or topological data analysis (TDA) (<bold><italic><xref ref-type="bibr" rid="c5">Curto and Itskov, 2008</xref>; <xref ref-type="bibr" rid="c36">Rybakken et al., 2019</xref>; <xref ref-type="bibr" rid="c12">Gardner et al., 2022</xref></italic></bold>). Such methods can be used without knowledge of specific covariates. However, single-cell classification is still useful, as it can be used to group neurons into coherent populations with a neural code which is likely correlated to the covariate observed (such as in <bold><italic><xref ref-type="bibr" rid="c12">Gardner et al. (2022</xref></italic></bold>)).</p>
<p>Finally, we emphasize that it is important that the statistical methods used are robust, so that the results they yield can be trusted and replicated.</p>
<sec id="s3a">
<title>Moving forward</title>
<p>Perhaps the most important aspect of these analyses is the data itself. When designing future experiments we would suggest both minimizing the number of possible candidates for covariates while also making these as complete as possible and recording for a long time. This occurs largely at the experimental design phase where care should be taken to limit the number of likely behaviors and engagement of different sensory-motor modalities. For example, simply turning off the lights removes a complex and difficult-to-track set of covariates, while engaging the animal actively in a specific task such as pursuit reduces the number of spurious behaviors that might not occur often enough to be properly studied. Second, it is worth the effort to properly track and invest in constructing compact postural and behavioral variables that account for the strong correlations between, for example, how the head and the body rotate together (<bold><italic><xref ref-type="bibr" rid="c26">Mimica et al., 2018</xref></italic></bold>). Third, the experiment should ensure that each covariate is sampled well. Untangling the potential spurious relationships between our covariates is important to improve our ability to interpret the results we obtain. In this regard we should strive for long experiments with rich and diverse behavior, to ensure the covariate space is properly explored. Longer data sets will also make it easier to account for temporal autocorrelation, as it allows for larger blocks of data for methods such as cross-validation.</p>
<p>With a minimal but hopefully close to a complete set of covariates, we would suggest using the cyclical shift permutation with the general Bonferroni correction for covariate selection. The other methods we study here are computationally more efficient, however, they can either be much more permissive or conservative than expected. If there is reason to be believe that the data is not stationary, an alternative to the cyclical shift should be used. If one suspects that there is a trend throughout the experiment in either behavior or neural activity, the linear shift test proposed by (<bold><italic>Harris, 2020b</italic></bold>) will be more appropriate. An option for trial-based experiments is the session permutation method described by <bold><italic>Harris</italic></bold> (<bold><italic>2020a</italic></bold>), where the shuffled distribution is constructed by repeatedly calculating the test statistic with neural data from the trial considered and behavioral variables from a trial selected randomly from the remaining ones.</p>
<p>If the computational cost of the cyclical shift is too high, we recommend the signed-rank test with the modifications to the cross-validation scheme and testing the new model against the one with the reversed covariate (mSRR<sub>MaxT</sub>). However, the extent to which the overlapping data affects the false positive rates should be further explored, and suitable modifications to account for potentially inflated error rates should be found.</p>
<p>We also suggest checking the false positive rates, regardless of the approach or model, by swapping the covariates from different experiments and constructing simulations. This does not ensure that the model is correct but is a simple way of showing that a seemingly significant result is indeed not (e.g. the case of potential mirror-like cells in mice (<bold><italic><xref ref-type="bibr" rid="c42">Tombaz et al., 2020</xref></italic></bold>)).</p>
<p>We stress again that interpretation of results is difficult, as these can be misleading in situations where there are strong task-dependent relationships between different covariates that may change depending on the behavior of the animal. Unobserved, but relevant variables further complicate the matter. A cell appearing tuned to speed might for instance very well be driven by a specific behavior that only occurs when the animal is moving at certain speeds. Thus when one observes a prominence of tuning to a selection of covariates, it is important to consider what alternative and possibly more likely explanations might exist. This issue is not unique to the GLM framework, and is something that should be taken into consideration when we draw our conclusions regardless of approach.</p>
</sec>
</sec>
<sec id="s4">
<title>Methods and Materials</title>
<sec id="s4a">
<title>Model framework</title>
<p>We consider recordings of neural activity and tracked external covariates over an interval of time. When using a generalized linear model (GLM) the response variable <italic>Y</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, 2, …, <italic>n</italic>, are assumed to be random variables from a distribution in the exponential family, typically either the number of spikes in a time bin, using the Poisson distribution, or a binary variable representing cell being inactive or active, using the Bernoulli distribution. The expected value of <italic>Y</italic><sub><italic>i</italic></sub> conditioned on observing the vector of covariates <italic>X</italic><sub><italic>i</italic></sub> = <italic>x</italic><sub><italic>i</italic></sub> is specified using a link function and functions of the covariates. More specifically,
<disp-formula id="eqn1">
<graphic xlink:href="563994v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>g</italic> is the link function connecting the expected value <italic>μ</italic> and the linear predictor <italic>η</italic> and <italic>f</italic> is an optional function of the covariates. We use a Bernoulli GLM with logit-link, <inline-formula><inline-graphic xlink:href="563994v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> Through the functions <italic>f</italic><sub><italic>j</italic></sub> the covariates can be represented in various ways, for example using binning. We represent our covariates by cubic splines.</p>
<p>In the GLM the responses are additionally assumed to be independent when conditioned on the covariates included in the model, which is unlikely to hold for temporal neural data. The temporal autocorrelation in the <italic>Y</italic><sub><italic>i</italic></sub> might in part be explained by biophysical properties of the neuron at high temporal resolution (time bins with widths of a few milliseconds) (<bold><italic><xref ref-type="bibr" rid="c43">Truccolo et al., 2005</xref></italic></bold>), but will also to a large extent be caused by the omission of relevant covariates (<bold><italic><xref ref-type="bibr" rid="c39">Stevenson, 2018</xref></italic></bold>). See <xref rid="fig1" ref-type="fig">Figure 1</xref> for a demonstration of how this model misspecification, and generally naive use of statistics, can cause trouble.</p>
</sec>
<sec id="s4b">
<title>Forward selection procedure</title>
<p>We use a forward selection procedure similar to that of <bold><italic><xref ref-type="bibr" rid="c16">Hardcastle et al. (2017</xref></italic></bold>), which is based on cross-validated (CV) log-likelihood values and performing a hypothesis test at each step. Starting with a model without covariates (hence modelling a constant mean firing rate), the procedure goes as follows</p>
<list list-type="order">
<list-item><p>Perform CV to find the covariate that improves the model the most by comparing the mean difference of paired CV log-likelihood values</p></list-item>
<list-item><p>Perform a hypothesis test, where the null hypothesis is that the added covariate does not improve the model (i.e. the expected value of the mean difference of paired CV log-likelihood values is 0)</p></list-item>
<list-item><p>If the new model is significantly better than the previous one, continue from step 1, otherwise the previous model is chosen as the final one</p></list-item>
</list>
</sec>
<sec id="s4c">
<title>Cross-validation scheme</title>
<p>Cross-validation is a popular method for model selection in many fields, which seeks to estimate model performance on held out data (<bold><italic><xref ref-type="bibr" rid="c19">Hastie et al., 2001</xref></italic></bold>). The data is partitioned into folds, where each fold once serves as a test set while the model is trained on the remaining folds. Caution is required for data with temporal structure, as otherwise each fold might contain highly correlated data points, resulting in an overly optimistic test performance.</p>
<p>One common approach to account for this is to temporally block the data points and construct folds of whole blocks (<bold><italic><xref ref-type="bibr" rid="c35">Roberts et al., 2017</xref></italic></bold>). In addition one might want to isolate the test fold further, meaning that we remove a few of the neighboring data points from the training set (<bold><italic><xref ref-type="bibr" rid="c34">Racine, 2000</xref></italic></bold>), illustrated in <xref rid="fig9" ref-type="fig">Figure 9</xref>.</p>
<p>In our main analyses on simulated and real data we use <italic>N</italic> = 12000 data points. In the modified CV scheme we use 20 folds, where each fold consists of 4 temporally separated blocks, each thus of 150 consecutive time bins. These 20 ⋅ 4 = 80 blocks are systematically chosen to belong to the 20 folds, meaning the first, 21st, and 41st block belong to fold 1, and so on. For the unmodified CV scheme we use 10 folds and blocks of 150 time bins, meaning each fold consists of 8 blocks. We also use skipping, and exclude folds <italic>i</italic> + 1 and <italic>i</italic> − 1 from the training set when using fold <italic>i</italic> as a test set, meaning the 150 time bins on each side of the blocks belonging to fold <italic>i</italic> are excluded. When fold 1 is used as a test set, fold 20 is excluded from the training set (and vice versa). The last (first) block is also then excluded even though it does not border any of the blocks from fold 1 (20), in order to keep the length of the training sets consistent.</p>
<p><bold><italic><xref ref-type="bibr" rid="c16">Hardcastle et al. (2017</xref></italic></bold>) use 10 folds, which we thus also use for the methods with the un-modified CV scheme (SR and SR<sub>Bonf</sub>). For the methods with a modified CV scheme (mSR<sub>MaxT</sub> and mSRR<sub>MaxT</sub>) we use 20 for two reasons. Firstly, with the added step of skipping neighboring blocks for the CV, more folds means more data is available in each training set, yielding a less biased estimate of the mean difference in log-likelihood values. Secondly, the signed-rank test statistic, and thus the corresponding <italic>p</italic>-value, can only take a finite number of values (2<sup><italic>k</italic></sup> with <italic>k</italic> folds), which with <italic>k</italic> = 10 scales poorly when you correct for multiple comparisons.</p>
</sec>
<sec id="s4d">
<title>Permutation test procedure</title>
<p>Permutation testing can be used when performing a hypothesis test where the distribution of the test statistic under the null hypothesis is not known (<bold><italic><xref ref-type="bibr" rid="c45">Westfall and Young, 1993</xref></italic></bold>). The key property that makes this possible is exchangeability, meaning that the distribution of a vector of random variables is unchanged by a permutation of the indices. This property is not fulfilled for arbitrary permutations when the random vector has temporal structure, since this is distorted. Instead it is common in the field (<bold><italic><xref ref-type="bibr" rid="c14">Grijseels et al., 2021</xref>; <xref ref-type="bibr" rid="c53">Zong et al., 2022</xref>; <xref ref-type="bibr" rid="c20">Høydal et al., 2019</xref></italic></bold>) and also fields involving spatial dependencies (<bold><italic><xref ref-type="bibr" rid="c6">Dale and Fortin, 2002</xref></italic></bold>) to use a restricted subset of possible permutations consisting of cyclical (toroidal in the spatial case) shifts for randomly drawn lags, illustrated in <xref rid="fig10" ref-type="fig">Figure 10</xref>.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10.</label>
<caption><p>Illustration of the cyclical shift permutation.</p></caption>
<graphic xlink:href="563994v1_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The procedure is thus as follows for some permutation test:</p>
<list list-type="order">
<list-item><p>Calculate the test statistic <italic>t</italic></p></list-item>
<list-item><p>Perform <italic>B</italic> permutations to generate a null distribution <inline-formula><inline-graphic xlink:href="563994v1_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, <italic>B</italic></p></list-item>
<list-item><p>Calculate permutation <italic>p</italic>-value as <inline-formula><inline-graphic xlink:href="563994v1_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="563994v1_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the number of times the permuted sample yields a test statistic larger than or equal to the originally observed one</p></list-item>
<list-item><p>If doing Bonferroni correction, calculate adjusted <italic>p</italic>-value as <italic>p</italic><sub>adj</sub> = <italic>p</italic> ⋅ <italic>m</italic></p></list-item>
</list>
<p>Note the adjustment <inline-formula><inline-graphic xlink:href="563994v1_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> as opposed to <inline-formula><inline-graphic xlink:href="563994v1_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which we do to prevent nonsensically reporting a <italic>p</italic>-value of 0, and also because the latter is not valid (<bold><italic><xref ref-type="bibr" rid="c32">Phipson and Smyth, 2010</xref></italic></bold>).</p>
</sec>
<sec id="s4e">
<title>Hypothesis test based on the Wilcoxon signed-rank test</title>
<p>The signed-rank test methods used are based on differences of cross-validated log-likelihood values. We use <italic>k</italic> = 20 or <italic>k</italic> = 10 fold CV, yielding values <italic>A</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, …, <italic>k</italic> for the current model and values <italic>B</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, …, <italic>k</italic> for the candidate model. We wish to test if the <italic>B</italic><sub><italic>i</italic></sub> are significantly larger than the <italic>A</italic><sub><italic>i</italic></sub>.</p>
<p>We consider the differences <italic>D</italic><sub><italic>i</italic></sub> = <italic>B</italic><sub><italic>i</italic></sub> − <italic>A</italic><sub><italic>i</italic></sub>. The null hypothesis is then <italic>H</italic><sub>0</sub>: the distribution of the <italic>D</italic><sub><italic>i</italic></sub> is symmetric about <italic>μ</italic> = 0, where <italic>μ</italic> is the expected value of each <italic>D</italic><sub><italic>i</italic></sub>. The test statistic used is the signed-rank:
<disp-formula id="eqn2">
<graphic xlink:href="563994v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>R</italic><sub><italic>i</italic></sub> is the rank of the difference <italic>D</italic><sub><italic>i</italic></sub> when they are ordered by their absolute values, so that the smallest |<italic>D</italic><sub><italic>i</italic></sub>| has rank 1 and the largest has rank <italic>k</italic>.</p>
<p>The observed test statistic is then either compared to a theoretical distribution (SR and SR<sub>Bonf</sub>), utilizing the <monospace>wilcox.test</monospace> function from the <monospace>stats</monospace> package in R, or to an empirical distribution constructed by permutations where we flip signs (mSR<sub>MaxT</sub> and mSRR<sub>MaxT</sub>), meaning we use the permutation <italic>ρ</italic>(<italic>D</italic><sub><italic>i</italic></sub>) = <italic>u</italic> ⋅ <italic>D</italic><sub><italic>i</italic></sub>, where <italic>u</italic> is a random vector where each element is independently sampled from {−1, 1} with equal probabilities. We run 999 permutations for this method.</p>
<p>Note that under the null hypothesis the Wilcoxon signed-rank test assumes that the distribution of the CV log-likelihood differences are centered at 0. When comparing a model to the one with an additional and irrelevant covariate, however, the new model will overfit and thus generally yield negative CV log-likelihood differences. This means that the resulting test will be overly conservative (provided the CV is done properly, since improper CV can yield large values even when overfitting, see <xref rid="fig1" ref-type="fig">Figure 1</xref>.</p>
<p>Because of this we also consider the variation where the proposed new model is compared not to the previous one, but to a model where the additional covariate is included in reversed order (mSRR<sub>MaxT</sub>). Under the null hypothesis that the added covariate has no effect on the response, and assuming the distribution of the covariate to be stationary, these two models should explain the response equally well, and hence the differences in CV log-likelihood values will be centered around 0.</p>
<p>Improper CV will also violate the assumption of the signed-rank test that the differences themselves must be independent. Without skipping and without or with too small blocks, neighboring folds in the CV will be highly correlated, which also means that the CV log-likelihood differences will be, further motivating the need for these adjustments. Even with these adjustments, however, there is overlap between the training sets in each iteration of the CV, potentially inducing correlation between the CV log-likelihood differences. The signed-rank test does appear robust to this problem in both the simulated and real data analyses in this work, but we do not know under what conditions it will be a problem.</p>
</sec>
<sec id="s4f">
<title>Hypothesis test based on cyclical shifts</title>
<p>For the cyclical shift permutation test the in-sample log-likelihood increase is used as a test statistic, and the covariate that is being added is shifted cyclically to generate a distribution of this test statistic under the null hypothesis. The covariate vector (or each column representing it if the covariate has multiple parameters) is shifted in time by a random lag, and the part that is shifted beyond the original end of the session is wrapped to the beginning, illustrated in <xref rid="fig10" ref-type="fig">Figure 10</xref>. We thus assume that the distribution of the covariate added is stationary.</p>
<p>When comparing a current model (A) and model (B) with the added covariate, our test statistic becomes
<disp-formula id="eqn3">
<graphic xlink:href="563994v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>ℓ</italic>(<italic>y, x</italic>) is the log-likelihood of the GLM with coefficients <italic>β</italic> for the Bernoulli case, i.e.
<disp-formula id="eqn4">
<graphic xlink:href="563994v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="563994v1_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn6">
<graphic xlink:href="563994v1_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Without temporal structure and unobserved covariates, 2<italic>T</italic><sub><italic>CS</italic></sub>(<italic>Y, X</italic>) (deviance of the Bernoulli GLM) asymptotically follows a <italic>χ</italic><sup>2</sup> distribution with degrees of freedom equal to the difference in number of parameters between the two models (<bold><italic><xref ref-type="bibr" rid="c47">Wilks, 1938</xref></italic></bold>). However, as we illustrated in <xref rid="fig1" ref-type="fig">Figure 1</xref>, this is not the case when these challenges are present, which motivates the use of a permutation test approach to estimate the actual distribution of the test statistic.</p>
<p>We permute by drawing a random lag <italic>l</italic> uniformly from {150, 151, …, <italic>n</italic> − 150}, so the permuted version of the covariate <italic>x</italic> = (<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x</italic><sub><italic>n</italic></sub>) becomes <inline-formula><inline-graphic xlink:href="563994v1_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The permutations are more costly with the cyclical shift than the permutations of signs, because the GLM has to be retrained in each permutation. For our analysis we ran 119 permutations for this method, which means the uncertainty of the obtained <italic>p</italic>-values are slightly higher than for the other permutation method.</p>
<p>The minimum lag of 150 is part of an additional adjustment when performing the cyclical shifts, similar to the skipping done for the CV, as a measure against concerns raised by <bold><italic>Harris</italic></bold> (<bold><italic>2020a</italic></bold>) about the resulting discontinuities at the ends and at the seam of the shifted vector. When calculating the initial test statistic of the unshifted data, we exclude 75 data points in each end and 150 data points in the middle, and when calculating the test statistic of shifted data we exclude 75 data points on each side of the seam and 75 data points at each end, thus there are gaps of 150 data points around each discontinuity, similar to what we end up with during the skipping in the CV.</p>
</sec>
<sec id="s4g">
<title>Accounting for multiple comparisons when using permutation tests</title>
<p>In each step of the forward selection procedure we are considering multiple covariates to include. Conducting a hypothesis test only for the best performing covariate is a form of selective inference (<bold><italic><xref ref-type="bibr" rid="c41">Taylor and Tibshirani, 2015</xref></italic></bold>), a well known problem with few good and few general solutions. In a step of the forward selection procedure where <italic>m</italic> covariates are considered we are ignoring <italic>m</italic> − 1 hypothesis tests, meaning we essentially have a multiple testing problem (<bold><italic><xref ref-type="bibr" rid="c13">Goeman and Solari, 2014</xref></italic></bold>). As illustrated in <xref rid="fig11" ref-type="fig">Figure 11</xref>, this leads to an inflated type I error rate (we falsely reject true hypotheses more often than the set significance level <italic>α</italic>).</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure 11.</label>
<caption><p>Illustration of how multiple comparisons results in too small <italic>p</italic>-values when not accounted for (<bold>A</bold>), and potential solutions using the Bonferroni (<bold>B</bold>) and maxT (<bold>C</bold>) correction methods.</p></caption>
<graphic xlink:href="563994v1_fig11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We solve this issue by correcting our hypothesis tests to obtain familywise error rate (FWER) control in each step of the forward selection procedure. This means that we ensure at each step that the probability is at most <italic>α</italic> for rejecting any true hypothesis. A Bonferroni correction (<bold><italic><xref ref-type="bibr" rid="c13">Goeman and Solari, 2014</xref></italic></bold>) may be used and is always valid, although often conservative. For permutation testing we may however also use the maxT method by <bold><italic>Westfall and Young</italic></bold> (<bold><italic>1993</italic></bold>), making the adjustment not on the <italic>p</italic>-values, but by specifically using max <italic>T</italic><sub><italic>l</italic></sub> as the test statistic, where <italic>T</italic><sub><italic>l</italic></sub>, <italic>l</italic> = 1, …, <italic>m</italic> are the individual test statistics for <italic>m</italic> covariates. Thus we create a shuffled distribution by calculating the maximum statistic over all candidate covariates in each permutation (meaning that it is not necessarily the same covariate that yields the maximum in each permutation), and comparing the observed maximum to this distribution.</p>
<p>For the sign-rank test method where we use sign flipping (mSR<sub>MaxT</sub> and mSRR<sub>MaxT</sub>) we use the maxT correction, meaning our resulting test statistic is
<disp-formula id="eqn7">
<graphic xlink:href="563994v1_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where each <italic>D</italic><sup><italic>l</italic></sup> is the vector containing the differences in cross-validated log-likelihood values between the current model and one of <italic>m</italic> candidate models (in our case <italic>m</italic> is 3 in the first step of the forward selection procedure, 2 in the second, and then 1 in the third, where only one candidate is tested and the corrected test statistic is just the single test statistic).</p>
<p>For the cyclical shift method we use the Bonferroni correction, because the in-sample log-likelihood that we use as a test statistic scales differently for covariates that are potentially parameterized with very different numbers of parameters. We also use the Bonferroni correction for the signed-rank test based method where we use the theoretical distribution to obtain the <italic>p</italic>-value (SR<sub>Bonf</sub>). In these cases the adjustment is made directly on the attained <italic>p</italic>-value, multiplying it by the number of candidates considered, <italic>m</italic>.</p>
<p>When using the Bonferroni correction it is important to keep in mind that when considering an increasing number of covariates, one needs to scale the number of permutations performed accordingly. If we consider 20 covariates, the correction is <italic>p</italic> ⋅ 20, meaning that to be able to obtain significant results with <italic>α</italic> = 0.05 we would need at least 399 permutations (as with <italic>B</italic> = 399 permutations and <italic>m</italic> = 20 covariates the smallest <italic>p</italic>-value we can obtain is <inline-formula><inline-graphic xlink:href="563994v1_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula>). Ideally we would prefer even more permutations, to reduce the uncertainty of the estimated <italic>p</italic>-value. While this means the computational complexity can grow large, it also serves as an incentive to limit the number of covariates one tests simultaneously.</p>
<p><bold><italic><xref ref-type="bibr" rid="c16">Hardcastle et al. (2017</xref></italic></bold>) limit the inflated familywise error rate by performing an additional hypothesis test at the end of the forward selection procedure, where the performance of the resulting model is compared to an empty one, concluding with no tuning if significance is not reached. In any given step there is however no correction, meaning the procedure may go on too far, and include enough irrelevant variables that the final test concludes that the final model should be discarded, even when there could be an intermediary model containing variables relevant to the neuron’s activity. Correction in each step is therefore preferable.</p>
</sec>
<sec id="s4h">
<title>Details on the simulated data</title>
<p>Here we describe the procedure we use to construct our simulated data. For our simulations we use <italic>N</italic> = 12000 data points. Covariates are simulated with temporal structure by first drawing <italic>N</italic> independent uniform variables from the interval (−2.5, 2.5), and then passing this vector through an exponential smoothing filter with scale parameter <italic>β</italic> = 20 (chosen to construct data appearing similar to the neural and behavioral data from <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>), through visual inspection of the covariates themselves, and their autocorrelation function (see <xref rid="fig12" ref-type="fig">Figure 12</xref>)). Finally the covariates are bounded between −0.3 and 0.3 by replacing any exceeding value <italic>v</italic> &gt; 0.3 or <italic>v</italic> &lt; −0.3 with 0.3 − (<italic>v</italic> − 0.3) or −0.3 + (−0.3 − <italic>v</italic>), respectively, thus reflecting them back into the domain.</p>
<fig id="fig12" position="float" fig-type="figure">
<label>Figure 12.</label>
<caption><p>Comparison of real and simulated data. <bold>A</bold>: Animal X-position over time. <bold>B</bold>: Autocorrelation function of the animal’s X-position. <bold>C</bold>: X- and Y-position from the real mouse data. <bold>D</bold>: Simulated X-position. <bold>E</bold>: Autocorrelation function of the simulated X-position. <bold>F</bold>: Simulated X- and Y-position.</p></caption>
<graphic xlink:href="563994v1_fig12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig13" position="float" fig-type="figure">
<label>Figure 13.</label>
<caption><p>Illustration of how the event probability is constructed for the simulated data. <bold>A</bold>: The contribution X and Y have on the probability of an event, <italic>p</italic><sub><italic>xy</italic></sub>. <bold>B</bold>: The contribution from <italic>h, p</italic><sub><italic>h</italic></sub>. <bold>C</bold>: The resulting probability as a function of X and Y for one set of simulated (<italic>x, y, h</italic>). <bold>D</bold>: A smoothed ratemap of a sample of Bernoulli variables generated from <italic>p</italic>(<italic>x, y, h</italic>).</p></caption>
<graphic xlink:href="563994v1_fig13.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig14" position="float" fig-type="figure">
<label>Figure 14.</label>
<caption><p>Illustration of calcium data. <bold>A</bold>: Raster of cell activity. <bold>B</bold>: Head direction of the animal over time. <bold>C</bold>: Animal position over time. <bold>D</bold>: Ratemap of the cell activity over position for a single neuron.</p></caption>
<graphic xlink:href="563994v1_fig14.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In a single simulation we generate 5 such covariates independently of each other. One of these, <italic>h</italic>, is treated as unobserved with respect to the covariate selection process, but is used to generate the response variable. The next two are treated as observed, one-dimensional covariates, while the last two are combined and treated as one two-dimensional covariate with <italic>x</italic>- and <italic>y</italic>-coordinates. When fitting the GLM we represent the covariates using natural cubic splines, using the R packages splines. We use 5 evenly spaced internal knots for the one-dimensional covariates and 2 evenly spaced internal knots for each dimension of the two-dimensional variable.</p>
<p>We generate two sets of simulated data, one where only <italic>h</italic> affects the underlying probability of firing, and one where the two-dimensional covariate also affects the probability. We generate new covariates for each simulated response variable (cell activity). The probability in each bin is first modelled using Gaussian bumps plus a base probability. The contribution from <italic>h</italic> is
<disp-formula id="eqn8">
<graphic xlink:href="563994v1_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the center of the bump is at <italic>μ</italic><sub><italic>h</italic></sub> = 0.1 and the standard deviation is <italic>σ</italic><sub><italic>h</italic></sub> = 0.06. For the data where the position (<italic>x, y</italic>) is relevant, the contribution, constructed using two two-dimensional Gaussian bumps, is
<disp-formula id="eqn9">
<graphic xlink:href="563994v1_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we have centers at (<italic>μ</italic><sub>1</sub>, <italic>μ</italic><sub>1</sub>) = (−0.15, −0.15) and (<italic>μ</italic><sub>2</sub>, <italic>μ</italic><sub>2</sub>) = (0.15, 0.15) and standard deviation <italic>σ</italic> = 0.06. The combined probability is then
<disp-formula id="eqn10">
<graphic xlink:href="563994v1_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
when position is relevant, and
<disp-formula id="eqn11">
<graphic xlink:href="563994v1_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
otherwise, where <italic>p</italic><sub>0</sub> = 0.03 is the base probability, and <italic>r</italic> is a scaling parameter set to 0.25 for the main analysis, in order to yield an overall rate of events similar to the real data. The parameters are set by trial and error in an attempt to construct ratemaps similar to what one could observe with actual neural data, albeit slightly less complex than the grid cells observed in the real data we explore in order to reduce computational labor. By increasing <italic>r</italic> we can make the response depend more strongly on the hidden covariate and on position, if it is included. With <italic>r</italic> = 1 all the methods compared in Results on simulated data will correctly classify the cells as position-tuned in essentially every trial (meaning we cannot say anything meaningful about their differences in test power).</p>
</sec>
<sec id="s4i">
<title>Details on the calcium data</title>
<p>The data used for the analysis of calcium data is from <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>) and <bold><italic><xref ref-type="bibr" rid="c28">Obenhaus et al. (2022</xref></italic></bold>), where they use a 2-photon miniscope to obtain calcium images of freely moving mice. We are considering specifically the dataset from mouse 97045, recorded March 17th, 2021, with calcium imaging of the medial entorhinal cortex. The neural activity is sampled at 7.5Hz and deconvolved using Suite2P (<bold><italic><xref ref-type="bibr" rid="c30">Pachitariu et al., 2016</xref></italic></bold>). We binarize their filtered events (setting any non-zero value to 1) and treat these as Bernoulli variables. They analyze the tracking data, which is sampled at 15Hz, using DeepLabCut (<bold><italic><xref ref-type="bibr" rid="c24">Mathis et al., 2018</xref></italic></bold>), and combine the tracking and neural data using their own software, NATEX. For our analyses we have used the 7.5Hz rate, combining every two time frames of the tracking data by taking the average.</p>
<p>The session we have used lasts for one hour, of which we use 12000 time bins (26 minutes and 40 seconds) of the first half as our correctly matched data, and the neural data from the 12000 first time bins of the second half to create our mismatched data when combined with the tracking of the first part. The session used has a total of 488 neurons, of which we consider the 249 neurons that have an event in at least 2% of the time bins in each of our two 12000 time bin parts. 147 of these are classified by <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>) as grid cells.</p>
<p>When fitting the GLM we represent the covariates using natural cubic splines. For head direction we use periodic B-splines, with the R package pbs. For head direction we use 6 evenly spaced internal knots and for speed we use 5, which results in the same number of parameters since the former is periodic. For position we use 4 evenly spaced internal knots for each dimension.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We would like to thank Ingeborg Hem and Mette Langaas for their comments on the manuscript, and thank the Department of Mathematical Sciences (NTNU). We would also like to thank <bold><italic><xref ref-type="bibr" rid="c53">Zong et al. (2022</xref></italic></bold>) and <bold><italic><xref ref-type="bibr" rid="c28">Obenhaus et al. (2022</xref></italic></bold>) for making their data publicly available. This work was supported by a grant by the Research Council of Norway (iMOD, NFR grant #325114).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Bassett</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Sporns</surname> <given-names>O.</given-names></string-name> <article-title>Network neuroscience</article-title>. <source>Nature neuroscience</source>. <year>2017</year>; <volume>20</volume>(<issue>3</issue>):<fpage>353</fpage>–<lpage>364</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Bjerke</surname> <given-names>M</given-names></string-name>, <string-name><surname>Schott</surname> <given-names>L</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>KT</given-names></string-name>, <string-name><surname>Battistin</surname> <given-names>C</given-names></string-name>, <string-name><surname>Klindt</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Dunn</surname> <given-names>BA</given-names></string-name>. <article-title>Understanding neural coding on latent manifolds by sharing features and dividing ensembles</article-title>. <source>International Conference on Learning Representations</source>. <year>2023</year>;.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><string-name><surname>Casella</surname> <given-names>G</given-names></string-name>, <string-name><surname>Berger</surname> <given-names>RL</given-names></string-name>. <article-title>Statistical inference</article-title>. <source>Cengage Learning</source>; <year>2002</year>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Cunningham</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>BM</given-names></string-name>. <article-title>Dimensionality reduction for large-scale neural recordings</article-title>. <source>Nature neuroscience</source>. <year>2014</year>; <volume>17</volume>(<issue>11</issue>):<fpage>1500</fpage>–<lpage>1509</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Curto</surname> <given-names>C</given-names></string-name>, <string-name><surname>Itskov</surname> <given-names>V.</given-names></string-name> <article-title>Cell groups reveal structure of stimulus space</article-title>.<source>PLoS computational biology</source>. <year>2008</year>; <volume>4</volume>(<issue>10</issue>):<fpage>e1000205</fpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Dale</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Fortin</surname> <given-names>MJ</given-names></string-name>. <article-title>Spatial autocorrelation and statistical tests in ecology</article-title>. <source>Ecoscience</source>. <year>2002</year>; <volume>9</volume>(<issue>2</issue>):<fpage>162</fpage>–<lpage>167</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Das</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fiete</surname> <given-names>IR</given-names></string-name>. <article-title>Systematic errors in connectivity inferred from activity in strongly recurrent networks</article-title>. <source>Nature Neuroscience</source>. <year>2020</year>; <volume>23</volume>(<issue>10</issue>):<fpage>1286</fpage>–<lpage>1296</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Datta</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Branson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Perona</surname> <given-names>P</given-names></string-name>, <string-name><surname>Leifer</surname> <given-names>A.</given-names></string-name> <article-title>Computational neuroethology: a call to action</article-title>. <source>Neuron</source>. <year>2019</year>; <volume>104</volume>(<issue>1</issue>):<fpage>11</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Dietterich</surname> <given-names>TG.</given-names></string-name> <article-title>Approximate statistical tests for comparing supervised classification learning algorithms</article-title>. <source>Neural computation</source>. <year>1998</year>; <volume>10</volume>(<issue>7</issue>):<fpage>1895</fpage>–<lpage>1923</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Dunn</surname> <given-names>B</given-names></string-name>, <string-name><surname>Battistin</surname> <given-names>C.</given-names></string-name> <article-title>The appropriateness of ignorance in the inverse kinetic Ising model</article-title>. <source>Journal of Physics A: Mathematical and Theoretical</source>. <year>2017</year>; <volume>50</volume>(<issue>12</issue>):<fpage>124002</fpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Gallego</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Perich</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Solla</surname> <given-names>SA</given-names></string-name>. <article-title>Neural manifolds for the control of movement</article-title>. <source>Neuron</source>. <year>2017</year>; <volume>94</volume>(<issue>5</issue>):<fpage>978</fpage>–<lpage>984</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Gardner</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Hermansen</surname> <given-names>E</given-names></string-name>, <string-name><surname>Pachitariu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Burak</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Baas</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Dunn</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name>. <article-title>Toroidal topology of population activity in grid cells</article-title>. <source>Nature</source>. <year>2022</year>; <volume>602</volume>(<issue>7895</issue>):<fpage>123</fpage>–<lpage>128</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Goeman</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Solari</surname> <given-names>A.</given-names></string-name> <article-title>Multiple hypothesis testing in genomics</article-title>. <source>Statistics in medicine</source>. <year>2014</year>; <volume>33</volume>(<issue>11</issue>):<fpage>1946</fpage>–<lpage>1978</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Grijseels</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Shaw</surname> <given-names>K</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hall</surname> <given-names>CN</given-names></string-name>. <article-title>Choice of method of place cell classification determines the population of cells identified</article-title>. <source>PLoS Computational Biology</source>. <year>2021</year>; <volume>17</volume>(<issue>7</issue>):<fpage>e1008835</fpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Hafting</surname> <given-names>T</given-names></string-name>, <string-name><surname>Fyhn</surname> <given-names>M</given-names></string-name>, <string-name><surname>Molden</surname> <given-names>S</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name>. <article-title>Microstructure of a spatial map in the entorhinal cortex</article-title>. <source>Nature</source>. <year>2005</year>; <volume>436</volume>(<issue>7052</issue>):<fpage>801</fpage>–<lpage>806</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Hardcastle</surname> <given-names>K</given-names></string-name>, <string-name><surname>Maheswaranathan</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ganguli</surname> <given-names>S</given-names></string-name>, <string-name><surname>Giocomo</surname> <given-names>LM</given-names></string-name>. <article-title>A multiplexed, heterogeneous, and adaptive code for navigation in medial entorhinal cortex</article-title>. <source>Neuron</source>. <year>2017</year>; <volume>94</volume>(<issue>2</issue>):<fpage>375</fpage>–<lpage>387</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="other"><string-name><surname>Harris</surname> <given-names>KD</given-names></string-name>. <article-title>Nonsense correlations in neuroscience</article-title>. <source>bioRxiv</source>. <year>2020</year>;.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="other"><string-name><surname>Harris</surname> <given-names>KD</given-names></string-name>. <article-title>A Shift Test for Independence in Generic Time Series</article-title>. arXiv preprint arXiv:201206862. <year>2020</year>;.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="book"><string-name><surname>Hastie</surname> <given-names>T</given-names></string-name>, <string-name><surname>Tibshirani</surname> <given-names>R</given-names></string-name>, <string-name><surname>Friedman</surname> <given-names>J.</given-names></string-name> <source>The elements of statistical learning</source>, vol. <volume>1</volume>. <publisher-name>Springer</publisher-name>; <year>2001</year>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Høydal</surname> <given-names>ØA</given-names></string-name>, <string-name><surname>Skytøen</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Andersson</surname> <given-names>SO</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name>. <article-title>Object-vector coding in the medial entorhinal cortex</article-title>. <source>Nature</source>. <year>2019</year>; <volume>568</volume>(<issue>7752</issue>):<fpage>400</fpage>–<lpage>404</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Hubel</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Wiesel</surname> <given-names>TN</given-names></string-name>. <article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title>. <source>The Journal of physiology</source>. <year>1959</year>; <volume>148</volume>(<issue>3</issue>):<fpage>574</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Keat</surname> <given-names>J</given-names></string-name>, <string-name><surname>Reinagel</surname> <given-names>P</given-names></string-name>, <string-name><surname>Reid</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Meister</surname> <given-names>M.</given-names></string-name> <article-title>Predicting every spike: a model for the responses of visual neurons</article-title>. <source>Neuron</source>. <year>2001</year>; <volume>30</volume>(<issue>3</issue>):<fpage>803</fpage>–<lpage>817</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Lawrence</surname> <given-names>N.</given-names></string-name> <article-title>Gaussian process latent variable models for visualisation of high dimensional data</article-title>. <source>Advances in neural information processing systems</source>. <year>2003</year>; <volume>16</volume>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Mathis</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mamidanna</surname> <given-names>P</given-names></string-name>, <string-name><surname>Cury</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Abe</surname> <given-names>T</given-names></string-name>, <string-name><surname>Murthy</surname> <given-names>VN</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M.</given-names></string-name> <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature neuroscience</source>. <year>2018</year>; <volume>21</volume>(<issue>9</issue>):<fpage>1281</fpage>–<lpage>1289</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>McCullagh</surname> <given-names>P</given-names></string-name>, <string-name><surname>Nelder</surname> <given-names>JA</given-names></string-name>. <article-title>Monographs on statistics and applied probability</article-title>. <source>Generalized linear models</source>. <year>1989</year>; <volume>37</volume>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Mimica</surname> <given-names>B</given-names></string-name>, <string-name><surname>Dunn</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Tombaz</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bojja</surname> <given-names>VS</given-names></string-name>, <string-name><surname>Whitlock</surname> <given-names>JR</given-names></string-name>. <article-title>Efficient cortical coding of 3D posture in freely behaving rats</article-title>. <source>Science</source>. <year>2018</year>; <volume>362</volume>(<issue>6414</issue>):<fpage>584</fpage>–<lpage>589</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Mimica</surname> <given-names>B</given-names></string-name>, <string-name><surname>Tombaz</surname> <given-names>T</given-names></string-name>, <string-name><surname>Battistin</surname> <given-names>C</given-names></string-name>, <string-name><surname>Fuglstad</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Dunn</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Whitlock</surname> <given-names>JR</given-names></string-name>. <article-title>Behavioral decomposition reveals rich encoding structure employed across neocortex in rats</article-title>. <source>Nature Communications</source>. <year>2023</year>; <volume>14</volume>(<issue>1</issue>):<fpage>3947</fpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Obenhaus</surname> <given-names>HA</given-names></string-name>, <string-name><surname>Zong</surname> <given-names>W</given-names></string-name>, <string-name><surname>Jacobsen</surname> <given-names>RI</given-names></string-name>, <string-name><surname>Rose</surname> <given-names>T</given-names></string-name>, <string-name><surname>Donato</surname> <given-names>F</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Cheng</surname> <given-names>H</given-names></string-name>, <string-name><surname>Bonhoeffer</surname> <given-names>T</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name>. <article-title>Functional network topography of the medial entorhinal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2022</year>; <volume>119</volume>(<issue>7</issue>):<fpage>e2121655119</fpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="other"><string-name><surname>O’Keefe</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dostrovsky</surname> <given-names>J.</given-names></string-name> <article-title>The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat</article-title>. <source>Brain research</source>. <year>1971</year>;.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="other"><string-name><surname>Pachitariu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Stringer</surname> <given-names>C</given-names></string-name>, <string-name><surname>Schröder</surname> <given-names>S</given-names></string-name>, <string-name><surname>Dipoppa</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rossi</surname> <given-names>LF</given-names></string-name>, <string-name><surname>Carandini</surname> <given-names>M</given-names></string-name>, <string-name><surname>Harris</surname> <given-names>KD</given-names></string-name>. <article-title>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</article-title>. <source>BioRxiv</source>. <year>2016</year>; p. <fpage>061507</fpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Pandarinath</surname> <given-names>C</given-names></string-name>, <string-name><surname>O’Shea</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jozefowicz</surname> <given-names>R</given-names></string-name>, <string-name><surname>Stavisky</surname> <given-names>SD</given-names></string-name>, <string-name><surname>Kao</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Trautmann</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Kaufman</surname> <given-names>MT</given-names></string-name>, <string-name><surname>Ryu</surname> <given-names>SI</given-names></string-name>, <string-name><surname>Hochberg</surname> <given-names>LR</given-names></string-name>, <etal>et al.</etal> <article-title>Inferring single-trial neural population dynamics using sequential auto-encoder</article-title>s. <source>Nature methods</source>. <year>2018</year>; <volume>15</volume>(<issue>10</issue>):<fpage>805</fpage>–<lpage>815</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Phipson</surname> <given-names>B</given-names></string-name>, <string-name><surname>Smyth</surname> <given-names>GK</given-names></string-name>. <article-title>Permutation P-values should never be zero: calculating exact P-values when permutations are randomly drawn</article-title>. <source>Statistical applications in genetics and molecular biology</source>. <year>2010</year>; <volume>9</volume>(<issue>1</issue>).</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="book"><collab>R Core Team</collab>. <chapter-title>R: A Language and Environment for Statistical Computing</chapter-title>. <source>R Foundation for Statistical Computing</source>, <publisher-loc>Vienna, Austria</publisher-loc>; <year>2019</year>, <ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Racine</surname> <given-names>J.</given-names></string-name> <article-title>Consistent cross-validatory model-selection for dependent data: hv-block cross-validation</article-title>. <source>Journal of econometrics</source>. <year>2000</year>; <volume>99</volume>(<issue>1</issue>):<fpage>39</fpage>–<lpage>61</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Roberts</surname> <given-names>DR</given-names></string-name>, <string-name><surname>Bahn</surname> <given-names>V</given-names></string-name>, <string-name><surname>Ciuti</surname> <given-names>S</given-names></string-name>, <string-name><surname>Boyce</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Elith</surname> <given-names>J</given-names></string-name>, <string-name><surname>Guillera-Arroita</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hauenstein</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lahoz-Monfort</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Schröder</surname> <given-names>B</given-names></string-name>, <string-name><surname>Thuiller</surname> <given-names>W</given-names></string-name>, <etal>et al.</etal> <article-title>Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure</article-title>. <source>Ecography</source>. <year>2017</year>; <volume>40</volume>(<issue>8</issue>):<fpage>913</fpage>–<lpage>929</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Rybakken</surname> <given-names>E</given-names></string-name>, <string-name><surname>Baas</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dunn</surname> <given-names>B.</given-names></string-name> <article-title>Decoding of neural data using cohomological feature extraction</article-title>. <source>Neural computation</source>. <year>2019</year>; <volume>31</volume>(<issue>1</issue>):<fpage>68</fpage>–<lpage>93</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Sargolini</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fyhn</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hafting</surname> <given-names>T</given-names></string-name>, <string-name><surname>McNaughton</surname> <given-names>BL</given-names></string-name>, <string-name><surname>Witter</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name>. <article-title>Conjunctive representation of position, direction, and velocity in entorhinal cortex</article-title>. <source>Science</source>. <year>2006</year>; <volume>312</volume>(<issue>5774</issue>):<fpage>758</fpage>–<lpage>762</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="other"><string-name><surname>Schneider</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>MW</given-names></string-name>. <article-title>Learnable latent embeddings for joint behavioural and neural analysis</article-title>. <source>Nature</source>. <year>2023</year>; p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Stevenson</surname> <given-names>IH</given-names></string-name>. <article-title>Omitted variable bias in GLMs of neural spiking activity</article-title>. <source>Neural computation</source>. <year>2018</year>; <volume>30</volume>(<issue>12</issue>):<fpage>3227</fpage>–<lpage>3258</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Taube</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Muller</surname> <given-names>RU</given-names></string-name>, <string-name><surname>Ranck</surname> <given-names>JB</given-names></string-name>. <article-title>Head-direction cells recorded from the postsubiculum in freely moving rats. I. Description and quantitative analysis</article-title>. <source>Journal of Neuroscience</source>. <year>1990</year>; <volume>10</volume>(<issue>2</issue>):<fpage>420</fpage>–<lpage>435</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Taylor</surname> <given-names>J</given-names></string-name>, <string-name><surname>Tibshirani</surname> <given-names>RJ</given-names></string-name>. <article-title>Statistical learning and selective inference</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year>; <volume>112</volume>(<issue>25</issue>):<fpage>7629</fpage>–<lpage>7634</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Tombaz</surname> <given-names>T</given-names></string-name>, <string-name><surname>Dunn</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Hovde</surname> <given-names>K</given-names></string-name>, <string-name><surname>Cubero</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Mimica</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mamidanna</surname> <given-names>P</given-names></string-name>, <string-name><surname>Roudi</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Whitlock</surname> <given-names>JR</given-names></string-name>. <article-title>Action representation in the mouse parieto-frontal network</article-title>. <source>Scientific reports</source>. <year>2020</year>; <volume>10</volume>(<issue>1</issue>):<fpage>5559</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Truccolo</surname> <given-names>W</given-names></string-name>, <string-name><surname>Eden</surname> <given-names>UT</given-names></string-name>, <string-name><surname>Fellows</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Donoghue</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Brown</surname> <given-names>EN</given-names></string-name>. <article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects</article-title>. <source>Journal of neurophysiology</source>. <year>2005</year>; <volume>93</volume>(<issue>2</issue>):<fpage>1074</fpage>–<lpage>1089</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Weber</surname> <given-names>AI</given-names></string-name>, <string-name><surname>Pillow</surname> <given-names>JW</given-names></string-name>. <article-title>Capturing the dynamical repertoire of single neurons with generalized linear models</article-title>. <source>Neural computation</source>. <year>2017</year>; <volume>29</volume>(<issue>12</issue>):<fpage>3260</fpage>–<lpage>3289</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="book"><string-name><surname>Westfall</surname> <given-names>PH</given-names></string-name>, <string-name><surname>Young</surname> <given-names>SS</given-names></string-name>. <source>Resampling-based multiple testing: Examples and methods for p-value adjustment</source>, vol. <volume>279</volume>. <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>1993</year>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Wilcoxon</surname> <given-names>F.</given-names></string-name> <article-title>Individual comparisons by ranking methods</article-title>. <source>Biometric Bulletin</source>. <year>1945</year>; <volume>1</volume>(<issue>6</issue>):<fpage>80</fpage>–<lpage>83</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Wilks</surname> <given-names>SS</given-names></string-name>. <article-title>The large-sample distribution of the likelihood ratio for testing composite hypotheses</article-title>. <source>The annals of mathematical statistics</source>. <year>1938</year>; <volume>9</volume>(<issue>1</issue>):<fpage>60</fpage>–<lpage>62</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname> <given-names>HR</given-names></string-name>. <article-title>Simplified dynamics of human and mammalian neocortical neurons</article-title>. <source>Journal of theoretical biology</source>. <year>1999</year>; <volume>200</volume>(<issue>4</issue>):<fpage>375</fpage>–<lpage>388</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Wu</surname> <given-names>A</given-names></string-name>, <string-name><surname>Roy</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Keeley</surname> <given-names>S</given-names></string-name>, <string-name><surname>Pillow</surname> <given-names>JW</given-names></string-name>. <article-title>Gaussian process based nonlinear latent structure discovery in multivariate spike train data</article-title>. <source>Advances in neural information processing systems</source>. <year>2017</year>; <volume>30</volume>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>York</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Carreira-Rosario</surname> <given-names>A</given-names></string-name>, <string-name><surname>Giocomo</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Clandinin</surname> <given-names>TR</given-names></string-name>. <article-title>Flexible analysis of animal behavior via time-resolved manifold embedding</article-title>. <source>BioRxiv</source>. <year>2020</year>; p. <fpage>2020</fpage>–<lpage>09</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Yu</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Cunningham</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Santhanam</surname> <given-names>G</given-names></string-name>, <string-name><surname>Ryu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Shenoy</surname> <given-names>KV</given-names></string-name>, <string-name><surname>Sahani</surname> <given-names>M.</given-names></string-name> <article-title>Gaussian-process factor analysis for lowdimensional single-trial analysis of neural population activity</article-title>. <source>Advances in neural information processing systems</source>. <year>2008</year>; <volume>21</volume>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Yule</surname> <given-names>GU</given-names></string-name>. <article-title>Why do we sometimes get nonsense-correlations between Time-Series?–a study in sampling and the nature of time-series</article-title>. <source>Journal of the royal statistical society</source>. <year>1926</year>; <volume>89</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>63</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Zong</surname> <given-names>W</given-names></string-name>, <string-name><surname>Obenhaus</surname> <given-names>HA</given-names></string-name>, <string-name><surname>Skytøen</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Eneqvist</surname> <given-names>H</given-names></string-name>, <string-name><surname>de Jong</surname> <given-names>NL</given-names></string-name>, <string-name><surname>Vale</surname> <given-names>R</given-names></string-name>, <string-name><surname>Jorge</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name>. <article-title>Large-scale two-photon calcium imaging in freely moving mice</article-title>. <source>Cell</source>. <year>2022</year>; <volume>185</volume>(<issue>7</issue>):<fpage>1240</fpage>–<lpage>1256</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92517.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents <bold>important</bold> new insights into how best to address common problems encountered in the statistical analysis of neural data, including those related to temporal autocorrelations and unknown variables. The authors show that certain approaches, including those using cross-validation and permutation tests, are better than others at controlling error rates, particularly false negatives. At present, the evidence presented is <bold>incomplete</bold>, including a lack of more rigorous theoretical justifications for the differences observed between the different approaches tested and a focus on p-values without considering effect sizes, but could be improved with substantial revisions that address these issues.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92517.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
This paper describes a comparison of different statistical methods for model comparison and covariate selection in neural encoding models. It shows in particular that issues arising from temporal autocorrelation and missing variables can lead to statistical tests with substantially higher false positive rates than expected from theory. The paper proposes methods for overcoming these problems, in particular cross-validation with cyclical shift permutation tests. The results are timely, important, and likely to have a broad impact. In particular, the paper shows that cell tuning classification can vary dramatically with the testing procedure, which is an important lesson for the field as a whole.</p>
<p>Strengths:</p>
<p>
- Novel and important comparison of different methods for variable selection in nested models.</p>
<p>Weaknesses:</p>
<p>
- Does not (yet) examine effect sizes</p>
<p>
- Does not motivate/explain key methods clearly enough in the main text.</p>
<p>General Comments:</p>
<p>
1. My first general comment is that the paper in its current form focuses on the &quot;null hypothesis significance testing&quot; (NHST) paradigm. That is, it is focused on binary tests about what variables to include (or not include) in a regression model, and the false-positive rates of such tests. However, the broader statistics community has recently seen a shift away from NHST and towards a statistical reporting paradigm focused on effect sizes. See for example:</p>
<p>
- &quot;Scientists rise up against statistical significance&quot;. Nature, March 2019.</p>
<p>
- Moving to a World Beyond &quot;p &lt; 0.05&quot;. RL Wasserstein, AL Schirm, NA Lazar. The American Statistician, 2019.</p>
<p>In light of this shift, I think the paper would be substantially strengthened if the authors could add a description of effect sizes for the statistical procedures they consider. Thus, for example, in cases where a procedure selects the wrong model (e.g., by selecting a variable that should not be included), how large is the inferred regression weight, and/or how large is the improvement in prediction performance (e.g. test log-likelihood) from including the erroneous regressor? How strong is the position tuning ascribed to a MEC cell that is inappropriately classified as having position tuning under one of the sub-optimal procedures? (Figure 7 shows some example place maps, but it would be nice to see a more thorough and rigorous analysis).</p>
<p>My suspicion would be that even when the hypothesis test gives a false positive, the effect sizes tend to remain small... but it is certainly possible that I'm mistaken, or that inferred effect sizes are more accurate for some procedures than others.</p>
<p>2. My only other major criticism relates to clarity and readability: in particular, the various procedures discussed in the paper (&quot;forward selection&quot;, &quot;maxT correction&quot;, &quot;permutation test with cyclic shifts&quot;) are not clearly explained in the main paper, but are relegated to the Methods. Although I think it is useful to keep many of the mathematical details in the methods section, it would benefit the reader to have a general and intuitive explanation of the key methods within the flow of the main paper. The first paragraph of the Results section is particularly underdeveloped and hard to read and could benefit from a substantial revision to introduce and motivate the terms and procedures more clearly. I would recommend moving much of the text from the Methods into the Results section, or at the very least adding a paragraph describing the general idea/motivation for each method in Results.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92517.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper considers methods for statistical analysis of autocorrelated neural recording time series: an important question for neuroscience, that is underappreciated in the community. The paper makes a valuable contribution to this topic by comparing methods based on cross-validation and cyclic shift on simulated grid-cell data. My main suggestions regard clarity, which would greatly benefit from a more didactic approach: explaining the methods compared to the main text and providing more explanatory figures. But there are also some additional analyses that would strengthen the paper.</p>
<p>There are two ways to build support for the validity of a statistical method: by mathematically proving that it is valid, or by empirically verifying it with simulated data where the correct answer is known. A mathematical proof removes all doubt to validity but empirical validation can still be useful even without proof, as it demonstrates that the method works in at least some circumstances. For empirical validation to be most convincing, it helps to also show some situations where the method doesn't work, ideally by varying a continuous parameter that reliably moves the simulation from a situation where it works to one where it doesn't. If the method works in all but extremely unrealistic cases, this builds confidence that it will work on real data.</p>
<p>The main conclusion of this paper's simulations is that the cyclic shift method most often detects valid correlations, while still not exceeding the false positive rate expected for a valid test. Readers may take this paper as indicating that the circular shift method is safe in all circumstances, but this is not correct. The authors acknowledge that circular shift can sometimes be invalid, and have made modifications to mitigate the problem. But there is neither a mathematical proof that these mitigations work, nor an analysis of the circumstances under which they succeed and fail. I doubt a formal proof is possible since there are likely situations in which even the new methods give false positive results. So the authors should include an empirical test of their modified circular shift method as compared to plain circular shift in various simulations. To gain confidence in the new method it is important to characterize the situations where both methods succeed; where the new method succeeds but traditional cyclic shift gives false positive errors; and situations in which both fail. If situations where the new method fails are so unrealistic that they would never occur in real data, we can have better confidence in the method.</p>
<p>The main contributions of the paper are the modifications to circular shifting and cross-validation that avoid problems of temporal contiguity, but these are only described in the Methods section. But this is a methods paper, so the description of the new methods should be in the main text, including explanatory figures currently in the Methods.</p>
<p>The introduction presents two problems that can occur in neural data: autocorrelation, and omitted variables. However, it is not clear that the current methods help with the problem of omitted variables. In fact, I don't see how any analysis method could solve the problem of omitted variables. If an experimenter observes a correlation between X and Y, there is no way to know this isn't because a third variable Z correlates with X and influences Y, without any effect of X on Y. It is generally impossible to prove causation without making randomized manipulations of one variable; although some methods claim to infer causality by observing all variables that could possibly have a causal effect, this is unlikely to occur in neuroscience. In any case, the problem of omitted variables seems irrelevant to the current study and could be removed.</p>
<p>The list of analysis methods mentioned in the first paragraph of the introduction (eg TDA, LVM) seems irrelevant: it is not clear how the methods evaluated here would be used to assess the significance of those methods. Better to stick to a description of how correlations are difficult to detect in autocorrelated signals, which is what the current methods address.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92517.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The authors consider various statistical testing frameworks for model selection in the context of neuronal tuning. They consider cross-validation as a baseline scheme, and show various corrections and modifications to existing cross-validation schemes together with the underlying data/sign shuffling procedures for finding null distributions. Through careful simulations, they show that some of these tests are expectedly too conservative or too optimistic, and show that a log-likelihood-based test statistic with a cyclic shift permutation test for obtaining null distribution and Bonferroni correction strikes the right balance between hits and false detection. They further apply these tests to calcium imaging data from the mouse entorhinal cortex to identify grid cells (i.e., cells for which position is selected as a relevant variable).</p>
<p>Strengths:</p>
<p>
The paper is very well written, easy to follow, and enjoyable to read. It addresses an important issue in modern neuroscience, which is drawing conclusions based on data with missing or (unaccounted for) auto-correlated covariates.</p>
<p>Weaknesses:</p>
<p>
The paper would benefit from including more rigorous theoretical justification on why some of the procedures examined here outperform the others. This could be done in a stylized example with a Gaussian linear model, for which some of the used statistics have well-known distributions.</p>
<p>Comparisons with false discovery rate (FDR) control, as a more appropriate measure of performance when dealing with many comparisons, would benefit the existing comparisons merely based on Bonferroni correction.</p>
<p>Including spiking history in the generalized linear models (GLMs) used in analyzing the mouse data could be beneficial, as existing literature points to the importance of spiking history as a relevant covariate.</p>
</body>
</sub-article>
</article>