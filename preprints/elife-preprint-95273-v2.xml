<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95273</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95273</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95273.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Cell Biology</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Unbiased identification of cell identity in dense mixed neural cultures</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8964-4480</contrib-id>
<name>
<surname>De Beuckeleer</surname>
<given-names>Sarah</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Van De Looverbosch</surname>
<given-names>Tim</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Van Den Daele</surname>
<given-names>Johanna</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ponsaerts</surname>
<given-names>Peter</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0960-6781</contrib-id>
<name>
<surname>De Vos</surname>
<given-names>Winnok H</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<email>winnok.devos@uantwerpen.be</email>
</contrib>
<aff id="a1"><label>1</label><institution>Laboratory of Cell Biology and Histology, University of Antwerp</institution>, <city>Antwerp</city>, <country>Belgium</country></aff>
<aff id="a2"><label>2</label><institution>Laboratory of Experimental Hematology, Vaccine and Infectious Disease Institute (Vaxinfectio), University of Antwerp</institution>, <city>Antwerp</city>, <country>Belgium</country></aff>
<aff id="a3"><label>3</label><institution>Antwerp Centre for Advanced Microscopy, University of Antwerp</institution>, <city>Antwerp</city>, <country>Belgium</country></aff>
<aff id="a4"><label>4</label><institution>µNeuro Research Centre of Excellence, University of Antwerp</institution>, <city>Antwerp</city>, <country>Belgium</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Zaritsky</surname>
<given-names>Assaf</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Ben-Gurion University of the Negev</institution>
</institution-wrap>
<city>Beer Sheva</city>
<country>Israel</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Campelo</surname>
<given-names>Felix</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Institute of Photonic Sciences</institution>
</institution-wrap>
<city>Barcelona</city>
<country>Spain</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2024-03-13">
<day>13</day>
<month>03</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-11-05">
<day>05</day>
<month>11</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95273</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-01-06">
<day>06</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-01-08">
<day>08</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.01.06.574474"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-03-13">
<day>13</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95273.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.95273.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95273.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95273.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95273.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, De Beuckeleer et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>De Beuckeleer et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95273-v2.pdf"/>
<abstract>
<title>Abstract</title><p>Induced pluripotent stem cell (iPSC) technology is revolutionizing cell biology. However, the variability between individual iPSC lines and the lack of efficient technology to comprehensively characterize iPSC-derived cell types hinder its adoption in routine preclinical screening settings. To facilitate the validation of iPSC-derived cell culture composition, we have implemented an imaging assay based on cell painting and convolutional neural networks to recognize cell types in dense and mixed cultures with high fidelity. We have benchmarked our approach using pure and mixed cultures of neuroblastoma and astrocytoma cell lines and attained a classification accuracy above 96%. Through iterative data erosion we found that inputs containing the nuclear region of interest and its close environment, allow achieving equally high classification accuracy as inputs containing the whole cell for semi-confluent cultures and preserved prediction accuracy even in very dense cultures. We then applied this regionally restricted cell profiling approach to evaluate the differentiation status of iPSC-derived neural cultures, by determining the ratio of postmitotic neurons and neural progenitors. We found that the cell-based prediction significantly outperformed an approach in which the time in culture was used as classification criterion (96% <italic>vs.</italic> 86%, resp.). In mixed iPSC-derived neuronal cultures, microglia could be unequivocally discriminated from neurons, regardless of their reactivity state. A tiered strategy, allowed for discriminating microglial cell states as well, albeit with lower accuracy. Thus, morphological single cell profiling provides a means to quantify cell composition in complex mixed neural cultures and holds promise for use in quality control of iPSC-derived cell culture models.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>morphological phenotyping</kwd>
<kwd>multiplexed imaging</kwd>
<kwd>iPSC culture validation</kwd>
<kwd>neural differentiation</kwd>
<kwd>computational biology</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We have carefully analyzed the reviewers' comments and deduced that the work could benefit from a better description of the operational window and a clearer validation of the nculeocentric profiling approach.
Therefore, we have now performed additional experiments to address the main comments. To prove the sensitivity of our approach, we have included predictions using cells types and cell states with more subtle differences. In addition, we have benchmarked our nucleocentric analysis by illustrating the prediction performance across a range of crop sizes. In doing so, we could conclude that there is an optimal nucleocentric size where precision and recall are balanced. To further convince the reviewers of our conclusions, we have added additional GradCAM images and performed several control experiments to rule out bias introduced by the background or the segmentation performance.
</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/dataset/Nucleocentric-Profiling/27141441?file=49522557">https://figshare.com/articles/dataset/Nucleocentric-Profiling/27141441?file=49522557</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/DeVosLab/Nucleocentric-Profiling">https://github.com/DeVosLab/Nucleocentric-Profiling</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Modelling the complexity of the human brain and its (dys)function has proven to be notoriously challenging. This is due to its intricate wiring, the cellular heterogeneity and species-specific differences<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. To increase the translational value of neuroscientific research, there is a need for physiologically relevant human models. With the advent of human induced pluripotent stem cell (iPSC) technology, it has become possible to generate a wealth of brain-resident cell types including neurons<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, astrocytes<sup><xref ref-type="bibr" rid="c3">3</xref></sup>, microglia<sup><xref ref-type="bibr" rid="c4">4</xref></sup>, oligodendrocytes<sup><xref ref-type="bibr" rid="c5">5</xref></sup> and endothelial cells<sup><xref ref-type="bibr" rid="c6">6</xref></sup>, allowing the study of complex polygenic pathologies that cannot be modelled in animals, opening avenues to precision pharmacology<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>. Furthermore, the ability of iPSC to develop into organoids renders them attractive for studying multi-cellular interactions in a 3D context that is closer to the <italic>in vivo</italic> situation<sup><xref ref-type="bibr" rid="c9">9</xref></sup>. However, genetic drift, clonal and patient heterogeneity cause variability in reprogramming and differentiation efficiency<sup><xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c11">11</xref></sup>. The differentiation outcome is further strongly influenced by variations in protocol<sup><xref ref-type="bibr" rid="c12">12</xref></sup>. This can significantly impact experimental outcomes, leading to inconsistent and potentially misleading results and consequently, it hinders the use of iPSC-derived cell systems in systematic drug screening or cell therapy pipelines. This is particularly true for iPSC-derived neural cultures, as their composition, purity and maturity directly affect gene expression and functional activity, which is essential for modelling neurological conditions<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref></sup>. Thus, from a preclinical perspective, there is the need for a fast and cost-effective quality control (QC) approach to increase experimental reproducibility and cell type specificity<sup><xref ref-type="bibr" rid="c15">15</xref></sup>. From a clinical perspective in turn, robust QC is required for safety and regulatory compliance (e.g., for cell therapy). This need for improved standardization is underscored by large-scale collaborative efforts such as the International Stem Cell Banking Initiative<sup><xref ref-type="bibr" rid="c16">16</xref></sup>, which focusses on clinical quality attributes and provides recommendations for iPSC validation testing for use as cellular therapeutics, or the CorEuStem network, aiming to harmonize iPSC practices across core facilities in Europe. Current culture validation methods include (combinations of) sequencing, flow cytometry and immunocytochemistry<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>. These methods are often low in throughput, costly and/or destructive. Thence, we set out to develop a method for evaluating the composition of such cultures based on high-content imaging, which is fast, affordable and scalable. The primary goal was to facilitate cell type identification in dense cultures, while ensuring compatibility with subsequent immunocytochemistry or molecular assays for further biological inquiries. To this end, we employed the Cell Painting (CP)<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref></sup> method, which is based on labelling cells with simple organic dyes and analysing the resulting phenotypes. CP has proven to be a powerful and generic method for predicting the mode-of-action of pharmacological or genetic perturbations, and this sheerly using a cell morphology readout<sup><xref ref-type="bibr" rid="c20">20</xref>–<xref ref-type="bibr" rid="c22">22</xref></sup>. Thus far, CP has primarily been utilized to predict conditions associated with pharmacological treatments or genetic modifications using images as input. For example, CP has allowed predicting patient diversity in lung adenocarcinoma-associated somatic variants<sup><xref ref-type="bibr" rid="c23">23</xref></sup>. or genetic variation across donors in iPSC cultures <sup><xref ref-type="bibr" rid="c24">24</xref></sup>. Methods such as PhenoRipper<sup><xref ref-type="bibr" rid="c25">25</xref></sup> and CP-CHARM<sup><xref ref-type="bibr" rid="c26">26</xref></sup> use whole-image features for classification, which circumvent the difficulty of cell segmentation allow classification of images with cells of similar phenotype or class <sup><xref ref-type="bibr" rid="c27">27</xref></sup>. However, they do not consider differences in cell density and disregard the inter-cellular heterogeneity within the field of view. Therefore, we explored the amenability of CP to predict individual cell types in dense and mixed cultures. By combining deep learning for cell segmentation and classification, we established an approach that allows recognizing cell types with high accuracy, even in very dense cultures. We employed the approach to evaluate the composition of iPSC-derived neural cultures as they are often quite dense and composed of heterogenous cell types. Varying the density, composition, and cell state allowed us to benchmark the discriminatory potential of cell-based profiling.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Neural cell lines have a unique morphotextural fingerprint</title>
<p>Several groups have demonstrated that morphological cell profiling can be used to discriminate pharmacogenomic perturbations based on phenotypic similarity<sup><xref ref-type="bibr" rid="c20">20</xref>–<xref ref-type="bibr" rid="c22">22</xref></sup>. We asked whether a similar approach could be leveraged to unequivocally distinguish individual cell types as well. To this end, we implemented a version of CP based on 4-channel confocal imaging (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>) and first applied it to monocultures of two neural cell lines from a different lineage, namely astrocyte-derived 1321N1 astrocytoma cells and neural crest-derived SH-SY5Y neuroblastoma cells. First, we explored whether traditional morphotextural feature extraction provided sufficient distinctive power. The features were calculated for each channel in three regions of interest, namely the nucleus, cytoplasm and whole cell. They describe shape, intensity and texture features of each ROI (<xref rid="tbl5" ref-type="table">Table 5</xref>). Representation of the resulting standardized feature set in UMAP space revealed a clear separation of both cell types along the first UMAP dimension. Clustering of instances was less pronounced after principal component analysis (<xref rid="figs1" ref-type="fig">Suppl. Fig. 1A</xref>), but UMAP better preserves local and global data structure <sup><xref ref-type="bibr" rid="c28">28</xref></sup>. Despite some separation of replicates across the second UMAP dimension, the absence of clear replicate clusters showed that the morphological differences between cell types were consistent across biological replicates (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). When projecting individual features onto the UMAP space, we found that both texture (<italic>e.g.,</italic> Nucleus channel 3 energy) and shape (<italic>e.g.,</italic> Cellular Area) metrics contributed to the cell type separation (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>). The contribution of intensity-related features (<italic>e.g.,</italic> Channel 3 Intensity) to cell type separation was less pronounced as they were more correlated with the biological replicate. Thus, we conclude that cell types can be separated across replicates based on a morphotextural fingerprint.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1</label>
<caption><title>Shallow classification using manually defined features.</title>
<p>(A) Image overview of 1321N1 (top) and SH-SY5Y (bottom) cells after CP staining acquired at higher magnification (Plan Apo VC 60xA WI DIC N2, NA = 1,20). Scale bar is 30µm. The channel, wavelength and dye combination used is listed in the table below the figure. This color code and channel numbering is used consistently across all figures. (B) UMAP dimensionality reduction using handcrafted features. Each dot represents a single cell. The color reflects either cell type condition (left) or replicate (right). This shows UMAP clustering is a result of cell type differences and not variability between replicates. (C) Feature importance deducted from the UMAP (feature maps). Each dot represents a single cell. Three exemplary feature maps are highlighted alongside the quantification per cell type. These feature representations help understanding the morphological features that underly the cluster separation in UMAP. (D) Random Forest classification performance on the manually defined feature dataframe with and without exclusion of redundant features. Average confusion matrix (with redundant features) and Mean Decrease in Impurity (reflecting how often this feature is used in decision tree splits across all random forest trees). All features used in the UMAP are used for RF building. Each dot in the violinplot represents the F-score of one classifier (model initialization, N = 30). Classifiers were trained 10x with 3 different random seeds.</p></caption>
<graphic xlink:href="574474v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2b">
<title>CNN outperforms random forest in cell type classification</title>
<p>To evaluate whether the morphotextural fingerprint could be used to predict cell type, we performed Random Forest (RF) classification using the full feature dataset, using different seeds for splitting up the data into training and validation sets. This resulted in a rather poor accuracy (F-score: 0,75±0,01), mainly caused by the significant (46%) misclassification of 1321N1 cells (<xref rid="fig1" ref-type="fig">Fig. 1D</xref>). The imbalance in recall and precision was surprising given the clear separation of cell populations in UMAP space using the same feature matrix. When inspecting the main contributions to the RF classifier using the mean decrease in impurity, we found very similar features as highlighted in UMAP space to add to the discrimination (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>). Where the most important features (<italic>e.g.,</italic> cellular area, channel 1 contrast) showed the expected gradient along the first UMAP direction, lower ranked parameters (e.g., cellular channel 3 mean intensity) had no contribution to UMAP separation (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>). Reducing noise by removing redundant features (correlation &gt; 0.95) could not ameliorate RF classification performance, which may be due to the documented bias in feature selection for node splitting in high-dimensional data<sup><xref ref-type="bibr" rid="c29">29</xref></sup>. This result drove us to evaluate a different classification approach based on a ResNet<sup><xref ref-type="bibr" rid="c30">30</xref></sup> convolutional neuronal network (CNN). Here, we no longer relied on the extraction of “hand-crafted” features from segmented cell objects for training the shallow RF classifier. Instead, we used isotropic image crops of 60µm centred on individual cell centroids and blanked for their surroundings as input for the CNN (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). Using this approach, we found a significantly higher prediction performance with an F-score of 0,96±0,01, with a much more balanced recall and precision (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>). The UMAP space built from the CNN feature embeddings showed a clear cell type separation (<xref rid="figs1" ref-type="fig">Suppl. Fig. 1B</xref>). Even with only 100 training instances per class, the CNN outperformed RF, but optimal performance was attained with 5000 training instances (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). A model trained on a combination of three biological replicates (“Mixed reps.”) outperformed those that were only trained on a single replicate (“Single rep.”) when predicting instances of an independent unseen replicate (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>). This emphasizes the need for including sufficient variation in the training set. Although much more performant, CNN classification does not allow direct retrieval of intuitive features, which complicates model interpretation. To gain a visual understanding of image information contributing most to the classification we therefore resorted to Gradient-weighted Class Activation Mapping (Grad-CAM)<sup><xref ref-type="bibr" rid="c31">31</xref></sup>. This revealed that the attention of the CNN was mainly focused on cell borders (edge information), nuclear and nucleolar signals (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>). When scrutinizing CNN misclassifications, we found that these are mainly caused by faulty cell detection (<italic>e.g.,</italic> oversegmentation of cell ramifications), unhealthy/dead cells, debris or visibly aberrant cell shapes (<xref rid="figs1" ref-type="fig">Suppl. Fig. 1C</xref>) - errors not necessarily attributed to the CNN.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2</label>
<caption><title>Convolutional neural network classification of monoculture conditions.</title>
<p>(A) Schematic of image pre-processing for CNN classification. (B) CNN accuracy and confusion matrix on cell type classification in monocultures. Each dot in the violinplot represents the F-score of one classifier (model initializations, N = 9). (C) Shallow vs. deep learning accuracy for a varying number of input instances (ROIs). Each dot in the violinplot represents the F-score of one classifier (model initialization, N). The ribbon represents the standard deviation on the classifiers. (D) The impact of experimental variability in the training dataset on model performance for shallow vs. deep learning. Classifiers were trained on either 1 (single rep.) or multiple (mixed reps.) replicates (where each replicate consists of N new biological experiment). Mann-Whitney-U-test, p-values resp. 0.7304 and 0.000197. Each violinplot represents the F-score of one classifiers (model initializations). (E) The performance of deep learning classifiers trained in panel D on single replicates (low variability) or mixed replicates (high variability) on unseen images from either the training replicate (cross-validation) or an independent replicate (independent testing) (where each replicate consists of a new biological experiment). Kruskal-Wallis test on single training condition, p-value of 0,026 with post-hoc Tukey test. Mann-Whitney-U-test on mixed training, p-value of 7,47e-6. Each violinplot represents the F-score of one classifier (model initialization, N = 3). (F) Images of example inputs given to the CNN. The composite image contains an overlay of all CP channels (left). The GradCAM image overlays the GradCAM heatmap on top of the composite image, highlighting the most important regions according to the CNN (right). One example is given per cell type.</p></caption>
<graphic xlink:href="574474v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To shed light on the contribution of individual markers to the classification, we eroded the input to single channel images. For all cases, the prediction performance was below or equal to 85,0% suggesting that no single channel contains all relevant information (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>). Combinations of two or three channels could not match the prediction accuracy of the full four-channel image either. Thus, we concluded that all channels contribute to the successful classification. As the CNN directly uses image crops as input, the image quality will determine the classification performance. Therefore, we assessed the impact of resolution and signal to noise ratio (SNR). We simulated the effect of decreasing spatial resolution through progressive pixel binning (from 192 pixels (original, pixel size 0.3µm) to 9 pixels (pixel size 6 µm)) (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>). For each iteration, 3 CNN models were trained and evaluated. A reduction in pixel size from 0.3µm to 0.6µm did not result in a significantly lower prediction performance, but decreasing the spatial resolution further caused a progressive decrease in F-score. In a similar manner, we tested the impact of decreasing SNR (by increasing the level of Gaussian noise) on classification accuracy. Starting from an original SNR value of 20,05 dB, we found that the F-score started to decrease when lowering the SNR below 14,27 dB.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3</label>
<caption><title>Required input quality for cell type classification.</title>
<p>(A) Performance of the CNN when only a selection of image channels is given as training input. Each boxplot represents the F-score of one classifier (model initialization, N = 3). Channel numbering is in accordance with <xref rid="fig1" ref-type="fig">fig. 1A</xref>. (1 = DAPI; 2 = FITC; 3 = Cy3; 4 = Cy5) (B) Simulation of the effect of increased pixel size (reduced spatial resolution) on the classification performance. Each point is the average of 3 CNN model initializations (N), with bars indicating the standard deviation between models. Red line indicates the average F-score of the original crops. (C) Simulation of the effect of added gaussian noise (reduced signal-to-noise ratio) on the classification performance. Each point is the average of 3 CNN model initializations, with bars indicating the standard deviation between models. Red line indicates the average F-score of the original images. Statistics were performed using a Kruskal-Wallis test with post-hoc Tukey test. (D) CNN performance on 1321N1, RPE1 and ARPE cells. Dots represent different model initializations (N = 3).</p></caption>
<graphic xlink:href="574474v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We observed that even with low input image quality, the CNN performance still retained approximately 80% accuracy. We attribute this to the predictive power of the nuclear size, which is a dominant discriminating feature between the cells (<xref rid="figs2" ref-type="fig">Suppl. Fig. 2</xref>). Given the relatively overt phenotypic difference between 1321N1 and SH-SY5Y cells, we asked whether the CNN could also discriminate more subtle phenotypes. To this end, we used 1321N1 cells and two related retinal pigment epithelial cell lines, hTERT-immortalized RPE1 and the spontaneously immortalized variant ARPE (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>). The CNN discriminated all three cell lines with high accuracy of 95,06±2,51%. Unsupervised UMAP dimensionality reduction using the CNN feature embeddings revealed three clusters of ROIs, with the two RPE lines located closer together (but well separated) in comparison to the morphologically more distinct 1321N1 cell type (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>). Together this work illustrates that a CNN approach can be used to distinguish diverse cell types within a given image quality window.</p>
</sec>
<sec id="s2c">
<title>Nucleocentric predictions remain accurate regardless of culture density</title>
<p>Morphological profiling relies on accurate cell detection. This may become difficult in dense cultures such as iPSC-cultures, clustered cells tissues and tissue-mimics. Having established a method to distinguish cell types with high accuracy, we next asked how robust the classification was to increasing cell density. To this end, we acquired images of 1321N1 and SH-SY5Y monocultures, grown at densities ranging from 0 to 100% confluency (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>). Based on the nuclear count, we binned individual fields into 6 density classes (0-20%, 20-40%, 40-60%, 60-80%, 80-95%, 95-100%) and trained a CNN with equal sampling of cell numbers per density class to avoid bias. No decrease in accuracy was observed until the culture density reached 80% confluency. Only for very high densities (95-100%), we found a significant decrease in the prediction accuracy (F-score: 0,92±0,05) (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>). We reasoned that under these conditions cell shape would be predominantly determined by neighbouring cells and cell segmentation performance would decrease (<xref rid="figs1" ref-type="fig">Suppl. Fig. 1D</xref>). Nuclei are less malleable and even in dense cultures remain largely separated, allowing their robust segmentation and avoiding CNN misclassifications resulting from segmentation errors. Hence, we asked whether using the nuclear ROI as input would improve classification performance at high densities. However, despite the relatively high average F-score of 0,91±0,05 (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>), the performance was consistently lower than whole cell ROI across the density range and the performance still decreased with full confluency. To understand these results, we inspected the GradCAM output for these predictions, and found that an important part of the attention of the CNN is diverted to the background (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>, <xref rid="figs3" ref-type="fig">Suppl. Fig. 3</xref>). We interpret this result as the CNN using the background as a proxy for nuclear size. To rule out bias by setting the background to zero in nuclear crops, the CNN was also trained on the same crops with randomly speckled background, but despite a shift in attention to the nucleus, similar prediction performance was attained (<xref rid="figs2" ref-type="fig">Suppl. Fig. 2</xref>). The classification performance was not biased by segmentation errors (<xref rid="figs1" ref-type="fig">Suppl. Fig. 1E</xref>) but may be influenced by the fact that nuclear area is affected by culture density (<xref rid="figs1" ref-type="fig">Suppl. Fig. 1F</xref>). In highly dense cultures, the dynamic range of the nuclear size decreases as all nuclei become more compact and the nuclear size range decreases (<xref rid="figs1" ref-type="fig">Suppl. Fig. 1F</xref>). Thence, we tested an intermediate condition which exploits the more robust nuclear segmentation but also includes part of the (sub-)cellular local surrounding information as input (<xref rid="figs1" ref-type="fig">Suppl. Fig. 1G</xref>). To identify the optimal patch size, we varied the size of a square box centred around the nuclear centroid from 0.6 to 150 µm (<xref rid="fig4" ref-type="fig">Fig. 4E</xref>). Within a range of 12-18µm, we found a maximal F-score of 0,96±0,02 (<xref rid="fig4" ref-type="fig">Fig. 4E</xref>). Further increasing the nucleocentric patch size did not majorly affect the F-score but significantly decreased the precision and variability of recall for CNN predictions (<xref rid="fig4" ref-type="fig">Fig. 4E</xref>). Hence, for further experiments, a nucleocentric patch size of 18µm was used. GradCAM images revealed that this latter approach led the CNN to focus on perinuclear structures (<xref rid="fig4" ref-type="fig">Fig. 4D</xref> and <xref rid="figs3" ref-type="fig">Suppl. Fig. 3</xref>). Interestingly, when using this nucleocentric approach, the prediction performance was maintained at almost confluent cell densities in contrast with whole cell approaches (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>). Thus, we conclude that using a nucleocentric region as input for the CNN could be a valuable strategy for accurate cell type identification in dense cultures.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4</label>
<caption><title>Required regional input for accurate CNN training in cultures of high density.</title>
<p>(A) Selected images and insets for increasing culture density with the density categories used for CNN training. (B) Results of 3 CNN models trained using different regional input (the full cell, only nucleus or nucleocentric area) and evaluated on data subsets with increasing density. Each dot represents the F-score of one classifier (model initialization, N = 3) tested on a density subset. Classifiers were trained with the same random seed. Ribbon represents the standard deviation. (C) CNN performance (F-score) of CNN models using different regional inputs (full cell, only nucleus or nucleocentric area). Each boxplot represents 3 model initializations for 3 different random seeds (N = 9). (D) Images of example inputs for both the nuclear and nucleocentric region. The composite image contains an overlay of all CP channels (top). The GradCAM image overlays the GradCAM heatmap on top of the composite image, highlighting the most important regions according to the CNN (bottom). One example is given per cell type. (E) Systematic in- and decrease (default of 18µm used in previous panels) of the patch size surrounding the nuclear centroid used to determine the nucleocentric area. Each dot represents the results of one classifier (model initialization, N = 3). Ribbon represents the standard deviation. The analysis was performed using a mixed culture dataset of 1321N1 and SH-SY5Y cells (<xref rid="fig5" ref-type="fig">Fig. 5</xref>).</p></caption>
<graphic xlink:href="574474v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2d">
<title>Cell prediction remains accurate in mixed cell culture conditions</title>
<p>Although a very high classification accuracy was obtained with nucleocentric CNN predictions, both training and testing were performed with input images drawn from monocultures. As our goal was to allow cell type prediction in complex, heterogeneous cultures, we next switched to a more faithful situation in which we co-cultured both cell types. Ground-truth for these predictions was generated by either performing post-hoc immunofluorescence (IF) with cell-type specific antibodies (CD44 for 1321N1 and TUBB3 for SH-SY5Y cells), or by differential replication labelling (<italic>i.e.,</italic> by incubating the two cell types with EdU and BrdU respectively, prior to mixing them) after dye quenching<sup><xref ref-type="bibr" rid="c32">32</xref></sup> (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>). Replication labelling proved significantly more successful for binary ground truth classification (through intensity thresholding) than IF labelling for the cell lines we used (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>). When training a CNN to recognize the cell types using these markers as ground truth, we found that the prediction accuracy on the left-out dataset of the co-culture (Co2Co) was almost as high as when a model was trained and tested on monocultures (Mono2Mono) (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5</label>
<caption><title>Cell type prediction in mixed cultures by post-hoc ground truth identification.</title>
<p>(A) Schematic overview of virtual vs. physically mixed cultures and the subsequent CNN models. For virtual mixing, individual cell types arise from distinct cultures, the cell type label was assigned based on the culture condition. For physical mixing, the true phenotype of each crop was determined by intensity thresholding after post-hoc staining. Three model types are defined: Mono2Mono (culture-based label), Co2Co (cell-based label) and Mono2Co (trained with culture-based label and tested on cell-based label). (B) Determining ground-truth phenotype by intensity thresholding on IF (top) or pre-label (bottom). The threshold was determined on the intensity of monocultures. Only the pre-labelled ground-truth resulted in good cell type separation by thresholding for 1321N1 vs. SH-SY5Y cells. (C) Mono2Mono (culture-based ground truth) vs. Co2Co (cell-based ground truth) models for cell type classification. Analysis performed with full cell segmentation. Mann-Whitney-U-test p-value 0,0027. Monoculture-trained models were tested on mixed cultures. Pretrained models were trained on independent biological replicates. These could be finetuned by additional training on monoculture images from the same replicate as the coculture. This was shown to reduce the variation between model iterations (Median performance: Mann-Whitney-U-test, p-value 0,0015; Coefficient of variation: Mann-Whitney-U-test, p-value 3,48e-4). Each dot in the violinplots represents the F-score of one classifier (model initialization, N = 9). Classifiers were trained 3x with 3 different random seeds.</p></caption>
<graphic xlink:href="574474v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We then tested whether it was possible to train a classifier based on monocultures only, for cell type prediction of cells in co-culture (Mono2Co). This resulted in an F-score of 0,86±0,01%. This drop in performance may be caused by the effect on cell phenotype exerted by the presence of other cell types in mixed cultures which is not captured in monocultures. As the images from monocultures and co-cultures were obtained from different plates, we suspected inter-replicate variability in culture and staining procedures to contribute in part to the lesser performance. Therefore, we tested whether we could improve the performance of the CNN by including monocultures from the same plate as the co-cultures. This finetuning indeed improved the average performance to 0,88±0,01%, and more importantly, it significantly reduced the variability (coefficient of variation) of the predictions making it more reproducible (<xref rid="fig5" ref-type="fig">Fig. 5D</xref>). Thus, while not yet reaching the same accuracy, it proves that it is possible to establish a model that recognizes cell types in co-cultures that is solely trained on monocultures.</p>
</sec>
<sec id="s2e">
<title>Cell type profiling can be applied to stage iPSC-derived neuronal cultures</title>
<p>iPSC-derived neuronal cell cultures suffer from significant inter- and intra-batch variability and could benefit from an efficient quality control<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup>. Thence, we applied our nucleocentric phenotyping pipeline to stage the maturity of a neuronal cell culture based on its cell type composition. Using a guided protocol<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, two differentiation stages were simulated: a primed stage, where most cells are assumed to be cycling neural progenitors (NPCs), and a differentiated stage where most cells are post-mitotic neurons (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>). The two cell types were discriminated by post-hoc IF labelling for the cell cycle marker Ki67 (NPC) and the microtubule marker ß-III-tubulin (TUBB3, neurons) (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>). Not all cells in the CP image could be assigned with a ground truth label due to cell detachment upon re-staining or sheer absence of the tested marker. Since no single monoculture consists of 100% of either cell type, we applied gates to retain only those cells that show unequivocal staining for either one of both markers. Based on these gates, ROIs were either classified as neuron (Ki67-/TUBB3+), NPC (Ki67+/TUBB3-) or undefined (outside of gating boundaries). We assume the latter category to represent transitioning cells in intermediate stages of neural development, un- or dedifferentiated iPSCs. This gating strategy resulted in a fractional abundance of neurons vs. total (neurons + NPC) of 36,4 % in the primed condition and 80,0% in the differentiated condition (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>). In a first attempt to classify cells within the guided culture, we trained a CNN on individual cell inputs using the culture condition (primed or differentiated) as ground truth, which given the heterogeneity, can be considered as a weak label. This resulted in a low F-score of 0,86±0,01%. When we used the cell-level IF ground truth labels instead, we obtained a classification performance of 0,96±0,00 %. For comparison, a shallow learner (RF), showed a significantly lower F-score of 0,87±0,02 (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>). Applying this cell-based (as opposed to condition-based) CNN to the two culture conditions resulted in predicted fractional abundance of neurons to NPC of 40,5% in images of the primed condition and 74,2% in images of the differentiated condition – aligning well with the manually defined ratios (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>). Both supervised and unsupervised UMAP dimensionality reduction on the feature embeddings of the cell-based classifier revealed a clear clustering of both phenotypes suggesting that the CNN captures the differences in morphotextural fingerprint between neurons and NPCs well (<xref rid="fig6" ref-type="fig">Fig. 6F</xref>). We then went on to evaluate the established cell-based classifier to a primed neuronal cell culture undergoing gradual spontaneous differentiation after dual SMAD inhibition<sup><xref ref-type="bibr" rid="c34">34</xref></sup>. We examined cell cultures at 13, 30, 60 and 90 days <italic>in vitro</italic> (DIV) after the start of the differentiation process and visually confirmed a gradual change in neural maturity. This is evidenced by a gradually smaller cell body and long, thin ramifications (<xref rid="fig6" ref-type="fig">Fig. 6G</xref>). The cell-based CNN model trained on the guided differentiation dataset tested on the spontaneous cultures, confirmed a shift in the (slower) neuron-to-NPC fractional abundance with increasing time in culture (<xref rid="fig6" ref-type="fig">Fig. 6H</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6</label>
<caption><title>iPSC-derived differentiation staging using morphology profiling.</title>
<p>(A) Schematic overview of guided vs. spontaneous neural differentiation. DIV = days in vitro. Selected timepoints for analysis of the spontaneously differentiation culture were 13, 30, 60 and 90 days from the start of differentiation of iPSCs. (B) Representative images of morphological staining (color code as defined in <xref rid="fig1" ref-type="fig">fig. 1A</xref>) and post-hoc IF of primed and differentiated iPSC-derived cultures (guided differentiation). Ground-truth determination is performed using TUBB3 (for mature neurons) and Ki67 (mitotic progenitors). (C) Fraction of neurons vs. NPC cells in the primed vs. differentiated condition as determined by IF staining. Upon guided differentiation, the fraction of neurons increased. (D) left: CNN performance when classifying neurons (Ki67-/TUBB3+) vs. NPC (Ki67+/TUBB3-) cells using either a condition-based or cell-type based ground truth. Each dot in the violinplots represents the F-score of one classifier (model initialization). Classifiers were trained with different random seeds. Mann-Whitney-U-test, p-value 4,04e-4. Right: comparison of CNN vs. RF performance. Mann-Whitney-U-test, p-value 2,78e-4. (E) Fractional abundance of predicted cell phenotypes (NPC vs. neurons) in primed vs. differentiated culture conditions using the cell-based CNN. (F) Unsupervised and supervised UMAP of the cell-based CNN feature embeddings. Plot color coded by cell type. Points represent individual cells. (G) Representative images of spontaneously differentiating neural cultures. Color code as defined in <xref rid="fig1" ref-type="fig">fig. 1A</xref>. (H) Prediction of differentiation status using the cell-based CNN model trained on guided differentiated culture.</p></caption>
<graphic xlink:href="574474v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2f">
<title>Cell type identification can be applied to mixed iPSC-derived neuronal cultures regardless of the activation state</title>
<p>Next to NPCs and neurons, iPSC-derived neuronal cultures are often studied in conjunction with other relevant cell types that influence neuronal connectivity and homeostasis such as astrocytes and microglia<sup><xref ref-type="bibr" rid="c13">13</xref></sup>. Therefore, we tested whether the cell-based approach could be extended to these cell types as well. We generated monocultures of iPSC-derived astrocytes, neurons, and microglia from the same parental iPSC line (<xref rid="fig7" ref-type="fig">Fig. 7A</xref>) and trained a nucleocentric CNN to identify each cell type. This led to a prediction accuracy of 96,81±0,95% (<xref rid="fig7" ref-type="fig">Fig. 7A</xref>). Recognizing that these cell types are visually morphologically distinct, we tested the robustness of the model to experimental perturbations in cell state (<xref rid="fig7" ref-type="fig">Fig. 7B</xref>). To this end, we induced reactivity in the iPSC-derived microglial culture by LPS treatment. A CNN trained on monocultures of neurons, unchallenged or LPS-treated microglia showed high accuracy in differentiating neurons from microglia (98% accurate), although the difference between reactive and resting-state microglia in this tripartite model proved more challenging (71% and 90% accuracy, resp.). However, when implementing a tiered approach, in which after neuron-microglia recognition, a second model was tasked with the classification of reactive vs. resting state microglia without the presence of neurons, an F-score of 0,92±0,00 was obtained. Repeating those experiments in mixed cultures of neurons and microglia (using Tubb3, resp. Iba1 as ground-truth IF markers), yielded an F-score of 0,98±0,01 for the classification of neurons vs. microglia. Again for comparison, CNN outperformed the classical RF approach (0,86±0,03) (<xref rid="fig7" ref-type="fig">Fig. 7C</xref>). In the presence of LPS, a CNN model yielded a high F-score of 0,97±0,01 (<xref rid="fig7" ref-type="fig">Fig. 7C</xref>). This implies that the shift in cell state does not significantly affect the CNN’s ability to distinguish neurons from microglia. As above, a separate CNN was trained to classify within the microglial subpopulation the LPS-treated (reactive) from the unchallenged (resting-state) microglia. Yet, this phenotype proved more challenging for the CNN to predict, as evidenced by an F-score of 0,80±0,01 (<xref rid="fig7" ref-type="fig">Fig. 7C</xref>). Based on these results, we conclude that nucleocentric phenotyping can be used to gauge the cell type composition of iPSC-derived cultures. Distinct cell types show sufficient morphological differences to allow highly accurate classification. Different cell states of individual cell types can still be recognized albeit with lower performance.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7</label>
<caption><title>iPSC cell type identification using morphology profiling.</title>
<p>(A) Representative images of iPSC-derived neurons, astrocytes and microglia in monoculture with morphological staining. Color code as defined in <xref rid="fig1" ref-type="fig">fig. 1A</xref>. Prediction accuracy of a CNN trained to classify monocultures of iPSC-derived astrocytes, microglia and neurons with confusion matrix (average of all models). Each dot in the boxplot represents the F-score of one classifier (model initialization, N = 9). Classifiers were trained 3x with 3 different random seeds. (B) Representative images of monocultures of iPSC-derived neurons and microglia treated with LPS or control. Color code as defined in <xref rid="fig1" ref-type="fig">fig. 1A</xref>. Prediction accuracy and confusion matrix (average of all models) are given. Each dot in the violinplot represents the F-score of one classifier (model initialization, N = 3). (C) Representative images of a mixed culture of iPSC-derived microglia and neurons. Ground-truth identification was performed using IF. Each dot in the violinplot represents the F-score of one classifier (model initialization). Classifiers were trained 3 different random seeds. Results of the CNN are compared to shallow learning (RF). The same analysis was performed for mixed cultures of neurons and microglia with LPS treatment or control. A layered approach was used where first the neurons were separated from the microglia before classifying treated vs. non-treated microglia. Each dot in the violinplot represents the F-score of one classifier (model initialization, N = 9). Classifiers were trained 3x with 3 different random seeds.</p></caption>
<graphic xlink:href="574474v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>iPSC-derived cell cultures have the potential to improve translatability of biomedical research but represent a challenge due to their variability and multicellular composition. With our work, we have developed a method to identify neural cell types in mixed cultures to aid in their validation and application in routine screening settings. We first benchmarked our approach using neural cell lines and found that a CNN-based approach outperforms shallow learning (RF) based on handcrafted feature extraction, even with a limited number of input images. This aligns well with recent data showing CNN-based feature representations outperform shallow techniques for condition-based drug response prediction<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. To assess the robustness and sensitivity of deep learning for cell type prediction, we evaluated the CNN performance as a function of input data size and quality. Next, we tested the performance of predictions for dense single and multi-cellular cultures. Finally, we assessed its performance in the presence of different cells states.</p>
<p>Although our benchmarking revealed a minimal image resolution of 0,6 µm/pixel and SNR of approximately 10 dB for optimal CNN performance, lower quality images may still yield acceptable performance, especially with the use of advanced image restoration algorithms<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. We found that all channels contribute to the overall prediction accuracy but that the whole cell is not required to obtain the highest prediction accuracy. In line with earlier studies that highlight the biomarker potential of the nucleus for patient stratification<sup><xref ref-type="bibr" rid="c37">37</xref></sup>, cell state identification<sup><xref ref-type="bibr" rid="c38">38</xref></sup> or drug response<sup><xref ref-type="bibr" rid="c20">20</xref></sup>, we found that the nuclear ROI as such already carries highly distinctive information for cell type prediction. However, extension to its direct surroundings proved to be the best and most robust input across a range of cell densities. The nucleocentric approach, which is based on more robust nuclear segmentation, reduces such mistakes whilst still retaining input information from the structures directly surrounding the nucleus. At higher cell density, the whole-cell segmentation becomes more error-prone, while also loosing morphological information (<xref rid="figs1" ref-type="fig">Suppl. Fig. 1D</xref>). The nucleocentric approach is more consistent as it relies on a more robust segmentation and does not blank the surrounding region. This way it also buffers for occasional nuclear segmentation errors (<italic>e.g.,</italic> where blebs or parts of the nucleus are left undetected). GradCAM maps on nucleocentric crops highlight specifically those structures surrounding the nucleus (reflecting ER, mitochondria, Golgi) indicating their importance in correct cell classification. We observed a severe imbalance in precision and recall when altering the size of the nucleocentric window. This imbalance was not reflected into an overall worse F-score, since both target classes had an equal number of input instances. However, this imbalance would strongly affect the prediction performance of cases with unknown ground truth were the input numbers are imbalanced. Nucleocentric prediction of other cell types with less pronounced differences within their nucleus and close environment might yield a different optimum. It opens possibilities for applying cell profiling in the future to even more complex 3D cell systems such as tissue or organoid slices, where accurate cell segmentation becomes extremely challenging. Indeed, our results indicate that it might be possible to discriminate individual cells in extremely crowded environments. For specific applications such as classification of healthy vs. tumour tissue, binary classification models can prove useful. It has been shown that a dual-task graph neural network can classify epithelioid and sacromatoid tumor cells with cellular resolution for accurate mesothelioma subtype determination<sup><xref ref-type="bibr" rid="c39">39</xref></sup>. However, the rich diversity of cell types and intermediates present in tissue may complicate classification and demand a more extensive ground truth based on cyclic immunostaining to further refine the predictions. In the further future, it may even be possible to include volumetric information, but this will require optimization of the sample preparation procedures as well.</p>
<p>One crucial realisation of this work is that cell types can be identified in mixed cultures solely using the input from monocultures. This implies that cells retain their salient characteristics even in more complex heterogeneous environments. While for now, predictions are still superior when based on training directly on mixed cultures, we found that the prediction accuracy of monoculture trained models can be increased by employing replicate controls. This suggests that it may become possible to apply or adapt existing models without the need for a cell-level ground truth as provided by post-hoc labelling. This technique could potentially be of use for cultures or tissues where no antibody- or pre-labelling is possible (<italic>e.g.,</italic> no unique IF marker is available, non-replicating cells). To increase the accuracy one could resort to intelligent data augmentation<sup><xref ref-type="bibr" rid="c40">40</xref></sup> or transfer learning<sup><xref ref-type="bibr" rid="c41">41</xref></sup> strategies. While a ground-truth independent method holds promise for bulk predictions (<italic>e.g.,</italic> for quality control purposes), the use of post-hoc labelling allows building more refined models that can distinguish multiple cell types and/or cell states at once. Especially using cyclic staining or spatial transcriptomics, much richer information content can be gained. With the method presented here, we combine the rich multidimensional information from cyclic IF with morphological information and show that it is possible to predict IF signatures. Multiplexed imaging has previously shown its utility in gaining in-depth information on culture composition and differentiation status in iPSC-derived neural cultures (progenitor, radial glia, astrocytes (immature and mature) and neurons (inhibitory and excitatory)<sup><xref ref-type="bibr" rid="c42">42</xref></sup>. Similarly, this could be expanded to cell states (apoptosis, DNA damage), albeit with a lesser performance depending on the phenotype<sup><xref ref-type="bibr" rid="c21">21</xref></sup>. Initial model training requires both CP and multiplexed IF images. After the initial training phase, the IF step can be omitted, and the CNN will predict the phenotype outcomes of the IF with high accuracy on the CP images alone. This significantly reduces experimental costs and time requirements.</p>
<p>Applying the method to iPSC-neuronal cultures revealed its potential to score their differentiation state. Although guided protocols manage to speed up the differentiation process and lead to enhanced culture purity, the neural differentiation process proves to be less than binary, as evidenced by the mixed presence of Ki67+/TUBB3- and Ki67-/TUBB3+ cells. The spontaneous differentiation protocol illustrated the unpredictable nature and late stage of the differentiation process. Many groups highlight the difficulty of reproducible neural differentiation and attribute this to culture conditions, cultivation time and variation in developmental signalling pathways in the source iPSC material<sup><xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c44">44</xref></sup>. Spontaneous neural differentiation has previously been shown to require approximately 80 days before mature neurons arise that can fire action potentials and show neural circuit formation. Although these differentiation processes display a stereotypical temporal sequence<sup><xref ref-type="bibr" rid="c34">34</xref></sup>, the exact timing and duration vary. This variation negatively affects the statistical power when testing drug interventions and thus prohibits the application of iPSC-culture derivatives in routine drug screening. Current solutions (e.g., immunocytochemistry, flow cytometry, …) are often cost-ineffective, tedious, and incompatible with longitudinal/multimodal interrogation. CP is a much more cost-effective solution and ideally suited for this purpose. Routine CP-based could add confidence to and save costs for the drug discovery pipeline. We have shown that CP can be leveraged to capture the morphological changes associated with neural differentiation. We could reliably predict the gradual transition from NPC to a more differentiated state. We realize that differentiating iPSC cultures are highly heterogenous and are composed of a landscape of transitioning progenitors in various stages of maturity that our binary models currently fail to capture. As a result, we filtered out cells with a ‘dubious’ IF profile (e.g., cells that might be transitioning or are of a different type) as they would negatively affect the model by introducing noise. In future iterations, one could envision defining more refined cell (sub-)types in a population based on richer post-hoc information (e.g., through cyclic immunofluorescence or spatial single cell transcriptomics). While we emphasize the value of identifying fixed states as a fast-track to gather the composition of an iPSC-derived culture, it is equally interesting to focus on the continuum that such cultures represent. To predict the maturation of iPSC-derived neural progenitors to differentiated neurons in a more continuous manner, would demand regression-based approaches <sup><xref ref-type="bibr" rid="c45">45</xref></sup>. Pioneering efforts using live-cell imaging and machine learning have allowed predicting gradual cell state transitions, for example in the context of myoblast, adipocyte or osteoblast differentiation <sup><xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c47">47</xref></sup>. Label-free timelapse imaging has also been shown to aid in the assessment of differentiation without disruption of the sample<sup><xref ref-type="bibr" rid="c46">46</xref></sup>. Given that many of the cell painting dyes are compatible with live cells, it is conceivable that our approach is amenable to further refining the assessment of iPSC differentiation status in real time.</p>
<p>After testing the CNN performance on heterogenous cultures, we have added an additional layer of complexity by inducing microglial reactivity in a coculture of neurons and microglia. We found that we could still predict the cell type regardless of the treatment. The increased variability within the microglial subpopulation did not impact the CNNs’ ability to discriminate cell types in mixed culture. Furthermore, using the layered approach, the resting-state and reactive subgroups within the microglial population could also be classified, albeit with a lesser prediction accuracy. This could be explained by the fact that microglia grown <italic>in vitro</italic> are not completely homeostatic, even in absence of LPS <sup><xref ref-type="bibr" rid="c48">48</xref></sup>, and may require different cell types to adopt a more natural state. Having more faithful mixed cultures with cells in their endogenous states, and an apt tool to recognize both cell type and state, holds promise to enhance the relevance of preclinical screens, increase the accuracy of drug targets and ultimately lead to more precise therapeutic strategies<sup><xref ref-type="bibr" rid="c49">49</xref></sup>. For now, we have tailored models to the individual datasets, but it is conceivable that a more generalized CNN could be established for multiple culture types. This would obviously demand a much larger dataset to encompass the variability encountered in such models (<italic>e.g.,</italic> various starting iPSC lines, various differentiation protocols). Publicly available datasets (<italic>e.g.,</italic> JUMP-Cell Painting Consortium) can aid in creating an input database containing a large variability (different iPSC lines, different neural differentiation protocols, …), which would ultimately lead to a more robust predictor. Our results showing the prediction accuracy of a guided differentiation model on spontaneously differentiating cultures indicate that the approach can be transferred to other differentiation protocols as well. Inclusion of more input images and variability should thus enable developing a generalist model for other differentiation protocols without the need for ground truth validation and further CNN training.</p>
<p>In conclusion, we have developed a novel application for unbiased morphological profiling by extending its use to complex mixed neural cultures using sequential multispectral imaging and convolutional network-informed cell phenotype classification. We show that the resulting predictors are robust with respect to biological variation and cell culture density. This method holds promise for use in quality control of iPSC cultures to allow their routine use in high-throughput and high-content screening applications.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Cell culture</title>
<p>Cells were cultured at 37 °C and 5% CO2. 1321N1 and SH-SY5Y cell lines were maintained in DMEM-F12 + Glutamax (Gibco, 10565018) supplemented with 10% Fetal Bovine Serum (Gibco, 10500064). Cell seeding prior to imaging was done in 96-well black multiwell plates with #1.5 glass-like polymer coverslip bottom (Cellvis, P96-1.5P). Only the inner 60 wells were used, while the outer wells were filled with PBS-/- to avoid plate effects. Plates were coated with Matrigel (Corning, 734-1440) After seeding, the imaging plate was centrifuged at 100g for 3min.</p>
<p>iPSCs (Sigma Aldrich, iPSC0028 Epithelial-1) were cultured on Matrigel (Corning, 734-1440) in Essential 8 medium (Gibco, A1517001). Upon cell thawing, Rock inhibitor (Y-27632 dichloride, MedChem, HY-10583) was added in 10µM concentration. Subculturing of iPSCs was performed with ReLeSR (Stemcell Technologies, 05872).</p>
<p>Unguided differentiation<sup><xref ref-type="bibr" rid="c34">34</xref></sup> of iPS cells to neural progenitor cells (NPCs) was started by subculturing the iPSCs single cell using Tryple Express Enzyme (Life technologies, 12605010) at a density of 10e4 cells/cm² in mTesR1 medium (Stemcell Technologies, 85850) and Rock inhibitor. The following day, differentiation to NPCs was started by dual-SMAD inhibition in neural maintenance medium (1:1 Neurobasal (Life technologies, 21103049):DMEM-F12+Glutamax (Gibco, 10565018), 0.5x Glutamax (Gibco, 35-050-061), 0.5% Mem Non Essential Amino Acids Solution (Gibco, 11140050), 0.5% Sodium Pyruvate (Gibco, 11360070), 50 μM 2-Mercaptoethanol (Gibco, 31350010), 0.025% Human Insulin Solution (Sigma Aldrich, I9278), 0.5X N2 (Gibco, 17502048), B27 (Gibco, 17504044), 50 U/ml Penicillin-Streptomycin (Gibco, 15140122)) supplemented with 1 μM LDN-193189 (Miltenyi, 130-106-540), SB431542 (Tocris, 1614). This dual blockade of SMAD signalling in iPSCs is induces neural differentiation by synergistically causing the loss of pluripotency and push towards neuroectodermal lineage<sup><xref ref-type="bibr" rid="c50">50</xref></sup>. Daily medium changes were performed for 11 consecutive days. Following neural induction, neural rosettes were selected by STEMdiff Neural Rosette Selection Reagent (Stemcell technologies, 05832). Maintenance of neural progenitor cells was performed in neural maintenance medium with 10 µM bFGF (Peprotech, 100-18C) added after subculturing. Single cell detachment of NPCs during maintenance was performed using Tryple Express Enzyme. Cell seeding prior to imaging is done in 96-well black multiwell plates µCLEAR (Greiner, 655090) coated with Poly-L-ornithine (Sigma-Aldrich, P4957) and laminin (Sigma-Aldrich, L2020). Only the inner 60 wells were used, while the outer wells were filled with PBS-/- to avoid plate effects. After seeding, the imaging plate was centrifuged at 100g for 3min.</p>
<p>Guided iPSC differentiation to neurons was performed according to Bell et al. (2019)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>. The initial phase of neural induction consisted of 12 days neural induction medium (DMEM-F12+Glutamax (Gibco, 10565018), 1x N2 (Gibco, 17502048), 1x B27 (Gibco, 17504044),1mg/ml BSA (Sigma-Aldrich, A7979), 0.5% Mem Non-Essential Amino Acids Solution (Gibco, 11140050)). Of these 12 days, the first 7 were supplemented with cytokines for dual-SMAD inhibition (1 μM LDN-193189 (Miltenyi, 130-106-540), SB431542 (Tocris, 1614)). Following neural induction, the NPCs were floated in uncoated MW6 culture plates in NPC medium (DMEM-F12+Glutamax (Gibco, 10565018), 1x N2 (Gibco, 17502048), 1x B27 (Gibco, 17504044), 10 µM bFGF (Peprotech, 100-18C), 10 µM EGF (Peprotech, 100-47)). After 4 days, NPC clusters of appropriate size were filtered using a cell strainer ((37 µm cell strainer, Stemcell technologies, 27250) and plated on Matrigel-coated (Corning, 734-1440) MW6 culture plates. NPCs can now be expanded in NPC medium. Guided differentiation into forebrain neurons can be induced by switching to neuronal medium (BrainPhys (STEMCELL Technologies, 05792), 1x N2 (Gibco, 17502048), 1x B27 (Gibco, 17504044), 10 µM BDNF (PeproTech, AF-450-02), 10 µM GDNF (PeproTech, AF-450-02) for 15 days before fixation. Differentiation of iPSC to microglia<sup><xref ref-type="bibr" rid="c4">4</xref></sup> was performed by the formation of embroid bodies (EBs) with 10e3 iPSCs/well in a 96-well U-bottom plate (Corning, 351177) coated with Anti-Adherence Rinsing Solution (Stemcell technologies, 07010) in mTeSR medium supplemented with Rock inhibitor, 50 ng/mL BMP4 (Peprotech, 120-05), 50 ng/mL VEGF (Peprotech, 100-20), 20 ng/mL SCF (Peprotech, 250-03). 75% medium is changed for 4 consecutive days. After mesoderm induction, EBs are transferred to a 6-well plate with 20 EBs/well and placed in macrophage precursor medium (X-vivo15 (Lonza, BE02-060Q), 100 ng/mL M-CSF (Peprotech, 300-25), 25 ng/mL IL-3 (Peprotech, 213-13), 1x Glutamax, 50 U/ml Penicillin-Streptomycin, 50 μM 2-Mercaptoethanol). 14 days after macrophage differentiation, macrophage precursors were harvested using a cell strainer (Stemcell technologies, 27250). Macrophage precursors were added to the NPC culture in 1:1 neural maintenance medium:microglia medium ((DMEM-F12+Glutamax, 100 ng/mL M-CSF (Peprotech, 300-25), 100 ng/mL IL-34 (Peprotech, 200-34), 1x Glutamax, 50 U/ml Penicillin-Streptomycin, 50 μM 2-Mercaptoethanol).</p>
</sec>
<sec id="s4b">
<title>Replication labelling</title>
<p>Prior to co-seeding of 1321N1 and SH-SY5Y mixed cultures, individual cultures were incubated with either 10 µM EdU (Click-iT® EdU Imaging Kit, Life Technologies, C10340) or 10 µM BrdU (Sigma-Aldrich, B5002) for 24h. This incubation time exceeded the doubling time, allowing incorporation of the nucleotide analog in all cells. This labelling period was followed by a 24h washout period in regular cell culture medium. After washout, the cells were subcultured and plated in coculture. In half of the replicate, SH-SY5Y cells received BrdU while 1321N1 cells received EdU. For the remainder of wells, the pre-label switched cell types.</p>
</sec>
<sec id="s4c">
<title>Morphological staining</title>
<p>Morphological staining (Cell Painting) was adapted from Bray et al. 2016<sup><xref ref-type="bibr" rid="c19">19</xref></sup>. After careful titration, all dye concentrations were adjusted and evaluated for compatibility with 4-channel laser and filter combinations available on the confocal microscope (see further). Staining was performed on cell cultures fixed in 4% PFA (roti-histofix 4% paraformaldehyde, Roth, 3105.2) for 15min. Cells were rinsed once with PBS-/- (Life Technologies, 10010015) prior to fixation and 3x 5min post fixation. Staining solutions were prepared fresh before staining in PBS-/- with 0.3% Triton-X-100 (Sigma Aldrich, X100) (<xref rid="tbl1" ref-type="table">Table 1</xref>). Each staining solution was incubated for 30min on a shaker at RT in the dark. After staining, the cells were washed 1x with PBS -/- and sealed for imaging.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1</label>
<caption><title>Specifications of morphological staining composition.</title></caption>
<graphic xlink:href="574474v2_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s4d">
<title>Cyclic staining and immunocytochemistry</title>
<p>Cyclic staining is executed by fluorescence quenching after each sequential imaging round. 1mg/ml in ddH2O LiBH4 solution<sup><xref ref-type="bibr" rid="c32">32</xref></sup> (Acros Organics, 206810050) was prepared fresh before use. 1.5h incubation of quenching solution was performed before each successive staining series. After incubation, the quenching solution was removed by washing 3x 5min in PBS-/-. Successful fluorescence quenching was microscopically verified before proceeding with immunofluorescence staining (IF) (<xref rid="tbl2" ref-type="table">Table 2</xref>, <xref rid="figs4" ref-type="fig">Suppl. Fig. 4</xref>).</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2</label>
<caption><title>Used antibodies.</title></caption>
<graphic xlink:href="574474v2_tbl2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>Cells are treated with PAV blocking buffer (Thimerosal 0.5% (Fluka, 71230), NaN3 0.1% (Merck, k 6688), Bovine serum albumin (Sigma-Aldrich, A7284), Normal horse serum, PBS-/-) for 8min. The desired primary antibodies (pAB) are diluted in PAV blocking buffer. pAB incubation was performed 12h (overnight) at 4°C after which the cells were washed 1x 5min in PBS-/- followed by incubation in secondary antibody solution (sAB) in PAV + DAPI for nuclear counterstain. sAB staining was performed for 3h at RT while shaking. Prior to imaging, the cells were washed 2x with PBS-/- and stored in PBS-/- + 0,1% NaN3.</p>
<p>BrdU staining was performed using IF, requiring DNA denaturation before pAB incubation. This was performed by 10 min incubation with 2N HCl at 37°C. HCl was neutralized with 0.1 M sodium borate buffer pH 8.5 for 30min at RT. Cells were washed 3x 5min in PBS-/- before continuing with the general IF protocol. EdU click-it labelling was performed according to the manufacturer’s instructions (Click-iT® EdU Imaging Kit, Life Technologies, C10340) (<xref rid="figs5" ref-type="fig">Suppl. Fig. 5A</xref>).</p>
</sec>
<sec id="s4e">
<title>Image acquisition</title>
<p>Images were acquired using a spinning disk confocal microscope (Nikon CSU-W1 SoRa) with a 20x 0.75 NA objective (Plan APO VC 20x DIC N2) and Kinetix sCMOS camera (pinhole 50µm; disk speed 4000 rpm; pinhole aperture 10; bit depth 12-bit, pixel size 0.325µm²). We opted for confocal microscopy instead of widefield to overcome image quality limitations resulting from highly dense cell clusters. 96-well plates were scanned, capturing a minimum of 16 images per well spread in a regular pattern (0,8 mm spacing in x and y) across the surface of the well. If multiple z-slices were acquired (to correct for surface inclinations in the field of view), a maximum projection was performed before further analysis. Morphological images were acquired in all 4 channels. (<xref rid="tbl3" ref-type="table">Table 3</xref>).</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3</label>
<caption><title>Specifications of the used laser lines, excitation and emission filters.</title></caption>
<graphic xlink:href="574474v2_tbl3.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="574474v2_tbl3a.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s4f">
<title>Software</title>
<p>Images were captured by the Nikon CSU-W1 confocal system in combination with the NIS elements software (RRID:SCR_014329). Image visualization was later performed using Fiji freeware<sup><xref ref-type="bibr" rid="c51">51</xref>,<xref ref-type="bibr" rid="c52">52</xref></sup>. In-depth image analysis, pre-processing and machine learning for cell classification were performed using Python programming language<sup><xref ref-type="bibr" rid="c53">53</xref></sup> in combination with Anaconda<sup><xref ref-type="bibr" rid="c54">54</xref></sup> (distribution and package managing software) and Visual Studio Code (code editor). The packages and versions used for data analysis are shown in <xref rid="tbl4" ref-type="table">Table 4</xref>.</p>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4</label>
<caption><title>Python packages used for image and data analysis alongside the software version.</title></caption>
<graphic xlink:href="574474v2_tbl4.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="574474v2_tbl4a.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s4g">
<title>Image pre-processing (<xref rid="figs5" ref-type="fig">Suppl. Fig. 5B</xref>)</title>
<p>Note: all image and data analysis scripts are available on GitHub. See further the data availability statement.</p>
<sec id="s4g1">
<title>Cell/nuclear segmentation</title>
<p>Using a tailor-made script implementing the Cellpose<sup><xref ref-type="bibr" rid="c55">55</xref></sup> (cell segmentation) or StarDist<sup><xref ref-type="bibr" rid="c56">56</xref></sup> (nuclear segmentation) package, images were pre-processed by normalizing the intensity values of each channel between the 1st and 99th quantile. Individual channels or channel combinations for segmentation can be selected depending on the desired outcome mask. Resulting outputs included the segmentation mask alongside the quality control images. For Stardist implementation, hyperparameter probability was set at 0,6 and overlap at 0,3. For Cellpose segmentation, 4 models (cyto2) were averaged to obtain the final mask. Segmentation was performed on the composite of all CP channels. An estimation of the cell’s diameter could be included to optimize cell detection. Cell segmentation was performed using Cellpose and used in all cases where the whole-cell crop was given as input to the CNN or data from the whole cell was used for feature extraction for RF (<xref rid="fig1" ref-type="fig">Fig. 1</xref>-<xref rid="fig4" ref-type="fig">4</xref>). Nuclear segmentation was performed using Stardist and used for nuclear and nucleocentric crops (<xref rid="fig4" ref-type="fig">Fig. 4</xref>-<xref rid="fig7" ref-type="fig">7</xref>).</p>
</sec>
<sec id="s4g2">
<title>Ground truth alignment</title>
<p>Following sequential staining and imaging rounds, multiple images were captured representing the same cell with different markers. Lifting the plate of the microscope stage and imaging in sequential rounds after several days results in small linear translations in the exact location of each image. These linear translations need to be corrected to align morphological with ground truth image data within the same ROI. All images were aligned using Fourier-based image cross correlation on the intensity-normalized multichannel images. The alignment shift between image1 and image2 was determined using scikit-image phase cross correlation. Image2 was then moved according to the predetermined shift to align morphological with ground truth images.</p>
</sec>
<sec id="s4g3">
<title>Ground truth phenotyping</title>
<p>The true cell phenotype was determined by the fluorescence intensity of the post-hoc immunostaining with class-specific markers or pre-labelling with Edu/Brdu. In this latter case, the base analogues were incorporated into each cell line prior to mixing them, i.e. when they were still growing in monoculture so they could be labelled and identified after co-seeding and morphological profiling. Each ground truth image was imported alongside the corresponding cell mask for that image. For each cell label, the fluorescence intensity was determined and tabulated. The threshold was set manually based on the fluorescence intensity of the monoculture controls. Ground truth labels were assigned to each region of interest (ROI).</p>
</sec>
<sec id="s4g4">
<title>Feature extraction</title>
<p>Handcrafted features were extracted using the scikit-image package (regionprops and GLCM functions). The definition of each feature extracted from the image is listed in <xref rid="tbl5" ref-type="table">table 5</xref>. All features were extracted for each channel in the cell painting image and for every region (cell, cytoplasm and nucleus) within the ROI.</p>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5</label>
<caption><title>Definition of handcrafted features (according to the scikit-image documentation). All of these features were extracted for each fluorescent channel (1-4) and region (nucleus, cytoplasm and whole-cell) in the cell painting images.</title></caption>
<graphic xlink:href="574474v2_tbl5.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="574474v2_tbl5a.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s4g5">
<title>Data filtering</title>
<p>Over the entire pipeline, ROIs could be discarded based on 3 conditions: (1) Cell detachment. Due to the harsh sample preparation and repeated washing steps, cells could detach from the imaging substrate thus resulting in lack of ground truth information for those cells. As a result, all ROIs for which there was no DAPI signal detected in the ground truth image, were removed from the dataset as incomplete. (2) Cells for which the ground truth fluorescence intensity was ambiguous. Ground truth labels were determined based on the presence of specific IF staining in either of the phenotype classes. If no class-specific IF staining was detected by thresholding, no true label could be assigned. These ROIs were therefore discarded due to uncertainty. (3) Likelihood of faulty ROI detection. The DAPI signal was used to discard ROIs that do not represent (complete) cells by thresholding on minimal DAPI intensity (mean nuclear DAPI intensity &gt; 500) and minimal nuclear size (nuclear area &gt; 160).</p>
</sec>
</sec>
<sec id="s4h">
<title>ROI classification</title>
<sec id="s4h1">
<title>Train-validation-test split</title>
<p>Model training requires splitting the available data in 3 subsets: training (60%), validation (10%), and testing (30%). The training dataset was used to train the machine learning models (either RF or CNN). The validation dataset, composing of 10% of the total data, was used to determine the hyperparameters and intermediate testing. The remaining 30% of the dataset was kept apart and used to test the final model when training is completed. For both RF and CNN, the testing dataset was never shown to the model during the training phase, but only used after training to determine the accuracy of predictions to the ground truth. The number of instances for each class was equalized by sampling equal number of instances from each predicted class. To account for variation between technical replicates, the train-validation-test split was stratified per well. As a result, no datapoints arising from the same well could appear simultaneously in the training, validation and testing subset. This data stratification was repeated 3 times with different random seeds (see Methods – Reproducibility).</p>
</sec>
<sec id="s4h2">
<title>Random Forest</title>
<p>For each ROI, a set of manually defined parameters was extracted corresponding to cell shape (area, convex area, filled area, length minor axis, length major axis, centroid, eccentricity, equivalent diameter area, ferret diameter max, orientation, perimeter, solidity), texture (GLCM: contrast, dissimilarity, homogeneity, energy, correlation, ASM) and intensity (maximal intensity, minimal intensity, mean intensity, standard deviation intensity). This was done for 3 regions (nucleus, cytoplasm and complete cell), and for all channel dimensions. Redundant parameters could be removed if above a 0.95 correlation threshold. All parameters were standardized per ROI, grouped per replicate. The number of trees within the forest was varied between 10 and 150, reaching maximum accuracy at around 30 trees.</p>
</sec>
<sec id="s4h3">
<title>Uniform Manifold Approximation and Projection</title>
<p>Dimensionality reduction using UMAP was performed using either the same feature matrix as used for RF prediction or the feature embeddings from the trained CNN classification network. Hyperparameters were set at the default settings.</p>
</sec>
<sec id="s4h4">
<title>Convolutional neural network</title>
<p>A ResNet50 model was trained for image classification. In contrast to classical machine learning techniques, no handcrafted features were extracted. Crops were defined based on the segmentation mask for each ROI. The bounding box was cropped out of the original image with a fixed patch size (60µm for whole cells, 18µm for nucleus and nucleocentric crops) surrounding the centroid of the segmentation mask. For the whole cell and nuclear crops, all pixels outside of the segmentation mask were set to zero. This was not the case for the nucleocentric crops. Each ROI was cropped out of the original morphological image and associated with metadata corresponding to its ground truth label. Images alongside their labels were fed to the network. Tensors were normalized per channel. Models are trained on a minimum of 5000 training inputs of each class for 50 epochs (training iterations). Each batch consisted of 100 samples. The training input was augmented by linear transformations (horizontal and vertical flip, random rotation). Each epoch, the current model was tested against a validation dataset (<xref rid="figs5" ref-type="fig">Suppl. Fig. 5C</xref>). The performance of the model on this validation subset determined whether the model was stored as new best (if the new accuracy exceeded the accuracy of the previous best model) or discarded. The learning rate at the start was set at 0,0001 and automatically reduced with a factor of 0,1 during training when no improvement was seen after 10 epochs. After 50 epochs, the best resulting model was tested on a separate test dataset to determine the accuracy on previously unseen data.</p>
</sec>
</sec>
<sec id="s4i">
<title>GradCAM</title>
<p>GradCAM analysis was used to visualize the regions used by the CNN for classification. This map is specific to each cell. Images are selected randomly out the full dataset for visualization. To avoid cherry-picking, a set of GradCam maps is reported alongside the random seed used for image selection.</p>
</sec>
<sec id="s4j">
<title>Reproducibility</title>
<p>Each model training was performed 3 independent times (model initializations, repetitions are indicated with ‘N’ on the figures). This was repeated for 3 different random seeds. Each model received input data arising from a minimum of 16 images per well, at least 15 technical replicates (wells). The optimization experiments (<xref rid="fig1" ref-type="fig">figures 1</xref>-<xref rid="fig4" ref-type="fig">4</xref>) were performed with cell lines with limited variability. These models were trained on 3 independent experiments where ground truth pre-labelling (Edu/BrdU) was performed at least once on either of the cell lines in coculture. For iPSC-derived cultures, as variability is inherent to these differentiations, 3 biological replicates (independent differentiations) were pooled for model training.</p>
</sec>
<sec id="s4k">
<title>Statistics</title>
<p>All statistical comparisons were made nonparametric using Mann-Whitney U (for two independent sample comparison) or Kruskal-Wallis (for multiple sample comparison) with pairwise tests using Tukey’s honestly significant difference test. We opted for nonparametric testing because the number of models in each group to be compared was &lt; 15. Significance levels are indicated on the figures using ns. (no statistical significance, p-value above 0,05), * (p-value between 0,05 and 5e-4), ** (p-value between 5e-4 and 5e-6) and *** (p-value smaller than 5e-6).</p>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Funding</title>
<p>This work was funded by Fonds Wetenschappelijk Onderzoek Vlaanderen (1SB7423N; 1274822N; G033322N), BOF (FFB210009) and IOF UAntwerpen (FFI210239; FFI230099) and VLAIO (HBC.2023.0155).</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Marlies Verschuuren and Hugo Steenberghen for their assistance and knowledge regarding the cell lines. By extension, we would like to thank all current and former members of the De Vos lab.</p>
</ack>
<sec id="s6">
<title>Data availability statement</title>
<p>The authors report that the results of this study are available within the manuscript and supplementary materials. All image analysis scripts are open-source available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/DeVosLab/Nucleocentric-Profiling">https://github.com/DeVosLab/Nucleocentric-Profiling</ext-link>)</p>
</sec>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item>
<term>BrdU</term>
<def><p>Bromodeoxyuridine</p></def>
</def-item>
<def-item>
<term>CNN</term>
<def><p>convolutional neural network</p></def>
</def-item>
<def-item>
<term>CP</term>
<def><p>cell painting</p></def>
</def-item>
<def-item>
<term>DAPI</term>
<def><p>4’,6-diamidino-2-fenylindool</p></def>
</def-item>
<def-item>
<term>DIV</term>
<def><p>days in vitro</p></def>
</def-item>
<def-item>
<term>EdU</term>
<def><p>5-ethynyl-2’-deoxyuridine</p></def>
</def-item>
<def-item>
<term>ER</term>
<def><p>endoplasmic reticulum</p></def>
</def-item>
<def-item>
<term>Grad-CAM</term>
<def><p>Gradient-weighted Class Activation Mapping</p></def>
</def-item>
<def-item>
<term>IF</term>
<def><p>immunofluorescence</p></def>
</def-item>
<def-item>
<term>iPSC</term>
<def><p>induced pluripotent stem cell</p></def>
</def-item>
<def-item>
<term>NPC</term>
<def><p>neural progenitor cell</p></def>
</def-item>
<def-item>
<term>RF</term>
<def><p>random forest</p></def>
</def-item>
<def-item>
<term>ROI</term>
<def><p>region of interest</p></def>
</def-item>
<def-item>
<term>TUBB3</term>
<def><p>beta-III-tubulin</p></def>
</def-item>
<def-item>
<term>UMAP</term>
<def><p>Uniform Manifold Approximation and Projection</p></def>
</def-item>
</def-list>
</glossary>
<sec id="s7">
<title>Additional information</title>
<sec id="s7a">
<title>Conflict of interest</title>
<p>The authors declare no conflict of interest.</p>
</sec>
<sec id="s9">
<title>Author contributions</title>
<p>The experiments were conceptualized by SDB, TVDL, JVDD, PP and WDV. Experiments were executed by SDB and JVDD. TVDL and SDB optimized the data analysis scripts. SDB analysed the imaging data and performed the data analysis. SDB and WDV prepared the original draft. PP and WDV supervised the work. All authors took part in reviewing and editing the manuscript.</p>
</sec>
</sec>
<sec id="s10">
<title>Supplementary figures</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary figure 1</label>
<caption><p>(A) PCA dimensionality reduction on handcrafted features extracted from monocultures of 1321N1 and SH-SY5Y cells. Each point represents an individual cell. (B) UMAP of feature embeddings of the CNN trained to classify 1321N1 and SH-SY5Y monocultures. Each point represents an individual cell. (C) Examples of misclassified ROIs. (D) Left: Representative images of 1321N1 cells with increasing density alongside their cell and nuclear mask produced using resp. Cellpose and Stardist. Images are numbered from 1-5 with increasing density. Upper right: The number of ROIs detected in comparison to the ground truth (manual segmentation). A ROI was considered undetected when the intersection over union (IoU) was below 0,15. Each bar refers to the image number on the left. The IoU quantifies the overlap between ground truth (manually segmented ROI) and the ROI detected by the segmentation algorithm. It is defined as the area of the overlapping region over the total area. IoU for increasing cell density for cell and nuclear masks is given in the bottom right. Each point represents an individual ROI. Each bar refers to the image number on the left. (E) Examples of segmentation mistakes made by the Stardist segmentation algorithm for nuclear segmentation for different culture densities. (F) Nuclear area in function of density. (G) Definition of cell regions given as training input for nuclear and nucleocentric model training.</p></caption>
<graphic xlink:href="574474v2_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary figure 2</label>
<caption><title>Influence of the nuclear/cellular size on CNN prediction and association with the input background (space artificially set to zero outside of the segmentation mask).</title>
<p>(A) Quantification of average nuclear/cellular size per cell type. (B) GradCAM images for 10 random seeds for crops and CNN models trained with background either set to zero or ‘random speckle’. (C) CNN prediction results for models trained on crops with background either set to zero or ‘random speckle’. Each dot in the violinplots represents the F-score of one classifier (model initialization, N = 3). (D) Feature map (see UMAP in <xref rid="fig1" ref-type="fig">figure 1 B</xref> and <xref rid="fig1" ref-type="fig">C</xref>) of 1321N1 and SH-SY5Y cells showing the contribution of nuclear/cellular area to the cell type cluster separation. Each point represents an individual cell.</p></caption>
<graphic xlink:href="574474v2_figs2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary figure 3</label>
<caption><title>GradCAM maps per region and density.</title></caption>
<graphic xlink:href="574474v2_figs3.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="574474v2_figs3a.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="574474v2_figs3b.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="574474v2_figs3c.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="574474v2_figs3d.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="574474v2_figs3e.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary figure 4</label>
<caption><title>Fluorescence quenching over time using LiBH4.</title>
<p>(A) Images before and after quenching for all 4 fluorescence channels. (B) Time curve of normalized image-level fluorescence intensity during incubation with 1mg/ml LiBH4.</p></caption>
<graphic xlink:href="574474v2_figs4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary figure 5</label>
<caption><title>Methods.</title>
<p>(A) Pipeline used for morphological profiling in mixed cultures. (B) Overview of the steps within the image analysis pipeline. (C) Evaluation of accuracy, true negative and true positive rate during CNN training (1321N1 vs. SH-SY5Y in monocultures) across all 50 epochs.</p></caption>
<graphic xlink:href="574474v2_figs5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Strooper</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Karran</surname>, <given-names>E</given-names></string-name></person-group>. <article-title>The Cellular Phase of Alzheimer’s Disease</article-title>. <source>Cell</source> <volume>164</volume>, <fpage>603</fpage>–<lpage>615</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bell</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Differentiation of Human Induced Pluripotent Stem Cells (iPSCs) into an Effective Model of Forebrain Neural Progenitor Cells and Mature Neurons</article-title>. <source>BIO-PROTOCOL</source> <volume>9</volume>, (<year>2019</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neyrinck</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>SOX9-induced Generation of Functional Astrocytes Supporting Neuronal Maturation in an All-human System</article-title>. <source>Stem Cell Rev and Rep</source> <volume>17</volume>, <fpage>1855</fpage>–<lpage>1873</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haenseler</surname>, <given-names>W.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A Highly Efficient Human Pluripotent Stem Cell Microglia Model Displays a Neuronal-Co-culture-Specific Expression Profile and Inflammatory Response</article-title>. <source>Stem Cell Reports</source> <volume>8</volume>, <fpage>1727</fpage>–<lpage>1742</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nevin</surname>, <given-names>Z. S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Modeling the Mutational and Phenotypic Landscapes of Pelizaeus-Merzbacher Disease with Human iPSC-Derived Oligodendrocytes</article-title>. <source>The American Journal of Human Genetics</source> <volume>100</volume>, <fpage>617</fpage>–<lpage>634</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abutaleb</surname>, <given-names>N. O.</given-names></string-name> &amp; <string-name><surname>Truskey</surname>, <given-names>G. A</given-names></string-name></person-group>. <article-title>Differentiation and characterization of human iPSC-derived vascular endothelial cells under physiological shear stress</article-title>. <source>STAR Protocols</source> <volume>2</volume>, <fpage>100394</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paik</surname>, <given-names>D. T.</given-names></string-name>, <string-name><surname>Chandy</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Wu</surname>, <given-names>J. C</given-names></string-name></person-group>. <article-title>Patient and Disease–Specific Induced Pluripotent Stem Cells for Discovery of Personalized Cardiovascular Drugs and Therapeutics</article-title>. <source>Pharmacol Rev</source> <volume>72</volume>, <fpage>320</fpage>–<lpage>342</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lopez-Gonzalez</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Poly(GR) in C9ORF72 -Related ALS/FTD Compromises Mitochondrial Function and Increases Oxidative Stress and DNA Damage in iPSC-Derived Motor Neurons</article-title>. <source>Neuron</source> <volume>92</volume>, <fpage>383</fpage>–<lpage>391</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amin</surname>, <given-names>N. D.</given-names></string-name> &amp; <string-name><surname>Paşca</surname>, <given-names>S. P</given-names></string-name></person-group>. <article-title>Building Models of Brain Disorders with Three-Dimensional Organoids</article-title>. <source>Neuron</source> <volume>100</volume>, <fpage>389</fpage>–<lpage>405</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rouhani</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Genetic Background Drives Transcriptional Variation in Human Induced Pluripotent Stem Cells</article-title>. <source>PLoS Genet</source> <volume>10</volume>, <fpage>e1004432</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salomonis</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Integrated Genomic Analysis of Diverse Induced Pluripotent Stem Cells from the Progenitor Cell Biology Consortium</article-title>. <source>Stem Cell Reports</source> <volume>7</volume>, <fpage>110</fpage>–<lpage>125</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamamoto</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Arita</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kuroda</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Suzuki</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Kawamata</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Improving the differentiation potential of pluripotent stem cells by optimizing culture conditions</article-title>. <source>Sci Rep</source> <volume>12</volume>, <fpage>14147</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuijlaars</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Sustained synchronized neuronal network activity in a human astrocyte co-culture system</article-title>. <source>Sci Rep</source> <volume>6</volume>, <fpage>36529</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hernández</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Culture Variabilities of Human iPSC-Derived Cerebral Organoids Are a Major Issue for the Modelling of Phenotypes Observed in Alzheimer’s Disease</article-title>. <source>Stem Cell Rev and Rep</source> <volume>18</volume>, <fpage>718</fpage>–<lpage>731</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>D’Antonio</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>High-Throughput and Cost-Effective Characterization of Induced Pluripotent Stem Cells</article-title>. <source>Stem Cell Reports</source> <volume>8</volume>, <fpage>1101</fpage>–<lpage>1111</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sullivan</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Quality Control Guidelines for Clinical-Grade Human Induced Pluripotent Stem Cell Lines</article-title>. <source>Regen. Med</source>. <volume>13</volume>, <fpage>859</fpage>–<lpage>866</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>C. X.-Q.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Standardized Quality Control Workflow to Evaluate the Reproducibility and Differentiation Potential of Human iPSCs into Neurons</article-title>. <source>bioRxiv</source> (<year>2021</year>) doi:<pub-id pub-id-type="doi">10.1101/2021.01.13.426620</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gustafsdottir</surname>, <given-names>S. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Multiplex Cytological Profiling Assay to Measure Diverse Cellular States</article-title>. <source>PLoS ONE</source> <volume>8</volume>, <fpage>e80999</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bray</surname>, <given-names>M.-A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cell Painting, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes</article-title>. <source>Nat Protoc</source> <volume>11</volume>, <fpage>1757</fpage>–<lpage>1774</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cimini</surname>, <given-names>B. A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Optimizing the Cell Painting assay for image-based profiling</article-title>. <source>Nat Protoc</source> <volume>18</volume>, <fpage>1981</fpage>–<lpage>2013</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Way</surname>, <given-names>G. P.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Predicting cell health phenotypes using image-based morphology profiling</article-title>. <source>MBoC</source> <volume>32</volume>, <fpage>995</fpage>–<lpage>1005</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schiff</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Integrating deep learning and unbiased automated high-content screening to identify complex disease signatures in human fibroblasts</article-title>. <source>Nat Commun</source> <volume>13</volume>, <fpage>1590</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caicedo</surname>, <given-names>J. C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cell Painting predicts impact of lung cancer variants</article-title>. <source>MBoC</source> <volume>33</volume>, <fpage>ar49</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tegtmeyer</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>High-dimensional phenotyping to define the genetic basis of cellular morphology</article-title>. <source>Nat Commun</source> <volume>15</volume>, <fpage>347</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rajaram</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pavie</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>L. F.</given-names></string-name> &amp; <string-name><surname>Altschuler</surname>, <given-names>S. J</given-names></string-name></person-group>. <article-title>PhenoRipper: software for rapidly profiling microscopy images</article-title>. <source>Nat Methods</source> <volume>9</volume>, <fpage>635</fpage>–<lpage>637</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Uhlmann</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Singh</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Carpenter</surname>, <given-names>A. E</given-names></string-name></person-group>. <article-title>CP-CHARM: segmentation-free image classification made accessible</article-title>. <source>BMC Bioinformatics</source> <volume>17</volume>, <fpage>51</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caicedo</surname>, <given-names>J. C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Data-analysis strategies for image-based cell profiling</article-title>. <source>Nat Methods</source> <volume>14</volume>, <fpage>849</fpage>–<lpage>863</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>McInnes</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Healy</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Melville</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</article-title>. <source>arXiv</source> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.48550/ARXIV.1802.03426</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname>, <given-names>T.-T.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>J. Z.</given-names></string-name> &amp; <string-name><surname>Nguyen</surname>, <given-names>T. T</given-names></string-name></person-group>. <article-title>Unbiased Feature Selection in Learning Random Forests for High-Dimensional Data</article-title>. <source>The Scientific World Journal</source> <volume>2015</volume>, <fpage>1</fpage>–<lpage>18</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Sun</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Deep Residual Learning for Image Recognition</article-title>. <source>arXiv</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</ext-link> (<year>2015</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Selvaraju</surname>, <given-names>R. R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization</article-title>. <source>Int J Comput Vis</source> <volume>128</volume>, <fpage>336</fpage>–<lpage>359</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radtke</surname>, <given-names>A. J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>IBEX: an iterative immunolabeling and chemical bleaching method for high-content imaging of diverse tissues</article-title>. <source>Nat Protoc</source> <volume>17</volume>, <fpage>378</fpage>–<lpage>401</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Volpato</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Webber</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Addressing variability in iPSC-derived models of human disease: guidelines to promote reproducibility</article-title>. <source>Disease Models &amp; Mechanisms</source> <volume>13</volume>, <fpage>dmm042317</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Kirwan</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Livesey</surname>, <given-names>F. J</given-names></string-name></person-group>. <article-title>Directed differentiation of human pluripotent stem cells to cerebral cortex neurons and neural networks</article-title>. <source>Nat Protoc</source> <volume>7</volume>, <fpage>1836</fpage>–<lpage>1846</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Dincer</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Celik</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hiranuma</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Lee</surname>, <given-names>S.-I</given-names></string-name></person-group>. <article-title>DeepProfile: Deep Learning of Cancer Molecular Profiles for Precision Medicine</article-title>. <source>bioRxiv</source> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.1101/278739</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weigert</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Content-aware image restoration: pushing the limits of fluorescence microscopy</article-title>. <source>Nat Methods</source> <volume>15</volume>, <fpage>1090</fpage>–<lpage>1097</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Vos</surname>, <given-names>W. H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Increased plasticity of the nuclear envelope and hypermobility of telomeres due to the loss of A–type lamins</article-title>. <source>Biochimica et Biophysica Acta (BBA) - General Subjects</source> <volume>1800</volume>, <fpage>448</fpage>–<lpage>458</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heckenbach</surname>, <given-names>I.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Nuclear morphology is a deep learning biomarker of cellular senescence</article-title>. <source>Nat Aging</source> <volume>2</volume>, <fpage>742</fpage>–<lpage>755</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eastwood</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>MesoGraph: Automatic profiling of mesothelioma subtypes from histological images</article-title>. <source>Cell Reports Medicine</source> <volume>4</volume>, <fpage>101226</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mikolajczyk</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Grochowski</surname>, <given-names>M</given-names></string-name></person-group>. <chapter-title>Data augmentation for improving deep learning in image classification problem</chapter-title>. in <source>2018 International Interdisciplinary PhD Workshop (IIPhDW) 117–122</source> (<publisher-name>IEEE</publisher-name>, <publisher-loc>Swinoujście</publisher-loc>, <year>2018</year>). doi:<pub-id pub-id-type="doi">10.1109/IIPHDW.2018.8388338</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Nguyen</surname>, <given-names>L. D.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>Z.</given-names></string-name> &amp; <string-name><surname>Cao</surname>, <given-names>J</given-names></string-name></person-group>. <chapter-title>Deep CNNs for microscopic image classification by exploiting transfer learning and feature concatenation</chapter-title>. in <source>2018 IEEE International Symposium on Circuits and Systems (ISCAS) 1–5</source> (<publisher-name>IEEE</publisher-name>, <publisher-loc>Florence</publisher-loc>, <year>2018</year>). doi:<pub-id pub-id-type="doi">10.1109/ISCAS.2018.8351550</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tomov</surname>, <given-names>M. L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Resolving cell state in iPSC-derived human neural samples with multiplexed fluorescence imaging</article-title>. <source>Commun Biol</source> <volume>4</volume>, <fpage>786</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Strano</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Tuck</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Stubbs</surname>, <given-names>V. E.</given-names></string-name> &amp; <string-name><surname>Livesey</surname>, <given-names>F. J</given-names></string-name></person-group>. <article-title>Variable Outcomes in Neural Differentiation of Human PSCs Arise from Intrinsic Differences in Developmental Signaling Pathways</article-title>. <source>Cell Reports</source> <volume>31</volume>, <fpage>107732</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galiakberova</surname>, <given-names>A. A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Different iPSC-derived neural stem cells shows various spectrums of spontaneous differentiation during long term cultivation</article-title>. <source>Front. Mol. Neurosci</source>. <volume>16</volume>, <fpage>1037902</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Du</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Xu</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Hierarchical deep neural network for multivariate regression</article-title>. <source>Pattern Recognition</source> <volume>63</volume>, <fpage>149</fpage>–<lpage>157</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shakarchy</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Machine learning inference of continuous single-cell state transitions during myoblast differentiation and fusion</article-title>. <source>Mol Syst Biol</source> (<year>2024</year>) doi:<pub-id pub-id-type="doi">10.1038/s44320-024-00010-3</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mai</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Morphology-based deep learning approach for predicting adipogenic and osteogenic differentiation of human mesenchymal stem cells (hMSCs)</article-title>. <source>Front. Cell Dev. Biol</source>. <volume>11</volume>, <fpage>1329840</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gosselin</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>An environment-dependent transcriptional network specifies human microglia identity</article-title>. <source>Science</source> <volume>356</volume>, <fpage>eaal3222</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engle</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Blaha</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Kleiman</surname>, <given-names>R. J</given-names></string-name></person-group>. <article-title>Best Practices for Translational Disease Modeling Using Human iPSC-Derived Neurons</article-title>. <source>Neuron</source> <volume>100</volume>, <fpage>783</fpage>–<lpage>797</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chambers</surname>, <given-names>S. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Highly efficient neural conversion of human ES and iPS cells by dual inhibition of SMAD signaling</article-title>. <source>Nat Biotechnol</source> <volume>27</volume>, <fpage>275</fpage>–<lpage>280</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rueden</surname>, <given-names>C. T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>ImageJ2: ImageJ for the next generation of scientific image data</article-title>. <source>BMC Bioinformatics</source> <volume>18</volume>, <fpage>529</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schindelin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rueden</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Hiner</surname>, <given-names>M. C.</given-names></string-name> &amp; <string-name><surname>Eliceiri</surname>, <given-names>K. W</given-names></string-name></person-group>. <article-title>The ImageJ ecosystem: An open platform for biomedical image analysis</article-title>. <source>Molecular Reproduction Devel</source> <volume>82</volume>, <fpage>518</fpage>–<lpage>529</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Van Rossum</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Drake</surname>, <given-names>F. L.</given-names></string-name></person-group> <source>Python 3 Reference Manual</source>. (<year>2009</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>Anaconda Software Distribution</collab></person-group>. <source>Anaconda Documentation</source>. <publisher-name>Anaconda Inc</publisher-name>. <year>no date</year></mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Michaelos</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Pachitariu</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title>. <source>Nat Methods</source> <volume>18</volume>, <fpage>100</fpage>–<lpage>106</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Weigert</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Haase</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sugawara</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Myers</surname>, <given-names>G</given-names></string-name></person-group>. <chapter-title>Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy</chapter-title>. in <source>2020 IEEE Winter Conference on Applications of Computer Vision (WACV) 3655–3662</source> (<publisher-name>IEEE</publisher-name>, <publisher-loc>Snowmass Village, CO, USA</publisher-loc>, <year>2020</year>). doi:<pub-id pub-id-type="doi">10.1109/WACV45572.2020.9093435</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95273.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zaritsky</surname>
<given-names>Assaf</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Ben-Gurion University of the Negev</institution>
</institution-wrap>
<city>Beer Sheva</city>
<country>Israel</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents an <bold>important</bold> application of high-content image-based morphological profiling to quantitatively and systematically characterize induced pluripotent stem cell-derived mixed neural cultures cell type compositions. <bold>Compelling</bold> evidence through rigorous experimental and computational validations support new potential applications of this cheap and simple assay.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95273.2.sa1</article-id>
<title-group>
<article-title>Joint Public Review:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors present a new application of the high-content image-based morphological profiling Cell Painting (CP) to single cell type classification in mixed heterogeneous induced pluripotent stem cell-derived mixed neural cultures. Machine learning models were trained to classify single cell types according to either &quot;engineered&quot; features derived from the image or from the raw CP multiplexed image. The authors systematically evaluated experimental (e.g., cell density, cell types, fluorescent channels) and computational (e.g., different models, different cell regions) parameters and convincingly demonstrated that focusing on the nucleus and its surroundings contain sufficient information for robust and accurate cell type classification. Models that were trained on mono-cultures (i.e., containing a single cell type) could generalize for cell type prediction in mixed co-cultures, and to describe intermediate states of the maturation process of iPSC-derived neural progenitors to differentiation neurons.</p>
<p>Strengths:</p>
<p>Automatically identifying single cell types in heterogeneous mixed cell populations hold great promise to characterize mixed cell populations and to discover new rules of spatial organization and cell-cell communication. Although the current manuscript focuses on the application of quality control of iPSC cultures, the same approach can be extended to a wealth of other applications including in depth study of the spatial context. The simple and high-content assay democratizes use and enables adoption by other labs.</p>
<p>The manuscript is supported by comprehensive experimental and computational validations that raises the bar beyond the current state of the art in the field of high-content phenotyping and makes this manuscript especially compelling. These include (i) Explicitly assessing replication biases (batch effects); (ii) Direct comparison of feature-based (a la cell profiling) versus deep-learning-based classification (which is not trivial/obvious for the application of cell profiling); (iii) Systematic assessment of the contribution of each fluorescent channel; (iv) Evaluation of cell-density dependency; (v) explicit examination of mistakes in classification; (vi) Evaluating the performance of different spatial contexts around the cell/nucleus; (vii) generalization of models trained on cultures containing a single cell type (mono-cultures) to mixed co-cultures; (viii) application to multiple classification tasks.</p>
<p>Comments on latest version:</p>
<p>I have consulted with Reviewer #3 and both of us were impressed by revised manuscript, especially by the clear and convincing evidence regarding the nucleocentric model use of the nuclear periphery and its benefit for the case of dense cultures. However, there are two issues that are incompletely addressed (see below). Until these are resolved, the &quot;strength of evidence&quot; was elevated to &quot;compelling&quot;.</p>
<p>First, the analysis of the patch size is not clearly indicating that the 12-18um range is a critical factor (Fig. 4E). On the contrary, the performance seems to be not very sensitive to the patch size, which is actually a desired property for a method. Still, Fig. 4B convincingly shows that the nucleocentric model is not sensitive to the culture density, while the other models are. Thus, the authors can adjust their text saying that the nucleocentric approach is not sensitive to the patch size and that the patch size is selected to capture the nucleus and some margins around it, making it less prone to segmentation errors in dense cultures.</p>
<p>Second, the GitHub does not contain sufficient information to reproduce the analysis. Its current state is sparse with documentation that would make reproducing the work difficult. What versions of the software were used? Where should data be downloaded? The README contains references to many different argparse CLI arguments, but sparse details on what these arguments actually are, and which parameters the authors used to perform their analyses. Links to images are broken. Ideally, all of these details would be present, and the authors would include a step-by-step tutorial on how to reproduce their work. Fixing this will lead to an &quot;exceptional&quot; strength of evidence.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95273.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>De Beuckeleer</surname>
<given-names>Sarah</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8964-4480</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Van De Looverbosch</surname>
<given-names>Tim</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Van Den Daele</surname>
<given-names>Johanna</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ponsaerts</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>De Vos</surname>
<given-names>Winnok H</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0960-6781</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the previous reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>The authors present a new application of the high-content image-based morphological profiling Cell Painting (CP) to single cell type classification in mixed heterogeneous induced pluripotent stem cellderived mixed neural cultures. Machine learning models were trained to classify single cell types according to either &quot;engineered&quot; features derived from the image or from the raw CP multiplexed image. The authors systematically evaluated experimental (e.g., cell density, cell types, fluorescent channels) and computational (e.g., different models, different cell regions) parameters and convincingly demonstrated that focusing on the nucleus and its surroundings contains sufficient information for robust and accurate cell type classification. Models that were trained on mono-cultures (i.e., containing a single cell type) could generalize for cell type prediction in mixed co-cultures, and describe intermediate states of the maturation process of iPSC-derived neural progenitors to differentiation neurons.</p>
<p>Strengths:</p>
<p>Automatically identifying single-cell types in heterogeneous mixed-cell populations holds great promise to characterize mixed-cell populations and to discover new rules of spatial organization and cell-cell communication. Although the current manuscript focuses on the application of quality control of iPSC cultures, the same approach can be extended to a wealth of other applications including an in-depth study of the spatial context. The simple and high-content assay democratizes use and enables adoption by other labs.</p>
<p>The manuscript is supported by comprehensive experimental and computational validations that raise the bar beyond the current state of the art in the field of high-content phenotyping and make this manuscript especially compelling. These include (i) Explicitly assessing replication biases (batch effects); (ii) Direct comparison of feature-based (a la cell profiling) versus deep-learning-based classification (which is not trivial/obvious for the application of cell profiling); (iii) Systematic assessment of the contribution of each fluorescent channel; (iv) Evaluation of cell-density dependency; (v) Explicit examination of mistakes in classification; (vi) Evaluating the performance of different spatial contexts around the cell/nucleus; (vii) Generalization of models trained on cultures containing a single cell type (mono-cultures) to mixed co-cultures; (viii) Application to multiple classification tasks.</p>
<p>I especially liked the generalization of classification from mono- to co-cultures (Figure 4C), and quantitatively following the gradual transition from NPC to Neurons (Figure 5H).</p>
<p>The manuscript is well-written and easy tofollow.</p>
</disp-quote>
<p>Thank you for the positive appreciation of our work and constructive comments.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>I am not certain how useful/important the specific application demonstrated in this study is (quality control of iPSC cultures), this could be better explained in the manuscript.</p>
</disp-quote>
<p>To clarify the importance we have added an additional explanation to the introduction (page 3) and also come back to it in the discussion (page 17).</p>
<p>Text from the introduction:</p>
<p>“However, genetic drift, clonal and patient heterogeneity cause variability in reprogramming and differentiation efficiency10,11. The differentiation outcome is further strongly influenced by variations in protocol12. This can significantly impact experimental outcomes, leading to inconsistent and potentially misleading results and consequently, it hinders the use of iPSC-derived cell systems in systematic drug screening or cell therapy pipelines. This is particularly true for iPSC-derived neural cultures, as their composition, purity and maturity directly affect gene expression and functional activity, which is essential for modelling neurological conditions13,14. Thus, from a preclinical perspective, there is the need for a fast and cost-effective QC approach to increase experimental reproducibility and cell type specificity15. From a clinical perspective in turn, robust QC is required for safety and regulatory compliance (e.g., for cell therapeutic solutions). This need for improved standardization and QC is underscored by large-scale collaborative efforts such as the International Stem Cell Banking Initiative16, which focusses on clinical quality attributes and provides recommendations for iPSC validation testing for use as cellular therapeutics, or the CorEuStem network, aiming to harmonize iPSC practices across core facilities in Europe.”</p>
<p>Text from the discussion:</p>
<p>“Many groups highlight the difficulty of reproducible neural differentiation and attribute this to culture conditions, cultivation time and variation in developmental signalling pathways in the source iPSC material43,44. Spontaneous neural differentiation has previously been shown to require approximately 80 days before mature neurons arise that can fire action potentials and show neural circuit formation. Although these differentiation processes display a stereotypical temporal sequence34, the exact timing and duration might vary. This variation negatively affects the statistical power when testing drug interventions and thus prohibits the application of iPSC-culture derivatives in routine drug screening. Current solutions (e.g., immunocytochemistry, flow cytometry, …) are often cost-ineffective, tedious, and incompatible with longitudinal/multimodal interrogation. CP is a much more cost-effective solution and ideally suited for this purpose. Routine CP-based could add confidence to and save costs for the drug discovery pipeline. We have shown that CP can be leveraged to capture the morphological changes associated with neural differentiation.”</p>
<disp-quote content-type="editor-comment">
<p>Another issue that I feel should be discussed more explicitly is how far can this application go - how sensitively can the combination of cell painting and machine learning discriminate between cell types that are more subtly morphologically different from one another?</p>
</disp-quote>
<p>Thank you for this interesting question. The fact that an approach based on a subregion not encompassing the whole cell (the “nucleocentric” approach) can predict cell types equally well, suggests that the cell shape as such is not the defining factor for accurate cell type profiling. And, while clearly neural progenitors, neurons or glia have vastly different cell shapes. We have shown that cells with closer phenotypes such as 1321N1 vs. SH-SY5Y or astrocytes vs. microglia can be distinguished with equal performance. However, triggered by the reviewers’ question, we have now tested additional conditions with more subtle phenotypes, including the classification of 1321N1 vs. two related retinal pigment epithelial cells with much more similar morphology (ARPE and RPE1 cells). We found that the CNN could discriminate these cells equally well and have added the results on page 8 and in Fig. 3D. To address this question from a different angle, we have also performed an experiment in which we changed cell states to assess whether discriminatory power remains high. Concretely, we exposed co-cultures of neurons and microglia to LPS to trigger microglial activation (more subtly visible as cytoskeletal changes and vacuole formation). This revealed that our approach still discriminates both cell types (neurons vs. microglia) with high accuracy, regardless of the microglial state. Furthermore, using a two-step approach, we could also distinguish LPS-treated (assumed to be activated) from unchallenged microglia (assumed to be more homeostatic), albeit with a lower accuracy. This experiment has been added as an extra results section (Cell type identification can be applied to mixed iPSC-derived neuronal cultures regardless of activation state, p12) and Fig. 7c. Finally, we have also added our take on what the possibilities could be for future applications in even more complex contexts such as tissue slice, 3D and live cell applications (page 17-18).</p>
<disp-quote content-type="editor-comment">
<p>Regarding evaluations, the use of accuracy, which is a measure that can be biased by class imbalance, is not the most appropriate measurement in my opinion. The confusion matrices are a great help, but I would recommend using a measurement that is less sensitive for class imbalance for cell-type classification performance evaluations.</p>
</disp-quote>
<p>Across all CNNs trained in this manuscript, the sample size of the input classes has always been equalized, ruling out any effects of class imbalance. Nevertheless, to follow the reviewers’ recommendation, we have now used the F-score to document performance as it is insensitive to such imbalance. For clarity, we have now also mentioned the input number (ROIs/class) in every figure.</p>
<disp-quote content-type="editor-comment">
<p>Another issue is that the performance evaluation is calculated on a subset of the full cell population - after exclusion/filtering. Could there be a bias toward specific cell types in the exclusion criteria? How would it affect our ability to measure the cell type composition of the population?</p>
</disp-quote>
<p>As explained in the M&amp;M section, filtering was performed based on three criteria:</p>
<p>(1) Nuclear size: values below a threshold of 160, objects are considered to represent debris;</p>
<p>(2) DAPI intensity: values below a threshold of 500 represent segmentation errors;</p>
<p>(3) IF staining intensity: gates were set onto the intensity of the fluorescent markers used with posthoc IF to only retain cells that are unequivocally positive for either marker and to avoid inclusion of double positive (or negative) cells in the ground truth training.</p>
<p>One could argue that the last criterion introduces a certain bias in that it does not consider part of the cell population. However, this is also not the purpose of our pioneering study that aims at identifying unique cell types for which ground truth is as pure and reliable as possible. Not filtering out these cells with a ‘dubious’ IF profile (<italic>e.g.,</italic> cells that might be transitioning or are of a different type) would negatively affect the model by introducing noise. It is correct that the predictions are based only on these inputs and so cells of a subsequent test set will only be classified according to these labels. For example, in the neuronal differentiation experiment (Fig. 6G-H), cells are either characterized as NPC or as neurons, which leaves the transitioning (or undefined) cells in either category. Despite this simplification, the model adequately predicted the increase in neuron/NPC ratio with culture age. In future iterations, one could envision defining more refined cell (sub-)types in a population based on richer post-hoc information (e.g., through cyclic immunofluorescence or spatial single cell transcriptomics) or longitudinal follow-up of cell-state transitions using live imaging. This notion has been added to page 17 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>I am not entirely convinced by the arguments regarding the superiority of the nucleocentric vs. the nuclear representations. Could it be that this improvement is due to not being sensitive/ influenced by nucleus segmentation errors?</p>
</disp-quote>
<p>The reviewer has a valid point that segmentation errors may occur. However, the algorithm we have used (Stardist classifier), is very robust to nuclear segmentation errors. To verify the performance, we have now quantified segmentation errors in 20 images for 3 different densities and found a consistently low error rate (0.6 -1.6%) without correlation to the culture density. Moreover, these errors include partial imperfections (<italic>e.g.,</italic> a missed protrusion or bleb) as well as over- (one nucleus detected as more) or under- (more nuclei detected as one) segmentations. The latter two will affect both the nuclear and nucleocentric predictions and should thus not affect the prediction performance. In the case of imperfect segmentations, there may be a specific impact on the nucleus-based predictions (which rely on blanking the non-nuclear part), but this alone cannot explain the significantly higher gain in accuracy for nucleocentric predictions (&gt;5%). Therefore, we conclude that segmentation errors may contribute in part, but not exclusively, to the overall improved performance of nucleocentric input models. We have added this notion in the discussion (pages 14-15 and Suppl. Fig. 1E).</p>
<disp-quote content-type="editor-comment">
<p>GRADCAM shows cherry-picked examples and is not very convincing.</p>
</disp-quote>
<p>To help convince the reviewer and illustrate the representativeness of selected images, we have now randomly selected for each condition and density 10 images (using random seeds to avoid cherrypicking) and added these in a Suppl. Fig. 3.</p>
<disp-quote content-type="editor-comment">
<p>There are many missing details in the figure panels, figure legend, and text that would help the reader to better appreciate some of the technical details, see details in the section on recommendations for the authors.</p>
</disp-quote>
<p>Please see further for our specific adaptations.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>This study uses an AI-based image analysis approach to classify different cell types in cultures of different densities. The authors could demonstrate the superiority of the CNN strategy used with nucleocentric cell profiling approach for a variety of cell types classification. The paper is very clear and well-written. I just have a couple of minor suggestions and clarifications needed for the reader.</p>
<p>The entire prediction model is based on image analysis. Could the authors discuss the minimal spatial resolution of images required to allow a good prediction? Along the same line, it would be interesting to the reader to know which metrics related to image quality (e.g. signal to noise ratio) allow a good accuracy of the prediction.</p>
</disp-quote>
<p>Thank you for the positive and relevant feedback.</p>
<p>The reviewer has a good point that it is important to portray the imaging conditions that are required for accurate predictions. To investigate this further we have performed additional experiments that give a better view on the operating window in terms of resolution and SNR (manuscript page 7-8 and new figure panels Fig. 3B-C). The initial image resolution was 0.325 µm/pixel. To understand the dependency on resolution we performed training and classifications for image data sets that were progressively binned. We found that a two-fold reduction in resolution did not significantly affect the F-score, but further degradation decreased the performance. At a resolution of 6,0 µm/pixel (20-fold binning), the F-score dropped to 0.79±0.02, comparable to the performance when only the DAPI (nuclear) channel was used as input. The effect of reduced image quality was assessed in a similar manner, by iteratively adding more Gaussian noise to the image. We found that above an SNR of 10 the prediction performance remains consistent but below it starts to degrade. While this exercise provides a first impression of the current confines of our method, we do believe it is plausible that its performance can be extended to even lower-quality images for example by using image restoration algorithms. We have added this notion in the discussion (page 14).</p>
<disp-quote content-type="editor-comment">
<p>The authors show that nucleocentric-based cell feature extraction is superior to feeding the CNN-based model for cell type prediction. Could they discuss what is the optimal size and shape of this ROI to ensure a good prediction? What if, for example, you increase or decrease the size of the ROI by a certain number of pixels?</p>
</disp-quote>
<p>To identify the optimal input, we varied the size of the square region around the nuclear centroid from 0.6 to 150 µm for the whole dataset. Within the nuclear-to-cell window (12µm- 30µm) the average Fscore is limited, but an important observation is the increasing error and differences in precision and recall with increasing nucleocentric patch sizes, which will become detrimental in cases of class imbalance. The F-score is maximal for a box of 12-18µm surrounding the nuclear centroid. In this “sweet spot”, the precision and recall are also in balance. Therefore, we have selected this region for the actual density comparison experiment. We have added our results to the manuscript (page 9 and 15).</p>
<disp-quote content-type="editor-comment">
<p>It would be interesting for the reader to know the number of ROI used to feed each model and know the minimal amount of data necessary to reach a high level of accuracy in the predictions.</p>
</disp-quote>
<p>The figures have now been adjusted so that the number of ROIs used as input to feed the model are listed. The minimal number of ROIs required to obtain high level accuracy is tested in Figure 2C. By systematically increasing the number of input ROIs for both RF and CNN, we found that a plateau is reached at 5000 input ROIs (per class) for optimal prediction performance. This is also documented in the results section page 6.</p>
<disp-quote content-type="editor-comment">
<p>From Figure 1 to Figure 4 the author shows that CNN based approach is efficient in distinguishing 1321N1 vs SH-SY5Y cell lines. The last two figures are dedicated to showing 2 different applications of the techniques: identification of different stages of neuronal differentiation (Figure 5) and different cell types (neurons, microglia, and astrocytes) in Figure 6. It would be interesting, for these 2 two cases as well, to assess the superiority of the CNN-based approach compared to the more classical Random Forest classification. This would reinforce the universal value of the method proposed.</p>
</disp-quote>
<p>To meet the reviewer’s request, we have now also compared CNN to RF for the classification of cells in iPSC-derived models (Figures 6 and 7). As expected, the CNN performed better in both cases. We have now added these results in Fig. 6 D and 7 C and pages 12 and 13 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Induced pluripotent stem cells, or iPSCs, are cells that scientists can push to become new, more mature cell types like neurons. iPSCs have a high potential to transform how scientists study disease by combining precision medicine gene editing with processes known as high-content imaging and drug screening. However, there are many challenges that must be overcome to realize this overall goal. The authors of this paper solve one of these challenges: predicting cell types that might result from potentially inefficient and unpredictable differentiation protocols. These predictions can then help optimize protocols.</p>
<p>The authors train advanced computational algorithms to predict single-cell types directly from microscopy images. The authors also test their approach in a variety of scenarios that one may encounter in the lab, including when cells divide quickly and crowd each other in a plate. Importantly, the authors suggest that providing their algorithms with just the right amount of information beyond the cells' nuclei is the best approach to overcome issues with cell crowding.</p>
<p>The work provides many well-controlled experiments to support the authors' conclusions. However, there are two primary concerns: (1) The model may be relying too heavily on the background and thus technical artifacts (instead of the cells) for making CNN-based predictions, and (2) the conclusion that their nucleocentric approach (including a small area beyond the nucleus) is not well supported, and may just be better by random chance. If the authors were to address these two concerns (through additional experimentation), then the work may influence how the field performs cell profiling in the future.</p>
</disp-quote>
<p>Thank you very much for confirming the potential value of our work and raising these relevant items. To better support our claims we have now performed additional validations, which we detail below.</p>
<disp-quote content-type="editor-comment">
<p>(1) The model may be relying too heavily on the background and thus technical artifacts (instead of the cells) for making CNN-based predictions</p>
</disp-quote>
<p>To address the first point, we have adapted the GradCAM images to show an overlay of the input crop and GradCAM heatmap to give a better view of the structures that are highlighted by the CNN. We further investigated the influence of the background on the prediction performance. Our finding that a CNN trained on a monoculture retains a relatively high performance on cocultures implies that the CNN uses the salient characteristics of a cell to recognize it in more complex heterogeneous environments. Assuming that the background can vary between experiments, the prediction of a pretrained CNN on a new dataset indicates that cellular characteristics are used for robust prediction.  When inspecting GradCAM images obtained from the nucleocentric CNN approaches (now added in Suppl. Fig. 3), we noticed that the nuclear periphery typically contributed the most (but not exclusively) to the prediction performance. When using only the nuclear region as input, GradCAMs were more strongly (but again not exclusively) directed to the background surrounding the nuclei. To train the latter CNN, we had cropped nuclei and set the background to a value of zero. To rule out that this could have introduced a bias, we have now performed the exact same training and classification, but setting the background to random noise instead (Suppl. Fig. 2). While this effectively diverted the attention of the GradCAM output to the nucleus instead of the background, the prediction performance was unaltered. We therefore assume that irrespective of the background, when using nuclear crops as input, the CNN is dominated by features that describe nuclear size. We observe that nuclear size is significantly different in both cell types (although intranuclear features also still contribute) which is also reflected in the feature map gradient in the first UMAP dimension (Suppl. Fig. 2). This notion has been added to the manuscript (page 9) and Suppl. Fig. 2.</p>
<disp-quote content-type="editor-comment">
<p>(2) The conclusion that their nucleocentric approach (including a small area beyond the nucleus) is not well supported, and may just be better by random chance.</p>
</disp-quote>
<p>To address this second concern, which was also raised by reviewer 2, we have performed a more extensive analysis in which the patch size was varied from 0.6 to 120µm around the nuclear centroid (Fig. 4E and page 9 of the manuscript). We observed that there is little effect of in- or decreasing patch size on the average F-score within the nuclear to cell window, but that the imbalance between the precision and recall increases towards the larger box sizes (&gt;18µm). Under our experimental conditions, the input numbers per class were equal, but this will not be the case in situations where the ground truth is unknown (and needs to be predicted by the CNN). Therefore, a well-balanced CNN is of high importance. This notion has been added to page 15 of the manuscript.</p>
<p>The main advantage of nucleocentric profiling over whole-cell profiling in dense cultures is that it relies on a more robust nuclear segmentation method and is less sensitive to differences in cell density (Suppl. Fig. 1D). In other words, in dense cultures, the segmentation mask will contain similar regional input as the nuclear mask and the nucleocentric crop will contain more perinuclear information which contributes to the prediction accuracy. Therefore, at high densities, the performance of the CNN on whole-cell crops decreases owing to poorer segmentation performance. A CNN that uses nucleocentric crops, will be less sensitive to these errors. This notion has been added to pages 14-15 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Additionally, the impact of this work will be limited, given the authors do not provide a specific link to the public source code that they used to process and analyze their data.</p>
</disp-quote>
<p>The source code is now available on the Github page of the DeVos lab, under the following URL: <ext-link ext-link-type="uri" xlink:href="https://github.com/DeVosLab/Nucleocentric-Profiling">https://github.com/DeVosLab/Nucleocentric-Profiling</ext-link></p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewing Editor (Recommendations For The Authors):</bold></p>
<p>Evaluation summary</p>
<p>The authors present a new application of the high-content image-based morphological profiling Cell Painting (CP) to single cell type classification in mixed heterogeneous induced pluripotent stem cellderived mixed neural cultures. Machine learning models were trained to classify single cell types according to either &quot;engineered&quot; features derived from the image or from the raw CP multiplexed image. The authors systematically evaluated experimental (e.g., cell density, cell types, fluorescent channels, replication biases) and computational (e.g., different models, different cell regions) parameters and argue that focusing on the nucleus and its surroundings contains sufficient information for robust and accurate cell type classification. Models that were trained on mono-cultures (i.e., containing a single cell type) could generalize for cell type prediction in mixed co-cultures, and describe intermediate states of the maturation process of iPSC-derived neural progenitors to differentiation neurons.</p>
<p>Strengths:</p>
<p>Automatically identifying single-cell types in heterogeneous mixed-cell populations is an important application and holds great promise. The simple and high-content assay democratizes use and enables adoption by other labs. The manuscript is supported by comprehensive experimental and computational validations. The manuscript is well-written and easy to follow.</p>
<p>Weaknesses:</p>
<p>The conclusion is that the nucleocentric approach (including a small area beyond the nucleus) is not well supported, and may just be better by random chance. If better supported by additional experiments, this may influence how the field performs cell profiling in the future. Model interpretability (GradCAM) analysis is not convincing. The lack of a public source code repository is also limiting the impact of this study. There are missing details in the figure panels, figure legend, and text that would help the reader to better appreciate some of the technical details.</p>
<p>Essential revisions:</p>
<p>To reach a &quot;compelling&quot; strength of evidence the authors are requested to either perform a comprehensive analysis of the effect of ROI size on performance, or tune down statements regarding the superior performance of their &quot;nucleocentric&quot; approach. Further addition of a public and reproducible source code GitHub repository will lead to an &quot;exceptional&quot; strength of evidence.</p>
</disp-quote>
<p>To answer the main comment, we have performed an experiment in which we varied the size of the nucleocentric patch and quantified CNN performance. We have also evaluated the operational window of our method by varying the resolution and SNR and we have experimented with different background blanking methods. We have expanded our examples of GradCAM images and now also made our source code and an example data set available via GitHub.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>I think that an evaluation of how the excluded cells affect our ability to measure the cell type composition of the population would be helpful to better understand the limitations and practical measurement noise introduced by this approach. A similar evaluation of the excluded cells can also help to better understand the benefit of nucleocentric vs. cell representations by more convincingly demonstrating the case for the nucleocentric approach. In any case, I recommend discussing in more depth the arguments for using the nucleocentric representation and why it is superior to the nuclear representation.</p>
</disp-quote>
<p>The benefits of nucleocentric representation over nuclear and whole-cell representation are discussed more in depth at pages 14-15 of the manuscript.</p>
<p>“The nucleocentric approach, which is based on more robust nuclear segmentation, minimizes such mistakes whilst still retaining input information from the structures directly surrounding the nucleus. At higher cell density, the whole-cell body segmentation becomes more error-prone, while also loosing morphological information (Suppl. Fig. 1D). The nucleocentric approach is more consistent as it relies on a more robust segmentation and does not blank the surrounding region. This way it also buffers for occasional nuclear segmentation errors (e.g., where blebs or parts of the nucleus are left undetected).”</p>
<disp-quote content-type="editor-comment">
<p>It is not entirely clear to me why Figure 5 moves back to &quot;engineered&quot; features after previous figures showed the superiority of the deep learning approach. Especially, where Figure 6 goes again to DL. Dimensionality reduction can be also applied to DL-based classifications (e.g., using the last layer).</p>
</disp-quote>
<p>Following up on the reviewers’ interesting comment, we extracted the embeddings from the trained CNN and performed UMAP dimensionality reduction. The results are shown in Fig. 3D, 6F and supplementary figure 1B and added to the manuscript on pages 6, 8 and 12.</p>
<p>We concluded that unsupervised dimensionality reduction using the feature embeddings could separate cell type clusters, where the distance between the clusters reflected the morphological similarity between the cell lines.</p>
<disp-quote content-type="editor-comment">
<p>I would recommend including more comprehensive GRADCAM panels in the SI to reduce the concern of cherry-picking examples. What is the interpretation of the nucleocentric area?</p>
</disp-quote>
<p>A more extensive set of GradCAM images have now been included in supplementary material (Supplementary figure 3) using the same random seeds for all conditions, thus avoiding any cherry picking. We interpret the GradCAM maps on the nucleocentric crops as highlighting the structures surrounding the nucleus (reflecting ER, mitochondria, Golgi) indicating their importance in correct cell classification. This was added to the manuscript on pages 9 and 15.</p>
<disp-quote content-type="editor-comment">
<p>Missing/lacking details and suggestions in the figure panels and figure legend:</p>
<list list-type="bullet">
<list-item><p>Scale bars missing in some of the images shown (e.g., Figure 2F, Figure 3D, Figure 4, Supplementary Figure 4), what are the &quot;composite&quot; channels (e.g., Figure 2F), missing x-label in Figure 3B.</p>
</list-item></list>
</disp-quote>
<p>These have now been added.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Terms that are not clear in the figure and not explained in the legend, such as FITC and cy3 energy (Figure 1C).</p>
</list-item></list>
</disp-quote>
<p>The figure has been adapted to better show the region, channel and feature. We have now added a Table (Table 5), detailing the definition of each morphological feature that is extracted. On page 27, information on feature extraction is noted.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Details that are missing or not sufficiently explained in the figure legends such as what each data point represents and what is Gini importance (Figure 1D)</p>
</list-item></list>
</disp-quote>
<p>We have added these explanations to the figure legends. The Gini importance or mean decrease in impurity reflects how often this feature is used in decision tree splits across all random forest trees.</p>
<disp-quote content-type="editor-comment">
<p>Is it the std shown in Figure 2C?</p>
</disp-quote>
<p>Yes, this has now been added to the legend.</p>
<disp-quote content-type="editor-comment">
<p>It is not fully clear what is single/mixed (Figure 2D)</p>
</disp-quote>
<p>Clarification is added to the legend and in the manuscript on page 6.</p>
<disp-quote content-type="editor-comment">
<p>explain what is DIV 13-90 in the legend (Figure 5).</p>
</disp-quote>
<p>DIV stands for days in vitro, here it refers to the days in culture since the start of the neural induction process. This has been added in the legend.</p>
<disp-quote content-type="editor-comment">
<p>and state what are img1-5 (Supplementary Figures 1B-C) Clarification has been added to the legend.</p>
<list list-type="bullet">
<list-item><p>Supplementary Figure 1. What is the y-axis in panel C and how do the results align with the cell mask in panel B?</p>
</list-item></list>
</disp-quote>
<p>The y-axis represents the intersection over union (IoU). The IoU quantifies the overlap between ground truth (manually segmented ROI) and the ROI detected by the segmentation algorithm. It is defined as the area of the overlapping region over the total area. This clarification has been added to the legend.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Supplementary Figure 1 and Methods. Please explain when CellPose and when StarDist were applied.</p>
</list-item></list>
</disp-quote>
<p>Added to supplementary figure and methods at page 24. In the case of nuclear segmentation (nucleus and nucleocentric crops), Stardist was used. For whole-cell crops, cell segmentation using Cellpose was used.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Supplementary Figure 4C - the color code is different between nuclear and nucleocentric - this is confusing.</p>
</list-item></list>
</disp-quote>
<p>We have changed to color code to correspond in both conditions in Fig. 1A.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Figure 3B - better to have a normalized measure in the x-axis (number of cells per area in um^2)</p>
</list-item></list>
</disp-quote>
<p>We agree and have changed this.</p>
<p>Suggestions and missing/lacking details in the text:</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #38: &quot;we then applied this&quot; because it is the first time that this term is presented.</p>
</list-item></list>
</disp-quote>
<p>This has been rephrased.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #88: a few words on what were the features extracted would be helpful.</p>
</list-item></list>
</disp-quote>
<p>Short description added to page 26-27 and detailed definition of all features added in table 5.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #91: PCA analysis - the authors can highlight what (known) features were important to PC1 using the linear transformation that defined it.</p>
</list-item></list>
</disp-quote>
<p>The 5 most important features of PC1 were (in order of decreasing importance): channel 1 dissimilarity, channel 1 homogeneity, nuclear perimeter, channel 4 dissimilarity and nuclear area.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #92: Order of referencing Supplementary Figure 4 before referencing Supplementary Figure 13.</p>
</list-item></list>
</disp-quote>
<p>The order of the Supplementary images was changed to follow the chronology.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #96: Can the authors show the data supporting this claim?</p>
</list-item></list>
</disp-quote>
<p>The unsupervised UMAP shown in fig. 1B is either color coded by cell type (left) or replicate (right). Based on this feature map, we observe clustering along the UMAP1 axis to be associated with the cell type. Variations in cellular morphology associated with the biological replicate are more visible along the UMAP2 axis. When looking at fig. 1C, the feature map reflecting the cellular area shows a gradient along the UMAP1 direction, supporting the assumption that cell area contributes to the cell type separation. On the other hand, the average intensity (Channel 2 intensity) has a gradient within the feature map along the UMAP2 direction. This corresponds to the pattern associated with the inter-replicate variability in panel B.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #108: what is &quot;nuclear Cy3 energy&quot;?</p>
</list-item></list>
</disp-quote>
<p>This represents the local change of pixel intensities within the ROI in the nucleus in the 3rd channel dimension. This parameter reflects the texture within the nuclear region for the phalloidin and WGA staining. The definitions of all handcrafted features are added in table 5 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #110-112: Can the authors show the data supporting this claim?</p>
</list-item></list>
</disp-quote>
<p>The figure has been changed to include the results from a filtered and unfiltered dataframe (exclusion and inclusion of redundant features). Features could be filtered out if the correlation was above a threshold of 0.95. This has been added to page 6 of the manuscript and fig. 1D.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #115-116: please state the size of the mask.</p>
</list-item></list>
</disp-quote>
<p>Added to the text (page 6). We used isotropic image crops of 60µm centred on individual cell centroids.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Lines 120-122: more details will make this more clear (single vs. mixed).</p>
</list-item></list>
</disp-quote>
<p>This has been changed on page 6 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #142: &quot;(mimics)&quot; - is it a typo?</p>
</list-item></list>
</disp-quote>
<p>Tissue mimics refers to organoids/models that are meant to replicate the physiological behaviour.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #159: the bounding box for nucleocentric analysis is 15x15um (and not 60), as stated in the Methods.</p>
</list-item></list>
</disp-quote>
<p>Thank you for pointing out this mistake. We have adapted this.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #165: what is the interpretation of what was important for the nucleocentric classification?</p>
</list-item></list>
</disp-quote>
<p>The colour code in GradCAM images is indicative of the attention of the CNN (the more to the red, the more attention). In fig. 4D and Suppl. Fig. 3 the structures directly surrounding the nucleus receive high attention from the CNN trained on nucleocentric crops. This has been added to the manuscript page 9 and 15.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Section starting in line #172: not explicitly stated what model was used (nucleocentric?).</p>
</list-item></list>
</disp-quote>
<p>Added in the legend of fig. 5. For these experiments, the full cell segmentation was still used.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Section starting in line #199: why use a feature-based model rather than nucleocentric? A short sentence would be helpful.</p>
</list-item></list>
</disp-quote>
<p>For CNN training, nucleocentric profiling was used. In response to a legitimate question of one of the reviewers, the feature-based UMAP analysis was replaced with the feature embeddings from the CNN.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #213: Fig. 5B does not show transitioning cells.</p>
</list-item></list>
</disp-quote>
<p>Thank you for pointing this out, this was a mistake and has been changed.</p>
<disp-quote content-type="editor-comment">
<p>Lines #218-220: not fully clear to some readers (culture condition as a weak label), more details can be helpful.</p>
</disp-quote>
<p>We changed this at page 11 of the manuscript for clarity.</p>
<p>“This gating strategy resulted in a fractional abundance of neurons vs. total (neurons + NPC) of 36,4 % in the primed condition and 80,0% in the differentiated condition (Fig. 6C). We therefore refer to the culture condition as a weak label as it does not take into account the heterogeneity within each condition (well).”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #230: &quot;increasing dendritic outgrowth&quot; - what does it mean? Can you explicitly highlight this phenotype in Figure 5G?</p>
</list-item></list>
</disp-quote>
<p>When the cells become more mature during differentiation, the cell body becomes smaller and the neurons form long, thin ramifications. This explanation has been added to page 12 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #243: is it the nucleocentric CNN?</p>
</list-item></list>
</disp-quote>
<p>Yes.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Lines #304-313, the authors might want to discuss other papers dealing with continuous (non-neural) differentiation state transitions (eg PMID: 38238594).</p>
</list-item></list>
</disp-quote>
<p>A discussion of the use of morphological profiling for longitudinal follow-up of continuous differentiation states has been added to the manuscript at page 18.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #444: cellpose or stardist? How did the authors use both?</p>
</list-item></list>
</disp-quote>
<p>Clarification has been added to supplementary figure 1 and methods at page 24. Stardist was used for nuclear segmentation, whereas Cellpose was used for whole-cell segmentation.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Line #470-474: I would appreciate seeing the performance on the full dataset without exclusions.</p>
</list-item></list>
</disp-quote>
<p>Cells have been excluded based on 3 arguments: the absence of DAPI intensity, too small nuclear size and absence of ground truth staining. The first two arguments are based on the assumption that ROIs that contain no DAPI signal or are too small are errors in cell segmentation and therefore should not be taken along in the analysis. The third filtering step was based on the ground-truth IF signal. Not filtering out these cells with a ‘dubious’ IF profile (<italic>e.g.,</italic> cells that might be transitioning or are of a different type) would negatively affect the model by introducing noise. It is correct that the predictions are based only on these inputs and so cells of a subsequent test set will only be classified according to these labels which might introduce bias. However, the model could predict increase in neuron/NPC ratio with culture age in absence of ground-truth staining (and thus IF-based filtering).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>Figure 1A: it would be interesting to the reader to see the SH-SY5Y data as well.</p>
</disp-quote>
<p>This has been added in fig. 1A.</p>
<disp-quote content-type="editor-comment">
<p>Figure 3A: 95-100% image: showing images with the same magnification as the others would help to appreciate the cell density.</p>
</disp-quote>
<p>Now fig. 4A. The figure has been changed to make sure all images have the same magnification.</p>
<disp-quote content-type="editor-comment">
<p>Figure Supp 4 (line 132) is referred to before Figure Supp1 (line 152).</p>
</disp-quote>
<p>The image order and numbering has been changed to solve this issue.</p>
<disp-quote content-type="editor-comment">
<p>Figure Supp 2 &amp; 3 are not referred to in the text.</p>
</disp-quote>
<p>This has been adjusted.</p>
<disp-quote content-type="editor-comment">
<p>Line 225: a statistical test would help to convince of the accuracy of these results (Figure 5C vs Figure 5F)?</p>
</disp-quote>
<p>These figures represent the total ROI counts and thus represent a single number.</p>
<disp-quote content-type="editor-comment">
<p>Line 227: Could you explain to the reader, in a few words, what a dual SMAD inhibition is?</p>
</disp-quote>
<p>This has been added to the manuscript at page 20.</p>
<p>“This dual blockade of SMAD signalling in iPSCs is induces neural differentiation by synergistically causing the loss of pluripotency and push towards neuroectodermal lineage.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>I have a few concerns and several comments that, if addressed, may strengthen conclusions, and increase clarity of an already technically sound paper.</p>
<p>Concerns</p>
<list list-type="bullet">
<list-item><p>The results presented in Figure 3 panel D, may indicate a critical error in data processing and interpretation that the authors must address. The GradCAM method highlights the background as having the highest importance. While it can be argued in the nucleocentric profiling method that GradCAM focuses on the nuclear membrane, the background is highly important even for the nuclear profiling method, which should provide little information. What procedure did the authors use for mask subtraction prior to CNN training? Could the segmentation algorithm be performing differently between cell lines? The authors interpret the GradCAM results to indicate a proxy for nuclear size, but then why did the CNN perform so much better than random forest using hand-crafted features that include this variable? The authors should also present size distributions between cell lines (and across seeding densities, in case one of the cell lines has different compaction properties with increasing density).</p>
</list-item></list>
<p>Perhaps clarifying this sentence (lines 166-168) would help as well: &quot;As nuclear area dropped with culture density, the dynamic range decreased, which could explain the increased error rate of the CNN for high densities unrelated to segmentation errors (Suppl. Fig. 4B).&quot; What do the authors mean by &quot;dynamic range&quot; and it is not clear how Supplementary Figure 4B provides evidence for this?</p>
<p>The dynamic range refers to the difference between the minimum and maximum nuclear area. We expect the difference to decrease at highe rdensity owing to the crowding that forces all nuclei to take on a more similar (smaller) size.</p>
</disp-quote>
<p>More clarification on this has been added to page 9 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>I certainly understand that extrapolating the GradCAM concern to the remaining single-cell images using only four (out of tens of thousands of options) is also dangerous, but so is &quot;cherry-picking&quot; these cells to visualize. Finally, I also recommend that the authors quantitatively diagnose the extent of the background influence according to GradCAM by systematically measuring background influence in all cells and displaying the results per cell line per density.</p>
</disp-quote>
<p>To avoid cherry picking of GradCAM images, we have now randomly selected for each condition and density 10 images (using random seeds to avoid cherry-picking) and added these in a Suppl. Fig. 3.</p>
<p>In answer to this concern, we refer to the response above:</p>
<p>“To address the first point, we have adapted the GradCAM images to show an overlay of the input crop and GradCAM heatmap to give a better view of the structures that are highlighted by the CNN. We further investigated the influence of the background on the prediction performance. Our finding that a CNN trained on a monoculture retains a relatively high performance on cocultures implies that the CNN uses the salient characteristics of a cell to recognize it in more complex heterogeneous environments. Assuming that the background can vary between experiments, the prediction of a pretrained CNN on a new dataset indicates that cellular characteristics are used for robust prediction.  When inspecting GradCAM images obtained from the nucleocentric CNN approaches (now added in Suppl. Fig. 3), we noticed that the nuclear periphery typically contributed the most (but not exclusively) to the prediction performance. When using only the nuclear region as input, GradCAMs were more strongly (but again not exclusively) directed to the background surrounding the nuclei. To train the latter CNN, we had cropped nuclei and set the background to a value of zero. To rule out that this could have introduced a bias, we have now performed the exact same training and classification, but setting the background to random noise instead (Suppl. Fig. 2). While this effectively diverted the attention of the GradCAM output to the nucleus instead of the background, the prediction performance was unaltered. We therefore assume that irrespective of the background, when using nuclear crops as input, the CNN is dominated by features that describe nuclear size. We observe that nuclear size is significantly different in both cell types (although intranuclear features also still contribute) which is also reflected in the feature map gradient in the first UMAP dimension (Suppl. Fig. 2). This notion has been added to the manuscript (page 9) and Suppl. Fig. 2.”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The data supporting the conclusion about nucleocentric profiling outperforming nuclear and full-cell profiling is minimal. I am picking on this conclusion in particular, because I think it is a super cool and elegant result that may change how folks approach issues stemming from cell density disproportionately impacting profiling. Figures 3B and 3C show nucleocentric slightly outperforming full cell, and the result is not significant. The authors state in lines 168-170: &quot;Thus, we conclude that using the nucleocentric region as input for the CNN is a valuable strategy for accurate cell phenotype identification in dense cultures.&quot; This is somewhat of a weak conclusion, that, with additional analysis, could be strengthened and add high value to the community. Additionally, the authors describe the nucleocentric approach insufficiently. In the methods, the authors state (lines 501-503): &quot;Cell crops (60μm whole cell - 15μm nucleocentric/nuclear area) were defined based on the segmentation mask for each ROI.&quot; This is not sufficient to reproduce the method. What software did the authors use?</p>
</list-item></list>
</disp-quote>
<p>Presumably, 60μm refers to a box size around cytoplasm? Much more detail is needed. Additionally, I suggest an analysis to confirm the impact of nucleocentric profiling, which would strengthen the authors' conclusions. I recommend systematically varying the subtraction (-30μm, -20μm, -10μm, 5μm, 0, +5μm, +10μm, etc.) and reporting the density-based analysis in Figure 3B per subtraction. I would expect to see some nucleocentric &quot;sweet spot&quot; where performance spikes, especially in high culture density. If we don't see this difference, then the non-significant result presented in Figures 3B and C is likely due to random chance. The authors mention &quot;iterative data erosion&quot; in the abstract, which might refer to what I am recommending, but do not describe this later.</p>
<p>More detail was added to the methods describing the image crops given as input to the CNN (page 28 of the manuscript).</p>
<p>“Crops were defined based on the segmentation mask for each ROI. The bounding box was cropped out of the original image with a fixed patch size (60µm for whole cells, 18µm for nucleus and nucleocentric crops) surrounding the centroid of the segmentation mask. For the whole cell and nuclear crops, all pixels outside of the segmentation mask were set to zero. This was not the case for the nucleocentric crops. Each ROI was cropped out of the original morphological image and associated with metadata corresponding to its ground truth label.”</p>
<p>To address this concern, we also refer to the answer above.</p>
<p>“We have performed a more extensive analysis in which the patch size was varied from 0.6 to 120µm around the nuclear centroid (Fig. 4E and page 9 of the manuscript). We observed that there is little effect of in- or decreasing patch size on the average F-score within the nuclear to cell window, but that the imbalance between the precision and recall increases towards the larger box sizes (&gt;18µm). Under our experimental conditions, the input numbers per class were equal, but this will not be the case in situations where the ground truth is unknown (and needs to be predicted by the CNN). Therefore, a well-balanced CNN is of high importance. This notion has been added to page 12 of the manuscript.</p>
<p>The main advantage of nucleocentric profiling over whole-cell profiling in dense cultures is that it relies on a more robust nuclear segmentation method and is less sensitive to differences in cell density (Suppl. Fig. 1D). In other words, in dense cultures, the segmentation mask will contain similar regional input as the nuclear mask and the nucleocentric crop will contain more perinuclear information which contributes to the prediction accuracy. Therefore, at high densities, the performance of the CNN on whole-cell crops decreases owing to poorer segmentation performance. A CNN that uses nucleocentric crops, will be less sensitive to these errors. This notion has been added to pages 14-15 of the manuscript.“</p>
<disp-quote content-type="editor-comment">
<p>Comments</p>
<list list-type="bullet">
<list-item><p>There is a disconnect between the abstract and the introduction. The abstract highlights the nucleocentric model, but then it is not discussed in the introduction, which focuses on quality control. The introduction would benefit from some additional description of the single-cell or whole-image approach to profiling.</p>
</list-item></list>
</disp-quote>
<p>We highlight the importance of QC of complex iPSC-derived neural cultures as an application of morphological profiling. We used single-cell profiling to facilitate cell identification in these mixed cultures where the whole-image approach would be unable to deal with the heterogeneity withing the field of view. In the introduction, we added a description of the whole-image vs. single-cell approach to profiling (page 4). In the discussion (page 18), we further highlight the application of this single-cell profiling approach for QC purposes.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Comments on Figure 1. It is unclear how panel B shows &quot;without replicate bias&quot;.</p>
</list-item></list>
</disp-quote>
<p>In response to this comment, we refer to the answer above: “The unsupervised UMAP shown in fig. 1B is either color coded by cell type (left) or replicate (right). Based on this feature map, we observe clustering along the UMAP1 axis to be associated with the cell type. Variations in cellular morphology associated with the biological replicate are more visible along the UMAP2 axis. When looking at fig. 1C, the feature map reflecting the cellular area shows a gradient along the UMAP1 direction, supporting the assumption that cell area contributes to the cell type separation. On the other hand, the average intensity (Channel 2 intensity) has a gradient within the feature map along the UMAP2 direction. This corresponds to the pattern associated with the inter-replicate variability in panel B.” We added this notion to page 5 of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>The paper would benefit from a description of how features were extracted sooner.</p>
</disp-quote>
<p>Information on the feature extraction was added to the manuscript at page 27. An additional table (table 5) has been added with the definition of each feature.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Comments on Supplementary Figure 4. The clustering with PCA is only showing 2 dimensions, so it is not surprising UMAP shows more distinct clustering.</p>
</list-item></list>
</disp-quote>
<p>We used two components for UMAP dimensionality reduction, so the data was also visualized in two dimensions. However, we agree that UMAP can show more distinct clustering as this method is non-linear.</p>
<disp-quote content-type="editor-comment">
<p>Why is Figure S4 the first referenced Supplementary Figure?</p>
</disp-quote>
<p>This has been changed.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Comments on Figure 2. Need discussion of the validation set - how was it determined? Panel E might have the answer I am looking for, but it is difficult to decipher exactly what is being done. The terminology needs to be defined somewhere, or maybe it is inconsistent. It is tough to tell. For example, what exactly are the two categories of model validation (cross-validation and independent testing)?</p>
</list-item></list>
</disp-quote>
<p>Additional clarification has been added to the manuscript at pages 6-7 and figure 2.</p>
<disp-quote content-type="editor-comment">
<p>The metric being reported is accuracy for the independent replicate if the other two are used to train?</p>
</disp-quote>
<p>Yes.</p>
<disp-quote content-type="editor-comment">
<p>Panel C is a very cool analysis. Panel F needs a description of how those images were selected, randomly?</p>
</disp-quote>
<p>Added in the methods section (page 29). GradCAM analysis was used to visualize the regions used by the CNN for classification. This map is specific to each cell. Images are selected randomly out the full dataset for visualization.</p>
<disp-quote content-type="editor-comment">
<p>They also need scale bars.</p>
</disp-quote>
<p>Added to the figures.</p>
<disp-quote content-type="editor-comment">
<p>Panel G would benefit from explicit channel labels (at least a legend would be good!).</p>
</disp-quote>
<p>Explanation has been added to the legend. All color code and channel numbering are consistent with fig. 1A.</p>
<disp-quote content-type="editor-comment">
<p>What do the dots and boxplots represent? The legend says, &quot;independent replicates&quot;, but independent replicates of, I assume, different model initializations?</p>
</disp-quote>
<p>Clarification has been added to the figure legends. For plots showing the performance of a CNN or RF classifier, each dot represents a different model initialization. Each classifier has been initialized at least 3 times. When indicated, the model training was performed with different random seeds for data splitting.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Comments on Figure 3. Panel A needs scale bar. See comment on Panel D in concern #1 described above.</p>
</list-item></list>
</disp-quote>
<p>This has been added.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Comments on Supplementary Figure 1. A reader will need a more detailed description in panel C. I assume that the grey bar is the average of the points, and the points represent different single cells?</p>
</list-item></list>
</disp-quote>
<p>How many cells? How were these cells selected?</p>
<p>This information on the figure (now Suppl. Fig. 1D), has been added to the legend.</p>
<p>“Left: Representative images of 1321N1 cells with increasing density alongside their cell and nuclear mask produced using resp. Cellpose and Stardist. Images are numbered from 1-5 with increasing density. Upper right: The number of ROIs detected in comparison to the ground truth (manual segmentation). A ROI was considered undetected when the intersection over union (IoU) was below 0,15. Each bar refers to the image number on the left. The IoU quantifies the overlap between ground truth (manually segmented ROI) and the ROI detected by the segmentation algorithm. It is defined as the area of the overlapping region over the total area. IoU for increasing cell density for cell and nuclear masks is given in the bottom right. Each point represents an individual ROI. Each bar refers to the image number on the left.”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Comments on Figure 4. More details on quenching are needed for a general audience. The markers chosen (EdU and BrdU) are generally not specific to cell type but to biological processes (proliferation), so it is confusing how they are being used as cell-type markers.</p>
</list-item></list>
</disp-quote>
<p>The base analogues were incorporated into each cell line prior to mixing them, i.e.  when they were still growing in monoculture so they could be labelled and identified after co-seeding and morphological profiling. Additional clarification has been added to the manuscript (page 26)</p>
<disp-quote content-type="editor-comment">
<p>It is also unclear why reducing CV is an important side-effect of finetuning. CV of what? The legend says, &quot;model iterations&quot;, but what does this mean?</p>
</disp-quote>
<p>The dots in the violinplot are different CNN initializations. A lower variability between model initializations is an indicator of certainty of the results. Prior to finetuning, the results of the CNN were highly variable leading to a high CoV between the different CNNs. This means the outcome after finetuning is more robust.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Comments on Figure 5. This is a very convincing and well-described result, kudos! This provides another opportunity to again compare other approaches (not just nucleocentric). Additionally, since the UMAP space uses hand-crafted features. The authors could consider interpreting the specific morphology features impacted by the striking gradual shift to neuron population by fitting a series of linear models per individual feature. This might confirm (or discover) how exactly the cells are shifting morphology.</p>
</list-item></list>
</disp-quote>
<p>The supervised UMAP on the handcrafted features did not highlight any features contributing to the separation. Using the supervised UMAP, the clustering is dominated by the known cell type. Unsupervised UMAP on the handcrafted features does not show any clustering. In response to a previous comment, we adapted the figure to show UMAP dimensionality reduction using the feature embeddings from the cell-based CNN. This unsupervised UMAP does show good cell type separation, but it does not use any directly interpretable shape descriptors.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>General comments on Methods. The section on &quot;ground truth alignment&quot; needs more details. Why was this performed?</p>
</list-item></list>
</disp-quote>
<p>Following sequential staining and imaging rounds, multiple images were captured representing the same cell with different markers. Lifting the plate of the microscope stage and imaging in sequential rounds after several days results in small linear translations in the exact location of each image. These linear translations need to be corrected to align (or register) morphological with ground truth image data within the same ROI. This notion has been added to the manuscript at page 26.</p>
<disp-quote content-type="editor-comment">
<p>Handcrafted features extracted using what software?</p>
</disp-quote>
<p>The complete analysis was performed in python. All packages used are listed in table 4. Handcrafted features were extracted using the scikit-image package (regionprops and GLCM functions). This has been added to the manuscript at page 27.</p>
<disp-quote content-type="editor-comment">
<p>Software should be cited more often throughout the manuscript.</p>
<p>Lastly, the GitHub URL points to the DeVosLab organization, but should point to a specific repository. Therefore, I was unable to review the provided code. A well-documented and reproducible analysis pipeline should be included.</p>
</disp-quote>
<p>A test dataset and source code are available on GitHub:  <ext-link ext-link-type="uri" xlink:href="https://github.com/DeVosLab/Nucleocentric-Profiling">https://github.com/DeVosLab/Nucleocentric-Profiling</ext-link></p>
</body>
</sub-article>
</article>