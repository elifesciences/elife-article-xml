<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">94198</article-id>
<article-id pub-id-type="doi">10.7554/eLife.94198</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.94198.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Speech-induced suppression and vocal feedback sensitivity in human cortex</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7472-4528</contrib-id>
<name>
<surname>Ozker</surname>
<given-names>Muge</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yu</surname>
<given-names>Leyao</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dugan</surname>
<given-names>Patricia</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Doyle</surname>
<given-names>Werner</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Friedman</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Devinsky</surname>
<given-names>Orrin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1247-1283</contrib-id>
<name>
<surname>Flinker</surname>
<given-names>Adeen</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Neurology Department, New York University</institution>, New York, 10016, NY, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Neurosurgery Department, New York University</institution>, New York, 10016, NY, <country>USA</country></aff>
<aff id="a3"><label>3</label><institution>Biomedical Engineering Department, New York University</institution>, Brooklyn, 11201, NY, <country>USA</country></aff>
<aff id="a4"><label>4</label><institution>Max Planck Institute for Psycholinguistics</institution>, 6525 XD Nijmegen, <country>The Netherlands</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ray</surname>
<given-names>Supratim</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bengaluru</city>
<country>India</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label><bold>*</bold></label><bold>Corresponding Author:</bold> Muge Ozker, <bold>e-mail:</bold> <email>muge.ozker-sertel@mpi.nl</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-02-26">
<day>26</day>
<month>02</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-07-31">
<day>31</day>
<month>07</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP94198</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-12-08">
<day>08</day>
<month>12</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-02-07">
<day>07</day>
<month>02</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.08.570736"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-02-26">
<day>26</day>
<month>02</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94198.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.94198.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.94198.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.94198.1.sa0">Reviewer #2 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>Â© 2024, Ozker et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Ozker et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-94198-v2.pdf"/>
<abstract>
<title>Abstract</title><p>Across the animal kingdom, neural responses in the auditory cortex are suppressed during vocalization, and humans are no exception. A common hypothesis is that suppression increases sensitivity to auditory feedback, enabling the detection of vocalization errors. This hypothesis has been previously confirmed in non-human primates, however a direct link between auditory suppression and sensitivity in human speech monitoring remains elusive. To address this issue, we obtained intracranial electroencephalography (iEEG) recordings from 35 neurosurgical participants during speech production. We first characterized the detailed topography of auditory suppression, which varied across superior temporal gyrus (STG). Next, we performed a delayed auditory feedback (DAF) task to determine whether the suppressed sites were also sensitive to auditory feedback alterations. Indeed, overlapping sites showed enhanced responses to feedback, indicating sensitivity. Importantly, there was a strong correlation between the degree of auditory suppression and feedback sensitivity, suggesting suppression might be a key mechanism that underlies speech monitoring. Further, we found that when participants produced speech with simultaneous auditory feedback, posterior STG was selectively activated if participants were engaged in a DAF paradigm, suggesting that increased attentional load can modulate auditory feedback sensitivity.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Figures 1 and 3 are updated. One paragraph in the Discussion section is changed.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>A major question in neuroscience is how do animals distinguish between stimuli originating from the environment and those produced by their own actions. Sensorimotor circuits share a common mechanism across the animal kingdom in which sensory responses to self-generated motor actions are suppressed. It is commonly hypothesized that suppressing responses to predicted self-generated stimuli increases sensitivity of the sensory system to external stimuli. (<xref ref-type="bibr" rid="c116">Poulet and Hedwig 2002</xref>, <xref ref-type="bibr" rid="c117">Poulet and Hedwig 2006</xref>, <xref ref-type="bibr" rid="c10">Crapse and Sommer 2008</xref>, <xref ref-type="bibr" rid="c13">E.Vonholstan, Glenn et al. 2011</xref>, <xref ref-type="bibr" rid="c121">Schneider and Mooney 2018</xref>). Furthermore, it enables detection and correction of motor errors by providing a template of the predicted sensory outcome to compare with the actual sensory outcome. In the domain of speech, this mechanism is described in models which suggest that neural responses in the auditory cortex are suppressed during speech production. When there is a mismatch between the predicted auditory outcome and the actual auditory feedback, responses in the auditory regions are enhanced to encode the mismatch and inform vocal-motor regions to correct vocalization (<xref ref-type="bibr" rid="c23">Hickok, Houde et al. 2011</xref>, <xref ref-type="bibr" rid="c27">Houde and Nagarajan 2011</xref>, <xref ref-type="bibr" rid="c125">Tourville and Guenther 2011</xref>).</p>
<p>A common experimental strategy to generate mismatch between the predicted auditory outcome and the actual auditory feedback is to perturb auditory feedback during speech production. Auditory feedback perturbations are usually applied either by delaying auditory feedback (DAF), which disrupts speech fluency (<xref ref-type="bibr" rid="c102">Lee 1950</xref>, <xref ref-type="bibr" rid="c16">Fairbanks 1955</xref>, <xref ref-type="bibr" rid="c123">Stuart, Kalinowski et al. 2002</xref>), or by shifting voice pitch and formants, which result in compensatory vocal changes in the opposite direction of the shift (<xref ref-type="bibr" rid="c26">Houde and Jordan 1998</xref>, <xref ref-type="bibr" rid="c30">Jones and Munhall 2000</xref>, <xref ref-type="bibr" rid="c109">Niziolek and Guenther 2013</xref>). Numerous electrophysiological and neuroimaging studies investigated neural responses during speech production both in the absence and presence of auditory feedback perturbations. In support of speech production models, these studies have repeatedly reported suppressed responses in auditory cortex during speaking compared with passive listening to speech (<xref ref-type="bibr" rid="c113">Numminen, Salmelin et al. 1999</xref>, <xref ref-type="bibr" rid="c130">Wise, Greene et al. 1999</xref>, <xref ref-type="bibr" rid="c76">Curio, Neuloh et al. 2000</xref>, <xref ref-type="bibr" rid="c28">Houde, Nagarajan et al. 2002</xref>, <xref ref-type="bibr" rid="c71">Christoffels, Formisano et al. 2007</xref>, <xref ref-type="bibr" rid="c82">Ford, Roach et al. 2010</xref>, <xref ref-type="bibr" rid="c110">Niziolek, Nagarajan et al. 2013</xref>), as well as enhanced responses when auditory feedback was perturbed indicating sensitivity to auditory feedback (<xref ref-type="bibr" rid="c126">Tourville, Reilly et al. 2008</xref>, <xref ref-type="bibr" rid="c2">Behroozmand, Karvelis et al. 2009</xref>, Chang, <xref ref-type="bibr" rid="c110">Niziolek et al. 2013</xref>, <xref ref-type="bibr" rid="c84">Greenlee, Behroozmand et al. 2013</xref>, <xref ref-type="bibr" rid="c100">Kort, Nagarajan et al. 2014</xref>, <xref ref-type="bibr" rid="c3">Behroozmand, Shebek et al. 2015</xref>, <xref ref-type="bibr" rid="c114">Ozker, Doyle et al. 2022</xref>). However, it is not clear whether the same or distinct neural populations in the auditory cortex show speech-induced suppression and sensitivity to auditory feedback.</p>
<p>While auditory responses are largely suppressed during speech production, detailed investigations using neurosurgical recordings revealed that the degree of suppression was variable across cortical sites, and auditory cortex also exhibited non-suppressed and enhanced responses (albeit less common) (<xref ref-type="bibr" rid="c11">Creutzfeldt and Ojemann 1989</xref>, <xref ref-type="bibr" rid="c17">Flinker, Chang et al. 2010</xref>, <xref ref-type="bibr" rid="c18">Greenlee, Jackson et al. 2011</xref>), mirroring results from non-human primate studies using single unit recordings (<xref ref-type="bibr" rid="c14">Eliades and Wang 2003</xref>, <xref ref-type="bibr" rid="c15">Eliades and Wang 2008</xref>). In the same non-human primate study, it was reported that neurons that were suppressed during vocalization showed increased activity when auditory feedback was perturbed (<xref ref-type="bibr" rid="c15">Eliades and Wang 2008</xref>). Based on this finding, we predicted that if speech-induced suppression enables detection and correction of speech errors, suppressed auditory sites should be sensitive to auditory feedback, thus exhibit enhanced neural responses to feedback perturbations. Alternatively, if suppression and speech monitoring are unrelated processes, then suppressed sites should be distinct from the ones that are sensitive to auditory feedback.</p>
<p>The level of attention during speech monitoring can vary depending on the speech task. During normal speech production, speech monitoring does not require a conscious effort, however it is a controlled, attentional process during an auditory feedback perturbation task (<xref ref-type="bibr" rid="c21">Hashimoto and Sakai 2003</xref>). It is well known that selective attention enhances auditory responses and improves speech perception under noisy listening conditions or when multiple speech streams are present (<xref ref-type="bibr" rid="c105">Mesgarani and Chang 2012</xref>, <xref ref-type="bibr" rid="c83">Golumbic, Ding et al. 2013</xref>). We predicted that increased attention to auditory feedback under adverse speaking conditions, such as during an auditory feedback perturbation task, should increase feedback sensitivity and elicit larger responses in the auditory cortex compared to normal speech production.</p>
<p>To summarize, in this study we aimed to test the hypothesis that speech-induced suppression increases sensitivity to auditory feedback in human neurophysiological recordings. We predicted that auditory sites showing speech induced suppression would elicit enhanced responses to auditory feedback perturbations. Further, we aimed to investigate the role of attention in auditory feedback sensitivity by comparing auditory responses during an auditory feedback perturbation task compared with normal speech production.</p>
<p>To address these aims, we used iEEG recordings in neurosurgical participants, which offers a level of spatial detail and temporal precision that would not be possible to achieve using non-invasive techniques. We first identified the sites that show auditory suppression during speech production, and then employed a DAF paradigm to test whether the same sites show sensitivity to perturbed feedback. Our results revealed that overlapping sites in the STG exhibited both speech-induced auditory suppression and sensitivity to auditory feedback with a strong correlation between the two measures, supporting the hypothesis that auditory suppression predicts sensitivity to speech errors in humans. Further, we showed that auditory responses in the posterior STG are enhanced in a DAF task compared to normal speech production, even for trials in which participants receive simultaneous auditory feedback (no-delay condition). This result suggests that increased attention during an auditory feedback perturbation task can modulate auditory feedback sensitivity and posterior STG is a critical region for this attentional modulation.</p>
</sec>
<sec id="s2">
<title>Materials and Methods</title>
<sec id="s2a">
<title>Participant Information</title>
<p>All experimental procedures were approved by the New York University School of Medicine Institutional Review Board. 35 neurosurgical epilepsy patients (19 females, mean age: 31, 23 left, 9 right and 3 bilateral hemisphere coverage) implanted with subdural and depth electrodes provided informed consent to participate in the research protocol. Electrode implantation and location were guided solely by clinical requirements. 3 patients were consented separately for higher density clinical grid implantation, which provided denser sampling of underlying cortex.</p>
</sec>
<sec id="s2b">
<title>Intracranial Electroencephalography (iEEG) Recording</title>
<p>iEEG was recorded from implanted subdural platinum-iridium electrodes embedded in flexible silicon sheets (2.3 mm diameter exposed surface, 8 x 8 grid arrays and 4 to 12 contact linear strips, 10 mm center-to-center spacing, Ad-Tech Medical Instrument, Racine, WI) and penetrating depth electrodes (1.1 mm diameter, 5-10 mm center-to-center spacing 1 x 8 or 1 x 12 contacts, Ad-Tech Medical Instrument, Racine, WI). 3 participants consented to a research hybrid grid implanted which included 64 additional electrodes between the standard clinical contacts (16 Ã 8 grid with sixty-four 2 mm macro contacts at 8 x 8 orientation and sixty-four 1 mm micro contacts in between, providing 10 mm center-to-center spacing between macro contacts and 5 mm center-to-center spacing between micro/macro contacts, PMT corporation, Chanassen, MN). Recordings were made using one of two amplifier types: NicoletOne amplifier (Natus Neurologics, Middleton, WI), bandpass filtered from 0.16-250 Hz and digitized at 512 Hz. Neuroworks Quantum Amplifier (Natus Biomedical, Appleton, WI) recorded at 2048 Hz, bandpass filtered at 0.01 to 682.67 Hz and then downsampled to 512 Hz. A two-contact subdural strip facing toward the skull near the craniotomy site was used as a reference for recording and a similar two-contact strip screwed to the skull was used for the instrument ground. iEEG and experimental signals (trigger pulses that mark the appearance of visual stimuli on the screen, microphone signal from speech recordings and feedback voice signal) were acquired simultaneously by the EEG amplifier in order to provide a fully synchronized dataset.</p>
</sec>
</sec>
<sec id="s3">
<title>Experimental Design</title>
<sec id="s3a">
<title>Experiment 1: Auditory word repetition (AWR)</title>
<p>35 participants performed the experiment. Stimuli consisted of 50 items (nouns) taken from the revised Snodgrass and Vanderwart object pictorial set (e.g. âdrumâ, âhatâ, âpencilâ) (<xref ref-type="bibr" rid="c120">Rossion and Pourtois 2004</xref>, <xref ref-type="bibr" rid="c122">Shum, Fanda et al. 2020</xref>). Auditory words presented randomly (2 repetitions) through speakers. Participants were instructed to listen to the presented words and repeat them out loud at each trial.</p>
</sec>
<sec id="s3b">
<title>Experiment 2: Visual word reading (VWR)</title>
<p>The same 35 participants performed the experiment. Stimuli consisted of the same 50 words used in Experiment 1, however visually presented as text stimuli on the screen in a random order (2 repetitions). Participants were instructed to read the presented word out loud at each trial.</p>
</sec>
<sec id="s3c">
<title>Experiment 3: Delayed auditory feedback (DAF)</title>
<p>A subgroup of 14 participants performed this experiment. Stimuli consisted of 10 different 3-syllable words visually presented as text stimuli on the screen (e.g. âenvelopeâ, âumbrellaâ, âviolinâ). Participants were instructed to read the presented word out loud at each trial. As participants spoke, their voices were recorded using the laptopâs internal microphone, delayed at 4 different amounts (no-delay, 50, 100, 200ms) using custom script (MATLAB, Psychtoolbox-3) and played back to them through earphones. Trials, which consisted of different stimulus-delay combinations, were presented randomly (3 to 8 repetitions). Behavioral and neural data from the DAF experiment were used in a previous publication from our group (<xref ref-type="bibr" rid="c114">Ozker, Doyle et al. 2022</xref>).</p>
</sec>
<sec id="s3d">
<title>Experiment 4: Visual word reading with auditory feedback (VWR-AF)</title>
<p>A subgroup of 4 participants performed an additional visual word reading experiment, in which they were presented with the word stimuli as in Experiment 3 and heard their simultaneous (no-delay) voice feedback through earphones.</p>
</sec>
<sec id="s3e">
<title>Statistical Analysis</title>
<p>Electrodes were examined for speech related activity defined as significant high gamma broadband responses. Unpaired t-tests were performed to compare responses to a baseline for each electrode and multiple comparisons were corrected using the false discovery rate (FDR) method (q=0.05). Electrodes that showed significant response increase (p &lt; 10<sup>-4</sup>) either before (â0.5 to 0 s) or after speech onset (0 to 0.5 s) with respect to a baseline period (â1 to â0.6 s) and at the same time had a large signal-to-noise ratio (Î¼/Ï &gt; 0.7) during either of these time windows were selected. Electrode selection was first performed for each task separately, then electrodes that were commonly selected were further analyzed. For the analysis of the DAF experiment, one-way ANOVA was calculated using the average neural response as a dependent variable and feedback delay as a factor to assess the statistical significance of response enhancement in a single electrode.</p>
</sec>
<sec id="s3f">
<title>Experiment Setup</title>
<p>Participants were tested while resting in their hospital bed in the epilepsy-monitoring unit. Visual stimuli were presented on a laptop screen positioned at a comfortable distance from the participant. Auditory stimuli were presented through speakers in the AWR and VWR experiments and through earphones (Bed Phones On-Ear Sleep Headphones Generation 3) in the DAF and in the VWR-AF experiment. Participants were instructed to speak at a normal voice level and sidetone volume was adjusted to a comfortable level at the beginning of the DAF experiment. DAF and VWR-AF experiments were performed consecutively and sidetone volume was kept the same in the two experiments. Participantsâ voice was recorded using an external microphone (Zoom H1 Handy Recorder). A TTL pulse marking the onset of a stimulus, the microphone signal (what the participant spoke) and the feedback voice signal (what the participant heard) were fed in to the EEG amplifier as an auxiliary input in order to acquire them in sync with EEG samples. Sound files recorded by the external microphone were used for voice intensity analysis. Average voice intensity for each trial was calculated in dB using the âIntensityâ object in Praat software (<xref ref-type="bibr" rid="c4">Boersma 2001</xref>).</p>
</sec>
<sec id="s3g">
<title>Electrode Localization</title>
<p>Electrode localization in individual space as well as MNI space was based on co-registering a preoperative (no electrodes) and postoperative (with electrodes) structural MRI (in some cases a postoperative CT was employed depending on clinical requirements) using a rigid-body transformation. Electrodes were then projected to the surface of cortex (preoperative segmented surface) to correct for edema induced shifts following previous procedures (<xref ref-type="bibr" rid="c131">Yang, Wang et al. 2012</xref>) (registration to MNI space was based on a non-linear DARTEL algorithm (<xref ref-type="bibr" rid="c1">Ashburner 2007</xref>). Within participant anatomical locations of electrodes was based on the automated FreeSurfer segmentation of the participantâs pre-operative MRI. We recorded from a total of 3591 subdural and 1361 depth electrode contacts in 35 participants. Subdural electrode coverage extended over lateral temporal, frontal, parietal and lateral occipital cortices. Depth electrodes covered additional regions to a limited extent including the transverse temporal gyrus, insula and fusiform gyrus. Contacts that were localized to the cortical white matter were excluded from the analysis. To categorize electrodes in the STG into anterior and posterior groups, lateral termination of the transverse temporal sulcus was used as an anatomical landmark (<xref ref-type="bibr" rid="c18">Greenlee, Jackson et al. 2011</xref>, <xref ref-type="bibr" rid="c111">Nourski, Steinschneider et al. 2016</xref>).</p>
</sec>
<sec id="s3h">
<title>Neural Data Analysis</title>
<p>Electrodes with epileptiform activity or artifacts caused by line noise, poor contact with cortex and high amplitude shifts were removed from further analysis. A common average reference was calculated by subtracting the average signal across all electrodes from each individual electrodeâs signal (after rejection of electrodes with artifacts). The analysis of the electrophysiologic signals focused on changes in broadband high gamma activity (70â150 Hz). To quantify changes in the high gamma range, the data were bandpass filtered between 70 and 150 Hz, and then a Hilbert transform was applied to obtain the analytic amplitude.</p>
<p>Recordings from the DAF and VWR-AF experiments were analyzed using the multitaper technique, which yields a more sensitive estimate of the power spectrum with lower variance, thus is more beneficial when comparing neural responses to incremental changes in stimuli. Continuous data streams from each channel were epoched into trials (from â1.5 s to 3.5 s with respect to speech onset). Line noise at 60, 120 and 180 Hz were filtered out. 3 Slepian tapers were applied in timesteps of 10 ms and frequency steps of 5 Hz, using temporal smoothing (tw) of 200 ms and frequency smoothing (fw) of Â±10 Hz. Tapered signals were then transformed to time-frequency space using discrete Fourier transform and power estimates from different tapers were combined (MATLAB, FieldTrip toolbox). The number of tapers (K) were determined by the Shannon number according to the formula: K=2*tw*fw-1 (<xref ref-type="bibr" rid="c115">Percival and Walden 1993</xref>). The high gamma broadband response (70-150 Hz) at each time point following stimulus onset was measured as the percent signal change from baseline, with the baseline calculated over all trials in a time window from â500 to â100 ms before stimulus onset.</p>
</sec>
<sec id="s3i">
<title>Suppression Index (SuppI) Calculation</title>
<p>Suppression of neural activity is measured by comparing responses in two time periods in the AWR task. First time period was during listening the stimulus (0-0.5 s) and the second time period was during speaking (0-0.5 s). For each trial, average responses over Listen and Speak periods were found and suppression was measured by calculating Listen-Speak/Listen+Speak. Then suppression values were averaged across trials to calculate a single suppression index for each electrode. For the neural activity, raw high gamma broadband signal power was used instead of the percent signal change to ensure that the suppression index values varied between â1 to 1, indicating a range from complete enhancement to complete suppression respectively.</p>
</sec>
<sec id="s3j">
<title>Sensitivity Index (SensI) Calculation</title>
<p>Sensitivity to DAF is measured by comparing neural responses to increasing amounts of feedback delay. Neural responses in each trial were averaged in a time period following the voice feedback (0-0.5 s). For each electrode, a sensitivity index was calculated by measuring the trial-by-trial Spearman correlation between the delay condition and the averaged neural response. A large sensitivity value indicated a strong response enhancement with increasing delays.</p>
</sec>
</sec>
<sec id="s4">
<title>Results</title>
<p>In order to assess cortical responses during perception and production of speech, and quantify speech-induced auditory suppression, participants (N = 35) performed an auditory word repetition (AWR) task. We examined the response patterns in seven different cortical regions including superior temporal gyrus (STG), middle temporal gyrus (MTG), supramarginal gyrus (SMG), inferior frontal gyrus (IFG), middle frontal gyrus (MFG), precentral gyrus (preCG) and postcentral gyrus (postCG) (<xref rid="fig1" ref-type="fig">Fig 1A</xref>). As an index of the neural response, we used the high gamma broadband signal (70-150 Hz, <italic>see Methods</italic>), which correlates with the spiking activity of the underlying neuronal population (<xref ref-type="bibr" rid="c107">Mukamel, Gelbard et al. 2005</xref>, <xref ref-type="bibr" rid="c12">Crone, Sinai et al. 2006</xref>, <xref ref-type="bibr" rid="c6">Cardin, Carlen et al. 2009</xref>, <xref ref-type="bibr" rid="c119">Ray and Maunsell 2011</xref>, <xref ref-type="bibr" rid="c101">Lachaux, Axmacher et al. 2012</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Cortical responses during speech tasks.</title>
<p><bold>A.</bold> Electrodes from all participants (n = 35) are shown on a template brain with different colors corresponding to different regions (number of electrodes in each region denoted in the parentheses). <bold>B.</bold> High gamma broadband responses (70-150 Hz) for individual trials in an Auditory Word Repetition task are shown for each region. <bold>C.</bold> High gamma responses for individual trials in a Visual Word Reading task are shown for each region. Trials are sorted with respect to speech onset (white line). <bold>D.</bold> Mean high gamma broadband response averaged across trials are shown for each region with the width representing the standard error of the mean across electrodes. Time=0 indicates speech production onset.</p></caption>
<graphic xlink:href="570736v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We analyzed the responses in two different time windows: During passive listening of the auditory stimulus (0-500 ms after stimulus onset) and during speaking when participants repeated the perceived auditory stimulus (0-500 ms after articulation onset). Average responses were larger during passive listening in STG (Average % signal change Â± SEM; Listen: 62.1Â±0.6, Speak: 29.8Â±0.4), MTG (32.7Â±0.9, 22.3Â±0.9) and SMG (27.4Â±0.8, 25.8Â±0.7) compared with speaking. Conversely, responses were larger during speaking in IFG (29.2Â±1.3, 31.2Â±1.3), MFG (28.3Â±1.6, 31.4Â±1.3), preCG (27.4Â±0.4, 37Â±0.5) and postCG (26Â±0.4, 42Â±0.5). These results suggested that auditory regions responded more strongly during passive listening compared to speaking, verifying previous reports of neural response suppression to self-generated speech in auditory cortex (<xref rid="fig1" ref-type="fig">Fig 1B-D</xref>).</p>
<p>In the AWR task, participants heard the same auditory stimulus twice in each trial, once from a recorded female voice and once from their own voice. It is well known that repeated presentation of a stimulus results in the suppression of neural activity in regions that process that stimulus, a neural adaptation phenomenon referred to as repetition suppression (<xref ref-type="bibr" rid="c19">Grill-Spector, Henson et al. 2006</xref>, <xref ref-type="bibr" rid="c124">Todorovic and de Lange 2012</xref>). To ensure that our observed suppression of neural activity in auditory regions was not due to repetition suppression, but rather was induced by speech production, we performed a visual word reading (VWR) task, in which participants hear the auditory stimulus only once (from their own voice). Response magnitudes during speaking in the AWR and VWR tasks were similar (paired t-test: t (466) = 0.62, p = 0.53), characterized by a strong correlation across electrodes (Pearsonâs Correlation: r = 0.9006, p = 0). These results suggested that repetition of the auditory stimulus in the AWR task did not affect response magnitudes and the observed reduction in response magnitudes was induced by speech production.</p>
<p>To quantify the amount of speech-induced suppression, we calculated a Suppression Index (SuppI) for each electrode by comparing neural responses during listening versus speaking in the AWR task (SuppI = Listen-Speak/Listen+Speak; <italic>see Methods</italic>). A positive SuppI indicated a response suppression during speaking compared to listening and was observed most strongly in middle to posterior parts of STG, followed by MTG and SMG. A negative SuppI indicated a response enhancement during speaking compared to listening and was observed in motor regions, most strongly in the postCG (<xref rid="fig2" ref-type="fig">Fig 2A-B</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Spatial topography of speech-induced auditory suppression.</title>
<p><bold>A.</bold> Suppression indices for all electrodes are shown on a template brain. Red color tones indicate smaller neural activity during speaking, while blue electrodes indicate larger neural activity during speaking compared to listening in the Auditory Word Repetition task. <bold>B.</bold> Suppression indices averaged across electrodes are shown for each region sorted from largest to smallest mean suppression index. Boxplots indicate mean Â± SD.</p></caption>
<graphic xlink:href="570736v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>After mapping the topographical distribution of suppression indices across the cortex, we focused on understanding the functional role of auditory suppression in speech monitoring. We hypothesized that the degree of speech-induced auditory suppression should be tightly linked to sensitivity to speech errors, as predicted by current models (<xref ref-type="bibr" rid="c27">Houde and Nagarajan 2011</xref>, <xref ref-type="bibr" rid="c125">Tourville and Guenther 2011</xref>) and neural data in non-human primates (<xref ref-type="bibr" rid="c15">Eliades and Wang 2008</xref>). To test this hypothesis, we used an additional task, in which we delayed the auditory feedback (DAF) during speech production to disrupt speech fluency. In this task, 14 participants repeated the VWR task while they were presented with their voice feedback through earphones either simultaneously (no-delay) or with a delay (50, 100 and 200 ms; <italic>see Methods</italic>). In a previous study (<xref ref-type="bibr" rid="c114">Ozker, Doyle et al. 2022</xref>), using the same data set, we demonstrated that participants slowed down their speech in response to DAF (Articulation duration; DAF<sub>0</sub>: 0.698, DAF<sub>50</sub>: 0.726, DAF<sub>100:</sub> 0.737, and DAF<sub>200:</sub> 0.749 milliseconds). Moreover, auditory regions exhibited an enhanced response that varied as a function of feedback delay, likely representing an auditory error signal encoding the mismatch between the expected and the actual feedback. However, those results were not directly linked to auditory suppression.</p>
<p>Here, we compared neural responses in the AWR and the DAF tasks to test whether auditory regions that exhibit strong speech-induced suppression also exhibit large auditory error responses to DAF, which would indicate strong sensitivity to speech errors. In a single participant, we demonstrated that a representative electrode on the STG with strong auditory suppression (Average % signal change in 0-500 ms; Listen: 124Â±7, Speak: 20Â±3, SuppI: 0.27) exhibited significant response enhancement (DAF<sub>0</sub>: 135Â±12, DAF<sub>50</sub>: 134Â±8, DAF<sub>100</sub>: 175Â±10, DAF<sub>200</sub>: 208Â±17, ANOVA: F (3, 116) = 8.5, p = 3.7e-05) (<xref rid="fig3" ref-type="fig">Fig 3A-B</xref>), while a nearby electrode with weaker auditory suppression (Listen: 116Â±6, Speak: 80Â±4, SuppI: 0.06) did not exhibit significant response enhancement with feedback delays (DAF<sub>0</sub>: 360Â±29, DAF<sub>50</sub>: 328Â±24, DAF<sub>100</sub>: 379Â±31, DAF<sub>200</sub>: 419Â±30, ANOVA: F (3, 116) = 1.73, p = 0.16) (<xref rid="fig3" ref-type="fig">Fig 3C-D</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Speech-induced auditory suppression and sensitivity to delayed auditory feedback in representative electrodes in a single participant.</title>
<p><bold>A.</bold> High gamma broadband response (70-150 Hz) in electrode G63 showing a large amount of auditory suppression during speaking words compared to listening to the same words. Error bars indicate SEM over trials. <bold>B.</bold> High gamma responses in electrode G63 to articulation of words with DAF. 0 seconds indicate the onset of the perceived auditory feedback. Inset figure shows the cortical surface model of the left hemisphere brain of a single participant. Black circles indicate the implanted electrodes. White highlighted electrodes are located on the middle (G63) and caudal (G54) STG. <bold>C.</bold> High gamma response in electrode G54 showing a small degree of auditory suppression during speaking words compared to listening. <bold>D.</bold> High gamma response in electrode G54 locked to articulation of words during DAF. 0 seconds indicate the onset of the perceived auditory feedback.</p></caption>
<graphic xlink:href="570736v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To quantify the auditory error response and measure the sensitivity of a cortical region to DAF, we calculated a Sensitivity Index (SensI) for each electrode by correlating the delay condition and the average neural response across trials (<italic>see Methods</italic>). A large SensI indicated a strong response enhancement (large auditory error response) with increasing delays. The degree of both speech-induced suppression and sensitivity to DAF were highly variable across the cortex, SuppI ranging from â0.46 to 0.53 and SensI ranging from â0.62 to 0.70. The largest suppression and sensitivity indices as well as a strong overlap between the two measures were observed in the STG, suggesting that auditory electrodes that show speech-induced suppression are also sensitive to auditory feedback perturbations (<xref rid="fig4" ref-type="fig">Fig 4A-C</xref>). We validated this relationship by revealing a significant correlation between suppression and sensitivity indices of auditory electrodes (n = 57, Pearsonâs Correlation: r = 0.4006, p = 0.002) supporting our hypothesis and providing evidence for a common neural mechanism (<xref rid="fig4" ref-type="fig">Fig 4D</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Correlation between speech-induced auditory suppression and sensitivity to delayed auditory feedback.</title>
<p><bold>A.</bold> Sensitivity indices for all electrodes are shown on a template brain (both right and left hemisphere electrodes were shown on the left hemisphere). Red tones indicate larger neural activity to increasing amount of delays in the Delayed Auditory Feedback task, while blue tones indicate the opposite. <bold>B.</bold> Suppression indices for all electrodes are shown on a template brain. Red tones indicate larger neural activity during listening compared to speaking in the Auditory Word Repetition task, while blue tones indicate the opposite. <bold>C.</bold> Electrodes that show either sensitivity to delayed auditory feedback (positive SensI value) or speech-induced auditory suppression (positive SuppI value), or both are shown on a template brain. <bold>D.</bold> Scatter plot and fitted regression showing a significant correlation between sensitivity to DAF and speech-induced auditory suppression across auditory electrodes. Each circle represents an electrodeâs sensitivity and suppression index.</p></caption>
<graphic xlink:href="570736v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Our neural analysis revealed that response magnitudes in auditory cortex were much larger when participants heard their simultaneous voice feedback in a DAF paradigm compared with producing speech without any feedback (DAF<sub>0</sub>: no-delay trials) (Average % signal change in 0-500 ms; DAF<sub>0</sub>: 113Â±14, VWR: 41Â±7, <italic>compare gray lines in</italic> <xref rid="fig3" ref-type="fig">Fig 3A</xref> and <xref rid="fig3" ref-type="fig">C</xref> with black lines in <bold>Fig3B</bold> and <bold>D,</bold> respectively). We were interested in dissociating if these larger responses were merely an effect of perceiving voice feedback through earphones instead of air or rather were specific to our DAF design, likely due to increased attentional demands. Therefore, 4 participants performed an additional visual word reading task in which they were presented with their simultaneous voice feedback through earphones (VWR-AF). As previous studies have reported that DAF can increase voice intensity (<xref ref-type="bibr" rid="c132">Yates 1963</xref>, <xref ref-type="bibr" rid="c96">Howell and Archer 1984</xref>), we first verified whether participants spoke louder during the DAF task. A comparison of their voice intensity between DAF<sub>0</sub> (no-delay trials in the DAF task) and the VWR-AF (standard word reading with simultaneous feedback through earphones) conditions did not show a significant difference (Voice intensity; DAF<sub>0</sub>: 50Â±11 dB, VWR: 49Â±12 dB; paired t-test: t (118) = 1.8, p = 0.08). After verifying that the sound volume entering the auditory system is not statistically different in the two conditions, we compared the responses in the auditory cortex and found that overall response magnitudes were now on par across the two conditions (DAF<sub>0</sub>: 89Â±17, VWR-AF: 82Â±17, <xref rid="fig5" ref-type="fig">Fig 5A</xref>). However, a detailed inspection of individual electrode responses revealed that some electrodes showed larger response to DAF<sub>0</sub>, while others showed either larger responses to VWR-AF or similar responses to both conditions (<xref rid="fig5" ref-type="fig">Fig 5B</xref>). In a single participant, we demonstrated that adjacent electrodes in the STG that are only 5 mm apart exhibited completely different response patterns. Electrodes in the more posterior parts of STG showed larger responses to DAF<sub>0</sub>, while electrodes in more anterior parts showed similar responses to DAF<sub>0</sub> and VWR-AF (<xref rid="fig5" ref-type="fig">Fig 5C</xref>). To determine an anatomical landmark at which the reversal of response patterns occurred in the STG, we used the lateral termination of the transverse temporal sulcus (TTS) (<xref ref-type="bibr" rid="c18">Greenlee, Jackson et al. 2011</xref>, <xref ref-type="bibr" rid="c111">Nourski, Steinschneider et al. 2016</xref>) based on the individual FreeSurfer segmentation of the participantâs pre-operative MRI. Across participants, this landmark corresponded to y coordinate = â22Â±2.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Effect of the delayed auditory feedback paradigm on neural responses during speech.</title>
<p><bold>A.</bold> High gamma broadband responses (70-150 Hz) averaged across auditory electrodes are similar during no-delay condition in the delayed auditory feedback task (DAF<sub>0</sub>) and during visual word reading with auditory feedback (VWR-AF). Error bars indicate SEM across electrodes. <bold>B.</bold> Scatter plot shows averaged high gamma responses (0-500 ms) for VWR-AF versus DAF<sub>0</sub> conditions for auditory electrodes. <bold>C.</bold> High gamma responses for DAF<sub>0</sub> and VWR-AF are shown in representative auditory electrodes in a single participant. Electrodes that are posteriorly located on the STG show larger responses to DAF<sub>0</sub> condition, while electrodes that are anteriorly located on the STG show similar responses to the two conditions. The lateral termination of the transverse temporal sulcus (TTS) is identified as a landmark (white zigzagged line) that separates the two different response patterns. <bold>D.</bold> High gamma responses for DAF<sub>0</sub> and VWR conditions were compared and resulting t-values are shown for all electrodes on a template brain. Pink color tones indicate larger responses to DAF<sub>0</sub>, while green color tones indicate larger responses to VWR condition. <bold>E.</bold> T-values calculated by comparing responses to DAF<sub>0</sub> and VWR conditions are shown for all auditory electrodes with respect to their anterior-to-posterior positions to the TTS.</p></caption>
<graphic xlink:href="570736v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, we compared the response patterns in the two conditions for all electrodes across participants by calculating a t-value for each electrode (unpaired t-test: average responses from â200 to 500 ms). We demonstrated that auditory regions in posterior STG showed larger responses to DAF<sub>0</sub> condition, while frontal motor regions showed larger responses to VWR-AF (<xref rid="fig5" ref-type="fig">Fig 5D</xref>). Lastly, we examined STG electrodes alone, sorted by their anterior-to-posterior positions with respect to the TTS. In line with the results from the single participant, electrodes that were located posteriorly within a 1 cm distance from this anatomical landmark showed significantly larger responses to the DAF<sub>0</sub> condition (<xref rid="fig5" ref-type="fig">Fig 5E</xref>). These results suggest that posterior STG is more activated when participants are engaged in a speech production task that requires increased effort and attention.</p>
</sec>
<sec id="s5">
<title>Discussion</title>
<p>Our study provides a detailed topographical investigation of speech-induced auditory suppression in a large cohort of neurosurgical participants. We found that while the strongest auditory suppression was observed in the STG, the degree of suppression was highly variable across different recording sites. To explain this variability, we considered the functional role of auditory suppression in speech monitoring. We showed that delaying auditory feedback during speech production enhanced auditory responses in the STG. The degree of sensitivity to feedback delays was also variable across different recording sites. We found a significant correlation between speech-induced suppression and feedback sensitivity, providing evidence for a shared mechanism between auditory suppression and speech monitoring. While there was no anatomical organization for auditory suppression and feedback sensitivity in the STG, we found an anterior-posterior organization for the effect of attention on feedback sensitivity. Auditory sites that lie posterior to the lateral termination of the TTS in the STG showed stronger activation during the DAF task compared to a standard word reading task, even for trials in which participants received simultaneous feedback, demonstrating attentional modulation of feedback sensitivity.</p>
<p>We observed the strongest speech-induced suppression in the middle and posterior parts of the STG. In line with previous iEEG studies, we found that degree of suppression was variable across different recording sites in the STG without any anatomical organization (<xref ref-type="bibr" rid="c17">Flinker, Chang et al. 2010</xref>, <xref ref-type="bibr" rid="c18">Greenlee, Jackson et al. 2011</xref>, <xref ref-type="bibr" rid="c111">Nourski, Steinschneider et al. 2016</xref>). So far, a clear gradient for speech-induced suppression has never been reported in the STG but only in the Heschlâs gyrus (HG) and superior temporal sulcus (STS) by studies that used comprehensive depth electrode coverage within the temporal lobe (<xref ref-type="bibr" rid="c111">Nourski, Steinschneider et al. 2016</xref>, <xref ref-type="bibr" rid="c112">Nourski, Steinschneider et al. 2021</xref>).</p>
<p>We found only a few sites with speech-induced enhancement and several sites with no response change. Based on single unit recordings in non-human primates, it is known that majority of neurons in the non-core auditory cortex exhibits suppression, while a smaller group exhibits excitation during vocalization. It is difficult to isolate speech-induced enhancement in human studies, because measurements reflect the average response of the underlying neural population, which is dominated by suppressed responses. A previous non-human primate study suggested that there might be a division of labor between the suppressed and excited neurons. They showed that when an external auditory stimulus is presented concurrently during vocalization, neurons that showed vocalization-induced suppression did not respond to the external stimulus. In contrary, neurons that showed vocalization-induced excitation responded even more when external stimulus is concurrently presented during vocalization, suggesting a role in maintaining sensitivity to the external acoustic environment (<xref ref-type="bibr" rid="c14">Eliades and Wang 2003</xref>). In humans there might be a similar division of labor between auditory sites that were suppressed and non-suppressed, such that while suppressed sites are engaged in monitoring self-generated sounds, non-suppressed sites maintain sensitivity to external sounds. But unfortunately, our study did not include the necessary experimental conditions to directly test this hypothesis.</p>
<p>Our broad topographical search using subdural electrodes revealed additional sites outside the canonical auditory regions in the STG that showed speech-induced suppression, mainly in the MTG, and a few others in the SMG and preCG. Sensorimotor regions in the preCG including inferior frontal and premotor cortices are known to activate during passive listening tasks (<xref ref-type="bibr" rid="c129">Wilson, Saygin et al. 2004</xref>, <xref ref-type="bibr" rid="c118">Pulvermuller, Huss et al. 2006</xref>, <xref ref-type="bibr" rid="c72">Cogan, Thesen et al. 2014</xref>), and show tuning to different acoustic properties of speech similar to the auditory regions in the STG (<xref ref-type="bibr" rid="c106">Mesgarani, Cheung et al. 2014</xref>, <xref ref-type="bibr" rid="c70">Cheung, Hamiton et al. 2016</xref>). Our results showed that isolated sites in these frontal motor regions were sensitive to DAF, confirming their auditory properties and suggesting their involvement in speech monitoring.</p>
<p>Current models of speech motor control predicted a shared mechanism between auditory suppression and sensitivity to speech errors suggesting a role for auditory suppression in speech monitoring (<xref ref-type="bibr" rid="c27">Houde and Nagarajan 2011</xref>, <xref ref-type="bibr" rid="c125">Tourville and Guenther 2011</xref>). Behavioral evidence in human studies showed that when auditory feedback is delayed in real time, speakers attempt to reset or slow down their speech (<xref ref-type="bibr" rid="c102">Lee 1950</xref>, <xref ref-type="bibr" rid="c16">Fairbanks 1955</xref>, <xref ref-type="bibr" rid="c123">Stuart, Kalinowski et al. 2002</xref>). Similarly, when fundamental frequency (pitch) or formant frequencies of the voice are shifted, speakers change their vocal output in the opposite direction of the shift to compensate for the spectral perturbation (<xref ref-type="bibr" rid="c26">Houde and Jordan 1998</xref>, <xref ref-type="bibr" rid="c30">Jones and Munhall 2000</xref>, <xref ref-type="bibr" rid="c109">Niziolek and Guenther 2013</xref>). Neurosurgical recordings and neuroimaging studies that investigate the brain mechanism of auditory feedback processing demonstrated that these feedback-induced vocal adjustments are accompanied by enhanced neural responses in various auditory regions (<xref ref-type="bibr" rid="c126">Tourville, Reilly et al. 2008</xref>, <xref ref-type="bibr" rid="c2">Behroozmand, Karvelis et al. 2009</xref>, <xref ref-type="bibr" rid="c3">Behroozmand, Shebek et al. 2015</xref>, <xref ref-type="bibr" rid="c114">Ozker, Doyle et al. 2022</xref>). However, it has not been clear whether it is the same or different neural populations that exhibit speech-induced suppression and enhanced responses to auditory feedback perturbations. Only in a non-human primate study, which recorded single-unit activity in auditory neurons of marmoset monkeys, it was shown that neurons that were suppressed during vocalization exhibited increased activity during frequency-shifted feedback (<xref ref-type="bibr" rid="c15">Eliades and Wang 2008</xref>). In contrast, to replicate this finding in humans, a previous iEEG study by Chang et al. (Chang, <xref ref-type="bibr" rid="c109">Niziolek et al. 2013</xref>) used frequency-shifted feedback during vowel production and found that most suppressed auditory sites did not overlap with those sensitive to feedback alterations. Using DAF instead of frequency-shifted feedback, we demonstrated a significant overlap of two neural populations in the STG, along with a strong correlation between the degree of speech-induced suppression and sensitivity to auditory feedback. This discrepancy may be due to different methods of calculating sensitivity to altered feedback. In our study, sensitivity was determined by comparing responses to delayed and non-delayed feedback during production, whereas Chang et al. compared perturbed feedback responses during production and listening. One possibility is that our approach identifies a larger auditory neural population in the STG sensitive to altered feedback. Alternatively, it could indicate a larger population highly sensitive to temporal rather than spectral perturbations in auditory feedback. Thus, we observe a wide overlap of the two neural populations in the STG showing both speech-induced suppression and sensitivity to auditory feedback. Replaying a recording of the participantsâ own delayed voice back to them, which we were unable to complete in this study, would have made the results of the two studies more comparable while also completely eliminating the possibility of a sensory explanation for the observed response enhancement.</p>
<p>Forward models of speech production suggest that a mismatch between the predicted and the actual auditory feedback is encoded by a response enhancement in the auditory cortex signifying an error signal (<xref ref-type="bibr" rid="c27">Houde and Nagarajan 2011</xref>, <xref ref-type="bibr" rid="c125">Tourville and Guenther 2011</xref>, <xref ref-type="bibr" rid="c22">Hickok 2012</xref>). Our results suggested that attention to oneâs own speech stream during adverse speaking conditions, such as during an auditory feedback perturbations task, might also contribute to the response enhancement in the auditory cortex. Auditory feedback control of speech was thought to be involuntary and not subject to attentional control, because several previous studies showed that participants produced compensatory responses to pitch shifts even when they were told to ignore feedback perturbations (<xref ref-type="bibr" rid="c108">Munhall, MacDonald et al. 2009</xref>, <xref ref-type="bibr" rid="c133">Zarate, Wood et al. 2010</xref>, <xref ref-type="bibr" rid="c31">Keough, Hawco et al. 2013</xref>). However, prolonging pitch shift duration resulted in an early vocal response that opposes the pitch shift direction and a later vocal response that follows the pitch shift direction suggesting an interplay between reflexive and top-down processes in controlling voice pitch (<xref ref-type="bibr" rid="c20">Hain, Burnett et al. 2000</xref>, <xref ref-type="bibr" rid="c5">Burnett and Larson 2002</xref>). More recent EEG studies demonstrated that dividing attention between auditory feedback and additional visual stimuli or increasing the attentional load of the task affected vocal responses as well as the magnitude of ERP components, suggesting that attention modulates auditory feedback control on both a behavioral and a cortical level (<xref ref-type="bibr" rid="c127">Tumber, Scheerer et al. 2014</xref>, Hu, <xref ref-type="bibr" rid="c104">Liu et al. 2015</xref>, Liu, <xref ref-type="bibr" rid="c29">Hu et al. 2015</xref>, <xref ref-type="bibr" rid="c103">Liu, Fan et al. 2018</xref>). In our study, we found that neural responses in the posterior STG were larger for DAF<sub>0</sub> (randomly presented simultaneous feedback condition in the DAF task) as compared with the VWR-AF condition (consistent simultaneous feedback throughout standard word reading task), even though participants displayed similar vocal behavior in these two conditions. In light of the previous literature, we interpret these response differences as arising from an attentional load difference between the two tasks. In the DAF experiment, the auditory feedback was not consistent since no-delay trials were randomized with delay trials. This randomized structure of the paradigm with interleaved long delay trials (causing slowed speech) required conscious effort for speech-monitoring and thus sustained attention. While remaining cautious about this interpretation and our studyâs limitation in attentional controls, we believe that this response enhancement represents an increased neural gain driven by attention to auditory feedback (<xref ref-type="bibr" rid="c25">Hillyard, Vogel et al. 1998</xref>), and highlights the critical role of the posterior STG in auditory-motor integration during speech monitoring (<xref ref-type="bibr" rid="c24">Hickok and Poeppel 2000</xref>), with its close proximity to the human ventral attention network comprising temporoparietal junction (TPJ) (<xref ref-type="bibr" rid="c128">Vossel, Geng et al. 2014</xref>). We leave it to future studies to include additional conditions to manipulate the direction and load of attention to further validate the influence of attention on speech monitoring.</p>
</sec>
</body>
<back>
<sec id="s6">
<title>Funding Information</title>
<p>This study was supported by grants from the NIH (F32 DC018200 to M.O. and R01NS109367, R01DC018805, R01NS115929 to A.F.) and the NSF (CRCNS 1912286 to A.F.) and by the Leon Levy Foundation Fellowship (to M.O.). Open access funding is provided by Max Planck Society.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Ashburner</surname>, <given-names>J</given-names></string-name>. (<year>2007</year>). â<article-title>A fast diffeomorphic image registration algorithm</article-title>.â <source>Neuroimage</source> <volume>38</volume>(<issue>1</issue>): <fpage>95</fpage>â<lpage>113</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Behroozmand</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>L.</given-names> <surname>Karvelis</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Liu</surname></string-name> and <string-name><given-names>C. R.</given-names> <surname>Larson</surname></string-name> (<year>2009</year>). â<article-title>Vocalization-induced enhancement of the auditory cortex responsiveness during voice F0 feedback perturbation</article-title>.â <source>Clin Neurophysiol</source> <volume>120</volume>(<issue>7</issue>): <fpage>1303</fpage>â<lpage>1312</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Behroozmand</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>R.</given-names> <surname>Shebek</surname></string-name>, <string-name><given-names>D. R.</given-names> <surname>Hansen</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Oya</surname></string-name>, <string-name><given-names>D. A.</given-names> <surname>Robin</surname></string-name>, <string-name><given-names>M. A.</given-names> <surname>Howard</surname>, <suffix>3rd</suffix></string-name> and <string-name><given-names>J. D.</given-names> <surname>Greenlee</surname></string-name> (<year>2015</year>). <article-title>âSensory-motor networks involved in speech production and motor control: an fMRI study.â</article-title> <source>Neuroimage</source> <volume>109</volume>: <fpage>418</fpage>â<lpage>428</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Boersma</surname>, <given-names>P</given-names></string-name>. (<year>2001</year>). â<article-title>Praat, a system for doing phonetics by computer</article-title>..â <source>Glot International</source> <volume>5</volume>:<issue><bold>9/10</bold></issue>: <fpage>341</fpage>â<lpage>345</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Burnett</surname>, <given-names>T. A.</given-names></string-name> and <string-name><given-names>C. R.</given-names> <surname>Larson</surname></string-name> (<year>2002</year>). â<article-title>Early pitch-shift response is active in both steady and dynamic voice pitch control</article-title>.â <source>The Journal of the Acoustical Society of America</source> <volume>112</volume>(<issue>3</issue>): <fpage>1058</fpage>â<lpage>1063</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Cardin</surname>, <given-names>J. A.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Carlen</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Meletis</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Knoblich</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Deisseroth</surname></string-name>, <string-name><given-names>L. H.</given-names> <surname>Tsai</surname></string-name> and <string-name><given-names>C. I.</given-names> <surname>Moore</surname></string-name> (<year>2009</year>). â<article-title>Driving fast-spiking cells induces gamma rhythm and controls sensory responses</article-title>.â <source>Nature</source> <volume>459</volume>(<issue>7247</issue>): <fpage>663</fpage>â<lpage>667</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Chang</surname>, <given-names>E. F.</given-names></string-name>, <string-name><given-names>C. A.</given-names> <surname>Niziolek</surname></string-name>, <string-name><given-names>R. T.</given-names> <surname>Knight</surname></string-name>, <string-name><given-names>S. S.</given-names> <surname>Nagarajan</surname></string-name> and <string-name><given-names>J. F.</given-names> <surname>Houde</surname></string-name> (<year>2013</year>). â<article-title>Human cortical sensorimotor network underlying feedback control of vocal pitch</article-title>.â <source>Proc Natl Acad Sci U S A</source> <volume>110</volume>(<issue>7</issue>): <fpage>2653</fpage>â<lpage>2658</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Cogan</surname>, <given-names>G. B.</given-names></string-name>, <string-name><given-names>T.</given-names> <surname>Thesen</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Carlson</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Doyle</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Devinsky</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Pesaran</surname></string-name> (<year>2014</year>). â<article-title>Sensory-motor transformations for speech occur bilaterally</article-title>.â <source>Nature</source> <volume>507</volume>(<issue>7490</issue>): <fpage>94</fpage>â<lpage>98</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Crapse</surname>, <given-names>T. B.</given-names></string-name> and <string-name><given-names>M. A.</given-names> <surname>Sommer</surname></string-name> (<year>2008</year>). â<article-title>Corollary discharge across the animal kingdom</article-title>.â <source>Nature Reviews Neuroscience</source> <volume>9</volume>(<issue>8</issue>): <fpage>587</fpage>â<lpage>600</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Creutzfeldt</surname>, <given-names>O.</given-names></string-name> and <string-name><given-names>G.</given-names> <surname>Ojemann</surname></string-name> (<year>1989</year>). â<article-title>Neuronal activity in the human lateral temporal lobe. III. Activity changes during music</article-title>.â <source>Exp Brain Res</source> <volume>77</volume>(<issue>3</issue>): <fpage>490</fpage>â<lpage>498</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Crone</surname>, <given-names>N. E.</given-names></string-name>, <string-name><given-names>A.</given-names> <surname>Sinai</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Korzeniewska</surname></string-name> (<year>2006</year>). â<article-title>High-frequency gamma oscillations and human brain mapping with electrocorticography</article-title>.â <source>Prog Brain Res</source> <volume>159</volume>: <fpage>275</fpage>â<lpage>295</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="book"><string-name><surname>Vonholstan</surname>, <given-names>E.</given-names></string-name>,  and <string-name><surname>Mittelstaedt</surname>, <given-names>H</given-names></string-name> eds <string-name><surname>Dodwell</surname>, <given-names>P.C.</given-names></string-name>, (<year>1950</year>). <chapter-title>The Principle of Reafference : Interactions Between the Central Nervous System and the Peripheral Organs</chapter-title>. <source>Perceptual Processing: Stimulus Equivalence and Pattern Recognition</source>, <publisher-name>Appleton-Century-Crofts</publisher-name>, <publisher-loc>New York, United States</publisher-loc>, <fpage>41</fpage>-<lpage>72</lpage></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Eliades</surname>, <given-names>S. J.</given-names></string-name> and <string-name><given-names>X.</given-names> <surname>Wang</surname></string-name> (<year>2003</year>). â<article-title>Sensory-motor interaction in the primate auditory cortex during self-initiated vocalizations</article-title>.â <source>Journal of neurophysiology</source> <volume>89</volume>(<issue>4</issue>): <fpage>2194</fpage>â<lpage>2207</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Eliades</surname>, <given-names>S. J.</given-names></string-name> and <string-name><given-names>X.</given-names> <surname>Wang</surname></string-name> (<year>2008</year>). â<article-title>Neural substrates of vocalization feedback monitoring in primate auditory cortex</article-title>.â <source>Nature</source> <volume>453</volume>(<issue>7198</issue>): <fpage>1102</fpage>â<lpage>1106</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Fairbanks</surname>, <given-names>G</given-names></string-name>. (<year>1955</year>). â<article-title>Selective vocal effects of delayed auditory feedback</article-title>.â <source>Journal of speech and hearing disorders</source> <volume>20</volume>(<issue>4</issue>): <fpage>333</fpage>â<lpage>346</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Flinker</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>E. F.</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>H. E.</given-names> <surname>Kirsch</surname></string-name>, <string-name><given-names>N. M.</given-names> <surname>Barbaro</surname></string-name>, <string-name><given-names>N. E.</given-names> <surname>Crone</surname></string-name> and <string-name><given-names>R. T.</given-names> <surname>Knight</surname></string-name> (<year>2010</year>). â<article-title>Single-trial speech suppression of auditory cortex activity in humans</article-title>.â <source>Journal of Neuroscience</source> <volume>30</volume>(<issue>49</issue>): <fpage>16643</fpage>â<lpage>16650</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Greenlee</surname>, <given-names>J. D.</given-names></string-name>, <string-name><given-names>A. W.</given-names> <surname>Jackson</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>C. R.</given-names> <surname>Larson</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Oya</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Kawasaki</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Chen</surname></string-name> and <string-name><given-names>M. A.</given-names> <surname>Howard</surname> <suffix>III</suffix></string-name> (<year>2011</year>). â<article-title>Human auditory cortical activation during self-vocalization</article-title>.â <source>PloS one</source> <volume>6</volume>(<issue>3</issue>): <fpage>e14744</fpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, <string-name><given-names>R.</given-names> <surname>Henson</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Martin</surname></string-name> (<year>2006</year>). â<article-title>Repetition and the brain: neural models of stimulus-specific effects</article-title>.â <source>Trends Cogn Sci</source> <volume>10</volume>(<issue>1</issue>): <fpage>14</fpage>â<lpage>23</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Hain</surname>, <given-names>T. C.</given-names></string-name>, <string-name><given-names>T. A.</given-names> <surname>Burnett</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kiran</surname></string-name>, <string-name><given-names>C. R.</given-names> <surname>Larson</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Singh</surname></string-name> and <string-name><given-names>M. K.</given-names> <surname>Kenney</surname></string-name> (<year>2000</year>). â<article-title>Instructing subjects to make a voluntary response reveals the presence of two components to the audio-vocal reflex</article-title>.â <source>Experimental Brain Research</source> <volume>130</volume>(<issue>2</issue>): <fpage>133</fpage>â<lpage>141</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Hashimoto</surname>, <given-names>Y.</given-names></string-name> and <string-name><given-names>K. L.</given-names> <surname>Sakai</surname></string-name> (<year>2003</year>). â<article-title>Brain activations during conscious self-monitoring of speech production with delayed auditory feedback: an fMRI study</article-title>.â <source>Hum Brain Mapp</source> <volume>20</volume>(<issue>1</issue>): <fpage>22</fpage>â<lpage>28</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Hickok</surname>, <given-names>G</given-names></string-name>. (<year>2012</year>). â<article-title>Computational neuroanatomy of speech production</article-title>.â <source>Nat Rev Neurosci</source> <volume>13</volume>(<issue>2</issue>): <fpage>135</fpage>â<lpage>145</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Houde</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Rong</surname></string-name> (<year>2011</year>). â<article-title>Sensorimotor integration in speech processing: computational basis and neural organization</article-title>.â <source>Neuron</source> <volume>69</volume>(<issue>3</issue>): <fpage>407</fpage>â<lpage>422</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name> and <string-name><given-names>D.</given-names> <surname>Poeppel</surname></string-name> (<year>2000</year>). â<article-title>Towards a functional neuroanatomy of speech perception</article-title>.â <source>Trends Cogn Sci</source> <volume>4</volume>(<issue>4</issue>): <fpage>131</fpage>â<lpage>138</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Hillyard</surname>, <given-names>S. A.</given-names></string-name>, <string-name><given-names>E. K.</given-names> <surname>Vogel</surname></string-name> and <string-name><given-names>S. J.</given-names> <surname>Luck</surname></string-name> (<year>1998</year>). â<article-title>Sensory gain control (amplification) as a mechanism of selective attention: electrophysiological and neuroimaging evidence</article-title>.â <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>353</volume>(<issue>1373</issue>): <fpage>1257</fpage>â<lpage>1270</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Houde</surname>, <given-names>J. F.</given-names></string-name> and <string-name><given-names>M. I.</given-names> <surname>Jordan</surname></string-name> (<year>1998</year>). â<article-title>Sensorimotor adaptation in speech production</article-title>.â <source>Science</source> <volume>279</volume>(<issue>5354</issue>): <fpage>1213</fpage>â<lpage>1216</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Houde</surname>, <given-names>J. F.</given-names></string-name> and <string-name><given-names>S. S.</given-names> <surname>Nagarajan</surname></string-name> (<year>2011</year>). â<article-title>Speech production as state feedback control</article-title>.â <source>Frontiers in human neuroscience</source> <volume>5</volume>: <fpage>82</fpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Houde</surname>, <given-names>J. F.</given-names></string-name>, <string-name><given-names>S. S.</given-names> <surname>Nagarajan</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Sekihara</surname></string-name> and <string-name><given-names>M. M.</given-names> <surname>Merzenich</surname></string-name> (<year>2002</year>). â<article-title>Modulation of the auditory cortex during speech: an MEG study</article-title>.â <source>J Cogn Neurosci</source> <volume>14</volume>(<issue>8</issue>): <fpage>1125</fpage>â<lpage>1138</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Hu</surname>, <given-names>H.</given-names></string-name>, <string-name><given-names>Y.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Guo</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Chen</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Liu</surname></string-name> (<year>2015</year>). â<article-title>Attention modulates cortical processing of pitch feedback errors in voice control</article-title>.â <source>Scientific Reports</source> <volume>5</volume>(<issue>1</issue>): <fpage>1</fpage>â<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Jones</surname>, <given-names>J. A.</given-names></string-name> and <string-name><given-names>K. G.</given-names> <surname>Munhall</surname></string-name> (<year>2000</year>). â<article-title>Perceptual calibration of F0 production: evidence from feedback perturbation</article-title>.â <source>J Acoust Soc Am</source> <volume>108</volume>(<issue>3 Pt 1</issue>): <fpage>1246</fpage>â<lpage>1251</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Keough</surname>, <given-names>D.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Hawco</surname></string-name> and <string-name><given-names>J. A.</given-names> <surname>Jones</surname></string-name> (<year>2013</year>). â<article-title>Auditory-motor adaptation to frequency-altered auditory feedback occurs when participants ignore feedback</article-title>.â <source>BMC Neuroscience</source> <volume>14</volume>(<issue>1</issue>): <fpage>25</fpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Kort</surname>, <given-names>N. S.</given-names></string-name>, <string-name><given-names>S. S.</given-names> <surname>Nagarajan</surname></string-name> and <string-name><given-names>J. F.</given-names> <surname>Houde</surname></string-name> (<year>2014</year>). â<article-title>A bilateral cortical network responds to pitch perturbations in speech feedback</article-title>.â <source>Neuroimage</source> <volume>86</volume>: <fpage>525</fpage>â<lpage>535</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Lachaux</surname>, <given-names>J. P.</given-names></string-name>, <string-name><given-names>N.</given-names> <surname>Axmacher</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Mormann</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Halgren</surname></string-name> and <string-name><given-names>N. E.</given-names> <surname>Crone</surname></string-name> (<year>2012</year>). â<article-title>High-frequency neural activity and human cognition: past, present and possible future of intracranial EEG research</article-title>.â <source>Prog Neurobiol</source> <volume>98</volume>(<issue>3</issue>): <fpage>279</fpage>â<lpage>301</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Lee</surname>, <given-names>B. S</given-names></string-name>. (<year>1950</year>). â<article-title>Effects of delayed speech feedback</article-title>.â <source>The Journal of the Acoustical Society of America</source> <volume>22</volume>(<issue>6</issue>): <fpage>824</fpage>â<lpage>826</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><given-names>H.</given-names> <surname>Fan</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Jones</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Zhang</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Liu</surname></string-name> (<year>2018</year>). â<article-title>Auditory-motor control of vocal production during divided attention: behavioral and ERP correlates</article-title>.â <source>Frontiers in Neuroscience</source> <volume>12</volume>: <fpage>113</fpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><given-names>H.</given-names> <surname>Hu</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Jones</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Guo</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Liu</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Liu</surname></string-name> (<year>2015</year>). â<article-title>Selective and divided attention modulates auditory-vocal integration in the processing of pitch feedback errors</article-title>.â <source>Eur J Neurosci</source> <volume>42</volume>(<issue>3</issue>): <fpage>1895</fpage>â<lpage>1904</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Cheung</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Johnson</surname></string-name> and <string-name><given-names>E. F.</given-names> <surname>Chang</surname></string-name> (<year>2014</year>). â<article-title>Phonetic feature encoding in human superior temporal gyrus</article-title>.â <source>Science</source> <volume>343</volume>(<issue>6174</issue>): <fpage>1006</fpage>â<lpage>1010</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Mukamel</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>H.</given-names> <surname>Gelbard</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Arieli</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Hasson</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Fried</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Malach</surname></string-name> (<year>2005</year>). â<article-title>Coupling between neuronal firing, field potentials, and FMRI in human auditory cortex</article-title>.â <source>Science</source> <volume>309</volume>(<issue>5736</issue>): <fpage>951</fpage>â<lpage>954</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Munhall</surname>, <given-names>K. G.</given-names></string-name>, <string-name><given-names>E. N.</given-names> <surname>MacDonald</surname></string-name>, <string-name><given-names>S. K.</given-names> <surname>Byrne</surname></string-name> and <string-name><given-names>I.</given-names> <surname>Johnsrude</surname></string-name> (<year>2009</year>). â<article-title>Talkers alter vowel production in response to real-time formant perturbation even when instructed not to compensate</article-title>.â <source>The Journal of the Acoustical Society of America</source> <volume>125</volume>(<issue>1</issue>): <fpage>384</fpage>â<lpage>390</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Niziolek</surname>, <given-names>C. A.</given-names></string-name> and <string-name><given-names>F. H.</given-names> <surname>Guenther</surname></string-name> (<year>2013</year>). â<article-title>Vowel category boundaries enhance cortical and behavioral responses to speech feedback alterations</article-title>.â <source>J Neurosci</source> <volume>33</volume>(<issue>29</issue>): <fpage>12090</fpage>â<lpage>12098</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Nourski</surname>, <given-names>K. V.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Steinschneider</surname></string-name> and <string-name><given-names>A. E.</given-names> <surname>Rhone</surname></string-name> (<year>2016</year>). â<article-title>Electrocorticographic Activation within Human Auditory Cortex during Dialog-Based Language and Cognitive Testing</article-title>.â <source>Front Hum Neurosci</source> <volume>10</volume>: <fpage>202</fpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Nourski</surname>, <given-names>K. V.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Steinschneider</surname></string-name>, <string-name><given-names>A. E.</given-names> <surname>Rhone</surname></string-name>, <string-name><given-names>C. K.</given-names> <surname>Kovach</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Banks</surname></string-name>, <string-name><given-names>B. M.</given-names> <surname>Krause</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Kawasaki</surname></string-name> and <string-name><given-names>M. A.</given-names> <surname>Howard</surname></string-name> (<year>2021</year>). â<article-title>Electrophysiology of the Human Superior Temporal Sulcus during Speech Processing</article-title>.â <source>Cereb Cortex</source> <volume>31</volume>(<issue>2</issue>): <fpage>1131</fpage>â<lpage>1148</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Numminen</surname>, <given-names>J.</given-names></string-name>, <string-name><given-names>R.</given-names> <surname>Salmelin</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Hari</surname></string-name> (<year>1999</year>). â<article-title>Subjectâs own speech reduces reactivity of the human auditory cortex</article-title>.â <source>Neurosci Lett</source> <volume>265</volume>(<issue>2</issue>): <fpage>119</fpage>â<lpage>122</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Ozker</surname>, <given-names>M.</given-names></string-name>, <string-name><given-names>W.</given-names> <surname>Doyle</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Devinsky</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Flinker</surname></string-name> (<year>2022</year>). â<article-title>A cortical network processes auditory error signals during human speech production to maintain fluency</article-title>.â <source>PLoS Biol</source> <volume>20</volume>(<issue>2</issue>): <fpage>e3001493</fpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="book"><string-name><surname>Percival</surname>, <given-names>D. B.</given-names></string-name> and <string-name><given-names>A. T.</given-names> <surname>Walden</surname></string-name> (<year>1993</year>). <source>Spectral analysis for physical applications</source>, <publisher-name>cambridge university press</publisher-name>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Poulet</surname>, <given-names>J. F.</given-names></string-name> and <string-name><given-names>B.</given-names> <surname>Hedwig</surname></string-name> (<year>2002</year>). â<article-title>A corollary discharge maintains auditory sensitivity during sound production</article-title>.â <source>Nature</source> <volume>418</volume>(<issue>6900</issue>): <fpage>872</fpage>â<lpage>876</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Poulet</surname>, <given-names>J. F.</given-names></string-name> and <string-name><given-names>B.</given-names> <surname>Hedwig</surname></string-name> (<year>2006</year>). â<article-title>The cellular basis of a corollary discharge</article-title>.â <source>Science</source> <volume>311</volume>(<issue>5760</issue>): <fpage>518</fpage>â<lpage>522</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Pulvermuller</surname>, <given-names>F.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Huss</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Kherif</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Moscoso del Prado Martin</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Hauk</surname></string-name> and <string-name><given-names>Y.</given-names> <surname>Shtyrov</surname></string-name> (<year>2006</year>). â<article-title>Motor cortex maps articulatory features of speech sounds</article-title>.â <source>Proc Natl Acad Sci U S A</source> <volume>103</volume>(<issue>20</issue>): <fpage>7865</fpage>â<lpage>7870</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Ray</surname>, <given-names>S.</given-names></string-name> and <string-name><given-names>J. H.</given-names> <surname>Maunsell</surname></string-name> (<year>2011</year>). â<article-title>Different origins of gamma rhythm and high-gamma activity in macaque visual cortex</article-title>.â <source>PLoS Biol</source> <volume>9</volume>(<issue>4</issue>): <fpage>e1000610</fpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name> and <string-name><given-names>G.</given-names> <surname>Pourtois</surname></string-name> (<year>2004</year>). â<article-title>Revisiting Snodgrass and Vanderwartâs Object Pictorial Set: The Role of Surface Detail in Basic-Level Object Recognition</article-title>.â <source>Perception</source> <volume>33</volume>(<issue>2</issue>): <fpage>217</fpage>â<lpage>236</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Schneider</surname>, <given-names>D. M.</given-names></string-name> and <string-name><given-names>R.</given-names> <surname>Mooney</surname></string-name> (<year>2018</year>). â<article-title>How movement modulates hearing</article-title>.â <source>Annual review of neuroscience</source> <volume>41</volume>: <fpage>553</fpage>â<lpage>572</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Shum</surname>, <given-names>J.</given-names></string-name>, <string-name><given-names>L.</given-names> <surname>Fanda</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Dugan</surname></string-name>, <string-name><given-names>W. K.</given-names> <surname>Doyle</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Devinsky</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Flinker</surname></string-name> (<year>2020</year>). â<article-title>Neural correlates of sign language production revealed by electrocorticography</article-title>.â <source>Neurology</source> <volume>95</volume>(<issue>21</issue>): <fpage>e2880</fpage>â<lpage>e2889</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Stuart</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Kalinowski</surname></string-name>, <string-name><given-names>M. P.</given-names> <surname>Rastatter</surname></string-name> and <string-name><given-names>K.</given-names> <surname>Lynch</surname></string-name> (<year>2002</year>). â<article-title>Effect of delayed auditory feedback on normal speakers at two speech rates</article-title>.â <source>J Acoust Soc Am</source> <volume>111</volume>(<issue>5 Pt 1</issue>): <fpage>2237</fpage>â<lpage>2241</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Todorovic</surname>, <given-names>A.</given-names></string-name> and <string-name><given-names>F. P.</given-names> <surname>de Lange</surname></string-name> (<year>2012</year>). â<article-title>Repetition suppression and expectation suppression are dissociable in time in early auditory evoked fields</article-title>.â <source>J Neurosci</source> <volume>32</volume>(<issue>39</issue>): <fpage>13389</fpage>â<lpage>13395</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Tourville</surname>, <given-names>J. A.</given-names></string-name> and <string-name><given-names>F. H.</given-names> <surname>Guenther</surname></string-name> (<year>2011</year>). â<article-title>The DIVA model: A neural theory of speech acquisition and production</article-title>.â <source>Lang Cogn Process</source> <volume>26</volume>(<issue>7</issue>): <fpage>952</fpage>â<lpage>981</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Tourville</surname>, <given-names>J. A.</given-names></string-name>, <string-name><given-names>K. J.</given-names> <surname>Reilly</surname></string-name> and <string-name><given-names>F. H.</given-names> <surname>Guenther</surname></string-name> (<year>2008</year>). â<article-title>Neural mechanisms underlying auditory feedback control of speech</article-title>.â <source>Neuroimage</source> <volume>39</volume>(<issue>3</issue>): <fpage>1429</fpage>â<lpage>1443</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Tumber</surname>, <given-names>A. K.</given-names></string-name>, <string-name><given-names>N. E.</given-names> <surname>Scheerer</surname></string-name> and <string-name><given-names>J. A.</given-names> <surname>Jones</surname></string-name> (<year>2014</year>). â<article-title>Attentional demands influence vocal compensations to pitch errors heard in auditory feedback</article-title>.â <source>PLoS One</source> <volume>9</volume>(<issue>10</issue>): <fpage>e109968</fpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Vossel</surname>, <given-names>S.</given-names></string-name>, <string-name><given-names>J. J.</given-names> <surname>Geng</surname></string-name> and <string-name><given-names>G. R.</given-names> <surname>Fink</surname></string-name> (<year>2014</year>). â<article-title>Dorsal and ventral attention systems: distinct neural circuits but collaborative roles</article-title>.â <source>Neuroscientist</source> <volume>20</volume>(<issue>2</issue>): <fpage>150</fpage>â<lpage>159</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname>, <given-names>S. M.</given-names></string-name>, <string-name><given-names>A. P.</given-names> <surname>Saygin</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Sereno</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Iacoboni</surname></string-name> (<year>2004</year>). â<article-title>Listening to speech activates motor areas involved in speech production</article-title>.â <source>Nat Neurosci</source> <volume>7</volume>(<issue>7</issue>): <fpage>701</fpage>â<lpage>702</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Wise</surname>, <given-names>R. J.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Greene</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Buchel</surname></string-name> and <string-name><given-names>S. K.</given-names> <surname>Scott</surname></string-name> (<year>1999</year>). â<article-title>Brain regions involved in articulation</article-title>.â <source>Lancet</source> <volume>353</volume>(<issue>9158</issue>): <fpage>1057</fpage>â<lpage>1061</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Yang</surname>, <given-names>A. I.</given-names></string-name>, <string-name><given-names>X.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>W. K.</given-names> <surname>Doyle</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Halgren</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Carlson</surname></string-name>, <string-name><given-names>T. L.</given-names> <surname>Belcher</surname></string-name>, <string-name><given-names>S. S.</given-names> <surname>Cash</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Devinsky</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Thesen</surname></string-name> (<year>2012</year>). â<article-title>Localization of dense intracranial electrode arrays using magnetic resonance imaging</article-title>.â <source>Neuroimage</source> <volume>63</volume>(<issue>1</issue>): <fpage>157</fpage>â<lpage>165</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Zarate</surname>, <given-names>J. M.</given-names></string-name>, <string-name><given-names>S.</given-names> <surname>Wood</surname></string-name> and <string-name><given-names>R. J.</given-names> <surname>Zatorre</surname></string-name> (<year>2010</year>). â<article-title>Neural networks involved in voluntary and involuntary vocal pitch regulation in experienced singers</article-title>.â <source>Neuropsychologia</source> <volume>48</volume>(<issue>2</issue>): <fpage>607</fpage>â<lpage>618</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Ashburner</surname>, <given-names>J</given-names></string-name>. (<year>2007</year>). â<article-title>A fast diffeomorphic image registration algorithm</article-title>.â <source>Neuroimage</source> <volume>38</volume>(<issue>1</issue>): <fpage>95</fpage>â<lpage>113</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>Behroozmand</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>L.</given-names> <surname>Karvelis</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Liu</surname></string-name> and <string-name><given-names>C. R.</given-names> <surname>Larson</surname></string-name> (<year>2009</year>). â<article-title>Vocalization-induced enhancement of the auditory cortex responsiveness during voice F0 feedback perturbation</article-title>.â <source>Clin Neurophysiol</source> <volume>120</volume>(<issue>7</issue>): <fpage>1303</fpage>â<lpage>1312</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Behroozmand</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>R.</given-names> <surname>Shebek</surname></string-name>, <string-name><given-names>D. R.</given-names> <surname>Hansen</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Oya</surname></string-name>, <string-name><given-names>D. A.</given-names> <surname>Robin</surname></string-name>, <string-name><given-names>M. A.</given-names> <surname>Howard</surname>, <suffix>3rd</suffix></string-name> and <string-name><given-names>J. D.</given-names> <surname>Greenlee</surname></string-name> (<year>2015</year>). â<article-title>Sensory-motor networks involved in speech production and motor control: an fMRI study</article-title>.â <source>Neuroimage</source> <volume>109</volume>: <fpage>418</fpage>â<lpage>428</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><surname>Boersma</surname>, <given-names>P</given-names></string-name>. (<year>2001</year>). â<article-title>Praat, a system for doing phonetics by computer</article-title>..â <source>Glot International</source> <volume>5</volume>:<issue>9/10</issue>: <fpage>341</fpage>â<lpage>345</lpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><string-name><surname>Burnett</surname>, <given-names>T. A.</given-names></string-name> and <string-name><given-names>C. R.</given-names> <surname>Larson</surname></string-name> (<year>2002</year>). â<article-title>Early pitch-shift response is active in both steady and dynamic voice pitch control</article-title>.â <source>The Journal of the Acoustical Society of America</source> <volume>112</volume>(<issue>3</issue>): <fpage>1058</fpage>â<lpage>1063</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>Cardin</surname>, <given-names>J. A.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Carlen</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Meletis</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Knoblich</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Deisseroth</surname></string-name>, <string-name><given-names>L. H.</given-names> <surname>Tsai</surname></string-name> and <string-name><given-names>C. I.</given-names> <surname>Moore</surname></string-name> (<year>2009</year>). â<article-title>Driving fast-spiking cells induces gamma rhythm and controls sensory responses</article-title>.â <source>Nature</source> <volume>459</volume>(<issue>7247</issue>): <fpage>663</fpage>â<lpage>667</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Chang</surname>, <given-names>E. F.</given-names></string-name>, <string-name><given-names>C. A.</given-names> <surname>Niziolek</surname></string-name>, <string-name><given-names>R. T.</given-names> <surname>Knight</surname></string-name>, <string-name><given-names>S. S.</given-names> <surname>Nagarajan</surname></string-name> and <string-name><given-names>J. F.</given-names> <surname>Houde</surname></string-name> (<year>2013</year>). â<article-title>Human cortical sensorimotor network underlying feedback control of vocal pitch</article-title>.â <source>Proc Natl Acad Sci U S A</source> <volume>110</volume>(<issue>7</issue>): <fpage>2653</fpage>â<lpage>2658</lpage>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><string-name><surname>Cheung</surname>, <given-names>C.</given-names></string-name>, <string-name><given-names>L. S.</given-names> <surname>Hamiton</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Johnson</surname></string-name> and <string-name><given-names>E. F.</given-names> <surname>Chang</surname></string-name> (<year>2016</year>). â<article-title>The auditory representation of speech sounds in human motor cortex</article-title>.â <source>eLife</source> <volume>5</volume>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><string-name><surname>Christoffels</surname>, <given-names>I. K.</given-names></string-name>, <string-name><given-names>E.</given-names> <surname>Formisano</surname></string-name> and <string-name><given-names>N. O.</given-names> <surname>Schiller</surname></string-name> (<year>2007</year>). â<article-title>Neural correlates of verbal feedback processing: an fMRI study employing overt speech</article-title>.â <source>Hum Brain Mapp</source> <volume>28</volume>(<issue>9</issue>): <fpage>868</fpage>â<lpage>879</lpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><string-name><surname>Cogan</surname>, <given-names>G. B.</given-names></string-name>, <string-name><given-names>T.</given-names> <surname>Thesen</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Carlson</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Doyle</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Devinsky</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Pesaran</surname></string-name> (<year>2014</year>). â<article-title>Sensory-motor transformations for speech occur bilaterally</article-title>.â <source>Nature</source> <volume>507</volume>(<issue>7490</issue>): <fpage>94</fpage>â<lpage>98</lpage>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><string-name><surname>Crapse</surname>, <given-names>T. B.</given-names></string-name> and <string-name><given-names>M. A.</given-names> <surname>Sommer</surname></string-name> (<year>2008</year>). â<article-title>Corollary discharge across the animal kingdom</article-title>.â <source>Nature Reviews Neuroscience</source> <volume>9</volume>(<issue>8</issue>): <fpage>587</fpage>â<lpage>600</lpage>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><string-name><surname>Creutzfeldt</surname>, <given-names>O.</given-names></string-name> and <string-name><given-names>G.</given-names> <surname>Ojemann</surname></string-name> (<year>1989</year>). â<article-title>Neuronal activity in the human lateral temporal lobe. III. Activity changes during music</article-title>.â <source>Exp Brain Res</source> <volume>77</volume>(<issue>3</issue>): <fpage>490</fpage>â<lpage>498</lpage>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><string-name><surname>Crone</surname>, <given-names>N. E.</given-names></string-name>, <string-name><given-names>A.</given-names> <surname>Sinai</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Korzeniewska</surname></string-name> (<year>2006</year>). â<article-title>High-frequency gamma oscillations and human brain mapping with electrocorticography</article-title>.â <source>Prog Brain Res</source> <volume>159</volume>: <fpage>275</fpage>â<lpage>295</lpage>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><string-name><surname>Curio</surname>, <given-names>G.</given-names></string-name>, <string-name><given-names>G.</given-names> <surname>Neuloh</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Numminen</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Jousmaki</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Hari</surname></string-name> (<year>2000</year>). â<article-title>Speaking modifies voice-evoked activity in the human auditory cortex</article-title>.â <source>Hum Brain Mapp</source> <volume>9</volume>(<issue>4</issue>): <fpage>183</fpage>â<lpage>191</lpage>.</mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><string-name><surname>Eliades</surname>, <given-names>S. J.</given-names></string-name> and <string-name><given-names>X.</given-names> <surname>Wang</surname></string-name> (<year>2003</year>). â<article-title>Sensory-motor interaction in the primate auditory cortex during self-initiated vocalizations</article-title>.â <source>Journal of neurophysiology</source> <volume>89</volume>(<issue>4</issue>): <fpage>2194</fpage>â<lpage>2207</lpage>.</mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><string-name><surname>Eliades</surname>, <given-names>S. J.</given-names></string-name> and <string-name><given-names>X.</given-names> <surname>Wang</surname></string-name> (<year>2008</year>). â<article-title>Neural substrates of vocalization feedback monitoring in primate auditory cortex</article-title>.â <source>Nature</source> <volume>453</volume>(<issue>7198</issue>): <fpage>1102</fpage>â<lpage>1106</lpage>.</mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><string-name><surname>Fairbanks</surname>, <given-names>G</given-names></string-name>. (<year>1955</year>). â<article-title>Selective vocal effects of delayed auditory feedback</article-title>.â <source>Journal of speech and hearing disorders</source> <volume>20</volume>(<issue>4</issue>): <fpage>333</fpage>â<lpage>346</lpage>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><string-name><surname>Flinker</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>E. F.</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>H. E.</given-names> <surname>Kirsch</surname></string-name>, <string-name><given-names>N. M.</given-names> <surname>Barbaro</surname></string-name>, <string-name><given-names>N. E.</given-names> <surname>Crone</surname></string-name> and <string-name><given-names>R. T.</given-names> <surname>Knight</surname></string-name> (<year>2010</year>). â<article-title>Single-trial speech suppression of auditory cortex activity in humans</article-title>.â <source>Journal of Neuroscience</source> <volume>30</volume>(<issue>49</issue>): <fpage>16643</fpage>â<lpage>16650</lpage>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><string-name><surname>Ford</surname>, <given-names>J. M.</given-names></string-name>, <string-name><given-names>B. J.</given-names> <surname>Roach</surname></string-name> and <string-name><given-names>D. H.</given-names> <surname>Mathalon</surname></string-name> (<year>2010</year>). â<article-title>Assessing corollary discharge in humans using noninvasive neurophysiological methods</article-title>.â <source>Nat Protoc</source> <volume>5</volume>(<issue>6</issue>): <fpage>1160</fpage>â<lpage>1168</lpage>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><string-name><surname>Golumbic</surname>, <given-names>E. M. Z.</given-names></string-name>, <string-name><given-names>N.</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Bickel</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Lakatos</surname></string-name>, <string-name><given-names>C. A.</given-names> <surname>Schevon</surname></string-name>, <string-name><given-names>G. M.</given-names> <surname>McKhann</surname></string-name>, <string-name><given-names>R. R.</given-names> <surname>Goodman</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Emerson</surname></string-name>, <string-name><given-names>A. D.</given-names> <surname>Mehta</surname></string-name> and <string-name><given-names>J. Z.</given-names> <surname>Simon</surname></string-name> (<year>2013</year>). â<article-title>Mechanisms underlying selective neuronal tracking of attended speech at a âcocktail partyâ</article-title>.â <source>Neuron</source> <volume>77</volume>(<issue>5</issue>): <fpage>980</fpage>â<lpage>991</lpage>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><string-name><surname>Greenlee</surname>, <given-names>J. D.</given-names></string-name>, <string-name><given-names>R.</given-names> <surname>Behroozmand</surname></string-name>, <string-name><given-names>C. R.</given-names> <surname>Larson</surname></string-name>, <string-name><given-names>A. W.</given-names> <surname>Jackson</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>D. R.</given-names> <surname>Hansen</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Oya</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Kawasaki</surname></string-name> and <string-name><given-names>M. A.</given-names> <surname>Howard</surname>, <suffix>3rd</suffix></string-name> (<year>2013</year>). â<article-title>Sensory-motor interactions for vocal pitch monitoring in non-primary human auditory cortex</article-title>.â <source>PLoS One</source> <volume>8</volume>(<issue>4</issue>): <fpage>e60783</fpage>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><string-name><surname>Greenlee</surname>, <given-names>J. D.</given-names></string-name>, <string-name><given-names>A. W.</given-names> <surname>Jackson</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>C. R.</given-names> <surname>Larson</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Oya</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Kawasaki</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Chen</surname></string-name> and <string-name><given-names>M. A.</given-names> <surname>Howard</surname> <suffix>III</suffix></string-name> (<year>2011</year>). â<article-title>Human auditory cortical activation during self-vocalization</article-title>.â <source>PloS one</source> <volume>6</volume>(<issue>3</issue>): <fpage>e14744</fpage>.</mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, <string-name><given-names>R.</given-names> <surname>Henson</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Martin</surname></string-name> (<year>2006</year>). â<article-title>Repetition and the brain: neural models of stimulus-specific effects</article-title>.â <source>Trends Cogn Sci</source> <volume>10</volume>(<issue>1</issue>): <fpage>14</fpage>â<lpage>23</lpage>.</mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><string-name><surname>Hain</surname>, <given-names>T. C.</given-names></string-name>, <string-name><given-names>T. A.</given-names> <surname>Burnett</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kiran</surname></string-name>, <string-name><given-names>C. R.</given-names> <surname>Larson</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Singh</surname></string-name> and <string-name><given-names>M. K.</given-names> <surname>Kenney</surname></string-name> (<year>2000</year>). â<article-title>Instructing subjects to make a voluntary response reveals the presence of two components to the audio-vocal reflex</article-title>.â <source>Experimental Brain Research</source> <volume>130</volume>(<issue>2</issue>): <fpage>133</fpage>â<lpage>141</lpage>.</mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><string-name><surname>Hashimoto</surname>, <given-names>Y.</given-names></string-name> and <string-name><given-names>K. L.</given-names> <surname>Sakai</surname></string-name> (<year>2003</year>). â<article-title>Brain activations during conscious self-monitoring of speech production with delayed auditory feedback: an fMRI study</article-title>.â <source>Hum Brain Mapp</source> <volume>20</volume>(<issue>1</issue>): <fpage>22</fpage>â<lpage>28</lpage>.</mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><string-name><surname>Hickok</surname>, <given-names>G</given-names></string-name>. (<year>2012</year>). â<article-title>Computational neuroanatomy of speech production</article-title>.â <source>Nat Rev Neurosci</source> <volume>13</volume>(<issue>2</issue>): <fpage>135</fpage>â<lpage>145</lpage>.</mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Houde</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Rong</surname></string-name> (<year>2011</year>). â<article-title>Sensorimotor integration in speech processing: computational basis and neural organization</article-title>.â <source>Neuron</source> <volume>69</volume>(<issue>3</issue>): <fpage>407</fpage>â<lpage>422</lpage>.</mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name> and <string-name><given-names>D.</given-names> <surname>Poeppel</surname></string-name> (<year>2000</year>). â<article-title>Towards a functional neuroanatomy of speech perception</article-title>.â <source>Trends Cogn Sci</source> <volume>4</volume>(<issue>4</issue>): <fpage>131</fpage>â<lpage>138</lpage>.</mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><string-name><surname>Hillyard</surname>, <given-names>S. A.</given-names></string-name>, <string-name><given-names>E. K.</given-names> <surname>Vogel</surname></string-name> and <string-name><given-names>S. J.</given-names> <surname>Luck</surname></string-name> (<year>1998</year>). â<article-title>Sensory gain control (amplification) as a mechanism of selective attention: electrophysiological and neuroimaging evidence</article-title>.â <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>353</volume>(<issue>1373</issue>): <fpage>1257</fpage>â<lpage>1270</lpage>.</mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><string-name><surname>Houde</surname>, <given-names>J. F.</given-names></string-name> and <string-name><given-names>M. I.</given-names> <surname>Jordan</surname></string-name> (<year>1998</year>). â<article-title>Sensorimotor adaptation in speech production</article-title>.â <source>Science</source> <volume>279</volume>(<issue>5354</issue>): <fpage>1213</fpage>â<lpage>1216</lpage>.</mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><string-name><surname>Houde</surname>, <given-names>J. F.</given-names></string-name> and <string-name><given-names>S. S.</given-names> <surname>Nagarajan</surname></string-name> (<year>2011</year>). â<article-title>Speech production as state feedback control</article-title>.â <source>Frontiers in human neuroscience</source> <volume>5</volume>: <fpage>82</fpage>.</mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><string-name><surname>Houde</surname>, <given-names>J. F.</given-names></string-name>, <string-name><given-names>S. S.</given-names> <surname>Nagarajan</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Sekihara</surname></string-name> and <string-name><given-names>M. M.</given-names> <surname>Merzenich</surname></string-name> (<year>2002</year>). â<article-title>Modulation of the auditory cortex during speech: an MEG study</article-title>.â <source>J Cogn Neurosci</source> <volume>14</volume>(<issue>8</issue>): <fpage>1125</fpage>â<lpage>1138</lpage>.</mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><string-name><surname>Howell</surname>, <given-names>P.</given-names></string-name> and <string-name><given-names>A.</given-names> <surname>Archer</surname></string-name> (<year>1984</year>). â<article-title>Susceptibility to the effects of delayed auditory feedback</article-title>.â <source>Percept Psychophys</source> <volume>36</volume>(<issue>3</issue>): <fpage>296</fpage>â<lpage>302</lpage>.</mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="journal"><string-name><surname>Hu</surname>, <given-names>H.</given-names></string-name>, <string-name><given-names>Y.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Guo</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Chen</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Liu</surname></string-name> (<year>2015</year>). â<article-title>Attention modulates cortical processing of pitch feedback errors in voice control</article-title>.â <source>Scientific Reports</source> <volume>5</volume>(<issue>1</issue>): <fpage>1</fpage>â<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="journal"><string-name><surname>Jones</surname>, <given-names>J. A.</given-names></string-name> and <string-name><given-names>K. G.</given-names> <surname>Munhall</surname></string-name> (<year>2000</year>). â<article-title>Perceptual calibration of F0 production: evidence from feedback perturbation</article-title>.â <source>J Acoust Soc Am</source> <volume>108</volume>(<issue>3 Pt 1</issue>): <fpage>1246</fpage>â<lpage>1251</lpage>.</mixed-citation></ref>
<ref id="c99"><mixed-citation publication-type="journal"><string-name><surname>Keough</surname>, <given-names>D.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Hawco</surname></string-name> and <string-name><given-names>J. A.</given-names> <surname>Jones</surname></string-name> (<year>2013</year>). â<article-title>Auditory-motor adaptation to frequency-altered auditory feedback occurs when participants ignore feedback</article-title>.â <source>BMC Neuroscience</source> <volume>14</volume>(<issue>1</issue>): <fpage>25</fpage>.</mixed-citation></ref>
<ref id="c100"><mixed-citation publication-type="journal"><string-name><surname>Kort</surname>, <given-names>N. S.</given-names></string-name>, <string-name><given-names>S. S.</given-names> <surname>Nagarajan</surname></string-name> and <string-name><given-names>J. F.</given-names> <surname>Houde</surname></string-name> (<year>2014</year>). â<article-title>A bilateral cortical network responds to pitch perturbations in speech feedback</article-title>.â <source>Neuroimage</source> <volume>86</volume>: <fpage>525</fpage>â<lpage>535</lpage>.</mixed-citation></ref>
<ref id="c101"><mixed-citation publication-type="journal"><string-name><surname>Lachaux</surname>, <given-names>J. P.</given-names></string-name>, <string-name><given-names>N.</given-names> <surname>Axmacher</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Mormann</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Halgren</surname></string-name> and <string-name><given-names>N. E.</given-names> <surname>Crone</surname></string-name> (<year>2012</year>). â<article-title>High-frequency neural activity and human cognition: past, present and possible future of intracranial EEG research</article-title>.â <source>Prog Neurobiol</source> <volume>98</volume>(<issue>3</issue>): <fpage>279</fpage>â<lpage>301</lpage>.</mixed-citation></ref>
<ref id="c102"><mixed-citation publication-type="journal"><string-name><surname>Lee</surname>, <given-names>B. S</given-names></string-name>. (<year>1950</year>). â<article-title>Effects of delayed speech feedback</article-title>.â <source>The Journal of the Acoustical Society of America</source> <volume>22</volume>(<issue>6</issue>): <fpage>824</fpage>â<lpage>826</lpage>.</mixed-citation></ref>
<ref id="c103"><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><given-names>H.</given-names> <surname>Fan</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Jones</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Zhang</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Liu</surname></string-name> (<year>2018</year>). â<article-title>Auditory-motor control of vocal production during divided attention: behavioral and ERP correlates</article-title>.â <source>Frontiers in Neuroscience</source> <volume>12</volume>: <fpage>113</fpage>.</mixed-citation></ref>
<ref id="c104"><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><given-names>H.</given-names> <surname>Hu</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Jones</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Guo</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Liu</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Liu</surname></string-name> (<year>2015</year>). â<article-title>Selective and divided attention modulates auditory-vocal integration in the processing of pitch feedback errors</article-title>.â <source>Eur J Neurosci</source> <volume>42</volume>(<issue>3</issue>): <fpage>1895</fpage>â<lpage>1904</lpage>.</mixed-citation></ref>
<ref id="c105"><mixed-citation publication-type="journal"><string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name> and <string-name><given-names>E. F.</given-names> <surname>Chang</surname></string-name> (<year>2012</year>). â<article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title>.â <source>Nature</source> <volume>485</volume>(<issue>7397</issue>): <fpage>233</fpage>â<lpage>236</lpage>.</mixed-citation></ref>
<ref id="c106"><mixed-citation publication-type="journal"><string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Cheung</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Johnson</surname></string-name> and <string-name><given-names>E. F.</given-names> <surname>Chang</surname></string-name> (<year>2014</year>). â<article-title>Phonetic feature encoding in human superior temporal gyrus</article-title>.â <source>Science</source> <volume>343</volume>(<issue>6174</issue>): <fpage>1006</fpage>â<lpage>1010</lpage>.</mixed-citation></ref>
<ref id="c107"><mixed-citation publication-type="journal"><string-name><surname>Mukamel</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>H.</given-names> <surname>Gelbard</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Arieli</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Hasson</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Fried</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Malach</surname></string-name> (<year>2005</year>). â<article-title>Coupling between neuronal firing, field potentials, and FMRI in human auditory cortex</article-title>.â <source>Science</source> <volume>309</volume>(<issue>5736</issue>): <fpage>951</fpage>â<lpage>954</lpage>.</mixed-citation></ref>
<ref id="c108"><mixed-citation publication-type="journal"><string-name><surname>Munhall</surname>, <given-names>K. G.</given-names></string-name>, <string-name><given-names>E. N.</given-names> <surname>MacDonald</surname></string-name>, <string-name><given-names>S. K.</given-names> <surname>Byrne</surname></string-name> and <string-name><given-names>I.</given-names> <surname>Johnsrude</surname></string-name> (<year>2009</year>). â<article-title>Talkers alter vowel production in response to real-time formant perturbation even when instructed not to compensate</article-title>.â <source>The Journal of the Acoustical Society of America</source> <volume>125</volume>(<issue>1</issue>): <fpage>384</fpage>â<lpage>390</lpage>.</mixed-citation></ref>
<ref id="c109"><mixed-citation publication-type="journal"><string-name><surname>Niziolek</surname>, <given-names>C. A.</given-names></string-name> and <string-name><given-names>F. H.</given-names> <surname>Guenther</surname></string-name> (<year>2013</year>). â<article-title>Vowel category boundaries enhance cortical and behavioral responses to speech feedback alterations</article-title>.â <source>J Neurosci</source> <volume>33</volume>(<issue>29</issue>): <fpage>12090</fpage>â<lpage>12098</lpage>.</mixed-citation></ref>
<ref id="c110"><mixed-citation publication-type="journal"><string-name><surname>Niziolek</surname>, <given-names>C. A.</given-names></string-name>, <string-name><given-names>S. S.</given-names> <surname>Nagarajan</surname></string-name> and <string-name><given-names>J. F.</given-names> <surname>Houde</surname></string-name> (<year>2013</year>). â<article-title>What does motor efference copy represent? Evidence from speech production</article-title>.â <source>Journal of Neuroscience</source> <volume>33</volume>(<issue>41</issue>): <fpage>16110</fpage>â<lpage>16116</lpage>.</mixed-citation></ref>
<ref id="c111"><mixed-citation publication-type="journal"><string-name><surname>Nourski</surname>, <given-names>K. V.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Steinschneider</surname></string-name> and <string-name><given-names>A. E.</given-names> <surname>Rhone</surname></string-name> (<year>2016</year>). â<article-title>Electrocorticographic Activation within Human Auditory Cortex during Dialog-Based Language and Cognitive Testing</article-title>.â <source>Front Hum Neurosci</source> <volume>10</volume>: <fpage>202</fpage>.</mixed-citation></ref>
<ref id="c112"><mixed-citation publication-type="journal"><string-name><surname>Nourski</surname>, <given-names>K. V.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Steinschneider</surname></string-name>, <string-name><given-names>A. E.</given-names> <surname>Rhone</surname></string-name>, <string-name><given-names>C. K.</given-names> <surname>Kovach</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Banks</surname></string-name>, <string-name><given-names>B. M.</given-names> <surname>Krause</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Kawasaki</surname></string-name> and <string-name><given-names>M. A.</given-names> <surname>Howard</surname></string-name> (<year>2021</year>). â<article-title>Electrophysiology of the Human Superior Temporal Sulcus during Speech Processing</article-title>.â <source>Cereb Cortex</source> <volume>31</volume>(<issue>2</issue>): <fpage>1131</fpage>â<lpage>1148</lpage>.</mixed-citation></ref>
<ref id="c113"><mixed-citation publication-type="journal"><string-name><surname>Numminen</surname>, <given-names>J.</given-names></string-name>, <string-name><given-names>R.</given-names> <surname>Salmelin</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Hari</surname></string-name> (<year>1999</year>). â<article-title>Subjectâs own speech reduces reactivity of the human auditory cortex</article-title>.â <source>Neurosci Lett</source> <volume>265</volume>(<issue>2</issue>): <fpage>119</fpage>â<lpage>122</lpage>.</mixed-citation></ref>
<ref id="c114"><mixed-citation publication-type="journal"><string-name><surname>Ozker</surname>, <given-names>M.</given-names></string-name>, <string-name><given-names>W.</given-names> <surname>Doyle</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Devinsky</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Flinker</surname></string-name> (<year>2022</year>). â<article-title>A cortical network processes auditory error signals during human speech production to maintain fluency</article-title>.â <source>PLoS Biol</source> <volume>20</volume>(<issue>2</issue>): <fpage>e3001493</fpage>.</mixed-citation></ref>
<ref id="c115"><mixed-citation publication-type="book"><string-name><surname>Percival</surname>, <given-names>D. B.</given-names></string-name> and <string-name><given-names>A. T.</given-names> <surname>Walden</surname></string-name> (<year>1993</year>). <source>Spectral analysis for physical applications</source>, <publisher-name>cambridge university press</publisher-name>.</mixed-citation></ref>
<ref id="c116"><mixed-citation publication-type="journal"><string-name><surname>Poulet</surname>, <given-names>J. F.</given-names></string-name> and <string-name><given-names>B.</given-names> <surname>Hedwig</surname></string-name> (<year>2002</year>). â<article-title>A corollary discharge maintains auditory sensitivity during sound production</article-title>.â <source>Nature</source> <volume>418</volume>(<issue>6900</issue>): <fpage>872</fpage>â<lpage>876</lpage>.</mixed-citation></ref>
<ref id="c117"><mixed-citation publication-type="journal"><string-name><surname>Poulet</surname>, <given-names>J. F.</given-names></string-name> and <string-name><given-names>B.</given-names> <surname>Hedwig</surname></string-name> (<year>2006</year>). â<article-title>The cellular basis of a corollary discharge</article-title>.â <source>Science</source> <volume>311</volume>(<issue>5760</issue>): <fpage>518</fpage>â<lpage>522</lpage>.</mixed-citation></ref>
<ref id="c118"><mixed-citation publication-type="journal"><string-name><surname>Pulvermuller</surname>, <given-names>F.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Huss</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Kherif</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Moscoso del Prado Martin</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Hauk</surname></string-name> and <string-name><given-names>Y.</given-names> <surname>Shtyrov</surname></string-name> (<year>2006</year>). â<article-title>Motor cortex maps articulatory features of speech sounds</article-title>.â <source>Proc Natl Acad Sci U S A</source> <volume>103</volume>(<issue>20</issue>): <fpage>7865</fpage>â<lpage>7870</lpage>.</mixed-citation></ref>
<ref id="c119"><mixed-citation publication-type="journal"><string-name><surname>Ray</surname>, <given-names>S.</given-names></string-name> and <string-name><given-names>J. H.</given-names> <surname>Maunsell</surname></string-name> (<year>2011</year>). â<article-title>Different origins of gamma rhythm and high-gamma activity in macaque visual cortex</article-title>.â <source>PLoS Biol</source> <volume>9</volume>(<issue>4</issue>): <fpage>e1000610</fpage>.</mixed-citation></ref>
<ref id="c120"><mixed-citation publication-type="journal"><string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name> and <string-name><given-names>G.</given-names> <surname>Pourtois</surname></string-name> (<year>2004</year>). â<article-title>Revisiting Snodgrass and Vanderwartâs Object Pictorial Set: The Role of Surface Detail in Basic-Level Object Recognition</article-title>.â <source>Perception</source> <volume>33</volume>(<issue>2</issue>): <fpage>217</fpage>â<lpage>236</lpage>.</mixed-citation></ref>
<ref id="c121"><mixed-citation publication-type="journal"><string-name><surname>Schneider</surname>, <given-names>D. M.</given-names></string-name> and <string-name><given-names>R.</given-names> <surname>Mooney</surname></string-name> (<year>2018</year>). â<article-title>How movement modulates hearing</article-title>.â <source>Annual review of neuroscience</source> <volume>41</volume>: <fpage>553</fpage>â<lpage>572</lpage>.</mixed-citation></ref>
<ref id="c122"><mixed-citation publication-type="journal"><string-name><surname>Shum</surname>, <given-names>J.</given-names></string-name>, <string-name><given-names>L.</given-names> <surname>Fanda</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Dugan</surname></string-name>, <string-name><given-names>W. K.</given-names> <surname>Doyle</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Devinsky</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Flinker</surname></string-name> (<year>2020</year>). â<article-title>Neural correlates of sign language production revealed by electrocorticography</article-title>.â <source>Neurology</source> <volume>95</volume>(<issue>21</issue>): <fpage>e2880</fpage>â<lpage>e2889</lpage>.</mixed-citation></ref>
<ref id="c123"><mixed-citation publication-type="journal"><string-name><surname>Stuart</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Kalinowski</surname></string-name>, <string-name><given-names>M. P.</given-names> <surname>Rastatter</surname></string-name> and <string-name><given-names>K.</given-names> <surname>Lynch</surname></string-name> (<year>2002</year>). â<article-title>Effect of delayed auditory feedback on normal speakers at two speech rates</article-title>.â <source>J Acoust Soc Am</source> <volume>111</volume>(<issue>5 Pt 1</issue>): <fpage>2237</fpage>â<lpage>2241</lpage>.</mixed-citation></ref>
<ref id="c124"><mixed-citation publication-type="journal"><string-name><surname>Todorovic</surname>, <given-names>A.</given-names></string-name> and <string-name><given-names>F. P.</given-names> <surname>de Lange</surname></string-name> (<year>2012</year>). â<article-title>Repetition suppression and expectation suppression are dissociable in time in early auditory evoked fields</article-title>.â <source>J Neurosci</source> <volume>32</volume>(<issue>39</issue>): <fpage>13389</fpage>â<lpage>13395</lpage>.</mixed-citation></ref>
<ref id="c125"><mixed-citation publication-type="journal"><string-name><surname>Tourville</surname>, <given-names>J. A.</given-names></string-name> and <string-name><given-names>F. H.</given-names> <surname>Guenther</surname></string-name> (<year>2011</year>). â<article-title>The DIVA model: A neural theory of speech acquisition and production</article-title>.â <source>Lang Cogn Process</source> <volume>26</volume>(<issue>7</issue>): <fpage>952</fpage>â<lpage>981</lpage>.</mixed-citation></ref>
<ref id="c126"><mixed-citation publication-type="journal"><string-name><surname>Tourville</surname>, <given-names>J. A.</given-names></string-name>, <string-name><given-names>K. J.</given-names> <surname>Reilly</surname></string-name> and <string-name><given-names>F. H.</given-names> <surname>Guenther</surname></string-name> (<year>2008</year>). â<article-title>Neural mechanisms underlying auditory feedback control of speech</article-title>.â <source>Neuroimage</source> <volume>39</volume>(<issue>3</issue>): <fpage>1429</fpage>â<lpage>1443</lpage>.</mixed-citation></ref>
<ref id="c127"><mixed-citation publication-type="journal"><string-name><surname>Tumber</surname>, <given-names>A. K.</given-names></string-name>, <string-name><given-names>N. E.</given-names> <surname>Scheerer</surname></string-name> and <string-name><given-names>J. A.</given-names> <surname>Jones</surname></string-name> (<year>2014</year>). â<article-title>Attentional demands influence vocal compensations to pitch errors heard in auditory feedback</article-title>.â <source>PLoS One</source> <volume>9</volume>(<issue>10</issue>): <fpage>e109968</fpage>.</mixed-citation></ref>
<ref id="c128"><mixed-citation publication-type="journal"><string-name><surname>Vossel</surname>, <given-names>S.</given-names></string-name>, <string-name><given-names>J. J.</given-names> <surname>Geng</surname></string-name> and <string-name><given-names>G. R.</given-names> <surname>Fink</surname></string-name> (<year>2014</year>). â<article-title>Dorsal and ventral attention systems: distinct neural circuits but collaborative roles</article-title>.â <source>Neuroscientist</source> <volume>20</volume>(<issue>2</issue>): <fpage>150</fpage>â<lpage>159</lpage>.</mixed-citation></ref>
<ref id="c129"><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname>, <given-names>S. M.</given-names></string-name>, <string-name><given-names>A. P.</given-names> <surname>Saygin</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Sereno</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Iacoboni</surname></string-name> (<year>2004</year>). â<article-title>Listening to speech activates motor areas involved in speech production</article-title>.â <source>Nat Neurosci</source> <volume>7</volume>(<issue>7</issue>): <fpage>701</fpage>â<lpage>702</lpage>.</mixed-citation></ref>
<ref id="c130"><mixed-citation publication-type="journal"><string-name><surname>Wise</surname>, <given-names>R. J.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Greene</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Buchel</surname></string-name> and <string-name><given-names>S. K.</given-names> <surname>Scott</surname></string-name> (<year>1999</year>). â<article-title>Brain regions involved in articulation</article-title>.â <source>Lancet</source> <volume>353</volume>(<issue>9158</issue>): <fpage>1057</fpage>â<lpage>1061</lpage>.</mixed-citation></ref>
<ref id="c131"><mixed-citation publication-type="journal"><string-name><surname>Yang</surname>, <given-names>A. I.</given-names></string-name>, <string-name><given-names>X.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>W. K.</given-names> <surname>Doyle</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Halgren</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Carlson</surname></string-name>, <string-name><given-names>T. L.</given-names> <surname>Belcher</surname></string-name>, <string-name><given-names>S. S.</given-names> <surname>Cash</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Devinsky</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Thesen</surname></string-name> (<year>2012</year>). â<article-title>Localization of dense intracranial electrode arrays using magnetic resonance imaging</article-title>.â <source>Neuroimage</source> <volume>63</volume>(<issue>1</issue>): <fpage>157</fpage>â<lpage>165</lpage>.</mixed-citation></ref>
<ref id="c132"><mixed-citation publication-type="journal"><string-name><surname>Yates</surname>, <given-names>A. J</given-names></string-name>. (<year>1963</year>). â<article-title>Delayed auditory feedback</article-title>.â <source>Psychol Bull</source> <volume>60</volume>: <fpage>213</fpage>â<lpage>232</lpage>.</mixed-citation></ref>
<ref id="c133"><mixed-citation publication-type="journal"><string-name><surname>Zarate</surname>, <given-names>J. M.</given-names></string-name>, <string-name><given-names>S.</given-names> <surname>Wood</surname></string-name> and <string-name><given-names>R. J.</given-names> <surname>Zatorre</surname></string-name> (<year>2010</year>). â<article-title>Neural networks involved in voluntary and involuntary vocal pitch regulation in experienced singers</article-title>.â <source>Neuropsychologia</source> <volume>48</volume>(<issue>2</issue>): <fpage>607</fpage>â<lpage>618</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94198.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ray</surname>
<given-names>Supratim</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bengaluru</city>
<country>India</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>The manuscript describes human intracranial neural recordings in the auditory cortex during speech production, showing that the effects of delayed auditory feedback correlate with the degree of underlying speech-induced suppression. This is an <bold>important</bold> finding, as previous work has suggested that speech suppression and feedback sensitivity often do not co-localize and may be distinct processes, in contrast with findings in non-human primates where there is a strong correlation. The strength of the evidence is <bold>convincing</bold>, with appropriate experimental methods, data, and analysis.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94198.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The manuscript describes a series of experiments using human intracranial neural recordings designed to evaluate processing of self-generated speech in the setting of feedback delays. Specifically, the authors aim to address the question about the relationship between speech-induced suppression and feedback sensitivity in the auditory cortex, which, relationship has been conflicting in the literature. They found a correlation between speech suppression and feedback delay sensitivity, suggesting a common process. Additional controls were done for possible forward suppression/adaptation, as well as controlling for other confounds due to amplification, etc.</p>
<p>Strengths:</p>
<p>
The primary strength of the manuscript is the use of human intracranial recording, which is a valuable resource and gives better spatial and temporal resolution than many other approaches. The use of delayed auditory feedback is also novel and has seen less attention than other forms of shifted feedback during vocalization. Analyses are robust and include demonstrating a scaling of neural activity with the degree of feedback delay, more robust evidence for error encoding than simply using a single feedback perturbation.</p>
<p>Weaknesses:</p>
<p>
Some of the analyses performed differ from those used in past work, which limits the ability to directly compare the results. Notably, past work has compared feedback effects between production and listening, which was not done here. There were also some unusual effects in the data, such as increased activity with no feedback delay when wearing headphones, that the authors attempted to control for with additional experiments, but remain unclear. Confounds by behavioral results of delayed feedback are also unclear.</p>
<p>Overall the work is well done and clearly explained. The manuscript addresses an area of some controversy and does so in a rigorous fashion, namely the correlation between speech-induced suppression and feedback sensitivity (or lack thereof). While the data presented overlap that collected and used for a previous paper, this is expected given the rare commodity these neural recordings represent. Contrasting these results to previous ones using pitch-shifted feedback should spawn additional discussion and research, including verification of the previous finding, looking at how the brain encodes feedback during speech over multiple acoustic dimensions, and how this information can be used in speech motor control.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94198.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
In &quot;Speech-induced suppression and vocal feedback sensitivity in human cortex&quot;, Ozker and colleagues use intracranial EEG to understand audiomotor feedback during speech production using a speech production and delayed auditory feedback task. The purpose of the paper is to understand where and how speaker induced suppression occurs, and whether this suppression might be related to feedback monitoring. First, they identified sites that showed auditory suppression during speech production using a single word auditory repetition task and a visual reading task, then observed whether and how these electrodes show sensitivity to auditory feedback using a DAF paradigm. The stimuli were single words played auditorily or shown visually and repeated or read aloud by the participant. Neural data were recorded from regular- and high-density grids from the left and right hemisphere. The main findings were:</p>
<p>
â¢ Speaker induced suppression is strongest in the STG and MTG, and enhancement is generally seen in frontal/motor areas except for small regions of interest in dorsal sensorimotor cortex and IFG, which can also show suppression.</p>
<p>
â¢ Delayed auditory feedback, even when simultaneous, induces larger response amplitudes compared to the typical auditory word repetition and visual reading tasks. The authors presume this may be due to effort and attention required to perform the DAF task.</p>
<p>
â¢ The degree of speaker induced suppression is correlated with sensitivity to delayed auditory feedback, and is strongest for ~200 ms of delayed auditory feedback.</p>
<p>
â¢ pSTG (behind TTS) is more strongly modulated by DAF than mid-anterior STG</p>
<p>Strengths:</p>
<p>
Overall, I found the manuscript to be clear, the methodology and statistics to be solid, and the findings mostly quite robust. The large number of participants with high density coverage over both the left and right lateral hemispheres allows for a greater dissection of the topography of speaker induced suppression and changes due to audiomotor feedback. The tasks were well-designed and controlled for repetition suppression and other potential caveats.</p>
<p>Weaknesses:</p>
<p>
I am happy with the changes the authors made in response to my first round of comments.</p>
<p>The authors addressed my comments relating to plotting relative to the onset of articulation in Figure 1 and also addressed whether the amount of suppression varies according to more interfering delayed auditory feedback (though the correlations between sensitivity and suppression are a little noisy, they are positive). Finally, I am also satisfied with the inclusion of more group data in Figure 4.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94198.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ozker</surname>
<given-names>Muge</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7472-4528</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Yu</surname>
<given-names>Leyao</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dugan</surname>
<given-names>Patricia</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Doyle</surname>
<given-names>Werner</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Friedman</surname>
<given-names>Daniel</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Devinsky</surname>
<given-names>Orrin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Flinker</surname>
<given-names>Adeen</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1247-1283</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authorsâ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>The manuscript describes a series of experiments using human intracranial neural recordings designed to evaluate the processing of self-generated speech in the setting of feedback delays. Specifically, the authors aim to address the question about the relationship between speech-induced suppression and feedback sensitivity in the auditory cortex, whose relationship has been conflicting in the literature. They found a correlation between speech suppression and feedback delay sensitivity, suggesting a common process. Additional controls were done for possible forward suppression/adaptation, as well as controlling for other confounds due to amplification, etc.</p>
<p>Strengths:</p>
<p>The primary strength of the manuscript is the use of human intracranial recording, which is a valuable resource and gives better spatial and temporal resolution than many other approaches. The use of delayed auditory feedback is also novel and has seen less attention than other forms of shifted feedback during vocalization. Analyses are robust, and include demonstrating a scaling of neural activity with the degree of feedback delay, and more robust evidence for error encoding than simply using a single feedback perturbation.</p>
<p>Weaknesses:</p>
<p>Some of the analyses performed differ from those used in past work, which limits the ability to directly compare the results. Notably, past work has compared feedback effects between production and listening, which was not done here. There were also some unusual effects in the data, such as increased activity with no feedback delay when wearing headphones, that the authors attempted to control for with additional experiments, but remain unclear. Confounds by behavioral results of delayed feedback are also unclear.</p>
<p>Overall the work is well done and clearly explained. The manuscript addresses an area of some controversy and does so in a rigorous fashion, namely the correlation between speech-induced suppression and feedback sensitivity (or lack thereof). While the data presented overlaps that collected and used for a previous paper, this is expected given the rare commodity these neural recordings represent. Contrasting these results to previous ones using pitch-shifted feedback should spawn additional discussion and research, including verification of the previous finding, looking at how the brain encodes feedback during speech over multiple acoustic dimensions, and how this information can be used in speech motor control.</p>
</disp-quote>
<p>We thank the reviewer for their comments and have addressed the concerns point by point in the section âRecommendation for Authorsâ.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>&quot;Speech-induced suppression and vocal feedback sensitivity in human cortex&quot;, Ozker and colleagues use intracranial EEG to understand audiomotor feedback during speech production using a speech production and delayed auditory feedback task. The purpose of the paper is to understand where and how speaker-induced suppression occurs, and whether this suppression might be related to feedback monitoring. First, they identified sites that showed auditory suppression during speech production using a single-word auditory repetition task and a visual reading task, then observed whether and how these electrodes show sensitivity to auditory feedback using a DAF paradigm. The stimuli were single words played auditorily or shown visually and repeated or read aloud by the participant. Neural data were recorded from regular- and high-density grids from the left and right hemispheres. The main findings were:</p>
<p>â¢ Speaker-induced suppression is strongest in the STG and MTG, and enhancement is generally seen in frontal/motor areas except for small regions of interest in the dorsal sensorimotor cortex and IFG, which can also show suppression.</p>
<p>
â¢ Delayed auditory feedback, even when simultaneous, induces larger response amplitudes compared to the typical auditory word repetition and visual reading tasks. The authors presume this may be due to the effort and attention required to perform the DAF task.</p>
<p>â¢ The degree of speaker-induced suppression is correlated with sensitivity to delayed auditory feedback. â¢ pSTG (behind TTS) is more strongly modulated by DAF than mid-anterior STG</p>
<p>Strengths:</p>
<p>Overall, I found the manuscript to be clear, the methodology and statistics to be solid, and the findings mostly quite robust. The large number of participants with high-density coverage over both the left and right lateral hemispheres allows for a greater dissection of the topography of speaker-induced suppression and changes due to audiomotor feedback. The tasks were well-designed and controlled for repetition suppression and other potential caveats.</p>
<p>Weaknesses:</p>
<p>(1) In Figure 1D, it would make more sense to align the results to the onset of articulation rather than the onset of the auditory or visual cue, since the point is to show that the responses during articulation are relatively similar. In this form, the more obvious difference is that there is an auditory response to the auditory stimulus, and none to the visual, which is expected, but not what I think the authors want to convey.</p>
</disp-quote>
<p>We agree with the reviewer. We have updated Figure 1 accordingly.</p>
<disp-quote content-type="editor-comment">
<p>(2) The DAF paradigm includes playing auditory feedback at 0, 50, 100, and 200 ms lag, and it is expected that some of these lags are more likely to induce dysfluencies than others. It would be helpful to include some analysis of whether the degree of suppression or enhancement varies by performance on the task, since some participants may find some lags more interfering than others.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion. In the original analysis, we calculated a Sensitivity Index for each electrode by correlating the high gamma response with the delay condition across trials. To address the reviewerâs question, we now compared delay conditions in pairs (DAF0 vs DAF50, DAF0 vs DAF100, DAF0 vs DAF200, DAF50 vs DAF100, DAF50 vs DAF200 and DAF100 vs DAF200).</p>
<p>Similar to our Suppression Index calculation, where we compared neural response to listening and speaking conditions (Listen-Speak/Listen+Speak), we now calculated the Sensitivity Index by comparing neural response to two delay conditions as follows:</p>
<p>e.g.  Sensitivity Index = (DAF50 â DAF0) / (DAF50 + DAF0). We used the raw high gamma broadband signal power instead of percent signal change to ensure that the Sensitivity Index values varied between -1 to 1.</p>
<p>As shown in the figure below, even when we break down the analysis by feedback delay, we still find a significant association between suppression and sensitivity (except for when we calculate sensitivity indices by comparing DAF50 and DAF100). Strongest correlation (Pearsonâs correlation) was found when sensitivity indices were calculated by comparing DAF0 and DAF200.</p>
<p>As the reviewer suggested, participants found DAF200 more interfering than the others and slowed down their speech the most (Articulation duration; DAF0: 0.698, DAF50: 0.726, DAF100: 0.737, and DAF200: 0.749 milliseconds; Ozker, Doyle et al. 2022).</p>
<fig id="sa3fig1">
<label>Author response image 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-94198-sa3-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>(3) Figure 3 shows data from only two electrodes from one patient. An analysis of how amplitude changes as a function of the lag across all of the participants who performed this task would be helpful to see how replicable these patterns of activity are across patients. Is sensitivity to DAF always seen as a change in amplitude, or are there ever changes in latency as well? The analysis in Figure 4 gets at which electrodes are sensitive to DAF but does not give a sense of whether the temporal profile is similar to those shown in Figure 3.</p>
</disp-quote>
<p>In Figure 4A, electrodes from all participants are color-coded to reflect the correlation between neural response amplitude and auditory feedback delay. A majority of auditory electrodes in the STG exhibit a positive correlation, indicating that response amplitude increases with increasing feedback delays. To demonstrate the replicability of the response patterns in Figure 3, here we show auditory responses averaged across 23 STG electrodes from 6 participants.</p>
<fig id="sa3fig2">
<label>Author response image 2.</label>
<graphic mime-subtype="jpg" xlink:href="elife-94198-sa3-fig2.jpg" mimetype="image"/>
</fig>
<p>Response latency in auditory regions also increases with increasing auditory feedback delays. But this delayed auditory response to delayed auditory feedback is expected. In Figure 3, signals were aligned to the perceived auditory feedback onset, therefore we donât see the latency differences. Below we replotted the same responses by aligning the signal to the onset of articulation. It is now clearer that responses are delayed as the auditory feedback delay increases. This is because participants start speaking at time=0, but they hear their voice with a lag so the response onset in these auditory regions are delayed.</p>
<p>According to models of speech production, when there is a mismatch between expected and perceived auditory feedback, the auditory cortex encodes this mismatch with an enhanced response, reflecting an error signal. Therefore, we referred to changes in response amplitude as a measure of sensitivity to DAF.</p>
<disp-quote content-type="editor-comment">
<p>(4) While the sensitivity index helps to show whether increasing amounts of feedback delay are correlated with increased response enhancement, it is not sensitive to nonlinear changes as a function of feedback delay, and it is not clear from Figure 3 or 4 whether such relationships exist. A deeper investigation into the response types observed during DAF would help to clarify whether this is truly a linear relationship, dependent on behavioral errors, or something else.</p>
</disp-quote>
<p>We compared responses to delay conditions in pairs in the analysis presented above (response #2). We hope these new results also clarifies this issue and address the reviewerâs concerns.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>Major points:</p>
<p>(1) While the correlation between SuppI and SensI is clear here (as opposed to Chang et al), it is unclear if this difference is a byproduct of how SensI was calculated (and not just different tasks). In that paper, the feedback sensitivity was calculated as a metric comparing feedback responses during production and listening, whereas here the SensI is a correlation coefficient during production only. If the data exists, it would be very helpful to also show an analysis similar to that used previously (i.e. comparing DAF effects in both production and playback, either in correlations or just the 200ms delay response). One could imagine that some differences are due to sensory properties, though it is certainly less clear what delay effects would be on listening compared to say pitch shift.</p>
</disp-quote>
<p>We thank the reviewer for pointing this out. Indeed, the calculation of SensI is different in the two studies. In Chang et al. study, SensI was calculated by comparing perturbed feedback responses during production and passive listening. This is a very meticulous approach as it controls for the acoustic properties of the auditory stimuli under both conditions.</p>
<p>In our study, we didnât have a passive listening condition. This would require recording the participantsâ voice as they were speaking with DAF and playing it back to them in a subsequent passive listening condition. Therefore, we canât completely eliminate the possibility that some differences are due to sensory properties. However, to address the reviewerâs concern, we examined the voice recordings of 8 participants for acoustic differences. Specifically, we compared voice intensities for different auditory feedback delays (0,50,100 and 200ms) and found no significant differences (F=0, p=0.091).</p>
<p>We think that the difference with the Chang et al. study is an important point to emphasize, therefore we now added in the Discussion:</p>
<p>âIn contrast, to replicate this finding in humans, a previous iEEG study by Chang et al. (Chang, Niziolek et al. 2013) used frequency-shifted feedback during vowel production and found that most suppressed auditory sites did not overlap with those sensitive to feedback alterations. Using DAF instead of frequency-shifted feedback, we demonstrated a significant overlap of two neural populations in the STG, along with a strong correlation between the degree of speech-induced suppression and sensitivity to auditory feedback. This discrepancy may be due to different methods of calculating sensitivity to altered feedback. In our study, sensitivity was determined by comparing responses to delayed and non-delayed feedback during production, whereas Chang et al. compared perturbed feedback responses during production and listening. One possibility is that our approach identifies a larger auditory neural population in the STG sensitive to altered feedback. Alternatively, it could indicate a larger population highly sensitive to temporal rather than spectral perturbations in auditory feedback. Thus, we observe a wide overlap of the two neural populations in the STG showing both speech-induced suppression and sensitivity to auditory feedback. Replaying a recording of the participants' own delayed voice back to them, which we were unable to complete in this study, would have made the results of the two studies more comparable while also completely eliminating the possibility of a sensory explanation for the observed response enhancement.â</p>
<disp-quote content-type="editor-comment">
<p>(2) I am still a bit unclear on how Experiment 4 is different than the no-delay condition in Experiment 3. Please clarify. Also, to be clear, in Experiments 1+2 the subjects were not wearing any headphones and had no additional sidetone?</p>
</disp-quote>
<p>It is correct that participants were not wearing earphones in Experiments 1&amp;2 (with no additional sidetone), and that they were wearing earphones in Experiments 3&amp;4.</p>
<p>For the âno delayâ condition in the DAF experiment (Experiment 3), participants were wearing earphones and reading words with simultaneous auditory feedback. So, this condition was equivalent to visual word reading (Experiment 2), except participants were wearing earphones. Yet, neural responses were much larger for the âno delayâ condition in the DAF experiment compared to visual word reading.</p>
<p>We suspected that larger neural responses in the DAF experiment were caused by hearing auditory feedback through earphones. To test and control for this possibility, in a subset of participants, we ran an additional visual word reading experiment (Experiment 4) with earphones and used the same volume settings as in the DAF experiment. We found that response magnitudes were now similar in the two experiments (Experiment 3 and 4) and earphones (with the associated increased sound amplitude) were indeed the reason for larger neural responses. Thus, Experiment 4 differs from the no-delay condition in Experiment 3 only in the stimuli read aloud.</p>
<disp-quote content-type="editor-comment">
<p>(3) In Figure 3, why is the DAF200 condition activity so much bigger than the other conditions, even prior to the DAF onset? I worry this might bias the rest of the response differences.</p>
</disp-quote>
<p>In Figure 3B and 3D, time=0 indicates the onset of the perceived auditory feedback. Below we replotted the responses in the same two electrodes but now time=0 indicates the onset of articulation. We see that the peaking time of the responses are delayed as the auditory feedback delay increases. This is because participants start speaking at time=0, but they hear their voice with a lag so the response onset in these auditory regions are delayed. However, like the reviewer pointed out, the response for the DAF200 condition in Electrode G54 is slightly larger even at the very beginning. We think that this small, early response might reflect a response to the bone-conducted auditory feedback, which might be more prominent for the DAF200 condition. Nevertheless, we still see that response amplitude increase with increasing feedback delays in Electrode 63.</p>
<disp-quote content-type="editor-comment">
<p>(4) Figure 4C, are the labeled recording sites limited to those with significant DAF and/or suppression?</p>
</disp-quote>
<p>In Figure 4C, we show electrodes that had significant high-gamma broadband responses during all tasks. We write in the Methods: âElectrodes that showed significant response increase (p &lt; 10â4) either before (â0.5 to 0 s) or after speech onset (0 to 0.5 s) with respect to a baseline period (â1 to â0.6 s) and at the same time had a large signal-to-noise ratio (Î¼/Ï &gt; 0.7) during either of these time windows were selected. Electrode selection was first performed for each task separately, then electrodes that were commonly selected were further analyzed.â</p>
<disp-quote content-type="editor-comment">
<p>(5) Were there any analyses done to control for the effects of vocal changes on the DAF neural responses? The authors' previous paper did note a behavioral effect. This is probably not trivial, as we may not know the 'onset time' of the response, in contrast to pitch shift where it is more regular. If the timing is unknown, one thing that could be tried is to only look early in DAF responses (first 50ms say) to make sure the DAF effects hold.</p>
</disp-quote>
<p>DAF involves two different perturbations: the absence of feedback at speech onset and the introduction of delayed feedback during playback. The timing of the behavioral effect in response to these two perturbations remains unclear. Aligning the neural responses to the production onset and examining the first 50ms would only capture the response to the acoustic feedback for the no-delay condition within that time window. Conversely, aligning the responses to the playback onset might miss the onset of the behavioral effect, which likely starts earlier as a response to the lack of feedback. We acknowledge the reviewer's point that this is a limitation of the DAF paradigm, and the behavioral effect is not as straightforward as that of pitch perturbation. However, we believe there is no clear solution to this issue.</p>
<disp-quote content-type="editor-comment">
<p>Minor points:</p>
<p>(1) Figure 3, it might be nice to show the SuppI and SensI on the plots to give the reader a better sense of what those values look like.</p>
</disp-quote>
<p>We included SuppI and SensI values in the new version of Figure 3.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>Minor Comments:</p>
<p>(1) In Figure 1, it is unclear whether the responses shown in B-D correspond to the ROIs shown in Figure A - I am guessing so, but the alignment of the labels makes this slightly unclear, so I suggest these be relabeled somehow for clarity.</p>
</disp-quote>
<p>This is fixed in the updated version of Figure 1.</p>
<disp-quote content-type="editor-comment">
<p>(2) In Figure 1D the difference in colors between AWR and VWR is difficult to appreciate - I suggest using two contrasting colors.</p>
</disp-quote>
<p>This is fixed in the updated version of Figure 1.</p>
<disp-quote content-type="editor-comment">
<p>(3) Please add y-axis labels for Fig 3B-D. (I believe these are % signal change, but it would be clearer if the label were included).</p>
</disp-quote>
<p>This is fixed in the updated version of Figure 3.</p>
<disp-quote content-type="editor-comment">
<p>(4) Can the authors comment on whether the use of speakers for AWR and VWR versus earphones for DAF and VWF- AF may have had an influence on the increased response in this condition? If the AWR were rerun using the headphone setup, or if DAF with 0 ms feedback were run with no other trials including lags, would the large differences in response amplitude be observed?</p>
</disp-quote>
<p>Participants were not wearing earphones in Experiments 1&amp;2, and that they were wearing earphones in Experiments 3&amp;4.</p>
<p>For the âno delayâ condition in the DAF experiment (Experiment 3), participants were wearing earphones and reading words with simultaneous auditory feedback. So, this condition was equivalent to VWR (Experiment 2), except participants were wearing earphones. Yet, neural responses were much larger for the âno delayâ condition in the DAF experiment compared to VWR.</p>
<p>Supporting the reviewerâs concerns, we suspected that larger neural responses in the DAF experiment were caused by hearing auditory feedback through earphones. To test and control for this possibility, in a subset of participants, we ran the VWR-AF experiment (Experiment 4) with earphones and used the same volume settings as in the DAF experiment. We found that response magnitudes were now similar in the two experiments (Experiment 3 and 4) and earphones were indeed the reason for larger neural responses.</p>
<disp-quote content-type="editor-comment">
<p>(5) No data or code were available, I did not see any statement about this nor any github link or OSF link to share their data and/or code.</p>
</disp-quote>
<p>Data is available in the Github repository: flinkerlab/Sensitivity-Suppression</p>
</body>
</sub-article>
</article>