<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">103512</article-id>
<article-id pub-id-type="doi">10.7554/eLife.103512</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103512.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Barcode activity in a recurrent network model of the hippocampus enables efficient memory binding</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3653-0057</contrib-id>
<name>
<surname>Fang</surname>
<given-names>Ching</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Lindsey</surname>
<given-names>Jack</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<xref ref-type="author-notes" rid="n2">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Abbott</surname>
<given-names>Larry F</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Aronov</surname>
<given-names>Dmitriy</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2045-3747</contrib-id>
<name>
<surname>Chettih</surname>
<given-names>Selmaan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>sc4551@columbia.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Zuckerman Mind Brain Behavior Institute, Columbia University</institution></institution-wrap>, <city>New York</city>, <country>United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute at Columbia University</institution></institution-wrap>, <city>New York</city>, <country>United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Colgin</surname>
<given-names>Laura L</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Texas at Austin</institution>
</institution-wrap>
<city>Austin</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>equal contribution</p></fn>
<fn id="n2" fn-type="present-address"><label>†</label><p>Present address: Anthropic, San Francisco, United States</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-01-09">
<day>09</day>
<month>01</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP103512</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-09-25">
<day>25</day>
<month>09</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-09-18">
<day>18</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.09.612073"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Fang et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Fang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-103512-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Forming an episodic memory requires binding together disparate elements that co-occur in a single experience. One model of this process is that neurons representing different components of a memory bind to an “index” — a subset of neurons unique to that memory. Evidence for this model has recently been found in chickadees, which use hippocampal memory to store and recall locations of cached food. Chickadee hippocampus produces sparse, high-dimensional patterns (“barcodes”) that uniquely specify each caching event. Unexpectedly, the same neurons that participate in barcodes also exhibit conventional place tuning. It is unknown how barcode activity is generated, and what role it plays in memory formation and retrieval. It is also unclear how a memory index (e.g. barcodes) could function in the same neural population that represents memory content (e.g. place). Here, we design a biologically plausible model that generates barcodes and uses them to bind experiential content. Our model generates barcodes from place inputs through the chaotic dynamics of a recurrent neural network and uses Hebbian plasticity to store barcodes as attractor states. The model matches experimental observations that memory indices (barcodes) and content signals (place tuning) are randomly intermixed in the activity of single neurons. We demonstrate that barcodes reduce memory interference between correlated experiences. We also show that place tuning plays a complementary role to barcodes, enabling flexible, contextually-appropriate memory retrieval. Finally, our model is compatible with previous models of the hippocampus as generating a predictive map. Distinct predictive and indexing functions of the network are achieved via an adjustment of global recurrent gain. Our results suggest how the hippocampus may use barcodes to resolve fundamental tensions between memory specificity (pattern separation) and flexible recall (pattern completion) in general memory systems.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Author affiliations are updated. Previous version was auto-filled and incorrect.</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/chingf/barcodes">https://github.com/chingf/barcodes</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Humans and other animals draw upon memories to shape their behaviors in the world. Memories of specific personal experiences — called episodic memories (<xref rid="c52" ref-type="bibr">Tulving et al., 1972</xref>) — are particularly important for livelihood. In animals, episodic-like memory is operationally defined as the binding of the “where”, “what”, and “when” components that comprise a single experience. This information can later be retrieved from memory to affect behavior flexibly, depending on context (<xref ref-type="bibr" rid="c10">Clayton and Dickinson, 1998</xref>). The binding of memory contents into a discrete memory is thought to occur in hippocampus. Previous work proposed that hippocampus supports memory by generating an “index”, that is, a signal distinct from the contents of a memory (<xref ref-type="bibr" rid="c49">Teyler and DiScenna, 1986</xref>; <xref ref-type="bibr" rid="c50">Teyler and Rudy, 2007</xref>). In this scheme, during memory formation, plasticity links the neurons that represent memory contents with the neurons that generate this index. Later reactivation of the index drives recall of the memory contents.</p>
<p>Indexing theory was originally articulated at an abstract level, without reference to particular neural representations(<xref ref-type="bibr" rid="c49">Teyler and DiScenna, 1986</xref>). More recently, signatures of index signals were identified in neural activity through experiments in food-caching chickadees (P<italic>oecile atricapillus</italic>), an influential animal model of episodic memory (<xref ref-type="bibr" rid="c44">Sherry, 1984</xref>). <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref> identified “barcode”-like activity in the chickadee hippocampus during memory formation and suggested that barcodes function as memory indices. Barcodes are sparse, high-dimensional patterns of hippocampal activity that occur transiently during caching. They are unique to each cache and are uncorrelated between cache sites, even for nearby sites with similar place tuning. Barcodes are then reactivated when a bird retrieves the cached item. Chickadee hippocampus also encodes the bird’s location — as expected, given the presence of place cells — as well as the presence of a cached sunflower seed, irrespective of location. Thus, <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref> found that hippocampal activity contains both putative memory indices (in the form of barcodes) and putative memory content (in the form of place and seed-related activity).</p>
<p>These findings raise several critical questions. How are barcodes generated and associated with place and seed-related activity during caching? How can hippocampal dynamics subsequently recall these same patterns during retrieval? Critically, neurons participate in both barcodes and place codes with near random overlap, in contrast with theoretical models where content and indexing functions occur in separate neurons (<xref ref-type="bibr" rid="c27">Krotov and Hopfield, 2016</xref>; <xref ref-type="bibr" rid="c8">Bricken and Pehlevan, 2021</xref>; <xref ref-type="bibr" rid="c53">Tyulmankov et al., 2021</xref>; <xref ref-type="bibr" rid="c55">Whittington et al., 2021</xref>; <xref ref-type="bibr" rid="c25">Kozachkov et al., 2023</xref>). It is unclear at a computational level how memory index and content signals can be functionally distinct when they coexist in the same network and even in the activities of single neurons.</p>
<p>In this paper, we use the findings of <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref> to guide the design of a biologically-plausible recurrent neural network (RNN) for cache memory formation and recall. The model generates barcodes and associates them with memory content in the same neural population. At recall time, the model can flexibly adjust the spatial scale of its memory retrieval, ranging from site-specific information to search of an extended area, depending on contextual demands. Using this model, we demonstrate the computational advantages of barcode-mediated memory by showing that it reduces interference between similar memories. We also show that place and barcode activities in the model play complementary roles in enabling memory specificity and flexible recall.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>Barcode representations in the hippocampus are observed during caching</title>
<p>We first review the key experimental results in <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref>. Black-capped chickadees were placed in a laboratory arena comprised of a grid of identical sites. Each site had a perch to land on and a hidden compartment, covered by a flap, where a seed could be hidden. Chickadees were allowed to behave freelyin this environment and collect sunflower seeds from feeders. A chickadee often visited sites without interacting with the hidden compartment (<xref rid="fig1" ref-type="fig">Figure 1A</xref>) but, at other times, the chickadee cached a seed into the compartment, or retrieved a previously cached seed (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). Previous experiments demonstrated that chickadees remember the precise sites of their caches in this behavioral paradigm (<xref ref-type="bibr" rid="c2">Applegate and Aronov, 2022</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Hippocampal activity during food-caching reveals sparse “barcodes” for each cache memory.</title>
<p><bold>A</bold>. A black-capped chickadee visits cache sites in a laboratory arena. The chickadee does not interact with cache sites during what we call a visit. <bold>B</bold>. A chickadee caches or retrieves at a cache site by peeling back the rubber flap at the site. This reveals a hidden compartment where seeds can be stored. <bold>C</bold>. Cartoon example of spiking activity during visits of four hippocampal neurons in a square arena. Spikes are in red. Gray diamonds indicate the location of sites with caches. <bold>D</bold>. As in (C), for the same cells during caching and retrieval. Neurons fire sparsely and randomly during caches (activity clusters at sites) independent of their place tuning. <bold>E</bold>. Correlation of population activity during visits to the same or different sites of increasing distance. Values are max-normalized. Reproduced with permission from <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref>. <bold>F</bold>. As in (E), but comparing activity from caches at a site to activity from retrievals at the same or different sites. Barcode activity shared between caches and retrievals at the same site produces a sharp deviation from smooth spatial tuning. Values are normalized by the same factor as in (E). Reproduced with permission from <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref>.</p></caption>
<graphic xlink:href="612073v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p><xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref> recorded hippocampal population activity during these behaviors. When chickadees visited sites, place cells were observed, similar to those previously found in birds and mammals(<xref ref-type="bibr" rid="c39">Payne et al., 2021</xref>) (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). Place cells did not change their spatial tuning after a cache. Instead, during caching and retrieval, neurons transiently displayed memory-related activity. During caching, neurons fired sparsely, with individual neurons producing large activity bursts for a small subset of caches at seemingly random locations (<xref rid="fig1" ref-type="fig">Figure 1D</xref>). These bursts occurred in both place cells and non-place cells, with the location of cache bursts unrelated to a neuron’s place tuning. At the population level, activity during a cache consisted of both typical place activity and a cache-specific component orthogonal to the place code, termed a “barcode”. Strikingly, barcode activity for a particular cache reactivated during later retrieval of that cache. These findings were evident when examining the correlation between population activity vectors for visits, caches and retrievals at different sites. When comparing two visits, the correlation profile decayed smoothly with distance, as expected from place tuning (<xref rid="fig1" ref-type="fig">Figure 1E</xref>). When comparing caching with retrieval, a similar smooth decay was observed for most distances, indicating the presence of place tuning. However, there was a substantial boost in correlation for caches and retrievals at the exact same site (<xref rid="fig1" ref-type="fig">Figure 1F</xref>). This site-specific boost resulted from reactivation of the cache barcode during subsequent retrieval.</p>
<p>Barcode activity during caching and retrieval, which are moments of memory formation and recall, suggests a mechanism supporting episodic memory. We hypothesize that the hippocampus generates a sparse, high-dimensional pattern of activity transiently during the formation of each memory, and that this serves as a unique index to which the contents of the memory are bound. Reactivation of the barcode at a later time drives the recall of the associated memory contents. In the food caching behavior studied by <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref>, the contents of memory include the location (“where”) and presence of a seed (“what”). Below, we implement this hypothesis in a computational model.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Generating barcode activity with random recurrent dynamics</title>
<p>We model the hippocampus as a recurrent neural network (RNN) (<xref ref-type="bibr" rid="c1">Alvarez and Squire, 1994</xref>; <xref ref-type="bibr" rid="c51">Tsodyks, 1999</xref>; <xref ref-type="bibr" rid="c19">Hopfield, 1982</xref>) and propose that recurrent dynamics can generate barcodes from place inputs (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). Place inputs into the network are termed <inline-formula><inline-graphic xlink:href="612073v3_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and are spatially correlated as follows. Each neuron in the RNN receives inputs that are maximal at one location in the environment and decay with distance from that location, causing the neuron to have a single place field centered at that location. Across neurons, place fields uniformly span the environment. The firing rate activity of RNN units is denoted as <inline-formula><inline-graphic xlink:href="612073v3_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The recurrent weights of the model are given by <italic>J</italic>, and the RNN activity <inline-formula><inline-graphic xlink:href="612073v3_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> follows standard dynamics equations:
<disp-formula id="eqn1">
<graphic xlink:href="612073v3_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn2">
<graphic xlink:href="612073v3_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="612073v3_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the voltage signal of the RNN units and <italic>g</italic>(·) is a leak rate that depends on the average activity of the full network, representing a form of global shunting inhibition that controls overall network activity. In our simulations, we simplify the task by using a 1D circular environment binned into 100 discrete spatial “states”. We set the spatial correlation length scale of place inputs such that the smallest distance between cache sites in the experiments of <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref> is equal to 8 of these states (see Methods for more details).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>A recurrent neural network generates barcode activity through recurrent dynamics.</title>
<p><bold>A</bold>. Diagram of a RNN; the activity of the network units is denoted as <inline-formula><inline-graphic xlink:href="612073v3_inline48.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Place information arrives from an input layer with activities <inline-formula><inline-graphic xlink:href="612073v3_inline49.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Recurrent weights are initialized randomly, that is, <inline-formula><inline-graphic xlink:href="612073v3_inline50.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the synapse connecting neuron <italic>j</italic> to neuron <italic>i</italic>, where <italic>N</italic><sub><italic>x</italic></sub> is the number of RNN neurons. <bold>B</bold>. Correlation of activity vectors across different locations, when RNN weights are initialized with <italic>σ</italic> = 1 (left), <italic>σ</italic> = 7 (center), and <italic>σ</italic> = 20 (right). We show correlation of place inputs (gray) and correlation of RNN activity (black) as <italic>σ</italic> is varied. X-axis is in units of site distance (see Methods for definition). <bold>C</bold>. Response of RNN units when simulating a visit to a location halfway around the circular track (with <italic>r</italic> = 0; <xref ref-type="disp-formula" rid="eqn3">equation 3</xref>). In gray is the activity of the RNN at <italic>t</italic> = 0. In black is the activity at <italic>t</italic> = 100. 200 RNN units are uniformly subsampled and sorted by the tuning of their inputs for plotting purposes. <bold>D</bold>. As in (C), but for <italic>r</italic> = 1. RNN activity is more sparsely distributed, including high activity in neurons without place tuning for the current location. For visualization purposes, 50 RNN units with nonzero activity and 50 RNN units with 0 activity are sampled at this time point (t = 100) for display, and responses greater than 1 are clipped to &gt; 1. <bold>E</bold>. RNN activity in response to state 1 compared to state 2, when <italic>r</italic> = 0. Each point corresponds to a single RNN unit. <bold>F</bold>. As in (E), but for <italic>r</italic> = 1. RNN activity for these neighboring states is substantially decorrelated by recurrence. <bold>G</bold>. Firing fields of three example units on the circular track displaying place-cell activity. Maximum value of each field is labeled and the colormap is max-normalized. <bold>H</bold>. Simulated spike counts of a RNN population during visits to each location on the circular track. Spikes are generated by simulating Poisson-distributed counts from underlying unit rates. Place tuning results in a strong diagonal structure when units are sorted by their input’s preferred location. The maximum limit of the colormap is set to the 99th percentile value of spike counts (≥ 3 spikes). <bold>I</bold>. Same neurons as (G), but for <italic>r</italic> = 1 with units now showing barcode activity. The location of the <italic>r</italic> = 0 place field peak of each unit (i.e., its corresponding peak in (G)) is marked by a red triangle. <bold>J</bold>. As in (H), but for <italic>r</italic> = 1. The independence of barcode activity for neighboring sites results in a matrix with reduced diagonal structure.</p></caption>
<graphic xlink:href="612073v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We initialize the recurrent weights from a random Gaussian distribution <inline-formula><inline-graphic xlink:href="612073v3_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>μ</italic> &lt; 0, reflecting global subtractive inhibition, and <italic>N</italic><sub><italic>x</italic></sub> is the number of RNN neurons). We first consider network activity before any learning-related changes. For the range of parameters we use, the network is in a chaotic state with a roughly constant overall level of activity but fluctuations in the activities of individual units. From an initial state of 0, we run recurrent dynamics until this steady-state level of overall activity has been achieved (<xref rid="figS1" ref-type="fig">Figure S1A,B</xref>). The chaotic recurrent dynamics induced by the random weights (<xref ref-type="bibr" rid="c45">Sompolinsky et al., 1988</xref>) effectively scrambles incoming place inputs. We demonstrate this by measuring the correlation of RNN activity across different locations and plotting this correlation as a function of distance (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Place inputs show a smoothly decaying correlation curve. At low values of <italic>σ</italic>, the network is primarily input driven, showing a smoothly decaying correlation matching inputs. At high values of <italic>σ</italic>, recurrence is so strong that it entirely eliminates the spatial correlation of nearby sites: activity at each state is decorrelated from activity at all other states.</p>
<p>Interestingly, at an intermediate level of <italic>σ</italic> = 7, the network retains elements from both high and low recurrence regimes (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). The network exhibits a smoothly decaying correlation curve reflecting its inputs, but each state’s activity also contains a strong decorrelated component, apparent in the large drop in correlations for nearby but non-identical sites. This intermediate network regime closely resembles spatial correlation profiles observed during caching (<xref ref-type="bibr" rid="c9">Chettih et al., 2024</xref>) (<xref rid="fig1" ref-type="fig">Figure 1F</xref>). The smoothly decaying component is caused by the place code, whereas the sharp peak at zero distance – reflecting barcode activity – is caused by the recurrent dynamics. We thus construct networks using intermediate <italic>σ</italic> values, which allow for the coexistence of place code and barcode components in population activity.</p>
<p>A key result from <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref> is that the hippocampus exhibits both place code and barcode activity during caching and retrieval, but only place activity during visits. We propose that this effect can result from a network in which recurrent strength is flexibly modulated. In a low-recurrence condition, the network produces the place code, whereas in a high-recurrence condition the same network produces a combination of place code and barcode activity. To simulate this in our model, we modify <xref ref-type="disp-formula" rid="eqn2">equation 2</xref> to
<disp-formula id="eqn3">
<graphic xlink:href="612073v3_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the newly included <italic>r</italic> ∈ {0, 1} scales recurrent strength such that the network may be in a feedforward (r = 0) or recurrent regime (r = 1). During visits, we assume the network is operating with low recurrent strength (r = 0). As a result, the activity in the RNN exhibits the spatial profile of its place inputs. We verify this is the case by visualizing early and late RNN activity given a place input (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). With low recurrence, early and late RNN activity have similar spatial profiles, and late activity patterns for nearby states are highly correlated (<xref rid="fig2" ref-type="fig">Figure 2E</xref>). In contrast, when recurrence is enabled (r = 1), network activity is sparse with a heavy tail of a few strongly active neurons, and is decorrelated between nearby states (<xref rid="fig2" ref-type="fig">Figure 2D,F</xref>). These changes match experimental observations of excitatory neurons during food caching (<xref ref-type="bibr" rid="c9">Chettih et al., 2024</xref>).</p>
<p>We visualized neural activity during visits (r = 0) and caches (r = 1). During visits, RNN units display ordinary place tuning (<xref rid="fig2" ref-type="fig">Figure 2G</xref>, <xref ref-type="fig" rid="figS1">S1C</xref>). We also simulated spikes for all units at each location, and visualized this as a matrix where units were sorted by the preferred location of their inputs (<xref rid="fig2" ref-type="fig">Figure 2H</xref>). This matrix exhibits a diagonal structure reflecting strong spatial correlations. During caches, single neurons develop sparse, scrambled spatial responses relative to their place tuning (<xref rid="fig2" ref-type="fig">Figure 2I</xref>, <xref ref-type="fig" rid="figS1">S1C</xref>). Accordingly, simulated spikes across the population have greater random, off-diagonal components during caching than during visits. Thus, recurrence in a randomly connected RNN is a simple and effective mechanism to generate barcode signals from spatially correlated inputs. We alternatively considered a feedforward mechanism to generate barcodes, in which barcodes are computed by a separate feedforward pathway (<xref rid="figS2" ref-type="fig">Figure S2</xref>). We found the feedforward mechanism required an unreasonably large number of neurons and sparsity levels to match the decorrelation level of the recurrent mechanism.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Storing memories by binding content inputs with a barcode</title>
<p>Having suggested a mechanism for the generation of barcode representations, we next propose how such a representation can be leveraged for memory storage in a network model. In our food-caching task, we assume that the contents of a memory include the location of the cache and the identity of a food item within the cache. Thus, we add an additional input source besides spatial location into our model – an input <italic>s</italic> representing the presence of a seed (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). The “seed” input projects randomly onto the RNN neurons. During caching, both place inputs and seed inputs arrive into the RNN, matching experimental findings (<xref ref-type="bibr" rid="c9">Chettih et al., 2024</xref>). This causes a mixed response in the network: one component of the response (place activity) is spatially tuned, another component (seed activity) indicates the presence of a seed and does not vary with location, and the third component (barcode activity) is generated by recurrent dynamics interacting with these inputs.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Barcode activity binds content to store memories in the RNN.</title>
<p><bold>A</bold>. Diagram of RNN activity during memory formation. Along with place inputs, a scalar seed input <italic>s</italic> is provided to the model. <italic>s</italic> connects to RNN units via 5000-dimensional weight vector <italic>j</italic><sub><italic>sx</italic></sub> ∼ 𝒩 (0, 1). During memory formation (i.e., when an animal caches a seed), place inputs representing the animal’s current location and a seed input are provided to the RNN. After recurrent dynamics are run for some time (t = 100), the network undergoes Hebbian learning. <bold>B</bold>. An example of memory recall in the network. The animal is at the same location as in (A). The place input encoding that location is provided to the model and results in the RNN recalling the stored attractor pattern seen in (A). <bold>C</bold>. Examples of RNN population activity during caching (left), retrieval (center), and visits (right) at three sites. During visits the RNN has <italic>r</italic> = 0, while during caches and retrievals <italic>r</italic> = 1. For visualization purposes, 50 units are randomly sampled and displayed in the “Caching” and the “Retrieval” plot. <bold>D</bold>. Correlation of RNN activity at two sites, plotted as a function of the distance between the two sites. In black is the correlation in activity between two visits. In purple is the correlation between caching and retrieval activity. Experiments were simulated with 20 simulations where 5 sites were randomly chosen for caching. 99% confidence intervals are calculated over the simulations and plotted over the markers. Compare to <xref ref-type="fig" rid="fig1">Figure 1E,F</xref>.</p></caption>
<graphic xlink:href="612073v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To store memories, we assume the RNN undergoes Hebbian learning after some fixed time point during the <italic>r</italic> = 1 recurrent dynamics (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). At this time, the synapse <italic>j</italic> → <italic>i</italic> changes by an amount
<disp-formula id="eqn4">
<graphic xlink:href="612073v3_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>β</italic> &gt; 0 provides an overall inhibition of the stored pattern. This term helps to prevent network activity from collapsing to previously stored attractors. Memory storage works as follows, following the experimentally observed sequence in <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref>: place inputs arrive into the RNN, recurrent dynamics form a barcode representation, seed inputs are activated, and then Hebbian learning binds a particular pattern of barcode activity to place- and seed-related activity. In this way, fixed-point attractors are formed corresponding to a conjunction of a barcode and memory content.</p>
<p>Memory recall in our network follows typical pattern completion dynamics, with recurrence strength set to <italic>r</italic> = 1 as for caches. As an example, consider a scenario in which an animal has already formed a memory at some location <italic>l</italic>, resulting in the storage of an attractor <inline-formula><inline-graphic xlink:href="612073v3_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> into the RNN. Later, the animal returns to the same location and attempts recall (i.e. sets <italic>r</italic> = 1, <xref rid="fig3" ref-type="fig">Figure 3B</xref>). The RNN receives inputs representing location <italic>l</italic>, which are partially correlated with <inline-formula><inline-graphic xlink:href="612073v3_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and recurrent attractor dynamics cause network activity to converge onto <inline-formula><inline-graphic xlink:href="612073v3_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This results in a reactivation of barcode activity, along with the place and seed components stored in the attractor state. The seed input can also affect recall, as discussed in the following section.</p>
<p>As an initial test of memory function in our model, we analyzed the activity patterns that are stored and recalled in the model. We simulated caching at different sites in the arena and extracted the population activity that is stored via Hebbian learning (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, left). We then simulated retrieval as an event in which the animal returns to the same site and runs memory recall dynamics (r = 1) in the RNN. Population activity during retrieval closely matches activity during caching, and is substantially decorrelated from activity during visits (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). We find that population activity in the RNN is more strongly correlated between caches and retrievals at the same site than two visits to a site that are not seed related (r = 0; <xref rid="fig3" ref-type="fig">Figure 3D</xref>). In addition, cache-retrieval correlations for non-identical sites rapidly drop to the level of visit-visit correlations because barcodes are orthogonal even for nearby sites. These correlation profiles closely resemble those observed in <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref> (compare <xref rid="fig3" ref-type="fig">Figures 3D</xref> and <xref ref-type="fig" rid="fig1">1E,F</xref>).</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Barcode-mediated memory supports precise and flexible context-based memory recall</title>
<p>What are the computational benefits of barcode-mediated memory storage? We designed two behavioral tasks for our model that quantify complementary aspects of memory performance. In both tasks, we simulate a sequence of three caches in the circular arena. We then test the model’s performance during memory recall (i.e. <italic>r</italic> = 1) using two distinct tasks. The “Cache Presence” task requires the network to identify the presence or absence of a cached seed at the current location (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). By evaluating the model on this task using different spacings between caches, we can measure the spatial precision of memory, i,e. how far apart two caches must be to prevent interference between their associated memories. The “Cache Location” task requires the network to identify the cache nearest to the current location by reactivating place activity corresponding to that cache location (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). This task measures the robustness of recall and requires attractor dynamics to accurately retrieve memories from potentially distant locations. Together, these questions probe the ability of the memory system to be both specific (pattern separation) and searchable (pattern completion).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Precise and flexible recall of barcode-mediated cache memories</title>
<p><bold>A</bold>. Cartoon of a chickadee at a site, trying to remember whether the site contains a seed. The chickadee cannot see inside the hidden compartment and must rely on memory to answer this question. <bold>B</bold>. Cartoon of a chickadee at a site, trying to recall the location of the closest cache.In this case, the animal must use its memory to recall the location of a cache three sites away. <bold>C</bold>. Seed output of the RNN at different locations along the circular track. Red dots above the line indicate locations where the value is greater than 0.5. Top, results after the first cache is made at a location 20% of the way through the track. The location of the first cache is marked on the x-axis as <italic>C</italic><sub>1</sub>. Middle, same but after the second cache (C<sub>2</sub>) is made at a location 35% of the way through the track. Bottom, same but after the third cache (C<sub>3</sub>) is made at a location 70% of the way through the track. <bold>D</bold>. The place output of the RNN at different locations along the track. In the heatmaps, each column shows the activities of all the output units when the animal is at a particular location (horizontal axis). The left side panels of the subfigure correspond to the model operating at low recall strength (s = 0). The right side panels correspond to the model operating at high recall strength (s = 1.5). As in (C), top, middle, and bottom plots correspond to RNN activity after caches are made at C1, C2, and C3. Place output correctly switches between the location of each cache depending on proximity to the current position. <bold>E</bold>. Probability of correct reject rate in the model when the animal is at a location between caches 1 and 2, after all caches have been made. X-axis shows the distance from the probed location to the surrounding caches, measured in site distance. The color of each line corresponds to the recall strength s. At low recall strengths, the model correctly identified the absence of a cache between C1 and C2, reflecting discrimination of the two cache locations. <bold>F</bold>. Probability of recalling the location of the closest cache, given the distance of the animal from the cache. A recall is considered successful if the seed output exceeds 0.5 and if the peak of the output corresponds to the location of the nearest cache. Lines are colored by recall strength <italic>s</italic>. The model correctly recalls the locations of nearby caches, with search radius increasing as recall strength increases.</p></caption>
<graphic xlink:href="612073v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To enable readout from the model, we add an output layer containing place and seed units. During memory formation, these units receive a copy of place and seed inputs, respectively, and undergo Hebbian plasticity (<xref ref-type="disp-formula" rid="eqn4">equation 4</xref> with <italic>β</italic> = 0) with RNN units. This enables the RNN to reactivate a copy of the inputs that were provided during memory formation. We measure the activity of the seed output to determine cache presence, and we measure place outputs to determine cache location. We note that other readout mechanisms would likely function similarly – for example, plastic feedback connectivity from recurrent onto inputs units. The output layer used here is not intended to correspond to a specific brain region, but simply to provide a window into what could be read out easily from network activity by downstream neural circuits.</p>
<p>We first show model performance on a single example of the Cache Presence and Cache Location tasks, where caches are made at the locations 20%, 35%, and 70% of the way around the circular track. For the Cache Presence task, we evaluate the model at each spatial location and plot the activity of the seed output (<xref rid="fig4" ref-type="fig">Figure 4C</xref>). After the first cache, we see that the seed output is only high at states around the vicinity of the first cache (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, top). The false positive rate is not zero since the states around the cache also have high output. However, the width of this false positive area is less than the distance between two sites in the arena used in <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref>, indicating that the model is able to identify caches at the spatial resolution observed in chickadee behavior in these experiments. After a second cache, the seed output correctly reports seeds in the areas around the two caches (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, middle). Importantly, the network separates the caches from each other, correctly identifying the absence of a seed at a position between the two caches. This behavior is maintained after the addition of the final third cache (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, bottom).</p>
<p>For the Cache Location task, we examine the place outputs (<xref rid="fig4" ref-type="fig">Figure 4D</xref>, “Narrow Search”). When the current position is near a cache, the network correctly outputs a place field for the location of the nearest cache. However, an animal relying on memory to guide behavior may need to recall cache locations even when far away from a cache. To enable increased search radius, we make use of the seed input, which can bias network activity towards all memory patterns previously associated with this input. Activating the seed input greatly increases the range of current positions over which cache memories are retrieved (<xref rid="fig4" ref-type="fig">Figure 4D</xref>, “Broad Search”). Critically, this does not cause interference between memories. For example, as current position moves from the location of cache 1 to cache 2, place outputs discretely jump from one cache location to the other, corresponding to correct selection of the nearest cache location. The search radius during recall can thus be flexibly adjusted according to task demands, allowing the trade-off between pattern completion and pattern separation to be dynamically regulated by simple scaling of a network input.</p>
<p>We next systematically quantified model performance on these tasks. For these analyses, we binarized the seed output as being above or below a threshold (<xref rid="figS3" ref-type="fig">Figure S3AB</xref>). When caches are sufficiently spaced, memory recall is near perfect (<xref rid="figS3" ref-type="fig">Figure S3AB</xref>). This led us to evaluate the resolution of the model’s memory as caches become more closely spaced. We decreased the distance between cache 1 and 2 to identify the minimum distance where caches are separable, i.e. where the model correctly indicates the absence of a cache at a midpoint between caches 1 and 2. <xref rid="fig4" ref-type="fig">Figure 4E</xref> shows this correct reject rate as a function of the distance from each cache and the seed input strength. With low seed input (e.g. <italic>s</italic> = 0), the correct reject rate is high when the current location is one site distance away from caches 1 and 2. In other words, if a cache is made at site 1 and site 3, our model recognizes that site 2 remains empty. This precision matches the single-site resolution measured by behavioral experiments in this arena (<xref ref-type="bibr" rid="c2">Applegate and Aronov, 2022</xref>). As expected, performance on the Cache Presence task decreases with greater seed input strengths, as these are suited to searching over a broad spatial range.</p>
<p>For the Cache Location task, we measure the probability that the model outputs the location of the nearest cache. We plot this probability as a function of distance from the current location to the nearest cache (<xref rid="fig4" ref-type="fig">Figure 4F</xref>). The model is able to correctly recall cache locations with almost perfect accuracy when it is near a cache. Performance drops sharply with distance when the seed input is low, but is substantially recovered by increasing the seed input strength. Critically, even when the search radius is broad enough to include multiple caches, attractor dynamics encourage selection of the single closest cache location rather than blending memories. Thus the seed input strength provides a flexible search radius during the recall process. Low values of <italic>s</italic> are more suitable for detecting the presence or absence of a seed near the current location, while high values of <italic>s</italic> are more suitable for finding remote caches. The complementary demands of the Cache Presence and Cache Location tasks demonstrate the utility of a flexible search radius within one memory system.</p>
<p>We further examined how model hyperparameters affected performance on these tasks. We find that the plasticity bias <italic>β</italic> is needed to prevent erroneous memory recall at sites without caches (<xref rid="figS3" ref-type="fig">Figure S3E-H</xref>). Without this, recall specificity is poor and model performance suffers on the Cache Presence task. We also verified that our model is robust to the order in which caches were made (<xref rid="figS3" ref-type="fig">Figure S3CD</xref>). Finally, we extended our model to work with random, spatially correlated inputs, rather than receiving place cell-like inputs (<xref rid="figS4" ref-type="fig">Figure S4</xref>). This shows that our results apply regardless of the specific format of spatial inputs, which are experimentally undetermined.</p>
</sec>
<sec id="s2e">
<label>2.5</label>
<title>Place activity and barcode activity play complementary roles in memory</title>
<p>We have shown that a barcode-mediated memory system is precise yet allows flexible, content-based retrieval. Below we identify the specific contributions of place and of barcode activity by ablating either of these components in our model. To ablate barcodes (“Place Code Only” in <xref rid="fig5" ref-type="fig">Figure 5</xref>), we initialize our model without the random recurrent weights that produce chaotic dynamics. This is akin to running caching dynamics in “visit mode”, i.e. <italic>r</italic> = 0 in Figure To ablate place activity (“Barcode Only” in <xref rid="fig5" ref-type="fig">Figure 5</xref>), we eliminate spatial correlations in the networks inputs. This is akin to having place fields with extremely narrow precision, rather than a spatially smooth place code. We test both ablated models on the three-cache tasks from above and compare their performance to our full model.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Ablations reveal complementary roles of place code and barcode activity in memory recall.</title>
<p><bold>A</bold>. Left, as in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, bottom, but for the “Place Code Only” model (barcode-ablated), which has recurrent weights initalized with <italic>μ</italic> = 0, <italic>σ</italic> = 0. Right, same, but for the “Barcode Only” model (place-ablated). The barcode only model is the same as the full model, but place inputs are uncorrelated for nearby locations. <bold>B</bold>. Left, as in <xref ref-type="fig" rid="fig4">Figure 4D</xref>, bottom, but for the place code only model. Right, the same but for the barcode only model (for visual clarity, the matrix is downsampled to remove zero-value rows). The place code only model has similar outputs for recall from all current locations. The barcode only model has no place output for recall from most locations. <bold>C</bold>. As in <xref ref-type="fig" rid="fig4">Figure 4E</xref>, but showing the full model (solid line), the place code only model (dotted line), and the barcode only model (dashed line). For all models, <italic>s</italic> = 0. The place code only model is unable to discriminate between nearby caches. <bold>D</bold>. As in <xref ref-type="fig" rid="fig4">Figure 4F</xref>, with lines as in (C). For all models, <italic>s</italic> = 0.4. The barcode only model recalls successfully only when the current location contains a cache. Only the full model reliably recalls remote cache locations.</p></caption>
<graphic xlink:href="612073v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The Place Code Only model directly binds memory contents of place and seed. This causes cache memories for nearby locations, where the place code is correlated, to interfere with each other. Interference is clearly visible in the performance on the Cache Presence task (<xref rid="fig5" ref-type="fig">Figure 5A</xref>, left). The seed output of the place code only network is high across a wide area including caches 1 and 2 and the empty locations between them. Indeed this network is unable to identify the absence of a cache at locations between caches 1 and 2, even with a substantial distance between them (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). Furthermore, the memories for caches 1 and 2 sometimes appears to suppress memory for a distant cache Further evidence of interference is apparent in the Cache Location task, where the network merges caches 1 and 2, and entirely fails to signal the cache at location 3 (<xref rid="fig5" ref-type="fig">Figure 5B</xref>, left). This network has a low true negative rate, and often a single cache dominates the output (<xref rid="figS5" ref-type="fig">Figure S5A-D</xref>). Accordingly, the network is able to signal the location of a cache, but this location is often not the nearest cache (<xref rid="fig5" ref-type="fig">Figure 5D</xref>). Intuitively, without barcodes the network is unable to distinguish individual memories at nearby spatial locations. Without this barcode-mediated competition, it forms a single agglomerated memory that is inflexibly recalled.</p>
<p>If correlations in inputs cause memory interference, why not do away with them entirely? The Barcode Only model shows that this is not a good option. This model performs well on the Cache Presence task (<xref rid="fig5" ref-type="fig">Figure 5A</xref>, right, <xref rid="figS5" ref-type="fig">Figure S5E-G</xref>), correctly identifying the presence of all three caches, and the absence of caches in other locations (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). However, the Barcode Only model fails on the Cache Location task. The model is unable to recall place fields that are not precisely at its current location (<xref rid="fig5" ref-type="fig">Figure 5B,D</xref>). With greater seed input, the model can sometimes recall memories at remote locations, but these are selected randomly with no preference for nearby caches (<xref rid="fig5" ref-type="fig">Figure 5D</xref>, <xref rid="figS5" ref-type="fig">Figure S5H</xref>). Intuitively, the model cannot distinguish nearby and distant caches because it lacks input correlations, which establish the measure of proximity.</p>
<p>In summary, place and barcode activity play complementary roles in our model. Barcode activity functions like an index for individual memories. This function supports discriminability of memories even when memory contents overlap – for example, for two nearby caches with correlated place activity. Barcodes also support selective recall of individual memories in our model via competitive attractor dynamics. However the discriminability advantage of barcodes is only useful if they can be reactivated during memory recall. Spatially correlated place inputs (and the seed input) allow efficient memory retrieval by defining a measure of proximity in memory contents, and support remote memory recall. The presence of both index and content signals in our model allows all of these functions to be performed by the same network, and for the trade-offs between them to be adjusted flexibly.</p>
</sec>
<sec id="s2f">
<label>2.6</label>
<title>Modulating recurrent strength allows the RNN to incorporate both predictive maps and barcode memory</title>
<p>We have constructed a model of a simple episodic memory, taking inspiration from hippocampal data (<xref ref-type="bibr" rid="c9">Chettih et al., 2024</xref>). However, the hippocampus has also been suggested to support functions beyond episodic memory. An especially influential line of prior work proposes that the hippocampus plays a role in generating predictive maps, with evidence from both experiments (including in food-caching birds) (<xref ref-type="bibr" rid="c35">Muller and Kubie, 1989</xref>; <xref ref-type="bibr" rid="c34">Mehta et al., 2000</xref>; <xref ref-type="bibr" rid="c39">Payne et al., 2021</xref>; <xref ref-type="bibr" rid="c3">Applegate et al., 2023</xref>) and theory (<xref ref-type="bibr" rid="c7">Blum and Abbott, 1996</xref>; <xref ref-type="bibr" rid="c46">Stachenfeld et al., 2017</xref>; <xref ref-type="bibr" rid="c54">Whittington et al., 2020</xref>). This raises the question – is our model consistent with predictive map theories? And if so, how might predictive maps influence episodic memory recall?</p>
<p>Interestingly, prior work has shown that biologically realistic RNNs can generate predictive maps with structured recurrent weights (<xref ref-type="bibr" rid="c12">Fang et al., 2023</xref>). Specifically, if recurrent weights encode the transition statistics of an animal’s experience, then predictive map-like activity will arise from the RNN dynamics, and can be controlled by recurrent strength (analogous to the value of <italic>r</italic> in <xref ref-type="disp-formula" rid="eqn3">equation 3</xref>). This prediction via structured recurrent weights invites comparison to the use of random recurrent weights in our model to generate barcodes. Inspired by this connection, we considered whether a single model could generate both predictive and barcode activity via recurrent dynamics. We constructed a hybrid model, in which recurrent weights are a blend of random weights, as above, and structured, predictive weights as in <xref ref-type="bibr" rid="c12">Fang et al. (2023)</xref> (<xref rid="figS6" ref-type="fig">Figure S6AB</xref>). We hypothesize that network activity changes from a predictive to a chaotic regime (supporting barcodes) with increasing recurrent strength, i.e. as we adjust <italic>r</italic> in the range 0 &lt; <italic>r</italic> &lt; 1.</p>
<p>We use the same circular arena for our simulations, but with the assumption that animal behavior is biased, i.e., the animal only moves clockwise (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). We visualize population activity at each site, with units sorted by their place field. When recurrent strength is at its lowest (r = 0, <xref rid="fig6" ref-type="fig">Figure 6B</xref>) network activity is place-like and reflects the current location of the animal provided in its inputs. When recurrent strength is slightly increased (r = 0.3, <xref rid="fig6" ref-type="fig">Figure 6C</xref>), the network exhibits predictive activity. That is, neural activity of recurrent units is shifted to reflect an expected future position, relative to the current spatial position encoded in inputs. This is consistent with observations from experimental data collected from hippocampus of animals with biased movements in a linear track (<xref ref-type="bibr" rid="c56">Wilson and McNaughton, 1993</xref>; <xref ref-type="bibr" rid="c33">Mehta et al., 1997</xref>, <xref ref-type="bibr" rid="c34">2000</xref>; <xref ref-type="bibr" rid="c29">Lisman and Redish, 2009</xref>). Finally, when recurrent strength is maximal (r = 1.0, <xref rid="fig6" ref-type="fig">Figure 6D</xref>) we observe barcodes, i.e. sparse activity at random positions relative to inputs. We plot spatial tuning curves of a few example RNN units under low, intermediate, and high recurrent strengths (<xref rid="fig6" ref-type="fig">Figure 6E</xref>, <xref ref-type="fig" rid="figS6">S6C</xref>). Individual units can display typical place fields (blue), skewed predictive place fields (orange), and barcode activity (purple).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Predictive coding and barcode generation are performed by the same RNN in different dynamical regimes</title>
<p><bold>A</bold>. Cartoon of a chickadee in a circular track, running in a consistently clockwise direction. <bold>B</bold>. Heatmap of RNN firing fields when <italic>r</italic> = 0, where each row corresponds to the tuning curve of one neuron across all locations. Red dashed line indicates the diagonal. <bold>C</bold>. As in (B), but for <italic>r</italic> = 0.3. Here, clockwise movement corresponds to movement from site <italic>i</italic> to site <italic>i</italic> + 1. Thus predictive activity appears as a shift in RNN activity below the matrix diagonal. <bold>D</bold>. As in (B), but for <italic>r</italic> = 1.0. Barcodes appear as random, off-diagonal structure in the activity matrix. <bold>E</bold>. The firing fields of four example units across different recurrent strengths. That is, each unit’s row in (B) is in blue, its row in (C) is in orange, and its row in (D) is in purple. Each curve is max-normalized. <bold>F</bold>. Average projection strength of RNN activity onto the place code, predictive code, and barcode vectors. The place code vector is defined as activity with <italic>r</italic> = 0. The predictive code vector is defined as the place field of the unit at the next clockwise site, minus the projection onto the place code. The barcode vector is defined as activity with <italic>r</italic> = 1.0, minus the projection onto the place code. Each line is max-normalized. <bold>G</bold>. Seed output of the model with predictive weights, given the animal’s location on the circular track. Here, there is only one cache, made at the halfway location of the circular track (labeled “C”).</p></caption>
<graphic xlink:href="612073v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>These results suggest that the structured and random components of recurrent connectivity can act somewhat independently of each other, with their relative contributions determined by recurrent strength. We quantify this by measuring the magnitude of the projection of population activity onto the place code, the predictive code, and barcodes as a function of recurrent strength (<xref rid="fig6" ref-type="fig">Figure 6F</xref>). At the lowest recurrent strength (r = 0), the population activity is solely concentrated on the place code. As recurrent strength is increased, place coding decreases and predictive activity increases with a peak around <italic>r</italic> = 0.4. Beyond this, both place and predictive activity decrease as barcode activity rises to a peak when <italic>r</italic> = 1.0.</p>
<p>Finally, we explore the functional implications of including predictive maps into our memory model. We first verify that model performance in the previous three-cache tasks is not disrupted by including predictive weights in the model (<xref rid="figS7" ref-type="fig">Figure S7</xref>). We then examine memory recall of a single cache for a predictive model, assuming a clockwise behavioral bias (<xref rid="fig6" ref-type="fig">Figure 6G</xref>). The model exhibits a profound skew: the cache is recalled much earlier on the path leading up to the cache location, at further distance from the cache than on the path after the cache location. This indicates that knowledge of the environment and prior experience, as reflected in predictive place activity, can shape memory recall in our model.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>We have proposed a biologically realistic model for a simple form of episodic memory using barcodes. Our work is related to previous auto-associative memory models of the hippocampus such as Hopfield networks (<xref ref-type="bibr" rid="c14">Gardner-Medwin, 1976</xref>; <xref ref-type="bibr" rid="c32">McNaughton and Morris, 1987</xref>; <xref ref-type="bibr" rid="c31">Marr et al., 1991</xref>; <xref ref-type="bibr" rid="c1">Alvarez and Squire, 1994</xref>; <xref ref-type="bibr" rid="c51">Tsodyks, 1999</xref>), but diverges in a few critical areas. Building on ideas from hippocampal indexing theory (<xref ref-type="bibr" rid="c49">Teyler and DiScenna, 1986</xref>; <xref ref-type="bibr" rid="c50">Teyler and Rudy, 2007</xref>), and following the discovery of barcodes (<xref ref-type="bibr" rid="c9">Chettih et al., 2024</xref>), we show how recurrent computation can implement memory indexing. Our model is further noteworthy in randomly intermixing representations of memory index and memory content in the activity of single neurons, matching experimental findings. This intermixing implies that single neurons cannot be definitively identified as “place cells” or “barcode cells”, despite clear differentiation between the place code and the barcode at the population level. A further innovation of our model is the ability to control the trade-off between pattern completion and pattern separation during memory recall, by simply turning up or down the strength of a memory content input (“search strength” in <xref rid="fig4" ref-type="fig">Figure 4</xref>). In this work we considered only place and a single “seed” input, but it is straightforward to generalize this to naturalistic cases where different food types are stored, or to memory contents beyond cached food. In principle, our approach would allow independent control of search strength for each potential element of memory content.</p>
<p>To generate barcodes during caching and retrieval without affecting place activity during visits, our model changes recurrent strength in an RNN between different behaviors. A major question is how the brain could implement such gain changes in recurrence. One possible mechanism is a change in recurrent inhibition, which is consistent with dramatic changes in the activity of inhibitory neurons observed during caching (<xref ref-type="bibr" rid="c9">Chettih et al., 2024</xref>). Neuromodulators like acetylcholine have been shown to bidirectionally modulate different inhibitory neuron subtypes (<xref ref-type="bibr" rid="c57">Xiang et al., 1998</xref>; <xref ref-type="bibr" rid="c30">Lovett-Barron, 2014</xref>), and proposed to control the recurrent gain of hippocampal processing (<xref ref-type="bibr" rid="c17">Hasselmo, 1999</xref>, <xref ref-type="bibr" rid="c18">2006</xref>). However, our model uses generic RNN units, and it is unclear precisely how units in the model should be mapped to real excitatory and inhibitory hippocampal neurons in the brain. Our model predicts a state change in hippocampal activity during memory formation and recall, allowing recurrent computation to generate or reactivate memory barcodes. Detailed modeling of realistic E-I networks is needed to further clarify its specific biological implementation.</p>
<p>Alternatively, other mechanisms may be involved in generating barcodes. We demonstrated that conventional feed-forward sparsification (<xref ref-type="bibr" rid="c4">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="c58">Xie et al., 2023</xref>) was highly inefficient, but more specialized computations may improve this (<xref ref-type="bibr" rid="c13">Földiak, 1990</xref>; <xref ref-type="bibr" rid="c38">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="c41">Sacouto and Wichert, 2023</xref>; <xref ref-type="bibr" rid="c36">Muscinelli et al., 2023</xref>). A separate possibility is that barcodes are generated in a circuit upstream of where memories are stored, and supplied as inputs to the hippocampal population. We suspect separating barcode generation and memory storage in separate networks would not fundamentally affect our conclusions.</p>
<p>We showed that barcodes allow for precise memory retrieval despite the presence of other correlated memories. This sharpened memory retrieval is similar to mechanisms used in key-value memory structures that are often embedded in machine learning architectures (<xref ref-type="bibr" rid="c15">Graves et al., 2014</xref>, <xref ref-type="bibr" rid="c16">2016</xref>; <xref ref-type="bibr" rid="c47">Sukhbaatar et al., 2015</xref>; <xref ref-type="bibr" rid="c28">Le et al., 2019</xref>; <xref ref-type="bibr" rid="c5">Banino et al., 2020</xref>). At their simplest, these key-value memory structures consist of memory slots. Each slot consists of a memory that can be content-addressed via “keys” such that their stored memory is returned as “values”. In machine learning, key-value memory has been connected to the dot-product attention mechanism used in transformers (<xref ref-type="bibr" rid="c27">Krotov and Hopfield, 2016</xref>; <xref ref-type="bibr" rid="c40">Ramsauer et al., 2020</xref>). Interestingly, prior theoretical work has suggested neural implementations for both key-value memory and attention mechanisms, arguing for their usefulness in neural systems such as long term memory (<xref ref-type="bibr" rid="c24">Kanerva, 1988</xref>; <xref ref-type="bibr" rid="c53">Tyulmankov et al., 2021</xref>; <xref ref-type="bibr" rid="c8">Bricken and Pehlevan, 2021</xref>; <xref ref-type="bibr" rid="c55">Whittington et al., 2021</xref>; <xref ref-type="bibr" rid="c25">Kozachkov et al., 2023</xref>; <xref ref-type="bibr" rid="c26">Krotov and Hopfield, 2020</xref>). Our work suggests that the hippocampus may use principles similar to those of key-value memory structures to store episodic memories.</p>
<p>Episodic memory is often studied at a behavioral level in humans performing free or cued recall of remembered word lists (<xref ref-type="bibr" rid="c22">Kahana, 1984</xref>; <xref ref-type="bibr" rid="c37">Naim et al., 2020</xref>). Temporal context models (TCM) of episodic memory have been highly successful in accounting for the sequential order effects observed reliably in this experimental setting (<xref ref-type="bibr" rid="c21">Howard and Kahana, 2002</xref>; <xref ref-type="bibr" rid="c20">Howard et al., 2005</xref>; <xref ref-type="bibr" rid="c43">Sederberg et al., 2008</xref>), and the idea of a “context vector” in TCM is closely related to use of barcodes as a memory index in our model. However experiments have shown that chickadee cache retrieval does not exhibit temporal order effects (<xref ref-type="bibr" rid="c2">Applegate and Aronov, 2022</xref>), suggesting that caches at different locations are likely not linked by a temporal context as in TCM. Interestingly, caches at the same location were found to have distinct but correlated barcodes (<xref ref-type="bibr" rid="c9">Chettih et al., 2024</xref>), which could be related to caches sharing a “spatial context” analogous to TCM. In the present study we did not consider memory for different caches at the same location, since it requires a mechanism for forgetting or overwriting cache memory following retrieval. Although such “directed forgetting” is observed in chickadee behavior (<xref ref-type="bibr" rid="c44">Sherry, 1984</xref>), there is no definitive solution for Hopfield-like networks, and it is thus beyond the scope of our current work.</p>
<p>Our hippocampal model focused on the implementation of episodic memory. Importantly, the proposed barcode mechanism is capable of coexisting with other hippocampal functions, such as predictive coding as formalized by the successor representation (SR) (<xref ref-type="bibr" rid="c46">Stachenfeld et al., 2017</xref>). Surprisingly, we found that a hybrid network can switch between SR-generating and barcode-generating modes of operation by adjusting the gain of recurrent connectivity. Further work is needed to characterize the general conditions under which barcode and SR functions do or do not mutually interfere. It is also unclear if these are separate functions of the same circuit, or if they are complementary in certain scenarios (<xref ref-type="bibr" rid="c42">Schapiro et al., 2017</xref>; <xref ref-type="bibr" rid="c6">Barron et al., 2020</xref>). For example, we found that the SR could bias barcode-mediated memory recall. In a complex environment, the Euclidean distance between two points may not correspond to its proximity in a practical sense, which the SR better captures. In this case, experience-dependent biases in memory recall can be functionally advantageous (<xref ref-type="bibr" rid="c11">Dasgupta and Gershman, 2021</xref>) and would be consistent with behavioral results (<xref ref-type="bibr" rid="c23">Kahana, 1996</xref>; <xref ref-type="bibr" rid="c48">Talmi and Moscovitch, 2004</xref>).</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Kim Stachenfeld, Ashok Litwin-Kumar, and members of the Aronov, Stachenfeld, and Abbott labs for feedback on this work. This research was supported by the Gatsby Charitable Foundation and the Kavli Foundation and by NSF award DBI-1707398, NIH Director’s New Innovator Award (DP2-AG071918), NIH Pathway to Independence Award (SC, 1K99NS136846), NSF GRFP (CF), and DOE CSGF (JL, DE–SC0020347).</p>
</ack>
<sec id="s4">
<title>Contributions</title>
<p>All authors contributed to conceptualization of this project. CF, JL, and SC designed the network and experiments. CF and JL performed formal analysis, with supervision by SC, DA and LA. CF, SC and DA prepared the manuscript with feedback from all authors.</p>
</sec>
<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<title>Caching Task</title>
<p>We simulate a caching task in a circular track, similar to <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref>. The track consists of <italic>N</italic><sub><italic>s</italic></sub> connected states. The goal of this task is to test how well an agent equipped with a memory model can precisely recall the locations where a cache (or memory) has been stored. Specifically, for each simulation we first choose a set of states <italic>C</italic> that will be the location where caches are made. For each state <italic>c</italic> ∈ <italic>C</italic> we assume the agent is currently at state <italic>c</italic> and allow the model to store a memory at that state. We then simulate what the output of the model is if the agent is at any of the other <italic>N</italic><sub><italic>s</italic></sub> states. We continue this procedure for the remaining states in the list.</p>
<p>In the three-cache task, <italic>C</italic> = {0, <italic>m, N</italic><sub><italic>s</italic></sub> ∗ 0.7}. We sweep over different values of 0 &lt; <italic>m</italic> &lt; <italic>N</italic><sub><italic>s</italic></sub> ∗ 0.7 to test the effects of site spacing. In the main figures, these three caches are made in increasing order of their location. However, we also randomly shuffle the order of caching in a supplementary figure (<xref rid="figS3" ref-type="fig">Figure S3CD</xref>) and do not find any effects on model performance.</p>
</sec>
<sec id="s5b">
<title>Barcode model</title>
<sec id="s5b1">
<title>Architecture</title>
<p>Place inputs into the model arrive from an input layer <inline-formula><inline-graphic xlink:href="612073v3_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The input layer feeds into a recurrent neural network with activity <inline-formula><inline-graphic xlink:href="612073v3_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>N</italic><sub><italic>x</italic></sub> = <italic>N</italic><sub><italic>p</italic></sub>. Place input units connect to recurrent units with one-to-one connections (that is, the weight matrix <italic>J</italic><sub><italic>xi</italic></sub> from the place input layer to the recurrent network is the size <italic>N</italic><sub><italic>x</italic></sub> identity matrix). Recurrent weights are encoded in the matrix <inline-formula><inline-graphic xlink:href="612073v3_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. At initialization, <inline-formula><inline-graphic xlink:href="612073v3_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>μ, σ</italic> are tunable hyperparameters controlling the mean and standard deviation of the distribution.</p>
<p>The input representing seeds arrives from a single unit <italic>s</italic>. The connections from <italic>s</italic> to recurrent units <inline-formula><inline-graphic xlink:href="612073v3_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are encoded in the vector <inline-formula><inline-graphic xlink:href="612073v3_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Each value of <inline-formula><inline-graphic xlink:href="612073v3_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is sampled from the standard normal distribution.</p>
</sec>
<sec id="s5b2">
<title>Recurrent dynamics</title>
<p>Recurrent dynamics are run over <italic>T</italic> timesteps. Let <inline-formula><inline-graphic xlink:href="612073v3_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula> be the preactivations of the recurrent population at time <italic>t</italic> and <inline-formula><inline-graphic xlink:href="612073v3_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula> be the activations of the recurrent population at time <italic>t</italic>. That is, <inline-formula><inline-graphic xlink:href="612073v3_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The recurrent dynamics are defined over <inline-formula><inline-graphic xlink:href="612073v3_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula>:
<disp-formula id="ueqn1">
<graphic xlink:href="612073v3_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>r</italic> ∈ {0, 1} is a modulatory factor that controls whether the network operates with recurrent dynamics or is purely feedforward driven. The first term in the equation corresponds to a voltage leak term where the leak at each neuron is proportional to global population activity. This effectively implements a form of divisive normalization, and helps keep network activity stable even as weights are updated during the caching task. The second term in the equation represents recurrent inputs, while the last two terms represent feedforward inputs into the network.</p>
</sec>
<sec id="s5b3">
<title>Input structure and timing</title>
<p>Place is encoded in <inline-formula><inline-graphic xlink:href="612073v3_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula> such that, at location <italic>k</italic>, each input neuron <italic>l</italic> has activity <inline-formula><inline-graphic xlink:href="612073v3_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>d</italic> is the shortest distance between <italic>k</italic> and <italic>l</italic> as a percentage of the circular arena size. We consider 100 evenly sampled states in this state space. Seed input <italic>s</italic> may be any nonnegative scalar value.</p>
<list list-type="bullet">
<list-item><p>Place mode: Input <inline-formula><inline-graphic xlink:href="612073v3_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is active over all <italic>T</italic> timesteps. Recurrence is turned off (r = 0), as is the seed input (s = 0).</p></list-item>
<list-item><p>Caching mode: Input <inline-formula><inline-graphic xlink:href="612073v3_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is active over all <italic>T</italic> timesteps and input <italic>s</italic> is active over timesteps [<italic>T</italic> − <italic>t</italic><sub><italic>s</italic></sub>, <italic>T</italic>] with strength λ. Recurrence is on (r = 1).</p></list-item>
<list-item><p>Recall mode: Input <inline-formula><inline-graphic xlink:href="612073v3_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and input <italic>s</italic> are both active over all <italic>T</italic> timesteps. Recurrence is on (r = 1). The value of <italic>s</italic> is flexibly modulated to adjust the search strength (s ≥ 0).</p></list-item>
</list>
</sec>
<sec id="s5b4">
<title>Update rule</title>
<p>Let <inline-formula><inline-graphic xlink:href="612073v3_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> be the activations of the recurrent network at the end of recurrent dynamics (in our case, at time <italic>T</italic>). At each cache event the update rule carried out is
<disp-formula id="ueqn2">
<graphic xlink:href="612073v3_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>η</italic> controls the learning rate. The weight update contains a Hebbian update through <inline-formula><inline-graphic xlink:href="612073v3_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and an inhibitory update through <inline-formula><inline-graphic xlink:href="612073v3_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>β</italic> is a negative scalar. This inhibitory term causes the connections between neurons which are not co-active during caching to weaken. The update rule can also be stated in terms of the synapse <italic>j</italic> → <italic>i</italic>:
<disp-formula id="ueqn3">
<graphic xlink:href="612073v3_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This is the form shown in <xref ref-type="disp-formula" rid="eqn4">equation 4</xref>.</p>
</sec>
<sec id="s5b5">
<title>Network readout</title>
<p>To detect cache presence, we define a seed output signal. The output <italic>y</italic><sub><italic>s</italic></sub> is read out from the recurrent network activity through weights <inline-formula><inline-graphic xlink:href="612073v3_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. At initialization, <inline-formula><inline-graphic xlink:href="612073v3_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Every time a cache is made, the following update rule is run: <inline-formula><inline-graphic xlink:href="612073v3_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Thus, seed output is read out as <inline-formula><inline-graphic xlink:href="612073v3_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>To recall cache location, we define a place field readout layer. The output <inline-formula><inline-graphic xlink:href="612073v3_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is read out from the recurrent network through weights <inline-formula><inline-graphic xlink:href="612073v3_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. At initialization, <italic>J</italic><sub><italic>yx</italic></sub> = 0. Every time a cache is made, the following update rule is run: <inline-formula><inline-graphic xlink:href="612073v3_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Thus, recalled cache locations are read out as <inline-formula><inline-graphic xlink:href="612073v3_inline35.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
</sec>
<sec id="s5b6">
<title>Ablations</title>
<p>To construct a place code only model, we ablate barcode generation by setting <italic>J</italic> = 0 at initialization. To construct a barcode only model, we ablate the presence of place-correlations in the input by setting the place input spatial scale parameter <italic>ν</italic> to a very small value 10<sup>−3</sup> in the place encoding equation.</p>
</sec>
</sec>
<sec id="s5c">
<title>Defining site distance in our simulation</title>
<p>To allow comparisons to data in <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref>, we first determine the number of states in our simulation that’s equivalent to the distance between adjacent cache sites in <xref ref-type="bibr" rid="c9">Chettih et al. (2024)</xref>. To do so, we note that the spatial correlation between population activity at two adjacent cache sites is around 0.75 (when spatial correlation profile is normalized to [0, 1]). We identify the number of states in our simulation such that the normalized correlation between visit activity is also around 0.75. We find that this is around 8 states. Thus, we define 8 states in our simulation as equivalent to the distance between adjacent cache sites.</p>
</sec>
<sec id="s5d">
<title>Simulating and visualizing spikes</title>
<p>We simulate Poisson spikes from our rate network in <xref rid="fig2" ref-type="fig">Figure 2HJ</xref>. Specifically, for a unit with rate <italic>r</italic>, we draw spikes from a Poisson distribution with mean and variance <italic>r</italic> + <italic>K</italic>. We set <italic>K</italic> = 0.2 to visually match observations from data.</p>
</sec>
<sec id="s5e">
<title>Spatial correlation of RNN activity at different sites</title>
<p>To calculate correlation values as in <xref rid="fig3" ref-type="fig">Figure 3D</xref>, we average over the correlations across multiple samples of Poisson-generated population spikes. Specifically, for RNN activity <inline-formula><inline-graphic xlink:href="612073v3_inline36.gif" mime-subtype="gif" mimetype="image"/></inline-formula> at location <italic>i</italic>, we first generate a sample vector of spikes <inline-formula><inline-graphic xlink:href="612073v3_inline37.gif" mime-subtype="gif" mimetype="image"/></inline-formula> Poisson <inline-formula><inline-graphic xlink:href="612073v3_inline38.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We choose <italic>k</italic> to be 0.2 to match experimental spatial correlation profiles in the “visit-visit” condition. To get the correlation of population activity at locations <italic>i, j</italic>, we calculate the population Pearson correlation coefficient between <inline-formula><inline-graphic xlink:href="612073v3_inline39.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="612073v3_inline40.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. To generate <xref rid="fig3" ref-type="fig">Figure 3D</xref>, we collect correlation values between sites with caches in an experiment where 5 caches are made in randomly chosen site locations. All values are normalized by the “visit-visit” correlation value at a site distance of 0. This is repeated over 20 random seeds.</p>
</sec>
<sec id="s5f">
<title>Barcode model with predictive map</title>
<p>The barcode model with prediction differs from the default model in the initialized weight matrix. Specifically, the weight matrix <italic>J</italic> = <italic>B</italic> + <italic>M</italic>, where <italic>B</italic> is the random Gaussian matrix of the default model. <italic>M</italic> is a successor representation-like matrix (<xref ref-type="bibr" rid="c46">Stachenfeld et al., 2017</xref>) defined as
<disp-formula id="ueqn4">
<graphic xlink:href="612073v3_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>T</italic> is the transition probability matrix and <italic>γ</italic> = 0.99 is a temporal discount factor. We also add a scaling factor <italic>ρ</italic> = 0.075 and an inhibitory offset <italic>δ</italic> = −0.015. <italic>M</italic> is a 5000-dimensional square matrix, and we truncate the summation at <italic>D</italic> = 300 steps. <italic>T</italic> is defined as
<disp-formula id="ueqn5">
<graphic xlink:href="612073v3_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s5g">
<title>Alternative model: feedforward barcode generation</title>
<p>We will construct a feedforward model to generate sparse, decorrelated barcodes (<xref rid="figS2" ref-type="fig">Figure S2A</xref>). Place inputs to the model are fed through a hidden expansion layer before being compressed again by an output layer to generate the barcode. We first define these layers via the random matrices <inline-formula><inline-graphic xlink:href="612073v3_inline41.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="612073v3_inline42.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <inline-formula><inline-graphic xlink:href="612073v3_inline43.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="612073v3_inline44.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We make the hidden layer very large: <italic>M</italic> = 20000. The activity of the model in the hidden layer is described as:
<disp-formula id="ueqn6">
<graphic xlink:href="612073v3_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>C</italic><sub><italic>θ</italic></sub> is a constant chosen such that the proportion of units in <inline-formula><inline-graphic xlink:href="612073v3_inline45.gif" mime-subtype="gif" mimetype="image"/></inline-formula> that are active is <italic>θ</italic>. In other words, <italic>θ</italic> is a hyperparameter of the model that sets how sparse the hidden layer activity is.</p>
<p>The hidden layer activity is then passed through the output weights to form the barcode activity generated by the feedforward model:
<disp-formula id="ueqn7">
<graphic xlink:href="612073v3_ueqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>C</italic> is a constant chosen such that the sparsity of <inline-formula><inline-graphic xlink:href="612073v3_inline46.gif" mime-subtype="gif" mimetype="image"/></inline-formula> matches the sparsity of barcodes generated in the default RNN barcode model.</p>
</sec>
<sec id="s5h">
<title>Alternative model: place encoding with Gaussian input weights</title>
<p>We simulate a version of the model with more realistic and complex place inputs. We generated the input currents to the RNN units according to a 0-mean multivariate Gaussian process. The statistics of the Gaussian process are chosen such that the covariance of the inputs to RNN unit <italic>i</italic> and unit <italic>j</italic> is an exponentially decaying function of their minimum spatial distance <italic>d</italic> around the circular arena: <inline-formula><inline-graphic xlink:href="612073v3_inline47.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. All other details of this alternative model is the same as in the default.</p>
</sec>
<sec id="s5i">
<title>Code</title>
<p>Code is publicly available on <ext-link ext-link-type="uri" xlink:href="https://github.com/chingf/barcodes">https://github.com/chingf/barcodes</ext-link>.</p>
</sec>
<sec id="s5j">
<title>Parameter values</title>
<table-wrap id="utbl1" orientation="portrait" position="float">
<graphic xlink:href="612073v3_utbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
<sec id="s6">
<title>Supplementary Figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1:</label>
<caption><p><bold>A</bold>. Difference between RNN activity across each step of recurrent dynamics. Specifically, at recurrent timestep <italic>t</italic>, we plot <inline-formula><inline-graphic xlink:href="612073v3_inline51.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Each line corresponds to a different random seed and model initialization. <bold>B</bold>. Difference between RNN activity at some step of recurrent dynamics and the initial RNN state. Specifically, at recurrent timestep <italic>t</italic>, we plot <inline-formula><inline-graphic xlink:href="612073v3_inline52.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Each line corresponds to a different random seed and model initialization. <bold>C</bold>. As in <xref rid="fig2" ref-type="fig">Figure 2GI</xref>, but for 20 additional randomly sampled units.</p></caption>
<graphic xlink:href="612073v3_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2:</label>
<caption><p><bold>A</bold>. Diagram of the feedforward model for barcode generation. Place inputs <inline-formula><inline-graphic xlink:href="612073v3_inline53.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are passed through an expansion layer <italic>W</italic><sub><italic>h</italic></sub> with a nonlinear activation function to get the hidden layer activity <italic>x</italic><sub><italic>h</italic></sub>. A subsequent compression layer <italic>W</italic><sub><italic>o</italic></sub> with a nonlinear activation function generates the final barcode activity <italic>x</italic>. <bold>B</bold>. As in <xref rid="fig2" ref-type="fig">Figure 2B</xref> but adding in red the results for the feddforward model as sparsity is varied. <bold>C</bold>. As in <xref rid="fig2" ref-type="fig">Figure 2GI</xref>, but for 16 randomly sampled units from the feedforward model.</p></caption>
<graphic xlink:href="612073v3_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3:</label>
<caption><p><bold>A</bold>. True positive rate of the seed output in the 3-cache task of <xref rid="fig4" ref-type="fig">Figure 4</xref>, as the choice of binarized threshold is varied. The color of each line corresponds to the recall strength <italic>s</italic>. Here, caches are all spaced at least two sites apart. <bold>B</bold>. As in (A), but for true negative rate. We will choose a threshold of 0.5 to binarize the seed output. <bold>CD</bold> As in (AB), but the order caches are made is random. <bold>E</bold>. True positive rate of the seed output binarized at 0.5 when plasticity bias <italic>β</italic> and search strength <italic>s</italic> is varied. <bold>F</bold>. As in (E) but for true negative rate. <bold>G</bold>. As in (E) but plotting the correct reject rate at a location between two caches. Here the two caches are 1.5 sites apart or closer. <bold>H</bold>. As in (E) but plotting the probability of recalling the location of the closest cache when the animal is 2 sites away from it.</p></caption>
<graphic xlink:href="612073v3_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4:</label>
<caption><p><bold>A</bold>. Diagram showing how place inputs are modified to be more realistic. Place encoding <inline-formula><inline-graphic xlink:href="612073v3_inline54.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is now drawn from a 0-mean Gaussian process with place-like covariance structure. This results in a more distributed representation in <inline-formula><inline-graphic xlink:href="612073v3_inline55.gif" mime-subtype="gif" mimetype="image"/></inline-formula> over spatial locations. Furthermore, <italic>J</italic><sub><italic>xp</italic></sub> is now a random Gaussian matrix (instead of the identity matrix). <bold>B</bold>. Correlations between place inputs at different locations, as a function of their distance. <bold>C</bold>. As in <xref rid="figS3" ref-type="fig">Figure S3A</xref>, but for the Gaussian model. <bold>D</bold>. As in <xref rid="figS3" ref-type="fig">Figure S3B</xref>, but for the Gaussian model. <bold>E</bold>. As in <xref rid="fig4" ref-type="fig">Figure 4E</xref>, but for the Gaussian model. <bold>F</bold>. As in <xref rid="fig4" ref-type="fig">Figure 4F</xref>, but for the Gaussian model.</p></caption>
<graphic xlink:href="612073v3_figS4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5:</label>
<caption><p><bold>AB</bold>. As in <xref rid="figS3" ref-type="fig">Figure S3AB</xref>, but for the place code only model shown in 5A. <bold>C</bold>. Probability that the <italic>N</italic>th cache location is successfully recalled as site spacing is increased in the 3 cache task. Results are shown for the place code only model. <bold>D</bold>. As in <xref rid="fig4" ref-type="fig">Figure 4F</xref>, but for the place code only model. <bold>EFGH</bold>. As in (ABCD), but for the barcode only model.</p></caption>
<graphic xlink:href="612073v3_figS5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6:</label>
<caption><p><bold>A</bold>. The predictive weights <italic>M</italic> that is added to <italic>J</italic>. <bold>B</bold>. As in (A), but zooming in 10x into the matrix for clarity. <bold>C</bold>. As in <xref rid="fig6" ref-type="fig">Figure 6E</xref>, but for 24 additional randomly sampled units.</p></caption>
<graphic xlink:href="612073v3_figS6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Figure S7:</label>
<caption><p><bold>AB</bold>. As in <xref rid="figS3" ref-type="fig">Figure S3AB</xref>, but for the model with predictive weights added in. <bold>CD</bold>. As in <xref rid="fig4" ref-type="fig">Figure 4EF</xref>, but the model with predictive weights added in.</p></caption>
<graphic xlink:href="612073v3_figS7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alvarez</surname>, <given-names>P.</given-names></string-name> and <string-name><given-names>L. R.</given-names> <surname>Squire</surname></string-name></person-group> (<year>1994</year>). <article-title>Memory consolidation and the medial temporal lobe: a simple network model</article-title>. <source>Proceedings of the national academy of sciences</source> <volume>91</volume>(<issue>15</issue>), <fpage>7041</fpage>–<lpage>7045</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Applegate</surname>, <given-names>M. C.</given-names></string-name> and <string-name><given-names>D.</given-names> <surname>Aronov</surname></string-name></person-group> (<year>2022</year>). <article-title>Flexible use of memory by food-caching birds</article-title>. <source>Elife</source> <volume>11</volume>, <elocation-id>e70600</elocation-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Applegate</surname>, <given-names>M. C.</given-names></string-name>, <string-name><given-names>K. S.</given-names> <surname>Gutnichenko</surname></string-name>, <string-name><given-names>E. L.</given-names> <surname>Mackevicius</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Aronov</surname></string-name></person-group> (<year>2023</year>). <article-title>An entorhinal-like region in food-caching birds</article-title>. <source>Current Biology</source> <volume>33</volume>(<issue>12</issue>), <fpage>2465</fpage>–<lpage>2477</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Babadi</surname>, <given-names>B.</given-names></string-name> and <string-name><given-names>H.</given-names> <surname>Sompolinsky</surname></string-name></person-group> (<year>2014</year>). <article-title>Sparseness and expansion in sensory representations</article-title>. <source>Neuron</source> <volume>83</volume>(<issue>5</issue>), <fpage>1213</fpage>–<lpage>1226</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Banino</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>A. P.</given-names> <surname>Badia</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Köster</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Chadwick</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Zambaldi</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Hassabis</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Barry</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Botvinick</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Kumaran</surname></string-name>, and <string-name><given-names>C.</given-names> <surname>Blundell</surname></string-name></person-group> (<year>2020</year>). <article-title>Memo: A deep network for flexible combination of episodic memories</article-title>. <source>arXiv</source> arXiv:<pub-id pub-id-type="arxiv">2001.10913</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barron</surname>, <given-names>H. C.</given-names></string-name>, <string-name><given-names>R.</given-names> <surname>Auksztulewicz</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Friston</surname></string-name></person-group> (<year>2020</year>). <article-title>Prediction and memory: A predictive coding account</article-title>. <source>Progress in neurobiology</source> <volume>192</volume>, <fpage>101821</fpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blum</surname>, <given-names>K.</given-names></string-name> and <string-name><given-names>L.</given-names> <surname>Abbott</surname></string-name></person-group> (<year>1996</year>). <article-title>A model of spatial map formation in the hippocampus of the rat</article-title>. <source>Neural Computation</source> <volume>8</volume>(<issue>1</issue>), <fpage>85</fpage>–<lpage>93</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bricken</surname>, <given-names>T.</given-names></string-name> and <string-name><given-names>C.</given-names> <surname>Pehlevan</surname></string-name></person-group> (<year>2021</year>). <article-title>Attention approximates sparse distributed memory</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>34</volume>, <fpage>15301</fpage>–<lpage>15315</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chettih</surname>, <given-names>S. N.</given-names></string-name>, <string-name><given-names>E. L.</given-names> <surname>Mackevicius</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Hale</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Aronov</surname></string-name></person-group> (<year>2024</year>). <article-title>Barcoding of episodic memories in the hippocampus of a food-caching bird</article-title>. <source>Cell</source> <volume>187</volume>(<issue>8</issue>), <fpage>1922</fpage>–<lpage>1935</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clayton</surname>, <given-names>N. S.</given-names></string-name> and <string-name><given-names>A.</given-names> <surname>Dickinson</surname></string-name></person-group> (<year>1998</year>). <article-title>Episodic-like memory during cache recovery by scrub jays</article-title>. <source>Nature</source> <volume>395</volume>(<issue>6699</issue>), <fpage>272</fpage>–<lpage>274</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dasgupta</surname>, <given-names>I.</given-names></string-name> and <string-name><given-names>S. J.</given-names> <surname>Gershman</surname></string-name></person-group> (<year>2021</year>). <article-title>Memory as a computational resource</article-title>. <source>Trends in cognitive sciences</source> <volume>25</volume>(<issue>3</issue>), <fpage>240</fpage>–<lpage>251</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fang</surname>, <given-names>C.</given-names></string-name>, <string-name><given-names>D.</given-names> <surname>Aronov</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Abbott</surname></string-name>, and <string-name><given-names>E. L.</given-names> <surname>Mackevicius</surname></string-name></person-group> (<year>2023</year>). <article-title>Neural learning rules for generating flexible predictions and computing the successor representation</article-title>. <source>elife</source> <volume>12</volume>, <elocation-id>e80680</elocation-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Földiak</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1990</year>). <article-title>Forming sparse representations by local anti-hebbian learning</article-title>. <source>Biological cybernetics</source> <volume>64</volume>(<issue>2</issue>), <fpage>165</fpage>–<lpage>170</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gardner-Medwin</surname>, <given-names>A.</given-names></string-name></person-group> (<year>1976</year>). <article-title>The recall of events through the learning of associations between their parts</article-title>. <source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source> <volume>194</volume>(<issue>1116</issue>), <fpage>375</fpage>–<lpage>402</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Graves</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>G.</given-names> <surname>Wayne</surname></string-name>, and <string-name><given-names>I.</given-names> <surname>Danihelka</surname></string-name></person-group> (<year>2014</year>). <article-title>Neural turing machines</article-title>. <source>arXiv</source> arXiv:<pub-id pub-id-type="arxiv">1410.5401</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Graves</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>G.</given-names> <surname>Wayne</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Reynolds</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Harley</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Danihelka</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Grabska-Barwińska</surname></string-name>, <string-name><given-names>S. G.</given-names> <surname>Colmenarejo</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Grefenstette</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Ramalho</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Agapiou</surname></string-name>, <etal>et al.</etal></person-group> (<year>2016</year>). <article-title>Hybrid computing using a neural network with dynamic external memory</article-title>. <source>Nature</source> <volume>538</volume>(<issue>7626</issue>), <fpage>471</fpage>–<lpage>476</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hasselmo</surname>, <given-names>M. E.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Neuromodulation: acetylcholine and memory consolidation</article-title>. <source>Trends in cognitive sciences</source> <volume>3</volume>(<issue>9</issue>), <fpage>351</fpage>–<lpage>359</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hasselmo</surname>, <given-names>M. E.</given-names></string-name></person-group> (<year>2006</year>). <article-title>The role of acetylcholine in learning and memory</article-title>. <source>Current opinion in neurobiology</source> <volume>16</volume>(<issue>6</issue>), <fpage>710</fpage>–<lpage>715</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hopfield</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>1982</year>). <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the national academy of sciences</source> <volume>79</volume>(<issue>8</issue>), <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Howard</surname>, <given-names>M. W.</given-names></string-name>, <string-name><given-names>M. S.</given-names> <surname>Fotedar</surname></string-name>, <string-name><given-names>A. V.</given-names> <surname>Datey</surname></string-name>, and <string-name><given-names>M. E.</given-names> <surname>Hasselmo</surname></string-name></person-group> (<year>2005</year>). <article-title>The temporal context model in spatial navigation and relational learning: toward a common explanation of medial temporal lobe function across domains</article-title>. <source>Psychological review</source> <volume>112</volume>(<issue>1</issue>), <fpage>75</fpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Howard</surname>, <given-names>M. W.</given-names></string-name> and <string-name><given-names>M. J.</given-names> <surname>Kahana</surname></string-name></person-group> (<year>2002</year>). <article-title>A distributed representation of temporal context</article-title>. <source>Journal of mathematical psychology</source> <volume>46</volume>(<issue>3</issue>), <fpage>269</fpage>–<lpage>299</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kahana</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>1984</year>). <article-title>Computational models of memory search</article-title>. <source>Annual Review of Psychology</source> <volume>71</volume>, <fpage>107</fpage>–<lpage>138</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kahana</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Associative retrieval processes in free recall</article-title>. <source>Memory &amp; cognition</source> <volume>24</volume>(<issue>1</issue>), <fpage>103</fpage>–<lpage>109</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kanerva</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1988</year>). <source>Sparse distributed memory</source>. <publisher-name>MIT press</publisher-name>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kozachkov</surname>, <given-names>L.</given-names></string-name>, <string-name><given-names>K. V.</given-names> <surname>Kastanenka</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Krotov</surname></string-name></person-group> (<year>2023</year>). <article-title>Building transformers from neurons and astrocytes</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>120</volume>(<issue>34</issue>), <fpage>e2219150120</fpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Krotov</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>J.</given-names> <surname>Hopfield</surname></string-name></person-group> (<year>2020</year>). <article-title>Large associative memory problem in neurobiology and machine learning</article-title>. <source>arXiv</source> arXiv:<pub-id pub-id-type="arxiv">2008.06996</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krotov</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>J. J.</given-names> <surname>Hopfield</surname></string-name></person-group> (<year>2016</year>). <article-title>Dense associative memory for pattern recognition</article-title>. <source>Advances in neural information processing systems</source> <volume>29</volume>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Le</surname>, <given-names>H.</given-names></string-name>, <string-name><given-names>T.</given-names> <surname>Tran</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Venkatesh</surname></string-name></person-group> (<year>2019</year>). <article-title>Neural stored-program memory</article-title>. <source>arXiv</source> arXiv:<pub-id pub-id-type="arxiv">1906.08862</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lisman</surname>, <given-names>J.</given-names></string-name> and <string-name><given-names>A. D.</given-names> <surname>Redish</surname></string-name></person-group> (<year>2009</year>). <article-title>Prediction, sequences and the hippocampus</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>364</volume>(<issue>1521</issue>), <fpage>1193</fpage>–<lpage>1201</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lovett-Barron</surname>, <given-names>M. e. a.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Dendritic inhibition in the hippocampus supports fear learning</article-title>. <source>Science</source> <volume>343</volume>(<issue>6173</issue>), <fpage>857</fpage>–<lpage>863</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Marr</surname>, <given-names>D.</given-names></string-name>, <string-name><given-names>D.</given-names> <surname>Willshaw</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>McNaughton</surname></string-name></person-group> (<year>1991</year>). <chapter-title>Simple memory: a theory for archicortex</chapter-title>. In <source>From the Retina to the Neocortex</source>, pp. <fpage>59</fpage>–<lpage>128</lpage>. <publisher-name>Springer</publisher-name>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McNaughton</surname>, <given-names>B. L.</given-names></string-name> and <string-name><given-names>R. G.</given-names> <surname>Morris</surname></string-name></person-group> (<year>1987</year>). <article-title>Hippocampal synaptic enhancement and information storage within a distributed memory system</article-title>. <source>Trends in neurosciences</source> <volume>10</volume>(<issue>10</issue>), <fpage>408</fpage>–<lpage>415</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mehta</surname>, <given-names>M. R.</given-names></string-name>, <string-name><given-names>C. A.</given-names> <surname>Barnes</surname></string-name>, and <string-name><given-names>B. L.</given-names> <surname>McNaughton</surname></string-name></person-group> (<year>1997</year>). <article-title>Experience-dependent, asymmetric expansion of hippocampal place fields</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>94</volume>(<issue>16</issue>), <fpage>8918</fpage>–<lpage>8921</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mehta</surname>, <given-names>M. R.</given-names></string-name>, <string-name><given-names>M. C.</given-names> <surname>Quirk</surname></string-name>, and <string-name><given-names>M. A.</given-names> <surname>Wilson</surname></string-name></person-group> (<year>2000</year>). <article-title>Experience-dependent asymmetric shape of hippocampal receptive fields</article-title>. <source>Neuron</source> <volume>25</volume>(<issue>3</issue>), <fpage>707</fpage>–<lpage>715</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Muller</surname>, <given-names>R. U.</given-names></string-name> and <string-name><given-names>J. L.</given-names> <surname>Kubie</surname></string-name></person-group> (<year>1989</year>). <article-title>The firing of hippocampal place cells predicts the future position of freely moving rats</article-title>. <source>Journal of Neuroscience</source> <volume>9</volume>(<issue>12</issue>), <fpage>4101</fpage>–<lpage>4110</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Muscinelli</surname>, <given-names>S. P.</given-names></string-name>, <string-name><given-names>M. J.</given-names> <surname>Wagner</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Litwin-Kumar</surname></string-name></person-group> (<year>2023</year>). <article-title>Optimal routing to cerebellum-like structures</article-title>. <source>Nature neuroscience</source> <volume>26</volume>(<issue>9</issue>), <fpage>1630</fpage>–<lpage>1641</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naim</surname>, <given-names>M.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Katkov</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Romani</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name></person-group> (<year>2020</year>). <article-title>Fundamental law of memory recall</article-title>. <source>Physical Reveiw Letters</source> <volume>124</volume>(<issue>1</issue>), <fpage>018101</fpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> and <string-name><given-names>D. J.</given-names> <surname>Field</surname></string-name></person-group> (<year>1996</year>). <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source> <volume>381</volume>(<issue>6583</issue>), <fpage>607</fpage>–<lpage>609</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Payne</surname>, <given-names>H. L.</given-names></string-name>, <string-name><given-names>G. F.</given-names> <surname>Lynch</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Aronov</surname></string-name></person-group> (<year>2021</year>). <article-title>Neural representations of space in the hippocampus of a food-caching bird</article-title>. <source>Science</source> <volume>373</volume>(<issue>6552</issue>), <fpage>343</fpage>–<lpage>348</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ramsauer</surname>, <given-names>H.</given-names></string-name>, <string-name><given-names>B.</given-names> <surname>Schäfl</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Lehner</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Seidl</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Widrich</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Adler</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Gruber</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Holzleitner</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Pavlović</surname></string-name>, <string-name><given-names>G. K.</given-names> <surname>Sandve</surname></string-name>, <etal>et al.</etal></person-group> (<year>2020</year>). <article-title>Hopfield networks is all you need</article-title>. <source>arXiv</source> arXiv:<pub-id pub-id-type="arxiv">2008.02217</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sacouto</surname>, <given-names>L.</given-names></string-name> and <string-name><given-names>A.</given-names> <surname>Wichert</surname></string-name></person-group> (<year>2023</year>). <article-title>Competitive learning to generate sparse representations for associative memory</article-title>. <source>Neural Networks</source> <volume>168</volume>, <fpage>32</fpage>–<lpage>43</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schapiro</surname>, <given-names>A. C.</given-names></string-name>, <string-name><given-names>N. B.</given-names> <surname>Turk-Browne</surname></string-name>, <string-name><given-names>M. M.</given-names> <surname>Botvinick</surname></string-name>, and <string-name><given-names>K. A.</given-names> <surname>Norman</surname></string-name></person-group> (<year>2017</year>). <article-title>Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>372</volume>(<issue>1711</issue>), <fpage>20160049</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sederberg</surname>, <given-names>P. B.</given-names></string-name>, <string-name><given-names>M. W.</given-names> <surname>Howard</surname></string-name>, and <string-name><given-names>M. J.</given-names> <surname>Kahana</surname></string-name></person-group> (<year>2008</year>). <article-title>A context-based theory of recency and contiguity in free recall</article-title>. <source>Psychological review</source> <volume>115</volume>(<issue>4</issue>), <fpage>893</fpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sherry</surname>, <given-names>D.</given-names></string-name></person-group> (<year>1984</year>). <article-title>Food storage by black-capped chickadees: Memory for the location and contents of caches</article-title>. <source>Animal Behaviour</source> <volume>32</volume>(<issue>2</issue>), <fpage>451</fpage>–<lpage>464</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sompolinsky</surname>, <given-names>H.</given-names></string-name>, <string-name><given-names>A.</given-names> <surname>Crisanti</surname></string-name>, and <string-name><given-names>H. J.</given-names> <surname>Sommers</surname></string-name></person-group> (<year>1988</year>, <month>Jul</month>). <article-title>Chaos in random neural networks</article-title>. <source>Phys. Rev. Lett</source>. <volume>61</volume>, <fpage>259</fpage>–<lpage>262</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stachenfeld</surname>, <given-names>K.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Botvinick</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Gershman</surname></string-name></person-group> (<year>2017</year>). <article-title>The hippocampus as a predictive map</article-title>. <source>Nature Neuroscience</source>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sukhbaatar</surname>, <given-names>S.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Weston</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Fergus</surname></string-name>, <etal>et al.</etal></person-group> (<year>2015</year>). <article-title>End-to-end memory networks</article-title>. <source>Advances in neural information processing systems</source> <volume>28</volume>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Talmi</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>M.</given-names> <surname>Moscovitch</surname></string-name></person-group> (<year>2004</year>). <article-title>Can semantic relatedness explain the enhancement of memory for emotional words?</article-title> <source>Memory &amp; cognition</source> <volume>32</volume>, <fpage>742</fpage>–<lpage>751</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teyler</surname>, <given-names>T. J.</given-names></string-name> and <string-name><given-names>P.</given-names> <surname>DiScenna</surname></string-name></person-group> (<year>1986</year>). <article-title>The hippocampal memory indexing theory</article-title>. <source>Behavioral neuroscience</source> <volume>100</volume>(<issue>2</issue>), <fpage>147</fpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teyler</surname>, <given-names>T. J.</given-names></string-name> and <string-name><given-names>J. W.</given-names> <surname>Rudy</surname></string-name></person-group> (<year>2007</year>). <article-title>The hippocampal indexing theory and episodic memory: updating the index</article-title>. <source>Hippocampus</source> <volume>17</volume>(<issue>12</issue>), <fpage>1158</fpage>–<lpage>1169</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsodyks</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Attractor neural network models of spatial maps in hippocampus</article-title>. <source>Hippocampus</source> <volume>9</volume>(<issue>4</issue>), <fpage>481</fpage>–<lpage>489</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tulving</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal></person-group> (<year>1972</year>). <article-title>Episodic and semantic memory</article-title>. <source>Organization of memory</source> <volume>1</volume>(<issue>381-403</issue>), <fpage>1</fpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tyulmankov</surname>, <given-names>D.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Fang</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Vadaparty</surname></string-name>, and <string-name><given-names>G. R.</given-names> <surname>Yang</surname></string-name></person-group> (<year>2021</year>). <article-title>Biological learning in key-value memory networks</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>34</volume>, <fpage>22247</fpage>–<lpage>22258</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whittington</surname>, <given-names>J. C.</given-names></string-name>, <string-name><given-names>T. H.</given-names> <surname>Muller</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Mark</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Barry</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Burgess</surname></string-name>, and <string-name><given-names>T. E.</given-names> <surname>Behrens</surname></string-name></person-group> (<year>2020</year>). <article-title>The tolmaneichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation</article-title>. <source>Cell</source> <volume>183</volume>(<issue>5</issue>), <fpage>1249</fpage>–<lpage>1263</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Whittington</surname>, <given-names>J. C.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Warren</surname></string-name>, and <string-name><given-names>T. E.</given-names> <surname>Behrens</surname></string-name></person-group> (<year>2021</year>). <article-title>Relating transformers to models and neural representations of the hippocampal formation</article-title>. <source>arXiv</source> arXiv:<pub-id pub-id-type="arxiv">2112.04035</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname>, <given-names>M. A.</given-names></string-name> and <string-name><given-names>B. L.</given-names> <surname>McNaughton</surname></string-name></person-group> (<year>1993</year>). <article-title>Dynamics of the hippocampal ensemble code for space</article-title>. <source>Science</source> <volume>261</volume>(<issue>5124</issue>), <fpage>1055</fpage>–<lpage>1058</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xiang</surname>, <given-names>Z.</given-names></string-name>, <string-name><given-names>J. R.</given-names> <surname>Huguenard</surname></string-name>, and <string-name><given-names>D. A.</given-names> <surname>Prince</surname></string-name></person-group> (<year>1998</year>). <article-title>Cholinergic switching within neocortical inhibitory networks</article-title>. <source>Science</source> <volume>281</volume>(<issue>5379</issue>), <fpage>985</fpage>–<lpage>988</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xie</surname>, <given-names>M.</given-names></string-name>, <string-name><given-names>S. P.</given-names> <surname>Muscinelli</surname></string-name>, <string-name><given-names>K. D.</given-names> <surname>Harris</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Litwin-Kumar</surname></string-name></person-group> (<year>2023</year>). <article-title>Task-dependent optimal representations for cerebellar learning</article-title>. <source>Elife</source> <volume>12</volume>, <elocation-id>e82914</elocation-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103512.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> work substantially advances our understanding of episodic memory by proposing a biologically plausible mechanism through which hippocampal barcode activity enables efficient memory binding and flexible recall. The evidence supporting the conclusions is <bold>convincing</bold>, with rigorously validated computational models and alignment with experimental findings. The work will be of broad interest to neuroscientists and computational modelers studying memory and hippocampal function.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103512.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this paper, the authors develop a biologically plausible recurrent neural network model to explain how the hippocampus generates and uses barcode-like activity to support episodic memory. They address key questions raised by recent experimental findings: how barcodes are generated, how they interact with memory content (such as place and seed-related activity), and how the hippocampus balances memory specificity with flexible recall. The authors demonstrate that chaotic dynamics in a recurrent neural network can produce barcodes that reduce memory interference, complement place tuning, and enable context-dependent memory retrieval, while aligning their model with observed hippocampal activity during caching and retrieval in chickadees.</p>
<p>Strengths:</p>
<p>(1) The manuscript is well-written and structured.</p>
<p>
(2) The paper provides a detailed and biologically plausible mechanism for generating and utilizing barcode activity through chaotic dynamics in a recurrent neural network. This mechanism effectively explains how barcodes reduce memory interference, complement place tuning, and enable flexible, context-dependent recall.</p>
<p>
(3) The authors successfully reproduce key experimental findings on hippocampal barcode activity from chickadee studies, including the distinct correlations observed during caching, retrieval, and visits.</p>
<p>
(4) Overall, the study addresses a somewhat puzzling question about how memory indices and content signals coexist and interact in the same hippocampal population. By proposing a unified model, it provides significant conceptual clarity.</p>
<p>Weaknesses:</p>
<p>The recurrent neural network model incorporates assumptions and mechanisms, such as the modulation of recurrent input strength, whose biological underpinnings remain unclear. The authors acknowledge some of these limitations thoughtfully, offering plausible mechanisms and discussing their implications in depth.</p>
<p>One thread of questions that authors may want to further explore is related to the chaotic nature of activity that generates barcodes when recurrence is strong. Chaos inherently implies sensitivity to initial conditions and noise, which raises questions about its reliability as a mechanism for producing robust and repeatable barcode signals. How sensitive are the results to noise in both the dynamics and the input signals? Does this sensitivity affect the stability of the generated barcodes and place fields, potentially disrupting their functional roles? Moreover, does the implemented plasticity mitigate some of this chaos, or might it amplify it under certain conditions? Clarifying these aspects could strengthen the argument for the robustness of the proposed mechanism.</p>
<p>It may also be worth exploring the robustness of the results to certain modeling assumptions. For instance, the choice to run the network for a fixed amount of time and then use the activity at the end for plasticity could be relaxed.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103512.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Striking experimental results by Chettih et al 2024 have identified high-dimensional, sparse patterns of activity in the chickadee hippocampus when birds store or retrieve food at a given site. These barcode-like patterns were interpreted as &quot;indexes&quot; allowing the birds to retrieve from memory the locations of stored food.</p>
<p>
The present manuscript proposes a recurrent network model that generates such barcode activity and uses it to form attractor-like memories that bind information about location and food. The manuscript then examines the computational role of barcode activity in the model by simulating two behavioral tasks, and by comparing the model with an alternate model in which barcode activity is ablated.</p>
<p>Strengths of the study:</p>
<p>- Proposes a potential neural implementation for the indexing theory of episodic memory</p>
<p>
- Provides a mechanistic model of striking experimental findings: barcode-like, sparse patterns of activity when birds store a grain at a specific location</p>
<p>
- A particularly interesting aspect of the model is that it proposes a mechanism for binding discrete events to a continuous spatial map, and demonstrates the computational advantages of this mechanism</p>
<p>Weaknesses:</p>
<p>- The relation between the model and experimentally recorded activity needs some clarification</p>
<p>
- The relation with indexing theory could be made more clear</p>
<p>
- The importance of different modeling ingredients and dynamical mechanisms could be made more clear</p>
<p>
- The paper would be strengthened by focusing on the most essential aspects</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103512.1.sa3</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Fang</surname>
<given-names>Ching</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3653-0057</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Lindsey</surname>
<given-names>Jack</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Abbott</surname>
<given-names>Larry F</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Aronov</surname>
<given-names>Dmitriy</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chettih</surname>
<given-names>Selmaan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2045-3747</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the reviewers for the thoughtful comments, and we hope to address these issues in a future revision. We will clarify that chaos only serves to generate barcodes, and show that once they are formed and assigned the memory mechanism is stable to initial conditions.  We will also clarify the model's assumptions and its connections to indexing theory and to experimental results.</p>
</body>
</sub-article>
</article>