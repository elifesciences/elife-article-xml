<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">88683</article-id>
<article-id pub-id-type="doi">10.7554/eLife.88683</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.88683.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Spectrally and temporally resolved estimation of neural signal diversity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1789-5894</contrib-id>
<name>
<surname>Mediano</surname>
<given-names>Pedro A.M.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<xref ref-type="corresp" rid="cor1">‚Ä†</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7790-6183</contrib-id>
<name>
<surname>Rosas</surname>
<given-names>Fernando E.</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<xref ref-type="corresp" rid="cor1">‚Ä†</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3461-6431</contrib-id>
<name>
<surname>Luppi</surname>
<given-names>Andrea I.</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0492-6954</contrib-id>
<name>
<surname>Noreika</surname>
<given-names>Valdas</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1421-6051</contrib-id>
<name>
<surname>Seth</surname>
<given-names>Anil K.</given-names>
</name>
<xref ref-type="aff" rid="a10">10</xref>
<xref ref-type="aff" rid="a11">11</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6062-7150</contrib-id>
<name>
<surname>Carhart-Harris</surname>
<given-names>Robin L.</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a12">12</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Barnett</surname>
<given-names>Lionel</given-names>
</name>
<xref ref-type="aff" rid="a10">10</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0741-8157</contrib-id>
<name>
<surname>Bor</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Computing, Imperial College London</institution>, <country>UK</country></aff>
<aff id="a2"><label>2</label><institution>Department of Psychology, University of Cambridge</institution>, <country>UK</country></aff>
<aff id="a3"><label>3</label><institution>Department of Informatics, University of Sussex</institution>, <country>UK</country></aff>
<aff id="a4"><label>4</label><institution>Centre for Psychedelic Research, Department of Brain Sciences, Imperial College London</institution>, <country>UK</country></aff>
<aff id="a5"><label>5</label><institution>Centre for Complexity Science, Imperial College London</institution>, <country>UK</country></aff>
<aff id="a6"><label>6</label><institution>Centre for Eudaimonia and Human Flourishing, University of Oxford</institution>, <country>UK</country></aff>
<aff id="a7"><label>7</label><institution>Division of Anaesthesia, School of Clinical Medicine, University of Cambridge</institution>, <country>UK</country></aff>
<aff id="a8"><label>8</label><institution>Montreal Neurological Institute, McGill University</institution>, <country>Canada</country></aff>
<aff id="a9"><label>9</label><institution>Department of Psychology, Queen Mary University of London</institution>, <country>UK</country></aff>
<aff id="a10"><label>10</label><institution>Sussex Centre for Consciousness Science, Department of Informatics, University of Sussex</institution>, <country>UK</country></aff>
<aff id="a11"><label>11</label><institution>CIFAR Program on Brain, Mind, and Consciousness</institution>, <country>Canada</country></aff>
<aff id="a12"><label>12</label><institution>Psychedelics Division, Neuroscape, University of California San Francisco</institution>, San Francisco, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Wyart</surname>
<given-names>Valentin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Inserm</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>‚Ä†</label><bold>For correspondence:</bold> <email>pmediano@imperial.ac.uk</email> (PM); <email>f.rosas@sussex.ac.uk</email> (FR)</corresp>
<fn id="n1" fn-type="equal"><label>*</label><p>These authors contributed equally to this work</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-09-12">
<day>12</day>
<month>09</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP88683</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-05-12">
<day>12</day>
<month>05</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-03-31">
<day>31</day>
<month>03</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.30.534922"/>
</event>
</pub-history>
<permissions>
<copyright-statement>¬© 2023, Mediano et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Mediano et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-88683-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Quantifying the complexity of neural activity has provided fundamental insights into cognition, consciousness, and clinical conditions. However, the most widely used approach to estimate the complexity of neural dynamics, Lempel-Ziv complexity (LZ), has fundamental limitations that substantially restrict its domain of applicability. In this article we leverage the information-theoretic foundations of LZ to overcome these limitations by introducing a complexity estimator based on state-space models ‚Äîwhich we dub <italic>Complexity via State-space Entropy Rate</italic> (CSER). While having a performance equivalent to LZ in discriminating states of consciousness, CSER boasts two crucial advantages: 1) CSER offers a principled decomposition into spectral components, which allows us to rigorously investigate the relationship between complexity and spectral power; and 2) CSER provides a temporal resolution two orders of magnitude better than LZ, which allows complexity analyses of e.g. event-locked neural signals. As a proof of principle, we use MEG, EEG and ECoG datasets of humans and monkeys to show that CSER identifies the gamma band as the main driver of complexity changes across states of consciousness; and reveals early entropy increases that <italic>precede</italic> the standard ERP in an auditory mismatch negativity paradigm by approximately 20ms. Overall, by overcoming the main limitations of LZ and substantially extending its range of applicability, CSER opens the door to novel investigations on the fine-grained spectral and temporal structure of the signal complexity associated with cognitive processes and conscious states.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://www.github.com/pmediano/entrate">https://www.github.com/pmediano/entrate</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<sec id="s1a">
<title>Complexity measures in neuroscience</title>
<p>Since the advent of complexity science, there has been great interest in its application to understand the brain (<bold><italic><xref ref-type="bibr" rid="c58">Turkheimer et al., 2022</xref></italic></bold>). Lempel and Ziv‚Äôs now-classical complexity measure (<bold><italic><xref ref-type="bibr" rid="c34">Lempel and Ziv, 1976</xref></italic></bold>) has been highly influential and widely applied in the neuroimaging literature, especially for the neuroscientific study of consciousness. LZ complexity is a simple scalar metric that has been consistently shown to be a robust indicator of depth of anaesthesia (<bold><italic><xref ref-type="bibr" rid="c68">Zhang et al., 2001</xref></italic></bold>), disorders of consciousness (<bold><italic><xref ref-type="bibr" rid="c11">Casali et al., 2013</xref></italic></bold>), sleep (<bold><italic><xref ref-type="bibr" rid="c1">Ab√°solo et al., 2015</xref></italic></bold>; <bold><italic>Schartner et al., 2017b</italic></bold>), and, more recently, the psychedelic state (<bold><italic>Schartner et al., 2017a</italic></bold>; <bold><italic><xref ref-type="bibr" rid="c57">Timmermann et al., 2019</xref></italic></bold>; <bold><italic>Mediano et al., 2020b</italic></bold>), making it one of the most effective known functional markers of conscious state in humans. LZ is also effective as a biomarker of mental disorders such as schizophrenia (<bold><italic><xref ref-type="bibr" rid="c31">Ib√°√±ez-Molina et al., 2018</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c51">Rajpal et al., 2022</xref></italic></bold>) and depression (<bold><italic><xref ref-type="bibr" rid="c3">Bachmann et al., 2015</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c2">Akar et al., 2015</xref></italic></bold>), as well as tracking fluctuations of normal wakefulness related to drowsiness (<bold><italic><xref ref-type="bibr" rid="c41">Mediano et al., 2021</xref></italic></bold>), mind wandering (<bold><italic><xref ref-type="bibr" rid="c29">Ib√°√±ez-Molina and Iglesias-Parro, 2014</xref></italic></bold>), or artistic improvisation (<bold><italic><xref ref-type="bibr" rid="c20">Dolan et al., 2018</xref></italic></bold>).</p>
<p>Despite its practical effectiveness and widespread use, LZ has longstanding limitations that severely restrict its domain of applicability. First, the application of LZ requires that the data be discretised, which loses information and introduces artificial non-linearities (<bold><italic><xref ref-type="bibr" rid="c30">Ib√°√±ez-Molina et al., 2015</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c65">Yeh and Shi, 2018</xref></italic></bold>). Second, the LZ algorithm needs to be provided with relatively large windows of data (on the order of a few seconds for typical EEG data), which limits its temporal resolution ‚Äîmaking it impossible to do complexity analyses of non-stationary data, such as the fast time-locked neural events studied with ERP signals. Third, it is not possible to do a principled spectral decomposition of LZ, a fundamental obstacle to our understanding of the relationship between complexity and power spectrum ‚Äîwhich is highly relevant in neuroscience as electrophysiological signals are often differently distributed across several frequency bands.</p>
<p>In this paper we introduce a novel approach to calculate an LZ-style complexity that overcomes all these limitations. To do so, our approach builds on the rich literature of <italic>state-space models</italic>, powerful and versatile statistical time series models widely used in neuroscience and beyond (<bold><italic><xref ref-type="bibr" rid="c21">Durbin and Koopman, 2012</xref></italic></bold>). Our method, which we call <italic>Complexity via State-space Entropy Rate</italic> (CSER), solves all of LZ‚Äôs above-mentioned issues:</p>
<list list-type="bullet">
<list-item><p>CSER does not require the signal to be discretized, allowing it to fully exploit continuous signals and avoiding potential artefacts introduced by the discretization procedure.</p></list-item>
<list-item><p>CSER allows instantaneous (i.e. sample-by-sample) estimation, bringing complexity analyses to the realm of event-related paradigms.</p></list-item>
<list-item><p>CSER has a principled spectral decomposition, closing the gap between complexity and traditional spectral analyses.</p></list-item>
</list>
<p>In the rest of the paper, we first describe the basic intuitions behind the common use of LZ in neuroscience. We then introduce our estimator, CSER, and discuss its tight mathematical links with signal diversity and prediction error (<bold><italic><xref ref-type="bibr" rid="c18">Den Ouden et al., 2012</xref></italic></bold>). Next, we showcase CSER‚Äôs properties in a comparison of different states of consciousness and in data from an auditory mismatch negativity paradigm. Finally, we conclude with a discussion on how to interpret complexity measures applied to neural activity, and note on the applicability of CSER to other neuroimaging modalities.</p>
</sec>
<sec id="s1b">
<title>Complexity and entropy rate</title>
<p>Before we delve into the formulation of our new estimator and the new results obtained with it, it is worth examining the theoretical foundations of LZ as a measure of complexity.</p>
<p>The informational content of a discrete signal ùí≥ = (<italic>X</italic><sub>1</sub>, <italic>X</italic><sub>2</sub>, ‚Ä¶) at time <italic>t</italic> can be quantified by its entropy,
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="534922v1_eqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
which can be seen as measuring the difficulty of guessing the signal‚Äôs value at that point in time‚Äîthe more difficult this is, the more information one gains by learning the signal‚Äôs actual value. If the signal preferentially takes some specific values, then the signal is easier to predict, which corresponds to a low entropy. On the other hand, if the signal takes all of its available values with similar frequency (i.e. its histogram is nearly flat), then there is high uncertainty about its current value and hence observing it will be very informative, which corresponds to a case of high entropy.</p>
<p>Crucially, entropy only takes into account the relative frequency of values, but not the order in which they appear. As an example, consider a signal A corresponding to the sequence 01010101, and a signal B, corresponding to the sequence 01110010. A naive plug-in estimator (<bold><italic><xref ref-type="bibr" rid="c50">Panzeri et al., 2007</xref></italic></bold>) applied to these signals would yield the same entropy for both, as they have the same frequency of 0s and 1s. However, if one knows the past trajectory of the sequence it is clearly much easier to predict the next value of signal A than signal B. This aspect of a signal‚Äôs unpredictability ‚Äîthe difficulty of predicting the next value <italic>after knowing all previous ones</italic> ‚Äîis quantified by the signal‚Äôs <italic>entropy rate</italic>, here denoted by <italic>h</italic>(ùí≥). In the above example, signals A and B have the same entropy, but signal A has lower entropy rate than B. In other words, entropy is invariant to reshuffling the order of elements in a sequence, whereas entropy rate is not ‚Äîmaking the latter a more appropriate measure of dynamical complexity.</p>
<p>Estimating entropy rate in practice from limited data is, however, a challenging task. This is due to several factors, most prominently the <italic>‚Äú</italic>curse of dimensionality<italic>‚Äù</italic> (<bold><italic><xref ref-type="bibr" rid="c49">Paninski, 2003</xref></italic></bold>). This is where the LZ algorithm comes into play.</p>
<p>In essence, the original algorithm by <bold><italic>Lempel and Ziv</italic></bold> (<bold><italic>1976</italic></bold>) breaks up a signal into patterns, and uses the number of distinct patterns to quantify the complexity of that signal. Regular sequences with repeating patterns have low LZ, and rich signals with many patterns have high LZ. A fundamental result by <bold><italic>Ziv</italic></bold> (<bold><italic>1978</italic></bold>) shows that this number of patterns can be used to efficiently estimate the entropy rate of the process that generated the data. Specifically, given a discrete stochastic process<sup><xref ref-type="fn" rid="fn1">1</xref></sup> ùí≥= (<italic>X</italic><sub>1</sub>, <italic>X</italic><sub>2</sub>, ‚Ä¶, <italic>X</italic><sub><italic>T</italic></sub>) with LZ complexity <italic>c</italic>(ùí≥), its entropy rate can be estimated as<sup><xref ref-type="fn" rid="fn2">2</xref></sup>
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="534922v1_eqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Furthermore, the concept of entropy rate allows us to draw a direct connection between the common interpretation of LZ as signal diversity and another important framework in neuroscience: <italic>predictive processing</italic>. In essence, the more diverse a signal is, the higher the prediction error one would incur when trying to forecast its future (see Methods for details).</p>
<p>In this work, we provide evidence that the empirical successes of LZ can be linked directly to the underlying and more general mathematical construct of entropy rate. This opens a way to formulate metrics, based on entropy rate, that are similar to LZ in spirit but do not suffer from the same limitations ‚Äîas we showcase in the remainder of this article.</p>
</sec>
</sec>
<sec id="s2">
<title>Results</title>
<p>Having outlined the core principles behind LZ, and also having described its inherent limitations, we can now introduce our novel estimator, CSER. We demonstrate that CSER preserves the utility of LZ while offering multiple clear advantages over it, illustrated across three sets of results. First, we show that CSER preserves the valuable empirical effectiveness of LZ, and can clearly discriminate between states of consciousness, in different species and different neuroimaging modalities. Then, we illustrate two examples of new analysis methods that can be performed by leveraging the advantageous properties of state-space models, which are not possible with LZ: spectral and temporal decomposition of entropy in neural signals. Mathematical details of CSER and validation analyses on artificial data can be found in the Methods section.</p>
<sec id="s2a">
<title>Estimating entropy rate via state-space models</title>
<p>The core principle of state-space analysis is to assume that the observed data, ùí≥, can be modelled as noisy observations of a hidden process ùíµ that is not accessible to direct measurement (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). Mathematically, a state-space model is fully specified by two ingredients: the dynamics of the hidden state ùíµ (horizontal arrows); and the observation process that relates ùíµ with the observed data ùí≥ (vertical arrows). A simple yet effective choice is to assume that both the dynamics and observation processes are linear and normally distributed. Fitting a state-space model, then, corresponds to finding the linear coefficients and covariance matrices that best describe the relationship between <italic>z</italic><sub><italic>t</italic></sub> and <italic>z</italic><sub><italic>t</italic>+1</sub>, and between <italic>z</italic><sub><italic>t</italic></sub> and <italic>x</italic><sub><italic>t</italic></sub> (see Methods for more details).</p>
<p>The properties of state-space models make it straightforward to calculate entropy rate from a fitted model. In particular, the key property of state-space models is that the hidden state <italic>z</italic><sub><italic>t</italic></sub> con-tains all the relevant information about <italic>x</italic><sub>1</sub> ‚Ä¶, <italic>x</italic><sub><italic>t</italic>‚àí1</sub> needed to predict <italic>x</italic><sub><italic>t</italic></sub>. Then, thanks to the assump-tion of normality and the link between entropyrate and prediction error, entropy rate can be cal-culated with the usual formula of the entropy of a Gaussian distribution (<bold><italic><xref ref-type="bibr" rid="c15">Cover and Thomas, 2006</xref></italic></bold>, Sec. 8.4) ‚Äîresulting in the CSER estimator. See the Methods section for details on state-space models, model selection, and the robustness of CSER to model misspecification.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Diagram of a state-space process.</p></caption>
<graphic xlink:href="534922v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Discriminative power between states of consciousness</title>
<p>The main feature that makes LZ complexity interesting for neuroscientists is its empirical predictive value: for instance, it consistently decreases in states of loss of consciousness such as sleep or general anaesthesia (<bold><italic><xref ref-type="bibr" rid="c69">Zhang et al., 1999</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c11">Casali et al., 2013</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c61">Varley et al., 2020</xref></italic></bold>);<sup><xref ref-type="fn" rid="fn3">3</xref></sup> and increases in states with richer subjective contents of consciousness like the psychedelic state (<bold><italic>Schartner et al., 2017a</italic></bold>; <bold><italic>Mediano et al., 2020b</italic></bold>). Therefore, one would expect that any candidate competing measure of complexity should preserve those desirable empirical properties.</p>
<p>To comprehensively validate this hypothesis, we analyse data of subjects in different well-defined states of consciousness obtained through different neuroimaging modalities. In particular, we use three datasets:</p>
<list list-type="order">
<list-item><p>MEG data from 15 human subjects under the effects of the psychedelic drug LSD, as well as a placebo (PLA) (<bold><italic><xref ref-type="bibr" rid="c10">Carhart-Harris et al., 2016</xref></italic></bold>).</p></list-item>
<list-item><p>EEG data from 9 human subjects in dreamless NREM sleep, as well as a wakeful rest baseline (<bold><italic><xref ref-type="bibr" rid="c63">Wong et al., 2020</xref></italic></bold>).</p></list-item>
<list-item><p>ECoG data from 4 macaque subjects under the effects of ketamine and medetomidine (KTMD) anaesthesia, as well as a wakeful rest baseline (<bold><italic><xref ref-type="bibr" rid="c64">Yanagawa et al., 2013</xref></italic></bold>).</p></list-item>
</list>
<p>In all cases, we compute CSER and LZ (for comparison) for each channel separately following the procedure outlined in the Methods section, and report results averaged across all channels (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). Results show that CSER fully agrees with LZ, increasing in subjects under the effects of LSD, and decreasing in subjects in NREM sleep or general anaesthesia. The effects have the same sign for LZ and CSER across all three datasets, and are significant with <italic>p &lt;</italic> 0.001 for the psychedelics and sleep datasets, and with <italic>p &lt;</italic> 0.05 for the anaesthesia dataset (details in Supplementary Table 1). Furthermore, the spatial distributions across the brain for LZ and CSER show qualitative agreement, especially for the modalities with good spatial resolution (MEG and ECoG). These results confirm that CSER is able to perform the same discriminatory role as LZ on a wide range of datasets and states. We next highlight two clear advantages of using CSER instead of LZ: spectral decomposition and temporal resolution.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>CSER discriminates between states of consciousness.</title>
<p>That is, it preserves the key empirical properties of LZ: entropy increases under the effects of psychedelics (<italic>left</italic>), and decreases in NREM sleep (<italic>middle</italic>) and general anaesthesia (<italic>right</italic>). Top row shows subject-level averages, bottom row shows spatial distributions of LZ and CSER. Given the different locations of ECoG sensors in each subject of the anaesthesia dataset, we show only one subject (Chibi) and use Cohen‚Äôs <italic>d</italic> instead of <italic>t</italic>-scores.</p></caption>
<graphic xlink:href="534922v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<title>Spectral decomposition of entropy changes</title>
<p>Given the increasing prominence of LZ analyses in neuroscience, multiple efforts have been made to elucidate whether, and to what extent, changes in LZ across states of consciousness relate to neural activity in different frequency bands (<bold><italic>Schartner et al., 2017b</italic></bold>; <bold><italic><xref ref-type="bibr" rid="c8">Boncompte et al., 2021</xref></italic></bold>). Answering this question, however, requires sophisticated surrogate data tests (<bold><italic>Mediano et al., 2020a</italic></bold>), and definite answers are yet to be established.</p>
<p>In contrast, the relation between linear models and spectral decomposition is well known (<bold><italic><xref ref-type="bibr" rid="c27">Hannan and Deistler, 2012</xref></italic></bold>), which allows us to perform an exact, analytical decomposition of the entropy rate into frequency components (see Methods for details). Therefore, one can split the complexity of a signal into different frequency bands, with the guarantee that the terms associated with bands that cover the whole spectrum will sum up to the broadband CSER. This allows us to properly interpret their relative and absolute magnitudes, and to attribute changes in entropy rate between conditions to a particular band in a principled manner.<sup><xref ref-type="fn" rid="fn4">4</xref></sup></p>
<p>As a proof of concept, we applied this spectral decomposition to the three datasets used in the previous section using the standard partition of the spectrum into frequency bands (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). Given the guarantee that differences in all spectral components sum up to the total difference, we can conclude that the changes in complexity across states of consciousness are mainly driven (in absolute magnitude) by high-frequency neural activity.</p>
<p>Our results provide formal validation of the long-standing hypotheses linking high-frequency and non-rhythmic signals to consciousness. Gamma oscillations were at the center of early consciousness work by <bold><italic>Crick and Koch</italic></bold> (<bold><italic>1990</italic></bold>),<sup><xref ref-type="fn" rid="fn5">5</xref></sup> and have been posited as the neural underpinning of self-awareness (<bold><italic><xref ref-type="bibr" rid="c37">Lou et al., 2017</xref></italic></bold>). They also align with more recent evidence showing the power of gamma activity in discriminating between states of consciousness (<bold><italic><xref ref-type="bibr" rid="c62">Walter and Hinterberger, 2022</xref></italic></bold>), as well as with results by <bold><italic><xref ref-type="bibr" rid="c9">Canales-Johnson et al. (2019</xref></italic></bold>) showing that it is broadband (rather than rhythmic) components that encode prediction errors, which in turn have been put forward as a tool to understand conscious contents (<bold><italic><xref ref-type="bibr" rid="c28">Hohwy and Seth, 2020</xref></italic></bold>). However, we emphasise that our results are distinct from the known results of decreased gamma coherence with loss of consciousness (e.g. <bold><italic><xref ref-type="bibr" rid="c12">Cavinato et al., 2015</xref></italic></bold>) ‚Äîeven when controlled for average gamma coherence, differences in CSER between conscious states remain qualitatively unchanged (Supp. Table 3).</p>
<p>Interestingly, CSER‚Äôs ability to decompose the complexity of a signal into frequency bands can give us new insights by effectively providing more <italic>‚Äú</italic>dimensions<italic>‚Äù</italic> to study the complexity of neural data. As an example, note that in the comparison between LSD and placebo (<xref rid="fig3" ref-type="fig">Fig. 3</xref>, left), all bands except gamma actually have a small, but significant, negative change in CSER, <italic>opposite</italic> to the overall trend. Future research should disentangle the mechanistic origins of these different complexity changes, and explore if they have different effects on behaviour and subjective experience.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Spectral decomposition of CSER differences across states of consciousness reveals largest contribution from <italic>Œ≥</italic> frequencies.</title>
<p>Plots show mean and standard deviation of subject-level differences in CSER decomposed into frequency bands. Note that in each plot the sum of complexity in all bands equals the total (or broadband) complexity. In all three cases, the largest contribution to the difference is high-frequency neural activity. Frequency bands used are <italic>Œ¥</italic>: 1‚Äì4 Hz, <italic>Œ∏</italic>: 4‚Äì8 Hz, <italic>Œ±</italic>: 8‚Äì14 Hz, <italic>Œ≤</italic>: 14‚Äì25 Hz, <italic>Œ≥</italic>: &gt;25 Hz.</p></caption>
<graphic xlink:href="534922v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>Temporally-resolved entropy measures</title>
<p>So far, LZ has been considered a measure of <italic>state</italic> ‚Äîi.e. a property of the typical dynamics observed in the resting-state spontaneous neural activity of a subject. A few recent studies have attempted to capture a more fine-grained temporal evolution of LZ, typically by using a sliding window, although such approaches suffer from a trade-off between time-resolution and estimation noise. Hence, this method only works when the process under consideration has comparatively slow dynamics.<sup><xref ref-type="fn" rid="fn6">6</xref></sup></p>
<p>This limitation of seeing LZ only as a <italic>‚Äú</italic>state measure<italic>‚Äù</italic> is caused, at least in part, by the requirement of large sequence lengths for enabling a robust estimation (a few thousand samples, or several seconds for EEG; c.f. <xref rid="fig5" ref-type="fig">Fig. 5</xref>). However, leveraging the explicit parametric form of the state-space models that CSER is based on, we can formulate an instantaneous entropy rate that provides an analogue of LZ computable <italic>for each time point</italic> in multi-trial data.</p>
<p>To illustrate this powerful capability of CSER, we analyse ECoG data from macaque monkeys undergoing an auditory oddball task (<bold><italic><xref ref-type="bibr" rid="c33">Komatsu et al., 2015</xref></italic></bold>). The auditory oddball is a passive listening task in which monkeys listen to one of two stimuli: a <italic>‚Äò</italic>standard‚Äô tone which is consistent with prior expectation, and a <italic>‚Äò</italic>deviant‚Äô tone which isn‚Äôt (<xref rid="fig4" ref-type="fig">Fig. 4</xref>, left). One of the most characteristic and widely studied phenomena related to this task is the presence of a <italic>mismatch negativity</italic> (MMN) reponse in the event-related potential (ERP): a strong negative peak following the deviant stimulus (<xref rid="fig4" ref-type="fig">Fig. 4</xref>, top right). The MMN occurs approximately 50 to 100 ms post-stimulus in marmosets (<bold><italic><xref ref-type="bibr" rid="c33">Komatsu et al., 2015</xref></italic></bold>) and 150 to 250 ms in humans (<bold><italic><xref ref-type="bibr" rid="c46">N√§√§t√§nen et al., 2007</xref></italic></bold>), and has been studied in a wide variety of experimental settings.</p>
<p>Importantly, one of the leading theories of the mechanisms behind MMN describes it as representing a violation of the brain‚Äôs predictions of incoming sensory signals ‚Äîin essence, when processing the deviant stimulus the brain incurs a <italic>‚Äò</italic>prediction error‚Äô that results in stronger activity (<bold><italic><xref ref-type="bibr" rid="c25">Garrido et al., 2009</xref></italic></bold>). Given that higher entropy rate is directly linked with a larger prediction error (see Methods), we hypothesised that the deviant stimulus would also elicit an increase in entropy rate (with respect to the standard stimulus) as measured by CSER.</p>
<p>To study the time-resolved entropy rate of this data, we employed the following procedure:</p>
<list list-type="roman-lower">
<list-item><label>i)</label><p>Fit a state-space model to the baseline pre-stimulus data, obtaining a set of model parameters.</p></list-item>
<list-item><label>ii)</label><p>Use the obtained parameters to evaluate the model and make one-step-ahead predictions in standard and deviant time series, estimating the residual time series.</p></list-item>
</list>
<p>With the estimated residuals and the known baseline residual variance we can compute a local (i.e. instantaneous) entropy rate, analogous to the model prediction error at time <italic>t</italic> (see Methods and Appendix for details). Using this method, we can compare the instantaneous entropy during the course of standard and deviant percepts in the macaque ECoG, and compare them to the known ERP traces (<xref rid="fig4" ref-type="fig">Fig. 4</xref>, right).</p>
<p>In agreement with our predictions, deviant stimuli induce a significant increase in prediction error (and thus, entropy rate) with respect to standard stimuli. Furthermore, and perhaps more interestingly, the peak difference in instantaneous entropy rate <italic>precedes</italic> the ERP peak by approximately 20 ms. We speculate that this highly temporally localised entropy peak could represent the onset of the prediction error itself, that steers neural dynamics in different trajectories depending on the nature of the stimulus, while the difference in neural activity (i.e. ERP amplitude) reflects the spontaneous evolution of these two trajectories.<sup><xref ref-type="fn" rid="fn7">7</xref></sup> Naturally, we do not make strong generalisation claims for this phenomenon based on a single subject, although we believe these results warrant further study of the temporal entropy profile of other prediction error-related tasks.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Time-resolved entropy estimates differentiate between time courses of standard and deviant percepts.</title>
<p>(<italic>top left</italic>) Layout of ECoG electrodes overlaid on the monkey‚Äôs cortex, with the selected electrode in red. (<italic>bottom left</italic>) Schematic diagram of experimental paradigm, in which the subject listens to a tone train composed of <italic>‚Äò</italic>standard‚Äô and <italic>‚Äò</italic>deviant‚Äô tones (see text for details). (<italic>right</italic>) Event-related potentials of standard and deviant trials (<italic>top</italic>), and the instantaneous entropy rate difference computed via CSER (<italic>bottom</italic>). Stars represent a significant cluster with <italic>p &lt;</italic> 0.001. Note that the entropy difference precedes the ERP by approximately 20 ms. Original data from <bold><italic><xref ref-type="bibr" rid="c33">Komatsu et al. (2015</xref></italic></bold>) and the <ext-link ext-link-type="uri" xlink:href="http://www.neurotycho.org/">Neurotycho</ext-link> database.</p></caption>
<graphic xlink:href="534922v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>LZ and CSER approximate entropy rate, but CSER converges faster.</title>
<p>(<italic>left</italic>) LZ-estimated entropy rates of discrete signals with different lengths (x-axis) and memory order <italic>q</italic>. Except for low values of <italic>q</italic>, LZ shows a slow convergence to the true entropy rate (black line). (<italic>right</italic>) Similar analysis using CSER and synthetic real-valued signals. CSER converges to the true entropy rate for all values of <italic>q</italic> within approximately 1000 samples, within the range of typical M/EEG datasets. Note the logarithmic scale of the x-axis</p></caption>
<graphic xlink:href="534922v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Electrophysiological data such as EEG, MEG and ECoG, have as their main advantage over other brain-scanning techniques, such as fMRI, that they provide temporally rich information about neuronal activity in different frequency bands. Over the last two decades, the most common method to examine the complexity of these signals has been Lempel-Ziv complexity (LZ), which can be seen as an estimator of the information-theoretic concept of entropy rate. Despite the remarkable practical efficacy of LZ, it is ‚Äîby construction ‚Äîunable to give information about the temporal or spectral distribution of this complexity.</p>
<p>The present work introduces a new information-theoretic tool to cognitive neuroscience, CSER, which has distinct advantages over LZ. First, we have shown that CSER has substantially better temporal resolution compared to LZ and is highly sensitive to changes in cognitive state on a mismatched negativity task, potentially detecting a cognitively important neural signal before standard ERPs do. Second, unlike LZ, CSER naturally provides a principled spectral decomposition, yielding intriguing insights about the relationship between gamma-band activity and changes in conscious state.</p>
<p>Across species and imaging modalities, we demonstrate the ability of CSER to provide information about both of these aspects, which LZ could simply not provide. These results may serve as a proof of concept, opening the door to a wide range of new investigations of spectral and temporal aspects of neural complexity observed on multiple kinds of neuroimaging data in cognitive and consciousness studies.</p>
<sec id="s3a">
<title>Advantages and limitations of state-space models in neuroimaging</title>
<p>Throughout this paper we have showcased the power of state-space time series models for a variety of neuroimaging data analyses. Our choice of state-space as models is due to their generality and flexibility, since <italic>i</italic>) they are easy to use and estimate, and do not require discretising the data, <italic>ii</italic>) they are robust to noise and misspecifications in the fitting process, and <italic>iii</italic>) they are able to model and account for a variety of noise factors common in neural data.<sup><xref ref-type="fn" rid="fn8">8</xref></sup></p>
<p>Furthermore, the rich analysis possibilities enabled by state-space modelling (as evidenced by the spectral and temporal decompositions above) enable us to explore new dimensions describing the complexity of neural dynamics ‚Äîwhich could possibly be mapped to different aspects of consciousness and cognition (<bold><italic><xref ref-type="bibr" rid="c39">Luppi et al., 2021</xref></italic></bold>). Additionally, the possibility of having sample-by-sample entropy rate estimates allows for the application of multiple analysis techniques from the field of local information dynamics, such as instantaneous information transfer (<bold><italic><xref ref-type="bibr" rid="c36">Lizier, 2010</xref></italic></bold>) or integrated information (<bold><italic><xref ref-type="bibr" rid="c43">Mediano et al., 2022</xref></italic></bold>). Conceptually, one could see the generation of entropy rate time series from data as a transformation <italic>‚Äú</italic>from volts to bits,<italic>‚Äù</italic> opening the door to new analyses on entropy time-series complementary to those on conventional ERPs. We believe these techniques could reveal new information about neural processes and could be a fruitful avenue of future research.</p>
<p>At the same time, however, there are some specific circumstances where state-space models need to be applied with care. Two of these are <italic>i</italic>) applications to fMRI data, and <italic>ii</italic>) M/EEG data after ICA component removal (or average re-referencing). In the case of fMRI, the problem can be related to the haemodynamic response filter, which may introduce singularities in the data (<bold><italic><xref ref-type="bibr" rid="c56">Solo, 2016</xref></italic></bold>),<sup><xref ref-type="fn" rid="fn9">9</xref></sup> and may be alleviated by restricting the maximum model order and regression horizon hyperparameters during the fitting process (see Methods for details). When dealing with preprocessed M/EEG data, ICA component removal may reduce the rank of the data and incur numerical errors when fitting a single state-space model to large sets of channels. In this case, one could mitigate the problem by fitting separate models to smaller subsets of channels, reducing the regression horizon, or adding a very small amount of white noise to the data.</p>
<p>Finally, note that state-space models are not the only alternative to LZ to estimate entropy rate: there exist other parametric methods, such as auto-regressive models (<bold><italic><xref ref-type="bibr" rid="c4">Barnett and Seth, 2014</xref></italic></bold>), as well as non-parametric methods, such as spectral factorisation (<bold><italic><xref ref-type="bibr" rid="c13">Chand and Dhamala, 2014</xref></italic></bold>). Nonetheless, state-space models remain our model of choice due to their greater tractability and flexibility, which makes them readily usable in most neuroscientific contexts. It is also pos-sible to build more sophisticated state-space modes, e.g. by combining them with multi-taper techniques (<bold><italic><xref ref-type="bibr" rid="c32">Kim et al., 2018</xref></italic></bold>).</p>
</sec>
<sec id="s3b">
<title>Concluding remarks</title>
<p>In this paper we have presented a new method, which we call <italic>Complexity via State-space Entropy Rate</italic> (CSER), as a principled estimator of signal diversity for electrophysiological time series. We have shown that CSER has the desirable empirical properties of successful complexity measures, like Lempel-Ziv complexity, while substantially extending Lempel-Ziv‚Äôs capabilities in several ways. Combining four datasets comprising three distinct neuroimaging modalities, several states of consciousness, and an auditory task, we have shown that CSER is a valuable analysis tool that can provide spectrally and temporally resolved insights that were previously impossible ‚Äîin the first case showing that the difference in complexity across states of consciousness is attributable to high-frequency activity; and in the latter, showing that instantaneous complexity peaks <italic>before</italic> the standard ERP signature of mismatch negativity. To make this method widely available for the neuroscience community, we provide an open-source implementation of CSER and other entropy rate estimators for several programming languages in <ext-link ext-link-type="uri" xlink:href="https://www.github.com/pmediano/EntRate//">https://www.github.com/pmediano/EntRate</ext-link>.</p>
<p>Overall, these results emphasise the neuroscientific value of a principled information-theoretic approach, helping us distil the key properties of known methods while empowering neuroscientists to investigate previously inaccessible dimensions of their data.</p>
</sec>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Datasets and preprocessing</title>
<p>In order to benchmark CSER against LZ, we decided to include data spanning both <italic>i</italic>) multiple neuroimaging modalities, and <italic>ii</italic>) a wide range of states of consciousness. With this in mind, the results in <xref rid="fig2" ref-type="fig">Figs. 2</xref> and <xref rid="fig3" ref-type="fig">3</xref> were obtained with the following data:</p>
</sec>
<sec id="s4b">
<title>Psychedelics</title>
<p>We use the MEG data first reported by <bold><italic><xref ref-type="bibr" rid="c10">Carhart-Harris et al. (2016</xref></italic></bold>) of <italic>N</italic> = 15 subjects after an infusion of intravenous LSD (or a placebo). Data were source-reconstructed to the centroids of each region in the Automated Anatomical Labelling (AAL) atlas (<bold><italic><xref ref-type="bibr" rid="c59">Tzourio-Mazoyer et al., 2002</xref></italic></bold>) using a standard LCMV beamformer. For a full description of the source-reconstruction pipeline see <bold><italic>Mediano et al. (2020b</italic></bold>).</p>
</sec>
<sec id="s4c">
<title>NREM sleep</title>
<p>We used the EEG data of <italic>N</italic> = 9 subjects during sleep, some of which were previously reported by <bold><italic><xref ref-type="bibr" rid="c63">Wong et al. (2020</xref></italic></bold>). Although the original study focused on the neurophysiology of dreams, here we used only segments of data from dreamless NREM sleep, and compared it against a wakeful rest baseline.</p>
</sec>
<sec id="s4d">
<title>Anaesthesia</title>
<p>We used the ECoG data of <italic>N</italic> = 4 marmoset monkeys sedated with KTMD anaesthesia first reported by <bold><italic><xref ref-type="bibr" rid="c64">Yanagawa et al. (2013</xref></italic></bold>). Data were obtained from the open access <ext-link ext-link-type="uri" xlink:href="http://www.neurotycho.org/">Neurotycho</ext-link> database and divided into <italic>‚Äò</italic>awake‚Äô and <italic>‚Äò</italic>sedated‚Äô periods.</p>
<p>For all datasets, in addition to modality-specific cleaning, we filtered the data using a lowpass filter with a 100 Hz cutoff, removed line noise by subtracting a least-squares-fit sinusoidal signal at 50 Hz and harmonics, and downsampled it to 200 Hz. For both LZ and CSER, we segmented the data into pseudo-stationary epochs, computed LZ and CSER for each epoch and channel (sources in the LSD dataset, electrodes in the others), and averaged across all epochs and channels. To compute LZ, each epoch was further detrended and binarised around its mean.</p>
<p>Finally, for the time-resolved analysis of an auditory oddball paradigm we use data previously presented by <bold><italic><xref ref-type="bibr" rid="c33">Komatsu et al. (2015</xref></italic></bold>). We used only data from the monkey Fr, since it is the only one publicly available in the NeuroTycho website. Given that ECoG data is typically less corrupted by noise, and that for the analysis in <xref rid="fig4" ref-type="fig">Fig. 4</xref> we were interested in the fine temporal structure of the process, we relaxed the lowpass filter to 150 Hz and the downsampling to 300 Hz.</p>
</sec>
<sec id="s4e">
<title>The two faces of LZ: signal diversity and intrinsic prediction error</title>
<p>Mathematically, the entropy rate of a stochastic process ùí≥ is defined as the asymptotic rate of growth of its entropy, i.e.
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="534922v1_eqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
This expression refers to the joint entropy of full trajectories (<italic>X</italic><sub>1</sub>, ‚Ä¶, <italic>X</italic><sub><italic>T</italic></sub>), denoted by <italic>H</italic>(<italic>X</italic><sub>1</sub>, ‚Ä¶, <italic>X</italic><sub><italic>T</italic></sub>). This quantity is related to how much the signal is exploring possible paths: the number of trajectories of <italic>T</italic> time-steps that are effectively visited by the system is approximately 2<sup><italic>T h</italic>(ùí≥)</sup>.<sup><xref ref-type="fn" rid="fn10">10</xref></sup> Hence, for sequences built on <italic>K</italic> different symbols, the signal is exploring a fraction 2<sup><italic>h</italic>(ùí≥)</sup>/<italic>K</italic> of the space of possible configurations. These results give the basis for a rigorous interpretation of LZ as <italic>signal diversity</italic>: a higher LZ implies that the system explores a larger fraction of its possible trajectories.</p>
<p>On the other hand, standard results in information theory (<bold><italic><xref ref-type="bibr" rid="c15">Cover and Thomas, 2006</xref></italic></bold>, Th. 4.2.1) state that the entropy rate can also be expressed as the entropy of the present state conditioned on its past:
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="534922v1_eqn4.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
i.e. the uncertainty in the moment-to-moment prediction of the next state of the system. In Gaussian systems this corresponds to the logarithm of the mean squared error (<bold><italic><xref ref-type="bibr" rid="c15">Cover and Thomas, 2006</xref></italic></bold>), and in discrete systems it is related to the probability of misclassification (<bold><italic><xref ref-type="bibr" rid="c22">Fano, 1961</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c23">Feder and Merhav, 1994</xref></italic></bold>). Thus, entropy rate is also formally linked with <italic>intrinsic prediction error</italic>:<sup><xref ref-type="fn" rid="fn11">11</xref></sup> a higher LZ value means it is harder to predict the next value of the signal ‚Äîeven with complete knowledge of its past trajectory.</p>
<p>Therefore, thanks to information theory and the concept of entropy rate we can bridge between two previously disconnected interpretations of complexity in neural dynamics: signal diversity and prediction error. We believe rigorous investigation of the mathematics underlying analysis frameworks can lead to more convergence between neuroscientific theories (<bold><italic><xref ref-type="bibr" rid="c52">Rosas et al., 2020</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c38">Luppi et al., 2020</xref></italic></bold>), and is a worthwhile avenue for future research.</p>
</sec>
<sec id="s4f">
<title>Model description and entropy rate estimation</title>
<p>Consider data generated by a stationary, real-valued, discrete-time <italic>d</italic>-dimensional stochastic process <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and <italic>T</italic> ‚àà ‚Ñï. The core principle of state-space (SS) modelling is to assume that ùí≥ can be modelled as noisy observations of an <italic>m</italic>-dimensional hidden process <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline3.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, with <bold><italic>y</italic></bold><sub><italic>t</italic></sub> ‚àà ‚Ñù<sup>m</sup>. Hence, the model is determined by two ingredients: the dynamics of the hidden process ùí¥, and the measurement function that relates ùí¥ with ùí≥. A simple and effective family of models are those with linear dynamics and normally distributed error terms,
<disp-formula id="eqn5a">
<alternatives><graphic xlink:href="534922v1_eqn5a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn5b">
<alternatives><graphic xlink:href="534922v1_eqn5b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>A</italic> ‚àà ‚Ñù<sup><italic>m√óm</italic></sup> is the state transition matrix, <italic>C</italic> ‚àà ‚Ñù <sup><italic>d</italic>√ó<italic>m</italic></sup> is the observation matrix, and <italic>u</italic><sub><italic>t</italic></sub>, <italic>v</italic><sub><italic>t</italic></sub> are zeromean white noise processes with covariance matrices <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline4.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline5.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.</p>
<p>We calculate the entropy rate of ùí≥ by representing the above SS model in <italic>innovations form</italic>. For this, we consider <italic>‚Äú</italic>innovations<italic>‚Äù</italic> of the form <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline6.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and define <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline7.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline8.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is a shorthand notation for the past trajectory of ùí≥. Then, one can show that the innovations are noise-like, zero-mean i.i.d. variables with covariance <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline9.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>.Then, the above SS model can be re-written as
<disp-formula id="eqn6a">
<alternatives><graphic xlink:href="534922v1_eqn6a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn6b">
<alternatives><graphic xlink:href="534922v1_eqn6b.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>K</italic> is typically referred to as the Kalman gain matrix. This is a standard result in time series analysis ‚Äîfor a proof see e.g. <bold><italic>Durbin and Koopman</italic></bold> (<bold><italic>2012</italic></bold>, Sec 4.3). With these relationships, one can show that
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="534922v1_eqn7.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where the first equality follows from the fact that (by definition) ùìè<sub><italic>t</italic></sub> is a deterministic function of <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline10.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and the second equality from the fact that <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline11.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> forms a Markov chain. This equation shows one of the key properties of (innovations form) state-space models: that the hidden state ùìè<sub><italic>t</italic></sub> encapsulates all the knowledge about the signal <bold><italic>x</italic></bold><sub><italic>t</italic></sub> one can acquire up to time <italic>t</italic> ‚àí 1. Using this, one can plug <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref> into the expression of entropy rate in <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref>, and find that
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="534922v1_eqn8.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
As a final step, given that the innovations are normally distributed, one can compute the entropy rate of ùí≥ using the standard formula for Gaussian distributions (<bold><italic><xref ref-type="bibr" rid="c15">Cover and Thomas, 2006</xref></italic></bold>, Sec. 8.4):
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="534922v1_eqn9.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Importantly, before fitting the model and computing CSER we normalise the signals to unit variance. This makes the results invariant to measurement units, prevents biases stemming from differences in total power between two conditions, and allows us to interpret CSER as a function of the proportion of signal variance that is not explained by the past of the signal itself.</p>
<p>It is also worth noting CSER can take negative values. This is because, unlike LZ, CSER estimates the entropy rate of a real-valued distribution (technically called a <italic>differential</italic> entropy rate; <bold><italic><xref ref-type="bibr" rid="c15">Cover and Thomas, 2006</xref></italic></bold>). Although differential entropy rates can be negative, their interpretation is the same as their discrete counterpart: higher entropy implies more randomness and less predictability.</p>
<p>In summary, state-space models reduce the daunting task of estimating the entropy rate of the observed process to the estimation the innovations covariance Œ£. As we show below, this can be done efficiently with off-the-shelf software, resulting in a flexible, multi-purpose complexity estimator.</p>
</sec>
<sec id="s4g">
<title>Model selection, convergence speed, and robustness</title>
<p>Given its focus on state-space models, it is no surprise that the core of the CSER computation lies on estimating the SS parameters themselves. To this end, we use the <italic>state-space subspace</italic> (SS-SS) algorithm by <bold><italic>Van Overschee and De Moor</italic></bold> (<bold><italic>2012</italic></bold>). There are two parameters we need to specify to run the SS-SS algorithm: the past and future regression horizons (<italic>p, J</italic>), and the model order <italic>m</italic>. Here we present our method for estimating the horizon and model order parameters, without describing the SS-SS algorithm in detail ‚Äîsee <bold><italic>Van Overschee and De Moor</italic></bold> (<bold><italic>2012</italic></bold>) for further details.</p>
<p>First, following <bold><italic>Bauer</italic></bold> (<bold><italic>2001</italic></bold>), we set the regression horizon using the estimated model order <italic>q</italic> of an AR model, which we fit with the LWR algorithm (<bold><italic><xref ref-type="bibr" rid="c45">Morf et al., 1978</xref></italic></bold>). We then use the Hannan-Quinn information criterion (<bold><italic><xref ref-type="bibr" rid="c26">Hannan and Quinn, 1979</xref></italic></bold>) to select the optimal AR order <italic>q</italic><sub>HQC</sub>, and set <italic>P</italic> = <italic>f</italic> = 2<italic>q</italic><sub>HQC</sub>. Finally, using these horizons we use the singular value criterion by <bold><italic>Bauer</italic></bold> (<bold><italic>2001</italic></bold>) to obtain the optimal state-space model order <italic>m</italic>. This protocol can be implemented with the MVGC toolbox (<bold><italic><xref ref-type="bibr" rid="c4">Barnett and Seth, 2014</xref></italic></bold>), and results in a method that is flexible, automated, and applicable to various types of data.</p>
<p>To test this procedure, we generated synthetic data from a set of univariate auto-regressive models of order <italic>q</italic> with varying residual variance, and compared the ground-truth entropy rate values with the CSER estimates. We simulated time series of varying length from each model, and computed CSER with the procedure outlined above (<xref rid="fig5" ref-type="fig">Fig. 5</xref>, right). For comparison, we did a similar analysis with LZ: we generated synthetic binary data of Markov order <italic>q</italic> (i.e. where <italic>x</italic><sub><italic>t</italic></sub> is a random boolean function of <italic>x</italic><sub><italic>t</italic>‚àí1</sub>, ‚Ä¶, <italic>x</italic><sub><italic>t</italic>‚àí<italic>q</italic></sub> with added noise), and estimated their entropy rate with LZ (<xref rid="fig5" ref-type="fig">Fig. 5</xref>, left).</p>
<p>The results show CSER‚Äôs clear advantages over LZ as an entropy rate estimator: it is unbiased even for relatively short time series, and variances vanish for longer time series. Even for highly autocorrelated data the variance in CSER drops for time series longer than 10<sup>3</sup> samples ‚Äîwell within reach of common M/EEG datasets. LZ, in contrast, takes orders of magnitude more samples to converge, especially for time series with long memory (which is the case in most neural data modalities).</p>
<p>On a separate front, one natural question that arises is whether a misestimation of model order <italic>m</italic> may lead to an erroneous estimation of entropy. To address this, we perform a similar test: first, we write an SS model of known order <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline12.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with fixed <italic>A, C, K</italic> parameters; then, we randomly sample a residual covariance matrix from an exponential distribution and simulate the resulting SS model; and finally, we estimate its entropy rate with models of lower order <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline13.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (<xref rid="fig6" ref-type="fig">Fig. 6</xref>).</p>
<p>These results again highlight the robustness of the CSER estimator: even when the model order is severely underestimated, CSER is still able to recover the entropy rate of the underlying generative process with high accuracy. Furthermore, CSER is also able to recover the original power spectrum, even if the incorrect model order is used ‚Äîalthough it should be noted that if the model order is significantly underestimated the spectrum estimation may suffer.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>State-space models accurately estimate entropy rate and power spectrum, and are robust to model order selection.</title>
<p>Synthetic data was generated from a known state-space model of order <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline14.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and CSER was computed using different model orders <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline23.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. (<italic>left</italic>) True and estimated entropy rate for various model orders. (<italic>top right</italic>) Average estimation error across all runs. (<italic>bottom right</italic>) True (black) and estimated (colours same as left panel) power spectral density (PSD). Although slightly more sensitive (spectrum estimation is visibly inaccurate for <italic>m</italic> ‚â§ 2), the estimator is still able to recover the true power spectrum with a mis-specified model order.</p></caption>
<graphic xlink:href="534922v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4h">
<title>Spectral decomposition of CSER</title>
<p>The core element of our spectral entropy rate decomposition is the following expression relating the residual variance and the spectral density of a stationary process, which we state here without proof (<bold><italic><xref ref-type="bibr" rid="c27">Hannan and Deistler, 2012</xref></italic></bold>, Th. 1.3.2):
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="534922v1_eqn10.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Given the simple expression of the entropy rate in <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>, it is straightforward to re-write the above equation into a spectral decomposition of entropy rate (<bold><italic><xref ref-type="bibr" rid="c14">Chicharro, 2011</xref></italic></bold>). We do this by adding <italic>d</italic> log(2<italic>œÄe</italic>) to both sides and using the fact that <bold><italic>x</italic></bold><sub><italic>t</italic></sub> is real-valued (and thus (<italic>œâ</italic>) = (‚àí<italic>œâ</italic>)):
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="534922v1_eqn11.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
With this expression at hand, we only need to compute the spectral density <italic>S</italic>(<italic>œâ</italic>) of the process from the parameters of the state-space model. This is a standard result (<bold><italic><xref ref-type="bibr" rid="c27">Hannan and Deistler, 2012</xref></italic></bold>), although we rehearse it here for completeness.</p>
<p>We begin by defining <italic>L</italic> as the backshift operator, such that <italic>Lz</italic><sub><italic>t</italic></sub> = <italic>z</italic><sub><italic>t</italic>‚àí1</sub>. With it, we can rewrite <xref ref-type="disp-formula" rid="eqn6a">Eq. (6a)</xref> as <italic>z</italic><sub><italic>t</italic></sub> = <italic>Az</italic><sub><italic>t</italic>‚àí1</sub> + <italic>K</italic><bold><italic>c</italic></bold><sub><italic>t</italic>‚àí1</sub> = <italic>AL</italic> <sub><italic>t</italic></sub> + <italic>KL</italic><bold><italic>c</italic></bold><sub><italic>t</italic></sub>. Solving for <italic>z</italic><sub><italic>t</italic></sub> and substituting in <xref ref-type="disp-formula" rid="eqn6b">Eq. (6b)</xref> we obtain the expression of the transfer function of the process, here denoted<sup><xref ref-type="fn" rid="fn12">12</xref></sup> by <italic>M</italic>(<italic>L</italic>), as
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="534922v1_eqn12.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
which allows us to write the MA representation of <bold><italic>x</italic></bold><sub><italic>t</italic></sub> as a convolution over a white noise process, <bold><italic>x</italic></bold><sub><italic>t</italic></sub> = <italic>M</italic>(<italic>L</italic>) * <bold><italic>œµ</italic></bold><sub><italic>t</italic></sub>. Now we can simply use the fact that the Fourier transform of a convolution is the product of the Fourier transforms, and arrive at the expression of the spectral density as a function of the transfer function and the residuals‚Äô variance:
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="534922v1_eqn13.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Putting everything together, to compute the results in <xref rid="fig3" ref-type="fig">Fig. 3</xref> we compute the spectral density using <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref> by evaluating <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref> at <italic>e</italic><sup><italic>i w</italic></sup>, and finally integrate <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref> numerically using the desired (normalised) frequency band as limits for the definite integral.</p>
</sec>
<sec id="s4i">
<title>Temporally resolved entropy rate</title>
<p>We model the response to the auditory stimulus as a perturbation to an otherwise stationary state-space model. In a nutshell, our method consists of <italic>i</italic>) train a single state-space model using the pre-stimulus baseline of all trials; <italic>ii</italic>) use the trained model to compute prediction errors post-stimulus; and <italic>iii</italic>) compute the log-likelihood of the prediction errors. In the following we describe this procedure in detail. See the Appendix for a technical discussion and validation of this modelling choice.</p>
<p>Recall the ECoG data used here has 720 standard and 720 deviant trials. To train the model, we extract the pre-stimulus baseline (‚àí400 to 0 ms w.r.t. stimulus presentation), stack all 1440 trials together, and train a single state-space model using the model selection procedure described above. This yields a set of (<italic>A, C, K</italic>, I-) parameters, which we leave fixed for the rest of the analysis. Then, for every trial we compute the residuals by performing one-step-ahead predictions ‚Äîi.e. running the Kalman filter algorithm with fixed parameters (<bold><italic><xref ref-type="bibr" rid="c21">Durbin and Koopman, 2012</xref></italic></bold>, Sec. 4.3). We compute the residuals for the whole duration of the trial (‚àí400 to 400 ms w.r.t. stimulus presentation), resulting in empirical prediction errors <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline15.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where <italic>t</italic> denotes the time index and <italic>k</italic> the trial number.</p>
<p>Next, we average the prediction errors across trials as <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline16.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where the average is taken over the standard and the deviant trials separately, resulting in two time series <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline17.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline18.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> respectively. Finally, using the fact that <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline19.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are i.i.d., we know that <bold><italic>e</italic></bold> <sub><italic>t</italic></sub> ‚àº ùí© (0, I-/ <italic>R</italic>), where <italic>R</italic> is the number of trials averaged, so we can compute their log-likelihood as <italic>h</italic><sub><italic>t</italic></sub> = log ùí©; (<bold><italic>e</italic></bold><sub><italic>t</italic>|</sub> 0, I-/<italic>R</italic>). The difference <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline20.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> corresponds to the blue curve in <xref rid="fig4" ref-type="fig">Fig. 4</xref>.</p>
<p>To determine the statistical significance of the results, we perform a nonparametric cluster test following <bold><italic>Maris and Oostenveld</italic></bold> (<bold><italic>2007</italic></bold>). To do this we obtain surrogate samples of <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline21.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> by randomly shuffing the trial labels. We follow the same process to compute surrogate <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline22.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> values, against which we compare the observed Œî<italic>h</italic><sub><italic>t</italic></sub> values to obtain a statistic suitable for the cluster test.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>PM and DB were funded by the Wellcome Trust (grant no. 210920/Z/18/Z). FR was supported by the Fellowship Programme of the Institute of Cultural and Creative Industries of the University of Kent. AIL is funded by the Gates Cambridge Trust (OPP 1144). AKS and LB are supported by European Research Council Grant ERC-AdG-CONSCIOUS, project number 101019254, to AKS.</p>
</ack>
<app-group>
<app id="app1">
<label>Appendix</label>
<title>Supplementary tables</title>
<p>In this appendix we report the full statistical analysis (mean, standard deviation, Cohen‚Äôs <italic>d, t</italic>-score and -value) of subject-level differences across states of consciousness. <xref ref-type="table" rid="tbl1">Tables 1</xref> and <xref ref-type="table" rid="tbl2">2</xref> correspond to the analyses shown in <xref rid="fig2" ref-type="fig">Figures 2</xref> and <xref rid="fig3" ref-type="fig">3</xref>, respectively.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>Subject-level differences in broadband CSER and LZ across states of consciousness.</p></caption>
<graphic xlink:href="534922v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><p>Subject-level differences in spectral CSER across states of consciousness. Frequency bands used are <italic>Œ¥</italic>: 1‚Äì4 Hz, <italic>Œ∏</italic>: 4‚Äì8 Hz, <italic>Œ±</italic>: 8‚Äì14 Hz, <italic>Œ≤</italic>: 14‚Äì25 Hz, <italic>Œ≥</italic>: &gt;25 Hz.</p></caption>
<graphic xlink:href="534922v1_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><p>Subject-level differences in spectral CSER across states of consciousness, controlled for gamma coherence. Results correspond to the regression coefficients for conscious state in the linear model ‚ÄòCSER State + Coherence‚Äô. Coherence was calculated using the Fieldtrip toolbox (<bold><italic><xref ref-type="bibr" rid="c47">Oostenveld et al., 2011</xref></italic></bold>) with a multitaper Fourier transform method, and averaged across all pairs of channels. Effect size <italic>d</italic> here was calculated as suggested by <bold><italic>Feingold</italic></bold> (<bold><italic>2013</italic></bold>), taking the regression coefficient divided by the residual standard deviation of the full model. Frequency bands used are <italic>Œ¥</italic>: 1‚Äì4 Hz, <italic>Œ∏</italic>: 4‚Äì8 Hz, <italic>Œ±</italic>: 8‚Äì14 Hz, <italic>Œ≤</italic>: 14‚Äì25 Hz, <italic>Œ≥</italic>: &gt;25 Hz.</p></caption>
<graphic xlink:href="534922v1_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</app>
<app id="app2">
<label>Appendix</label>
<title>LZ is a measure of entropy, not algorithmic complexity</title>
<p>A significant portion of the neuroscience literature that employs LZ argues that it is somewhat akin to the Kolmogorov-Chaitin algorithmic complexity (<bold><italic><xref ref-type="bibr" rid="c55">Sitt et al., 2014</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c11">Casali et al., 2013</xref></italic></bold>). This is by no means an unmotivated belief, as the original ideas of Lempel and Ziv were certainly motivated by Kolmogorov‚Äôs work (<bold><italic><xref ref-type="bibr" rid="c34">Lempel and Ziv, 1976</xref></italic></bold>), and well-known references promote the use of LZ as an upper bound to Kolmogorov-Chaitin algorithmic complexity (<bold><italic>Li et al., 2008</italic></bold>). Although this is not technically incorrect, in what follows we argue that this can lead to undesirable misinterpretations.</p>
<p>Recent investigations have shown that, despite superficial similarities, Shannon‚Äôs entropy and algorithmic complexity are fully dissociated (<bold><italic><xref ref-type="bibr" rid="c67">Zenil et al., 2019</xref></italic></bold>). In particular, while Shannon‚Äôs entropy can always be used as upper bound of the algorithmic complexity, this bound can be infinitely inaccurate.<sup><xref ref-type="fn" rid="fn13">13</xref></sup> Furthermore, there is reason to be skeptical of alleged estimators of algorithmic complexity from experimental data. Kolmogorov‚Äôs <italic>Invariance</italic> (or <italic>Universality</italic>) <italic>Theorem</italic> states that the Kolmogorov complexity of a sequence <bold><italic>x</italic></bold> read with two different Turing machines differs by a constant independent of <bold><italic>x</italic></bold> (<bold><italic><xref ref-type="bibr" rid="c15">Cover and Thomas, 2006</xref></italic></bold>, Ch. 14). An important corollary for practical applications is that said constant can be <italic>arbitrarily large</italic> for any finite sequence. For practical purposes, that means that a measured difference between two conditions may be a genuine difference in algorithmic complexity, or may just be a Turing machine mismatch ‚Äîwhich has a substantially different interpretation. This is not to say that algorithmic complexity cannot be estimated from data ‚Äîsee e.g. <bold><italic><xref ref-type="bibr" rid="c66">Zenil et al. (2018</xref></italic></bold>) ‚Äì, although there is great nuance involved which cannot be simply swept under the rug of the LZ algorithm.</p>
<p>In addition to the concerns above, there is one more hurdle to the Kolmogorov complexity interpretation of LZ: it relies on the fundamental assumption that the sequence must be exactly reproduced by a Turing machine (<bold><italic><xref ref-type="bibr" rid="c15">Cover and Thomas, 2006</xref></italic></bold>). This is a very strong assumption to make regarding neural dynamics, which clashes with contemporary accounts of the brain as a nonlinear stochastic system (<bold><italic><xref ref-type="bibr" rid="c17">Deco et al., 2009</xref></italic></bold>). For these reasons, we believe the interpretation of LZ as entropy rate is both more mathematically principled and more parsimonious, and thus should be preferred when interpreting empirical results.</p>
</app>
<app id="app3">
<label>Appendix</label>
<title>Alternatives for entropy estimation in non-stationary data</title>
<p>In general, modelling of ERPs is a difficult task due to the strongly non-stationary nature of event-related data (<bold><italic><xref ref-type="bibr" rid="c19">Ding et al., 2000</xref></italic></bold>). As mentioned in the Methods section, the time-resolved entropy analysis shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref> was performed assuming a constant model subject to a non-stationary perturbation. For completeness, here we discuss an alternative approach: one could leverage the trial structure of the data by fitting SS models using all trials and sliding temporal windows, and computing CSER for each of them.</p>
<p>In essence, the approach used in the main text corresponds to modelling the ERP as nonstationary innovations in a stationary model, and the approach introduced here corresponds to stationary innovations in a non-stationary model. In principle, these two alternatives can be pitted against each other through standard model selection tests, e.g. via a likelihood ratio test. However, the likelihood function of a non-stationary state-space model is complicated, and therefore this approach is difficult in practice.</p>
<p>Nonetheless, as an example of this approach and as a validation of the approach from the main text, we performed a sliding-window CSER analysis on the same oddball task data as before. For this, we processed the data following recommendations by <bold><italic><xref ref-type="bibr" rid="c19">Ding et al. (2000</xref></italic></bold>) (including time-wise ensemble demeaning and normalisation) and computed CSER in sliding windows of 20 samples. We performed this analysis both with and without filtering and downsampling, to obtain a more granular picture of the CSER time-course (<xref rid="fig7" ref-type="fig">Fig. 7</xref>).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Sliding-window CSER estimates show no signi1cant differences between standard and deviant tones.</title>
<p>Results are shown before (<italic>left</italic>) and after (<italic>right</italic>) applying a 150 Hz lowpass filter to the data and downsampling it to 300 Hz. A standard cluster test found no significant differences between conditions in either case.</p></caption>
<graphic xlink:href="534922v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The most visible result from this analysis is that indeed there is temporal structure in the sliding-window CSER across the trial, most noticeably a sharp dip around the stimulus time. This could be interpreted as the stimulus dominating the signal and suppressing endogenous noise; or, in more cognitive terms, as there being some uncertainty about the precise stimulus timing which is resolved as soon as the stimulus is presented.</p>
<p>Strictly speaking, this means the assumptions behind the model from <xref rid="fig4" ref-type="fig">Fig. 4</xref> are not fully correct, since at least one parameter (Œ£) changes over the course of the trial. However, when comparing the standard and deviant CSER values, there are no significant differences between the two types of stimuli surviving a cluster test (<bold><italic><xref ref-type="bibr" rid="c40">Maris and Oostenveld, 2007</xref></italic></bold>). Therefore, although it does seem likely that model parameters change throughout the course of the trial, this change does not account for the difference between stimuli found in <xref rid="fig4" ref-type="fig">Fig. 4</xref>.</p>
<p>To summarise, we have described two different ways in which a time-resolved entropy rate time course can be obtained from multi-trial data leveraging state-space models. These make different assumptions, and although they are difficult to test rigorously, they may have different advantages in different situations.</p>
</app>
</app-group>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Ab√°solo</surname> <given-names>D</given-names></string-name>, <string-name><surname>Simons</surname> <given-names>S</given-names></string-name>, <string-name><surname>Morgado da Silva</surname> <given-names>R</given-names></string-name>, <string-name><surname>Tononi</surname> <given-names>G</given-names></string-name>, <string-name><surname>Vyazovskiy</surname> <given-names>VV</given-names></string-name>. <article-title>Lempel-Ziv complexity of cortical activity during sleep and waking in rats</article-title>. <source>Journal of neurophysiology</source>. <year>2015</year>; <volume>113</volume>(<issue>7</issue>):<fpage>2742</fpage>‚Äì<lpage>2752</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Akar</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Kara</surname> <given-names>S</given-names></string-name>, <string-name><surname>Agambayev</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bilgi√ß</surname> <given-names>V.</given-names></string-name> <article-title>Nonlinear analysis of EEGs of patients with major depression during different emotional states</article-title>. <source>Computers in Biology and Medicine</source>. <year>2015</year>; <volume>67</volume>:<fpage>49</fpage>‚Äì<lpage>60</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="book"><string-name><surname>Bachmann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kalev</surname> <given-names>K</given-names></string-name>, <string-name><surname>Suhhova</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lass</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hinrikus</surname> <given-names>H.</given-names></string-name> <chapter-title>Lempel Ziv complexity of EEG in depression</chapter-title>. <source>In: 6th European Conference of the International Federation for Medical and Biological Engineering</source> <publisher-name>Springer</publisher-name>; <year>2015</year>. p. <fpage>58</fpage>‚Äì<lpage>61</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Barnett</surname> <given-names>L</given-names></string-name>, <string-name><surname>Seth</surname> <given-names>AK</given-names></string-name>. <article-title>The MVGC multivariate Granger causality toolbox: A new approach to Granger-causal inference</article-title>. <source>Journal of Neuroscience Methods</source>. <year>2014</year>; <volume>223</volume>:<fpage>50</fpage>‚Äì<lpage>68</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Barnett</surname> <given-names>L</given-names></string-name>, <string-name><surname>Seth</surname> <given-names>AK</given-names></string-name>. <article-title>Granger causality for state-space models</article-title>. <source>Physical Review E</source>. <year>2015</year>; <volume>91</volume>(<issue>4</issue>):<fpage>040101</fpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Barnett</surname> <given-names>L</given-names></string-name>, <string-name><surname>Seth</surname> <given-names>AK</given-names></string-name>. <article-title>Detectability of Granger causality for subsampled continuous-time neurophysiological processes</article-title>. <source>Journal of Neuroscience Methods</source>. <year>2017</year>; <volume>275</volume>:<fpage>93</fpage>‚Äì<lpage>121</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Bauer</surname> <given-names>D.</given-names></string-name> <article-title>Order estimation for subspace methods</article-title>. <source>Automatica</source>. <year>2001</year>; <volume>37</volume>(<issue>10</issue>):<fpage>1561</fpage>‚Äì<lpage>1573</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Boncompte</surname> <given-names>G</given-names></string-name>, <string-name><surname>Medel</surname> <given-names>V</given-names></string-name>, <string-name><surname>Cort√≠nez</surname> <given-names>LI</given-names></string-name>, <string-name><surname>Ossand√≥n</surname> <given-names>T.</given-names></string-name> <article-title>Brain activity complexity has a nonlinear relation to the level of propofol sedation</article-title>. <source>British Journal of Anaesthesia</source>. <year>2021</year>; <volume>127</volume>(<issue>2</issue>):<fpage>254</fpage>‚Äì<lpage>263</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="other"><string-name><surname>Canales-Johnson</surname> <given-names>A</given-names></string-name>, <string-name><surname>Borges</surname> <given-names>AFT</given-names></string-name>, <string-name><surname>Komatsu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Fujii</surname> <given-names>N</given-names></string-name>, <string-name><surname>Fahrenfort</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Noreika</surname> <given-names>V.</given-names></string-name> <article-title>Broadband Signal Rather than Frequency-Speci1c Rhythms Underlies Prediction Error in the Primate Auditory Cortex</article-title>. <source>BioRxiv</source>. <year>2019</year>; p. <fpage>821942</fpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Carhart-Harris</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Muthukumaraswamy</surname> <given-names>S</given-names></string-name>, <string-name><surname>Roseman</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kaelen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Droog</surname> <given-names>W</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>K</given-names></string-name>, <string-name><surname>Tagliazucchi</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schenberg</surname> <given-names>EE</given-names></string-name>, <string-name><surname>Nest</surname> <given-names>T</given-names></string-name>, <string-name><surname>Orban</surname> <given-names>C</given-names></string-name>, <etal>et al.</etal> <article-title>Neural correlates of the LSD experience revealed by multimodal neuroimaging</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2016</year>; <volume>113</volume>(<issue>17</issue>):<fpage>4853</fpage>‚Äì<lpage>4858</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Casali</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Gosseries</surname> <given-names>O</given-names></string-name>, <string-name><surname>Rosanova</surname> <given-names>M</given-names></string-name>, <string-name><surname>Boly</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sarasso</surname> <given-names>S</given-names></string-name>, <string-name><surname>Casali</surname> <given-names>KR</given-names></string-name>, <string-name><surname>Casarotto</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bruno</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Laureys</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tononi</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal> <article-title>A theoretically based index of consciousness independent of sensory processing and behavior</article-title>. <source>Science Translational Medicine</source>. <year>2013</year>; <volume>5</volume>(<issue>198</issue>):<fpage>198ra105</fpage>‚Äì<lpage>198ra105</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Cavinato</surname> <given-names>M</given-names></string-name>, <string-name><surname>Genna</surname> <given-names>C</given-names></string-name>, <string-name><surname>Manganotti</surname> <given-names>P</given-names></string-name>, <string-name><surname>Formaggio</surname> <given-names>E</given-names></string-name>, <string-name><surname>Storti</surname> <given-names>SF</given-names></string-name>, <string-name><surname>Campostrini</surname> <given-names>S</given-names></string-name>, <string-name><surname>Arcaro</surname> <given-names>C</given-names></string-name>, <string-name><surname>Casanova</surname> <given-names>E</given-names></string-name>, <string-name><surname>Petrone</surname> <given-names>V</given-names></string-name>, <string-name><surname>Piperno</surname> <given-names>R</given-names></string-name>, <etal>et al.</etal> <article-title>Coherence and consciousness: Study of fronto-parietal gamma synchrony in patients with disorders of consciousness</article-title>. <source>Brain Topography</source>. <year>2015</year>; <volume>28</volume>(<issue>4</issue>):<fpage>570</fpage>‚Äì<lpage>579</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Chand</surname> <given-names>GB</given-names></string-name>, <string-name><surname>Dhamala</surname> <given-names>M.</given-names></string-name> <article-title>Spectral factorization-based current source density analysis of ongoing neural oscillations</article-title>. <source>Journal of Neuroscience Methods</source>. <year>2014</year>; <volume>224</volume>:<fpage>58</fpage>‚Äì<lpage>65</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Chicharro</surname> <given-names>D.</given-names></string-name> <article-title>On the spectral formulation of Granger causality</article-title>. <source>Biological Cybernetics</source>. <year>2011</year>; <volume>105</volume>(<issue>5</issue>):<fpage>331</fpage>‚Äì<lpage>347</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="other"><string-name><surname>Cover</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Thomas</surname> <given-names>JA</given-names></string-name>. <article-title>Elements of Information Theory</article-title>. <source>Hoboken: Wiley</source>; <year>2006</year>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Crick</surname> <given-names>F</given-names></string-name>, <string-name><surname>Koch</surname> <given-names>C.</given-names></string-name> <article-title>Towards a neurobiological theory of consciousness</article-title>. <source>In: Seminars in the Neurosciences</source>, vol. <volume>2</volume>; <year>1990</year>. p. <fpage>203</fpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Deco</surname> <given-names>G</given-names></string-name>, <string-name><surname>Rolls</surname> <given-names>ET</given-names></string-name>, <string-name><surname>Romo</surname> <given-names>R.</given-names></string-name> <article-title>Stochastic dynamics as a principle of brain function</article-title>. <source>Progress in Neurobiology</source>. <year>2009</year>; <volume>88</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Den Ouden</surname> <given-names>HE</given-names></string-name>, <string-name><surname>Kok</surname> <given-names>P</given-names></string-name>, <string-name><surname>De Lange</surname> <given-names>FP</given-names></string-name>. <article-title>How prediction errors shape perception, attention, and motivation</article-title>. <source>Frontiers in Psychology</source>. <year>2012</year>; <volume>3</volume>:<fpage>548</fpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Ding</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bressler</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>W</given-names></string-name>, <string-name><surname>Liang</surname> <given-names>H.</given-names></string-name> <article-title>Short-window spectral analysis of cortical event-related potentials by adaptive multivariate autoregressive modeling: Data preprocessing, model validation, and variability assessment</article-title>. <source>Biological Cybernetics</source>. <year>2000</year>; <volume>83</volume>(<issue>1</issue>):<fpage>35</fpage>‚Äì<lpage>45</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Dolan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>HJ</given-names></string-name>, <string-name><surname>Mediano</surname> <given-names>P</given-names></string-name>, <string-name><surname>Molina-Solana</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rajpal</surname> <given-names>H</given-names></string-name>, <string-name><surname>Rosas</surname> <given-names>F</given-names></string-name>, <string-name><surname>Sloboda</surname> <given-names>JA</given-names></string-name>. <article-title>The improvisational state of mind: A multidisciplinary study of an improvisatory approach to classical music repertoire performance</article-title>. <source>Frontiers in Psychology</source>. <year>2018</year>; <volume>9</volume>:<fpage>1341</fpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="book"><string-name><surname>Durbin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Koopman</surname> <given-names>SJ</given-names></string-name>. <source>Time Series Analysis by State Space Methods</source>. <publisher-name>Oxford University Press</publisher-name>; <year>2012</year>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="book"><string-name><surname>Fano</surname> <given-names>RM</given-names></string-name>. <source>Transmission of Information: A Statistical Theory of Communication</source>. <publisher-name>MIT Press</publisher-name>; <year>1961</year>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Feder</surname> <given-names>M</given-names></string-name>, <string-name><surname>Merhav</surname> <given-names>N.</given-names></string-name> <article-title>Relations between entropy and error probability</article-title>. <source>IEEE Transactions on Information Theory</source>. <year>1994</year>; <volume>40</volume>(<issue>1</issue>):<fpage>259</fpage>‚Äì<lpage>266</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Feingold</surname> <given-names>A.</given-names></string-name> <article-title>A regression framework for effect size assessments in longitudinal modeling of group differences</article-title>. <source>Review of General Psychology</source>. <year>2013</year>; <volume>17</volume>(<issue>1</issue>):<fpage>111</fpage>‚Äì<lpage>121</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Garrido</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Kilner</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Stephan</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>KJ</given-names></string-name>. <article-title>The mismatch negativity: A review of underlying mechanisms</article-title>. <source>Clinical Neurophysiology</source>. <year>2009</year>; <volume>120</volume>(<issue>3</issue>):<fpage>453</fpage>‚Äì<lpage>463</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Hannan</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Quinn</surname> <given-names>BG</given-names></string-name>. <article-title>The determination of the order of an autoregression</article-title>. <source>Journal of the Royal Statistical Society: Series B (Methodological)</source>. <year>1979</year>; <volume>41</volume>(<issue>2</issue>):<fpage>190</fpage>‚Äì<lpage>195</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="other"><string-name><surname>Hannan</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Deistler</surname> <given-names>M.</given-names></string-name> <article-title>The Statistical Theory of Linear Systems</article-title>. <source>SIAM</source>; <year>2012</year>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Hohwy</surname> <given-names>J</given-names></string-name>, <string-name><surname>Seth</surname> <given-names>A.</given-names></string-name> <article-title>Predictive processing as a systematic basis for identifying the neural correlates of consciousness</article-title>. <source>Philosophy and the Mind Sciences</source>. <year>2020</year>; <volume>1</volume>(<issue>II</issue>).</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Ib√°√±ez-Molina</surname> <given-names>A</given-names></string-name>, <string-name><surname>Iglesias-Parro</surname> <given-names>S.</given-names></string-name> <article-title>Fractal characterization of internally and externally generated conscious experiences</article-title>. <source>Brain and cognition</source>. <year>2014</year>; <volume>87</volume>:<fpage>69</fpage>‚Äì<lpage>75</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Ib√°√±ez-Molina</surname> <given-names>A J</given-names></string-name>, <string-name><surname>Iglesias-Parro</surname> <given-names>S</given-names></string-name>, <string-name><surname>Soriano</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Aznarte</surname> <given-names>JI</given-names></string-name>. <article-title>Multiscale Lempel‚ÄìZiv complexity for EEG measures</article-title>. <source>Clinical Neurophysiology</source>. <year>2015</year>; <volume>126</volume>(<issue>3</issue>):<fpage>541</fpage>‚Äì<lpage>548</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Ib√°√±ez-Molina</surname> <given-names>A J</given-names></string-name>, <string-name><surname>Lozano</surname> <given-names>V</given-names></string-name>, <string-name><surname>Soriano</surname> <given-names>M</given-names></string-name>, <string-name><surname>Aznarte</surname> <given-names>J</given-names></string-name>, <string-name><surname>G√≥mez-Ariza</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Bajo</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal> <article-title>EEG multiscale complexity in schizophrenia during picture naming</article-title>. <source>Frontiers in physiology</source>. <year>2018</year>; <volume>9</volume>:<fpage>1213</fpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Kim</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Behr</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Ba</surname> <given-names>D</given-names></string-name>, <string-name><surname>Brown</surname> <given-names>EN</given-names></string-name>. <article-title>State-space multitaper time-frequency analysis</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2018</year>; <volume>115</volume>(<issue>1</issue>):<fpage>E5</fpage>‚Äì<lpage>E14</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Komatsu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Takaura</surname> <given-names>K</given-names></string-name>, <string-name><surname>Fujii</surname> <given-names>N.</given-names></string-name> <article-title>Mismatch negativity in common marmosets: Whole-cortical recordings with multi-channel electrocorticograms</article-title>. <source>Scienti1c Reports</source>. <year>2015</year>; <volume>5</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>7</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Lempel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ziv</surname> <given-names>J.</given-names></string-name> <article-title>On the complexity of 1nite sequences</article-title>. <source>IEEE Transactions on Information Theory</source>. <year>1976</year>; <volume>22</volume>(<issue>1</issue>):<fpage>75</fpage>‚Äì<lpage>81</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="book"><string-name><surname>Li</surname> <given-names>M</given-names></string-name>, <string-name><surname>Vit√°nyi</surname> <given-names>P</given-names></string-name>, <etal>et al.</etal> <source>An introduction to Kolmogorov complexity and its applications</source>, vol. <volume>3</volume>. <publisher-name>Springer</publisher-name>; <year>2008</year>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="book"><string-name><surname>Lizier</surname> <given-names>J.</given-names></string-name> <source>The Local Information Dynamics of Distributed Computation in Complex Systems</source>. <publisher-name>PhD thesis, University of Sydney</publisher-name>; <year>2010</year>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Lou</surname> <given-names>H</given-names></string-name>, <string-name><surname>Changeux</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Rosenstand</surname> <given-names>A.</given-names></string-name> <article-title>Towards a cognitive neuroscience of self-awareness</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>. <year>2017</year>; <volume>83</volume>:<fpage>765</fpage>‚Äì<lpage>773</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="other"><string-name><surname>Luppi</surname> <given-names>AI</given-names></string-name>, <string-name><surname>Mediano</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Rosas</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Allanson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Pickard</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Carhart-Harris</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>GB</given-names></string-name>, <string-name><surname>Craig</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Finoia</surname> <given-names>P</given-names></string-name>, <string-name><surname>Owen</surname> <given-names>AM</given-names></string-name>, <etal>et al.</etal> <article-title>A synergistic workspace for human consciousness revealed by integrated information decomposition</article-title>. <source>BioRxiv</source>. <year>2020</year>;.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Luppi</surname> <given-names>AI</given-names></string-name>, <string-name><surname>Mediano</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Rosas</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Harrison</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Carhart-Harris</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Bor</surname> <given-names>D</given-names></string-name>, <string-name><surname>Stamatakis</surname> <given-names>EA</given-names></string-name>. <article-title>What it is like to be a bit: An integrated information decomposition account of emergent mental phenomena</article-title>. <source>Neuroscience of Consciousness</source>. <year>2021</year>; <volume>2021</volume>(<issue>2</issue>):<fpage>iab027</fpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Oostenveld</surname> <given-names>R.</given-names></string-name> <article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title>. <source>Journal of Neuroscience Methods</source>. <year>2007</year>; <volume>164</volume>(<issue>1</issue>):<fpage>177</fpage>‚Äì<lpage>190</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="other"><string-name><surname>Mediano</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ikkala</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kievit</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Jagannathan</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Varley</surname> <given-names>TF</given-names></string-name>, <string-name><surname>Stamatakis</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Bekinschtein</surname> <given-names>TA</given-names></string-name>, <string-name><surname>Bor</surname> <given-names>D.</given-names></string-name> <article-title>Fluctuations in neural complexity during wakefulness relate to conscious level and cognition</article-title>. <source>bioRxiv</source>. <year>2021</year>;.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="other"><string-name><surname>Mediano</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Rosas</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Barrett</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Bor</surname> <given-names>D.</given-names></string-name> <article-title>Decomposing spectral and phasic differences in non-linear features between datasets</article-title>. <source>arXiv</source>:200910015. <year>2020</year>;.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Mediano</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Rosas</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Farah</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Shanahan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bor</surname> <given-names>D</given-names></string-name>, <string-name><surname>Barrett</surname> <given-names>AB</given-names></string-name>. <article-title>Integrated information as a common signature of dynamical and information-processing complexity</article-title>. <source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source>. <year>2022</year>; <volume>32</volume>(<issue>1</issue>):<fpage>013115</fpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="other"><string-name><surname>Mediano</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Rosas</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Timmermann</surname> <given-names>C</given-names></string-name>, <string-name><surname>Roseman</surname> <given-names>L</given-names></string-name>, <string-name><surname>Nutt</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Feilding</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kaelen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kringelbach</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Barrett</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Seth</surname> <given-names>AK</given-names></string-name>, <etal>et al.</etal> <article-title>Effects of external stimulation on psychedelic state neurodynamics</article-title>. <source>bioRxiv</source>. <year>2020</year>;.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Morf</surname> <given-names>M</given-names></string-name>, <string-name><surname>Vieira</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>DT</given-names></string-name>, <string-name><surname>Kailath</surname> <given-names>T.</given-names></string-name> <article-title>Recursive multichannel maximum entropy spectral estimation</article-title>. <source>IEEE Transactions on Geoscience Electronics</source>. <year>1978</year>; <volume>16</volume>(<issue>2</issue>):<fpage>85</fpage>‚Äì<lpage>94</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>N√§√§t√§nen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Paavilainen</surname> <given-names>P</given-names></string-name>, <string-name><surname>Rinne</surname> <given-names>T</given-names></string-name>, <string-name><surname>Alho</surname> <given-names>K.</given-names></string-name> <article-title>The mismatch negativity (MMN) in basic research of central auditory processing: A review</article-title>. <source>Clinical Neurophysiology</source>. <year>2007</year>; <volume>118</volume>(<issue>12</issue>):<fpage>2544</fpage>‚Äì<lpage>2590</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name><surname>Fries</surname> <given-names>P</given-names></string-name>, <string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schoffelen</surname> <given-names>JM</given-names></string-name>. <article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source>Computational Intelligence and Neuroscience</source>. 2011 <month>Jan</month>; <year>2011</year>:<volume>1</volume>:<fpage>1</fpage>‚Äì<lpage>1</lpage>:9.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Pal</surname> <given-names>D</given-names></string-name>, <string-name><surname>Li</surname> <given-names>D</given-names></string-name>, <string-name><surname>Dean</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Brito</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>T</given-names></string-name>, <string-name><surname>Fryzel</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Hudetz</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Mashour</surname> <given-names>GA</given-names></string-name>. <article-title>Level of consciousness is dissociable from electroencephalographic measures of cortical connectivity, slow oscillations, and complexity</article-title>. <source>Journal of Neuroscience</source>. <year>2020</year>; <volume>40</volume>(<issue>3</issue>):<fpage>605</fpage>‚Äì<lpage>618</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Paninski</surname> <given-names>L.</given-names></string-name> <article-title>Estimation of entropy and mutual information</article-title>. <source>Neural Computation</source>. <year>2003</year>; <volume>15</volume>(<issue>6</issue>):<fpage>1191</fpage>‚Äì<lpage>1253</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Panzeri</surname> <given-names>S</given-names></string-name>, <string-name><surname>Senatore</surname> <given-names>R</given-names></string-name>, <string-name><surname>Montemurro</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>RS</given-names></string-name>. <article-title>Correcting for the sampling bias problem in spike train information measures</article-title>. <source>Journal of Neurophysiology</source>. <year>2007</year>; <volume>98</volume>(<issue>3</issue>):<fpage>1064</fpage>‚Äì<lpage>1072</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="other"><string-name><surname>Rajpal</surname> <given-names>H</given-names></string-name>, <string-name><surname>Mediano</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Rosas</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Timmermann</surname> <given-names>CB</given-names></string-name>, <string-name><surname>Brugger</surname> <given-names>S</given-names></string-name>, <string-name><surname>Muthukumaraswamy</surname> <given-names>S</given-names></string-name>, <string-name><surname>Seth</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Bor</surname> <given-names>D</given-names></string-name>, <string-name><surname>Carhart-Harris</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>HJ</given-names></string-name>. <article-title>Psychedelics and schizophrenia: Distinct alterations to Bayesian inference</article-title>. <source>bioRxiv</source>. <year>2022</year>;</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Rosas</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Mediano</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>HJ</given-names></string-name>, <string-name><surname>Seth</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Barrett</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Carhart-Harris</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Bor</surname> <given-names>D.</given-names></string-name> <article-title>Reconciling emergences: An information-theoretic approach to identify causal emergence in multivariate data</article-title>. <source>PLoS Computational Biology</source>. <year>2020</year>; <volume>16</volume>(<issue>12</issue>):<fpage>e1008289</fpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Schartner</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Carhart-Harris</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Barrett</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Seth</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Muthukumaraswamy</surname> <given-names>SD</given-names></string-name>. <article-title>Increased spontaneous MEG signal diversity for psychoactive doses of ketamine, LSD and psilocybin</article-title>. <source>Scienti1c Reports</source>. <year>2017</year> <month>apr</month>; <volume>7</volume>:<fpage>46421</fpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Schartner</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Pigorini</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gibbs</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Arnulfo</surname> <given-names>G</given-names></string-name>, <string-name><surname>Sarasso</surname> <given-names>S</given-names></string-name>, <string-name><surname>Barnett</surname> <given-names>L</given-names></string-name>, <string-name><surname>Nobili</surname> <given-names>L</given-names></string-name>, <string-name><surname>Massimini</surname> <given-names>M</given-names></string-name>, <string-name><surname>Seth</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Barrett</surname> <given-names>AB</given-names></string-name>. <article-title>Global and local complexity of intracranial EEG decreases during NREM sleep</article-title>. <source>Neuroscience of Consciousness</source>. 2017 <volume>01</volume>; <year>2017</year>(<issue>1</issue>). <fpage>Niw022</fpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Sitt</surname> <given-names>JD</given-names></string-name>, <string-name><surname>King</surname> <given-names>JR</given-names></string-name>, <string-name><surname>El Karoui</surname> <given-names>I</given-names></string-name>, <string-name><surname>Rohaut</surname> <given-names>B</given-names></string-name>, <string-name><surname>Faugeras</surname> <given-names>F</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sigman</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dehaene</surname> <given-names>S</given-names></string-name>, <string-name><surname>Naccache</surname> <given-names>L.</given-names></string-name> <article-title>Large scale screening of neural signatures of consciousness in patients in a vegetative or minimally conscious state</article-title>. <source>Brain</source>. <year>2014</year>; <volume>137</volume>(<issue>8</issue>):<fpage>2258</fpage>‚Äì<lpage>2270</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Solo</surname> <given-names>V.</given-names></string-name> <article-title>State-space analysis of Granger-Geweke causality measures with application to fMRI</article-title>. <source>Neural Computation</source>. <year>2016</year>; <volume>28</volume>(<issue>5</issue>):<fpage>914</fpage>‚Äì<lpage>949</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Timmermann</surname> <given-names>C</given-names></string-name>, <string-name><surname>Roseman</surname> <given-names>L</given-names></string-name>, <string-name><surname>Schartner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Milliere</surname> <given-names>R</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>LT</given-names></string-name>, <string-name><surname>Erritzoe</surname> <given-names>D</given-names></string-name>, <string-name><surname>Muthukumaraswamy</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ashton</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bendrioua</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kaur</surname> <given-names>O</given-names></string-name>, <etal>et al.</etal> <article-title>Neural correlates of the DMT experience assessed with multivariate EEG</article-title>. <source>Scienti1c Reports</source>. <year>2019</year>; <volume>9</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Turkheimer</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Rosas</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Dipasquale</surname> <given-names>O</given-names></string-name>, <string-name><surname>Martins</surname> <given-names>D</given-names></string-name>, <string-name><surname>Fagerholm</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Expert</surname> <given-names>P</given-names></string-name>, <string-name><surname>V√°≈°a</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lord</surname> <given-names>LD</given-names></string-name>, <string-name><surname>Leech</surname> <given-names>R.</given-names></string-name> <article-title>A complex systems perspective on neuroimaging studies of behavior and its disorders</article-title>. <source>The Neuroscientist</source>. <year>2022</year>; <volume>28</volume>(<issue>4</issue>):<fpage>382</fpage>‚Äì<lpage>399</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Tzourio-Mazoyer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Landeau</surname> <given-names>B</given-names></string-name>, <string-name><surname>Papathanassiou</surname> <given-names>D</given-names></string-name>, <string-name><surname>Crivello</surname> <given-names>F</given-names></string-name>, <string-name><surname>Etard</surname> <given-names>O</given-names></string-name>, <string-name><surname>Delcroix</surname> <given-names>N</given-names></string-name>, <string-name><surname>Mazoyer</surname> <given-names>B</given-names></string-name>, <string-name><surname>Joliot</surname> <given-names>M.</given-names></string-name> <article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title>. <source>Neuroimage</source>. <year>2002</year>; <volume>15</volume>(<issue>1</issue>):<fpage>273</fpage>‚Äì<lpage>289</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="other"><string-name><surname>Van Overschee</surname> <given-names>P</given-names></string-name>, <string-name><surname>De Moor</surname> <given-names>B.</given-names></string-name> <article-title>Subspace Identi1cation for Linear Systems: Theory, Implementation, Applications</article-title>. <source>Springer Science &amp; Business Media</source>; <year>2012</year>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Varley</surname> <given-names>TF</given-names></string-name>, <string-name><surname>Luppi</surname> <given-names>AI</given-names></string-name>, <string-name><surname>Pappas</surname> <given-names>I</given-names></string-name>, <string-name><surname>Naci</surname> <given-names>L</given-names></string-name>, <string-name><surname>Adapa</surname> <given-names>R</given-names></string-name>, <string-name><surname>Owen</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Menon</surname> <given-names>DK</given-names></string-name>, <string-name><surname>Stamatakis</surname> <given-names>EA</given-names></string-name>. <article-title>Consciousness &amp; brain functional complexity in propofol anaesthesia</article-title>. <source>Scienti1c Reports</source>. <year>2020</year>; <volume>10</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Walter</surname> <given-names>N</given-names></string-name>, <string-name><surname>Hinterberger</surname> <given-names>T.</given-names></string-name> <article-title>Determining states of consciousness in the electroencephalogram based on spectral, complexity, and criticality features</article-title>. <source>Neuroscience of Consciousness</source>. <year>2022</year>; <volume>2022</volume>(<issue>1</issue>):<fpage>iac008</fpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Wong</surname> <given-names>W</given-names></string-name>, <string-name><surname>Noreika</surname> <given-names>V</given-names></string-name>, <string-name><surname>M√≥r√≥</surname> <given-names>L</given-names></string-name>, <string-name><surname>Revonsuo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Windt</surname> <given-names>J</given-names></string-name>, <string-name><surname>Valli</surname> <given-names>K</given-names></string-name>, <string-name><surname>Tsuchiya</surname> <given-names>N.</given-names></string-name> <article-title>The Dream Catcher experiment: Blinded analyses failed to detect markers of dreaming consciousness in EEG spectral power</article-title>. <source>Neuroscience of Consciousness</source>. <year>2020</year>; <volume>2020</volume>(<issue>1</issue>):<fpage>iaa006</fpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>Yanagawa</surname> <given-names>T</given-names></string-name>, <string-name><surname>Chao</surname> <given-names>ZC</given-names></string-name>, <string-name><surname>Hasegawa</surname> <given-names>N</given-names></string-name>, <string-name><surname>Fujii</surname> <given-names>N.</given-names></string-name> <article-title>Large-scale information 2ow in conscious and unconscious states: An ECoG study in monkeys</article-title>. <source>PloS ONE</source>. <year>2013</year>; <volume>8</volume>(<issue>11</issue>):<fpage>e80845</fpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Yeh</surname> <given-names>CH</given-names></string-name>, <string-name><surname>Shi</surname> <given-names>W.</given-names></string-name> <article-title>Generalized multiscale Lempel‚ÄìZiv complexity of cyclic alternating pattern during sleep</article-title>. <source>Nonlinear Dynamics</source>. <year>2018</year>; <volume>93</volume>(<issue>4</issue>):<fpage>1899</fpage>‚Äì<lpage>1910</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><surname>Zenil</surname> <given-names>H</given-names></string-name>, <string-name><surname>Hern√°ndez-Orozco</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kiani</surname> <given-names>N</given-names></string-name>, <string-name><surname>Soler-Toscano</surname> <given-names>F</given-names></string-name>, <string-name><surname>Rueda-Toicen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Tegn√©r</surname> <given-names>J.</given-names></string-name> <article-title>A decomposition method for global evaluation of Shannon entropy and local estimations of algorithmic complexity</article-title>. <source>Entropy</source>. <year>2018</year>; <volume>20</volume>(<issue>8</issue>):<fpage>605</fpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><string-name><surname>Zenil</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kiani</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Marabita</surname> <given-names>F</given-names></string-name>, <string-name><surname>Deng</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Elias</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schmidt</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ball</surname> <given-names>G</given-names></string-name>, <string-name><surname>Tegn√©r</surname> <given-names>J.</given-names></string-name> <article-title>An algorithmic information calculus for causal discovery and reprogramming systems</article-title>. <source>iScience</source>. <year>2019</year>; <volume>19</volume>:<fpage>1160</fpage>‚Äì<lpage>1172</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>XS</given-names></string-name>, <string-name><surname>Roy</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>EW</given-names></string-name>. <article-title>EEG complexity as a measure of depth of anesthesia for patients</article-title>. <source>IEEE transactions on Biomedical Engineering</source>. <year>2001</year>; <volume>48</volume>(<issue>12</issue>):<fpage>1424</fpage>‚Äì<lpage>1433</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>XS</given-names></string-name>, <string-name><surname>Zhu</surname> <given-names>YS</given-names></string-name>, <string-name><surname>Thakor</surname> <given-names>NV</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>ZZ</given-names></string-name>. <article-title>Detecting ventricular tachycardia and 1brillation by complexity measure</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>1999</year>; <volume>46</volume>(<issue>5</issue>):<fpage>548</fpage>‚Äì<lpage>555</lpage>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="other"><string-name><surname>Ziv</surname> <given-names>J.</given-names></string-name> <article-title>Coding theorems for individual sequences</article-title>. <source>IEEE Transactions on Information Theory</source>. <year>1978</year>;.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1">
<label><sup>1</sup></label>
<p>Technically, this applies only under certain conditions. The process must be ergodic (and strict-sense stationary), so that its joint probability distribution does not change over time ‚Äîi.e. for any set of indices { <sub>1</sub>, ‚Ä¶,} the process satisfies that <inline-formula><alternatives><inline-graphic xlink:href="534922v1_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for all Although strict, this is a common requirement in time series analysis methods.</p></fn>
<fn id="fn2">
<label><sup>2</sup></label>
<p>Note that this normalisation applies only to the LZ76 algorithm proposed in <bold><italic>Lempel and Ziv</italic></bold> (<bold><italic>1976</italic></bold>). Later versions (e.g. LZ77 or LZ78) also converge to the entropy rate but need other normalisation values.</p></fn>
<fn id="fn3">
<label><sup>3</sup></label>
<p>Although see contradictory reports in <bold><italic><xref ref-type="bibr" rid="c48">Pal et al. (2020</xref></italic></bold>).</p></fn>
<fn id="fn4">
<label><sup>4</sup></label>
<p>Note that this is different from simply pre-filtering the signals at specific frequency bands and then computing LZ (or CSER) on the filtered sequence, which has no relation to the broadband LZ (or CSER).</p></fn>
<fn id="fn5">
<label><sup>5</sup></label>
<p>Though they later relaxed their claims on the topic.</p></fn>
<fn id="fn6">
<label><sup>6</sup></label>
<p>For example, in the case of <bold><italic><xref ref-type="bibr" rid="c57">Timmermann et al. (2019</xref></italic></bold>), the LZ changes are driven by the pharmacokinetics of DMT that take place at a much slower time scale than neural activity.</p></fn>
<fn id="fn7">
<label><sup>7</sup></label>
<p>As an analogy, consider the case of an orbiting planet being hit by an asteroid. The planet‚Äôs trajectory is highly predictable (knowing Kepler‚Äôs laws), up to the moment of impact ‚Äîwhen a high prediction error would occur in a hypothetical observer. After the impact, the ability to predict is restored, as Kepler‚Äôs laws continue to apply. Importantly, the prediction error (increase in instantaneous entropy rate) <italic>precedes</italic> the time when the new orbit maximally deviates from the original one (akin to difference in activity).</p></fn>
<fn id="fn8">
<label><sup>8</sup></label>
<p>More technically, we say that state-space models are <italic>closed</italic> with respect to several transformations that neural data typically goes through. For example, if a system of variables following a state-space model is linearly mixed (e.g. with a forward or inverse model for source reconstruction), or is temporally or spatially subsampled, the resulting system is also a state-space model. The same is not true of other time series models, such as auto-regressive models.</p></fn>
<fn id="fn9">
<label><sup>9</sup></label>
<p>Technically, this is because the HRF may not be a <italic>‚Äò</italic>minimum-phase‚Äô filter ‚Äîalthough this claim has been contested (<bold><italic><xref ref-type="bibr" rid="c5">Barnett and Seth, 2015</xref></italic></bold>). A separate concern is that fMRI operates at timescales much slower than the underlying neural process, which makes drawing conclusions about neural dynamics from fMRI a difficult task (<bold><italic><xref ref-type="bibr" rid="c6">Barnett and Seth, 2017</xref></italic></bold>).</p></fn>
<fn id="fn10">
<label><sup>10</sup></label>
<p>This is a direct consequence of the Asymptotic Equipartition Property, and most fundamentally from the Shannon-McMillan-Breiman theorem for stationary ergodic processes (<bold><italic><xref ref-type="bibr" rid="c15">Cover and Thomas, 2006</xref></italic></bold>, Ch. 3).</p></fn>
<fn id="fn11">
<label><sup>11</sup></label>
<p><italic>‚Äò</italic>Intrinsic‚Äô here means that it is a property of the stochastic process itself, and does not depend on which model one may choose to predict the next value of the signal (since it is the minimum error achievable by any model).</p></fn>
<fn id="fn12">
<label><sup>12</sup></label>
<p>Unfortunately, the standard symbol for both entropy and the transfer function is the letter <italic>H</italic>. Although it should be clear from the context, to avoid confusion we adopt the non-standard symbol <italic>M</italic> for the transfer function.</p></fn>
<fn id="fn13">
<label><sup>13</sup></label>
<p>As a simple example of this, consider strings generated via this short snippet in the Python programming language: <monospace>negate = lambda x: 1 - x; reduce(lambda s,r: s + list(map(negate, s)), range(T), [0])</monospace> The usual dictionary-based implementation of LZ yields large complexity values, which diverge to infinity as T grows.</p></fn>
</fn-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88683.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Wyart</surname>
<given-names>Valentin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Inserm</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This paper describes a new complexity estimator for time series based on state-space modeling, which can directly decompose signal entropy in both time and frequency. The authors compare their estimator to Lempel-Ziv (LZ) complexity using a variety of time series neurophysiological data from humans and non-human primates. This represents a potentially <bold>valuable</bold> methodological contribution for existing studies using LZ complexity in their analyses, although the paper currently ignores much of the existing literature which has already developed related solutions to the same issues. The strength of the evidence supporting the superiority of the new complexity metric is currently <bold>incomplete</bold>, and should be backed by additional analyses.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88683.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this paper, the authors attempt to overcome the &quot;fundamental limitations&quot; of Lempel-Ziv complexity by developing and testing a complexity estimator based on state-space modelling (CSER) that they argue allows higher temporal resolution and spectral decomposition of entropy directly. They test the performance of this approach using MEG, EEG, and ECoG data from monkeys and humans. Although in principle, these developments might be useful for those already using LZ complexity in their analyses, these developments ignore much of the non-LZ entropy community which has already developed related solutions to the same issues. It is thus not clear currently whether this approach is necessary or unique per se:</p>
<p>‚Ä¢ As the authors intimate, LZ is a relatively crude but efficient estimator; it leverages a simple binarization of time points above and below the time series mean to look at patterns (in turn disregarding the magnitude of the signal itself). The unique benefit of LZ in and of itself is not at all clear to this reviewer. It is nearly guaranteed that LZ will be extremely highly correlated with various other common measures of &quot;discrete&quot; entropy (especially permutation entropy, which ranks all time-series points prior to computing motifs/patterns rather than anchor anything by the mean (as does LZ), but nevertheless ignores the value range of the signal). The general appeal of the authors' intended developments to further improve LZ specifically would dramatically boost should they be able to make a case that LZ is somehow special, to begin with.</p>
<p>‚Ä¢ Beyond this, we can now turn to the authors' rationale for the LZ developments proposed. Despite the authors' statement in the abstract that LZ complexity is &quot;the most widely used approach complexity of neural dynamics,&quot; to my knowledge, sample entropy (and its multiscale variant, MSE) is much more commonly used in cognitive neuroscience. Such measures of entropy already enjoy several benefits over LZ. First, the continuous magnitude of the signal is relevant in sample entropy (i.e., it is not discrete in the same way as LZ because the values of each data point matter prior to the estimation of patterns). This is important for people in that community because electrophysiologists/neuroimagers often assume the values of the signal to matter (e.g., for ERPs, the magnitude of power, etc.). Ignoring the magnitude of signal values altogether, as in LZ, is a somewhat dramatic choice, especially if the authors then end up arguing that the spectral decomposition of entropy itself is valuable (after signal value ranges have been ignored!). In any case, as far as I know, LZ has never been shown the be more sensitive than e.g., sample entropy/MSE in relation to any outcome variable, but perhaps the authors can provide evidence for this and argue what LZ should practically do that is unique. Second, the use of MSE more easily allows (although not without its challenges) to directly compare spectral power and single/multiscale entropy straight away, which has been done in quite some depth already without the need for a state-space model of any kind (e.g., Kosciessa et al., 2020, PLOS CB). Instead of using a standard spectral power approach and comparing to entropy, the authors propose the spectrally decompose CSER entropy time series directly. Why? What should this do over standard multi-scale entropy approaches (like MSE, which estimate &quot;fast&quot; and &quot;slow&quot; complexity dynamics), which do not require a Fourier? And if they already believe that the spectrum cannot capture entropy (hence rationalizing the use of LZ-type measures in general), why do they want to invoke spectral estimation assumptions into the estimation of entropy when they could just compare the standard spectrum to entropy to begin with, without any complex modelling in between? I just don't see the need for a lot of what is proposed here; the authors provide solutions to problems that (at least for several in this community) may not exist at all.</p>
<p>‚Ä¢ Figure 2: the authors show results descriptively comparing LZ and CSER, but without comparing the two measures directly. The patterns overall look extremely similar; why not correlate the values from the two measures in each dataset to make a case for what CSER is adding here? By eye test, it appears they will be extremely highly correlated, which leaves the reader wondering what CSER (with all of its model complexities and assumptions) has added.</p>
<p>‚Ä¢ On the logic of and evidence for the use of CSER: The use of a state space model to allow estimation of &quot;prediction errors&quot; appears to be akin to a latent autocorrelation model with a lag/step size of 1 time-point, and trained only on prestim baseline data. When a successive time point is &quot;deviant&quot; from that autocorrelative function, the authors argue that this provides a measure of instantaneous entropy. This seems simple at first glance, but it is very difficult for this reviewer to wrap their head around. This approach anchors stim-related entropy estimation to prestim entropy for every subject, disallowing the direct comparison of values across subjects during the stimulus phase itself. This does not directly provide a measure of instantaneous task-related entropy, but a mixture of pre and post stim sources based on a state-space model. Does it need to be this complicated? Why does a simple window-based function not suffice to generate temporal dynamics of entropy without coupling the task-based signal to the prestim period? There are many such approaches already existing in the field.</p>
<p>‚Ä¢ Figure 3: The authors show that gamma-band CSER is the most sensitive. Isn't it true that this is the exact inverse of the dominance of typical spectral effects under such conditions (that across the literature in psychedelics, sleep, and anaesthesia, there are dominant shifts in low-frequency spectral power)? Although low-frequency power is expected to be a dominant determinant of entropy in the entire signal (see Kosciessa et al., 2020, PLOS CB), something else appears to be happening here. At face value, because gamma is the spectral band with the lowest power in every imaging modality we know of, there is inherently less repeatability/autocorrelation in that same signal, which necessarily should produce more &quot;prediction error/instantaneous entropy&quot; in any condition. When the authors then take the &quot;mean difference&quot; of gamma-based entropy values from each of the two conditions in each sample, any condition-based shift in entropy should inherently be easier to detect. In any case, why not simply show these CSER spectral results next to a standard spectrum over the same conditions and then directly compare the unique utility of e.g., gamma power to CSER gamma? And if you compute something like the percent change between conditions for each spectral band, do you maintain gamma dominance?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88683.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper presents a novel measure of complexity that can be applied to recorded neurophysiological time series. The paper first introduces an existing measure, Lempel-Ziv complexity, reviewing its computation, application, and potential issues. They then present their new metric: CSER. They show CSER values change similarly to LZ under psychedelics, sleep, and general anaesthesia. A key advantage of CSER is that it can be decomposed in both time and frequency. They give example applications for each of these. They show the differences in CSER in the previous examples are mostly located in the gamma band. For a temporal example, they consider monkey ecog in an oddball task and so CSER changes between oddballs and deviants.</p>
<p>Major comments</p>
<p>
Most of the technical details are rightly in the methods, but it would be nice as a reader to have more of a concrete idea of the type of state space model used in the main text, the assumptions underlying this, and typical orders used perhaps with a schematic diagram etc. I appreciate they have written the paper to appeal to a broad general audience, but it seems like this is an important part of the method that anyone using the method should understand in more detail.</p>
<p>It might be nice to cover some other methods of signal variation e.g. as reviewed in Washke et al. Neuron 2021 and how CSER fits into the broader taxonomy of measures of neural variability (even if restricted to information-theoretic ones e.g. multi-scale entropy and permutation entropy, which have also been linked to prediction in the brain Washke et al. elife 2019).</p>
<p>While the examples are clear and well-motivated, the novel parts could be more developed in terms of interpretation, or linking to existing measures. For example, the frequency results show the complexity changes in &quot;gamma&quot; which is defined as &gt;25Hz. From a biological point of view, it would be nice to understand this better, perhaps splitting low gamma (including 40Hz oscillations) from high gamma (ie MUA). How is the frequency measure affected by the width of the frequency band considered? I understand the sum of the shown terms equals the broadband result but e.g. in Figure 3 if the values were normalised by the bandwidth of each band, gamma might not stand out so much (as it is by far the widest band, 75Hz vs 3Hz for the delta). So if gamma is not contributing more per-unit of frequency, the interpretation might be different. What is it about the gamma band activity that is changing between the conditions: autocorrelation of power, more variability in phase procession? What would this measure give for simulated systems with known changes (for example, changes in oscillatory power, or changes in 1/f slope). What sort of system would give the profiles in Figure 3?</p>
<p>For the temporal example, the result is a nice proof of concept. It looks quite reminiscent of &quot;novel mutual information&quot; time-course (e.g. compare the absolute value of CSER difference to Figure 13, Ince et al HBM 2017, which also showed two peaks of novel information at the time where the gradient of the ERP starts to change, 20-30ms prior to the ERP peak, but in a task with no predictive component). It might be nice to explicitly compare the statistical power to this existing method (conditional mutual information between signal+gradient and experimental condition, conditioning out the selection of previous time points with peak conditional MI). Deviant stimuli initially seem to decrease entropy - by eye, it's surprising this isn't significant (stands out a lot from baseline). Was a two-sided or one-sided (matching the prior hypothesis) test performed here? Could it be that the change in entropy rate is a property of any ERP signal (ie it looks like the change in CSER reflects the following difference in peak ERP - for the first negative peak, the deviant amplitude is lower, for the second positive peak the deviant amplitude is higher), and a lower level signal interpretation (ie amplitude of CSER difference is related to the difference in ERP amplitude, rather than directly reflecting neural mechanisms of prediction).</p>
</body>
</sub-article>
</article>