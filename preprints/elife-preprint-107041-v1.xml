<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107041</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107041</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107041.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Listening to the room: disrupting activity of dorsolateral prefrontal cortex impairs learning of room acoustics in human listeners</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9135-3973</contrib-id>
<name>
<surname>Hernández-Pérez</surname>
<given-names>Heivet</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>heivet.hernandez-perez@mq.edu.au</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1416-4164</contrib-id>
<name>
<surname>Monaghan</surname>
<given-names>Jessica JM</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6085-9269</contrib-id>
<name>
<surname>Mikiel-Hunter</surname>
<given-names>Jason</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0006-3262-9833</contrib-id>
<name>
<surname>Traer</surname>
<given-names>James</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sowman</surname>
<given-names>Paul F</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5467-6725</contrib-id>
<name>
<surname>McAlpine</surname>
<given-names>David</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sf06y89</institution-id><institution>Department of Linguistics and Macquarie University Hearing, The Australian Hearing Hub, Macquarie University</institution></institution-wrap>, <city>Sydney</city>, <country country="AU">Australia</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02swxtp23</institution-id><institution>National Acoustic Laboratories</institution></institution-wrap>, <city>Macquarie Park</city>, <country country="AU">Australia</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/036jqmy94</institution-id><institution>Psychological and Brain Sciences Department, The University of Iowa</institution></institution-wrap>, <city>Iowa City</city>, <country country="US">United States</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zvqw119</institution-id><institution>Auckland University of Technology, Physiotherapy</institution></institution-wrap>, <city>Auckland</city>, <country country="NZ">New Zealand</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Herrmann</surname>
<given-names>Björn</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Baycrest Hospital</institution>
</institution-wrap>
<city>Toronto</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>King</surname>
<given-names>Andrew J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-09-02">
<day>02</day>
<month>09</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107041</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-04-20">
<day>20</day>
<month>04</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-04-05">
<day>05</day>
<month>04</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.04.04.644835"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Hernández-Pérez et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Hernández-Pérez et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107041-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Navigating complex sensory environments is critical to survival, and brain mechanisms have evolved to cope with the wide range of surroundings we encounter. To determine how listeners learn the statistical properties of acoustic spaces, we assessed their ability to perceive speech in a range of noisy and reverberant rooms. Listeners were also exposed to repetitive transcranial stimulation (rTMS) to disrupt the dorsolateral prefrontal cortex (dlPFC) activity, a region believed to play a role in statistical learning. Our data suggest listeners rapidly adapt to statistical characteristics of an environment to improve speech understanding. This ability is impaired when rTMS is applied bilaterally to the dlPFC. The data demonstrate that speech understanding in noise is best when exposed to a room with reverberant characteristics common to human-built environments, with performance declining for higher and lower reverberation times, including fully anechoic (non-reverberant) environments. Our findings provide evidence for a reverberation “sweet spot” and the presence of brain mechanisms that might have evolved to cope with the acoustic characteristics of listening environments encountered every day.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Learning occurs over multiple time scales—evolutionary, developmental and moment-to-moment—to support a diverse range of abilities: navigating complex sensory environments (<xref ref-type="bibr" rid="c29">Bregman, 1994</xref>; <xref ref-type="bibr" rid="c90">Lewicki et al., 2014</xref>; <xref ref-type="bibr" rid="c130">Smith &amp; Lewicki, 2006</xref>), expressing intricate behaviours in social settings (<xref ref-type="bibr" rid="c58">Gariépy et al., 2014</xref>; van den Bos et al., 2013), or acquiring communication skills such as bird song (<xref ref-type="bibr" rid="c24">Brainard &amp; Doupe, 2002</xref>; <xref ref-type="bibr" rid="c89">Lauay et al., 2004</xref>; <xref ref-type="bibr" rid="c93">Marler, 1970</xref>) or spoken language (<xref ref-type="bibr" rid="c7">Aslin et al., 1998</xref>; <xref ref-type="bibr" rid="c118">Saffran et al., 1996</xref>; <xref ref-type="bibr" rid="c120">Saffran, 2003</xref>). Although some learning requires active or explicit involvement in a task (<xref ref-type="bibr" rid="c76">Huyck &amp; Wright, 2011</xref>; <xref ref-type="bibr" rid="c94">Mathews et al., 1989</xref>; <xref ref-type="bibr" rid="c113">Rebuschat, 2015</xref>), perhaps with a system of rewards and punishments to make the learning ‘stick’ (<xref ref-type="bibr" rid="c14">Barberis, 2013</xref>; <xref ref-type="bibr" rid="c125">Schultz, 2002</xref>; <xref ref-type="bibr" rid="c139">Wächter et al., 2009</xref>), other forms of learning seem automatic or implicit (<xref ref-type="bibr" rid="c112">Reber, 1967</xref>), acquired with individuals seemingly unaware it is taking place. This type of learning, referred to as ‘statistical learning’ (<xref ref-type="bibr" rid="c4">Ambrus et al., 2020</xref>; <xref ref-type="bibr" rid="c118">Saffran et al., 1996</xref>), ‘sequence learning’ (<xref ref-type="bibr" rid="c103">Nissen &amp; Bullemer, 1987</xref>; <xref ref-type="bibr" rid="c137">Vékony et al., 2022</xref>) or ‘sequential learning’ (<xref ref-type="bibr" rid="c39">Conway &amp; Christiansen, 2001</xref>; <xref ref-type="bibr" rid="c137">Vékony et al., 2022</xref>), is thought to entail automatic and incidental extraction of regularities or patterns within external stimuli or in the environment (<xref ref-type="bibr" rid="c37">Conway, 2020</xref>; <xref ref-type="bibr" rid="c133">Takács et al., 2021</xref>). Evident across sensory modalities to support automatic learning of tonal (<xref ref-type="bibr" rid="c119">Saffran et al., 1999</xref>) or linguistic (<xref ref-type="bibr" rid="c118">Saffran et al., 1996</xref>) sequences, strings of letters (<xref ref-type="bibr" rid="c112">Reber, 1967</xref>), visual scenes and shapes (<xref ref-type="bibr" rid="c54">Fiser &amp; Aslin, 2001</xref>), visual-motor patterns (<xref ref-type="bibr" rid="c103">Nissen &amp; Bullemer, 1987</xref>), and even tactile input (<xref ref-type="bibr" rid="c40">Conway &amp; Christiansen, 2005</xref>) statistical learning appears to be a unitary, domain-general phenomenon, potentially governed by a single mechanism or neurocognitive principle (<xref ref-type="bibr" rid="c37">Conway, 2020</xref>; <xref ref-type="bibr" rid="c85">Kirkham et al., 2002</xref>).</p>
<p>Increasing evidence suggests that statistical learning is a critical part of how listeners deal with complex and cluttered acoustic scenes. Potential background sounds such as rain or insects—referred to as sound textures (<xref ref-type="bibr" rid="c70">Hicks &amp; McDermott, 2024</xref>; <xref ref-type="bibr" rid="c96">McDermott et al., 2013</xref>; <xref ref-type="bibr" rid="c98">McWalter &amp; McDermott, 2018</xref>) as well as changes in the regularity of sound patterns (<xref ref-type="bibr" rid="c11">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c18">Bianco et al., 2020</xref>) are processed—seemingly unconsciously—in terms of their summary statistics. This form of statistical learning likely contributes to our ability to follow conversations in background noise (‘cocktail party listening’; <xref ref-type="bibr" rid="c34">Cherry, 1953</xref>) and to deal with reverberant spaces where multiple, delayed copies of the same sound reach a listener in the form of reflections from walls and other acoustically opaque surfaces (<xref ref-type="bibr" rid="c21">Blesser &amp; Salter, 2009</xref>; <xref ref-type="bibr" rid="c117">Sabine, 1953</xref>; <xref ref-type="bibr" rid="c124">Schroeder, 1962</xref>). Though listeners rely on early-arriving sound energy to determine source location—suppressing potentially conflicting localization cues in later-arriving, often more intense, sound energy (<xref ref-type="bibr" rid="c21">Blesser &amp; Salter, 2009</xref>; <xref ref-type="bibr" rid="c23">Bradley et al., 1999</xref>; <xref ref-type="bibr" rid="c44">Culling et al., 2003</xref>; <xref ref-type="bibr" rid="c72">Houtgast &amp; Steeneken, 1985</xref>; <xref ref-type="bibr" rid="c102">Nielsen &amp; Dau, 2010</xref>)—the perception of reflected sound energy is informative of the listening environment more broadly (<xref ref-type="bibr" rid="c30">Bronkhorst &amp; Houtgast, 1999</xref>; <xref ref-type="bibr" rid="c127">Shinn-Cunningham, 2000</xref>). This includes whether environments are indoors or outdoors, their identity and dimensions (<xref ref-type="bibr" rid="c31">Brumm &amp; Naguib, 2009</xref>; <xref ref-type="bibr" rid="c32">Cabrera et al., 2005</xref>; <xref ref-type="bibr" rid="c87">Kolarik et al., 2021</xref>; <xref ref-type="bibr" rid="c148">Zahorik &amp; Wightman, 2001</xref>), as well as the number of occupants or potential interfering sources (<xref ref-type="bibr" rid="c23">Bradley et al., 1999</xref>; <xref ref-type="bibr" rid="c44">Culling et al., 2003</xref>; <xref ref-type="bibr" rid="c66">Hawley et al., 2004</xref>; <xref ref-type="bibr" rid="c72">Houtgast &amp; Steeneken, 1985</xref>; <xref ref-type="bibr" rid="c102">Nielsen &amp; Dau, 2010</xref>; <xref ref-type="bibr" rid="c107">Peissig &amp; Kollmeier, 1997</xref>).</p>
<p>Nevertheless, despite its potential utility for understanding background features of a sound environment, the accumulation over time of late-arriving, reverberant sound energy is thought to generate an additional burden on listening performance beyond that from sound energy direct from interfering sources (<xref ref-type="bibr" rid="c73">Houtgast &amp; Steeneken, 1973</xref>; <xref ref-type="bibr" rid="c86">Knudsen, 1929</xref>; <xref ref-type="bibr" rid="c92">Lochner &amp; Burger, 1961</xref>; <xref ref-type="bibr" rid="c123">Santon, 1976</xref>; <xref ref-type="bibr" rid="c128">Shinn-Cunningham &amp; Kawakyu, 2003</xref>), smearing the acoustic waveform and occluding temporal gaps that might otherwise be helpful for ‘glimpsing’ speech in background noise (<xref ref-type="bibr" rid="c41">Cooke, 2006</xref>). If listeners are able to utilize late-arriving, reverberant energy of known acoustic environments to supress disruptive spatial cues and enhance speech understanding (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>; <xref ref-type="bibr" rid="c138">Vlahou et al., 2019</xref>; <xref ref-type="bibr" rid="c141">Watkins, 2005a</xref>, <xref ref-type="bibr" rid="c142">2005b</xref>), this suggests that the acoustic characteristics of an environment can be learned and stored for later use in complex listening tasks.</p>
<p>Here, using an ecologically relevant listening task—understanding speech in background noise—we assessed the ability of human listeners to learn the statistical structure of different sound environments defined by their RT<sub>60,</sub> the time it takes for reverberant energy to decay by 60 decibels (dB), and found that speech understanding in background noise improved over time. Specifically, when asked to report words from unfamiliar and semantically uninformative spoken sentences of varying duration in noise convolved with the reverberant qualities of different acoustic environments, listeners’ performance improved with increasing sentence duration, and with repeated exposure to each environment (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>). This capacity for learning the acoustic environment to aid speech understanding was diminished when repetitive transcranial magnetic stimulation (rTMS) was applied bilaterally to impair the function of dorsolateral prefrontal cortex (dlPFC), a cortical locus hypothesised to contribute to statistical learning (<xref ref-type="bibr" rid="c4">Ambrus et al., 2020</xref>; <xref ref-type="bibr" rid="c137">Vékony et al., 2022</xref>). Counter to expectations that reverberant energy can only harm speech understanding, listeners’ abilities to leverage the knowledge of a sound environment (talker identity, speech corpus, noise characteristics, spatial configuration) to improve speech understanding was best when a ‘typical’ room with reverberant characteristics close to the average of a wide range of common (built) environments was included as one of the three environments presented in a single experimental run. Performance declined systematically when this room was switched for one with more, or less, reverberation, including a fully anechoic (i.e., non-reverberant or dry) environment in which listeners initially trained on the recall task. Importantly in the context of other potentially learnable acoustic features, talker identity (three male and three female) represented a random variable in our experimental design. Evidence for a reverberation ‘sweet spot’ suggests the existence of brain mechanisms adapted to the longer-term structure of common listening environments (<xref ref-type="bibr" rid="c135">Traer &amp; McDermott, 2016</xref>).</p>
<p>Our data suggest listeners rapidly ‘tune in’ or adapt to the background (the statistical structure of the sound environment, including its reverberation profile) and use this knowledge to improve their performance in a listening task. This benefit of learned listening appears most evident when encountering levels of reverberation commonly experienced in everyday settings. The data also support the view that listeners retain information about the acoustic background long enough for it to influence listening performance at some later time, consistent with <italic>in vivo</italic> experimental evidence of increased adaptive capacity for neural learning of sound environments upon repeated exposure to those environments (<xref ref-type="bibr" rid="c50">Dean et al., 2005</xref>, <xref ref-type="bibr" rid="c51">2008</xref>; <xref ref-type="bibr" rid="c79">Ivanov et al., 2022</xref>) and that cortical circuits modulate this capacity (<xref ref-type="bibr" rid="c115">Robinson et al., 2016</xref>). If long-term learning of reverberant environments relies on, or even establishes, a preferred range of RT<sub>60</sub>s for speech understanding, it suggests modifications might be required to listening assessments and technologies—routinely developed under anechoic conditions—to generate optimal performance outcomes.</p>
</sec>
<sec id="s2">
<title>Materials and methods</title>
<sec id="s2a">
<title>Participants</title>
<p>All 62 participants (53 females, ages between 19-26 years old (mean ± SD = 22 ± 2 years old)) included in the study were Australian native-English speakers, had normal pure tone thresholds (&lt; 20 dB HL tested at octave intervals between 0.5-8 kHz (<xref ref-type="bibr" rid="c75">Hughson &amp; Westlake, 1944</xref>); Interacoustics Hearing Aid Fitting Analyzer Affinity 2.0 Audiometry) and normal middle ear function (assessed using standard 226 Hz tympanometry; Titan, Interacoustics). To ensure normal outer hair cell function, all participants were screened for Distortion Product Otoacoustic Emissions (DPOAEs) between 0.5-10 kHz (stimulus parameters: f1/f2 =1.2, f1 =65 dB SPL; f2 =55 dB SPL; responses parameters: SNR &gt; 6 dB, signal level &gt; −10 dB SPL, and reliability &gt; 98% (DPOAE440; Titan, Interacoustics). All participants had normal steady-state ipsilateral broadband noise middle ear muscle reflexes (MEMR) (Titan, Interacoustics).</p>
</sec>
<sec id="s2b">
<title>Acoustic stimuli</title>
<p>To assess listeners’ abilities to understand speech in background noise across different sound environments, they were asked to verbally report keywords spoken by a talker virtually located in front of the participant that were masked by white noise arriving from a virtual source 90° to the left (<xref rid="fig1" ref-type="fig">Figure 1A</xref>) (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>). A binaural configuration was necessary as speech intelligibility improvements following exposure to room acoustics is significantly reduced under monaural listening conditions (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>). The speech stimuli used in this study were from the Coordinate Response Measure (CRM) corpus (<xref ref-type="bibr" rid="c22">Bolia et al., 2000</xref>), and all combinations of, |Callsigns| (‘Baron’, ‘Eagle’, ‘Charlie’, ‘Tiger’, ‘Arrow’, ‘Ringo’, ‘Laker’, ‘Hoper’), |Colors| (four monosyllabic choices, ‘red’, ‘white’, ‘blue’, or ‘green’), and |Numbers| (the English digits between ‘one’ and ‘eight’) were used in this experiment (<xref rid="fig1" ref-type="fig">Figure 1B</xref>) (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>). The masking white noise was randomly generated on a PC using MATLAB: <italic>RRID:SCR_001622</italic> (The MathWorks, Inc. of Natick, MA: <ext-link ext-link-type="uri" xlink:href="https://au.mathworks.com/">https://au.mathworks.com/</ext-link>) and preceded the speech stimuli by 150 ms, during which its amplitude linearly increased from zero to full scale. The masker was present throughout the speech and ended (without ramping) with the speech.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig 1.</label>
<caption><title>Experimental environment.</title>
<p><bold>A.</bold> Speech-in-noise immersed in real reverberant environments reproduced in an anechoic chamber over a 41-loudspeaker array. All speech material was rendered as if it was originating from directly in front of a listener and was masked by spatially separated (90° to the left) white noise at −15 dB signal-to-noise ratio (SNR). The noise masker level was fixed at 70 dB SPL. The speech material was convolved with the measured impulse response of a variety of different real rooms, from anechoic to highly reverberant real rooms such as an underground car park (see <xref ref-type="table" rid="tbl1">Table 1</xref> methods for acoustic details). <bold>B.</bold> Structure of the speech material. Sentences from the Coordinated Response Measures (CRM) corpus were modified to vary their duration depending on how many words preceded the target phrase i.e., ‘carrier phrase’ (CP) length. Participants were asked to recognize and repeat target words: |Color| from a list of four and |Number| from a 1-8 list. <bold>C.</bold> Example of the experimental paradigm. Trials consisted of ‘carrier phrases’ spoken in a specific room i.e., ‘go to blue 1 now’ in the Lecture Room. Participants were never exposed to the same room consecutively and the task lasted no longer than 45 minutes. <bold>D.</bold> Sensitivity to |Color| and |Number| combined in the Lecture Room, Open-Plan Office and Underground Car Park. Mean d’ (measure of accuracy calculated as: Z (correct responses) – Z (false alarm)] denoted as circles in the boxplot (n=22 for all rooms). The horizontal line denotes the median. Upper and lower limits of the boxplot represent <sup>1s</sup>t (q1) and <sup>3r</sup>d (q3) quartiles respectively, while whiskers denote the interquartile range (IQR =q3-q1). As previously reported (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>), we observed an increase in performance for longer ‘carrier phrases’ in all environments. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25949/24295342.v1">https://doi.org/10.25949/24295342.v1</ext-link>. All images in this figure were generated using artificial intelligence, with the exception of the Anechoic Room photograph, which was taken by the authors</p></caption>
<graphic xlink:href="644835v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Real rooms acoustic characteristics.</title></caption>
<graphic xlink:href="644835v1_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s2c">
<title>Virtual Environments</title>
<p>Reverberant environments were reproduced by convolving the generated acoustic signals with Room Impulse Responses (RIRs) obtained using 62-channel microphone array recordings of real rooms that were subsequently decoded into 41 higher-order Ambisonic (HOA) channels (<xref ref-type="bibr" rid="c8">Badajoz-Davila et al., 2020</xref>; <xref ref-type="bibr" rid="c145">Weisser et al., 2019</xref>). These final decoded channels corresponded to the spherical array of 41 Tannoy V8 concentric loudspeakers (Tannoy) installed in the anechoic chamber at the Australian Hearing Hub where testing took place. To optimize the “directionality” of the acoustic stimuli, the speech and masking noise signals were separately convolved with RIRs that had enhanced direct sound components in loudspeakers associated with their respective virtual source locations (i.e., 0° azimuth loudspeaker for the target talker and 90° azimuth loudspeaker for the masking noise) (<xref ref-type="bibr" rid="c8">Badajoz-Davila et al., 2020</xref>; <xref ref-type="bibr" rid="c145">Weisser et al., 2019</xref>). In the anechoic condition, RIRs only incorporating these enhanced direct sound components were convolved with the speech and masking noise signals. The SNR for speech-in-noise during data collection was −15 dB to avoid ceiling performance and was manipulated after convolution with the RIR by adjusting the gain of the speech target relative to a fixed masker level of 70 dB SPL. All signal processing was performed on a PC using MATLAB (Mathworks) and the final spatialized signal were presented via the RME MADI sound card (RME Audio) to two RME 32-channel digital-to-analog converters (M-32, RME Audio). These, in turn, fed 11 Yamaha XM4180 power amplifiers (Yamaha) that drove the loudspeaker array.</p>
<p>To compare our recordings to RIR data recorded and analyzed by <xref ref-type="bibr" rid="c135">Traer and McDermott (2016)</xref>, we also calculated RT<sub>60</sub>s for each room by integrating across the 41 HOA channels of each RIR and then calculating the median RT<sub>60</sub> value in 31 frequency sub-bands with centre frequencies ranging from 80 Hz to 10 kHz (<xref ref-type="bibr" rid="c135">Traer &amp; McDermott, 2016</xref>). We refer to these measures of reverberation as RT<sub>60traer</sub>. The range of sub-bands and their frequency selectivity was chosen to match that of the human ear (<xref ref-type="bibr" rid="c62">Glasberg &amp; Moore, 1990</xref>; <xref ref-type="bibr" rid="c97">McDermott &amp; Simoncelli, 2011</xref>). See <italic>SI Materials and Methods,</italic> (<xref ref-type="bibr" rid="c135">Traer &amp; McDermott, 2016</xref>), for further information concerning how individual RT<sub>60traer</sub>s were extracted from each sub-band.</p>
</sec>
<sec id="s2d">
<title>Identity of the selected sound environments</title>
<p>Three rooms were selected, with similar Reverberation Times (RT<sub>60</sub>s) to those employed by <xref ref-type="bibr" rid="c28">Brandewie and Zahorik (2013)</xref>. As Bajadoz-Davila et al. (2020) and <xref ref-type="bibr" rid="c28">Brandewie and Zahorik (2013)</xref> calculated these RT<sub>60</sub>s according to ISO-3382 guidelines (ISO, 2009), they are referred to in the text as RT<sub>60iso</sub>: Lecture room (RT<sub>60iso</sub> = 0.46 s), Open-Plan Office (RT<sub>60iso</sub> = 0.96 s) and Underground Car Park (RT<sub>60iso</sub> = 2.42 s) (<xref rid="tbl1" ref-type="table">Table 1</xref>). Additionally, we assessed performance in different combinations of rooms to determine whether a specific combination was necessary to improve speech performance as ‘carrier phrase length’ was increased. For each combination, Lecture Room was swapped with one of three virtual environments: Anechoic (RT<sub>60iso</sub> = n/a), Living Room (RT<sub>60iso</sub> = 0.33 s) and Highly Reflective Room (RT<sub>60iso</sub> of 1.55 s) (<xref rid="tbl1" ref-type="table">Table 1</xref>). Naïve participants were recruited for each room combination.</p>
</sec>
<sec id="s2e">
<title>Continuous theta-burst stimulation</title>
<p>Applying repetitive Transcranial Magnetic Stimulation (rTMS)—specifically continuous theta burst stimulation (cTBS)—to the right and left dlPFC, elicits a period of depressed cortical excitability in the targeted area that lasts about an hour post-stimulation (<xref ref-type="bibr" rid="c57">Gamboa et al., 2010</xref>; <xref ref-type="bibr" rid="c71">Hoogendam et al., 2010</xref>; <xref ref-type="bibr" rid="c74">Huang et al., 2005</xref>), the time during which speech recall in reverberant rooms was assessed. Participants were screened for contra-indications to rTMS (see Supplemental Information. Transcranial Magnetic Stimulation screening form). Two cTBS conditions (‘real’ and ‘sham’ TMS) were counterbalanced across normal-hearing participants, and participants were blinded to the type of manipulation they might receive.</p>
<p>cTBS intensity was individually measured for every participant (i.e., both ‘real’ and ‘sham’ TMS conditions) as a function of their corticospinal excitability assessed with single-pulse TMS motor evoked potentials (MEPs). First, single-pulse TMS (Magstim Rapid2 system, Magstim) was delivered over the left and right primary motor cortex to determine the optimal site for MEP elicitation and to determine resting and active motor thresholds for each participant and each side of the brain. A 70 mm figure-of-eight coil was orientated at 45° to the scalp with current flowing posterior-anterior across the primary motor cortex. Coil position and angle was adjusted until the optimal site for consistent elicitation of MEPs was identified. Once the optimal site for stimulation was located, this was marked on the scalp. The resting motor threshold was then determined by the experimenter visually identifying the minimal single-pulse TMS intensity necessary to elicit a MEP from the right and left first dorsal interosseous muscle, while the hand was at rest in 5 out of 10 consecutive stimulations (<xref ref-type="bibr" rid="c116">Rothwell et al., 1999</xref>; <xref ref-type="bibr" rid="c122">Sandrini et al., 2011</xref>).</p>
<p>Participants’ resting left and right motor thresholds ranged from 46-57% of the maximum stimulator output (mean = 51 ± 5). Participants’ individual motor threshold for left and right hemisphere were recorded to set the intensity for bilateral cTBS stimulation which was administered by placing the same figure-of-eight coil over the right and left dlFPC located over electrode positions F3 and F4 (10-20 system EEG) (<xref ref-type="bibr" rid="c69">Herwig et al., 2003</xref>; <xref ref-type="bibr" rid="c80">Jurcak et al., 2005</xref>). Bilateral TMS was performed serially i.e., first the right or the left dlFPC (order was alternated among subjects), was chosen to limit possible compensation effects of the non-stimulated hemisphere (<xref ref-type="bibr" rid="c4">Ambrus et al., 2020</xref>). Each cTBS burst consisted of three pulses at 50 Hz, with bursts repeated at a frequency of 5 Hz, applied continuously for 40s, and delivered at an intensity of 80% of the right or left resting motor threshold (<xref ref-type="bibr" rid="c74">Huang et al., 2005</xref>).</p>
</sec>
<sec id="s2f">
<title>Procedure</title>
<p>All participants completed first a familiarization task that consisted of 10 trials (i.e., different ‘carrier phrase lengths’) presented in anechoic conditions at 0 dB SNR where listeners were asked to report |Color| and |Number|. Participants exposed to ‘real’ or ‘sham’ TMS completed the familiarization task after cTBS procedures. All participants performed the familiarization phase with 80-100% accuracy, confirming that all participants including exposed to cTBS understood the task and procedural learning was achieved and not affected by either ‘sham’ or ‘real’ TMS exposure. The CRM sentences were spoken by six talkers (3 males and 3 females) and were of varying duration where all, some, or none of the preceding sentence (the ‘carrier phrase’ (CP, <xref rid="fig1" ref-type="fig">Figure 1B</xref>)) before ‘|Color| |Number|’ was included (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>). Thus, for CP0, listeners heard ‘|Color| |Number| now’, CP1—‘go to |Color| |Number| now’, CP2—‘|Callsign| go to |Color| |Number| now’, CP3—‘Ready |Callsign| go to |Color| |Number| now’ (<xref rid="fig1" ref-type="fig">Figure 1B and C</xref>). After each phrase was presented, participants reported the |Color| and |Number| they heard to the experimenter and performance was assessed based on keywords correctly identified. When collecting performance data in reverberant environments at −15 dB SNR (SNR that allows a level of performance similar to (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c27">2011</xref>, <xref ref-type="bibr" rid="c28">2013</xref>; <xref ref-type="bibr" rid="c149">Zahorik, 2009</xref>)), the room environment was selected randomly for each trial except that the same room could not appear in two consecutive trials. The target |Color|, |Number| and talker were selected randomly for each trial. For each experimental session, three different listening environments were assessed, and listeners were presented 360 trials (30 repeats of each ‘room’ x ‘carrier phrase length’ condition) with a total test time of 45 min per session. Each listener participated in one session only. Throughout the session, listeners were required to verbally repeat the appropriate |Color| and |Number| combination they heard from corpus lists located to the sides of the 0° azimuth loudspeaker. The CRM is a close-set corpus, therefore participants were provided with |Color| and |Number| choices to select from (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c27">2011</xref>, <xref ref-type="bibr" rid="c28">2013</xref>; Pavel <xref ref-type="bibr" rid="c149">Zahorik, 2009</xref>). The experimenter continuously monitored participants’ responses while scoring performance, but no feedback was provided.</p>
</sec>
<sec id="s2g">
<title>Speech Performance Analysis and Time course-fittings of mean cumulative hit rates</title>
<p>d’ was first calculated individually for both |Color| and |Number| in different room/carrier combinations using <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>, where <italic>z(H)</italic> and <italic>z(F)</italic> were the <italic>z</italic> transforms of the hit rate and false alarm, respectively. Hit rates consisted of the correct |Color| or |Number| being selected, whereas false alarms were considered when the same |Color| or |Number| was reported as an incorrect response to the presentation of any other |Color||Number| combination. To prevent infinite values of d’, hit rates/false alarms of 1 were set to 0.99, and corresponding false alarms/hit rates of 0 were set to 0.01 leading to maximal/minimal d’ values of ± 4.65. The total d’ for a carrier phrase length in a particular room was calculated by averaging |Color| and |Number| d’ values for that combination of room and ‘carrier phrase length’ across individuals.
<disp-formula id="eqn1">
<graphic xlink:href="644835v1_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Given how <italic>d’</italic> was calculated for |Color| and |Number|, it was not considered a useful metric to describe performance across time in different environments: the paucity of data for each |Color| and |Number| affecting the temporal resolution of any generated curve. Therefore, to understand how the average performance developed in each listening environment as a function of time, we averaged cumulative hit rate across all subjects, after a 5-point moving average was implemented for each individual trace (equivalent to ∼7 seconds) and plotted this as a function of the mean exposure time accumulated in each environment in question. Time courses of the developing performance were quantified by fitting a double exponential function to the curves, whose model is described as follows:
<disp-formula id="eqn2">
<graphic xlink:href="644835v1_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Where <italic>f(t)</italic> is the fit as a function of time; <italic>mag<sub>fast</sub></italic>/<italic>mag<sub>slow</sub></italic> are the magnitudes and τ<italic><sub>fast</sub></italic>/τ<sub>slow</sub> are the time constants for the two exponential fits. <italic>C</italic> is a constant term that describes their offset on the y-axis.</p>
<p>Constrained optimization of the fits was achieved using the fmincon function in Matlab to find local minima in mean-squared errors. Fits were reinitialized using a combination of 50 values linearly spaced between 50 s and 500 s for τ<italic><sub>slow</sub></italic> and between 0 s and 100 s for τ<italic><sub>fast</sub></italic>; upper bounds of 2000 s and 200 s were set for τ<italic><sub>slow</sub></italic> and τ<italic><sub>fast</sub></italic> respectively. The optimal fit for a listening environment was selected from the resulting 2450 fits by identifying the fit with the smallest objective function value, fval. During optimization, fits were weighted by their variance; therefore, greater importance was attributed to performance at later exposure times. Adjusted R<sup>2</sup> values are quoted for each fit in <xref rid="figs1" ref-type="fig">Supplemental Figure 1</xref>, <xref rid="figs4" ref-type="fig">4</xref> and <xref rid="figs5" ref-type="fig">5</xref>. For analysis of ‘global learning’, we calculated the absolute percentage deviation of cumulative hit rate curves relative to the final time point in each curve i.e., their Final Hit Rate (FHR), we then calculated back in time to determine where fitted curves to the data first deviated from FHR by a threshold of 10%.</p>
</sec>
<sec id="s2h">
<title>Statistical analysis</title>
<p>Repeated measures ANOVAs (rANOVA) were performed to assess if speech understanding (d’) was affected by the factors: rooms (levels: Lecture Room, Open-Plan Office and Carpark) and carrier phrase length (levels: CP0, CP1, CP2 and CP3), in a within-subjects factor analysis. Univariate ANOVA was employed to assess whether speech understanding was affected by the factors: TMS exposure (levels Sham and real TMS), rooms (levels: Lecture Room, Open-Plan Office and Carpark) and carrier phrase length (levels: CP0, CP1, CP2 and CP3) in a between-subjects factor analysis.</p>
<p>Univariate ANOVA was also used to assess whether speech understanding was affected by room-context (i.e., the 3<sup>rd</sup> room environment in which Open-Plan Office and Underground Car Park are learnt). Only one factor was assessed in this between-subjects room context (levels: Anechoic room, Living room, Lecture room and Highly Reflectant room). Effect sizes were calculated for all statistical analysis (Partial Eta-squared (ŋp2) or Cohen’s d when appropriate). Further two-tailed t-tests (alpha=0.05, with Bonferroni corrections for multiple comparisons) were also performed. The statistical analysis was performed in SPSS: RRID:SCR_002865 (IBM Corp. Released 2023. IBM SPSS Statistics for Windows, Version 29.0.2.0 Armonk, NY).</p>
</sec>
<sec id="s2i">
<title>Power Analysis</title>
<p>Initial sample size estimation (≥ 18) was computed using G*Power: <italic>RRID:SCR_013726</italic> (<xref ref-type="bibr" rid="c53">Faul et al., 2007</xref>) (Effect size f = 0.25; α err prob = 0.05; Power (1-β err prob) = 0.95). However, given the large effect size observed in previous studies (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>; <xref ref-type="bibr" rid="c131">Srinivasan &amp; Zahorik, 2012</xref>; Pavel <xref ref-type="bibr" rid="c150">Zahorik &amp; Brandewie, 2016</xref>) using sample sizes = 9-16 listeners, we expected relatively large effect sizes with a sample ≥ 10. All variables variance were tested for normal distribution (Shapiro-Wilk test, (<xref ref-type="bibr" rid="c126">Shapiro &amp; Wilk, 1965</xref>).</p>
</sec>
<sec id="s2j" sec-type="data-availability">
<title>Data availability statement</title>
<p>All data and related metadata were deposited in Figshare, an appropriate public repository (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25949/24295342.v1">https://doi.org/10.25949/24295342.v1</ext-link>)</p>
</sec>
<sec id="s2k">
<title>Code Information</title>
<p>All custom code used for data collection and analysis will be available upon request.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>Human listeners can incorporate knowledge of a listening environment, specifically its reverberant characteristics, to improve speech understanding in noise (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>; <xref ref-type="bibr" rid="c131">Srinivasan &amp; Zahorik, 2012</xref>; <xref ref-type="bibr" rid="c150">Zahorik &amp; Brandewie, 2016</xref>). To understand how this learning of sound environments is achieved, we recreated acoustic characteristics of real rooms using an array of loudspeakers located in an anechoic chamber (<xref rid="fig1" ref-type="fig">Figure 1A</xref>) and assessed listeners’ performance in a speech-in-noise task using sentences from the Coordinate Response Measure (CRM) corpus— “Ready |Callsign| go to |Color| |Number| now” (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). The CRM corpus is commonly used to quantify listening performance in noisy or cluttered environments (<xref ref-type="bibr" rid="c22">Bolia et al., 2000</xref>). Listeners reported the |Color| (one of four monosyllabic choices, ‘red’, ‘white’, ‘blue’, or ‘green’) and the |Number| (the English digits between ‘one’ and ‘eight’) they heard for sentences of varying duration where all, some, or none of the preceding sentence (the ‘carrier phrase length’) before ‘|Color| |Number|’ was included (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). Importantly, words in the CRM phrase preceding ‘|Color| |Number|’ are uninformative as to the color and number (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>). CRM sentences—complete or partial (see Methods and <xref rid="fig1" ref-type="fig">Figure 1B</xref>)—were presented as if originating from in front of a participant, whilst a randomly generated white noise was presented from a source 90° to their left. Listeners’ abilities to recognize |Color| and |Number| from whole or partial CRM sentences were assessed. Although sentences varied in duration from trial to trial, they always contained the element ‘|Color| |Number| now’ embedded in noise were convolved with the impulse response of the real rooms (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). Sentence length varied by adjusting the duration of the preceding ‘carrier phrase’ (CP; <xref rid="fig1" ref-type="fig">Figure 1B-C</xref>), which was always constructed from the same CRM sentence embedded in noise and convolved with the same impulse response as the remainder of the phrase. Thus, for CP0, listeners heard only ‘|Color| |Number| now’, CP1—‘go to |Color| |Number| now’, CP2—‘|Callsign| go to |Color| |Number| now’, CP3—‘Ready |Callsign| go to |Color| |Number| now’. After each phrase was presented, participants verbally reported the |Color| and |Number| they heard to the experimenter and performance was assessed based on keywords correctly identified (<xref rid="fig1" ref-type="fig">Figure 1D</xref>).</p>
<p>We first confirmed that understanding speech in background noise depends on the reverberant characteristics of the listening spaces from which impulse responses were obtained. Overall, RT<sub>60iso</sub>s [RT<sub>60</sub>s according to ISO-3382 guidelines (ISO, 2009)] of the (six) environments varied from fully anechoic to a highly reverberant underground car park with RT<sub>60iso</sub> = 2.42 s. Our initial assessment (<xref rid="fig1" ref-type="fig">Figure 1D</xref>) examined listening performance in three environments: a Lecture Room (RT<sub>60iso</sub> = 0.45 s), an Open-Plan Office (RT<sub>60iso</sub> = 0.96 s), and an Underground Car Park (RT<sub>60iso</sub> = 2.42 s). Listeners reported |Color| and |Number| from CRM phrases of varying length, spoken by one of 6 talkers (3 female, 3 male) in one of the three environments. ‘Carrier phrase length’ and talker were interleaved in a pseudorandom order to avoid the same environment being presented in consecutive trials (e.g., <xref rid="fig1" ref-type="fig">Figure 1C</xref>). Overall, listeners performed better (quantified in terms of d’ values for reporting the correct |Color| and |Number|) in the Lecture Room—the least reverberant of the 3 environments (<xref rid="fig2" ref-type="fig">Figure 2A</xref>)—with a significant main effect of room [rANOVA: F (2,42) = 76.75, p&lt;0.001, ŋp2 = 0.79]. Bonferroni-corrected <italic>post-hoc</italic> pairwise comparisons demonstrated that performance in the Lecture Room was significantly better than in the Open-Plan Office [mean difference = 0.41, p &lt; 0.001] and Underground Car Park [mean difference = 1.05, p &lt; 0.001]. Performance was also significantly better in the Open-Plan Office when compared to the Underground Car Park [mean difference = 0.65, p &lt; 0.001].</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Learning effects in three real rooms.</title>
<p><bold>A.</bold> Overal<bold>l</bold> Performance (d’) (including correct and incorrect responses to all |<italic>Colors|</italic> and |<italic>Numbers|</italic>) in the Lecture Room (LR), Open-Plan Office (OPO) and Car Park (CP). Mean d’ denoted as circles in the boxplot (n=22 for all rooms). The horizontal line denotes the median. Upper and lower limits of the boxplot represent <sup>1s</sup>t (q1) and <sup>3r</sup>d (q3) quartiles respectively, while whiskers denote the interquartile range (IQR =q3-q1). <bold>B.</bold> Time course of performance in each room for 22 listeners. Correct responses (Hit Rate) across time spent in each room are shown, solid curves in color correspond to mean data after 5-time point moving averages, equivalent to ∼7s, were applied). Optimal time course-fittings (using a two-phase association model) are plotted as color markers in each room. Associated shaded areas representing standard errors of the mean. Dashed coloured lines represent the time point at which participants reached a stable performance in each environment [i.e., ± 10% of the Final Hit Rate (FHR)], here ‘global learning’. Notice that no statistical differences were found for ‘global learning’ among the different rooms. <bold>C.</bold> Shows a correlation analysis between FHR and the time where 10% of the FHR is achieved in each environment. Significant negative correlations (Pearson) were observed for Lecture Room, and Open Plan Office, suggesting the earlier participants reach a stable performance, the higher their FHR is. <bold>D.</bold> Performance for each carrier phrase length, d’ was significantly better as the ‘carrier phrases length’ increased except for CP2 vs. CP3 (n.s), where a roll over effect was observed. <bold>E.</bold> Performance to |Color| and |Number| for CP0 i.e., our proxy for short-term adaptation the carrier phrase where exposure to an environment remained minimal. Here all the environments have been collapsed i.e., rANOVA, main effect of “target word”. <bold>F-G.</bold> Hit Rate for Initial (1-2) and Steady (9-10) trials for keword |Color| in the CP0 condition, (F) in Reverberant conditions, indicating a significant improvement in performance for Steady trials, likely due to the accumulation of acoustic/environmental knowledge i.e., meta-adaptation (G) in anechoic (no echoes) showing a lack of improvement in performance i.e., lack of meta-adaptation in the absence of reverberation. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25949/24295342.v1">https://doi.org/10.25949/24295342.v1</ext-link></p></caption>
<graphic xlink:href="644835v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<sec id="s3a">
<title>Statistical learning of reverberant environments occurs over long and short-time courses</title>
<p>Statistical learning likely occurs over different time courses subject to a range of possible brain mechanisms controlling or modulating the different cadences over which learning emerges (<xref ref-type="bibr" rid="c115">Robinson et al., 2016</xref>; <xref ref-type="bibr" rid="c129">Simpson et al., 2014</xref>). We sought to distinguish short-term from longer-term learning by assessing benefit to word recall of prior exposure to the listening environment over multiple time courses. The design of our paradigm—with talker, length of carrier phrase, target words |Color| &amp; |Number| and reverberation time (listening environment) constituting random variables—means that the initial phase of learning may also be highly variable; listeners, idiosyncratically, likely experience very different parameters over the first few trials, making it difficult to assess the rate at which learning accumulates over these early epochs, and we indeed found this to be the case (see <xref rid="figs1" ref-type="fig">Supplemental Figure 1</xref>). Given this, we assessed the rate at which listeners accumulate knowledge to improve listening performance across the task by assessing ‘global likelihood learning’—defined here as the time at which participants achieved stable performance in each environment and quantified as the time point (backwards in time) at which performance reached ± 10% of the Final Hit Rate (FHR)]. For the 22 listeners, this measure of global learning (<xref rid="fig2" ref-type="fig">Figure 2B</xref>) was similar across all rooms (a total of 180 trials per room and average trial length of 1.5s): Lecture Room: [42.95 ± 38.35 s]; Open-Plan Office: [54.50 ± 29.50 s] and Car Park: [56.95 ± 33.15 s], with a one-way ANOVA revealing no statistical differences in global learning for the three reverberant environments, suggesting listeners learned these environments at the same rate.</p>
<p>We next wondered whether the FHR achieved by a participant within an environment was related to the time point at which global learning had manifest. To this end, a Pearson’s correlation analysis revealed FHR and the timepoint at which stable performance was achieved were negatively correlated for the Lecture Room: r<sub>(22)</sub> = −0.63, p = 0.002, 95% CI [−0.84, −0.32] and Open Plan Office r<sub>(22)</sub> =-0.49, p = 0.02, 95% CI [−0.74, −0.11], but not for the more-highly reverberant Car Park r<sub>(22)</sub> = 0.02, p = 0.94, 95% CI [−0.29, 0.39], <xref rid="fig2" ref-type="fig">Figure 2C</xref>. This analysis suggests that the earlier in time a listener achieves asymptotic performance (i.e., the time point defined as global learning)—presumably by accumulating knowledge about these environments over time—the higher their overall performance (defined by FHR) in the task, at least in the two less-reverberant environments assessed here.</p>
<p>We next analysed whether speech understanding—quantified in terms of d’— improves as the length of the ‘carrier phrase’ was increased (<xref rid="fig2" ref-type="fig">Figure 2D</xref>). Consistent with <xref ref-type="bibr" rid="c26">Brandewie and Zahorik (2010</xref>; <xref ref-type="bibr" rid="c28">2013</xref>), we found a significant main effect of length of ‘carrier phrase’: [rANOVA: F (3,63) =26.59, p&lt;0.001, ŋp2 =0.56]. Bonferroni-corrected <italic>post-hoc</italic> pairwise comparisons revealed a significant effect for most carrier phrase lengths (<xref rid="tbls1" ref-type="table">Supplemental Table 1</xref>) other than between the two longest, CP2 <italic>vs</italic>. CP3. This plateau in performance between CP2 and CP3 suggests an upper limit in the ability to exploit/accumulate information in reverberant listening environments with increasing sound duration, consistent with Brandewie and Zahorik’s (<xref ref-type="bibr" rid="c28">Brandewie &amp; Zahorik, 2013</xref>) observation of a plateau (and a decline in some reverberant environments) in listening performance with increasing length of carrier phrase, and the existence of an upper limit of listeners to exploit prior exposure to sound environments to benefit listening (<xref ref-type="bibr" rid="c70">Hicks &amp; McDermott, 2024</xref>; <xref ref-type="bibr" rid="c96">McDermott et al., 2013</xref>; <xref ref-type="bibr" rid="c98">McWalter &amp; McDermott, 2018</xref>).</p>
<p>We hypothesized that if prior exposure to the statistics of reverberant rooms arises from an increase in the length of the carrier phrase, then performance to |Number| will always be better than to |Color| for the shortest carrier phrase (CP0: ‘|Color| |Number| now’; <xref rid="fig2" ref-type="fig">Figure 2E</xref>) as |Number| is subject to a longer preceding phrase than |Color|. Assessed in terms of reporting |Color| and |Number| alone, listeners performed significantly better (i.e., showed greater sensitivity) for |Number| compared to |Color|: main effect of target word: [rANOVA F (1,21) =48.79, p&lt;0.001, ŋp2 =0.70], [mean difference = 0.38, p &lt; 0.001] despite there being twice as many (eight compared to four) possibilities. Moreover, a significant interaction ‘carrier phrase length’ x ‘target word’ was observed: [rANOVA F (1,21) =4.33, p=0.008, ŋp2 =0.17]. <italic>Post-hoc</italic> pairwise comparisons (see <xref rid="tbls2" ref-type="table">Supplemental Table 2</xref>) indicate that for all carrier phrases except CP3, performance to |Number| was significantly higher than for |Color|. This result also suggests that a roll-over effect i.e., a lack or limit to the improvement in performance is evident for carrier phrases longer than CP2 across environments (<xref ref-type="bibr" rid="c28">Brandewie &amp; Zahorik, 2013</xref>; <xref ref-type="bibr" rid="c98">McWalter &amp; McDermott, 2018</xref>).</p>
<p>A potential explanation for poorer performance in accurately reporting |Color| relative to |Number| for the shortest carrier phrase (CP0) impact of the utterance of |Color|, namely its immediate appearance at the start of the phrase i.e., the impact of “order/certainty or predictability” in statistical learning (<xref ref-type="bibr" rid="c38">Conway et al., 2010</xref>; <xref ref-type="bibr" rid="c46">Daikoku &amp; Yumoto, 2023</xref>). One way to determine whether |Color| presented in the context of the CP0 is at all subject to statistical learning (and is therefore not wholly reliant on the short-term accumulation of information within a CP0 trial; see <xref rid="fig3" ref-type="fig">Figure 3G</xref>) is to assess whether performance for |Color| for the shortest carrier phrase (CP0) improves from the longer-term accumulation of knowledge i.e., the process of meta-adaptation (<xref ref-type="bibr" rid="c115">Robinson et al., 2016</xref>) in which adaptation to short-term statistics improves with repeated exposure to those statistics. Specifically, we tested the hypothesis that, independent of the environment, performance for |Color| in later <italic>steady</italic> trials (here, trials 9-10) of the shortest carrier phrase, CP0, are better compared to <italic>initial</italic> (1-2) trials (<xref rid="fig2" ref-type="fig">Figure 2F</xref>). If this hypothesis is supported, it suggests performance for |Color| for CP0 benefits from meta-adaptive information conveyed in later trials as knowledge about the global structure of the environment accumulates over time. Consistent with this hypothesis, a Wilcoxon signed rank test revealed significant better performance (n=22, Z= −2.16, p=0.03) for |Color| for <italic>steady</italic> trials (9-10; assumed to reflect a meta-adaptive state; <xref ref-type="bibr" rid="c115">Robinson et al., 2016</xref>) compared to <italic>initial</italic> trials (1-2; i.e., short-term adaptation). Interestingly, in anechoic conditions i.e., in the absence of reverberation, this meta-adaptive process—the expected improvement in performance between <italic>initial</italic> and <italic>steady</italic> trials—was not observed: Wilcoxon signed rank test [n=10, Z= −0.71, p=0.48]. This suggests that performance for |Color| conveyed in the shortest carrier phrase, CP0, improves over time even in the absence of immediate information in the form of any preceding carrier phrase, with knowledge of the statistical/acoustical properties of environments accumulating over the course of the experimental task.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Learning effects in ‘sham’ and ‘TMS’ conditions.</title>
<p><bold>A.</bold> Schematic representation of functional connections between the dorsolateral prefrontal cortex (dlPFC) and primary auditory cortex (A1) under transcranial magnetic stimulation (TMS). <bold>B.</bold> Average exposure time (s) ‘sham TMS’ and ‘real TMS’ conditions. Correct responses across time spent in each room for ‘sham’ and ‘TMS’ conditions. Solid curves in pink and gray correspond to mean data from ‘TMS’ and ‘Sham’ conditions respectively (after 5-time point moving averages, equivalent to ∼7s, were applied), with the associated shaded areas representing standard errors of the mean. Dashed black (sham TMS) and pink (real TMS) lines represent the time point at which participants reached a stable performance in each environment [i.e., ± 10% of the Final Hit Rate (FHR)], here ‘global learning’, no statistical differences were found for ‘global learning’ between TMS conditions. <bold>C.</bold> Shows a correlation analysis between FHR and the time where 10% of the FHR is achieved in under TMS (pink) and Sham (gray) conditions. Significant negative correlations (Pearson) were observed under Sham stimulation, suggesting the earlier participants reach a stable performance, the higher their FHR is. This relationship was disrupted (lack of significant correlation) under TMS conditions. <bold>D.</bold> Overall Performance in TMS and Sham exposed participants i.e., collapsed sensitivity (d’) to |Color| and |Number| for all carrier phrases and environments (significant main effect of TMS conditions: Univariate ANOVA). d’ values plotted in bright pink corresponds to the ‘real TMS’ condition whereas the ‘sham TMS’ accuracy is plotted in gray. Mean d’ is denoted as circles in the boxplot (n=11 for ‘sham TMS’ conditions and n=10 for ‘real TMS’ conditions). <bold>E.</bold> Significant Interaction ‘TMS condition’ x ‘carrier phrase length’. d’ was significantly better as the ‘carrier phrases length’ increased for ‘sham TMS’ compared to ‘real TMS’ except for CPO (n.s), i.e., our proxy for short-term adaptation and minimal exposure to acoustic environments. <bold>F.</bold> Performance to |Color| and |Number| for CP0 i.e., our proxy for short-term adaptation and the carrier phrase where exposure to an environment remained minimal. Here all the environments have been collapsed as only a main effect of “target word”, performance to number always significantly higher than for colour independent of TMS condition. <bold>G.</bold> Schematic for assessing short-term adaptation (performance to |Number| in the Initial Trials) and meta-adaptation (performance to |Color| between Initial and Steady Trials). <bold>H.</bold> Isolated short-term adaptation effects for performance to |Number| in CP0. Hit Rate for Initial (1-2) trials only for |Number| in the CP0 condition for both TMS (pink) and Sham (gray) conditions. Notice that no differences were observed between performance |Number| under Sham or TMS conditions, suggesting no disruption of short-term adaptation.<bold>I.</bold> Hit Rate for Initial (1-2) and Steady (9-10) trials only for |Color| in the CP0 condition for both TMS (pink) and Sham (gray) conditions. Notice that only a significant improvement in performance for from Intitial to Steady trials was observed in the ‘sham TMS’ condition. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25949/24295342.v1">https://doi.org/10.25949/24295342.v1</ext-link></p></caption>
<graphic xlink:href="644835v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3b">
<title>Improvements in performance are explained by exposure to the environment not talker idiosyncrasies</title>
<p>Normal-hearing listeners are reported to understand speech slightly better when listening to male talkers in a mixture of male and female talkers (<xref ref-type="bibr" rid="c88">Larsby et al., 2015</xref>), and quickly adapt to talkers’ idiosyncrasies such as non-native accented speech (<xref ref-type="bibr" rid="c77">Idemaru &amp; Holt, 2011</xref>, <xref ref-type="bibr" rid="c78">2014</xref>; <xref ref-type="bibr" rid="c91">Liu &amp; Holt, 2015</xref>). Here, we wondered if exposure to different talkers (3 female and 3 male) could explain the improvement in speech as the length of the carrier phrase was increased (<xref rid="figs1" ref-type="fig">Supplemental Figure 1</xref>). However, this was not the case. Despite listening performance to some talkers appearing overall better than for others [main effect of talkers: rANOVA: F (5,105) = 27.19, p&lt;0.001, ŋp2 =0.56, i.e., significantly worse performance for Talker 1 (male) and Talker 6 (female), see <xref rid="figs2" ref-type="fig">Supplemental Figure 2A</xref> and <xref rid="tbls3" ref-type="table">Supplemental Table 3</xref>], overall differences in performance due to a specific talker did not explain the benefit to speech understanding as the length of the carrier phrase was increased in each environment (i.e., lack of significant interaction: or ‘talker’ x ‘carrier’ x ‘room’: F (30,630) = 0.73, p=0.85, ŋp2 =0.034]).</p>
<p>To explore further the possibility of any talker-specific effects on performance, we assessed the potential benefit of experiencing the same talker on consecutive trials across an experiment, hypothesizing that, independent of room characteristics or length of carrier phrase, experiencing the same talker on consecutive trials would provide a benefit to listening performance. Across all our participants, a total of 1262, 217, and 40 trials were identified as having two, three, or four consecutive trials in which the same talker appeared (see <xref rid="figs3" ref-type="fig">Supplemental Figure 3</xref>). Despite the possibility that sustained experience of the same talker might lead to improved listening performance, a Wilcoxon signed rank test revealed no significant differences in performance between trial 1 <italic>vs</italic>. 2 consecutive/same talker trials (n=1262, Z= −0.42, p=0.68), trial 1 <italic>vs</italic>. 3 consecutive/same talker trials (n=217, Z= −0.17, p=0.87) or trial 1 vs. 4 consecutive/same talker trials (n=40, Z= −0.159, p=0.11). Our data suggest that listeners do not use talker identity, at least in the task reported here, to benefit their speech-in-noise understanding.</p>
<p>We also explored the extent to which a talker and/or the task itself could be learned by analysing improvements in performance with increasing exposure to carrier phrases and talkers in the absence of reverberation i.e., under anechoic conditions. Although a significant rANOVA was observed: F (3,27) = 9.10, p=0.007, ŋp2 =0.57; post-hoc pairwise comparisons, with Bonferroni corrections, revealed that only a significant improvement in performance was observed only between CP0 and CP2 [mean difference = 15.67, p = 0.02; see <xref rid="tbls4" ref-type="table">Supplemental Table 4</xref>], suggesting that in anechoic conditions very little improvement in performance is achieved by learning the talker/task as length of carrier phrase increases. These data support us employing a listening task with high ecological relevance—comprehending speech—to demonstrate the potential benefits of learning background acoustic features to leverage listening performance without requiring listeners to attend to or report features related to the statistics of presumed background sounds <italic>per se</italic> (e.g., <xref ref-type="bibr" rid="c1">Agus et al., 2014</xref>, <xref ref-type="bibr" rid="c98">McWalter &amp; McDermott 2018</xref>, <xref ref-type="bibr" rid="c18">Bianco et al., 2020</xref>).</p>
</sec>
<sec id="s3c">
<title>TMS disrupts long-but not short-term adaptation to an environment’s reverberation profile</title>
<p>The capacity for learning the statistical structure of acoustic environments, the better to understand speech embedded in background noise, suggests a real-world benefit to this form of learned listening. To determine possible brain mechanisms underlying this ability, we reversibly impaired dorsolateral prefrontal cortex (dlPFC, <xref rid="fig3" ref-type="fig">Figure 3A</xref>) — a brain region implicated in listening performance in noise (<xref ref-type="bibr" rid="c73">Houtgast &amp; Steeneken, 1973</xref>; <xref ref-type="bibr" rid="c86">Knudsen, 1929</xref>; <xref ref-type="bibr" rid="c92">Lochner &amp; Burger, 1961</xref>)—using repetitive transcranial magnetic stimulation (rTMS) and then assessed the ability of listeners to recall |Color| and |Number| in our modified CRM sentences. Repetitive TMS is posited to elicit a period of depressed cortical excitability in the targeted area that persists for about an hour post-stimulation (<xref ref-type="bibr" rid="c57">Gamboa et al., 2010</xref>; <xref ref-type="bibr" rid="c71">Hoogendam et al., 2010</xref>; <xref ref-type="bibr" rid="c74">Huang et al., 2005</xref>). Specifically, 10 naïve, normal-hearing listeners were subjected to ‘real’ TMS— continuous theta burst on dlPFC for 40s on each side—and then transferred to the anechoic chamber, where they were presented sequences of CRM phrases of varying phrase length in background noise convolved with one of the three listening environments (Lecture Room, Open-Plan Office, and Underground Car Park) as before. A second, control, group of 11 naïve participants underwent ‘sham’ TMS stimulation in which an otherwise-identical TMS procedure was performed with a ‘sham’ TMS coil. All participants were naïve to differences in the TMS procedure as well as to the listening task; indeed, as with the experimental group, participants in the ‘sham’ group experienced a short period of single-pulse TMS stimulation to obtain motor thresholds, followed by the ‘sham’ TMS with the stimulator positioned over dlPFC. This process familiarised all participants with the procedure and the influence of TMS in generating involuntary finger movements through direct stimulation of motor cortex. We assume that participants in the ‘sham’ group believed later assessment of speech recall occurred under the influence of ‘real’ TMS.</p>
<p>Given the reduction in sample size, as well as the potential placebo effects of a ‘sham’ TMS stimulation, we first confirmed that our sample of 11 ‘sham’ TMS participants exhibited similar behavioural performance to the larger sample of 22 participants who had not been exposed to any TMS manipulation. To avoid sample size imbalances in these comparisons, a random sample of 11 subjects was selected from the 22 participants. A Univariate analysis confirmed that overall performance in these two populations was comparable, with no significant main effect of conditions (‘sham’ <italic>vs</italic>. no exposure to TMS) observed: [F (1,263) =1.10, p=0.29, ŋp2 = 0.005]. As expected, significant main effect of rooms [F (2,131) = 83.84, p&lt;0.001, ŋp2 = 0.41] and carrier phrase length [F (3, 43) =20.52, p&lt; 0.001, ŋp2 = 0.20] were observed, confirming that participants experiencing ‘sham’ TMS did not perform significantly differently from the ‘no exposure to TMS’ population.</p>
<p>We first assessed learning over the time-course of the task itself—‘global learning’— as the mean cumulative hit rates for listeners reporting |Color| and |Number|, fitted for each TMS condition and environment with a two-phase association model (<xref rid="fig3" ref-type="fig">Figure 3B</xref> and Supplemental Figure 4 and 5). Our metric for ‘global learning’ was achieved for Lecture room at: ‘sham’ TMS [40.40 ± 32.57 s] and ‘real’ TMS [63.00 ± 20.82 s]; for Open-Plan Office: ‘sham’ [53.70 ± 30.72 s] and ‘real’ TMS [36.40 ± 37.36 s]; and for Car Park: ‘sham’ [61.40 ± 41.42 s] and ‘real’ TMS [69.80 ± 37.31 s]. A Univariate ANOVA between ‘sham’ TMS and ‘real’ TMS exposed participants revealed no statistical differences in global learning between conditions (mean global learning ‘sham’ TMS= 47.93 ± 33.34s) and mean global learning ‘real’ TMS= 56.40 ± 34.85s) suggesting that participants achieved a stable level of performance (± 10% of FHR) at a similar time point in the task under both ‘real’ and ‘sham’ TMS. However, Pearson’s correlation analysis (collapsed across all environments) revealed a negative correlation between FHR and ‘global learning’ for ‘sham’ TMS listeners (see <xref rid="fig3" ref-type="fig">Figure 3C</xref>): [r<sub>(11)</sub> =-0.45, p = 0.01, 95% CI [−0.70, −0.17] but not for ‘real’ TMS listeners: [r<sub>(10)</sub> = 0.08, p = 0.69, 95% CI [−0.22, 0.37]. Specifically, participants undergoing ‘sham’ TMS who achieved stable performance (±10% of the Final Hit Rate) relatively early in the task maintained this level of performance throughout. Relative delay in attaining stable performance was associated with a lower FHR, with the time at which early ‘global learning’ is achieved predicting overall task performance. This relationship was not evident in listeners subject to ‘real’ TMS, where the time to reach a stable performance was not predictive of the magnitude of final performance (FHR). Under TMS manipulation of dlPFC, ‘global learning’ had no predictive value, indicating that task performance was less stable and reliable for participants exposed to TMS.</p>
<p>We next employed a Univariate analysis to compare overall performance (d’)— including FHR and False Alarm Rate, (see <xref rid="fig3" ref-type="fig">Figure 3D</xref>)— of ‘sham’ participants with those who received ‘real’ TMS, and observed a main effect of TMS conditions [Univariate ANOVA [F (1,501) = 26.34, p&lt;0.001, ŋp2 = 0.1], where ‘sham’ participants showed significantly better abilities to recall |Color| and |Number| compared to participants subjected to ‘real’ TMS [<italic>post-hoc</italic> pairwise comparisons with Bonferroni corrections: [mean difference = 0.32, p &lt; 0.001]. In addition, a significant interaction ‘TMS condition’ x ‘carrier phrase length’ was observed, <xref rid="fig3" ref-type="fig">Figure 3E</xref>: [F (3,123) = 2.82, p=0.039, ŋp2 = 0.02]. A <italic>post-hoc</italic> pairwise comparison with Bonferroni corrections showed that performance in the CP0 condition was not statistically different between ‘sham’ and ‘real’ TMS conditions (mean difference = 0.02, p = 0.86), however performance in the recall task (see <xref rid="fig3" ref-type="fig">Figure 3E</xref> and Supplemental Table 5) was better found for ‘sham’ compared to ‘real’ TMS for CP1 (mean difference = 0.34, p = 0.006), CP2 (mean difference = 0.41, p = 0.001), and CP3 (mean difference = 0.50, p &lt; 0.001). Listeners exposed to ‘sham’ TMS retained the improvements of performance as the length of carrier phrase was increased whereas those listeners receiving ‘real’ TMS— in which activity in dlFPC is presumably impaired—appeared unable to accumulate over time knowledge of the sound environment.</p>
<p>The different time courses over which statistical learning might occur suggests the involvement of multiple neural mechanisms and brain circuits (<xref ref-type="bibr" rid="c5">Anderson &amp; Malmierca, 2013</xref>; <xref ref-type="bibr" rid="c6">Antunes &amp; Malmierca, 2011</xref>; <xref ref-type="bibr" rid="c115">Robinson et al., 2016</xref>), with different cadences of learning potentially controlled, or at least modulated, by different brain centres. The time-course of midbrain neurons to adapt to different sound environments, for example, is slowed, and the capacity for retaining a ‘memory’ of those sound environments disappears, when cortex is inactivated through cooling (<xref ref-type="bibr" rid="c115">Robinson et al., 2016</xref>). This suggests feed-forward and feed-back mechanisms, with their own time-courses or time-constants, contribute to overall performance, including the rate at which sound environments are learned. We specifically wondered if ‘real’ TMS had a detrimental effect on short-term adaptation i.e., the observed advantage in performance for |Number| compared to |Color|, for the shortest carrier phrase CP0 (<xref rid="fig2" ref-type="fig">Figure 2E</xref> and <xref rid="fig3" ref-type="fig">3F</xref>). A Univariate analysis revealed a main effect of ‘target word’, where performance for |Number| was always significantly better than for |Color| for all the environments explored and across TMS conditions [F (1,498) = 53.53, p&lt;0.001, ŋp2 = 0.11]. However, no interaction between ‘TMS conditions’ and ‘target word’ was observed, indicating that in both ‘real’ and ‘sham’ TMS conditions, the same trend of better performance to |Number| compared to |Color| was evident, <xref rid="fig3" ref-type="fig">Figure 3E</xref>: [‘sham’ TMS: mean difference = 0.81, p &lt; 0.001 and ‘real’ TMS: mean difference = 0.66, p &lt; 0.001]. However, overall d’ has contributions of short-term and meta-adaptive (long-term accumulation of knowledge about the environments) across the task.</p>
<p>To dissociate specifically short-term adaptation from longer-term meta-adaptation, we compared performance (Hit Rate) to |Number| for <italic>initial</italic> trials (1-2), for CP0 only, as performance for this length of carrier phrase should be influenced only by short-term accumulation of information (i.e., within trial; see <xref rid="fig3" ref-type="fig">Figure 3G</xref>). A non-parametric Wilcoxon signed rank test revealed no significant differences in |Number| performance for <italic>initial</italic> trials between ‘sham’ and ‘real’ TMS exposed listeners (n=10, Z= −0.28, p=0.78; <xref rid="fig3" ref-type="fig">Figure 3H</xref>), suggesting that rapid learning of the statistical structure of the reverberant environment—in the order of a few hundreds of milliseconds—is resistant to the effects of ‘real’ TMS applied to dlPFC. To determine how much meta-adaptative improvement in performance could be disrupted by applying ‘real’ TMS to dlPFC, we then compared <italic>initial</italic> trials (1-2) and <italic>steady</italic> trials (9-10) for CP0—our proxy for meta-adaptation (<xref rid="fig3" ref-type="fig">Figure 3G</xref>). We tested the hypothesis that disruption of dlPFC would affect late (meta-) but not early adaptation. To this end, we expected no differences between performance in <italic>initial</italic> trials between ‘real’ and ‘sham’ TMS (as <italic>initial</italic> trials are influenced only by short-term adaptation), and a relative lack of improvement in performance in later, <italic>steady</italic> trials (meta-adaptation influenced) in ‘real’ compared to ‘sham’ TMS. Consistent with our hypothesis, a Wilcoxon signed rank test revealed no significant differences in performance for <italic>initial</italic> trials between ‘sham’ and ‘real’ TMS exposed listeners (n=10, Z= −0.21, p=0.83), but improved performance for <italic>steady</italic> compared to <italic>initial</italic> trials (n=10, Z=-2.06, p=0.04) for ‘sham’ TMS only (<xref rid="fig3" ref-type="fig">Figure 3I</xref>). These data suggest that performance in |Color| for the shortest carrier phrase, CP0, was disrupted due to impaired accumulation of knowledge about the environments encountered. In turn, performance under ‘TMS’ in <italic>steady</italic> trials (presumed to be influenced by meta-adaptation) was reminiscent of performance observed in initial trials where only immediate knowledge could be used to improve performance. This is reminiscent of adaptation to sound environments reported <italic>in vivo</italic> (<xref ref-type="bibr" rid="c50">Dean et al., 2005</xref>, <xref ref-type="bibr" rid="c51">2008</xref>; <xref ref-type="bibr" rid="c115">Robinson et al., 2016</xref>; <xref ref-type="bibr" rid="c146">Wen et al., 2009</xref>), where interrupting efferent feedback by cortical cooling impairs the capacity of midbrain neurons to learn the statistical structure of sound environments; though neurons adapt to the short-term statistical structure of a sound environment each time they are exposed to it, they fail to demonstrate the acceleration of adaptation and improvement in (neural) discrimination performance as the same environment is re-encountered, adapting to it only as if exposed to it the first time.</p>
</sec>
<sec id="s3d">
<title>Statistical learning of room acoustics is tuned to commonly experienced reverberation times</title>
<p>Reverberation is a common feature of many acoustic environments—natural and built (<xref ref-type="bibr" rid="c135">Traer &amp; McDermott, 2016</xref>). Interestingly, <xref ref-type="bibr" rid="c150">Zahorik and Brandewie (2016)</xref> reported that improvements in speech understanding when a ‘simulated room’ is repeatedly encountered were best when listeners experienced moderately reverberant environments. This suggests that brain mechanisms responsible for listening in noise might be adapted to conditions of reverberation. Francl &amp; McDermott (2022) described an auditory model able to reproduce several ‘human-like’ spatial hearing features when successfully trained under realistic listening conditions, such as in noise and reverberation. To this end, we wondered if the learning effects we observed similarly rely on the combination of real-world environments we employed, i.e., the RT<sub>60</sub>s in which listening performance was assessed. Specifically, we hypothesized that the ability to understand speech in noise depends on exposure to specific values of RT<sub>60</sub> across our experimental paradigm, including those in the range commonly experienced by human listeners in natural and built environments (<xref ref-type="bibr" rid="c135">Traer &amp; McDermott, 2016</xref>). To test this hypothesis, we recruited a new population of naïve participants with no previous exposure to our experimental paradigm and assessed their ability to understand speech in noise using the same CRM corpus as before, but in different combinations of acoustic environments i.e., room contexts defined by their RT<sub>60</sub>s.</p>
<p>We first tested the ability of 10 participants to recall |Color| and |Number| in Open-Plan Office and Underground Car Park as before, but with the Lecture Room (RT<sub>60iso</sub> = 0.42 s) swapped out for a more highly reflective elevator lobby (, <xref rid="fig4" ref-type="fig">Figure 4</xref>) with an RT<sub>60iso</sub> of 1.55 s—i.e., between that of the Open-Plan Office (0.96s) and the Underground Car Park (2.42s). We therefore changed the ‘room-context’ in which Open-Plan Office and Underground Car Park were learned, replacing the Lecture Room with a Highly Reflective Room. When contrasting (Univariate ANOVA) the performance (hit rate) of these 10 naïve participants <italic>vs</italic>. the 11 subjects randomly selected from the initial 22 participants sample exposed to the initial three rooms (i.e., Lecture Room, Open-Plan Office and Underground Car Park), overall performance was significantly better in the context of the Lecture Room compared to the Highly Reflective Room (<xref rid="fig4" ref-type="fig">Figure 4 A-B</xref>), with a main effect of room-context [F(1,251) = 207.49, p&lt;0.001, ŋp2 = 0.46], mean difference = 20.09, p &lt; 0.001] and a significant interaction of room-context x room [F(2, 125) = 26.64, p&lt;0.001, ŋp2 = 0.18]. <italic>Post hoc</italic> comparisons focused only on the overlapping rooms i.e., Underground Carpark and Open-Plan Office as it was expected that longer reverberation times, such as those in the Highly Reflective Room, have detrimental effects in speech understanding compared to shorter reverberation times i.e., Lecture Room (<xref ref-type="bibr" rid="c73">Houtgast &amp; Steeneken, 1973</xref>; <xref ref-type="bibr" rid="c86">Knudsen, 1929</xref>; <xref ref-type="bibr" rid="c92">Lochner &amp; Burger, 1961</xref>) [mean difference = 34.48, p &lt; 0.001]. Performance in the Underground Car Park was significantly better when presented in the context of the Lecture Room than in the context of the Highly Reflective Room [mean difference = 13.30, p &lt; 0.001]. A similar trend of improved performance in the Open-Plan Office was observed in the context of the Lecture Room when compared to the Highly Reflective Room context: [mean difference = 12.50, p &lt; 0.001].</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Performance in different 3-room combinations where acoustics of 3 rooms span an ecological range.</title>
<p><bold>A.</bold> Overall hit rate for different 3-room combinations. Mean data for four different 3-room combinations are plotted where Open-Plan Office (<italic>dark blue</italic>) and Underground Car Park (<italic>yellow</italic>) were always included and the context room was either Anechoic (<italic>black</italic>), Living Room (<italic>red</italic>), Lecture Room (<italic>green</italic>), Lecture Room (<italic>green</italic>) or Highly Reflective (<italic>purple)</italic>. <bold>B.</bold> Performance in Open-Plan Office and Car Park when paired with different context rooms. Final hit rates were highest for Office and Car Park when presented in conjunction with the Lecture Room, followed by the Living Room and Anechoic with poorest performance observed when the Highly Reflective space was the context. <bold>C.</bold> Environments tested compared to ecological range. The RT<sub>60traer</sub>s for the five test rooms, calculated as the median RT<sub>60</sub> value across 31 frequency sub-bands (<xref ref-type="bibr" rid="c135">Traer &amp; McDermott, 2016</xref>), are plotted (<italic>colored boxes</italic>) above a histogram of the median RT<sub>60traer</sub>s calculated for 199 indoor (<italic>filled bars</italic>) and 72 outdoor (<italic>empty bars</italic>) spaces recorded by <xref ref-type="bibr" rid="c135">Traer and McDermott (2016)</xref>. <bold>D</bold>. Frequency dependence of reverberation time (RT<sub>60traer</sub>) in test rooms compared to ecological range. RT<sub>60</sub>s of the 5 test rooms are displayed for frequency sub-bands used to calculate RT<sub>60traer</sub> (<italic>colored lines</italic>). Quartiles for combined indoor and outdoor spaces (<xref ref-type="bibr" rid="c135">Traer &amp; McDermott, 2016</xref>) are plotted as <italic>black dashed lines</italic>. Decreasing reverberation time above 0.5kHz is observed for Living Room, Lecture Room and Car Park as has been typically described for indoor spaces (<xref ref-type="bibr" rid="c135">Traer &amp; McDermott, 2016</xref>); however RT<sub>60traer</sub> profiles for Open-Plan Office and Highly Reflective space were notable for their longer reverberation times at and above 1kHz. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25949/24295342.v1">https://doi.org/10.25949/24295342.v1</ext-link></p></caption>
<graphic xlink:href="644835v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Wondering whether the ‘room-context’ effect was determined by task difficulty i.e., the more-highly reverberant lobby being intrinsically more difficult for listening than the Lecture Room, we recruited a further 11 naïve participants and performed the same task in which they were exposed to another combination of three rooms, here with the Lecture Room swapped for a Living Room with a shorter RT<sub>60iso</sub> (0.33 s) i.e., an expected ‘easier’ room. Surprisingly, although performance was improved overall relative to that in the combination of the three highly reverberant rooms (<xref rid="fig4" ref-type="fig">Figure 4</xref> A-B), it remained marginally poorer [mean difference = 4.04] than when the Lecture Room was included in combination with the two fixed rooms, i.e., the Open-Plan Office and Underground Car Park (main effect of room-context [F (1,263) = 9.32, p=0.002, ŋp2 = 0.04].</p>
<p>Although reverberation might be considered detrimental to speech-in-noise performance, short-term benefits of more, compared to less, room reverberation for speech understanding have, in fact, been reported, including for the task we report here (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>). Anechoic spaces—rooms whose walls are treated to remove completely reflected sound energy—have been extensively reported to aid speech understanding, especially in background noise. Notwithstanding this possibility, however, anechoic listening environments are rare— whether natural or built—and it is possible that listening performance in noise is indeed better when listeners have access to more commonly experienced, or ethologically realistic, levels of reverberation, a possibility suggested by the small, but significant, reduction in performance in all three rooms when the living room replaced the Lecture Room. To test this hypothesis directly, we recruited a further 10 naïve participants and compared their speech-in-noise performance in a combination of an Anechoic Room and the original Open-Plan Office and Underground Car Park, with the performance of the 11 participants in the original three rooms. Although task difficulty might be expected to be at its lowest i.e., easiest, in the less-reverberant Anechoic Room, this was not the case. Performance was poorer when the combination of three rooms contained an Anechoic Room and the other two, more highly reverberant rooms (<xref rid="fig4" ref-type="fig">Figure 4 A-B</xref>), with a main effect of ‘room-context’ [F (1,251) = 34.42, p&lt;0.001, ŋp2 = 0.12], [mean difference = 9.19, p &lt; 0.001].</p>
<p>Seeking to explain why some rooms might be better than others for the improvement of speech understanding over time, we recalculated reverberation times for our 5 reverberant rooms according to the method of <xref ref-type="bibr" rid="c135">Traer and McDermott (2016)</xref> where RT<sub>60traer</sub> equals the median RT<sub>60</sub> measured in 31 sub-bands with centre frequencies between 80 Hz and 10 kHz (see Methods). We then compared these with the values collected for a wide range (n= 271) of built and natural environments by the same authors (<xref rid="fig4" ref-type="fig">Figure 4C-D</xref>. RT<sub>60traer</sub>s for 5 rooms conditions and Traer’s data, plotted as log<sub>10</sub> RT<sub>60traer</sub> for visualisation). In particular, the RT<sub>60traer</sub> of the Lecture Room, at 0.49 s, lies extremely close to the median and mean RT<sub>60traer</sub> values of the built environments [0.42 s and 0.50 s respectively generated by the skewed distribution (n= 199), <xref rid="fig4" ref-type="fig">Figure 4C</xref>]. This suggests that superior performance in understanding speech understanding in noise in the Lecture Room—and the positive impact on speech understanding when the Lecture Room is included in the three-room listening task— is related to its reverberant characteristics being commonly encountered in everyday listening situations. Notably, the mean and median RT<sub>60traer</sub>s of the recorded natural environments were much lower (0.12 s and 0.16 s, respectively [n = 72], <xref rid="fig4" ref-type="fig">Figure 4C</xref>) than the RT<sub>60traer</sub>s of the Lecture Room. This suggests that brain mechanisms contributing to effective speech understanding in reverberant environments might have adapted to the range of environments experienced over some longer time course than we assessed here.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>We assessed the ability of listeners to understand speech embedded in background noise and convolved with the room impulse responses (RIRs) of different indoor environments defined by their reverberation decay time, or RT<sub>60</sub>—the time taken for reverberant sound energy to decay 60 decibels (dB). We found speech understanding improved with repeated exposure to an environment, a form of learning that was impaired following continuous, bilateral theta-burst transcranial magnetic stimulation (TMS) of dorsolateral prefrontal cortex (dlPFC). Specifically, we observed rapid learning i.e., an improvement in speech understanding over the first few seconds of room exposure, likely due to listeners learning the acoustic reverberation—the only variable held constant across trials for a given environment. This learning, on the timescale of several seconds, was impaired by TMS, whilst learning at shorter or longer timescales was not, suggesting that TMS applied to dlPFC specifically disrupted learning of the reverberant characteristics of an environment. Listeners also showed better listening performance in moderately reverberant environments, and this performance was transferable between different acoustic environments. Specifically, the ability to correctly report keywords spoken in environments with the more extreme—lower or higher—RIRs was best when these environments were encountered in experimental blocks also containing the moderately reverberant Lecture Room or Living Room. This transference suggests an ethological tuning to more commonly encountered environments when learning acoustic backgrounds.</p>
<sec id="s4a">
<title>A role for dlPFC in statistical learning of room acoustics</title>
<p>Inactivation of dlPFC using theta-burst TMS reduced overall performance in our speech-in-noise task, assessed in terms of the hit rate for |Color| and |Number| in CRM phrases as a function of duration of exposure to a sound environment. Notably, however, TMS did not impair listening performance for the shortest-duration phrase, ‘|Color| |Number| now’. For this duration of carrier phrase, overall performance, and the tendency to report more accurately |Number| compared to |Color|—despite |Number| having twice as many options as |Color|—was unaffected by TMS stimulation of dlPFC. The robustness of listening performance to the shortest carrier phrase suggests that the ability to hear out speech in background noise and potentially to learn this over time in complex acoustic environments does not rely solely on cortical feedback from dlPFC. The ability to leverage performance on |Number| by exposure to |Color| presented in background noise convolved with the room impulse response is evident even when the function of dlPFC is (presumably) impaired. It is the case, however, that the rate of learning—i.e., how rapidly performance improved with increasing time of exposure to an acoustic environment—was influenced when dlPFC was impaired. Under the influence of prior TMS stimulation, listeners were less able, and less reliable when learning the acoustic background features of an environment to enhance their speech understanding, particularly in the more challenging environments of the Underground Car Park and Open-Plan Office.</p>
<p>Dense connections between dlPFC and the sensory cortices provide the conduit for prediction-based, top-down regulation (<xref ref-type="bibr" rid="c101">Morrone, 2010</xref>). These model-based predictions rely on learned templates of sensory environments (<xref ref-type="bibr" rid="c3">Alexander &amp; Brown, 2018</xref>), which are critical for making inferences under situations of sensory uncertainty such as those encountered in noisy and challenging listening environments (<xref ref-type="bibr" rid="c15">Bartolo &amp; Averbeck, 2021</xref>). Dorsolateral prefrontal cortex can influence auditory processing by means of direct connections to auditory cortex (<xref ref-type="bibr" rid="c65">Hackett et al., 1999</xref>; <xref ref-type="bibr" rid="c111">Plakke &amp; Romanski, 2014</xref>)—see Figure 3Ai—and to the auditory thalamic reticular nucleus, a key modulator of auditory thalamo-cortical loops (<xref ref-type="bibr" rid="c152">Zikopoulos &amp; Barbas, 2006</xref>). In return, dlPFC receives projections from primary and secondary auditory cortex (<xref ref-type="bibr" rid="c12">Barbas &amp; Pandya, 1987</xref>; <xref ref-type="bibr" rid="c13">Barbas &amp; Pandya, 1991</xref>; <xref ref-type="bibr" rid="c63">Goldman-Rakic &amp; Schwartz, 1982</xref>; Pandya &amp; Barnes, 2019; <xref ref-type="bibr" rid="c109">Petrides &amp; Pandya, 2002</xref>), indicating that a reciprocal auditory-to-prefrontal ‘listening loop’ exists to support hierarchical predictions and prediction-error feedback. These ‘listening loops’ could potentially extend to cortical efferent feedback (<xref ref-type="bibr" rid="c20">Blackwell et al., 2020</xref>; <xref ref-type="bibr" rid="c95">McAlpine &amp; de Hoz, 2023</xref>) and modulate the function of the the inner-ear sensitivity in attended speech-in-noise tasks (<xref ref-type="bibr" rid="c49">de Boer &amp; Thornton, 2008</xref>; <xref ref-type="bibr" rid="c59">Garinis et al., 2011</xref>; <xref ref-type="bibr" rid="c61">Giraud et al., 1997</xref>; <xref ref-type="bibr" rid="c100">Mishra &amp; Lutman, 2014</xref>; <xref ref-type="bibr" rid="c67">Hernández-Pérez et al., 2021</xref>).</p>
<p>The specific mechanism by which TMS-induced modulation of dlPFC impairs listening performance remains unknown. Neurostimulation studies targeting dlPFC during learning paradigms have generated conflicting directions of effect i.e., stimulation of dlPFC is reported to impair implicit learning (<xref ref-type="bibr" rid="c104">Nydam et al., 2018</xref>; <xref ref-type="bibr" rid="c106">Pascual-Leone et al., 1996</xref>) or enhance it (<xref ref-type="bibr" rid="c4">Ambrus et al., 2020</xref>). Ambrus and colleagues argued that impairment of dlPFC during implicit/statistical learning allows for increased engagement of a learning mechanism that is ‘model-free’ i.e., does not rely on inherent sensory representations and therefore takes longer to consolidate. Here, we speculate that the short periods over which learning is required in our task are subserved by a pre-frontally mediated, ‘model-based’ learning able to construct predictions ‘on the fly’ (<xref ref-type="bibr" rid="c48">Daw et al., 2005</xref>). We suggest, in our study, that disrupting a rapidly acting model-based form of learning supported by dlPFC, as part of the auditory-prefrontal ‘listening loop’, leads to impaired learning of reverberant environments.</p>
<p>Dorsolateral prefrontal cortex is also implicated in several, high-level cognitive functions including multi-sensory integration (<xref ref-type="bibr" rid="c56">Fuster et al., 2000</xref>), executive functions such as inhibition and working memory (<xref ref-type="bibr" rid="c33">Castro-Meneses et al., 2016</xref>; <xref ref-type="bibr" rid="c36">Coltheart et al., 2018</xref>; <xref ref-type="bibr" rid="c140">Wang et al., 2015</xref>) and listening in noisy environments (<xref ref-type="bibr" rid="c52">Du et al., 2016</xref>). The impact of TMS on the overall performance of our speech-in-noise task may be linked to the involvement of dlPFC in more generalized cognitive functions such as executive, memory, and attention processes. Dissociating the potential influence of TMS on such processes relative to a specific role in speech-in-noise understanding in noisy, reverberant environments would be an ideal next step. Specifically frontal regions have been linked to sensorimotor integration during speech-in-noise perception in young and older human participants (<xref ref-type="bibr" rid="c52">Du et al., 2016</xref>), suggesting that TMS might disrupt global functioning required for the analysis and understanding of speech. Whilst not entirely unrelated, all participants completed a familiarization task preceding the main experiment, which consisted of 10 trials presented in anechoic conditions at 0 dB Signal-to-Noise Ratio (SNR). Participants exposed to either ‘real’ or ‘sham’ TMS underwent this task following cTBS procedures. Notably, all participants, including those subjected to cTBS, achieved 75-100% accuracy during the familiarization phase, with no statistical differences between TMS groups [mean difference=1.25, t<sub>(1, 9)</sub>=0.43, p=0.68, d=0.14], confirming their comprehension of the task and successful procedural learning (i.e. learning the task <italic>per se</italic>). In addition, speech understanding in the most challenging condition (CP0) was not disrupted following TMS manipulations, suggesting that the effect of TMS was specifically in terms of influencing the way these environments were learnt over time. Nevertheless, we acknowledge that the primary task may have imposed greater cognitive demands, potentially requiring a more significant involvement of the dlPFC to meet such task requirements.</p>
</sec>
<sec id="s4b">
<title>What is being learned in statistical learning of acoustic features?</title>
<p>The need to communicate in reverberant environments is common to daily life. It is well known that long reverberation times have detrimental effects on speech quality and intelligibility (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>; <xref ref-type="bibr" rid="c131">Srinivasan &amp; Zahorik, 2012</xref>; <xref ref-type="bibr" rid="c150">Zahorik &amp; Brandewie, 2016</xref>) and that, specifically, reverberation blurs phoneme boundaries, increasing the extent to which similar words might be confused, e.g., ‘sir’/’stir’ (<xref ref-type="bibr" rid="c142">Watkins, 2005b</xref>; <xref ref-type="bibr" rid="c143">Watkins &amp; Makin, 2007</xref>). However, with sufficient exposure to longer reverberation times in the form of a carrier phrase, similar levels of word identification to those achieved in minimal reverberation are observed (<xref ref-type="bibr" rid="c142">Watkins, 2005b</xref>; <xref ref-type="bibr" rid="c143">Watkins &amp; Makin, 2007</xref>). Based on the premise that providing sufficient contextual information enables the auditory system to adapt and compensate for the detrimental effects of reverberation, Zahorik and colleagues (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>; <xref ref-type="bibr" rid="c131">Srinivasan &amp; Zahorik, 2012</xref>; <xref ref-type="bibr" rid="c150">Zahorik &amp; Brandewie, 2016</xref>) and, here, ourselves, demonstrate that prior exposure to the reverberant characteristics of a room can enhance sentences understanding when that environment is re-encountered. Moreover, Zahorik and colleagues, and our own data (see <xref rid="fig4" ref-type="fig">Figure 4A</xref>), demonstrate that improvements in speech understanding are always greater in reverberant compared to anechoic, environments (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>; <xref ref-type="bibr" rid="c131">Srinivasan &amp; Zahorik, 2012</xref>), i.e., improved performance arises only through exposure to a reverberant environment rather than to the speech material <italic>per se</italic>.</p>
<p>Adaptation to continuous noise is a well-known phenomenon in the auditory system (<xref ref-type="bibr" rid="c43">Costalupes et al., 1984</xref>; <xref ref-type="bibr" rid="c60">Gibson et al., 1985</xref>; <xref ref-type="bibr" rid="c110">Phillips, 1985</xref>; <xref ref-type="bibr" rid="c114">Rees &amp; Palmer, 1988</xref>) that could potentially also explain the speech improvements observed among carrier lengths. This is particularly relevant in our experiments because the masking noise preceded the onset of the speech material, potentially providing a window for noise-adaptation to occur (<xref ref-type="bibr" rid="c2">Ainsworth &amp; Meyer, 1994</xref>; <xref ref-type="bibr" rid="c16">Ben-David et al., 2016</xref>). However, Zahorik and Brandewie reported similar effect sizes in speech enhancements when 1s (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>) or 150 ms (<xref ref-type="bibr" rid="c28">Brandewie &amp; Zahorik, 2013</xref>) of white noise was presented prior to the carrier phrase (i.e., CP0—|Color| |Number|). From this the authors concluded that noise alone did not convey sufficient information about the acoustic environment to elicit improvements in speech understanding.</p>
<p>Adult listeners can also acclimatise to speech acoustics that deviate from the norm or from long-term language regularities such as dialects and foreign accents, with sufficient exposure (<xref ref-type="bibr" rid="c77">Idemaru &amp; Holt, 2011</xref>, <xref ref-type="bibr" rid="c78">2014</xref>; <xref ref-type="bibr" rid="c91">Liu &amp; Holt, 2015</xref>). Even if our participants were all Australian-English native speakers exposed to an American-English speech corpus—the CRM—some perceptual learning of individual talker’s idiosyncratic speech patterns might arise over the course of the task (<xref ref-type="bibr" rid="c35">Choi &amp; Perrachione, 2019</xref>; <xref ref-type="bibr" rid="c91">Liu &amp; Holt, 2015</xref>; <xref ref-type="bibr" rid="c132">Stilp, 2020</xref>). In addition, it has been reported that ‘target voice continuity’ i.e., the same talker within trials—similar to our design—can enhance the build-up of selective attention and improve listeners abilities to report correctly digits sequences in the presence of other talkers (<xref ref-type="bibr" rid="c17">Best et al., 2008</xref>). However, when the room build-up effect—the effect assessed here—is explored in an isolated anechoic space i.e., not intermingled with room acoustic of other reverberant spaces, there is little or no improvement in speech understanding with increasing exposure to the talker <italic>per se</italic>; i.e., in the form of an increasing length of carrier phrase (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>). Interrupting the continuity of the room—it’s reverberation profile—rather than the continuity of the target talker, is the factor reported to disrupt significantly the improvement in speech understanding with increasing exposure (<xref ref-type="bibr" rid="c25">Brandewie &amp; Zahorik, 2018</xref>). If listeners adapted to a talker in the course of our experiment, this occurred rapidly, within a few speech tokens (<xref ref-type="bibr" rid="c42">Cooke et al., 2022</xref>; <xref ref-type="bibr" rid="c81">Kakehi, 1992</xref>; <xref ref-type="bibr" rid="c82">Kato &amp; Kakehi, 1988</xref>) and contributed little to the enhancement of speech understanding we observed in reverberant rooms.</p>
<p>A key feature of our study, one that distinguishes it from previous assessments of statistical learning of acoustic features in human listeners, is our use of an ethologically valid listening task—understanding speech in background noise. Given the perceptual biases inherent in sensory processing, it is notable that investigators have previously reported different noise tokens to be differently learnable (<xref ref-type="bibr" rid="c1">Agus et al., 2014</xref>; <xref ref-type="bibr" rid="c45">Daikhin et al., 2017</xref>), for example, and that textures must be carefully controlled to ensure listeners are not exploiting subtle spectro-temporal features in their judgments of statistical similarity (<xref ref-type="bibr" rid="c96">McDermott et al., 2013</xref>). The potential for listeners to hear out specific—potentially unique to them—spectro-temporal features of otherwise statistically identical sound tokens, or to make judgments on the regularity or similarity of tone sequences based on unique listening experiences (<xref ref-type="bibr" rid="c11">Barascud et al., 2016</xref>; <xref ref-type="bibr" rid="c18">Bianco et al., 2020</xref>), presents a potential confound for these studies. Our study countermands this problem by actively exploiting the propensity for human listeners to ascribe meaning to spectro-temporal fluctuations (here, speech) in acoustic waveforms (<xref ref-type="bibr" rid="c26">Brandewie &amp; Zahorik, 2010</xref>, <xref ref-type="bibr" rid="c28">2013</xref>). Though still targeting the learning of background acoustic features, it engages listeners in a highly relevant listening task, one for which humans have rapidly evolved—i.e., understanding speech in background noise. Listeners’ attention, therefore, was called away from the background acoustic features to the ethologically relevant foreground task of attending to human speech, void of semantics; additionally, length of phrase, talker, and |Callsign| were uninformative to |Color| and |Number| in our utterances, as were |Color| and |Number| to each other. This misdirection allows us to exploit speech as a ‘reporter’ or biomarker for statistical learning of background, more-abstract, acoustic features, and without the potential confounding factor of (individualised) perceptual bias. Further, we have demonstrated that prior exposure to ethologically relevant environments matters when learning less-commonly encountered acoustic scenes.</p>
<p>The impact on speech understanding of less-ecologically realistic reverberant environments is reminiscent of the detrimental effect of TMS, with lower overall listening performance. This suggests that learning the statistical structure of sound environments, the better to understand speech in reverberant background noise, requires some longer form of memory (experience, developmental, or evolutionary). Brain circuits, therefore, might act to improve listening performance, operating best when at least some of the exposure to listening environments includes plausible, and commonly experienced, reverberation times (<xref ref-type="bibr" rid="c135">Traer &amp; McDermott, 2016</xref>). When the acoustics of commonly encountered rooms were replaced with those of other environments having higher or lower amounts of reverberation, performance declined over the course of an experimental session. Intriguingly, the most effective reverberation time we employed, at least in terms of its capacity to be learned and potentially from which learning could be transferred—the RT<sub>60traer</sub> of 0.49 s of the small Lecture Room is very close to the median and mean RT<sub>60traer</sub> of 0.42 and 0.50 s previously recorded for a wide range of built environments (indoors and outdoors). This suggests that statistical learning of acoustic background features might be tuned to ethologically relevant environments. Alternatively, built environments might be constructed to generate reverberation subjectively most suited to maximising speech-in-noise understanding, though we are unaware as to whether such a constraint is consciously applied in the design of listening spaces given contingencies such as the range and relative consistency and constancy of potential contents of any given space that influence its reverberant characteristics. Compared to built environments, however, the RT<sub>60traer</sub>s of natural environments were much lower than all the rooms we assessed, save for that of the anechoic room, which was suboptimal in terms of listening performance.</p>
</sec>
<sec id="s4c">
<title>Neural adaptation as a contributing mechanism to the learning of room acoustics</title>
<p>Our data are consistent with improved performance in a relevant ‘foreground’ task emerging as the brain adapts to the statistics of background features of the listening environment. Adaptation to stimulus statistics has been proposed as a neurophysiological mechanism underlying selective suppression of background noise in auditory cortex (<xref ref-type="bibr" rid="c55">Fuglsang et al., 2017</xref>; <xref ref-type="bibr" rid="c83">Kell &amp; McDermott, 2019</xref>; <xref ref-type="bibr" rid="c84">Khalighinejad et al., 2019</xref>; <xref ref-type="bibr" rid="c99">Mesgarani et al., 2014</xref>). More recently, it has been shown that spectro-temporal receptive fields of auditory-cortical neurons are sensitive to the RT<sub>60</sub>s of reverberant noise i.e., a form of adaptation specific to reverberation and consistent with a de-reverberation of the environment encountered (<xref ref-type="bibr" rid="c79">Ivanov et al., 2022</xref>). Cortical adaptation to reverberation has been also reported in awake listeners (<xref ref-type="bibr" rid="c55">Fuglsang et al., 2017</xref>; <xref ref-type="bibr" rid="c99">Mesgarani et al., 2014</xref>) and to this end, our data suggest a role for top-down mechanisms in improving over time an attended task—recalling spoken words—in reverberant environments. It is therefore possible that goal-directed behaviours and the feedback the auditory cortex receives form areas such as dlPFC may contribute to fine-tuning the adaptation to reverberant environments.</p>
<p>Our data are also consistent with reports of midbrain auditory neurons recorded <italic>in vivo</italic> adapting to the statistical structure of evolving sound environments over the course of several hundred milliseconds (<xref ref-type="bibr" rid="c50">Dean et al., 2005</xref>, <xref ref-type="bibr" rid="c51">2008</xref>) and this capacity for adaptation increases with repeated exposures to the same statistically structured environment. Importantly, whilst rapid adaptation is retained when descending cortical influences are disrupted (through cortical cooling), the longer-term learning effect—a speeding up of adaptation referred to as meta-adaptation—is abolished (<xref ref-type="bibr" rid="c115">Robinson et al., 2016</xref>). As with auditory neurons recorded <italic>in vivo</italic> (<xref ref-type="bibr" rid="c10">Bakay et al., 2018</xref>; <xref ref-type="bibr" rid="c50">Dean et al., 2005</xref>, <xref ref-type="bibr" rid="c51">2008</xref>; <xref ref-type="bibr" rid="c79">Ivanov et al., 2022</xref>), it seems listeners might adapt to the statistical structure of a sound environment within a few hundred milliseconds of exposure to enhance performance in a listening task, an initial, rapid, learning phase impervious to inactivation of the dorsolateral prefrontal cortex by theta-burst TMS. The nested set of temporal sensitivities to room acoustics we observe is consistent with features of statistical learning that emerges from the level of the auditory nerve to primary cortex <italic>in vivo</italic> (<xref ref-type="bibr" rid="c50">Dean et al., 2005</xref>, <xref ref-type="bibr" rid="c51">2008</xref>; <xref ref-type="bibr" rid="c144">Watkins &amp; Barbour, 2008</xref>; <xref ref-type="bibr" rid="c146">Wen et al., 2009</xref>), and likely modulated by efferent influences that span the entire auditory pathway all the way to the sensory receptors of the inner ear (<xref ref-type="bibr" rid="c67">Hernández-Pérez et al., 2021</xref>; <xref ref-type="bibr" rid="c108">Perrot et al., 2006</xref>; <xref ref-type="bibr" rid="c134">Terreros &amp; Delano, 2015</xref>) and even the mechanical sensitivity to sound of the eardrum and ossicles (middle-ear bones) (<xref ref-type="bibr" rid="c64">Gruters et al., 2018</xref>).</p>
<p>Dissociating auditory cortical feedback circuits to the midbrain impairs the capacity of midbrain neurons to ‘recall’ previous experience of that environment (<xref ref-type="bibr" rid="c9">Bajo et al., 2019</xref>; <xref ref-type="bibr" rid="c115">Robinson et al., 2016</xref>). Like the performance in our listening tasks, these neurons were still able to adapt rapidly (within hundreds of milliseconds) each time an environment was encountered as if for the first time, but showed no capacity to exploit a memory of that environment to improve neural coding (i.e., meta-adaptation) (<xref ref-type="bibr" rid="c115">Robinson et al., 2016</xref>). Meta-adaptation was consistent with neurons in lower brain centres adapting to the current sound environment, and those in higher brain centres learning the longer-term statistical structure of changing environments. Once higher brain centres learnt the experienced environment, this information, conveyed to early brain centres, ensures they were ‘primed’ for coding an environment when it is re-encountered.</p>
</sec>
<sec id="s4d" sec-type="ethics-statement">
<title>Inclusion and Ethics statement</title>
<p>This study was approved by the Human Research Ethics Committee of Macquarie University (ref: 5201833344874). Each participant signed a written informed consent form and was given a small financial remuneration for their time.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>The study was supported by the Australian Research Council (DP180102524 and FL 160100108 awarded to D.M.). The authors would like to thank Jörg Bulcholz and Javier Badajoz-Davila for their assistance with the spatialization and sound field simulation of the acoustic stimuli. They would also like to thank Kurt Shulver for his assistance during rTMS manipulations. We sincerely thank Yuranny Cabral-Calderin for her valuable insights and constructive feedback on improving this manuscript.</p>
</ack>
<sec id="d1e2117" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Authors’ contributions</title>
<p>Conceptualization, H.H.P., D.M., J.J.M.M. and P.F.S. Methodology, H.H.P., D.M., J.J.M.M. and P.F.S. Investigation, H.H.P. Software, J.J.M.M., J.M-H. and J.T. Formal Analysis, H.H.P., J.M-H. and J.T. Visualization, H.H.P., J.M-H. and J.T. Writing – Original Draft, H.H.P. and D.M. Writing – Review &amp; Editing, H.H.P., D.M., J.J.M.M., P.F.S., J.M-H. and J.T.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Agus</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Carrión-Castillo</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pressnitzer</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Ramus</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Perceptual Learning of Acoustic Noise by Individuals With Dyslexia. <italic>Journal of Speech</italic></article-title>, <source>Language, and Hearing Research: JSLHR</source>, <volume>57</volume>(<issue>3</issue>), <fpage>1069</fpage>–<lpage>1077</lpage>. <pub-id pub-id-type="doi">10.1044/1092-4388(2013/13-0020)</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ainsworth</surname>, <given-names>W. A.</given-names></string-name>, &amp; <string-name><surname>Meyer</surname>, <given-names>G. F</given-names></string-name></person-group>. (<year>1994</year>). <article-title>Recognition of plosive syllables in noise: comparison of an auditory model with human performance</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>96</volume>(<issue>2 Pt 1</issue>), <fpage>687</fpage>–<lpage>694</lpage>. <pub-id pub-id-type="doi">10.1121/1.410306</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alexander</surname>, <given-names>W. H.</given-names></string-name>, &amp; <string-name><surname>Brown</surname>, <given-names>J. W</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Frontal cortex function as derived from hierarchical predictive coding</article-title>. <source>Scientific Reports</source>, <volume>8</volume>(<issue>1</issue>), <fpage>3843</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-018-21407-9</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ambrus</surname>, <given-names>G. G.</given-names></string-name>, <string-name><surname>Vékony</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Janacsek</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Trimborn</surname>, <given-names>A. B. C.</given-names></string-name>, <string-name><surname>Kovács</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Nemeth</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2020</year>). <article-title>When less is more: Enhanced statistical learning of non-adjacent dependencies after disruption of bilateral DLPFC</article-title>. <source>Journal of Memory and Language</source>, <volume>114</volume>, <fpage>104144</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>L. A.</given-names></string-name>, &amp; <string-name><surname>Malmierca</surname>, <given-names>M. S</given-names></string-name></person-group>. (<year>2013</year>). <article-title>The effect of auditory cortex deactivation on stimulus-specific adaptation in the inferior colliculus of the rat</article-title>. <source>The European Journal of Neuroscience</source>, <volume>37</volume>(<issue>1</issue>), <fpage>52</fpage>–<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1111/ejn.12018</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Antunes</surname>, <given-names>F. M.</given-names></string-name>, &amp; <string-name><surname>Malmierca</surname>, <given-names>M. S</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Effect of auditory cortex deactivation on stimulus-specific adaptation in the medial geniculate body</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>31</volume>(<issue>47</issue>), <fpage>17306</fpage>–<lpage>17316</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1915-11.2011</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, <string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Newport</surname>, <given-names>E. L</given-names></string-name></person-group>. (<year>1998</year>). <article-title>Computation of Conditional Probability Statistics by 8-Month-Old Infants</article-title>. <source>Psychological Science</source>, <volume>9</volume>(<issue>4</issue>), <fpage>321</fpage>–<lpage>324</lpage>. <pub-id pub-id-type="doi">10.1111/1467-9280.00063</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Badajoz-Davila</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Buchholz</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Van-Hoesel</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Effect of noise and reverberation on speech intelligibility for cochlear implant recipients in realistic sound environments</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>147</volume>(<issue>5</issue>), <fpage>3538</fpage>. <pub-id pub-id-type="doi">10.1121/10.0001259</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bajo</surname>, <given-names>V. M.</given-names></string-name>, <string-name><surname>Nodal</surname>, <given-names>F. R.</given-names></string-name>, <string-name><surname>Korn</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Constantinescu</surname>, <given-names>A. O.</given-names></string-name>, <string-name><surname>Mann</surname>, <given-names>E. O.</given-names></string-name>, <string-name><surname>Boyden</surname>, <given-names>E. S</given-names>, <suffix>3rd</suffix></string-name>., &amp; <string-name><surname>King</surname>, <given-names>A. J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Silencing cortical activity during sound-localization training impairs auditory perceptual learning</article-title>. <source>Nature Communications</source>, <volume>10</volume>(<issue>1</issue>), <fpage>3075</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-10770-4</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bakay</surname>, <given-names>W. M. H.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Garcia-Lazaro</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>McAlpine</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Schaette</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Hidden hearing loss selectively impairs neural adaptation to loud sound environments</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>), <fpage>4298</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-06777-y</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barascud</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Pearce</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, &amp; <string-name><surname>Chait</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>(<issue>5</issue>), <fpage>E616</fpage>– <lpage>E625</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1508523113</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barbas</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Pandya</surname>, <given-names>D. N</given-names></string-name></person-group>. (<year>1987</year>). <article-title>Architecture and frontal cortical connections of the premotor cortex (area 6) in the rhesus monkey</article-title>. <source>The Journal of Comparative Neurology</source>, <volume>256</volume>(<issue>2</issue>), <fpage>211</fpage>–<lpage>228</lpage>. <pub-id pub-id-type="doi">10.1002/cne.902560203</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Barbas</surname>, <given-names>Helen</given-names></string-name>, &amp; <string-name><surname>Pandya</surname>, <given-names>D. N.</given-names></string-name></person-group> (<year>1991</year>). <chapter-title>Patterns of connections of the prefrontal cortex in the rhesus monkey associated with cortical architecture</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>H. S.</given-names> <surname>Levin</surname></string-name></person-group> (Ed.), <source>Frontal lobe function and dysfunction</source>, (pp (Vol. <volume>427</volume>, pp. <fpage>35</fpage>–<lpage>58</lpage>). <publisher-name>Oxford University Press</publisher-name>, xv. <ext-link ext-link-type="uri" xlink:href="https://psycnet.apa.org/fulltext/1992-97203-002.pdf">https://psycnet.apa.org/fulltext/1992-97203-002.pdf</ext-link></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barberis</surname>, <given-names>N. C</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Thirty Years of Prospect Theory in Economics: A Review and Assessment</article-title>. <source>The Journal of Economic Perspectives: A Journal of the American Economic Association</source>, <volume>27</volume>(<issue>1</issue>), <fpage>173</fpage>–<lpage>196</lpage>. <pub-id pub-id-type="doi">10.1257/jep.27.1.173</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bartolo</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Averbeck</surname>, <given-names>B. B</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Inference as a fundamental process in behavior</article-title>. <source>Current Opinion in Behavioral Sciences</source>, <volume>38</volume>, <fpage>8</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1016/j.cobeha.2020.06.005</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ben-David</surname>, <given-names>B. M.</given-names></string-name>, <string-name><surname>Avivi-Reich</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Schneider</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Does the degree of linguistic experience (native versus nonnative) modulate the degree to which listeners can benefit from a delay between the onset of the maskers and the onset of the target speech?</article-title> <source>Hearing Research</source>, <volume>341</volume>, <fpage>9</fpage>–<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1016/j.heares.2016.07.016</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Best</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Ozmeral</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Kopco</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Shinn-Cunningham</surname>, <given-names>B. G</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Object continuity enhances selective auditory attention</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>105</volume>(<issue>35</issue>), <fpage>13174</fpage>–<lpage>13178</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0803718105</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bianco</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Harrison</surname>, <given-names>P. M. C.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bolger</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Picken</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pearce</surname>, <given-names>M. T.</given-names></string-name>, &amp; <string-name><surname>Chait</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Long-term implicit memory for sequential auditory patterns in humans</article-title>. <source>eLife</source>, <volume>9</volume>, <elocation-id>e56073</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.56073</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Binder</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Liebenthal</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Possing</surname>, <given-names>E. T.</given-names></string-name>, <string-name><surname>Medler</surname>, <given-names>D. A.</given-names></string-name>, &amp; <string-name><surname>Ward</surname>, <given-names>B. D</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Neural correlates of sensory and decision processes in auditory object identification</article-title>. <source>Nature Neuroscience</source>, <volume>7</volume>(<issue>3</issue>), <fpage>295</fpage>–<lpage>301</lpage>. <pub-id pub-id-type="doi">10.1038/nn1198</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blackwell</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Lesicko</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Rao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>De Biasi</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Geffen</surname>, <given-names>M. N.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Auditory cortex shapes sound responses in the inferior colliculus</article-title>. <source>eLife</source>, <volume>9</volume>. <pub-id pub-id-type="doi">10.7554/eLife.51890</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Blesser</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Salter</surname>, <given-names>L.-R</given-names></string-name></person-group>. (<year>2009</year>). <source>Spaces Speak, Are You Listening?: Experiencing Aural Architecture</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bolia</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Nelson</surname>, <given-names>W. T.</given-names></string-name>, <string-name><surname>Ericson</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Simpson</surname>, <given-names>B. D</given-names></string-name></person-group>. (<year>2000</year>). <article-title>A speech corpus for multitalker communications research</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>107</volume>(<issue>2</issue>), <fpage>1065</fpage>–<lpage>1066</lpage>. <pub-id pub-id-type="doi">10.1121/1.428288</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bradley</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Reich</surname>, <given-names>R. D.</given-names></string-name>, &amp; <string-name><surname>Norcross</surname>, <given-names>S. G</given-names></string-name></person-group>. (<year>1999</year>). <article-title>On the combined effects of signal-to-noise ratio and room acoustics on speech intelligibility</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>106</volume>(<issue>4 Pt 1</issue>), <fpage>1820</fpage>–<lpage>1828</lpage>. <pub-id pub-id-type="doi">10.1121/1.427932</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname>, <given-names>M. S.</given-names></string-name>, &amp; <string-name><surname>Doupe</surname>, <given-names>A. J</given-names></string-name></person-group>. (<year>2002</year>). <article-title>What songbirds teach us about learning</article-title>. <source>Nature</source>, <volume>417</volume>(<issue>6886</issue>), <fpage>351</fpage>–<lpage>358</lpage>. <pub-id pub-id-type="doi">10.1038/417351a</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brandewie</surname>, <given-names>E. J.</given-names></string-name>, &amp; <string-name><surname>Zahorik</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Speech intelligibility in rooms: Disrupting the effect of prior listening exposure</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>143</volume>(<issue>5</issue>), <fpage>3068</fpage>. <pub-id pub-id-type="doi">10.1121/1.5038278</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brandewie</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Zahorik</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Prior listening in rooms improves speech intelligibility</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>128</volume>(<issue>1</issue>), <fpage>291</fpage>–<lpage>299</lpage>. <pub-id pub-id-type="doi">10.1121/1.3436565</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brandewie</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Zahorik</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Adaptation to Room Acoustics Using the Modified Rhyme Test</article-title>. <source>Proceedings of Meetings on Acoustics Acoustical Society of America</source>, <volume>129</volume>(<issue>4</issue>), <fpage>2487</fpage>. <pub-id pub-id-type="doi">10.1121/1.3588198</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brandewie</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Zahorik</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Time course of a perceptual enhancement effect for noise-masked speech in reverberant environments</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>134</volume>(<issue>2</issue>), <fpage>EL265</fpage>-<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1121/1.4816263</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bregman</surname>, <given-names>A. S</given-names></string-name></person-group>. (<year>1994</year>). <source>Auditory Scene Analysis: The Perceptual Organization of Sound</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bronkhorst</surname>, <given-names>A. W.</given-names></string-name>, &amp; <string-name><surname>Houtgast</surname>, <given-names>T</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Auditory distance perception in rooms</article-title>. <source>Nature</source>, <volume>397</volume>(<issue>6719</issue>), <fpage>517</fpage>–<lpage>520</lpage>. <pub-id pub-id-type="doi">10.1038/17374</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Brumm</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Naguib</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2009</year>). <chapter-title>Chapter 1 Environmental Acoustics and the Evolution of Bird Song</chapter-title>. In <source>Advances in the Study of Behavior</source> (Vol. <volume>40</volume>, pp. <fpage>1</fpage>–<lpage>33</lpage>). <publisher-name>Academic Press</publisher-name>. <pub-id pub-id-type="doi">10.1016/S0065-3454(09)40001-9</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Cabrera</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Jeong</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kwak</surname>, <given-names>H. J.</given-names></string-name>, &amp; <string-name><surname>Kim</surname>, <given-names>J.-Y</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Auditory room size perception for modeled and measured rooms</article-title>. <conf-name>INTER-NOISE and NOISE-CON Congress and Conference Proceedings</conf-name>, <volume>2005</volume>, <fpage>2995</fpage>–<lpage>3004</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.academia.edu/download/41945460/Auditory_room_size_percepti">https://www.academia.edu/download/41945460/Auditory_room_size_percepti</ext-link> on_for_modele20160203-29142-1gyb08z.pdf</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Castro-Meneses</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>B. W.</given-names></string-name>, &amp; <string-name><surname>Sowman</surname>, <given-names>P. F</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Vocal response inhibition is enhanced by anodal tDCS over the right prefrontal cortex</article-title>. <source>Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale</source>, <volume>234</volume>(<issue>1</issue>), <fpage>185</fpage>–<lpage>195</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-015-4452-0</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cherry</surname>, <given-names>E. C</given-names></string-name></person-group>. (<year>1953</year>). <article-title>Some Experiments on the Recognition of Speech, with One and with Two Ears</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>25</volume>(<issue>5</issue>), <fpage>975</fpage>– <lpage>979</lpage>. <pub-id pub-id-type="doi">10.1121/1.1907229</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Choi</surname>, <given-names>J. Y.</given-names></string-name>, &amp; <string-name><surname>Perrachione</surname>, <given-names>T. K</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Time and information in perceptual adaptation to speech</article-title>. <source>Cognition</source>, <volume>192</volume>, <fpage>103982</fpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2019.05.019</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Coltheart</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Cox</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sowman</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Morgan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Barnier</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Langdon</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Connaughton</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Teichmann</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Polito</surname>, <given-names>V</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Belief, delusion, hypnosis, and the right dorsolateral prefrontal cortex: A transcranial magnetic stimulation study</article-title>. <source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source>, <volume>101</volume>, <fpage>234</fpage>–<lpage>248</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2018.01.001</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conway</surname>, <given-names>C. M</given-names></string-name></person-group>. (<year>2020</year>). <article-title>How does the brain learn environmental structure? Ten core principles for understanding the neurocognitive mechanisms of statistical learning</article-title>. <source>Neuroscience and Biobehavioral Reviews</source>, <volume>112</volume>, <fpage>279</fpage>–<lpage>299</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.01.032</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conway</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Bauernschmidt</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>S. S.</given-names></string-name>, &amp; <string-name><surname>Pisoni</surname>, <given-names>D. B</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Implicit statistical learning in language processing: word predictability is the key</article-title>. <source>Cognition</source>, <volume>114</volume>(<issue>3</issue>), <fpage>356</fpage>–<lpage>371</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2009.10.009</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conway</surname>, <given-names>C. M.</given-names></string-name>, &amp; <string-name><surname>Christiansen</surname>, <given-names>M. H</given-names></string-name></person-group>. (<year>2001</year>). <article-title>Sequential learning in non-human primates</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>5</volume>(<issue>12</issue>), <fpage>539</fpage>–<lpage>546</lpage>. <pub-id pub-id-type="doi">10.1016/s1364-6613(00)01800-3</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conway</surname>, <given-names>C. M.</given-names></string-name>, &amp; <string-name><surname>Christiansen</surname>, <given-names>M. H</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Modality-constrained statistical learning of tactile, visual, and auditory sequences</article-title>. <source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source>, <volume>31</volume>(<issue>1</issue>), <fpage>24</fpage>–<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1037/0278-7393.31.1.24</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cooke</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2006</year>). <article-title>A glimpsing model of speech perception in noise</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>119</volume>(<issue>3</issue>), <fpage>1562</fpage>–<lpage>1573</lpage>. <pub-id pub-id-type="doi">10.1121/1.2166600</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cooke</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Scharenborg</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Meyer</surname>, <given-names>B. T</given-names></string-name></person-group>. (<year>2022</year>). <article-title>The time course of adaptation to distorted speech</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>151</volume>(<issue>4</issue>), <fpage>2636</fpage>. <pub-id pub-id-type="doi">10.1121/10.0010235</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Costalupes</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Young</surname>, <given-names>E. D.</given-names></string-name>, &amp; <string-name><surname>Gibson</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>1984</year>). <article-title>Effects of continuous noise backgrounds on rate response of auditory nerve fibers in cat</article-title>. <source>Journal of Neurophysiology</source>, <volume>51</volume>(<issue>6</issue>), <fpage>1326</fpage>–<lpage>1344</lpage>. <pub-id pub-id-type="doi">10.1152/jn.1984.51.6.1326</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Culling</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Hodder</surname>, <given-names>K. I.</given-names></string-name>, &amp; <string-name><surname>Toh</surname>, <given-names>C. Y</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Effects of reverberation on perceptual segregation of competing voices</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>114</volume>(<issue>5</issue>), <fpage>2871</fpage>–<lpage>2876</lpage>. <pub-id pub-id-type="doi">10.1121/1.1616922</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Daikhin</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Raviv</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Ahissar</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Auditory Stimulus Processing and Task Learning Are Adequate in Dyslexia, but Benefits From Regularities Are Reduced. <italic>Journal of Speech</italic></article-title>, <source>Language, and Hearing Research: JSLHR</source>, <volume>60</volume>(<issue>2</issue>), <fpage>471</fpage>–<lpage>479</lpage>. <pub-id pub-id-type="doi">10.1044/2016_JSLHR-H-16-0114</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Daikoku</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Yumoto</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Order of statistical learning depends on perceptive uncertainty</article-title>. <source>Current Research in Neurobiology</source>, <volume>4</volume>(<issue>100080</issue>), <fpage>100080</fpage>. <pub-id pub-id-type="doi">10.1016/j.crneur.2023.100080</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davis</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Ford</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Kherif</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Johnsrude</surname>, <given-names>I. S</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Does semantic context benefit speech understanding through “top--down” processes? Evidence from time-resolved sparse fMRI</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>23</volume>(<issue>12</issue>), <fpage>3914</fpage>– <lpage>3932</lpage>. <ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/jocn/article-abstract/23/12/3914/5277">https://direct.mit.edu/jocn/article-abstract/23/12/3914/5277</ext-link></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, <string-name><surname>Niv</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Dayan</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>(<issue>12</issue>), <fpage>1704</fpage>–<lpage>1711</lpage>. <pub-id pub-id-type="doi">10.1038/nn1560</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de Boer</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Thornton</surname>, <given-names>A. R. D.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Neural correlates of perceptual learning in the auditory brainstem: efferent activity predicts and reflects improvement at a speech-in-noise discrimination task</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>28</volume>(<issue>19</issue>), <fpage>4929</fpage>–<lpage>4937</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dean</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Harper</surname>, <given-names>N. S.</given-names></string-name>, &amp; <string-name><surname>McAlpine</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Neural population coding of sound level adapts to stimulus statistics</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>, <fpage>1684</fpage>. <pub-id pub-id-type="doi">10.1038/nn1541</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dean</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>B. L.</given-names></string-name>, <string-name><surname>Harper</surname>, <given-names>N. S.</given-names></string-name>, &amp; <string-name><surname>McAlpine</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Rapid Neural Adaptation to Sound Level Statistics</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>28</volume>(<issue>25</issue>), <fpage>6430</fpage>–<lpage>6438</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.0470-08.2008</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Du</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Buchsbaum</surname>, <given-names>B. R.</given-names></string-name>, <string-name><surname>Grady</surname>, <given-names>C. L.</given-names></string-name>, &amp; <string-name><surname>Alain</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Increased activity in frontal motor cortex compensates impaired speech perception in older adults</article-title>. <source>Nature Communications</source>, <volume>7</volume>, <fpage>12241</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms12241</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Faul</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Erdfelder</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lang</surname>, <given-names>A.-G.</given-names></string-name>, &amp; <string-name><surname>Buchner</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2007</year>). <article-title>G*Power 3: a flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title>. <source>Behavior Research Methods</source>, <volume>39</volume>(<issue>2</issue>), <fpage>175</fpage>–<lpage>191</lpage>. <pub-id pub-id-type="doi">10.3758/bf03193146</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiser</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R. N</given-names></string-name></person-group>. (<year>2001</year>). <article-title>Unsupervised statistical learning of higher-order spatial structures from visual scenes</article-title>. <source>Psychological Science</source>, <volume>12</volume>(<issue>6</issue>), <fpage>499</fpage>–<lpage>504</lpage>. <pub-id pub-id-type="doi">10.1111/1467-9280.00392</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fuglsang</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Dau</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Hjortkjær</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Noise-robust cortical tracking of attended speech in real-world acoustic scenes</article-title>. <source>NeuroImage</source>, <volume>156</volume>, <fpage>435</fpage>–<lpage>444</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.04.026</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fuster</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Bodner</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Kroger</surname>, <given-names>J. K</given-names></string-name></person-group>. (<year>2000</year>). <article-title>Cross-modal and cross-temporal association in neurons of frontal cortex</article-title>. <source>Nature</source>, <volume>405</volume>(<issue>6784</issue>), <fpage>347</fpage>–<lpage>351</lpage>. <pub-id pub-id-type="doi">10.1038/35012613</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gamboa</surname>, <given-names>O. L.</given-names></string-name>, <string-name><surname>Antal</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Moliadze</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Paulus</surname>, <given-names>W</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Simply longer is not better: reversal of theta burst after-effect with prolonged stimulation</article-title>. <source>Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale</source>, <volume>204</volume>(<issue>2</issue>), <fpage>181</fpage>–<lpage>187</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-010-2293-4</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gariépy</surname>, <given-names>J.-F.</given-names></string-name>, <string-name><surname>Watson</surname>, <given-names>K. K.</given-names></string-name>, <string-name><surname>Du</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Xie</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>Erb</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Amasino</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Platt</surname>, <given-names>M. L</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Social learning in humans and other animals</article-title>. <source>Frontiers in Neuroscience</source>, <volume>8</volume>, <fpage>58</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2014.00058</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garinis</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Glattke</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Cone</surname>, <given-names>B. K</given-names></string-name></person-group>. (<year>2011</year>). <article-title>The MOC reflex during active listening to speech</article-title>. <source>Journal of Speech, Language, and Hearing Research: JSLHR</source>, <volume>54</volume>(<issue>5</issue>), <fpage>1464</fpage>–<lpage>1476</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gibson</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Young</surname>, <given-names>E. D.</given-names></string-name>, &amp; <string-name><surname>Costalupes</surname>, <given-names>J. A</given-names></string-name></person-group>. (<year>1985</year>). <article-title>Similarity of dynamic range adjustment in auditory nerve and cochlear nuclei</article-title>. <source>Journal of Neurophysiology</source>, <volume>53</volume>(<issue>4</issue>), <fpage>940</fpage>–<lpage>958</lpage>. <pub-id pub-id-type="doi">10.1152/jn.1985.53.4.940</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giraud</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Garnier</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Micheyl</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lina</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Chays</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Chéry-Croze</surname>, <given-names>S</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Auditory efferents involved in speech-in-noise intelligibility</article-title>. <source>Neuroreport</source>, <volume>8</volume>(<issue>7</issue>), <fpage>1779</fpage>–<lpage>1783</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasberg</surname>, <given-names>B. R.</given-names></string-name>, &amp; <string-name><surname>Moore</surname>, <given-names>B. C</given-names></string-name></person-group>. (<year>1990</year>). <article-title>Derivation of auditory filter shapes from notched-noise data</article-title>. <source>Hearing Research</source>, <volume>47</volume>(<issue>1–2</issue>), <fpage>103</fpage>–<lpage>138</lpage>. <pub-id pub-id-type="doi">10.1016/0378-5955(90)90170-t</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goldman-Rakic</surname>, <given-names>P. S.</given-names></string-name>, &amp; <string-name><surname>Schwartz</surname>, <given-names>M. L</given-names></string-name></person-group>. (<year>1982</year>). <article-title>Interdigitation of contralateral and ipsilateral columnar projections to frontal association cortex in primates</article-title>. <source>Science</source>, <volume>216</volume>(<issue>4547</issue>), <fpage>755</fpage>–<lpage>757</lpage>. <pub-id pub-id-type="doi">10.1126/science.6177037</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gruters</surname>, <given-names>K. G.</given-names></string-name>, <string-name><surname>Murphy</surname>, <given-names>D. L. K.</given-names></string-name>, <string-name><surname>Jenson</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>D. W.</given-names></string-name>, <string-name><surname>Shera</surname>, <given-names>C. A.</given-names></string-name>, &amp; <string-name><surname>Groh</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2018</year>). <article-title>The eardrums move when the eyes move: A multisensory effect on the mechanics of hearing</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>115</volume>(<issue>6</issue>), <fpage>E1309</fpage>–<lpage>E1318</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1717948115</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hackett</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Stepniewska</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Kaas</surname>, <given-names>J. H</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Prefrontal connections of the parabelt auditory cortex in macaque monkeys</article-title>. <source>Brain Research</source>, <volume>817</volume>(<issue>1–2</issue>), <fpage>45</fpage>–<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1016/s0006-8993(98)01182-2</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hawley</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Litovsky</surname>, <given-names>R. Y.</given-names></string-name>, &amp; <string-name><surname>Culling</surname>, <given-names>J. F</given-names></string-name></person-group>. (<year>2004</year>). <article-title>The benefit of binaural hearing in a cocktail party: effect of location and type of interferer</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>115</volume>(<issue>2</issue>), <fpage>833</fpage>–<lpage>843</lpage>. <pub-id pub-id-type="doi">10.1121/1.1639908</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hernández-Pérez</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Mikiel-Hunter</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>McAlpine</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Dhar</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Boothalingam</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Monaghan</surname>, <given-names>J. J. M.</given-names></string-name>, &amp; <string-name><surname>McMahon</surname>, <given-names>C. M</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Understanding degraded speech leads to perceptual gating of a brainstem reflex in human listeners</article-title>. <source>PLoS Biology</source>, <volume>19</volume>(<issue>10</issue>), <fpage>e3001439</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3001439</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hertrich</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Dietrich</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Blum</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Ackermann</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2021</year>). <article-title>The role of the dorsolateral prefrontal cortex for speech and language processing</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>15</volume>, <fpage>645209</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2021.645209</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herwig</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Satrapi</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Schönfeldt-Lecuona</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Using the international 10-20 EEG system for positioning of transcranial magnetic stimulation</article-title>. <source>Brain Topography</source>, <volume>16</volume>(<issue>2</issue>), <fpage>95</fpage>–<lpage>99</lpage>. <pub-id pub-id-type="doi">10.1023/b:brat.0000006333.93597.9d</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hicks</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>McDermott</surname>, <given-names>J. H</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Noise schemas aid hearing in noise</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>121</volume>(<issue>47</issue>). <pub-id pub-id-type="doi">10.1073/pnas.2408995121</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoogendam</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Ramakers</surname>, <given-names>G. M. J.</given-names></string-name>, &amp; <string-name><surname>Di Lazzaro</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Physiology of repetitive transcranial magnetic stimulation of the human brain</article-title>. <source>Brain Stimulation</source>, <volume>3</volume>(<issue>2</issue>), <fpage>95</fpage>–<lpage>118</lpage>. <pub-id pub-id-type="doi">10.1016/j.brs.2009.10.005</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Houtgast</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Steeneken</surname>, <given-names>H. J. M</given-names></string-name></person-group>. (<year>1985</year>). <article-title>A review of the MTF concept in room acoustics and its use for estimating speech intelligibility in auditoria</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>77</volume>(<issue>3</issue>), <fpage>1069</fpage>–<lpage>1077</lpage>. <pub-id pub-id-type="doi">10.1121/1.392224</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Houtgast</surname>, <given-names>To</given-names></string-name>, &amp; <string-name><surname>Steeneken</surname>, <given-names>H. J</given-names></string-name></person-group>. (<year>1973</year>). <article-title>The modulation transfer function in room acoustics as a predictor of speech intelligibility</article-title>. <source>Acta Acustica United with Acustica</source>, <volume>28</volume>(<issue>1</issue>), <fpage>66</fpage>–<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1121/1.1913632</pub-id></mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname>, <given-names>Y.-Z.</given-names></string-name>, <string-name><surname>Edwards</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Rounis</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Bhatia</surname>, <given-names>K. P.</given-names></string-name>, &amp; <string-name><surname>Rothwell</surname>, <given-names>J. C</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Theta Burst Stimulation of the Human Motor Cortex</article-title>. <source>Neuron</source>, <volume>45</volume>(<issue>2</issue>), <fpage>201</fpage>–<lpage>206</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2004.12.033</pub-id></mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hughson</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Westlake</surname>, <given-names>H</given-names></string-name></person-group>. (<year>1944</year>). <article-title>Manual for program outline for rehabilitation of aural casualties both military and civilian</article-title>. <source>Transactions - American Academy of Ophthalmology and Otolaryngology</source>. <publisher-name>American Academy of Ophthalmology and Otolaryngology</publisher-name>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huyck</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Wright</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Late maturation of auditory perceptual learning</article-title>. <source>Developmental Science</source>, <volume>14</volume>(<issue>3</issue>), <fpage>614</fpage>–<lpage>621</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-7687.2010.01009.x</pub-id></mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Idemaru</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Holt</surname>, <given-names>L. L</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Word recognition reflects dimension-based statistical learning</article-title>. <source>Journal of Experimental Psychology. Human Perception and Performance</source>, <volume>37</volume>(<issue>6</issue>), <fpage>1939</fpage>–<lpage>1956</lpage>. <pub-id pub-id-type="doi">10.1037/a0025641</pub-id></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Idemaru</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Holt</surname>, <given-names>L. L</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Specificity of dimension-based statistical learning in word recognition</article-title>. <source>Journal of Experimental Psychology. Human Perception and Performance</source>, <volume>40</volume>(<issue>3</issue>), <fpage>1009</fpage>–<lpage>1021</lpage>. <pub-id pub-id-type="doi">10.1037/a0035269</pub-id></mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ivanov</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>King</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Willmore</surname>, <given-names>B. D. B.</given-names></string-name>, <string-name><surname>Walker</surname>, <given-names>K. M. M.</given-names></string-name>, &amp; <string-name><surname>Harper</surname>, <given-names>N. S</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Cortical adaptation to sound reverberation</article-title>. <source>eLife</source>, <volume>11</volume>. <pub-id pub-id-type="doi">10.7554/eLife.75090</pub-id></mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jurcak</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Okamoto</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Singh</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Dan</surname>, <given-names>I</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Virtual 10–20 measurement on MR images for inter-modal linking of transcranial and tomographic neuroimaging methods</article-title>. <source>NeuroImage</source>, <volume>26</volume>(<issue>4</issue>), <fpage>1184</fpage>–<lpage>1192</lpage>. <ext-link ext-link-type="uri" xlink:href="https://ac.els-cdn.com/S1053811905001862/1-s2.0-S1053811905001862-main.pdf?_tid=87d3462c-b7fa4780-b7b0-289d930895bd&amp;acdnat=1544065940_494439b3ca37d9077258347769daf7e0">https://ac.els-cdn.com/S1053811905001862/1-s2.0-S1053811905001862-main.pdf?_tid=87d3462c-b7fa4780-b7b0-289d930895bd&amp;acdnat=1544065940_494439b3ca37d9077258347769daf7e0</ext-link></mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kakehi</surname>, <given-names>K</given-names></string-name></person-group>. (<year>1992</year>). <article-title>Adaptability to differences between talkers in Japanese monosyllabic perception. <italic>Speech Perception</italic></article-title>, <source>Speech Production, and Linguistic Structure</source>, <fpage>135</fpage>–<lpage>142</lpage>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kato</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Kakehi</surname>, <given-names>K</given-names></string-name></person-group>. (<year>1988</year>). <article-title>Listener adaptability to individual speaker differences in monosyllabic speech perception</article-title>. <source>J. Acoust. Soc. Jpn</source>, <volume>44</volume>(<issue>3</issue>), <fpage>180</fpage>–<lpage>186</lpage>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kell</surname>, <given-names>A. J. E.</given-names></string-name>, &amp; <string-name><surname>McDermott</surname>, <given-names>J. H</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Invariance to background noise as a signature of non-primary auditory cortex</article-title>. <source>Nature Communications</source>, <volume>10</volume>(<issue>1</issue>), <fpage>3958</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-11710-y</pub-id></mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khalighinejad</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Herrero</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>A. D.</given-names></string-name>, &amp; <string-name><surname>Mesgarani</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Adaptation of the human auditory cortex to changing background noise</article-title>. <source>Nature Communications</source>, <volume>10</volume>(<issue>1</issue>), <fpage>2509</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-10611-4</pub-id></mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kirkham</surname>, <given-names>N. Z.</given-names></string-name>, <string-name><surname>Slemmer</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>S. P</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Visual statistical learning in infancy: evidence for a domain general learning mechanism</article-title>. <source>Cognition</source>, <volume>83</volume>(<issue>2</issue>), <fpage>B35</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1016/s0010-0277(02)00004-5</pub-id></mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knudsen</surname>, <given-names>V. O</given-names></string-name></person-group>. (<year>1929</year>). <article-title>The hearing of speech in auditoriums</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>1</volume>(<issue>1</issue>), <fpage>56</fpage>–<lpage>82</lpage>. <pub-id pub-id-type="doi">10.1121/1.1901470</pub-id></mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kolarik</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Moore</surname>, <given-names>B. C. J.</given-names></string-name>, <string-name><surname>Cirstea</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Aggius-Vella</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Gori</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Campus</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Pardhan</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Factors Affecting Auditory Estimates of Virtual Room Size: Effects of Stimulus, Level, and Reverberation</article-title>. <source>Perception</source>, <volume>50</volume>(<issue>7</issue>), <fpage>646</fpage>–<lpage>663</lpage>. <pub-id pub-id-type="doi">10.1177/03010066211020598</pub-id></mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Larsby</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Hällgren</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nilsson</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>McAllister</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2015</year>). <article-title>The influence of female versus male speakers’ voice on speech recognition thresholds in noise: Effects of low- and high-frequency hearing impairment. <italic>Speech</italic></article-title>, <source>Language and Hearing</source>, <volume>18</volume>, <fpage>83</fpage>–<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1179/2050572814Y.0000000053</pub-id></mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lauay</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gerlach</surname>, <given-names>N. M.</given-names></string-name>, <string-name><surname>Adkins-Regan</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>DeVoogd</surname>, <given-names>T. J</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Female zebra finches require early song exposure to prefer high-quality song as adults</article-title>. <source>Animal Behaviour</source>, <volume>68</volume>(<issue>6</issue>), <fpage>1249</fpage>–<lpage>1255</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2003.12.025</pub-id></mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lewicki</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Surlykke</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Moss</surname>, <given-names>C. F</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Scene analysis in the natural environment</article-title>. <source>Frontiers in Psychology</source>, <volume>5</volume>, <fpage>199</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2014.00199</pub-id></mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Holt</surname>, <given-names>L. L</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Dimension-based statistical learning of vowels</article-title>. <source>Journal of Experimental Psychology. Human Perception and Performance</source>, <volume>41</volume>(<issue>6</issue>), <fpage>1783</fpage>– <lpage>1798</lpage>. <pub-id pub-id-type="doi">10.1037/xhp0000092</pub-id></mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lochner</surname>, <given-names>J. P. A.</given-names></string-name>, &amp; <string-name><surname>Burger</surname>, <given-names>J. F</given-names></string-name></person-group>. (<year>1961</year>). <article-title>The intelligibility of speech under reverberant conditions</article-title>. <source>Acta Acustica United with Acustica</source>, <volume>11</volume>(<issue>4</issue>), <fpage>195</fpage>–<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marler</surname>, <given-names>P</given-names></string-name></person-group>. (<year>1970</year>). <article-title>A comparative approach to vocal learning: Song development in white-crowned sparrows</article-title>. <source>Journal of Comparative and Physiological Psychology</source>, <volume>71</volume>(<issue>2p2</issue>), <fpage>1</fpage>–<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1037/h0029144</pub-id></mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathews</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Buss</surname>, <given-names>R. R.</given-names></string-name>, <string-name><surname>Stanley</surname>, <given-names>W. B.</given-names></string-name>, <string-name><surname>Blanchard-Fields</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Cho</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Druhan</surname>, <given-names>B</given-names></string-name></person-group>. (<year>1989</year>). <article-title>Role of implicit and explicit processes in learning from examples: A synergistic effect</article-title>. <source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source>, <volume>15</volume>(<issue>6</issue>), <fpage>1083</fpage>–<lpage>1100</lpage>. <pub-id pub-id-type="doi">10.1037/0278-7393.15.6.1083</pub-id></mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McAlpine</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>de Hoz</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Listening loops and the adapting auditory brain</article-title>. <source>Frontiers in Neuroscience</source>, <volume>17</volume>, <fpage>1081295</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2023.1081295</pub-id></mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Schemitsch</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Simoncelli</surname>, <given-names>E. P</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Summary statistics in auditory perception</article-title>. <source>Nature Neuroscience</source>, <volume>16</volume>(<issue>4</issue>), <fpage>493</fpage>–<lpage>498</lpage>.</mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name>, &amp; <string-name><surname>Simoncelli</surname>, <given-names>E. P</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis</article-title>. <source>Neuron</source>, <volume>71</volume>(<issue>5</issue>), <fpage>926</fpage>– <lpage>940</lpage>.</mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McWalter</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>McDermott</surname>, <given-names>J. H</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Adaptive and Selective Time Averaging of Auditory Scenes</article-title>. <source>Current Biology: CB</source>, <volume>28</volume>(<issue>9</issue>), <fpage>1405</fpage>–<lpage>1418.e10.</lpage> <pub-id pub-id-type="doi">10.1016/j.cub.2018.03.049</pub-id></mixed-citation></ref>
<ref id="c99"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>David</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Fritz</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>Shamma</surname>, <given-names>S. A</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Mechanisms of noise robust representation of speech in primary auditory cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>111</volume>(<issue>18</issue>), <fpage>6792</fpage>– <lpage>6797</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1318017111</pub-id></mixed-citation></ref>
<ref id="c100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mishra</surname>, <given-names>S. K.</given-names></string-name>, &amp; <string-name><surname>Lutman</surname>, <given-names>M. E</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Top-down influences of the medial olivocochlear efferent system in speech perception in noise</article-title>. <source>PloS One</source>, <volume>9</volume>(<issue>1</issue>), <fpage>e85756</fpage>.</mixed-citation></ref>
<ref id="c101"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morrone</surname>, <given-names>M. C</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Brain development: critical periods for cross-sensory plasticity [Review of <italic>Brain development: critical periods for cross-sensory plasticity</italic>]</article-title>. <source>Current Biology: CB</source>, <volume>20</volume>(<issue>21</issue>), <fpage>R934</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2010.09.052</pub-id></mixed-citation></ref>
<ref id="c102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nielsen</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>Dau</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Revisiting perceptual compensation for effects of reverberation in speech identification</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>128</volume>(<issue>5</issue>), <fpage>3088</fpage>–<lpage>3094</lpage>. <pub-id pub-id-type="doi">10.1121/1.3494508</pub-id></mixed-citation></ref>
<ref id="c103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nissen</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Bullemer</surname>, <given-names>P</given-names></string-name></person-group>. (<year>1987</year>). <article-title>Attentional requirements of learning: Evidence from performance measures</article-title>. <source>Cognitive Psychology</source>, <volume>19</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1016/0010-0285(87)90002-8</pub-id></mixed-citation></ref>
<ref id="c104"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nydam</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Sewell</surname>, <given-names>D. K.</given-names></string-name>, &amp; <string-name><surname>Dux</surname>, <given-names>P. E</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Cathodal electrical stimulation of frontoparietal cortex disrupts statistical learning of visual configural information</article-title>. <source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source>, <volume>99</volume>, <fpage>187</fpage>–<lpage>199</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2017.11.008</pub-id></mixed-citation></ref>
<ref id="c105"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pandya</surname> <given-names>D.Ν.</given-names></string-name>, <string-name><surname>Barnes</surname> <given-names>C.L</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Architecture and connections of the frontal lobe</article-title>. <source>The Frontal Lobes Revisited</source>. <pub-id pub-id-type="doi">10.4324/9781315788975-3/architecture-connections-frontal-lobe-deepak-pandya-clifford-barnes</pub-id></mixed-citation></ref>
<ref id="c106"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wassermann</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Grafman</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Hallett</surname>, <given-names>M</given-names></string-name></person-group>. (<year>1996</year>). <article-title>The role of the dorsolateral prefrontal cortex in implicit procedural learning</article-title>. <source>Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale</source>, <volume>107</volume>(<issue>3</issue>), <fpage>479</fpage>–<lpage>485</lpage>. <pub-id pub-id-type="doi">10.1007/BF00230427</pub-id></mixed-citation></ref>
<ref id="c107"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peissig</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Kollmeier</surname>, <given-names>B</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Directivity of binaural noise reduction in spatial multiple noise-source arrangements for normal and impaired listeners</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>101</volume>(<issue>3</issue>), <fpage>1660</fpage>–<lpage>1670</lpage>. <pub-id pub-id-type="doi">10.1121/1.418150</pub-id></mixed-citation></ref>
<ref id="c108"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perrot</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ryvlin</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Isnard</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Guénot</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Catenoix</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Fischer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Mauguière</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Collet</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Evidence for corticofugal modulation of peripheral auditory activity in humans</article-title>. <source>Cerebral Cortex</source>, <volume>16</volume>(<issue>7</issue>), <fpage>941</fpage>–<lpage>948</lpage>.</mixed-citation></ref>
<ref id="c109"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petrides</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Pandya</surname>, <given-names>D. N</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Comparative cytoarchitectonic analysis of the human and the macaque ventrolateral prefrontal cortex and corticocortical connection patterns in the monkey</article-title>. <source>The European Journal of Neuroscience</source>, <volume>16</volume>(<issue>2</issue>), <fpage>291</fpage>–<lpage>310</lpage>. <pub-id pub-id-type="doi">10.1046/j.1460-9568.2001.02090.x</pub-id></mixed-citation></ref>
<ref id="c110"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Phillips</surname>, <given-names>D. P</given-names></string-name></person-group>. (<year>1985</year>). <article-title>Temporal response features of cat auditory cortex neurons contributing to sensitivity to tones delivered in the presence of continuous noise</article-title>. <source>Hearing Research</source>, <volume>19</volume>(<issue>3</issue>), <fpage>253</fpage>–<lpage>268</lpage>. <pub-id pub-id-type="doi">10.1016/0378-5955(85)90145-5</pub-id></mixed-citation></ref>
<ref id="c111"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Plakke</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Romanski</surname>, <given-names>L. M</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Auditory connections and functions of prefrontal cortex</article-title>. <source>Frontiers in Neuroscience</source>, <volume>8</volume>, <fpage>199</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2014.00199</pub-id></mixed-citation></ref>
<ref id="c112"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reber</surname>, <given-names>A. S</given-names></string-name></person-group>. (<year>1967</year>). <article-title>Implicit learning of artificial grammars</article-title>. <source>Journal of Verbal Learning and Verbal Behavior</source>, <volume>6</volume>(<issue>6</issue>), <fpage>855</fpage>–<lpage>863</lpage>. <pub-id pub-id-type="doi">10.1016/S0022-5371(67)80149-X</pub-id></mixed-citation></ref>
<ref id="c113"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rebuschat</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2015</year>). <source>Implicit and Explicit Learning of Languages</source>. <publisher-name>John Benjamins Publishing Company</publisher-name>.</mixed-citation></ref>
<ref id="c114"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rees</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Palmer</surname>, <given-names>A. R</given-names></string-name></person-group>. (<year>1988</year>). <article-title>Rate-intensity functions and their modification by broadband noise for neurons in the guinea pig inferior colliculus</article-title>. <source>The Journal of the Acoustical Society of America</source>. <ext-link ext-link-type="uri" xlink:href="https://pubs.aip.org/asa/jasa/article-abstract/83/4/1488/826235">https://pubs.aip.org/asa/jasa/article-abstract/83/4/1488/826235</ext-link></mixed-citation></ref>
<ref id="c115"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Robinson</surname>, <given-names>B. L.</given-names></string-name>, <string-name><surname>Harper</surname>, <given-names>N. S.</given-names></string-name>, &amp; <string-name><surname>McAlpine</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Meta-adaptation in the auditory midbrain under cortical influence</article-title>. <source>Nature Communications</source>, <volume>7</volume>, <fpage>13442</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms13442</pub-id></mixed-citation></ref>
<ref id="c116"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rothwell</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Hallett</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Berardelli</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Eisen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rossini</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Paulus</surname>, <given-names>W</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Magnetic stimulation: motor evoked potentials. The International Federation of Clinical Neurophysiology</article-title>. <source>Electroencephalography and Clinical Neurophysiology. Supplement</source>, <volume>52</volume>, <fpage>97</fpage>.</mixed-citation></ref>
<ref id="c117"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sabine</surname>, <given-names>H</given-names></string-name></person-group>. (<year>1953</year>). <article-title>Room acoustics</article-title>. <source>Trans IRE</source>, <volume>1</volume>, <fpage>4</fpage>–<lpage>12</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.theatrecrafts.com/archive/cue/cue_18_5.pdf">https://www.theatrecrafts.com/archive/cue/cue_18_5.pdf</ext-link></mixed-citation></ref>
<ref id="c118"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, &amp; <string-name><surname>Newport</surname>, <given-names>E. L</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Statistical learning by 8-month-old infants</article-title>. <source>Science</source>, <volume>274</volume>(<issue>5294</issue>), <fpage>1926</fpage>–<lpage>1928</lpage>. <pub-id pub-id-type="doi">10.1126/science.274.5294.1926</pub-id></mixed-citation></ref>
<ref id="c119"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>E. K.</given-names></string-name>, <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, &amp; <string-name><surname>Newport</surname>, <given-names>E. L</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Statistical learning of tone sequences by human infants and adults</article-title>. <source>Cognition</source>, <volume>70</volume>(<issue>1</issue>), <fpage>27</fpage>–<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1016/s0010-0277(98)00075-4</pub-id></mixed-citation></ref>
<ref id="c120"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saffran</surname>, <given-names>Jenny R</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Statistical Language Learning: Mechanisms and Constraints</article-title>. <source>Current Directions in Psychological Science</source>, <volume>12</volume>(<issue>4</issue>), <fpage>110</fpage>–<lpage>114</lpage>. <pub-id pub-id-type="doi">10.1111/1467-8721.01243</pub-id></mixed-citation></ref>
<ref id="c121"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salvi</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Lockwood</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Frisina</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Coad</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Wack</surname>, <given-names>D. S.</given-names></string-name>, &amp; <string-name><surname>Frisina</surname>, <given-names>D. R</given-names></string-name></person-group>. (<year>2002</year>). <article-title>PET imaging of the normal human auditory system: responses to speech in quiet and in background noise</article-title>. <source>Hearing Research</source>, <volume>170</volume>(<issue>1–2</issue>), <fpage>96</fpage>–<lpage>106</lpage>. <pub-id pub-id-type="doi">10.1016/s0378-5955(02)00386-6</pub-id></mixed-citation></ref>
<ref id="c122"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sandrini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Umiltà</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Rusconi</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2011</year>). <article-title>The use of transcranial magnetic stimulation in cognitive neuroscience: a new synthesis of methodological issues</article-title>. <source>Neuroscience and Biobehavioral Reviews</source>, <volume>35</volume>(<issue>3</issue>), <fpage>516</fpage>–<lpage>536</lpage>.</mixed-citation></ref>
<ref id="c123"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Santon</surname>, <given-names>F</given-names></string-name></person-group>. (<year>1976</year>). <article-title>Numerical prediction of echograms and of the intelligibility of speech in rooms</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>59</volume>(<issue>6</issue>), <fpage>1399</fpage>–<lpage>1405</lpage>. <pub-id pub-id-type="doi">10.1121/1.381027</pub-id></mixed-citation></ref>
<ref id="c124"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schroeder</surname>, <given-names>M. R</given-names></string-name></person-group>. (<year>1962</year>). <article-title>Frequency-Correlation Functions of Frequency Responses in Rooms</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>34</volume>(<issue>12</issue>), <fpage>1819</fpage>– <lpage>1823</lpage>. <pub-id pub-id-type="doi">10.1121/1.1909136</pub-id></mixed-citation></ref>
<ref id="c125"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schultz</surname>, <given-names>W</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Getting formal with dopamine and reward</article-title>. <source>Neuron</source>, <volume>36</volume>(<issue>2</issue>), <fpage>241</fpage>–<lpage>263</lpage>. <pub-id pub-id-type="doi">10.1016/s0896-6273(02)00967-4</pub-id></mixed-citation></ref>
<ref id="c126"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shapiro</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Wilk</surname>, <given-names>M</given-names></string-name></person-group>. (<year>1965</year>). <article-title>An analysis of variance test for normality (complete samples)</article-title>. <source>Biometrika</source>, <volume>52</volume>(<issue>3–4</issue>), <fpage>591</fpage>–<lpage>611</lpage>. <pub-id pub-id-type="doi">10.1093/BIOMET/52.3-4.591</pub-id></mixed-citation></ref>
<ref id="c127"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Shinn-Cunningham</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2000</year>, July). <source>Learning Reverberation: Considerations for Spatial Auditory Displays</source>. <ext-link ext-link-type="uri" xlink:href="https://www.researchgate.net/profile/Barbara-Shinn-Cunningham/publication/2414695_Learning_Reverberation_Considerations_for_Spatial_Auditory_Displays/links/0f31752f91fb83f924000000/Learning-Reverberation-Considerations-for-Spatial-Auditory-Displays.pdf">https://www.researchgate.net/profile/Barbara-Shinn-Cunningham/publication/2414695_Learning_Reverberation_Considerations_for_Spatial_Auditory_Displays/links/0f31752f91fb83f924000000/Learning-Reverberation-Considerations-for-Spatial-Auditory-Displays.pdf</ext-link></mixed-citation></ref>
<ref id="c128"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Shinn-Cunningham</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Kawakyu</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Neural representation of source direction in reverberant space</article-title>. <conf-name>2003 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (IEEE Cat. No.03TH8684)</conf-name>, <fpage>79</fpage>–<lpage>82</lpage>. <pub-id pub-id-type="doi">10.1109/ASPAA.2003.1285824</pub-id></mixed-citation></ref>
<ref id="c129"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simpson</surname>, <given-names>A. J. R.</given-names></string-name>, <string-name><surname>Harper</surname>, <given-names>N. S.</given-names></string-name>, <string-name><surname>Reiss</surname>, <given-names>J. D.</given-names></string-name>, &amp; <string-name><surname>McAlpine</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Selective adaptation to “oddball” sounds by the human auditory system</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>34</volume>(<issue>5</issue>), <fpage>1963</fpage>–<lpage>1969</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4274-13.2013</pub-id></mixed-citation></ref>
<ref id="c130"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>E. C.</given-names></string-name>, &amp; <string-name><surname>Lewicki</surname>, <given-names>M. S</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Efficient auditory coding</article-title>. <source>Nature</source>, <volume>439</volume>(<issue>7079</issue>), <fpage>978</fpage>–<lpage>982</lpage>. <pub-id pub-id-type="doi">10.1038/nature04485</pub-id></mixed-citation></ref>
<ref id="c131"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Srinivasan</surname>, <given-names>N. K.</given-names></string-name>, &amp; <string-name><surname>Zahorik</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Prior listening exposure to a reverberant room improves open-set intelligibility of high-variability sentences</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>133</volume>(<issue>1</issue>), <fpage>EL33</fpage>–<lpage>EL39</lpage>. <pub-id pub-id-type="doi">10.1121/1.4771978</pub-id></mixed-citation></ref>
<ref id="c132"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stilp</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Acoustic context effects in speech perception</article-title>. <source>Wiley Interdisciplinary Reviews. Cognitive Science</source>, <volume>11</volume>(<issue>1</issue>), <fpage>e1517</fpage>. <pub-id pub-id-type="doi">10.1002/wcs.1517</pub-id></mixed-citation></ref>
<ref id="c133"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takács</surname>, <given-names>Á.</given-names></string-name>, <string-name><surname>Kóbor</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kardos</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Janacsek</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Horváth</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Beste</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Nemeth</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Neurophysiological and functional neuroanatomical coding of statistical and deterministic rule information during sequence learning</article-title>. <source>Human Brain Mapping</source>, <volume>42</volume>(<issue>10</issue>), <fpage>3182</fpage>–<lpage>3201</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.25427</pub-id></mixed-citation></ref>
<ref id="c134"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Terreros</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Delano</surname>, <given-names>P. H</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Corticofugal modulation of peripheral auditory responses</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>9</volume>. <pub-id pub-id-type="doi">10.3389/fnsys.2015.00134</pub-id></mixed-citation></ref>
<ref id="c135"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Traer</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>McDermott</surname>, <given-names>J. H</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Statistics of natural reverberation enable perceptual separation of sound and space</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>(<issue>48</issue>), <fpage>E7856</fpage>–<lpage>E7865</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1612524113</pub-id></mixed-citation></ref>
<ref id="c136"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van den Bos</surname> <given-names>R.</given-names></string-name>, <string-name><surname>Jolles</surname> <given-names>J.W.</given-names></string-name>, &amp; <string-name><surname>Homberg</surname> <given-names>J.R.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Social modulation of decision-making: a cross-species review</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>7</volume>, <fpage>301</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2013.00301</pub-id></mixed-citation></ref>
<ref id="c137"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vékony</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Ambrus</surname>, <given-names>G. G.</given-names></string-name>, <string-name><surname>Janacsek</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Nemeth</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Cautious or causal? Key implicit sequence learning paradigms should not be overlooked when assessing the role of DLPFC (Commentary on Prutean et al.)</article-title>. <source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source>, <volume>148</volume>, <fpage>222</fpage>–<lpage>226</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2021.10.001</pub-id></mixed-citation></ref>
<ref id="c138"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vlahou</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Seitz</surname>, <given-names>A. R.</given-names></string-name>, &amp; <string-name><surname>Kopčo</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Nonnative implicit phonetic training in multiple reverberant environments. <italic>Attention</italic></article-title>, <source>Perception &amp; Psychophysics</source>, <volume>81</volume>(<issue>4</issue>), <fpage>935</fpage>–<lpage>947</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-019-01680-0</pub-id></mixed-citation></ref>
<ref id="c139"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wächter</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lungu</surname>, <given-names>O. V.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Willingham</surname>, <given-names>D. T.</given-names></string-name>, &amp; <string-name><surname>Ashe</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Differential effect of reward and punishment on procedural learning</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>29</volume>(<issue>2</issue>), <fpage>436</fpage>–<lpage>443</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4132-08.2009</pub-id></mixed-citation></ref>
<ref id="c140"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Hsiao</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Lenz</surname>, <given-names>F. A.</given-names></string-name>, <string-name><surname>Bodner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>Y.-D.</given-names></string-name>, &amp; <string-name><surname>Fuster</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Differential roles of delay-period neural activity in the monkey dorsolateral prefrontal cortex in visual–haptic crossmodal working memory</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>112</volume>(<issue>2</issue>), <fpage>E214</fpage>–<lpage>E219</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1410130112</pub-id></mixed-citation></ref>
<ref id="c141"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watkins</surname>, <given-names>A. J</given-names></string-name></person-group>. (<year>2005a</year>). <article-title>Perceptual compensation for effects of echo and of reverberation on speech identification</article-title>. <source>Acta Acustica United with Acustica</source>, <volume>91</volume>(<issue>5</issue>), <fpage>892</fpage>–<lpage>901</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.ingentaconnect.com/content/dav/aaua/2005/00000091/00000005/art00010">https://www.ingentaconnect.com/content/dav/aaua/2005/00000091/00000005/art00010</ext-link></mixed-citation></ref>
<ref id="c142"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watkins</surname>, <given-names>A. J</given-names></string-name></person-group>. (<year>2005b</year>). <article-title>Perceptual compensation for effects of reverberation in speech identification</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>118</volume>(<issue>1</issue>), <fpage>249</fpage>–<lpage>262</lpage>. <pub-id pub-id-type="doi">10.1121/1.1923369</pub-id></mixed-citation></ref>
<ref id="c143"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watkins</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Makin</surname>, <given-names>S. J</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Steady-spectrum contexts and perceptual compensation for reverberation in speech identification</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>121</volume>(<issue>1</issue>), <fpage>257</fpage>–<lpage>266</lpage>. <pub-id pub-id-type="doi">10.1121/1.2387134</pub-id></mixed-citation></ref>
<ref id="c144"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watkins</surname>, <given-names>P. V.</given-names></string-name>, &amp; <string-name><surname>Barbour</surname>, <given-names>D. L</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Specialized neuronal adaptation for preserving input sensitivity</article-title>. <source>Nature Neuroscience</source>, <volume>11</volume>(<issue>11</issue>), <fpage>1259</fpage>–<lpage>1261</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2201</pub-id></mixed-citation></ref>
<ref id="c145"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weisser</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Buchholz</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Oreinos</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Badajoz-Davila</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Galloway</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Beechey</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Keidser</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The Ambisonic Recordings of Typical Environments (ARTE) database</article-title>. <source>Acta Acustica United with Acustica: The Journal of the European Acoustics Association (EEIG)</source>, <volume>105</volume>(<issue>4</issue>), <fpage>695</fpage>–<lpage>713</lpage>. <pub-id pub-id-type="doi">10.3813/aaa.919349</pub-id></mixed-citation></ref>
<ref id="c146"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wen</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>G. I.</given-names></string-name>, <string-name><surname>Dean</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Delgutte</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Dynamic range adaptation to sound level statistics in the auditory nerve</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>29</volume>(<issue>44</issue>), <fpage>13797</fpage>–<lpage>13808</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5610-08.2009</pub-id></mixed-citation></ref>
<ref id="c147"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong</surname>, <given-names>P. C. M.</given-names></string-name>, <string-name><surname>Ettlinger</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sheppard</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Gunasekera</surname>, <given-names>G. M.</given-names></string-name>, &amp; <string-name><surname>Dhar</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Neuroanatomical Characteristics and Speech Perception in Noise in Older Adults</article-title>. <source>Ear &amp; Hearing</source>, <volume>31</volume>(<issue>4</issue>), <fpage>471</fpage>–<lpage>479</lpage>. <pub-id pub-id-type="doi">10.1097/aud.0b013e3181d709c2</pub-id></mixed-citation></ref>
<ref id="c148"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zahorik</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Wightman</surname>, <given-names>F. L</given-names></string-name></person-group>. (<year>2001</year>). <article-title>Loudness constancy with varying sound source distance</article-title>. <source>Nature Neuroscience</source>, <volume>4</volume>(<issue>1</issue>), <fpage>78</fpage>–<lpage>83</lpage>. <pub-id pub-id-type="doi">10.1038/82931</pub-id></mixed-citation></ref>
<ref id="c149"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zahorik</surname>, <given-names>Pavel</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Perceptually relevant parameters for virtual listening simulation of small room acoustics</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>126</volume>(<issue>2</issue>), <fpage>776</fpage>–<lpage>791</lpage>. <pub-id pub-id-type="doi">10.1121/1.3167842</pub-id></mixed-citation></ref>
<ref id="c150"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zahorik</surname>, <given-names>Pavel</given-names></string-name>, &amp; <string-name><surname>Brandewie</surname>, <given-names>E. J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Speech intelligibility in rooms: Effect of prior listening exposure interacts with room acoustics</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>140</volume>(<issue>1</issue>), <fpage>74</fpage>–<lpage>86</lpage>. <pub-id pub-id-type="doi">10.1121/1.4954723</pub-id></mixed-citation></ref>
<ref id="c151"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zekveld</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Heslenfeld</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Festen</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Schoonhoven</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Top-down and bottom-up processes in speech comprehension</article-title>. <source>NeuroImage</source>, <volume>32</volume>(<issue>4</issue>), <fpage>1826</fpage>–<lpage>1836</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.04.199</pub-id></mixed-citation></ref>
<ref id="c152"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zikopoulos</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Barbas</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Prefrontal projections to the thalamic reticular nucleus form a unique circuit for attentional mechanisms</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>26</volume>(<issue>28</issue>), <fpage>7348</fpage>–<lpage>7361</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5511-05.2006</pub-id></mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<title>Supplemental tables and figures</title>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Supplemental Table 1.</label>
<caption><title>Pairwise comparisons between performance (d’) for carrier phrases length n 22 participants.</title></caption>
<graphic xlink:href="644835v1_tbls1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Supplemental Table 2.</label>
<caption><title>Pairwise comparisons among performance (d’) for Interaction ‘target word’ x ‘carrier phrase length’ in 22 participants</title></caption>
<graphic xlink:href="644835v1_tbls2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbls3" orientation="portrait" position="float">
<label>Supplemental Table 3.</label>
<caption><title>Pairwise comparisons among performance (Final Hit Rate) for the six talkers in 22 participants.</title></caption>
<graphic xlink:href="644835v1_tbls3.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbls4" orientation="portrait" position="float">
<label>Supplemental Table 4.</label>
<caption><title>Pairwise comparisons between performance (Final Hit Rate) for carrier phrases length in 22 participants anechoic conditions.</title></caption>
<graphic xlink:href="644835v1_tbls4.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbls5" orientation="portrait" position="float">
<label>Supplemental Table 5.</label>
<caption><title>Pairwise comparisons for interaction ‘TMS condition’ x ‘carrier phrase length’.</title></caption>
<graphic xlink:href="644835v1_tbls5.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 1.</label>
<caption><title>Time courses of performance for 22 subjects in lecture theatre (green), open plan office (blue) and car park (yellow).</title>
<p>Solid lines represent actual cumulative hit rate data for each room that were calculated from raw data using a 5-point moving average (equivalent to ∼7s). Open circles represents best fit of double exponential with adjusted R2 values shown above for each individual. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25949/24295342.v1">https://doi.org/10.25949/24295342.v1</ext-link></p></caption>
<graphic xlink:href="644835v1_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 2.</label>
<caption><title>Comparison of Final Hit Rate (FHR) performances for different talkers.</title>
<p><bold>A.</bold> Performance, calculated as FHR in all rooms, is shown for the 6 different talkers (1-3 male and 4-6 female) presented to 22 individuals who did not undergo experimental TMS during the task involving the Lecture Room/Open-Plan Office and Car Park RIRs. <bold>B.</bold> Performance was also similarly calculated for the different talkers but is now separated by the room RIR associated with the specific talker. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25949/24295342.v1">https://doi.org/10.25949/24295342.v1</ext-link></p></caption>
<graphic xlink:href="644835v1_figs2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 3.</label>
<caption><title>Hit Rate performances for |Color| and |Number| independent of environment presented or carrier phrase length.</title>
<p><bold>A.</bold> Performance, calculated for the 1262 trials where 22 participants heard in 2 consecutive trials the same talker, a Wilcoxon signed rank test revealed no significant differences in performance between trial 1 vs. trial 2 (n=1262, Z= −0.42, p=0.68). <bold>B.</bold> Performance was also similarly calculated when listeners heard the same talker in three consecutive trials, this happened in a total of 217 trials and a Wilcoxon signed rank test revealed no significant differences in performance between trial 1 vs. trial 3 (n=217, Z= −0.17, p=0.87). <bold>C.</bold> Performance calculated in 40 trials were listeners heard the same talker in four consecutive trials. No statistical differences were observed between Trial 1 and Trial 4 (n=40, Z= −0.159, p=0.11).</p></caption>
<graphic xlink:href="644835v1_figs3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 4.</label>
<caption><title>Time courses of performance for 11 sham-TMS subjects in lecture theatre (green), open plan office (blue) and car park (yellow).</title>
<p>Solid lines represent actual cumulative hit rate data for each room that were calculated from raw data using a 5-point moving average (equivalent to ∼7s). Open circles represents best fit of double exponential with adjusted R2 values shown above for each individual. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25949/24295342.v1">https://doi.org/10.25949/24295342.v1</ext-link></p></caption>
<graphic xlink:href="644835v1_figs4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 5.</label>
<caption><title>Time courses of performance for 10 TMSin lecture theatre (green), open plan office (blue) and car park (yellow).</title>
<p>Solid lines represent actual cumulative hit rate data for each room that were calculated from raw data using a 5-point moving average (equivalent to ∼7s). Open circles represents best fit of double exponential with adjusted R2 values shown above for each individual. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25949/24295342.v1">https://doi.org/10.25949/24295342.v1</ext-link></p></caption>
<graphic xlink:href="644835v1_figs5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</app>
<app id="app2">
<title>Supplemental Information. Transcranial Magnetic Stimulation screening form</title>
<table-wrap id="utbl1" orientation="portrait" position="float">
<graphic xlink:href="644835v1_utbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107041.1.sa5</article-id>
<title-group>
<article-title>eLife Assessment:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Herrmann</surname>
<given-names>Björn</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Baycrest Hospital</institution>
</institution-wrap>
<city>Toronto</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study addresses <bold>valuable</bold> questions about the neural mechanisms underlying statistical learning of room acoustics, combining robust behavioral measures with non-invasive brain stimulation. The behavioral findings are strong and extend previous work in psychoacoustics, but the TMS results are modest, with methodological limitations and over-interpretation that weaken the mechanistic conclusions. The strength of evidence is therefore <bold>incomplete</bold>, and a more cautious interpretation of the stimulation findings, alongside strengthened analyses, would improve the manuscript.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107041.1.sa4</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript describes the results of an experiment that demonstrates a disruption in statistical learning of room acoustics when transcranial magnetic stimulation (TMS) is applied to the dorsolateral prefrontal cortex in human listeners. The work uses a testing paradigm designed by the Zahorik group that has shown improvement in speech understanding as a function of listening exposure time in a room, presumably through a mechanism of statistical learning. The manuscript is comprehensive and clear, with detailed figures that show key results. Overall, this work provides an explanation for the mechanisms that support such statistical learning of room acoustics and, therefore, represents a major advancement for the field.</p>
<p>Strengths:</p>
<p>The primary strength of the work is its simple and clear result, that the dorsolateral prefrontal cortex is involved in human room acoustic learning.</p>
<p>Weaknesses:</p>
<p>A potential weakness of this work is that the manuscript is quite lengthy and complex.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107041.1.sa3</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study investigated how listeners adapt to and utilize statistical properties of different acoustic spaces to improve speech perception. The researchers used repetitive TMS to perturb neural activity in DLPFC, inhibiting statistical learning compared to sham conditions. The authors also identified the most effective room types for the effective use of reverberations in speech in noise perception, with regular human-built environments bringing greater benefits than modified rooms with lower or higher reverberation times.</p>
<p>Strengths:</p>
<p>The introduction and discussion sections of the paper are very interesting and highlight the importance of the current study, particularly with regard to the use of ecologically valid stimuli in investigating statistical learning. However, they could be condensed into parts. TMS parameters and task conditions were well-considered and clearly explained.</p>
<p>Weaknesses</p>
<p>(1) The Results section is difficult to follow and includes a lot of detail, which could be removed. As such, it presents as confusing and speculative at times.</p>
<p>(2) The hypotheses for the study are not clearly stated.</p>
<p>(3) Multiple statistical models are implemented without correcting the alpha value. This leaves the analyses vulnerable to Type I errors.</p>
<p>(4) It is confusing to understand how many discrete experiments are included in the study as a whole, and how many participants are involved in each experiment.</p>
<p>(5) The TMS study is significantly underpowered and not robust. Sample size calculations need further explanation (effect sizes appear to be based on behavioural studies?). I would caution an exploratory presentation of these data, and calculate a posteriori the full sample size based on effect sizes observed in the TMS data.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107041.1.sa2</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript presents a well-designed and insightful behavioural study examining human adaptation to room acoustics, building on prior work by Brandewie &amp; Zahorik. The psychophysical results are convincing and add incremental but meaningful knowledge to our understanding of reverberation learning. However, I find the transcranial magnetic stimulation (TMS) component to be over-interpreted. The TMS protocol, while interesting, lacks sufficient anatomical specificity and mechanistic explanation to support the strong claims made regarding a unique role of the dorsolateral prefrontal cortex (dlPFC) in this learning process. More cautious interpretation is warranted, especially given the modest statistical effects, the fact that the main TMS result of interest is a null result, the imprecise targeting of dlPFC (which is not validated), and the lack of knowledge about the timescale of TMS effects in relation to the behavioural task. I recommend revising the manuscript to shift emphasis toward the stronger behavioural findings and to present a more measured and transparent discussion of the TMS results and their limitations.</p>
<p>Strengths:</p>
<p>(1) Well-designed acoustical stimuli and psychophysical task.</p>
<p>(2) Comparisons across room combinations are well conducted.</p>
<p>(3) The virtual acoustic environment is impressive and applied well here.</p>
<p>(4) A timely study with interesting behavioural results.</p>
<p>Weaknesses:</p>
<p>(1) Lack of hypotheses, particularly for TMS.</p>
<p>(2) Lack of evidence for targeting TMS in [brain] space and time.</p>
<p>(3) The most interesting effect of TMS is a null result compared to a weak statistical effect for &quot;meta adaptation&quot;</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107041.1.sa1</article-id>
<title-group>
<article-title>Reviewer #4 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Several behavioral experiments and one TMS experiment were performed to examine adaptation to room reverberation for speech intelligibility in noise. This is an important topic that has been extensively studied by several groups over the years. And the study is unique in that it examines one candidate brain area, dlPFC, potentially involved in this learning, and finds that disrupting this area by TMS results in a reduction in the learning. The behavioral conditions are in many ways similar to previous studies. However, they find results that do not match previous results (e.g., performance in anechoic condition is worse than in reverberation), making it difficult to assess the validity of the methods used. One unique aspect of the behavioral experiments is that Ambisonics was used to simulate the spaces, while headphone simulation was mostly used previously. The main behavioral experiment was performed by interleaving 3 different rooms and measuring speech intelligibility as a function of the number of words preceding the target in a given room on a given trial. The findings are that performance improves on the time scale of seconds (as the number of words preceding the target increases), but also on a much larger time scale of tens to hundreds of seconds (corresponding to multiple trials), while for some listeners it is degraded for the first couple of trials. The study also finds that the performance is best in the room that matches the T60 most commonly observed in everyday environments. These are potentially interesting results. However, there are issues with the design of the study and analysis methods that make it difficult to verify the conclusions based on the data.</p>
<p>Strengths:</p>
<p>(1) Analysis of the adaptation to reverberation on multiple time scales, for multiple reverberant and anechoic environments, and also considering contextual effects of one environment interleaved with the other two environments.</p>
<p>(2) TMS experiment showing reduction of some of the learning effects by temporarily disabling the dlPFC.</p>
<p>Weaknesses:</p>
<p>While the study examines the adaptation for different carrier lengths, it keeps multiple characteristics (mainly talker voice and location) fixed in addition to reverberation. Therefore, it is possible that the subjects adapt to other aspects of the stimuli, not just to reverberation. A condition in which only reverberation would switch for the target would allow the authors to separate these confounding alternatives. Now, the authors try to address the concerns by indirect evidence/analyses. However, the evidence provided does not appear sufficient.</p>
<p>The authors use terms that are either not defined or that seem to be defined incorrectly. The main issue then is the results, which are based on analysis of what the authors call d', Hit Rate, and Final Hit rate. First of all, they randomly switch between these measures. Second, it's not clear how they define them, given that their responses are either 4-alternative or 8-alternative forced choice. d', Hit Rate, and False Alarm Rate are defined in Signal detection theory for the detection of the presence of a target. It can be easily extended to a 2-alternative forced choice. But how does one define a Hit, and, in particular, a False Alarm, in a 4/8-alternative? The authors do not state how they did it, and without that, the computation of d' based on HR and FAR is dubious. Also, what the authors call Hit Rate, is presumably the percent correct performance (PCC), but even that is not clear. Then they use FHR and act as if this was the asymptotic value of their HR, even though in many conditions their learning has not ended, and randomly define a variable of +-10 from FHR, which must produce different results depending on whether the asymptote was reached or not. Other examples of usage of strange usage of terms: they talk about &quot;global likelihood learning&quot; (L426) without a definition or a reference, or about &quot;cumulative hit rate&quot; (L1738), where it is not clear to me what &quot;cumulative&quot; means there.</p>
<p>There are not enough acoustic details about the stimuli. The authors find that reverberant performance is overall better than anechoic in 2 rooms. This goes contrary to previous results. And the authors do not provide enough acoustic details to establish that this is not an artefact of how the stimuli were normalized (e.g., what were the total signal and noise levels at the two ears in the anechoic and reverberant conditions?).</p>
<p>There are some concerns about the use of statistics. For example, the authors perform two-way ANOVA (L724-728) in which one factor is room, but that factor does not have the same 3 levels across the two levels of the other factor. Also, in some comparisons, they randomly select 11 out of 22 subjects even though appropriate test correct for such imbalances without adding additional randomness of whether the 11 selected subjects happened to be the good or the bad ones.</p>
<p>Details of the experiments are not sufficiently described in the methods (L194-205) to be able to follow what was done. It should be stated that 1 main experiment was performed using 3 rooms, and that 3 follow-ups were done on a new set of subjects, each with the room swapped.</p>
</body>
</sub-article>
<sub-article id="sa5" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107041.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Hernández-Pérez</surname>
<given-names>Heivet</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9135-3973</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mikiel-Hunter</surname>
<given-names>Jason</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6085-9269</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Traer</surname>
<given-names>James</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0006-3262-9833</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Monaghan</surname>
<given-names>Jessica JM</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1416-4164</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Sowman</surname>
<given-names>Paul F</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>McAlpine</surname>
<given-names>David</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5467-6725</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We sincerely thank the reviewers for the time and care they have invested in evaluating our manuscript. We greatly appreciate their thoughtful feedback, which highlights both the strengths and the areas where the work can be improved. We recognize the importance of the concerns raised, particularly regarding the TMS analyses and interpretation, as well as aspects of the manuscript structure and clarity. The authors are committed to transparency and a rigorous scientific process, and we will therefore carefully consider all reviewer comments. In the coming months, we will revise the manuscript to incorporate additional analyses, provide clearer methodological detail, and refine the interpretation of the stimulation results.</p>
</body>
</sub-article>
</article>