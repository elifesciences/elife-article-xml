<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">86464</article-id>
<article-id pub-id-type="doi">10.7554/eLife.86464</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.86464.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Courtship behaviour reveals temporal regularity is a critical social cue in mouse communication</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4915-5113</contrib-id>
<name>
<surname>Perrodin</surname>
<given-names>Catherine</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5796-0924</contrib-id>
<name>
<surname>Verzat</surname>
<given-names>Colombine</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Bendor</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Institute of Behavioural Neuroscience, Department of Experimental Psychology, University College London</institution>, WC1H 0AP, <country>United Kingdom</country></aff>
<aff id="a2"><label>2</label><institution>Idiap Research Institute</institution>, CH-1920, Martigny, <country>Switzerland</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bathellier</surname>
<given-names>Brice</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Centre National de la Recherche Scientifique</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>For correspondence: <email>c.perrodin@ucl.ac.uk</email> (CP), <email>d.bendor@ucl.ac.uk</email> (DB)</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-04-18">
<day>18</day>
<month>04</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP86464</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-02-12">
<day>12</day>
<month>02</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2020-09-02">
<day>02</day>
<month>09</month>
<year>2020</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.01.28.922773"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Perrodin et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Perrodin et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-86464-v1.pdf"/>
<abstract>
<title>Summary</title>
<p>While animals navigating the real world face a barrage of sensory input, their brains evolved to perceptually compress multidimensional information by selectively extracting the features relevant for survival. Notably, communication signals supporting social interactions in several mammalian species consist of acoustically complex sequences of vocalizations. However, little is known about what information listeners extract from such time-varying sensory streams. Here, we utilize female mice’s natural behavioural response to male courtship songs to identify the relevant acoustic dimensions used in their social decisions. We found that females were highly sensitive to disruptions of song temporal regularity, and preferentially approached playbacks of intact over rhythmically irregular versions of male songs. In contrast, female behaviour was invariant to manipulations affecting the songs’ sequential organization, or the spectro-temporal structure of individual syllables. The results reveal temporal regularity as a key acoustic cue extracted by mammalian listeners from complex vocal sequences during goal-directed social behaviour.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>natural behaviour</kwd>
<kwd>neuroethology</kwd>
<kwd>acoustics</kwd>
<kwd>social</kwd>
<kwd>mice</kwd>
<kwd>sequence processing</kwd>
<kwd>perception</kwd>
<kwd>ultrasound</kwd>
<kwd>vocalizations</kwd>
<kwd>auditory</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>New analyses of temporal dynamics; inclusion of supplementary information; update of discussion section</p></fn>
</fn-group>
</notes>
</front>
<body>
<p>Animals behaving in the real world need to efficiently discriminate and monitor relevant sensory input, such as information communicated by conspecifics, from a constantly evolving, complex and multidimensional barrage of stimulation. In many social species, including humans, vocal communication takes the form of time-varying sequences of acoustic elements and demands a rapid, socially-appropriate behavioural response. Yet how the listener’s brain effectively extracts salient information from acoustically complexity vocal patterns during social goal-directed behaviour remains unclear. Here, we asked which acoustic cues from communication sequences were informative to female mouse listeners when responding to male courtship songs.</p>
<p>Mice, like most other mammals, use complex acoustic patterns to communicate with each other in a number of different social contexts<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup>. In particular, adult males produce rhythmic sequences of discrete, frequency-modulated pure tone elements in response to sensing the recent presence of a fertile female<sup><xref ref-type="bibr" rid="c3">3</xref>-<xref ref-type="bibr" rid="c5">5</xref></sup>. These courtship songs are attractive to sexually receptive females<sup><xref ref-type="bibr" rid="c6">6</xref></sup>, who respond to song playback with approach behaviour<sup><xref ref-type="bibr" rid="c7">7</xref>-<xref ref-type="bibr" rid="c12">12</xref></sup>. Male songs facilitate copulatory success<sup><xref ref-type="bibr" rid="c13">13</xref></sup> and are thought to serve as fitness displays<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c15">15</xref></sup>.</p>
<p>Female mice are able to detect and use acoustic information in male vocalizations. On the one hand, reinforcement learning experiments have shown that mouse listeners are able to detect and report spectro-temporal differences between individual ultrasonic call elements<sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>, indicating that mice are <italic>perceptually</italic> sensitive to a wide range of acoustic features in the vocalizations. On the other hand, experiments using ethologically-relevant place preference paradigms without external reward have shown that females use, or are <italic>behaviourally</italic> sensitive to, certain acoustic characteristics in male songs to guide social decisions, such as discriminating between social contexts<sup><xref ref-type="bibr" rid="c9">9</xref></sup>, singer species<sup><xref ref-type="bibr" rid="c10">10</xref></sup>, strain<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref></sup> and kin<sup><xref ref-type="bibr" rid="c20">20</xref></sup>. However, which of the many acoustic cues available in male courtship songs are used by motivated female listeners during natural behaviour remains unclear.</p>
<p>In this study, we asked how listeners process and use complex vocal sequences during natural behaviour. We exploited female mice’s natural behavioural response to playbacks of male courtship songs in order to independently manipulate and identify which of several candidate acoustic features are monitored by the listeners. We found that female approach behaviour was highly sensitive to disruptions of song temporal regularity, but that it was not affected by global manipulations of the songs’ sequential structure, or the local removal of syllable spectro-temporal dynamics. The results highlight temporal regularity as a key social cue monitored by female listeners during goal-directed social behaviour.</p>
<sec id="s1">
<title>Results</title>
<sec id="s1a">
<title>Acoustic characteristics of vocal sequences emitted by male C57Bl/6 mice</title>
<p>In order to study female listeners’ behavioural response to vocal sequences, we first collected a database of male C57Bl/6 mice vocalizations for acoustic stimulation in playback experiments. Following a previously published protocol<sup><xref ref-type="bibr" rid="c5">5</xref></sup>, we obtained audio recordings of ultrasonic vocalizations emitted by males in response to the presentation of mixtures of urine samples collected from conspecific females in estrus. This paradigm was deemed optimal for generating stimuli to use in ethologically-inspired playback experiments, as vocalizations emitted in this context are emitted by solitary males and reflect long-range acoustic “courtship” displays with the purpose of attracting a female<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>. The vocal patterns emitted in this context consisted of sequences of individual frequency-modulated calls (see example <xref rid="fig1" ref-type="fig">Fig. 1A</xref>, <xref ref-type="fig" rid="fig3">3A</xref>, S1A and <sup><xref rid="c3" ref-type="bibr">3</xref>,<xref rid="c9" ref-type="bibr">9</xref>,<xref rid="c23" ref-type="bibr">23</xref>-<xref rid="c25" ref-type="bibr">25</xref></sup>). Across the set of recorded male vocalizations, the duration of continuous call elements, or syllables, followed a bimodal distribution, with a majority of short syllables (local maximum = 23ms, 86% (2039/2373) of recorded syllables with durations shorter than 63ms (local minimum)) and a minority of long syllables (local maximum = 88 ms, 14% (334/2373) of recorded syllables with durations longer than 63ms, <xref rid="fig1" ref-type="fig">Fig. 1B</xref>). These male calls were restricted to the ultrasonic frequency range, with the maximal energy in individual syllables occurring around 76kHz (median peak frequency and 95% confidence interval (C.I.), 76 ± 0.3 kHz, n = 2373 syllables) and spanning a bandwidth of 16 kHz ([68-84 kHz]; <xref rid="fig1" ref-type="fig">Fig. 1C</xref>). Individual syllables were organized in temporally regular sequences and often started at an approximately constant time delay from each other<sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>, as evidenced by the narrow concentration of inter-syllable interval durations around 104 ± 2 milliseconds (median inter-syllable interval and 95% C.I.; interquartile range of syllable interval = 61ms, n = 2322 syllables followed by another syllable within 2 seconds; <xref rid="fig1" ref-type="fig">Fig. 1D</xref>). From this set of ultrasonic vocalizations, we selected a smaller, representative sample for use in subsequent playback experiments (n = 957 syllables, dark grey histograms in <xref rid="fig1" ref-type="fig">Fig. 1B-D</xref>). This stimulus set was composed of 7 distinct songs produced by C57Bl/6 male mice aged 22 weeks on average (median age, range [10-27 weeks]) in response to the presentation of female urine. The songs varied in duration, lasting on average 33.4s (median duration, range [13-41s]), and were composed of an average of 145 syllables (median, range [95-186]).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Acoustic features of C57Bl/6 mouse courtship songs</title>
<p>A. Spectrogram of a segment of ultrasonic male vocalizations used for stimulation</p>
<p>B. Distribution of individual ultrasonic syllable durations across a large set of male mouse vocalizations (n = 2373 syllables, light grey histogram; red line is smoothed distribution), including the stimulus set used in subsequent playback experiments (n = 957 syllables, dark grey), emitted in response to the presentation of urine samples from females in estrus.</p>
<p>C. Distribution of syllable peak frequency (point of maximum amplitude across the call element in kHz) across all recorded syllables (n = 2373, light grey), and the subset of syllables used for playback (n = 957, dark grey).</p>
<p>D. Distribution of inter-syllable interval durations. The analysis was restricted to syllables with a subsequent syllable starting within 2 seconds (all recorded syllables: n = 2322, light grey; playback stimulus set: n = 942, dark grey).</p></caption>
<graphic xlink:href="922773v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s1b">
<title>Female mice preferentially approach playbacks of male songs over silence</title>
<p>Using our pre-recorded subset of male vocal sequences for stimulation, we then aimed to confirm that female mice displayed a preferential approach response to male songs in our ethologically-inspired behavioural paradigm. We used a place preference assay to evaluate female listeners’ behaviour in response to the playback of our pre-recorded song set in a two-compartment behavioural chamber (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). Importantly, olfactory stimulation from mixed male bedding samples placed under each of the loudspeakers was present throughout the testing session, in order to increase sexual arousal and motivation in the female listeners<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup> and create a multisensory coherent approximation of a natural social situation. Following a silent habituation period during which the animal was free to explore the behavioural box (at least 10min; grey traces in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>, C), one-sided playback of male vocalizations was initiated while the second speaker remained inactive (red traces in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>, C). The pre-recorded set of 7 songs was repeated 4 times during one behavioural session, with the order of individual songs randomized in each trial, and the playback side alternating between trials (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, C). A female mouse’s approach response to song playback was measured by quantifying the time the animal spent within a “speaker zone” over the course of a behavioural session (white (grey) trapezoidal outlines in <xref rid="fig2" ref-type="fig">Fig. 2A</xref> (C), respectively). Young female C57Bl/6 mice (aged 5-11 weeks; Fig. S3) participated once in the playback experiment, while in the fertile (proestrus or estrus) stage of their estrous cycle (as assessed by vaginal cytology, see Methods).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Female mice preferentially approach playbacks of male songs over silence</title>
<p>A. Video frame showing the testing box with a soundproof partition (middle) positioned between two ultrasonic loudspeakers. White outlines show the two “speaker zones” used as regions of interest for quantifying the animal’s position.</p>
<p>B. Experimental timeline</p>
<p>C. Tracking of the animal’s position during an example behavioural session, in which a silent baseline period (leftmost panel) is followed by the playback of intact songs from one side contrasted with silence from the other side (coloured panels). Colour saturation indicates time since start of the experimental period of interest. Dark grey outlines indicate the speaker zones.</p>
<p>D. Time the animal spent in the speaker zones during the silent baseline period in C.</p>
<p>E. Index quantifying the animal’s relative place preference to the left vs right speaker zones during the silent baseline. This index was computed as the difference between the time spent in either speaker zones, normalized by the sum of the time in both speaker zones (see Methods). A preference index value close to zero indicates no preference to either side.</p><p>F. Time spent in the speaker zones corresponding to song playback and silence during each song presentation trial. Error bars: standard error of the mean.</p>
<p>G. Preference index in response to the playback of intact songs compared to silence over the course of the example session. The preference index is the difference between the time in the song playback and the silent speaker zones, divided by the sum of the time in both speaker zones. A positive index value reflects the animal’s preferential approach to the sources of song playback over silence.</p>
<p>H. Population summary of female approach response to playback of intact male songs (positive values) over silence (negative). Each circle is the preference index displayed by individual animals in one behavioural session (median of 4 sound presentation trials). The red circle corresponds to the example session in C &amp; G. Open circles: sound playback at 58 dB SPL, filled circles: 68 dB SPL. Bar plot shows mean preference index across sessions and 95% confidence interval (C.I.). One-sample two-tailed <italic>t</italic>-test, **: p &lt; 0.01.</p>
<p>I. Temporal profile of approach behaviour over the four sound presentation trials in the example session in C, calculated as the cumulative sum of time in the intact song playback (positively weighted) vs silent (negatively weighted) speaker zone.</p>
<p>J. Trial-averaged profile of approach behaviour to song playback in the example session, calculated as in I.</p>
<p>K. Population-averaged approach behaviour time course in response to intact mouse songs vs silence, calculated as in J. The dark grey trace indicates the mean of trial-based temporal profiles across all sessions (n = 29). For each session, the median of 4 sound presentation trials (e.g., black trace in J) was normalized to its maximal amplitude. The horizontal black bar indicates time bins during the course of a sound playback trial in which the cumulative approach behaviour significantly deviates from zero (one-sample two-tailed <italic>t</italic>-test).</p>
<p>L. Normalized temporal profiles of approach behaviour to mouse songs vs silence over the course of 4 sound presentation trials (x-axis, coloured bars) for each of the behavioural sessions (y-axis, n = 29), calculated as in I. Sessions (lines) are ordered by the amplitude of their last element.</p>
<p>F, G, I, J: Black traces indicate the session average (median) across the four sound presentation trials (coloured traces)</p></caption>
<graphic xlink:href="922773v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In the example session illustrated in <xref rid="fig2" ref-type="fig">Fig. 2</xref>, the mouse occupied both speaker zones equally during the silent baseline (<xref rid="fig1" ref-type="fig">Fig. 1D</xref>) and the first sound playback trial (red trace in <xref rid="fig2" ref-type="fig">Fig. 2F</xref>, I). Over the course of the 3 subsequent trials, the mouse displayed a consistent preferential approach towards the source of male song playback over the silent speaker, irrespective of the playback side (blue, purple and green traces, <xref rid="fig2" ref-type="fig">Fig. 2F</xref>, I and video files S2A, B). An animal’s approach behaviour over the course of an entire session can be quantified using a preference index (see Methods and <xref rid="fig2" ref-type="fig">Fig. 2E</xref>, G), that accurately captures the cumulative preferential dwell time at the song playback vs silent speaker (see correspondence between preference index values in <xref rid="fig2" ref-type="fig">Fig. 2G</xref> and endpoints of trial temporal profiles in <xref rid="fig2" ref-type="fig">Fig. 2J</xref>). This preference index metric was also shown by a number of previous studies to be the most sensitive readout for similar assays of female approach to male song playback<sup><xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup>. In this example, the mouse preferentially approached the song playback over silence in ¾ trials, resulting in a preference index value of 25.3% (median of 4 trials; black dot in <xref rid="fig2" ref-type="fig">Fig. 2G</xref>). The behaviour of female listeners tested in this paradigm significantly discriminated between song playback and silence (n = 29 sessions, mean preference index significantly different from 0, one-sample <italic>t</italic>-test, t(28) = 2.81, p = 0.0088; <xref rid="fig2" ref-type="fig">Fig. 2H</xref>), and mouse listeners were overwhelmingly attracted to the playback of our subset of male songs. The preference for song over silence was robust to different lengths of the speaker zone used to calculate the preference index (Fig. S4A, B). Mouse listeners demonstrated significant approach response after as little as 16.9 seconds of sound playback across the 4 sound presentation trials (normalized cumulative time in speaker zone significantly different from 0, one-sample t-test, p&lt;0.05; <xref rid="fig2" ref-type="fig">Fig. 2K</xref>), with preference further increasing over the course of song playback. The build-up of preferential approach to song was evident in the vast majority of animals, mostly sustained over a behavioural session (<xref rid="fig2" ref-type="fig">Fig. 2L</xref>), and similar across the 4 sound presentation trials (Fig. S4B). Together, these results replicate previous work by other laboratories<sup><xref ref-type="bibr" rid="c7">7</xref>-<xref ref-type="bibr" rid="c11">11</xref></sup> and confirm the feasibility of recreating an ethologically-driven behavioural assay of female approach response to male song playback in the controlled laboratory environment.</p>
</sec>
<sec id="s1c">
<title>Female approach behaviour is not affected by changes to global song structure</title>
<p>Given this proof of principle that socially-motivated female listeners approach the source of male song playback, we then extended this natural behavioural paradigm to directly evaluate how listeners perceive and use acoustic features from vocal sequences. Specifically, we tested the hypothesis that, during naturalistic goal-directed behaviour in a social context, female listeners perceptually compress the high sensory dimensionality of male songs by selectively monitoring a reduced subset of meaningful acoustic features in isolation. We quantified females’ place preference in the same behavioural assay in response to the near-simultaneous playback of the pre-recorded set of male songs from one speaker, and an acoustically modified version of the songs from the second speaker. Given that females preferentially approach the speaker playing the male song, differences in the relative time spent at the source of intact vs manipulated sound playback are interpreted to demonstrate the behavioural relevance of the manipulated acoustic dimension. In contrast, equal approach behaviour to both intact and manipulated song playback is interpreted as females not perceiving, or not relying on, the tested acoustic dimension in their natural behavioural response to courtship song.</p>
<p>In the first instance we evaluated whether listeners would be sensitive to two types of global manipulations affecting the song structure at longer timescales. Building on previous work suggesting that female mice use syntactic information to discriminate songs produced in different social contexts<sup><xref ref-type="bibr" rid="c9">9</xref></sup>, we tested whether randomizing the order of the syllables in the song would affect their approach behaviour (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, left and S1B). We found that the sequential organization of syllables in the male songs was not necessary for normal approach behaviour (mean preference index in response to intact vs randomized songs did not differ from zero, one-sample <italic>t-</italic>test, t(20) = 0.126, p = 0.90; <xref rid="fig3" ref-type="fig">Fig. 3A</xref>, right). This behavioural invariance to changes in the syllable order was apparent at all timepoints of song set playback (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>), as well as at different speaker zone lengths (Fig. S4C), and was comparable across the 4 sound presentation trials (Fig. S5D). This confirms that the lack of sensitivity to syllable sequence structure we observe here was not caused by the selection of specific temporal or spatial parameters for analysis.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Female approach behaviour is not affected by changes to global song structure or the removal of syllable spectro-temporal dynamics</title>
<p>A. Female approach behaviour during simultaneous playback of intact male songs (top) and corresponding randomized syllable sequences (bottom,), displayed as in <xref rid="fig2" ref-type="fig">Fig. 2H</xref>. Each circle is the preference index displayed by individual animals in one behavioural session (median of 4 sound presentation trials). Open circles indicate sound playback at 58 dB SPL, filled circles at 68 dB SPL. Bar plot shows mean preference index across sessions, and error bar show 95% confidence interval (C.I.). One-sample two-tailed <italic>t</italic>-test, ns: non-significant, p &gt; 0.05.</p>
<p>B. Population timecourse of approach behaviour during the playback of intact (positively weighted) vs randomized songs (negatively weighted), displayed as in <xref rid="fig2" ref-type="fig">Fig. 2K</xref>. The dark grey trace indicates the population mean of trial-based temporal profiles across all sessions (n = 21 sessions). For each session, the median of 4 sound presentation trials (e.g., black trace in J,) was normalized to the absolute value of its maximal amplitude. At no time bins during the course of a sound playback trial did the cumulative approach behaviour significantly deviate from zero (one-sample two-tailed <italic>t</italic>-test, all p&gt;0.05).</p>
<p>C Playback of intact songs (top) contrasted with temporally reversed songs (bottom)</p>
<p>D. Typical timecourse of approach behaviour during the playback of intact (positively weighted) vs reversed songs (negatively weighted).</p>
<p>E. Playback of intact songs (top) and sequences of phase-scrambled syllables (bottom).</p>
<p>F. Typical timecourse of approach behaviour during the playback of intact (positively weighted) vs phase-scrambled syllable sequences (negatively weighted).</p>
<p>G. Playback of intact songs (top) contrasted with sequences of pure tones (bottom).</p>
<p>H. Typical timecourse of approach behaviour during the playback of intact (positively weighted) vs pure tone sequences (negatively weighted).</p></caption>
<graphic xlink:href="922773v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Then, we contrasted intact songs with a time-reversed version of the songs. While preserving the long-term spectra and range of spectral changes, this manipulation generated sounds with novel temporal features that different subsets of auditory neurons are tuned to, such as reversing the direction of frequency trajectories<sup><xref ref-type="bibr" rid="c26">26</xref>-<xref ref-type="bibr" rid="c32">32</xref></sup> (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>, left and S1C). Temporal reversal of the vocal sequence did not significantly affect females’ approach behaviour (mean preference index in response to intact vs reversed songs did not differ from zero, one-sample <italic>t-</italic>test, t(20) = −1.46, p = 0.16; <xref rid="fig3" ref-type="fig">Fig. 3C</xref>, right). This lack of behavioural sensitivity to song temporal reversal was present throughout sound playback in the vast majority of animals tested (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>), across the sound presentation trials (Fig. S5E), and for different spatial regions of interest (Fig. S4F). Together, these observations suggest that female listeners are not relying on global cues reflecting the overall structure of male song sequences when selecting which sound source to approach.</p>
</sec>
<sec id="s1d">
<title>Female approach behaviour is robust to the removal of syllable spectro-temporal dynamics</title>
<p>Since auditory neurons are typically tuned to specific spectro-temporal features in the song syllables, and mice are able to use those features to discriminate between individual syllables<sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>, we next wondered whether manipulations disrupting the fine acoustic structure of individual calls in the male vocal sequences would be more easily discriminated by female listeners. We first created a phase-scrambled version of the songs that preserved the average frequency spectrum of the syllables, but removed the spectro-temporal dynamics of individual syllables and shaped each noise burst with a flat envelope (<xref rid="fig3" ref-type="fig">Fig. 3E</xref>, left and S1D). Females approached the intact and phase-scrambled song playback similarly (mean preference index in response to intact vs phase-scrambled songs did not differ from zero, one-sample <italic>t-</italic>test, t(22) = −0.27, p = 0.79; <xref rid="fig3" ref-type="fig">Fig. 3E</xref>, right). Second, comparing intact songs with more drastically simplified sound sequences that stripped down each syllable into a pure tone at the peak syllable frequency (76kHz) also failed to elicit differential approach behaviour (mean preference index in response to intact vs pure tone sequences did not differ from zero, one-sample <italic>t-</italic>test, t(20) = 0.034, p = 0.97; <xref rid="fig3" ref-type="fig">Fig. 3G</xref> and S1E). These two instances of behavioural insensitivity to changes in the syllable acoustic trajectories were robust to variations in temporal (<xref rid="fig3" ref-type="fig">Fig. 3F</xref>, H and Fig. S5F, G) and spatial (Fig. S4D, G) parameters used for analysis. Together, these findings show that female behaviour was robust to the removal of fast spectro-temporal structure in the syllables, at least when sound energy was still present at the natural syllable peak frequency.</p>
</sec>
<sec id="s1e">
<title>Female approach behaviour is reduced by the disruption of song temporal regularity</title>
<p>Lastly, following the earlier observation that syllable onsets occur at consistent time intervals from each other across the song sequences (see <sup><xref ref-type="bibr" rid="c25">25</xref></sup> and <xref rid="fig1" ref-type="fig">Fig. 1D</xref>), we directly probed whether female listeners would be sensitive to manipulations of the temporal regularity in the male song. To do this we generated a set of temporally irregular songs that consisted of the same syllable sequence with a broadened distribution of silent pauses compared to intact songs (interquartile range of syllable interval in irregular songs = 134ms, compared to 61ms for intact songs; <xref rid="fig4" ref-type="fig">Fig. 4A</xref>, B, E left panel and S1F). Interestingly, females were highly sensitive to the disruption of song temporal regularity, and preferentially approached intact over irregular song playback (mean preference index in response to intact vs irregular songs significantly differed from zero, one-sample <italic>t-</italic>test, t(23) = 3.43, p = 0.00023, <xref rid="fig4" ref-type="fig">Fig. 4E</xref>, right). The sensitivity to temporal regularity was robust and similarly strong across all sizes of speaker zones (Fig. S4E). The temporal profile of approach behaviour of a majority of animals favoured intact, regular songs over the course of a session (<xref rid="fig4" ref-type="fig">Fig. 4G</xref>), and across sound presentation trials (S5A). On average, mouse listeners started showed this sustained preference for regular over irregular songs after 85.4 seconds of sound playback (normalized cumulative time in speaker zone significantly different from 0, one-sample t-test, p&lt;0.05; <xref rid="fig4" ref-type="fig">Fig. 4F</xref>). This longer latency to behavioural discrimination after song onset compared to when approaching intact song playback over silence (16.9 seconds, <xref rid="fig3" ref-type="fig">Fig. 3B</xref>) might reflect the need for additional accumulation of evidence and temporal integration when discriminating between the two concurrent sound streams. These striking results in response to disruptions of the song temporal regularity contrast with the lack of noticeable effect of other manipulations of the song and syllable structure on female approach behaviour.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Female approach behaviour is sensitive to disruption of courtship temporal regularity.</title>
<p>A. Distribution of inter-syllable interval (ISI) durations across the set of temporally irregular songs, calculated as in <xref rid="fig1" ref-type="fig">Fig. 1D</xref>.</p>
<p>B. Sequential relationships of ISI durations in the intact (shaded grey dots) and temporally irregular (shaded blue dots).</p>
<p>C. Distribution of ISI durations across the super-regular song set.</p>
<p>D. Sequential relationship of ISI durations in the intact (shaded grey dots) and super-regular (shaded red dots).</p>
<p>A-D: n = 957 syllables</p>
<p>E. Female approach behaviour during simultaneous playback of intact male songs (top) and temporally irregular songs (bottom), displayed as in <xref rid="fig2" ref-type="fig">Fig. 2H</xref>. Each circle indicates the preference index displayed by individual animals in one behavioural session (median of 4 sound presentation trials). Open circles: sound playback at 58 dB SPL, filled circles: 68 dB SPL. One-sample two-tailed <italic>t</italic>-test. **: p &lt; 0.01, ns: non significant, p &gt; 0.05. Bar plots show means, and error bars show 95% C.I.</p>
<p>F. Population-averaged time course of approach behaviour in response to intact (positively weighted) vs temporally irregular (negatively weighted) mouse songs, displayed as in <xref rid="fig2" ref-type="fig">Fig. 2K</xref>. The dark grey trace indicates the mean of normalized, trial-based temporal profiles across all sessions (n = 24). The black bar indicates time bins in which the cumulative approach behaviour significantly deviates from zero (one-sample two-tailed <italic>t</italic>-test).</p>
<p>G. Temporal profiles of approach behaviour to intact vs temporally irregular songs over the course of 4 sound presentation trials (x-axis, coloured bars) for each of the behavioural sessions (y-axis. n = 24), displayed as in <xref rid="fig2" ref-type="fig">Fig. 2L</xref>.</p>
<p>H. Female approach behaviour during simultaneous playback of intact songs (top) and temporally super-regular songs (bottom), displayed as in panel E.</p>
<p>I. Population time course of approach behaviour to intact (positively weighted) vs temporally super-regular (negatively weighted) mouse songs, displayed as in panel F.</p>
<p>J. Temporal profiles of approach behaviour to intact vs temporally super-regular songs over the course of 4 sound presentation trials, displayed as in panel G.</p></caption>
<graphic xlink:href="922773v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Finally, we asked whether female mice, being obviously attracted by stronger rhythmic regularity in the songs, would preferentially approach an artificial set of “super-regular” songs over intact ones. To this end we created a set of highly rhythmic songs in which the silent breaks were modified such that the onset of the syllables in each song occurred exactly at the dominant inter-syllable interval, effectively narrowing the distribution of inter-syllable times (interquartile range of syllable interval in super-regular songs = 24ms, compared to 61ms for intact songs; <xref rid="fig4" ref-type="fig">Fig. 4C</xref>, D, H left panel and S1G). Female listeners did not show any consistent behavioural preference when the playback of intact songs was contrasted with that of super-regular songs (one-sample <italic>t</italic>-test, t(21) = 1.48, p = 0.15; <xref rid="fig4" ref-type="fig">Fig. 4H</xref>, right). This was reflected in the time course of approach behaviour, which showed large variability across animals over a session with no clear emerging pattern (<xref rid="fig4" ref-type="fig">Fig. 4J</xref>). All 4 sound presentation trials elicited comparable response profiles (Fig. S5C). This behavioural invariance to improvements of the songs’ temporal regularity remained when tested at different timepoints during song playback (<xref rid="fig4" ref-type="fig">Fig. 4I</xref>), or using different spatial parameters for analysis (Fig. S4H). Together, these results suggest that the rhythmicity of natural male songs is already sufficiently salient to reach the criterion for approach behaviour by female listeners, and cannot be improved by additional regularisation of syllable onsets.</p>
</sec>
</sec>
<sec id="s2">
<title>Discussion</title>
<p>In this study, we developed an ethological assay to probe the behavioural relevance of several candidate acoustic features in mouse communication. By exploiting female mice’s natural approach response to male-produced courtship songs and using competing playbacks of intact and manipulated songs, we directly tested how listeners use acoustic information from vocal sequences. The results show that female mice monitor temporal regularity in male songs, while their behaviour was unaffected by changes in the song sequential organization, or the fine acoustic structure of individual syllables. The findings highlight the selective extraction of song temporal regularity, which may be an acoustic signature of singer fitness, as an effective, potentially evolutionarily conserved behavioural strategy for the sensory compression of complex vocal sequences during goal-directed social behaviour.</p>
<p>On the one hand, neurons across the mouse auditory system are tuned to different spectro-temporal features in song syllables, and mouse listeners have been shown to be perceptually sensitive to, or able to detect, the acoustic manipulations of individual syllables used in the current study. The upper limit of this perceptual ability has been determined using go/no-go operant conditioning to extensively train animals to discriminate between syllables from spectro-temporally distinct categories, between intact and temporally reversed or pure tone versions<sup><xref ref-type="bibr" rid="c16">16</xref></sup>, or between partial from whole syllables<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. However, such externally rewarded reinforcement learning and the associated sharpening of sensory acuity and neuronal tuning likely bias and amplify the perceptual ability of a naive animal<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>. On the other hand, social behaviour in naturalistic settings requires the selective monitoring of specific informative sensory features, while cognitively suppressing or ignoring other dimensions that may however be perceptible, as classically demonstrated in the toad’s visual system during prey capture<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. Our work complements previous studies that quantified the upper bounds of mouse hearing sensitivity. We show that, in the context of an ethologically-valid behaviour, female listeners show behavioural invariance to several acoustic features that they are, or can be trained to become, perceptually sensitive to.</p>
<p>We found that females’ approach to male song playback was not consistently affected by changes to the global structure of the vocal sequence, such as the temporal reversal of the song and the randomization of the syllable order in the song. Both of these manipulations preserve the relative spectro-temporal relationships within each syllable, as well as the physical complexity and acoustic characteristics of the sequence, and tested whether the order of the syllables was informative to the listener. Indeed, songs from male mice have been shown to demonstrate some characteristic sequential features, with short syllables typically dominating the beginning of a vocal phrase<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. This structure would be violated both by the syllable order randomization, and the temporal reversal of the song. While mouse listeners are known to perceptually discriminate between syllable types differing in their spectro-temporal features<sup><xref ref-type="bibr" rid="c16">16</xref></sup>, our finding that female approach behaviour was not affected by a shuffling of the syllable sequence indicates that, during courtship behaviour, females monitor the presence/absence or relative occurrence of specific syllable types, or other acoustic features of the songs, independent of the syllable order. Our result also disambiguates previous findings on mouse sensitivity to syntactic information in male songs: Chabout and colleagues<sup><xref ref-type="bibr" rid="c9">9</xref></sup> previously showed, in a similar behavioural paradigm, that female listeners discriminated between syntactically simple and complex songs produced by males in different social contexts. Songs produced in the two tested social contexts differed both in the first-order syllable sequencing and in the composition of the syllable repertoire. Our finding on females’ behavioural insensitivity to syllable order thus suggests that listeners may have been primarily exploiting differences in repertoire composition to discriminate and ultimately select songs produced from a specific social context. Thus our results show that female listeners do not strongly rely on the global structure of the syllable sequence during courtship behaviour.</p>
<p>When manipulating the songs at the more local level of individual syllables, we found that female listeners approached sequences of noise bursts or pure tones at the syllable peak frequency similarly to intact male songs. Our finding on female mice’s behavioural invariance to replacing syllables by pure tone sequences replicates previous work that showed female mice approaching playbacks of synthetic 70 kHz ultrasounds over silence, when these were presented from behind one of two devocalized males<sup><xref ref-type="bibr" rid="c6">6</xref></sup>. In contrast, Hammerschmidt and colleagues<sup><xref ref-type="bibr" rid="c11">11</xref></sup> used a place preference paradigm comparable to the one used in this study, and an artificial sequence of irregularly timed short ultrasounds that matched neither the duration or syllable rate of mouse songs. In this case, the authors showed that female mice preferentially approached male songs over pure tones. Our results resolve the apparent contradiction between these previous studies: by showing both females’ behavioural invariance to pure tone approximations of syllables and their high sensitivity to temporal regularity, our work suggests that the preference for intact over ultrasound sequence observed in the Hammerschmidt study is driven by the co-occurring disruption in sequence temporal regularity. Thus, the evidence suggests that sound energy in and around a critical band corresponding to the syllable frequency range, displayed at the natural temporal organization of male songs, is a sufficient approximation of male courtship songs to drive female approach behaviour. Our and others’ results on female courtship behaviour complement the body of work on maternal behaviour by lactating mothers: as long as the temporal structure of pup calls is maintained, pup retrieval behaviour is robust to the removal of most of the spectral structure of ultrasonic pup isolation calls<sup><xref ref-type="bibr" rid="c38">38</xref></sup>, and three-harmonic stack approximations of sonic wriggling calls elicit normal maternal responses<sup><xref ref-type="bibr" rid="c39">39</xref></sup>. Similar importance of temporal patterns for the perception of social communication, over the fine acoustic structure of individual elements, has been demonstrated behaviourally in other animal species, including humans: as illustrated in vocoded speech, the intelligibility of speech degraded in the spectral domain remains partially preserved if it is shaped with the correct amplitude envelopes<sup><xref ref-type="bibr" rid="c40">40</xref>,<xref ref-type="bibr" rid="c41">41</xref></sup>.</p>
<p>Of the subset of acoustic features tested in this study, temporal regularity in the song was the only feature we observed to be extracted by female listeners from male acoustic courtship displays. Our other tested contrasts show that several other perceptually dramatic disruptions of song features were nevertheless not impacting on the female approach behaviour, as long as the temporal patterning of the songs was preserved, suggesting a high degree of saliency for temporal regularity as a socially informative cue. We found that mouse listeners preferentially approached intact over irregular courtship songs, suggesting that regularity is attractive to females in the context of vocal-based mate selection. The temporal regularity observed in mouse songs is a consequence of breathing patterns regulating the production of syllables by the emitter, as individual calls are generated following the onset of exhalation<sup><xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c42">42</xref></sup>. Disruption to the temporal regularity of male song, as artificially introduced in this study, may for instance result from irregular breathing cycles and thus “stuttering” singing by the male, or by the inability to sustain bouts of vocal production of sufficient duration to elicit a salient percept of regularity in the listener. A recent study of courtship songs by Foxp2 knockout mice provided direct evidence for a link between male emitter fitness and song temporal regularity, by showing songs by mutant mice contain rhythmic irregularities<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. From the perspective of the listener, temporal predictability has been shown to facilitate auditory detection of near-threshold stimuli, compared to an identical aperiodic sequence<sup><xref ref-type="bibr" rid="c43">43</xref></sup>. Additionally, in human listeners, speech intelligibility is disrupted by temporal jittering the sentences<sup><xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>. Thus, perhaps by improving the perceptual signal-to-noise ratio of male courtship songs, temporal regularity may represent an acoustic cue signalling the fitness or suitability of the potential mating partner to the listener<sup><xref ref-type="bibr" rid="c14">14</xref></sup>, to which we show that socially motivated listeners are highly attuned to.</p>
<p>Female behaviour did not appear to discriminate between intact songs and an artificial “super-regular” version of the songs, suggesting that natural mouse songs might already at ceiling in terms of the perceptual saliency of their temporal regularity and therefore, their attractiveness to female listeners. In the somatosensory system, neuronal responses in barrel cortex responded indistinguishably to lightly temporally jittered and perfectly regular sequences of whisker deflections, when the stimulus presentation rate was slower than 20Hz<sup><xref ref-type="bibr" rid="c46">46</xref></sup>. In a hypothesis that remains to be directly tested in the auditory system, it is thus possible that, at the natural syllable rates of mouse songs, removing the low temporal jitter present in intact songs to create super-regular songs does not significantly impact on perceptually relevant neuronal substrates.</p>
<p>The syllable rate in mouse songs (∼ 7Hz<sup><xref ref-type="bibr" rid="c3">3</xref></sup>) broadly matches with that of other mammalian vocal patterns such as monkey vocalizations and human speech<sup><xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup>. On the one hand, this stimulus presentation rate in mouse songs is directly determined by the breathing rhythm of the singer. Given that active sensation during whisking and sniffing operates within the frequency range of both respiratory-coupled oscillations<sup><xref ref-type="bibr" rid="c49">49</xref></sup> and the hippocampal theta rhythm in rodents<sup><xref ref-type="bibr" rid="c50">50</xref>-<xref ref-type="bibr" rid="c53">53</xref></sup>, similar constraints may be placed on the cortical mechanisms of hearing<sup><xref ref-type="bibr" rid="c54">54</xref></sup>, especially during natural behaviour. This intriguing hypothesis could be addressed in future work by simultaneously tracking the listener’s hippocampal theta oscillations and sniffing behaviour during song playback. On the other hand, the syllable rate in mouse songs generally corresponds to the timescale of slow amplitude envelope in human speech, which is known to contribute to the identification of both prosodic and syllabic content<sup><xref ref-type="bibr" rid="c55">55</xref></sup>. While vocal sequences such as human speech and mouse songs are quasi-periodic rather than metronomically regular signals, they both contain sufficiently salient temporal regularity to allow listeners to make predictions about the incoming signal<sup><xref ref-type="bibr" rid="c55">55</xref></sup>. In particular, the comparable range of temporal regularities found in mouse and human vocal sequences corresponds to the theta frequency of cortical oscillations that have been shown to be susceptible to entrainment by streams of regularly presented stimuli<sup><xref ref-type="bibr" rid="c55">55</xref>,<xref ref-type="bibr" rid="c56">56</xref></sup>. An influential model of speech processing proposes that cortical theta oscillations in the 4-8 Hz range entrain to the slow temporal envelope modulations in speech, and may play a role in parsing continuous sensory input by coordinating neuronal excitability to optimally support the decoding of phonemes<sup><xref ref-type="bibr" rid="c57">57</xref></sup>. Whether similar neuronal dynamics can be observed in response to courtship song sequences in the mouse auditory system, and what neuronal substrates causally support the behaviourally-relevant extraction and representation of acoustic features identified in this study, remains to be determined.</p>
</sec>
<sec id="s3">
<title>Conclusion</title>
<p>In summary, we used an ethologically-driven approach to evaluate the behavioural relevance of several acoustic features in mouse communication. Our results identify a key role of temporal regularity, but invariance to global and local song structure, in the goal-directed use of vocal patterns by female listeners. The findings highlight the selective monitoring of temporal regularity in communication sounds as an effective, potentially evolutionarily conserved behavioural strategy for the sensory compression of complex vocal sequences during mammalian vocal perception.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Animals</title>
<p>83 female C57Bl/6J inbred mice (<italic>M. musculus</italic>, Charles River Laboratories, UK) participated in the experiments. All experimental procedures were carried out in accordance with a UK Home Office Project License approved under the United Kingdom Animals (Scientific Procedures) Act of 1986, and in compliance with international legislation governing the maintenance and use of laboratory animals in scientific experiments (European Communities Council Directive of November 24, 1986, 86/609/EEC). Animals were housed in same-sex groups of 3-5 per cage and under a reversed 12 hours light/dark cycle. Food pellets and water were provided <italic>ad libitum</italic>. Animals were tested in 8 batches of 8-12 animals between September 2017 and September 2019. Mice participated in the playback experiments between the ages of 5-11 weeks (mean = 9 weeks; Fig. S3). In order prevent any impact of handling stress from influencing behavioural testing, the animals were regularly handled and habituated to all aspects the behavioural protocol for at least a week before starting experiments. Tube handling<sup><xref ref-type="bibr" rid="c58">58</xref></sup> was used exclusively. The mice had no sexual experience, but were exposed to two different groups of males through a mesh division for 5 min each following each behavioural testing session. This allowed olfactory, auditory and visual contact with males while preventing physical contact and intercourse, and has been suggested to help maintain female’s motivation for approach behaviour<sup><xref ref-type="bibr" rid="c8">8</xref></sup>.</p>
</sec>
<sec id="s4b">
<title>Acoustic stimuli</title>
<p>Ultrasonic vocalizations were obtained from 5 male C57BL/6 mice in response to urine samples from conspecific females in estrus, across 11 recording sessions. The urine stimulus consisted of the tip of one sterile cotton bud soaked in a mixture of freshly-collected urine from at least two animals. Sounds were recorded at a sampling frequency of 250 kHz (and, Avisoft Bioacoustics, Germany) using a condenser ultrasonic microphone (CM16/CMPA), recording interface (UltrasoundGate 416H) and software (Avisoft Recorder version 5.2.09, all from Avisoft Bioacoustics, Germany).</p>
<p>The stimulus set used in the playback experiments (“intact/original songs”) consisted of a subset of seven songs produced by 3 C57BL/6 male mice (median age 22 weeks) over 6 recording sessions. The sound files were high-pass filtered above 40 kHz, and were denoised using the frequency-domain noise reduction algorithm in Avisoft SASlab Pro (version 5.2.09, Avisoft Bioacoustics, Germany). Syllable detection and segmentation was verified manually. Sound files were up-sampled to 260420 Hz with anti-aliasing for playback.</p>
</sec>
<sec id="s4c">
<title>Acoustic manipulations</title>
<p>An artificial set of randomized syllable sequences was created by shuffling the sequential order of the syllables within each song. Each syllable remained paired with its subsequent silent interval, in order to preserve the distribution of inter-syllable intervals across the songs.</p>
<p>The set of temporally-reversed songs was creating by playing each original song backwards, e.g. flipped along the time axis such that the last sample of the last syllable was played first, etc, until the first sample of the first syllable which was played last.</p>
<p>Phase-scrambled songs were created by replacing the phase of the original signal with a random phase value, for each original song separately. The resulting high-frequency noise syllables were then ramped with a 0.5 ms linear rise/fall time.</p>
<p>Pure tone sequences were created by replacing each syllable with a pure tone of the matching duration, including a 0.5 ms linear rise/fall time. Tone frequency was selected to match the median peak frequency of all recorded syllables, e.g. 76k Hz.</p>
<p>For the phase-scrambled and pure tone sequence sets, sound amplitude was constant across all syllables in one song, and was adjusted for each song separately in order to match the mean root mean square amplitude of syllables in the corresponding original songs.</p>
<p>A set of temporally irregular songs was created by replacing the silent pauses associated with each syllable (time intervals between the offset of one syllable and the onset of following syllable) with randomly generated break durations, under the condition that the duration of the new song be equal to that of the original song.</p>
<p>A super-rhythmic version of each of the original songs was created by scaling the silent pauses between each pair of successive syllables such that every inter-syllable interval (time between the onset of one syllable and the offset of the following syllable) matched the median inter-syllable interval of a given song. For syllables that were longer than the median inter-syllable interval, the silent pause was adjusted to “skip a cycle” such that the next syllable occurred at twice the median syllable interval. The total duration of each super-rhythmic song was matched to that of the corresponding original song by modifying the longer inter-bout interval (silent pause between regular sequences of syllables).</p>
<p>Sounds were played using MATLAB (Matlab version R20015a; MathWorks, Natwick, MA, USA) and a digital signal processor (RX6, Tucker-Davis Technologies, FL, USA). Acoustic stimuli were delivered via two free-field electrostatic speakers driven (ES1, Tucker-Davis Technologies, FL, USA), placed at ear level, 3 cm from the mesh window at the edge of the enclosure. The frequency response of the loudspeaker was ± 6 dB across the frequency range used for stimulation [68-84 kHz]. Sounds were played either at 53-59 dbSPL, or 63-70 dbSPL, as measured just inside the testing box (7cm from the speakers). Before the start of the first behavioural session of the day, correct playback of ultrasonic stimuli was confirmed visually using the Avisoft-RECORDER software and condenser ultrasonic microphones (Avisoft CM16/CMPA, both by Avisoft Bioacoustics, Germany) placed at the edge of the behavioural box.</p>
</sec>
<sec id="s4d">
<title>Estrous staging</title>
<p>Estrous stage was assessed daily based on vaginal cytology<sup><xref ref-type="bibr" rid="c59">59</xref>,<xref ref-type="bibr" rid="c60">60</xref></sup>. Samples were collected by flushing warmed sterile saline over the vaginal opening. The unstained cell samples collected via vaginal lavage were then visualized using bright-field microscopy (Leica DFC365FX). Estrus stage was estimated visually based on the relative proportions of leukocyte, nucleated and cornified epithelial cells, following the estrous cycle stage identification tool outlined in <sup><xref ref-type="bibr" rid="c59">59</xref></sup>. In a subset of animals tested between October 2017 and March 2018 estrous stage was identified based on visual examination of the vagina<sup><xref ref-type="bibr" rid="c59">59</xref>,<xref ref-type="bibr" rid="c60">60</xref></sup>. Both estrus staging methods required brief (10s) tail restraint, and took place daily at similar times in the morning, at least 2-3 hours before a behavioural experiment in order to prevent any handling stress from impacting the behaviour.</p>
</sec>
<sec id="s4e">
<title>Behavioural testing</title>
<p>All behavioural tests were conducted during the dark (active) phase of the light cycle, between 12:00 and 20:00. Animals participated in the experiments once per estrous cycle, when identified as being in estrus or proestus. A minimum of 5 days separated successive test sessions for each mouse. Animals were tested at most once in each given sound contrast. The order of experimental contrasts each animal participated in was assigned pseudo-randomly, in order to balance the ages of animals taking part in each type of behavioural tests (no difference in mean listener age across all tested contrasts, one-way ANOVA, F(6, 153) = 0.013, p = 1.0; Fig. S3).</p>
<p>Behavioural assays took place in a two-compartment Plexiglas behavioural box (22.7 × 25.2 × 22.5cm), under infrared LED illumination (no visible light, <xref rid="fig2" ref-type="fig">Fig. 2B</xref>). A soundproof partition partly divided the behavioural box down the middle into two “speaker zones” joined by a larger “neutral zone”. Wall panels with a mesh insert were positioned at the end of each speaker zone, such that the mesh-covered openings were level with two speakers placed outside of the box, on either side of the partition. This allowed undistorted delivery of acoustic stimuli separately into each of the two speaker zones, while playbacks from both speakers were equally audible in the centre of the neutral zone. The behavioural box was placed inside a double-walled soundproof booth (IAC Acoustics), whose interior was covered by 4-cm thick acoustic absorption foam (E-foam, UK). Two trays containing 2 g of a mixture of soiled bedding freshly collected from two cages of males were placed below each speaker, outside of the behavioural box. After each behavioural testing session, the wall panels were washed with soapy water and dried, and the behavioural boxed wiped down first with 70% ethanol, then with distilled water.</p>
<p>One behavioural session lasted for 40 min, and contained a 10 min silent habituation period, and 4 sound presentation trials (160s of song playback) each followed by a 3 min break (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>). The session started with the mouse being placed in the middle of the neutral zone using the handling cylinder. After the 10min silent habituation period, the first playback trial was initiated manually as soon as the animal returned to the middle of the neutral zone. Each playback trial consisted of one concatenated presentation of the songs in the stimulus set (7 songs, 2min 40s total sound duration), played in random order. Each trial (playback of one full stimulus set) was followed by a 3min silent break, before the next trial could be initiated.</p>
<p>In a “song vs silence” contrast experiment, the song set was played back through one of the two speakers, while the other remained silent. In a “original vs manipulated song” contrast experiment, the original song set and corresponding manipulated version of the song set were played back simultaneously, each through a different speaker. The playback onsets of individual pairs of original and manipulated songs were jittered with respect to each other by a time delay randomly sampled between 20-90 ms, with the leading stimulus set randomly assigned for each trial. The speaker/playback side for each sound presentation programmes was randomly assigned at the beginning of one session, and alternated across the 4 consecutive trials within each session. This strategy allowed to distinguish, for each session, between a behavioural preference for one of the sound types despite changes in the location of the sound source, and a preference for one of the physical locations (side bias).</p>
</sec>
<sec id="s4f">
<title>Video tracking</title>
<p>The position of the animal was recorded under infrared illumination, using an overhead camera (640×480 pixel resolution, Play Station 3 Eye, Sony) that had been customized by manual removal of its infrared blocking filter. A custom image processing pipeline in Bonsai <sup><xref ref-type="bibr" rid="c61">61</xref></sup> detected the position of the animal against the while background and saved the x and y coordinates of the centroid of the body, as well as the corresponding timestamps (ca 30 frames/s). On-and onset timestamps for song playback were recorded by manual key presses.</p>
</sec>
<sec id="s4g">
<title>Data analysis</title>
<p>Tracking data were analyzed in MATLAB using custom scripts (version R2018a; MathWorks, Natwick, MA, USA). For timestamps during which the detection of the mouse’s body failed, the x-y position was interpolated by repeating the previously available position. For each playback side, a “speaker zone” was defined as the area of the behavioural box adjacent to each mesh opening in the wall panel, ranging 86 mm into each arm of the divided section of the box. The relative strength of the animal’s place preference for one stimulus over the other was captured using a preference index defined as follows: <inline-formula><alternatives><inline-graphic xlink:href="922773v2_inline1.gif" mime-subtype="gif" mimetype="image"/></alternatives></inline-formula>, where <italic>t(song)</italic> is the time in seconds the animal spent in the speaker zone adjacent to the side of original song playback for this trial and <italic>t(manipulated song)</italic> is the time spent in the other speaker zone, e.g. the side of manipulated song playback or silence. The preference index was computed for each sound presentation trial. The preference index for one behavioural session was calculated as the median of the 4 trial-based sound preference indices for that session.</p>
<p>Side bias across a testing session was assessed by quantifying the time in seconds an animal spent in the left and right speaker zone during each of the four sound presentation trials. Sessions in which the animal displayed a clear side bias (consistent preference for the left or right side of the behavioural box despite changes in the sound playback side, evaluated as all four trials showing a positive, or all negative, difference between the times in the left and right speaker zones) were excluded from the analysis (29% ± 7% of behavioural sessions excluded due to side bias, mean and 95% C.I.).</p>
<p>To evaluate the temporal evolution of each session’s approach behaviour, one positive (resp. negative) increment was assigned to each video frame during which the animal was inside the area (speaker zone) in front of the speaker playing back intact (resp. manipulated) songs. Frames during which the animal was outside either speaker zones were assigned a value of zero. Temporal profiles quantifying the accumulation of dwell time in either speaker zones were computed by taking the cumulative sum of these traces along the relevant trial(s), and dividing by the video frame rate to obtain times in seconds. Before comparing or pooling data across animals, each trace from an individual session or trial was normalized to the absolute value of its maximum amplitude. When displaying all traces individually, sessions were sorted by the amplitude of their last element.</p>
<p>All statistical analyses were performed using MATLAB (version R2018A). Data were tested for deviation from normality using Lilliefors’ composite goodness-of-fit test (Matlab function lillietest) and Shapiro-Wilk’s parametric hypothesis test of composite normality (Matlab function swtest) at p&lt;0.05, before using parametric tests. Significant population place preference for each contrast was tested using one-sample two-tailed <italic>t</italic>-test against zero. Unless otherwise stated all statistical tests are two-tailed. In all figures, significance levels are indicated as follows: ×: significant at the 0.05 level, ××: significant at the 0.01 level.</p>
</sec>
<sec id="s4h">
<title>Data and code availability</title>
<p>All data used to produce the figures in this study are available in figshare with the identifier [final data DOI(s) TBC ahead of publication]. The computer code used in this study is available at [final github location TBC ahead of publication].</p>
</sec>
</sec>
<sec id="d1e1078" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1158">
<label>Supplementary Information</label>
<media xlink:href="supplements/922773_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>1</label><mixed-citation publication-type="journal"><string-name><surname>Portfors</surname>, <given-names>C. V.</given-names></string-name> <article-title>Types and functions of ultrasonic vocalizations in laboratory rats and mice</article-title>. <source>J Am Assoc Lab Anim Sci</source> <volume>46</volume>, <fpage>28</fpage>–<lpage>34</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c2"><label>2</label><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. D.</given-names></string-name>, <string-name><surname>Merzenich</surname>, <given-names>M. M.</given-names></string-name> &amp; <string-name><surname>Schreiner</surname>, <given-names>C. E.</given-names></string-name> <article-title>Acoustic variability and distinguishability among mouse ultrasound vocalizations</article-title>. <source>J Acoust Soc Am</source> <volume>114</volume>, <fpage>3412</fpage>–<lpage>3422</lpage>, doi: <pub-id pub-id-type="doi">10.1121/1.1623787</pub-id> (<year>2003</year>).</mixed-citation></ref>
<ref id="c3"><label>3</label><mixed-citation publication-type="journal"><string-name><surname>Holy</surname>, <given-names>T. E.</given-names></string-name> &amp; <string-name><surname>Guo</surname>, <given-names>Z.</given-names></string-name> <article-title>Ultrasonic songs of male mice</article-title>. <source>PLoS Biol</source> <volume>3</volume>, <fpage>e386</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.0030386</pub-id> (<year>2005</year>).</mixed-citation></ref>
<ref id="c4"><label>4</label><mixed-citation publication-type="journal"><string-name><surname>Chabout</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>Adult Male Mice Emit Context-Specific Ultrasonic Vocalizations That Are Modulated by Prior Isolation or Group Rearing Environment</article-title>. <source>Plos One</source> <volume>7</volume>, doi: ARTN e29401 DOI <pub-id pub-id-type="doi">10.1371/journal.pone.0029401</pub-id> (<year>2012</year>).</mixed-citation></ref>
<ref id="c5"><label>5</label><mixed-citation publication-type="other"><string-name><surname>Chabout</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jones-Macopson</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Jarvis</surname>, <given-names>E. D.</given-names></string-name> <article-title>Eliciting and Analyzing Male Mouse Ultrasonic Vocalization (USV) Songs</article-title>. <source>J Vis Exp</source>, doi: <pub-id pub-id-type="doi">10.3791/54137</pub-id> (<year>2017</year>).</mixed-citation></ref>
<ref id="c6"><label>6</label><mixed-citation publication-type="journal"><string-name><surname>Pomerantz</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Nunez</surname>, <given-names>A. A.</given-names></string-name> &amp; <string-name><surname>Bean</surname>, <given-names>N. J.</given-names></string-name> <article-title>Female behavior is affected by male ultrasonic vocalizations in house mice</article-title>. <source>Physiol Behav</source> <volume>31</volume>, <fpage>91</fpage>–<lpage>96</lpage>, doi: <pub-id pub-id-type="doi">10.1016/0031-9384(83)90101-4</pub-id> (<year>1983</year>).</mixed-citation></ref>
<ref id="c7"><label>7</label><mixed-citation publication-type="journal"><string-name><surname>Asaba</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title>Developmental social environment imprints female preference for male song in mice</article-title>. <source>PLoS One</source> <volume>9</volume>, <fpage>e87186</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0087186</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c8"><label>8</label><mixed-citation publication-type="journal"><string-name><surname>Shepard</surname>, <given-names>K. N.</given-names></string-name> &amp; <string-name><surname>Liu</surname>, <given-names>R. C.</given-names></string-name> <article-title>Experience restores innate female preference for male ultrasonic vocalizations</article-title>. <source>Genes Brain Behav</source> <volume>10</volume>, <fpage>28</fpage>–<lpage>34</lpage>, doi: <pub-id pub-id-type="doi">10.1111/j.1601-183X.2010.00580.x</pub-id> (<year>2011</year>).</mixed-citation></ref>
<ref id="c9"><label>9</label><mixed-citation publication-type="journal"><string-name><surname>Chabout</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sarkar</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dunson</surname>, <given-names>D. B.</given-names></string-name> &amp; <string-name><surname>Jarvis</surname>, <given-names>E. D.</given-names></string-name> <article-title>Male mice song syntax depends on social contexts and influences female preferences</article-title>. <source>Front Behav Neurosci</source> <volume>9</volume>, <fpage>76</fpage>, doi: <pub-id pub-id-type="doi">10.3389/fnbeh.2015.00076</pub-id> (<year>2015</year>).</mixed-citation></ref>
<ref id="c10"><label>10</label><mixed-citation publication-type="journal"><string-name><surname>Musolf</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Meindl</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Larsen</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Kalcounis-Rueppell</surname>, <given-names>M. C.</given-names></string-name> &amp; <string-name><surname>Penn</surname>, <given-names>D. J.</given-names></string-name> <article-title>Ultrasonic Vocalizations of Male Mice Differ among Species and Females Show Assortative Preferences for Male Calls</article-title>. <source>PLoS One</source> <volume>10</volume>, <fpage>e0134123</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0134123</pub-id> (<year>2015</year>).</mixed-citation></ref>
<ref id="c11"><label>11</label><mixed-citation publication-type="journal"><string-name><surname>Hammerschmidt</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Radyushkin</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ehrenreich</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Fischer</surname>, <given-names>J.</given-names></string-name> <article-title>Female mice respond to male ultrasonic ‘songs’ with approach behaviour</article-title>. <source>Biol Lett</source> <volume>5</volume>, <fpage>589</fpage>–<lpage>592</lpage>, doi: <pub-id pub-id-type="doi">10.1098/rsbl.2009.0317</pub-id> (<year>2009</year>).</mixed-citation></ref>
<ref id="c12"><label>12</label><mixed-citation publication-type="journal"><string-name><surname>Asaba</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title>Male mice ultrasonic vocalizations enhance female sexual approach and hypothalamic kisspeptin neuron activity</article-title>. <source>Horm Behav</source> <volume>94</volume>, <fpage>53</fpage>–<lpage>60</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.yhbeh.2017.06.006</pub-id> (<year>2017</year>).</mixed-citation></ref>
<ref id="c13"><label>13</label><mixed-citation publication-type="journal"><string-name><surname>Nomoto</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> <article-title>Female mice exhibit both sexual and social partner preferences for vocalizing males</article-title>. <source>Integr Zool</source> <volume>13</volume>, <fpage>735</fpage>–<lpage>744</lpage>, doi: <pub-id pub-id-type="doi">10.1111/1749-4877.12357</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c14"><label>14</label><mixed-citation publication-type="journal"><string-name><surname>Egnor</surname>, <given-names>S. R.</given-names></string-name> &amp; <string-name><surname>Seagraves</surname>, <given-names>K. M.</given-names></string-name> <article-title>The contribution of ultrasonic vocalizations to mouse courtship</article-title>. <source>Curr Opin Neurobiol</source> <volume>38</volume>, <fpage>1</fpage>–<lpage>5</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.conb.2015.12.009</pub-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c15"><label>15</label><mixed-citation publication-type="journal"><string-name><surname>Hoffmann</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Musolf</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Penn</surname>, <given-names>D. J.</given-names></string-name> <article-title>Spectrographic analyses reveal signals of individuality and kinship in the ultrasonic courtship vocalizations of wild house mice</article-title>. <source>Physiol Behav</source> <volume>105</volume>, <fpage>766</fpage>–<lpage>771</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.physbeh.2011.10.011</pub-id> (<year>2012</year>).</mixed-citation></ref>
<ref id="c16"><label>16</label><mixed-citation publication-type="journal"><string-name><surname>Neilans</surname>, <given-names>E. G.</given-names></string-name>, <string-name><surname>Holfoth</surname>, <given-names>D. P.</given-names></string-name>, <string-name><surname>Radziwon</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Portfors</surname>, <given-names>C. V.</given-names></string-name> &amp; <string-name><surname>Dent</surname>, <given-names>M. L.</given-names></string-name> <article-title>Discrimination of ultrasonic vocalizations by CBA/CaJ mice (Mus musculus) is related to spectrotemporal dissimilarity of vocalizations</article-title>. <source>PLoS One</source> <volume>9</volume>, <fpage>e85405</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0085405</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c17"><label>17</label><mixed-citation publication-type="journal"><string-name><surname>Holfoth</surname>, <given-names>D. P.</given-names></string-name>, <string-name><surname>Neilans</surname>, <given-names>E. G.</given-names></string-name> &amp; <string-name><surname>Dent</surname>, <given-names>M. L.</given-names></string-name> <article-title>Discrimination of partial from whole ultrasonic vocalizations using a go/no-go task in mice</article-title>. <source>J Acoust Soc Am</source> <volume>136</volume>, <fpage>3401</fpage>, doi: <pub-id pub-id-type="doi">10.1121/1.4900564</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c18"><label>18</label><mixed-citation publication-type="journal"><string-name><surname>Asaba</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hattori</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Mogi</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Kikusui</surname>, <given-names>T.</given-names></string-name> <article-title>Sexual attractiveness of male chemicals and vocalizations in mice</article-title>. <source>Front Neurosci</source> <volume>8</volume>, <fpage>231</fpage>, doi: <pub-id pub-id-type="doi">10.3389/fnins.2014.00231</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c19"><label>19</label><mixed-citation publication-type="journal"><string-name><surname>Sugimoto</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal> <article-title>A role for strain differences in waveforms of ultrasonic vocalizations during male-female interaction</article-title>. <source>PLoS One</source> <volume>6</volume>, <fpage>e22093</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0022093</pub-id> (<year>2011</year>).</mixed-citation></ref>
<ref id="c20"><label>20</label><mixed-citation publication-type="journal"><string-name><surname>Musolf</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Hoffmann</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Penn</surname>, <given-names>D. J.</given-names></string-name> <article-title>Ultrasonic courtship vocalizations in wild house mice, <italic>Mus musculus musculus</italic></article-title>. <source>Animal Behaviour</source> <volume>79</volume>, <fpage>757</fpage>–<lpage>764</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.anbehav.2009.12.034</pub-id> (<year>2010</year>).</mixed-citation></ref>
<ref id="c21"><label>21</label><mixed-citation publication-type="journal"><string-name><surname>Portfors</surname>, <given-names>C. V.</given-names></string-name> &amp; <string-name><surname>Perkel</surname>, <given-names>D. J.</given-names></string-name> <article-title>The role of ultrasonic vocalizations in mouse communication</article-title>. <source>Curr Opin Neurobiol</source> <volume>28</volume>, <fpage>115</fpage>–<lpage>120</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.conb.2014.07.002</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c22"><label>22</label><mixed-citation publication-type="journal"><string-name><surname>Matsumoto</surname>, <given-names>Y. K.</given-names></string-name> &amp; <string-name><surname>Okanoya</surname>, <given-names>K.</given-names></string-name> <article-title>Phase-Specific Vocalizations of Male Mice at the Initial Encounter during the Courtship Sequence</article-title>. <source>PLoS One</source> <volume>11</volume>, <fpage>e0147102</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0147102</pub-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c23"><label>23</label><mixed-citation publication-type="journal"><string-name><surname>Tschida</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> <article-title>A Specialized Neural Circuit Gates Social Vocalizations in the Mouse</article-title>. <source>Neuron</source> <volume>103</volume>, <fpage>459</fpage>–<lpage>472</lpage> e454, doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2019.05.025</pub-id> (<year>2019</year>).</mixed-citation></ref>
<ref id="c24"><label>24</label><mixed-citation publication-type="journal"><string-name><surname>Hage</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Gavrilov</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Salomon</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Stein</surname>, <given-names>A. M.</given-names></string-name> <article-title>Temporal vocal features suggest different call-pattern generating mechanisms in mice and bats</article-title>. <source>BMC Neurosci</source> <volume>14</volume>, <fpage>99</fpage>, doi: <pub-id pub-id-type="doi">10.1186/1471-2202-14-99</pub-id> (<year>2013</year>).</mixed-citation></ref>
<ref id="c25"><label>25</label><mixed-citation publication-type="journal"><string-name><surname>Castellucci</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Calbick</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>McCormick</surname>, <given-names>D.</given-names></string-name> <article-title>The temporal organization of mouse ultrasonic vocalizations</article-title>. <source>PLoS One</source> <volume>13</volume>, <fpage>e0199929</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0199929</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c26"><label>26</label><mixed-citation publication-type="journal"><string-name><surname>Issa</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Haeffele</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>Young</surname>, <given-names>E. D.</given-names></string-name> &amp; <string-name><surname>Yue</surname>, <given-names>D. T.</given-names></string-name> <article-title>Multiscale mapping of frequency sweep rate in mouse auditory cortex</article-title>. <source>Hear Res</source> <volume>344</volume>, <fpage>207</fpage>–<lpage>222</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.heares.2016.11.018</pub-id> (<year>2017</year>).</mixed-citation></ref>
<ref id="c27"><label>27</label><mixed-citation publication-type="journal"><string-name><surname>Lui</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Mendelson</surname>, <given-names>J. R.</given-names></string-name> <article-title>Frequency modulated sweep responses in the medial geniculate nucleus</article-title>. <source>Exp Brain Res</source> <volume>153</volume>, <fpage>550</fpage>–<lpage>553</lpage>, doi: <pub-id pub-id-type="doi">10.1007/s00221-003-1618-y</pub-id> (<year>2003</year>).</mixed-citation></ref>
<ref id="c28"><label>28</label><mixed-citation publication-type="journal"><string-name><surname>Sollini</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chapuis</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Chadderton</surname>, <given-names>P.</given-names></string-name> <article-title>ON-OFF receptive fields in auditory cortex diverge during development and contribute to directional sweep selectivity</article-title>. <source>Nat Commun</source> <volume>9</volume>, <fpage>2084</fpage>, doi: <pub-id pub-id-type="doi">10.1038/s41467-018-04548-3</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c29"><label>29</label><mixed-citation publication-type="journal"><string-name><surname>Tian</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Rauschecker</surname>, <given-names>J. P.</given-names></string-name> <article-title>Processing of frequency-modulated sounds in the cat’s posterior auditory field</article-title>. <source>J Neurophysiol</source> <volume>79</volume>, <fpage>2629</fpage>–<lpage>2642</lpage>, doi: <pub-id pub-id-type="doi">10.1152/jn.1998.79.5.2629</pub-id> (<year>1998</year>).</mixed-citation></ref>
<ref id="c30"><label>30</label><mixed-citation publication-type="journal"><string-name><surname>Fuzessery</surname>, <given-names>Z. M.</given-names></string-name> <article-title>Response selectivity for multiple dimensions of frequency sweeps in the pallid bat inferior colliculus</article-title>. <source>J Neurophysiol</source> <volume>72</volume>, <fpage>1061</fpage>–<lpage>1079</lpage>, doi: <pub-id pub-id-type="doi">10.1152/jn.1994.72.3.1061</pub-id> (<year>1994</year>).</mixed-citation></ref>
<ref id="c31"><label>31</label><mixed-citation publication-type="journal"><string-name><surname>Razak</surname>, <given-names>K. A.</given-names></string-name> &amp; <string-name><surname>Fuzessery</surname>, <given-names>Z. M.</given-names></string-name> <article-title>Neural mechanisms underlying selectivity for the rate and direction of frequency-modulated sweeps in the auditory cortex of the pallid bat</article-title>. <source>J Neurophysiol</source> <volume>96</volume>, <fpage>1303</fpage>–<lpage>1319</lpage>, doi: <pub-id pub-id-type="doi">10.1152/jn.00020.2006</pub-id> (<year>2006</year>).</mixed-citation></ref>
<ref id="c32"><label>32</label><mixed-citation publication-type="journal"><string-name><surname>Geis</surname>, <given-names>H. R.</given-names></string-name> &amp; <string-name><surname>Borst</surname>, <given-names>J. G.</given-names></string-name> <article-title>Intracellular responses to frequency modulated tones in the dorsal cortex of the mouse inferior colliculus</article-title>. <source>Front Neural Circuits</source> <volume>7</volume>, <fpage>7</fpage>, doi: <pub-id pub-id-type="doi">10.3389/fncir.2013.00007</pub-id> (<year>2013</year>).</mixed-citation></ref>
<ref id="c33"><label>33</label><mixed-citation publication-type="journal"><string-name><surname>Fritz</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Elhilali</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Shamma</surname>, <given-names>S. A.</given-names></string-name> <article-title>Differential dynamic plasticity of A1 receptive fields during multiple spectral tasks</article-title>. <source>J Neurosci</source> <volume>25</volume>, <fpage>7623</fpage>–<lpage>7635</lpage>, doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1318-05.2005</pub-id> (<year>2005</year>).</mixed-citation></ref>
<ref id="c34"><label>34</label><mixed-citation publication-type="journal"><string-name><surname>Fritz</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shamma</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Elhilali</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Klein</surname>, <given-names>D.</given-names></string-name> <article-title>Rapid task-related plasticity of spectrotemporal receptive fields in primary auditory cortex</article-title>. <source>Nat Neurosci</source> <volume>6</volume>, <fpage>1216</fpage>–<lpage>1223</lpage>, doi: <pub-id pub-id-type="doi">10.1038/nn1141</pub-id> (<year>2003</year>).</mixed-citation></ref>
<ref id="c35"><label>35</label><mixed-citation publication-type="journal"><string-name><surname>Wachowitz</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Ewert</surname>, <given-names>J. P.</given-names></string-name> <article-title>A key by which the toad’s visual system gets access to the domain of prey</article-title>. <source>Physiol Behav</source> <volume>60</volume>, <fpage>877</fpage>–<lpage>887</lpage>, doi: <pub-id pub-id-type="doi">10.1016/0031-9384(96)00070-4</pub-id> (<year>1996</year>).</mixed-citation></ref>
<ref id="c36"><label>36</label><mixed-citation publication-type="journal"><string-name><surname>Castellucci</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>McGinley</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>McCormick</surname>, <given-names>D. A.</given-names></string-name> <article-title>Knockout of Foxp2 disrupts vocal development in mice</article-title>. <source>Sci Rep</source> <volume>6</volume>, <fpage>23305</fpage>, doi: <pub-id pub-id-type="doi">10.1038/srep23305</pub-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c37"><label>37</label><mixed-citation publication-type="journal"><string-name><surname>Hertz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Weiner</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Perets</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>London</surname>, <given-names>M.</given-names></string-name> <article-title>Temporal structure of mouse courtship vocalizations facilitates syllable labeling</article-title>. <source>Commun Biol</source> <volume>3</volume>, <fpage>333</fpage>, doi: <pub-id pub-id-type="doi">10.1038/s42003-020-1053-7</pub-id> (<year>2020</year>).</mixed-citation></ref>
<ref id="c38"><label>38</label><mixed-citation publication-type="journal"><string-name><surname>Ehret</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Haack</surname>, <given-names>B. J. J. o. c. p.</given-names></string-name> <source>Ultrasound recognition in house mice: Key-Stimulus configuration and recognition mechanism</source>. <volume>148</volume>, <fpage>245</fpage>–<lpage>251</lpage>, doi: <pub-id pub-id-type="doi">10.1007/bf00619131</pub-id> (<year>1982</year>).</mixed-citation></ref>
<ref id="c39"><label>39</label><mixed-citation publication-type="journal"><string-name><surname>Gaub</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Ehret</surname>, <given-names>G.</given-names></string-name> <article-title>Grouping in auditory temporal perception and vocal production is mutually adapted: the case of wriggling calls of mice</article-title>. <source>J Comp Physiol A Neuroethol Sens Neural Behav Physiol</source> <volume>191</volume>, <fpage>1131</fpage>–<lpage>1135</lpage>, doi: <pub-id pub-id-type="doi">10.1007/s00359-005-0036-y</pub-id> (<year>2005</year>).</mixed-citation></ref>
<ref id="c40"><label>40</label><mixed-citation publication-type="journal"><string-name><surname>Shannon</surname>, <given-names>R. V.</given-names></string-name>, <string-name><surname>Zeng</surname>, <given-names>F. G.</given-names></string-name>, <string-name><surname>Kamath</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Wygonski</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Ekelid</surname>, <given-names>M.</given-names></string-name> <article-title>Speech recognition with primarily temporal cues</article-title>. <source>Science</source> <volume>270</volume>, <fpage>303</fpage>–<lpage>304</lpage>, doi: <pub-id pub-id-type="doi">10.1126/science.270.5234.303</pub-id> (<year>1995</year>).</mixed-citation></ref>
<ref id="c41"><label>41</label><mixed-citation publication-type="journal"><string-name><surname>Van Tasell</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Soli</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Kirby</surname>, <given-names>V. M.</given-names></string-name> &amp; <string-name><surname>Widin</surname>, <given-names>G. P.</given-names></string-name> <article-title>Speech waveform envelope cues for consonant recognition</article-title>. <source>J Acoust Soc Am</source> <volume>82</volume>, <fpage>1152</fpage>–<lpage>1161</lpage>, doi: <pub-id pub-id-type="doi">10.1121/1.395251</pub-id> (<year>1987</year>).</mixed-citation></ref>
<ref id="c42"><label>42</label><mixed-citation publication-type="journal"><string-name><surname>Sirotin</surname>, <given-names>Y. B.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>M. E.</given-names></string-name> &amp; <string-name><surname>Laplagne</surname>, <given-names>D. A.</given-names></string-name> <article-title>Rodent ultrasonic vocalizations are bound to active sniffing behavior</article-title>. <source>Front Behav Neurosci</source> <volume>8</volume>, <fpage>399</fpage>, doi: <pub-id pub-id-type="doi">10.3389/fnbeh.2014.00399</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c43"><label>43</label><mixed-citation publication-type="journal"><string-name><surname>Lawrance</surname>, <given-names>E. L.</given-names></string-name>, <string-name><surname>Harper</surname>, <given-names>N. S.</given-names></string-name>, <string-name><surname>Cooke</surname>, <given-names>J. E.</given-names></string-name> &amp; <string-name><surname>Schnupp</surname>, <given-names>J. W.</given-names></string-name> <article-title>Temporal predictability enhances auditory detection</article-title>. <source>J Acoust Soc Am</source> <volume>135</volume>, <fpage>EL357</fpage>–<lpage>363</lpage>, doi: <pub-id pub-id-type="doi">10.1121/1.4879667</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c44"><label>44</label><mixed-citation publication-type="journal"><string-name><surname>Ghitza</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>Greenberg</surname>, <given-names>S.</given-names></string-name> <article-title>On the possible role of brain rhythms in speech perception: intelligibility of time-compressed speech with periodic and aperiodic insertions of silence</article-title>. <source>Phonetica</source> <volume>66</volume>, <fpage>113</fpage>–<lpage>126</lpage>, doi: <pub-id pub-id-type="doi">10.1159/000208934</pub-id> (<year>2009</year>).</mixed-citation></ref>
<ref id="c45"><label>45</label><mixed-citation publication-type="journal"><string-name><surname>Pichora-Fuller</surname>, <given-names>M. K.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Macdonald</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Pass</surname>, <given-names>H. E.</given-names></string-name> &amp; <string-name><surname>Brown</surname>, <given-names>S.</given-names></string-name> <article-title>Temporal jitter disrupts speech intelligibility: a simulation of auditory aging</article-title>. <source>Hear Res</source> <volume>223</volume>, <fpage>114</fpage>–<lpage>121</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.heares.2006.10.009</pub-id> (<year>2007</year>).</mixed-citation></ref>
<ref id="c46"><label>46</label><mixed-citation publication-type="journal"><string-name><surname>Lak</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Arabzadeh</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Diamond</surname>, <given-names>M. E.</given-names></string-name> <article-title>Enhanced response of neurons in rat somatosensory cortex to stimuli containing temporal noise</article-title>. <source>Cereb Cortex</source> <volume>18</volume>, <fpage>1085</fpage>–<lpage>1093</lpage>, doi: <pub-id pub-id-type="doi">10.1093/cercor/bhm144</pub-id> (<year>2008</year>).</mixed-citation></ref>
<ref id="c47"><label>47</label><mixed-citation publication-type="journal"><string-name><surname>Ghazanfar</surname>, <given-names>A. A.</given-names></string-name> &amp; <string-name><surname>Takahashi</surname>, <given-names>D. Y.</given-names></string-name> <article-title>The evolution of speech: vision, rhythm, cooperation</article-title>. <source>Trends Cogn Sci</source> <volume>18</volume>, <fpage>543</fpage>–<lpage>553</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.tics.2014.06.004</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c48"><label>48</label><mixed-citation publication-type="journal"><string-name><surname>Chandrasekaran</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Trubanova</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Stillittano</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Caplier</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Ghazanfar</surname>, <given-names>A. A.</given-names></string-name> <article-title>The natural statistics of audiovisual speech</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>, <fpage>e1000436</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000436</pub-id> (<year>2009</year>).</mixed-citation></ref>
<ref id="c49"><label>49</label><mixed-citation publication-type="journal"><string-name><surname>Tort</surname>, <given-names>A. B. L.</given-names></string-name> <etal>et al.</etal> <article-title>Parallel detection of theta and respiration-coupled oscillations throughout the mouse brain</article-title>. <source>Sci Rep</source> <volume>8</volume>, <fpage>6432</fpage>, doi: <pub-id pub-id-type="doi">10.1038/s41598-018-24629-z</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c50"><label>50</label><mixed-citation publication-type="journal"><string-name><surname>Kleinfeld</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ahissar</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Diamond</surname>, <given-names>M. E.</given-names></string-name> <article-title>Active sensation: insights from the rodent vibrissa sensorimotor system</article-title>. <source>Curr Opin Neurobiol</source> <volume>16</volume>, <fpage>435</fpage>–<lpage>444</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.conb.2006.06.009</pub-id> (<year>2006</year>).</mixed-citation></ref>
<ref id="c51"><label>51</label><mixed-citation publication-type="journal"><string-name><surname>Kepecs</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Uchida</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Mainen</surname>, <given-names>Z. F.</given-names></string-name> <article-title>Rapid and precise control of sniffing during olfactory discrimination in rats</article-title>. <source>J Neurophysiol</source> <volume>98</volume>, <fpage>205</fpage>–<lpage>213</lpage>, doi: <pub-id pub-id-type="doi">10.1152/jn.00071.2007</pub-id> (<year>2007</year>).</mixed-citation></ref>
<ref id="c52"><label>52</label><mixed-citation publication-type="journal"><string-name><surname>Grion</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Akrami</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zuo</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Stella</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Diamond</surname>, <given-names>M. E.</given-names></string-name> <article-title>Coherence between Rat Sensorimotor System and Hippocampus Is Enhanced during Tactile Discrimination</article-title>. <source>PLoS Biol</source> <volume>14</volume>, <fpage>e1002384</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.1002384</pub-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c53"><label>53</label><mixed-citation publication-type="journal"><string-name><surname>Kleinfeld</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Deschenes</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Ulanovsky</surname>, <given-names>N.</given-names></string-name> <article-title>Whisking, Sniffing, and the Hippocampal theta-Rhythm: A Tale of Two Oscillators</article-title>. <source>PLoS Biol</source> <volume>14</volume>, <fpage>e1002385</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.1002385</pub-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c54"><label>54</label><mixed-citation publication-type="journal"><string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Radman</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Scharfman</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name> <article-title>Dynamics of Active Sensing and perceptual selection</article-title>. <source>Curr Opin Neurobiol</source> <volume>20</volume>, <fpage>172</fpage>–<lpage>176</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.conb.2010.02.010</pub-id> (<year>2010</year>).</mixed-citation></ref>
<ref id="c55"><label>55</label><mixed-citation publication-type="journal"><string-name><surname>Peelle</surname>, <given-names>J. E.</given-names></string-name> &amp; <string-name><surname>Davis</surname>, <given-names>M. H.</given-names></string-name> <article-title>Neural Oscillations Carry Speech Rhythm through to Comprehension</article-title>. <source>Front Psychol</source> <volume>3</volume>, <fpage>320</fpage>, doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2012.00320</pub-id> (<year>2012</year>).</mixed-citation></ref>
<ref id="c56"><label>56</label><mixed-citation publication-type="journal"><string-name><surname>Doelling</surname>, <given-names>K. B.</given-names></string-name>, <string-name><surname>Arnal</surname>, <given-names>L. H.</given-names></string-name>, <string-name><surname>Ghitza</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> <article-title>Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing</article-title>. <source>Neuroimage</source> <volume>85</volume> <issue>Pt 2</issue>, <fpage>761</fpage>–<lpage>768</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.06.035</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c57"><label>57</label><mixed-citation publication-type="journal"><string-name><surname>Giraud</surname>, <given-names>A. L.</given-names></string-name> &amp; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> <article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title>. <source>Nat Neurosci</source> <volume>15</volume>, <fpage>511</fpage>–<lpage>517</lpage>, doi: <pub-id pub-id-type="doi">10.1038/nn.3063</pub-id> (<year>2012</year>).</mixed-citation></ref>
<ref id="c58"><label>58</label><mixed-citation publication-type="journal"><string-name><surname>Gouveia</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Hurst</surname>, <given-names>J. L.</given-names></string-name> <article-title>Reducing mouse anxiety during handling: effect of experience with handling tunnels</article-title>. <source>PLoS One</source> <volume>8</volume>, <fpage>e66401</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0066401</pub-id> (<year>2013</year>).</mixed-citation></ref>
<ref id="c59"><label>59</label><mixed-citation publication-type="journal"><string-name><surname>Byers</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Wiles</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Dunn</surname>, <given-names>S. L.</given-names></string-name> &amp; <string-name><surname>Taft</surname>, <given-names>R. A.</given-names></string-name> <article-title>Mouse estrous cycle identification tool and images</article-title>. <source>PLoS One</source> <volume>7</volume>, <fpage>e35538</fpage>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0035538</pub-id> (<year>2012</year>).</mixed-citation></ref>
<ref id="c60"><label>60</label><mixed-citation publication-type="other"><string-name><surname>Caligioni</surname>, <given-names>C. S.</given-names></string-name> <article-title>Assessing reproductive status/stages in mice</article-title>. <source>Curr Protoc Neurosci Appendix 4, Appendix 4I</source>, doi: <pub-id pub-id-type="doi">10.1002/0471142301.nsa04is48</pub-id> (<year>2009</year>)</mixed-citation></ref>
<ref id="c61"><label>61</label><mixed-citation publication-type="journal"><string-name><surname>Lopes</surname>, <given-names>G.</given-names></string-name> <etal>et al.</etal> <article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title>. <source>Front Neuroinform</source> <volume>9</volume>, <fpage>7</fpage>, doi: <pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id> (<year>2015</year>).</mixed-citation></ref>
</ref-list>
<ack>
<title>Acknowledgments</title>
<p>We thank Margaux Silvestre for help with testing initial experimental designs, Camille Suess for contributing to data collection, and Jennifer Linden for feedback and support. We thank Joshua Jones-Macopson and Erich Jarvis for advice on optimizing murine singing behaviour, and developing an approach behaviour paradigm. Tania Barkat, Florian Studer and Sebastian Reinartz provided helpful comments on the manuscript. This work was supported by the Swiss National Science Foundation (P2SKP3_158691, CP) the Wellcome Trust (110238/Z/15/Z, CP), the Medical Research Council (MR/M002889/1, DB), and a Royal Society Research Grant (RG130622, DB).</p></ack>
<sec id="s5">
<title>Author Contributions</title>
<p>Conceptualization, Data curation, Formal Analysis, Visualization and Writing – original draft: CP; Funding acquisition and Methodology: CP, DB; Resources: CP, CV, DB Investigation and Software: CP, CV; Writing – review &amp; editing: CP, DB.</p>
</sec>
<sec id="s6">
<title>Competing Interests</title>
<p>The authors declare no competing financial interest.</p>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.86464.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bathellier</surname>
<given-names>Brice</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Centre National de la Recherche Scientifique</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> work advances our understanding of the acoustic features driving the attraction of female mice to male vocalizations. The evidence supporting the conclusions is <bold>solid</bold>, with well-designed place preference assays and manipulations of male song structure. The work will be of broad interest to neurobiologists and ethologists working on mouse social interactions, auditory processing and communication.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.86464.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This work deals with courtship behaviour in mice. Authors try to identify the acoustic features that influence the attractivity level of male courtship songs to females. Courtship songs are made of sequences of short ultrasound syllables emitted at a rate of 7-10Hz. Authors manipulated these syllables by changing either the spectrotemporal content of each syllable or the intersyllable intervals. The authors found that it was only when sequences of syllables were irregular (with highly variable intersyllable intervals) that the female was less attracted to the song. The data, therefore, brings evidence that the acoustic features of syllables account less than the song's temporal regularity for the attractivity of courtship songs. The authors suggest that temporal regularity of syllable emission, building on breathing patterns, could reflect male fitness. They also suggest that temporal regularity could be an acoustic cue compressing the complex acoustic information carried by songs.</p>
<p>Strengths:</p>
<p>The study is well-written, very straightforward, and easy to follow. Behavioral tasks are well-designed and many tests, on a large enough set of animals have been done to support the conclusions. Results are clearly presented and provide enough details to see individual points. The discussion makes interesting connections between syllable rhythms and animals' fitness or brain rhythms.</p>
<p>Weaknesses:</p>
<p>Although the study is easy to understand and provides interesting results, the data analysis remains incomplete, and the interpretation of results is not cautious enough.</p>
<p>For instance, Fig. 2 shows a preference for song playback but we cannot determine if it is a general preference for a sound or a specific preference for male songs because only the difference between the presence of song or silence is tested. I acknowledge that the authors did not overstate their results, but the experimental design is incomplete and hard to interpret in that respect. For instance, the expression &quot;preferential approach to song&quot; is ambiguous.</p>
<p>There is no analysis of individual preference across tests and we might have the feeling that the effect shown mostly depends on the preference of only a few animals. Indeed, it seems that roughly one-third of animals showed a strong preference for the intact song while another third showed a strong preference for the modified song, whatever the modification. A few animals are therefore &quot;swing voters&quot;. It would have been interesting, if not pertinent, to have a deeper analysis of the behavior of these later animals. Do they choose less (i.e. spend less time close to speakers) or do they swing from one corner to another? What about the animals which always chose the modified song? Are these animals that already showed a weak or strong preference for silence, therefore showing they were not comfortable with the songs played? There is no discussion of these aspects either.</p>
<p>Also, on page 11, it is written &quot;female listeners perceptually compress the high sensory dimensionality of male songs by selectively monitoring a reduced subset of meaningful acoustic features in isolation.&quot; This statement or hypothesis is questionable. After all, if someone would change the inter-syllable intervals in human speech, that would become cryptic or at least annoying for the listener. Humans would definitely prefer normal speech. Is this because we compress acoustic features? Not really. It is likely that this modified speech just differs too much from the set of parameters typically encountered and therefore understood/interpreted while learning a language in childhood. Thus, the hypothesis here is rather to determine, for a given acoustic feature, if there is a range within which the perception of the message carried by the song (courtship) is maintained. Interpretation of &quot;compressed acoustic features&quot; with regards to animals' preference seems an overinterpretation. Same remark at the end of the conclusion.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.86464.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In the present manuscript, Perrodin et al. investigated which properties of ultrasonic vocalizations determine their attractiveness for female mice. They collected a set of male courtship vocalizations and compared their attractiveness for female mice against a number of conditions, including silence, and a number of modified sequences.</p>
<p>The study has a clear design and used insightful modifications on the vocalization sequences, which allow the present results to be linked to previous results. The most interesting outcome of the study is that female mice prefer regularly timed sequences of vocalizations over less regularly timed sequences. This result is novel and adds to our understanding of the determinants of social communication between mice. Overall the study is likely underpowered, which was, however, hard to assess as animal numbers were largely not reported for the individual tests, and statistical analysis was carried out on the level of sessions only.</p>
<p>The study has a very good discussion embedding the current results with the previous literature, although the discussion steps beyond the results in a few respects, in particular when trying to determine the underlying reasons for the preference for regularly spaced sequences.</p>
<p>Methodologically the study is carried out at the appropriate level, although some improvements could be made to the experimental apparatus to avoid reflections.</p>
<p>The study will likely have a substantial impact on the field of mouse communication because the regularity of spacing has not been a focus of previous research. In addition, the confirmation that a lot of other modifications are less determining for the attractiveness of the vocalizations provides solid data on which to base future work.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.86464.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Perrodin, Verzat and Bendor describe the response of female mice to the playback of male mouse ultrasonic songs. The experiments were performed in a Y-maze-like apparatus with two acoustically separate response chambers. Sounds were presented in 4 trials, alternating strictly between the left and right branches of the Y. Cumulative dwell time in the two chambers was measured, and used as an index of female preference. They first show, consistent with previous observations, that female mice will spend more time near a speaker playing a male mouse song than near a speaker playing nothing. They then performed several manipulations-time reversals, syllable order randomization, phase scrambled replacement, pure tone replacement, and 'hyper-regular' inter-syllable-intervals-which female mice did not discriminate from the normal song in this assay. Finally, they show that females spent more time near normal songs than near songs with more variable inter-syllable-intervals</p>
<p>The authors' approach to the problem was ethologically sensible -- females were tested in proestrus and estrus, the male odor was used to increase motivation, mouse handling was with tube transfers to reduce stress, mice were age-matched across conditions, and experiments were conducted in the dark (active) phase. In addition, animals were habituated to handling and to the apparatus.</p>
<p>The acoustics were very good. The acoustic structure of the vocal signals was well described. Specific ranges of dB SPL were reported, speaker flatness was evaluated, the sound amplitude was matched in manipulated and unmanipulated songs, and playback onset timing jittered randomly between manipulated and unmanipulated signals.</p>
<p>I think it is a reasonable result. My concerns are the following:</p>
<p>1. The authors use &quot;approach&quot; as it has been used in other publications, but what is actually measured is dwell time. Pomerantz et al, 1983 observed that female mice approached mute and singing males the same number of times (e.g. approached both at the same rate), but spent more time with the singing than the mute male. Their use of &quot;approach&quot; to describe dwell time was a bit confusing to me, but sticking with the way the literature is defensible. However, they also refer to the assay as a &quot;place preference assay&quot;, which I found confusing.</p>
<p>2. I am a bit worried about their method of removing side bias (29% of trials). It certainly seems like a reasonable thing to exclude mice that simply picked one side or the other, but, because the stimulus always alternated between the sides, this exclusion of mice exhibiting a side bias is also excluding, specifically, behavior that would be incorrect.</p>
<p>3. Given the observation by Hammerschmidt et al, 2009, that female mice would only discriminate male songs in a playback assay on the first presentation, it is important to know whether females were used across the different manipulations. How many conditions did each female experience? How often did a female display positive discrimination in a condition after having displayed no discrimination?</p>
<p>Specific comments:</p>
<p>1. For Figure 2L</p>
<p>The heat map legend is labeled &quot;Towards&quot; indicating a motion towards either the speaker playing the song or the silent speaker. However, there is nothing in the methods that indicates that the direction of movement was ever measured. I may have missed it, but I can't figure out how this heat map was generated and what it represents. The figure legend states: &quot;Normalized temporal profiles of approach behaviour to mouse songs vs silence over the course of 4 sound presentation trials (x-axis, coloured bars) for each of the behavioural sessions (y-axis, each animal is one line, n = 29), calculated as in I. Sessions (lines) are ordered by the amplitude of their last element.&quot; 2I states &quot; I. Temporal profile of approach behaviour over the four sound presentation trials in the example session in C, calculated as the cumulative sum of time in the intact song playback (positively weighted) vs silent (negatively weighted) speaker zone.&quot; I interpret this to mean that &quot;Towards&quot; is an inaccurate description of what is being plotted, as there is no motion, only dwell time.</p>
<p><bold>References</bold></p>
<p>K. Hammerschmidt, K. Radyushkin, H. Ehrenreich &amp; J. Fischer (2009) Female mice respond to male ultrasonic 'songs' with approach behavior. Biol. Lett. 5:589-592.</p>
<p>Pomerantz, S.M., Nunez, A.A. &amp; Bean, J (1983) Female behavior is affected by male ultrasonic vocalizations in house mouse. Physiol. Behav. 31:91-96.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.86464.1.sa4</article-id>
<title-group>
<article-title>Author Response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Perrodin</surname>
<given-names>Catherine</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4915-5113</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Verzat</surname>
<given-names>Colombine</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5796-0924</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Bendor</surname>
<given-names>Daniel</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>We would like to thank the editor and the three reviewers for their time and effort taken in reviewing our manuscript and providing constructive feedback. Unfortunately, the first author of this manuscript is no longer involved in academia, and does not wish to further revise this manuscript.  However, we agree with the entirety of the feedback and critiques provided by the referees, and feel these points should be taken into account when interpreting our results and conclusions.</p>
</body>
</sub-article>
</article>