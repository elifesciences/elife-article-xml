<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108403</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108403</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108403.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Genetics and Genomics</subject>
</subj-group>
</article-categories><title-group>
<article-title>A retrospective analysis of 400 publications reveals patterns of irreproducibility across an entire life sciences research field</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2677-6574</contrib-id>
<name>
<surname>Lemaitre</surname>
<given-names>Joseph</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>jo.lemaitresamra@gmail.com</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Popelka</surname>
<given-names>Désirée</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3366-6231</contrib-id>
<name>
<surname>Ribotta</surname>
<given-names>Blandine</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0000-0013-8980</contrib-id>
<name>
<surname>Westlake</surname>
<given-names>Hannah</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5591-3718</contrib-id>
<name>
<surname>Chakrabarti</surname>
<given-names>Sveta</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Xiaoxue</surname>
<given-names>Li</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6125-3672</contrib-id>
<name>
<surname>Hanson</surname>
<given-names>Mark A</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1357-1315</contrib-id>
<name>
<surname>Jiang</surname>
<given-names>Haobo</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6973-3232</contrib-id>
<name>
<surname>Di Cara</surname>
<given-names>Francesca</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4795-4074</contrib-id>
<name>
<surname>Kurant</surname>
<given-names>Estee</given-names>
</name>
<xref ref-type="aff" rid="a8">8</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8539-865X</contrib-id>
<name>
<surname>David</surname>
<given-names>Fabrice</given-names>
</name>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7970-1667</contrib-id>
<name>
<surname>Lemaitre</surname>
<given-names>Bruno</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<email>bruno.lemaitre@epfl.ch</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0130frc33</institution-id><institution>Department of Epidemiology, Gillings School of Global Public Health, University of North Carolina at Chapel Hill</institution></institution-wrap>, <city>Chapel Hill</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Global Health Institute, School of life science, EPFL</institution></institution-wrap>, <city>Lausanne</city>, <country country="CH">Switzerland</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02xzytt36</institution-id><institution>Manipal Institute of Regenerative Medicine, Bengaluru, Manipal Academy of Higher Education</institution></institution-wrap>, <city>Manipal</city>, <country country="IN">India</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/023b72294</institution-id><institution>College of Plant Science and Technology, Huazhong Agricultural University</institution></institution-wrap>, <city>Wuhan</city>, <country country="CN">China</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03yghzc09</institution-id><institution>Centre for Ecology and Conservation, University of Exeter, Penryn Campus</institution></institution-wrap>, <city>Penryn</city>, <country country="GB">United Kingdom</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01g9vbr38</institution-id><institution>Department of Entomology and Plant Pathology, Oklahoma State University</institution></institution-wrap>, <city>Stillwater</city>, <country country="US">United States</country></aff>
<aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01e6qks80</institution-id><institution>Dalhousie University, Department of Microbiology and Immunology</institution></institution-wrap>, <city>Halifax</city>, <country country="CA">Canada</country></aff>
<aff id="a8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f009v59</institution-id><institution>Department of Human Biology, Faculty of Natural Sciences, University of Haifa</institution></institution-wrap>, <city>Haifa</city>, <country country="IL">Israel</country></aff>
<aff id="a9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>BioInformatics Competence Center of EPFL-UNIL, School of Life Sciences, EPFL</institution></institution-wrap>, <city>Lausanne</city>, <country country="CH">Switzerland</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Rodgers</surname>
<given-names>Peter</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8332-936X</contrib-id><role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>eLife</institution>
</institution-wrap>
<city>Cambridge</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Rodgers</surname>
<given-names>Peter</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8332-936X</contrib-id><role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>eLife</institution>
</institution-wrap>
<city>Cambridge</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2026-01-19">
<day>19</day>
<month>01</month>
<year>2026</year>
</pub-date>
<volume>15</volume>
<elocation-id>RP108403</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-09-24">
<day>24</day>
<month>09</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-07-09">
<day>09</day>
<month>07</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.07.07.663460"/>
</event>
</pub-history>
<permissions>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">
<ali:license_ref>https://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref>
<license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108403-v1.pdf"/>
<abstract><p>The <italic>ReproSci</italic> project retrospectively analyzed the reproducibility of 1006 claims from 400 papers published between 1959 and 2011 in the field of <italic>Drosophila</italic> immunity. This project attempts to provide a comprehensive assessment, 14 years later, of the replicability of nearly all publications across an entire scientific community in experimental life sciences. We found that 61% of claims were verified, while only 7% were directly challenged (not reproducible), a replicability rate higher than previous assessments. Notably, 24% of claims had never been independently tested and remain unchallenged. We performed experimental validations of a selection of 45 unchallenged claim, that revealed that a significant fraction (38/45) of them is in fact non-reproducible. We also found that high-impact journals and top-ranked institutions are more likely to publish challenged claims. In line with the reproducibility crisis narrative, the rates of both challenged and unchallenged claims increased over time, especially as the field gained popularity. We characterized the uneven distribution of irreproducibility among first and last authors. Surprisingly, irreproducibility rates were similar between PhD students and postdocs, and did not decrease with experience or publication count. However, group leaders, who had prior experience as first authors in another <italic>Drosophila</italic> immunity team, had lower irreproducibility rates, underscoring the importance of early-career training. Finally, authors with a more exploratory, short-term engagement with the field exhibited slightly higher rates of challenged claims and a markedly higher proportion of unchallenged ones. This systematic, field-wide retrospective study offers meaningful insights into the ongoing discussion on reproducibility in experimental life sciences.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00yjd3n13</institution-id>
<institution>Swiss National Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>310030_189085</award-id>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01rvn4p91</institution-id>
<institution>ETH Domain</institution>
</institution-wrap>
</funding-source>
<award-id>Open Research Data (ORD) Program (2022)</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>The text has been updated for clarity and the addition of some references. No change in figures and conclusion.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Science predominantly thrives on the continuous aggregation of knowledge, with researchers progressively building upon one another’s discoveries. As results are replicated (when independent studies reach the same conclusions as the original authors of a claim), confidence in the evidence supporting the claim grows. Confidence in publishing results is critical to sustain trust within a scientific community and to allow new research to build on previous findings (<xref ref-type="bibr" rid="c20">Fidler and Wilcox, 2018</xref>). Over the past decades, this confidence has been on a decline.</p>
<p>Several recent reports point out that reproducibility is lower than desirable in molecular life sciences (<xref ref-type="bibr" rid="c2">Amaral et al., 2025</xref>; <xref ref-type="bibr" rid="c6">Begley and Ioannidis, 2015</xref>; <xref ref-type="bibr" rid="c19">Fidler et al., 2017</xref>; <xref ref-type="bibr" rid="c26">Ioannidis et al., 2009</xref>; <xref ref-type="bibr" rid="c32">Macleod et al., 2014</xref>). In 2016, an extensive survey indicated that more than 70% of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments (<xref ref-type="bibr" rid="c4">Baker, 2016</xref>). Another study suggested that about 85% of biomedical research is ‘wasted’, meaning that inefficiencies and methodological flaws throughout the research process undermine the value of scientific findings (<xref ref-type="bibr" rid="c6">Begley and Ioannidis, 2015</xref>; <xref ref-type="bibr" rid="c9">Chalmers and Glasziou, 2009</xref>). In addition, 52% of scientists that were surveyed by Nature agree that we are currently facing a significant ‘crisis’ of replicability. Ongoing concerns about replicability in academic research carry important consequences for the private sector. Two pharmaceutical companies, Bayer and Amgen, reported replicability rates of 11% and 25% in two independent efforts to reproduce findings from dozens of groundbreaking basic studies in cancer and related areas (<xref ref-type="bibr" rid="c38">Prinz et al., 2011</xref>). Moreover, an extensive reproducibility project in cancer biology reveals that a very significant fraction of findings reported in high profile journals cannot be replicated or have smaller effect size that initially reported (<xref ref-type="bibr" rid="c15">Errington et al., 2021</xref>, <xref ref-type="bibr" rid="c16">2014</xref>). Lack of replicability can have multiple causes, ranging from the presence cryptic factors not accounted for by authors, misuse of technical tools, spoiled reagents, bias in data reporting, lack of statistical power or to even outright fraud (<xref ref-type="bibr" rid="c13">Devezer and Buzbas, 2023</xref>; <xref ref-type="bibr" rid="c14">Eisner, 2018</xref>; <xref ref-type="bibr" rid="c31">Lesperance and Broderick, 2021</xref>; <xref ref-type="bibr" rid="c35">Peng, 2015</xref>).</p>
<p>Misleading or erroneous findings can disrupt scientific progress, wasting valuable time and resources as researchers attempt to replicate or use the results of a published article (<xref ref-type="bibr" rid="c43">Stern et al., 2014</xref>). In some cases, entire research initiatives may be built upon findings that are later discovered to be inaccurate or flawed (<xref ref-type="bibr" rid="c36">Piller, 2022</xref>). This might also result in unfair career outcomes if sensational but irreproducible results are rewarded over rigorous studies, especially in high-impact publications (<xref ref-type="bibr" rid="c29">Lemaitre, 2016</xref>; <xref ref-type="bibr" rid="c42">Steneck, 2011</xref>). However, zero irreproducibility is neither realistic nor necessarily desirable. A certain level of irreproducibility may reflect scientific boldness and risk-taking. Cutting-edge explorations of the unknown are expected to involve occasional backtracking as the bigger picture gradually comes into focus. Thus, striving for perfect replicability could paradoxically stifle innovation and promote overly conservative science. Determining the ideal balance is currently an open question (<xref ref-type="bibr" rid="c41">Shiffrin et al., 2018</xref>).</p>
<p>Considering the prominent role played by science in contemporary society, there is a growing need to understand how academic incentives influence the reliability of published results. Indeed, the field of metascience - the scientific investigation of science itself - is gaining popularity (<xref ref-type="bibr" rid="c21">Fortunato et al., 2018</xref>). Analyzing how replicability affects the scientific enterprise is required to better characterize how science progresses despite mistakes. It can also help to influence policy to improve science replicability, or at least to limit its most detrimental impacts.</p>
<p>Research focusing on the immune system of <italic>Drosophila</italic> flourished in the early 1990s and is still an active field of investigation. It boasts two thousand articles published since its beginning, dealing with various facets of the immune response of this insect, such as humoral immunity, cellular immunity, response to pathogens, immuno-metabolism, and evolution of the immune system (<xref ref-type="bibr" rid="c49">Westlake et al., 2024</xref>). While the field remains modest in size, it gained visibility in mid 1990s when researchers realized that some mechanisms regulating the <italic>Drosophila</italic> immune defense are conserved in mammals, including humans. The discovery of Toll-like receptors, stemming from the identification of Toll’s role in the <italic>Drosophila</italic> immune response (<xref ref-type="bibr" rid="c30">Lemaitre, 2004</xref>), established the value and utility of the field to immunological research, a finding that was acknowledged by a Nobel prize awarded to Jules Hoffmann in 2011.</p>
<p>The medium size of this research field and the lack of direct biomedical influence are ideal for a metascience analysis. The <italic>Drosophila</italic> immunity field is substantially smaller than other fields of research such as HIV, microbiota or cancer, where the rapid accumulation of articles forbids any comprehensive evaluation of scientific claims by an individual or a laboratory. With fewer direct translational or biomedical influences, we expect scientists working on <italic>Drosophila</italic> immunity to be less affected by external biases, and to prioritize recognition by peers. Moreover, <italic>Drosophila</italic> research is generally perceived as solid, because it relies predominantly on its powerful genetics and well-controlled experimental conditions, so the field is particularly interesting for this analysis.</p>
<p>In the framework of the <italic>ReproSci</italic> project, we retrospectively analyzed the replication of 400 articles published between 1959 and 2011 in the field of <italic>Drosophila</italic> immunity research-that is, nearly all the major articles published by a community of scientists in experimental life science over 50 years (<xref ref-type="bibr" rid="c48">Westlake et al., 2025</xref>). For each article, we identified the main, major, and minor scientific conclusions, hereafter referred to as “claims”. The 14-year interval between the time the latest article was published and our study, during which scientific knowledge has advanced and other laboratories have had the opportunity to attempt replication, provides the necessary perspective to efficiently assess the validity of the claims. In a first step, we created a publicly accessible website, displaying all the information on the articles, the claims, and their validity (<ext-link ext-link-type="uri" xlink:href="https://ReproSci.epfl.ch/">https://ReproSci.epfl.ch/</ext-link>). Second, we contacted the <italic>Drosophila</italic> scientific community to provide feedback via this website on the replicability of the claims. In a third step, several teams attempted to replicate 45 major and 11 minor claims that remained unchallenged in the literature at the time. The present article focuses on the meta-science findings, while the scientific consequences of the <italic>ReproSci</italic> project in the <italic>Drosophila</italic> immunity field are presented in a companion article (<xref ref-type="bibr" rid="c48">Westlake et al., 2025</xref>).</p>
<p>This article analyzes the pattern of replicability with two objectives: first, to comprehend the level of replicability within the experimental life science community, and second, to examine whether instances of non-replicability tend to aggregate among journals, first authors, and lead researchers. Such clustering may identify individual characteristics and environmental factors that influence science replicability. We open the Results section with a detailed exploratory analysis of the effect of individual categorical variables on reproducibility outcomes. Then, to assess the joint influence of all covariates and account for the clustering of multiple claims by the same author, we fit a multivariate model to account for confounding effects across our covariates. Because challenged claims are rare and the overall sample is modest, statistical power remains limited. However, taken together, i) the exploratory and ii) multivariate analyses form a single narrative, the first describing exhaustively the observed patterns, and the second testing the robustness of the associations.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Methodology and claim assessment</title>
<p>Scientific claims were divided into three categories. The main (one per article), major (3-4) and minor (2-4) claims were manually extracted from 400 articles in the field of <italic>Drosophila</italic> immunity published before 2011. We then assessed the claims of each article to determine if they were supported or refuted by subsequent publications. The claims were manually categorized by H.W. and B.L. into four broad groups: i) verified by subsequent studies (when later studies confirmed the claim), ii) challenged (when later studies contradicted or disproved the claim), iii) unchallenged (when no later study addressed or attempted to replicate the claim), and iv) partially verified or mixed (when later studies did not lead to a definitive conclusion (see <xref rid="tbl1" ref-type="table">Table 1</xref> in Methods for classification and <xref ref-type="bibr" rid="c48">Westlake et al., 2025</xref>). Then, experimental work was performed in several laboratories to test the validity of 45 major and 11 minor unchallenged claims using alternate methods. These claims subjected to experimental validation were selected according to criteria such as appearing suspicious due to the absence of direct follow up or being straightforward to test experimentally. Our data and findings were put on a community-accessible website that opened to the public in early July 2023. The present article focusses on the pattern of irreproducibility and statistical analyses were performed on the 1006 major claims only (referred to as claims below) of the database as updated on December 20<sup>th</sup> 2024. In this article, challenged claims will interchangeably be called irreproducible. Our project focuses on assessing the strength of the claims themselves (i.e. indirect reproducibility or conceptual replicability) rather than testing whether the original methods produce repeatable results (direct reproducibility/replicability). Thus, our conclusions do not directly challenge the initial results leading to a claim, but rather the general applicability (i.e. generalizability) of the claim itself (<xref ref-type="bibr" rid="c23">Goodman et al., 2016</xref>; <xref ref-type="bibr" rid="c33">Meng, 2020</xref>).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Verification categories used to assess claims.</title></caption>
<graphic xlink:href="663460v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="663460v2_tbl1a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2b">
<title><italic>Drosophila</italic> immunity claims are mostly replicable</title>
<p>After an extensive literature review to assess the validity of claims published from 1959 to 2011, combined with experimental testing of 45 major claims, we found that approximately 61% of major claims (610 out of 1006 across the 400 selected articles) were fully verified (<xref rid="tbl2" ref-type="table">Table 2</xref>). Among these 610 verified claims, 91.7% were corroborated by independent laboratories, 7.2% were confirmed by the same laboratory, and 0.7% were verified through experimental work conducted as part of the <italic>ReproSci</italic> project. This verification rate is higher than that observed in other fields where reproducibility has been systematically assessed (<xref ref-type="bibr" rid="c2">Amaral et al., 2025</xref>; <xref ref-type="bibr" rid="c3">Amaral et al., 2019</xref>; <xref ref-type="bibr" rid="c10">Da Costa et al., 2022</xref>; <xref ref-type="bibr" rid="c15">Errington et al., 2021</xref>; <xref ref-type="bibr" rid="c19">Fidler et al., 2017</xref>; <xref ref-type="bibr" rid="c50">Youyou et al., 2023</xref>). These results may reflect the robust scientific standards and methodological rigor of <italic>Drosophila</italic> research, which benefits from the availability of diverse and powerful genetics approaches and from a collaborative culture of rapid reagent sharing, that facilitates replication across laboratories.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><title>Proportion of claims in broad reproducibility assessment categories before and after experimental validation by the <italic>ReproSci</italic> project.</title><p>See description of each category in Material and Methods. In this article, we are using indirect reproducibility, also called conceptual replicability, to assess the validity of a claim using alternate methods to those used in the original study.</p></caption>
<graphic xlink:href="663460v2_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>We also observed that 23.8% of major claims had no published follow-up (<xref rid="tbl2" ref-type="table">Table 2</xref>, <xref rid="fig1" ref-type="fig">Figure 1</xref>); these unchallenged claims are particularly interesting and will be discussed in detail in the next section. Additionally, 7.5% of the claims were categorized as partially verified. This category encompasses cases where insightful data were accompanied by incomplete interpretations, or where incomplete data were paired with insightful interpretations. Importantly, 6.8% of claims (69 out of 1006) were challenged. Among these, 44.9% (31 out of 69) were contested by published articles including 7.2% by the same authors, and 55.1% (38 out of 69) were challenged experimentally as part of the <italic>ReproSci</italic> project. Thus, the experimental validation performed during this project doubled the number of reported challenged claims. Finally, 1.2% of major claims were classified as “mixed” (<xref rid="tbl2" ref-type="table">Table 2</xref>). This category includes claims that presented particularly challenging issues (e.g., conflicting data), making it impossible to definitively categorize them as either verified or challenged.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Flow of the study.</title>
<p>Claims were first classified in five categories based on literature. 45 unchallenged claims were selected to be experimentally tested an classified either as challenged or as verified. We further classify some unchallenged as <italic>consistent</italic> or <italic>inconsistent</italic> depending on their consistency with current knowledge.s</p></caption>
<graphic xlink:href="663460v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In conclusion, contrary to the prevailing “reproducibility crisis” narrative, a significant proportion of the findings published in the field of <italic>Drosophila</italic> immunity before 2011 has been subsequently verified by independent studies.</p>
</sec>
<sec id="s2c">
<title>A significant fraction of unchallenged claims is non-reproducible</title>
<p>Our analysis highlights a significant proportion of “unchallenged” claims that were not followed by subsequent studies and whose validity was unknown prior to our experimental evaluation. Most reproducibility studies have focused on verified or challenged claims so little is known about the vast number of statements generated in a scientific community without subsequent direct validation.</p>
<p>Despite the lack of validation, these “unchallenged claims” are often assumed valid and contribute to the broader construction of scientific knowledge. An important challenge is to estimate the percentage of unchallenged major claims that are actually non-reproducible.</p>
<p>Prior to our experimental validation, we identified a total of 285 unchallenged claims, representing 28.3% of all major claims published up to 2011. We conducted experimental evaluations of 45 unchallenged major claims with the assistance of multiple laboratories (<xref ref-type="bibr" rid="c48">Westlake et al., 2025</xref>). The selection of unchallenged claims for testing was intentionally biased toward two categories: (i) those that appeared suspicious in light of the existing literature (31 claims), and (ii) those that were relatively straightforward to test experimentally (14 claims). Among the 45 unchallenged major claims tested, 84% (38 out of 45) could not be experimentally reproduced and were then reclassified as “challenged by the <italic>ReproSci</italic> project.” Conversely, only 16% (7 out of 45) were successfully reproduced and reclassified as “verified by the <italic>ReproSci</italic> project” (<xref rid="fig1" ref-type="fig">Figure 1</xref>).</p>
<p>Notably, all 31 suspicious unchallenged claims were challenged, while 7 out of the 14 unchallenged “easy to test” claims were also challenged. Excluding the 45 unchallenged major claims that were experimentally tested, we categorized the remaining 240 unchallenged claims into three groups: 1) unchallenged logically consistent – claims not directly tested but consistent with current knowledge of the field (111 claims); 2) unchallenged logically inconsistent – claims that appear inconsistent with current knowledge (22 claims); 3) Unclassified – claims that could not be definitively categorized (107 claims) (<xref rid="fig1" ref-type="fig">Figure 1</xref>). In the remainder of the text, we analyze the claims following <italic>ReproSci</italic> experimental validation, labelling the 45 selected claims as either challenged or verified.</p>
<p>This analysis reveals that a substantial number of claims remain unchallenged even a decade after publication. Moreover, our findings suggest that a considerable proportion of these claims may not be reproducible.</p>
</sec>
<sec id="s2d">
<title>Higher representation of challenged claims in trophy journals and from top universities</title>
<p>Articles published in high-impact journals receive high visibility, a key factor in career progression and funding security (<xref ref-type="bibr" rid="c46">van Wesel, 2016</xref>). Additionally, high-impact publications are increasingly used as metrics to evaluate university rankings. Consequently, scientists face significant internal and external pressures to publish in such prestigious journals (<xref ref-type="bibr" rid="c45">Udesky, 2025</xref>). On the other hand, high-impact journals aim to attract the most surprising findings, which are inherently more likely to be challenged than more conventional studies. Based on this, we hypothesized that articles published in high-impact journals are more likely to include non-reproducible results. To test this hypothesis, we estimated the number of challenged claims published in journals categorized as having low or medium-impact factors (&lt;10), high-impact factors (&gt;10 and &lt;50), and “trophy journals” of the highest reputation (&gt;50: <italic>Science</italic>, <italic>Nature</italic>, and <italic>Cell</italic>) in the <italic>Drosophila</italic> immunity at that time. The results are summarized in <xref rid="fig2" ref-type="fig">Figure 2</xref>, with a more detailed breakdown by journal provided in <xref ref-type="supplementary-material" rid="supp1">Table S1</xref>. Compared with low-impact journals, where 5.8 % of major claims were challenged, the proportion rose to 7.8 % in high-impact journals (odds ratio [OR] = 1.36, 95 % confidence interval [CI] 0.80–2.32) and more than doubled to 12.9 % in trophy journals (OR = 2.39, 95 % CI 1.06–5.40). We recall that the OR expresses how much more (OR &gt; 1) or less (OR &lt; 1) likely a claim is to be challenged relative to articles published in low-impact journals, while the 95 % CI indicates the range of values compatible with random sampling error; OR intervals that exclude 1 denote a statistically significant effect. Thus, our analysis show major claims published in low-impact journals are significantly more likely to be reproducible than major claims published in trophy journals. One explanation for this trend is that claims published in high-impact journals are more frequently subjected to replication attempts or used as a foundation for further research, thereby increasing the likelihood of identifying irreproducible results. Indeed, low-impact journals include a higher share of claims remaining unchallenged (29.3 %, 95 % CI 25.9–32.9 %) compared to high-impact journals (13.9 %, 95 % CI 10.5–18.2 %) and to trophy journals (17.7 %, 95 % CI 10.2–29.0 %). However, variation in the number of unchallenged claims does explain the higher proportion of challenged claims in trophy journals compared to high-impact journals. If we take into consideration the percentage of verified, unchallenged and challenged claims, <xref rid="fig2" ref-type="fig">Figure 2B</xref> shows that high-impact but not trophy or low-impact journals contain the highest proportion of verified claims. Thus, we conclude that while trophy journals may indeed attract some of the best science, they also appear more likely to include irreproducible claims.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Percentage of claims by category according to journal-impact and institution ranking.</title>
<p>(<bold>A</bold>, <bold>B</bold>) The distribution of claims across the five assessed categories for articles published in low-impact, high-impact, and trophy journals. The number of claims is indicated at the top. Panel A presents raw counts, while Panel B shows normalized data. <bold>(C)</bold> The distribution of claims across the five assessed categories according to the ranking of the last author institution in Shanghai Ranking’s 2010 Academic Ranking of World Universities. The number of claims is indicated at the top, while the number of institutions in each category is indicated at the bottom. The Not Ranked categories encompass research institutes (ex. CNRS), hospitals and other institution that are not included in the Shanghai ranking. In this figure, and in the rest of the paper, the number n<sub>c</sub> indicates the number of claims in each category, while the number at the bottom highlight the count in each category (e.g there are 11 high-impact journals, and 73 Top-50 institutions).</p></caption>
<graphic xlink:href="663460v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We then investigated whether the percentage of irreproducibility varies among laboratories affiliated according to the prestige of their affiliate universities. To test this hypothesis, we classified all the major claims published according to the Shanghai ranking of the last author university. <xref rid="fig2" ref-type="fig">Figure 2C</xref> shows that claims from laboratories at top-50 universities were challenged far more often 15.8 % (95 % CI 11.4–21.6 %) than other institutions, up to a four-fold increase in the odds of being questioned (OR = 4.69, 95 % CI 2.53–8.70) versus 3.9 % for unranked institutions, which notably include hospitals and research institutes such as the French CNRS. Even after adjusting for journal tier (<xref ref-type="supplementary-material" rid="supp1">see model SI1</xref>), university prestige remains the dominant signal: claims from top-50 institutions are 3.6 times more likely to be challenged than those from unranked universities (OR 3.57, 95 % HDI 1.67–7.92). By comparison, publishing in a “trophy” journal adds only a notable and uncertain risk (OR 1.63, 0.66–3.88), while other journal or ranking bands show no clear effect. Put simply, elite affiliation more than journal prestige best predicts irreproducibility in our sample.</p>
</sec>
<sec id="s2e">
<title>The irreproducibility rate has increased over time as the field has grown in popularity</title>
<p>The reproducibility crisis narrative suggests that irreproducibility has increased over recent decades, posing a significant challenge to the life sciences (<xref ref-type="bibr" rid="c4">Baker, 2016</xref>; <xref ref-type="bibr" rid="c25">Hunter, 2017</xref>). Our dataset spans 50 years, from the onset of <italic>Drosophila</italic> immunity research as a relatively marginal field to its raise to prominence, with a notable increase in publications since the mid-1990s driven by growing interest in innate immunity. We analyzed fluctuations in irreproducibility over time by grouping major claim assessments into time windows: pre-1991, followed by 5-year intervals until 2011. The percentage of irreproducibility was calculated for each time window (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Across the five windows examined, the proportion of challenged claims climbed steadily from 0% in the small corpus prior to 1991 (0/29 articles), to 4.6% during 1992–1996, and then further increased to 7.7% between 1997 and 2001. This upward trend peaked at 8.3% during the 2002–2006 period, before slightly declining to 6.1% between 2007 and 2011 (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Despite these apparent temporal shifts, none of the pairwise comparisons between periods reached statistical significance, with p-values consistently ≥ 0.15. Thus, while the trends highlighted in our study are consistent with a raise of irreproducibility, we cannot exclude that the observed upward trajectory might reflect sampling variation rather than a definitive sustained temporal trend. Further studies focusing on articles published after 2011 could clarify whether the mild decrease we observed marks the beginning of a downward trend or reflects fluctuations at a consistently high level.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Evolution of claim replicability over time.</title>
<p><bold>(A, B)</bold> The distribution of claims by categories of assessment over time is shown in A, and the proportion normalized over the total number of claims per time period is shown in B.</p></caption>
<graphic xlink:href="663460v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Interestingly, we observed a peak in verified claims between 1992 and 2001, accompanied by a lower percentage of unchallenged claims (<xref rid="fig3" ref-type="fig">Figure 3</xref>). This period coincided with the early expansion of the field when many researchers worked on similar research questions with a relatively narrow conceptual framework of innate immunity. The subsequent increase in unchallenged claims may reflect the rapid conceptual expansion of the field, which likely outpaced the growth in the number of researchers, reducing opportunities for overlapping or validating research efforts. An alternative explanation for this trend is that in the early stages of the field, researchers uncovered the largest, easily detectable effects. As the field matured, focus shifted to subtler phenotypes and complex regulatory mechanisms, making replication more challenging.</p>
<p>Overall, our time-course analysis suggests that irreproducibility has increased over the decades, aligning with the reproducibility crisis narrative. However, an alternative hypothesis is that irreproducibility varied alongside the field’s popularity, peaking once when the field became popular and had attracted newcomers who may not have been fully acquainted with its established standards or who joined the field by opportunistically. Furthermore, the rise in the percentage of challenged claims was accompanied by an increase in unchallenged claims, indicating a reduced degree of validation within the field.</p>
</sec>
<sec id="s2f">
<title>First-author patterns of irreproducibility</title>
<p>Having analyzed the pattern of irreproducibility across a large number of articles, representing contributions from multiple laboratories over five decades, we next investigated the distribution of irreproducibility among authors and laboratories. A central question is whether irreproducibility is evenly distributed across the scientific community, or if hubs — specific authors or laboratories — are responsible for producing more invalid statements. First, we explored whether reproducibility is associated with the first author, who typically conducts the work, or with the last author, who manages the laboratory and may foster a research environment that produces irreproducible results.</p>
<p>To explore the relationship between irreproducibility and first authorship, we computed the percentage of “challenged” articles attributed to each first author. Additional information on first author was obtained through interviews with principal investigators (PIs) or via public LinkedIn profiles, including variables such as gender and position in the laboratory.</p>
<p>In total, 289 first authors contributed to the 400 articles analyzed by the <italic>ReproSci</italic> project, with an average of 3.45 claims per author. Notably, 47 authors (16.3%, 95% CI: 11.4–19.6%) published at least one non-reproducible claim, with one author reaching up to 83% irreproducibility (5 challenged claims out of 6 from two articles). <xref rid="fig4" ref-type="fig">Figure 4A</xref> displays the distribution of challenged claims among first authors. A statistical dispersion analysis revealed a Gini index of 0.88 (inequality index ranging from 0 -perfect equality- to 1 -extreme inequality-), where the top 10% of first authors accounted for 73.9% of all challenged claims, and the top 20% accounted for all 100% of challenged claims (<xref rid="fig4" ref-type="fig">Figure 4B</xref>), suggesting a highly unequal distribution of irreproducibility according to first author.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Distribution of irreproducibility according to first authors.</title><p>A) Distribution of irreproducible claims among first author. We note that all irreproducible claims are produced by less than 20% of the first authors. B) Distribution of irreproducibility for all first authors. Each dot represents an author, ranked according to the proportion of challenged claims. The size of the circle indicates the number of claims and the color the percentage of verified claims.</p></caption>
<graphic xlink:href="663460v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We next investigated whether irreproducibility varied by the gender of the first author. The proportions of challenged articles were similar for male and female first authors: 7.3% and 6.7%, respectively, and this difference was not statistically significant (OR = 1.09, 95% CI: 0.66–1.78, Fisher’s exact p = 0.802; <xref rid="fig5" ref-type="fig">Figure 5A</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Influence of parameters associated with first author on reproducibility.</title>
<p>A) Repartition of claims in each assessment category according to the gender of the first authors. The gender of four authors could not be inferred based on first name ambiguity explaining why the total number of authors is 285. B) Repartition of claims in each assessment category according to status of the first authors. The stages of career of 279 first authors were obtained by interviewing last authors, querying Linkedln public profiles or extrapolated according to the publication list. Senior staff refers to permanent position scientist working in a lab. ND stands for Not Determined. C) Proportion of challenged claims as a function of the number of published articles for first-authors. The color indicates the percentage of claims that are verified.</p></caption>
<graphic xlink:href="663460v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We were able to determine the status of most of first authors at the time of the article was published, most being PhD students (n=168) or postdoctoral researchers (n=87), with smaller numbers identified as technicians (n=5), Masters students (n=5), PIs (n=8), or permanent staff (n=6) (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). No major differences in irreproducibility were observed between PhD students and post-docs. While we did observe a higher proportion of challenged claims among PIs and Master’s students, and a lower proportion among technicians and permanent staff, these trends are difficult to interpret due to small sample sizes.</p>
<p>Finally, we tested the relation between the number of articles published by a first author and the proportion of irreproducible research. No significant trend was observed (linear regression; r<sup>2</sup>=0.01, p=0.088; <xref rid="fig5" ref-type="fig">Figure 5C</xref>), suggesting the level of author productivity is independent of irreproducibility. In summary, we observed considerable variability in irreproducibility among first authors, suggesting that individual authors may substantially contribute to reproducibility issues.</p>
</sec>
<sec id="s2g">
<title>Lead authors pattern of irreproducibility</title>
<p>We now turn to the question of whether irreproducibility varies according to laboratory. As it is common practice in biology, we assigned the last author (referred to as PI for principal investigator) to represent the laboratory. The 400 articles analyzed were co-authored by 156 leading authors from various institutions, predominantly from France, Sweden, Japan, South Korea, and the USA. For each last author, we estimated the number of major claims in each of the five categories. The variation in irreproducibility across these PIs is shown in <xref rid="fig6" ref-type="fig">Figure 6A</xref>. A statistical dispersion analysis in <xref rid="fig6" ref-type="fig">Figure 6B</xref> revealed a Gini coefficient of 0.856: the top 10% of all last authors accounted for 71.0% of all challenged claims, while the top 20% accounted for 94.2%, suggesting a highly unequal distribution of irreproducibility according to laboratory. We next explored whether there were differences in irreproducibility between male and female last authors. The percentage of challenged claims was slightly higher for male last authors (7.0%, 95% CI: 7.0%–7.1%) compared to female last authors (5.8%, 95% CI: 5.7%–6.0%), but this difference was not statistically significant (OR = 1.22, 95% CI: 0.59–2.51; Fisher’s exact p = 0.73; <xref rid="fig7" ref-type="fig">Figure 7A</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Distribution of irreproducibility according to last authors (PI).</title>
<p>A) Distribution of irreproducible claims among leading authors. B) Distribution of irreproducibility for all leading authors. Each dot represents a leading author, ranked according to the proportion of challenged claims. The size of the circle indicates the number of claims and the color the percentage of verified claims.</p></caption>
<graphic xlink:href="663460v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Influence of various parameters associated with the last author on reproducibility.</title>
<p>A) Distribution of claims by category according to the gender of the last authors. B) Distribution of claims by category according to the seniority of the last authors. Senior PI were defined as having published a last author article at least 5 years before the considered articles. The total of junior and senior authors is higher than the total of last authors as some PIs published both as junior and senior. C) Proportion of challenged claims by last author as a function of number of articles.</p></caption>
<graphic xlink:href="663460v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Pressure to publish is typically stronger among junior PIs who often lack tenure and must establish their laboratory. To examine whether this pressure might be associated with irreproducibility, we classified PIs as junior or senior based on whether they had published a last-author article at least five years prior to the considered publication. Although <xref rid="fig7" ref-type="fig">Figure 7B</xref> indicates a slightly higher proportion of challenged claims among junior PIs (7.2%, 95% CI: 7.2%–7.3%) compared to senior PIs (6.5%, 95% CI: 6.5%–6.6%), this difference was not statistically significant (OR = 1.12, 95% CI: 0.68–1.83, Fisher’s exact p = 0.70). This modest difference was further attenuated when considering mixed and partially verified claims.</p>
<p>We then explored whether the percentage of irreproducibility tended to decline according to the number of published articles as last author (<xref rid="fig7" ref-type="fig">Figure 7C</xref>). We could hypothesize that PIs that have transiently work on <italic>Drosophila</italic> immunity would have more irreproducible claims, or on the contrary that the most productive laboratories compromise speed over rigor. However, no significant trend was observed between the percentage of challenged claims and the number of articles published by an author (linear regression; r<sup>2</sup>=0.004, p=0.67).</p>
<p>We found that the percentage of irreproducibility has increased over time as the field became more popular. We therefore explored if this increase in irreproducibility affects all PIs or only new PIs that joined the field at a later time. We show in <xref rid="fig8" ref-type="fig">Figure 8A</xref> the proportion of challenged claims by last authors according to the publication date of their first article on <italic>Drosophila</italic> immunity, either as last or as first authors. We observed a significant association between irreproducibility and the period when principal investigators (PIs) established their laboratories. Specifically, PIs, who started their laboratories after 1995, had a significantly higher percentage of challenged claims (8.1%, 95% CI: 8.0%–8.1%) compared to those who began their labs before 1995 (2.6%, 95% CI: 2.6%–2.7%; OR = 3.24, 95% CI: 1.38–7.59, Fisher’s exact p &lt; 0.001).</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8:</label>
<caption><title>Patterns of irreproducibility by last author according to time of starting their laboratory and research style.</title>
<p>A. The proportion of challenged claims by last author is shown according to the time at which the author published their first paper as a first or last authors. The increase in irreproducibility is associated with PIs who started their lab after 1995. Circle size indicates the number of published articles by each author. B. Distribution of claims by last author according to whether they have published at least one first author article on <italic>Drosophila</italic> immunity in another laboratory. C. Distribution of claims according to research style. Proportion of challenged claims by PIs who have continuously work on the <italic>Drosophila</italic> immunity (‘continuity PIs’) or made a transient incursion into the field (‘exploratory PIs’). Only PIs, who published as last authors after 1995, were taken into consideration.</p></caption>
<graphic xlink:href="663460v2_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Among the PIs working in the <italic>Drosophila</italic> immunity field, who established their lab after 1995, we identified 22 who previously published at least one first-author article in this field during their PhD or postdoctoral training in another laboratory. We hypothesized that these authors, having gained prior hands-on experience on <italic>Drosophila</italic> immunity, would be less prone to publishing irreproducible claims. Consistent with this hypothesis, we observed significantly lower levels of irreproducibility among these experienced authors (3.5%, 95% CI: 3.4%–3.5%) compared to authors who had not been trained in laboratories working on <italic>Drosophila</italic> immunity (8.2%, 95% CI: 8.1%– 8.2%; OR = 2.48, 95% CI: 1.19–5.18, Fisher’s exact p = 0.01; <xref rid="fig8" ref-type="fig">Figure 8B</xref>).</p>
<p>We conclude that irreproducibility varies significantly across laboratories with a limited number of laboratories accounting for most of the irreproducible claims. As for first authors, author gender is not significantly associated with reproducibility. Our studies point to a significant role of experience: PIs that had previously worked in another <italic>Drosophil</italic>a immunity laboratory to the extent that they published a first author article in that laboratory were less prone to publish irreproducible claims.</p>
</sec>
<sec id="s2h">
<title>Irreproducibility according to research styles</title>
<p>Among the various career progression strategies, one involves deepening expertise and specialization within the same field, while another entails transitioning to different fields or exploring different career paths. This may reflect the personality of the scientist, either opting for a more detailed, focused approach or seeking continual new challenges. Scientists from the past were often spending their life on a specific topic within a field. In the last decades, changing field has become the norm (<xref ref-type="bibr" rid="c51">Zeng et al., 2019</xref>). Accordingly, scientists are not funded by institutions to work on a topic long-term anymore, but rather funding is granted by agencies for specific short-term projects. Project grants define research with the primary goal to do ‘breakthrough discoveries’, not accumulative research, which is sometimes treated as the ‘pedestrian’ or the ‘stamp collector’ way of doing research. This leads scientists to move quickly into new research fields in search for opportunities of funding and recognition, a nuanced hypothesis supported by the recent description of a “pivot penalty” for scientists that change career tracks (<xref ref-type="bibr" rid="c24">Hill et al., 2025</xref>). We can define these two strategies as either “continuity PIs,” which spend their career on one core topic, and “exploratory PIs”, which regularly venture into new topics. To further explore this question, we classified all last authors that set up their laboratory after 1995 as ‘continuity’ PIs or ‘exploratory’ PIs. Exploratory PIs are defined as those who made a transient incursion in the <italic>Drosophila</italic> immunity field before moving on to another field or leaving academia. In contrast continuity PIs worked continuously in the <italic>Drosophila</italic> immunity field throughout their careers. We observed that continuity PIs had a lower proportion of challenged claims (6.1%, 95% CI: 6.0%–6.2%) compared to exploratory PIs (8.2%, 95% CI: 8.1%–8.3%). However, this difference was not statistically significant (OR = 1.38, 95% CI: 0.84–2.26, Fisher’s exact p = 0.24; <xref rid="fig8" ref-type="fig">Figure 8C</xref>). We also noted a markedly higher share of unchallenged claims among exploratory PIs than among continuity PIs, 32.2 % (95 % CI 32.1– 32.4 %) versus 19.1 % (95 % CI 19.0–19.2 %), respectively. This difference was statistically significant (OR = 2.02, 95 % CI 1.50–2.71; Fisher’s exact <italic>p</italic> &lt; 0.001), reinforcing the idea that researchers, who move on to new topics, tend to leave a larger fraction of their earlier findings without follow-up or validation by the community. Since unchallenged claims are more likely to be irreproducible, this suggests that irreproducibility is higher in the work of PIs with an exploratory style, as opposed to those engaged in a continuity-driven research style.</p>
</sec>
<sec id="s2i">
<title>Multivariable analysis of predictors of claim irreproducibility</title>
<p>We have presented above a comprehensive exploratory data analysis, examining one variable at a time. However, drawing robust conclusions necessitates the joint analysis of all covariates, while appropriately accounting for the non-independence of claims made by the same authors. Thus, we fitted a multivariate Bayesian mixed-effects logistic model to uncover the effect of each covariate, while taking into account all other confounding variables at our disposal (<xref ref-type="bibr" rid="c1">Abril-Pla et al., 2023</xref>; <xref ref-type="bibr" rid="c8">Capretto et al., 2022</xref>)(<xref rid="fig10" ref-type="fig">Figure 10</xref>). The model simultaneously adjusts for author demographics, laboratory attributes and journal or institutional prestige and predict a binary outcome for each claim: challenged or not challenged (includings Verified, Partially Verified, Mixed and Unchallenged) (See in <xref ref-type="supplementary-material" rid="supp1">Supplementary figure S3</xref> a similar model to predict unchallenged or not unchallenged claims). As the claims from the same first or last author are not independent, we included a mixed effect for each first author (289) and each last author (156). These effects capture the influence of a particular author in driving a claim reproducibility. We conducted extensive test on model formulation and convergence (see methods, and Supplementary information for posterior predictive and diagnostics plot). Note that as this analysis is Bayesian, we report the Highest Density Interval (HDI) instead of the confidence interval. We observe that, none of the first-author factors (gender: OR= 0.94, 94% HDI: 0.42–2.16; trainee stage: OR = 1.17, 94% HDI: 0.51–2.87) or leading-author factors (gender, seniority, continuity, or having entered the field before 1995) showed a decisive association with irreproducibility once every other covariate was taken into account (all HDIs span 1). We still observe a trend toward reduction of challenged claims for lead authors having being trained in <italic>Drosophila</italic> immunity or have started their laboratory before 1995. <xref ref-type="supplementary-material" rid="supp1">Figure S3</xref> shows that lead author with an exploratory life style tends to have significantly more unchallenged claims (OR: 2.22, 1.18, 4.55). In contrast, university ranking and journal level covariates dominated the model: articles from top-50 universities were almost four times more likely to contain a challenged claim than those from un-ranked institutions (OR 3.72, 1.55–9.43), and papers in trophy journals (Science, Nature, Cell) carried an elevated uncertain risk (OR 1.76, 0.62–5.00) compared to low-impact journals.</p>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure">
<label>Figure 10.</label>
<caption><title>Multivariable analysis of predictors of claim irreproducibility.</title></caption>
<graphic xlink:href="663460v2_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Intermediate effects for high-impact journals and mid-ranked universities were positive but imprecise. Publication year was modelled with a three-knot B-spline to allow for non-linear time trends. None of the individual spline coefficients differed credibly from zero, indicating that once journal tier and the other covariates were included the residual calendar-year effect on irreproducibility was negligible, indicating that time of publication itself is not a major factor.</p>
<p>The hierarchical component comprised 289 random intercepts for first authors and 156 for PIs. The posterior distribution of these intercepts (see Supplementary Information) is broad, confirming that some individuals or laboratories have a systematically higher or lower propensity for challenged claims. Nevertheless, only the three most extreme first-author effects had 95 % highest-density intervals that excluded zero (and all with positive irreproducibility) meaning that statistically robust clustering of irreproducibility is confined to a handful of contributors; for the vast majority of authors the collected evidence does not support significant differences in reproducibility from what is expected from a random distribution.</p>
<p>Taken together, the model confirms our exploratory data analysis, namely that it is the prestige of institution and publication in trophy journals rather than the personal characteristics of the scientists that best predicts which claims will later prove to be irreproducible.</p>
<p>The forest plot reports the odds ratio and the 94% highest density interval of each covariate effect on a claim becoming challenged.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>The <italic>ReproSci</italic> project presents an in-depth analysis of reproducibility in an experimental science field with a 14-year retrospective period. This initiative focuses specifically on <italic>conceptual replicability</italic> —the ability to independently reproduce key findings based on the information provided— rather than direct reproducibility using the exact same data and conditions (<xref ref-type="bibr" rid="c37">Plesser, 2018</xref>). Replicability is of critical importance to the scientific community who seeks to build upon published claims. One of our project’s key strengths lies in its comprehensive scope: it spans over fifty years of research, from 1959 to 2011, within a specific scientific community, resulting in a robust database that synthesizes the main findings across decades. This time frame encompasses the formative years of the field, characterized by a small number of pioneering laboratories, followed by a period of rapid expansion driven by increased interest, heightened competition, and publication in high-impact journals. This growth phase was marked by a proliferation of knowledge and conceptual innovation, with novel approaches to studying the immune system emerging. With the awarding of the Nobel Prize in 2011 and subsequently the shift toward more translational research, the growth pace of the <italic>Drosophila</italic> immunity field has since stabilized, and the field remains highly active.</p>
<p>Although our study centers on a specific experimental domain at a particular moment in time with a specific geographical development, it reveals broader patterns of irreproducibility that are likely generalizable. These findings contribute meaningfully to the ongoing conversation about the reproducibility crisis in the experimental life sciences (<xref ref-type="bibr" rid="c17">Fanelli, 2018</xref>; <xref ref-type="bibr" rid="c22">França and Monserrat, 2018</xref>; <xref ref-type="bibr" rid="c25">Hunter, 2017</xref>; <xref ref-type="bibr" rid="c35">Peng, 2015</xref>).</p>
<p>Contrary to the more dramatic narratives surrounding the reproducibility crisis (<xref ref-type="bibr" rid="c2">Amaral et al., 2025</xref>; <xref ref-type="bibr" rid="c5">Baker, 2015</xref>; <xref ref-type="bibr" rid="c15">Errington et al., 2021</xref>), our study shows that a substantial proportion of claims—61%— can be considered as verified. The proportion of claims that have been directly challenged remains relatively low (7%), though likely underestimated. This indicates that research in the <italic>Drosophila</italic> immunity field is rather solid, possibly due to the power of <italic>Drosophila</italic> genetics, the collective ethos of sharing within the community, a less competitive environment, and a lack of influence associated with translational research.</p>
<p>A novel and important contribution of our work is the identification of a large fraction of <italic>unchallenged</italic> claims —findings that, despite being published, have never been independently tested. The presence of these unchallenged claims could result from the rapid expansion of the field limiting overlapping research that would validate them. However, the existence of unchallenged claims could also be explained if they are not reproducible. For instance, the absence of follow-up studies could reflect authors that are active in the field being aware of the fragility of specific unchallenged claims, and so not pursuing them. The existence of unchallenged claims could also stem from the reluctance of other researchers to publish contradictory statement due to the stigma of publishing negative results, the so-called “file drawer” effect (<xref ref-type="bibr" rid="c40">Rosenthal, 1979</xref>). Indeed, many scientists report abandoning verification efforts when unable to reproduce a colleague’s findings, fearing personal conflict or professional retaliation (<xref ref-type="bibr" rid="c11">DeCoursey, 2006</xref>; <xref ref-type="bibr" rid="c27">Kannan and Gowri, 2014</xref>; <xref ref-type="bibr" rid="c34">Nissen et al., 2016</xref>). Our experimental validation demonstrates that a significant proportion of unchallenged claims are, in fact, non-reproducible. However, we should note that we did not randomly select the unchallenged claims that were to be experimentally tested, and we favored suspicious claims and claims that were easy to experimentally test. Thus, the true rate of reproducibility of unchallenged claims should not be directly extrapolated from our experimental validation attempts. However, we found that <italic>none</italic> of unchallenged, suspicious claims could be reproduced. While confirmation bias driven by our initial suspicion cannot be entirely excluded, the absence of follow-up in the published literature for over a decade later supports our interpretation that these findings were not robust. Of note, we considered many of these unchallenged claims were considered suspicious based on informal exchanges within research communities, that reported difficulties in reproducing the findings. Most of these non-reproducible suspicious claims were published in high-impact or trophy journals (26/31) with great visibility, perhaps contributing to this privately-shared stigma within the community. Taken together, our findings support the notion that “big stories” published without subsequent follow-up may be indicative of irreproducibility.</p>
<p>Another takeaway from our experimental validation is that approximately 50% of the unchallenged claims were not previously flagged as problematic failed the experimental validation test regardless. While a claim could remain unchallenged for years because it is valid and no one has anything novel to add to it, our results suggest the status of a claim being “unchallenged”is not a reliable proxy for its validity. If we reclassify the dataset by counting logically consistent unchallenged claims as “verified” and logically inconsistent ones as challenged, and conservatively assume that 50% of the remaining unchallenged claims fall into the latter, we estimate that the true proportion of challenged claims may approach 14%. When we include mixed and partially verified claims, a rough upper-bound estimate suggests that at least 80% of all published claims on <italic>Drosophila</italic> immunity between 1959-2011 are verified or likely to be true leaving 20% of claims either challenged or difficult to interpret. We will take this moment to remind that challenged claims can include work that was done well, but where the field has advanced in its understanding and can better interpret the original findings.We believe the percentage of challenged claims likely varies across scientific fields, depending on intrinsic challenges such as the complexity of biological processes, methodological variability, and the field’s attractiveness, which may increase competitive pressure.</p>
<p>In a recent editorial, Fiala and Diamendis, suggested that reproducibility efforts should focus on high-impact papers as these articles are most deleterious to the scientific community (<xref ref-type="bibr" rid="c18">Fiala and Diamandis, 2018</xref>). In our database, we indeed found that “trophy journals”, considered to contain the most impactful science, are more likely to publish articles with challenged claims than high or low-impact, although the difference was not significant. Three intertwined factors can explain why high-impact factor journals have higher rate of non-reproducibility. First, the uncertainty that comes with exploring the frontiers of knowledge may lead to investigations that are founded on an incomplete understanding of the major factors influencing the results. Second, the unconventional nature of some high-impact discoveries, and the fact that claims reported in these journals generally attract more attention, increases the likelihood that other researchers will attempt to replicate them — thereby raising the number of challenged claims. And third, systemic biases that pressure scientists to publish in these journals for prestige might encourage scientists to take shortcuts in finalizing their studies, increasing the chance of challenged claims. In our study, we observed a decreased proportion of unchallenged claims in articles published in high-impact and trophy journals indicating that claims published in these journals are more reproduced. However, this higher rate of follow-up work cannot fully explain by itself the higher proportion of challenged claims in trophy journals.</p>
<p>Compared to low rank and trophy journals, high-ranking journals have the best ratio of verified to challenged claims, as low-impact journals have a high percentage of unchallenged claims. Strikingly, we observed higher rate of non-reproducible claims for scientists working in universities in the top 50 Shanghai ranking. Previous work has suggested journal rank as a major associate of irreproducibility (<xref ref-type="bibr" rid="c7">Brembs et al., 2013</xref>), and that high-impact journals favour authors from leading institutions (<xref ref-type="bibr" rid="c28">Kulal et al., 2025</xref>). Our study highlights how university ranking is among the most striking correlates of irreproducibility. Taken together, the cause and effect relationship at play here may be driven most by factors associated with institutional prestige, such as trust among networks of colleagues that act as editors and reviewers, leading to greater acceptance rates at high-impact journals. Importantly, our results suggest this greater acceptance does not strictly reflect a high quality of the work being published, but rather a willingness to trust irreproducible work more frequently if it comes from a prestigious institute. Our study spans a period during which most research was done in North America, Europe, Japan and South Korea and does not capture the rise of science in China, or rapid growth of scientific output across the global north and south, where scientists in some countries face strong incentive for ‘high impact’ publications (<xref ref-type="bibr" rid="c12">Delgado-López-Cózar et al., 2021</xref>; <xref ref-type="bibr" rid="c39">Quan et al., 2017</xref>). The fact that the <italic>Drosophila</italic> immunity field monitored in this study developed initially in Sweden and France, which have unique geographic and research community characteristics, might affect some our conclusions or limit them to a specific period.</p>
<p>Future studies in other areas of research might help to confirm if working in top ranked universities correlates with higher rate of non-reproducible science.</p>
<p>Our data align with the broader narrative of increasing rates of non-reproducible science. One possible explanation is a general decline in the standards of rigor in recent years in the life sciences; however, a non-mutually exclusive hypothesis is that the rise in irreproducibility may reflect the increasing popularity of certain fields, creating greater variance by attracting more researchers, who themselves may not be fully acquainted with the field’s established norms and methodological rigor, or who move opportunistically from field to field to publish striking papers. When research on the gut microbiota gained momentum in the 1990s, it was rapidly positioned as a promising avenue to explain a wide array of diseases. Substantial funding and media attention enabled the swift development of the field. However, many early studies lacked methodological rigor and could not be reproduced (<xref ref-type="bibr" rid="c47">Wesolowska-Andersen et al., 2014</xref>). The lower scientific standards tolerated in this emerging area —compared to more established fields— facilitated the publication of high-profile articles, encouraging a rapid influx of new research groups. In this context, scientific novelty and perceived importance often took precedence over precision and reproducibility. A similar but more acute pattern was observed during the SARS-CoV-2 pandemic. Alternatively, this reproducibility crisis might be more linked to specific features of our time. Although this notion is disputed (<xref ref-type="bibr" rid="c29">Lemaitre, 2016</xref>; <xref ref-type="bibr" rid="c44">Twenge and Campbell, 2009</xref>), an increase of narcissism in society might increase the chance of distorting science to favor our own success.</p>
<p>Importantly, our analysis shows that irreproducibility is not evenly distributed across all researchers. While irreproducibility can stem from the inherent difficulty or novelty of certain research questions, our findings point to the significant influence of both the first author (who performs the work) and the laboratory environment. Surprisingly, we did not observe a major effect of experience level: irreproducibility rates were similar between PhD students and postdoctoral researchers and did not markedly decrease with the number of publications. Similarly, we only observe a slightly but not significant higher irreproducibility rate among junior faculty compared to senior professors. This suggests that irreproducibility is not inherent to early career stage but rather to certain research practices and environments. That said, we did observe a higher irreproducibility rate among PIs that entered the field from a non-<italic>Drosophila</italic> immunity background. This observation supports the value of traditional scientific training pathways, where early-career researchers are mentored in well-established environments before gaining independence.</p>
<p>Another noteworthy finding is our suggested link between irreproducibility and “continuous” or “exploratory” research style. We observed that authors with a more exploratory research style have slightly higher proportions of challenged claims and a significantly higher proportion of unchallenged claims, the latter being more likely to be challenged as shown by our experimental replication. Exploratory researchers have worked only transiently in the field, often leaving it after its peak period of popularity. In recent years funding agencies and universities favored this exploratory research style focusing on breakthroughs, notably by funding projects that may be rapidly discontinued. As a counterpoint of this, our study highlights the value and reliability of laboratories that tend to maintain long-term commitment to a specific field.</p>
<p>An important aspiration in the field of metascience study would be the ability to predict claims or articles that are likely irreproducible. Our retrospective analysis identifies several criteria that correlate with irreproducibility, although not all of them reach significance. These include i) articles being published by authors working in top-ranked institutions, ii) articles being published in trophy journals, iii) the absence of follow-up studies (i.e. being ‘unchallenged’), and iv) articles being published by authors with an exploratory research style.In contrast, having published as first author in another laboratory within the field, and publishing in high-impact journals but not trophy journals correlate with higher reproducibility. The absence of follow-up studies regarding claims published in high-impact journals is likely the most useful proxy to question the reproducibility of these claims.</p>
<sec id="s3a">
<title>Limitations</title>
<p>This work has several limitations. First, the definition of the claim, the validity, the determination of the validation was influenced by the annotating authors (HW and BL). While the <italic>ReproSci</italic> website was opened to the community, many PIs were reluctant or not interested in the project of testing old claims that represent “cold scientific cases”. Second the leading author is stakeholder of the project having published several articles, that were included in the analysis, including 44 articles with B.L. as co-author with 4 as first and 22 as last authors. Thus, this may not represent the general view of the community. Finally, the findings might be specific to this field and strongly influenced by specific factors idiosyncratic to its development. However, we believe that despite these limitations the systematic and retrospective analysis of a research field provides insights to better apprehend how science progresses despite errors and the impact of irreproducibility.</p>
</sec>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Selected articles, annotation, experimental validation of the <italic>ReproSci</italic> project</title>
<p>We provide here a succinct method of the <italic>ReproSci</italic> reproducibility project (see the companion article (<xref ref-type="bibr" rid="c48">Westlake et al., 2025</xref>) for more details). In brief, a list of 400 publications published before 2011 was generated using a curated search string on the publicly available PubMed database. This limitation was imposed to allow a time frame in which follow-up studies may be published that may verify, expand on or contradict the content of the publications selected for the study. Selected primary articles were annotated by a single researcher (HW) and reviewed by a second researcher (BL) to ensure consistency and accuracy across the project. An online database, <italic>ReproSci</italic> (<ext-link ext-link-type="uri" xlink:href="https://ReproSci.epfl.ch/">https://ReproSci.epfl.ch/</ext-link>), was constructed to organize the annotations and provide an interactive and searchable interface. Claims were extracted through careful reading of the selected primary articles and organized according to their importance within the paper: a single <bold>Main claim</bold> representing the title claim of the paper or overarching finding; 3-4 <bold>Major claims</bold> emphasized in the abstract or those contributing to the overarching finding; and a variable number of <bold>Minor claims</bold> representing other findings of note not emphasized in the abstract. Claims were cross-checked with evidence from previous, contemporary and subsequent publications and assigned a verification category (see <xref rid="tbl1" ref-type="table">Table 1</xref>). A short statement explaining the evidence and citing sources was written to justify the categorization and provide additional context for the claim; this assessment was linked to each claim and can be viewed on the <italic>ReproSci</italic> website. A subset of unchallenged claims (45 major and 11 minor) was selected for direct experimental reproduction by various authors from different laboratories with relevant expertise. This project is concerned with the accuracy of the conclusions themselves rather than the reliability of methods, so we employed <bold>indirect</bold> or <bold>inferential reproducibility,</bold> where experimental procedures that may be different from those originally employed are used to independently verify the claim. The <italic>ReproSci</italic> website was made available to the <italic>Drosophila</italic> scientific community on July 11, 2023, with an email to encourage to comment on the annotations and verifications and contribute evidence.</p>
</sec>
<sec id="s4b">
<title>First authors and last author classification</title>
<p>Information regarding the gender and status of first authors were either obtained by interviewing PIs (for 191 first authors) or found on publicly available websites (LinkedIn and university websites) or were implied based on their first name usage (gender) and their PubMed publication lists (for the other 100 first authors). When some of these information could not be retrieved, these authors were not taken in consideration.</p>
<p>PIs were manually classified as senior or junior whether they have or have not published a last author article at least five years before the considered publication. Continuity and exploratory PIs were manually classified according to their PubMed publication list. Continuity PIs are PIs that have continued to work in the <italic>Drosophila</italic> immunity field until now or to the end of their scientific career (deceased, retired, left academia). Exploratory PIs are PIs that have done a transient incursion in the <italic>Drosophila</italic> immunity field moving to other topics of research.</p>
</sec>
<sec id="s4c">
<title>Statistical analysis</title>
<p>The analysis was done on the 1006 major claims of the 400 articles. The cleaned database contains one row per major claim with linked article-, author- and laboratory-level covariates, which was subsequently analyzed. Every output, counts, proportions and graphics, exploratory summaries and Bayesian mixed-effects models are defined under a reproducible Python workflow, which enable the full replication of the main results and every figure in the manuscript.</p>
<p>The ninety-five-percent confidence intervals (CI) for single proportions were determined using Wilson’s score method. Whenever a binary predictor (e.g. author gender) was cross-classified with the binary outcome “claim challenged”, we reported the odds ratio with its 95% CI and the two-sided <italic>p</italic>-value from Fisher’s exact test; The raw contingency tables for every binary predictor appear in <xref ref-type="supplementary-material" rid="supp1">Supplementary Document</xref>.</p>
</sec>
<sec id="s4d">
<title>Multivariable modelling</title>
<p>Two Bayesian regressions were fitted with weakly-informative priors. Posterior summaries are presented as odds ratios (OR) with 94 % highest-density intervals (HDI).
<list list-type="order">
<list-item><p>Hierarchical logistic regression for claim irreproducibility. The outcome was whether a major claim was later classified as challenged. Fixed-effect predictors captured article prestige, author characteristics and laboratory context: journal tier (Low, High, Trophy), university-ranking tier (Not ranked, 101 +, 51–100, Top 50), publication year modelled with a three-knot B-spline, first-author gender and training stage (PhD vs post-doc), last-author gender, laboratory seniority (junior vs senior PI), continuity style (continuous vs exploratory) and whether the laboratory had published in the field before 1995. Claims from one author not independent, so random intercepts were included for both first-author key and last-author key.</p></list-item>
<list-item><p>Another simpler hierarchical regression model was used with only authors random intercepts, university ranking and journal impact factor to help the conclusion in section Supplementary Figure S2.</p></list-item>
</list>
All models were estimated with the dynamic-HMC NUTS sampler (target _accept = 0.9); four independent chains of 2 000 post-warm-up draws were run for every model. Convergence was confirmed both visually and numerically (all Rhat≤1.01; all effective sample sizes &gt; 400).</p>
<p>Supplementary Fig. S4 shows trace plots, posterior-predictive checks and Pareto-k diagnostics; no systematic lack of fit was detected. Exact formulas, priors, sampling statements and notebooks are available in the public repository (<xref ref-type="supplementary-material" rid="supp1">Supplementary Document</xref>).</p>
</sec>
</sec>

</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank the BioInformatics Competence Center of EPFL-UNIL for generating the <italic>ReproSci</italic> website. This work was supported by Swiss National Science Foundation 310030_189085 and the ETH-Domain’s Open Research Data (ORD) Program (2022). We thank Florent Masson and Samuel Rommelaere for comments on the manuscript.</p>
</ack>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplemental information</label>
<caption>
<p>Provides information on the full code (S1), the Multivariate model (S2, Figures S1), the Multivariate model using Unchallenged as outcome (S3, Figure S3), a table of the odd-rations and values for the categorical variables shown in the main text, for first and last-author (S4) and other categorical comparisons (S5) and Table of journals with impact factor, and claim assessment (S6, Figure S4).</p>
</caption>
<media xlink:href="supplements/663460_file03.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abril-Pla</surname> <given-names>O</given-names></string-name>, <string-name><surname>Andreani</surname> <given-names>V</given-names></string-name>, <string-name><surname>Carroll</surname> <given-names>C</given-names></string-name>, <string-name><surname>Dong</surname> <given-names>L</given-names></string-name>, <string-name><surname>Fonnesbeck</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Kochurov</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kumar</surname> <given-names>R</given-names></string-name>, <string-name><surname>Lao</surname> <given-names>J</given-names></string-name>, <string-name><surname>Luhmann</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>OA</given-names></string-name>, <string-name><surname>Osthege</surname> <given-names>M</given-names></string-name>, <string-name><surname>Vieira</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wiecki</surname> <given-names>T</given-names></string-name>, <string-name><surname>Zinkov</surname> <given-names>R</given-names></string-name></person-group>. <year>2023</year>. <article-title>PyMC: a modern, and comprehensive probabilistic programming framework in Python</article-title>. <source>PeerJ Comput Sci</source> <volume>9</volume>:<fpage>e1516</fpage>. doi:<pub-id pub-id-type="doi">10.7717/peerj-cs.1516</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Amaral</surname></string-name> <etal>et al.</etal></person-group>,. <year>2025</year>. <article-title>Estimating the replicability of Brazilian biomedical science</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amaral</surname> <given-names>OB</given-names></string-name>, <string-name><surname>Neves</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wasilewska-Sampaio</surname> <given-names>AP</given-names></string-name>, <string-name><surname>Carneiro</surname> <given-names>CF</given-names></string-name></person-group>. <year>2019</year>. <article-title>The Brazilian Reproducibility Initiative</article-title>. <source>eLife</source> <volume>8</volume>:<elocation-id>e41602</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.41602</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baker</surname> <given-names>M</given-names></string-name></person-group>. <year>2016</year>. <article-title>1,500 scientists lift the lid on reproducibility</article-title>. <source>Nature</source> <volume>533</volume>:<fpage>452</fpage>–<lpage>454</lpage>. doi:<pub-id pub-id-type="doi">10.1038/533452a</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baker</surname> <given-names>M</given-names></string-name></person-group>. <year>2015</year>. <article-title>Over half of psychology studies fail reproducibility test</article-title>. <source>Nature nature</source>.<volume>2015</volume>.<fpage>18248</fpage>. doi:<pub-id pub-id-type="doi">10.1038/nature.2015.18248</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Begley</surname> <given-names>CG</given-names></string-name>, <string-name><surname>Ioannidis</surname> <given-names>JPA</given-names></string-name></person-group>. <year>2015</year>. <article-title>Reproducibility in Science: Improving the Standard for Basic and Preclinical Research</article-title>. <source>Circulation Research</source> <volume>116</volume>:<fpage>116</fpage>–<lpage>126</lpage>. doi:<pub-id pub-id-type="doi">10.1161/CIRCRESAHA.114.303819</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brembs</surname> <given-names>B</given-names></string-name>, <string-name><surname>Button</surname> <given-names>K</given-names></string-name>, <string-name><surname>Munafò</surname> <given-names>M</given-names></string-name></person-group>. <year>2013</year>. <article-title>Deep impact: unintended consequences of journal rank</article-title>. <source>Front Hum Neurosci</source> <volume>7</volume>. doi:<pub-id pub-id-type="doi">10.3389/fnhum.2013.00291</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Capretto</surname> <given-names>T</given-names></string-name>, <string-name><surname>Piho</surname> <given-names>C</given-names></string-name>, <string-name><surname>Kumar</surname> <given-names>R</given-names></string-name>, <string-name><surname>Westfall</surname> <given-names>J</given-names></string-name>, <string-name><surname>Yarkoni</surname> <given-names>T</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>OA.</given-names></string-name></person-group> <year>2022</year>. <article-title>Bambi: A simple interface for fitting Bayesian linear models in Python</article-title>. <source>arXiv</source> doi:<pub-id pub-id-type="doi">10.48550/arXiv.2012.10754</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chalmers</surname> <given-names>I</given-names></string-name>, <string-name><surname>Glasziou</surname> <given-names>P</given-names></string-name></person-group>. <year>2009</year>. <article-title>Avoidable Waste in the Production and Reporting of Research Evidence</article-title>. <source>Lancet</source> <volume>374</volume>:<fpage>86</fpage>-<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Da Costa</surname> <given-names>GG</given-names></string-name>, <string-name><surname>Neves</surname> <given-names>K</given-names></string-name>, <string-name><surname>Amaral</surname> <given-names>OB.</given-names></string-name></person-group> <year>2022</year>. <article-title>Estimating the replicability of highly cited clinical research (2004-2018) (preprint)</article-title>. <source>Epidemiology</source>. doi:<pub-id pub-id-type="doi">10.1101/2022.05.31.22275810</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DeCoursey</surname> <given-names>TE</given-names></string-name></person-group>. <year>2006</year>. <article-title>It’s difficult to publish contradictory findings</article-title>. <source>Nature</source> <volume>439</volume>:<fpage>784</fpage>–<lpage>784</lpage>. doi:<pub-id pub-id-type="doi">10.1038/439784b</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delgado-López-Cózar</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ràfols</surname> <given-names>I</given-names></string-name>, <string-name><surname>Abadal</surname> <given-names>E</given-names></string-name></person-group>. <year>2021</year>. <article-title>Letter: A call for a radical change in research evaluation in Spain</article-title>. <source>Epi</source>. doi:<pub-id pub-id-type="doi">10.3145/epi.2021.may.09</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Devezer</surname> <given-names>B</given-names></string-name>, <string-name><surname>Buzbas</surname> <given-names>EO</given-names></string-name></person-group>. <year>2023</year>. <article-title>Rigorous exploration in a model-centric science via epistemic iteration</article-title>. <source>Journal of Applied Research in Memory and Cognition</source> <volume>12</volume>:<fpage>189</fpage>–<lpage>194</lpage>. doi:<pub-id pub-id-type="doi">10.1037/mac0000121</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eisner</surname> <given-names>DA</given-names></string-name></person-group>. <year>2018</year>. <article-title>Reproducibility of science: Fraud, impact factors and carelessness</article-title>. <source>Journal of Molecular and Cellular Cardiology</source> <volume>114</volume>:<fpage>364</fpage>–<lpage>368</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.yjmcc.2017.10.009</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Errington</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Denis</surname> <given-names>A</given-names></string-name>, <string-name><surname>Perfito</surname> <given-names>N</given-names></string-name>, <string-name><surname>Iorns</surname> <given-names>E</given-names></string-name>, <string-name><surname>Nosek</surname> <given-names>BA</given-names></string-name></person-group>. <year>2021</year>. <article-title>Challenges for assessing replicability in preclinical cancer biology</article-title>. <source>eLife</source> <volume>10</volume>:<elocation-id>e67995</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.67995</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Errington</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Iorns</surname> <given-names>E</given-names></string-name>, <string-name><surname>Gunn</surname> <given-names>W</given-names></string-name>, <string-name><surname>Tan</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Lomax</surname> <given-names>J</given-names></string-name>, <string-name><surname>Nosek</surname> <given-names>BA</given-names></string-name></person-group>. <year>2014</year>. <article-title>An open investigation of the reproducibility of cancer biology research</article-title>. <source>eLife</source> <volume>3</volume>. doi:<pub-id pub-id-type="doi">10.7554/eLife.04333</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fanelli</surname> <given-names>D</given-names></string-name></person-group>. <year>2018</year>. <article-title>Opinion: Is science really facing a reproducibility crisis, and do we need it to?</article-title> <source>Proceedings of the National Academy of Sciences</source> <volume>115</volume>:<fpage>2628</fpage>–<lpage>2631</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1708272114</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiala</surname> <given-names>C</given-names></string-name>, <string-name><surname>Diamandis</surname> <given-names>EP</given-names></string-name></person-group>. <year>2018</year>. <article-title>Benign and malignant scientific irreproducibility</article-title>. <source>Clinical Biochemistry</source> <volume>55</volume>:<fpage>1</fpage>–<lpage>2</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.clinbiochem.2018.03.015</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fidler</surname> <given-names>F</given-names></string-name>, <string-name><surname>Chee</surname> <given-names>YE</given-names></string-name>, <string-name><surname>Wintle</surname> <given-names>BC</given-names></string-name>, <string-name><surname>Burgman</surname> <given-names>MA</given-names></string-name>, <string-name><surname>McCarthy</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Gordon</surname> <given-names>A</given-names></string-name></person-group>. <year>2017</year>. <article-title>Metaresearch for Evaluating Reproducibility in Ecology and Evolution</article-title>. <source>BioScience biw</source><volume>159</volume>. doi:<pub-id pub-id-type="doi">10.1093/biosci/biw159</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Fidler</surname> <given-names>F</given-names></string-name>, <string-name><surname>Wilcox</surname> <given-names>J.</given-names></string-name></person-group> <year>2018</year>. <chapter-title>Reproducibility of scientific results</chapter-title>. <source>The Stanford Encyclopedia of Philosophy</source>. <publisher-name>Stanford University</publisher-name></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fortunato</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bergstrom</surname> <given-names>CT</given-names></string-name>, <string-name><surname>Börner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Evans</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Helbing</surname> <given-names>D</given-names></string-name>, <string-name><surname>Milojević</surname> <given-names>S</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Radicchi</surname> <given-names>F</given-names></string-name>, <string-name><surname>Sinatra</surname> <given-names>R</given-names></string-name>, <string-name><surname>Uzzi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Vespignani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Waltman</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Barabási</surname> <given-names>A-L</given-names></string-name></person-group>. <year>2018</year>. <article-title>Science of science</article-title>. <source>Science</source> <volume>359</volume>. doi:<pub-id pub-id-type="doi">10.1126/science.aao0185</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>França</surname> <given-names>TF</given-names></string-name>, <string-name><surname>Monserrat</surname> <given-names>JM</given-names></string-name></person-group>. <year>2018</year>. <article-title>Reproducibility crisis in science or unrealistic expectations?</article-title> <source>EMBO reports</source> <volume>19</volume>:<fpage>e46008</fpage>. doi:<pub-id pub-id-type="doi">10.15252/embr.201846008</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodman</surname> <given-names>SN</given-names></string-name>, <string-name><surname>Fanelli</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ioannidis</surname> <given-names>JPA</given-names></string-name></person-group>. <year>2016</year>. <article-title>What does research reproducibility mean?</article-title> <source>Science Translational Medicine</source> <volume>8</volume>:<elocation-id>341ps12–341ps12</elocation-id>. doi:<pub-id pub-id-type="doi">10.1126/scitranslmed.aaf5027</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hill</surname> <given-names>R</given-names></string-name>, <string-name><surname>Yin</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Stein</surname> <given-names>C</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>BF</given-names></string-name></person-group>. <year>2025</year>. <article-title>The pivot penalty in research</article-title>. <source>Nature</source> <fpage>1</fpage>–<lpage>8</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41586-025-09048-1</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hunter</surname> <given-names>P</given-names></string-name></person-group>. <year>2017</year>. <article-title>The reproducibility “crisis.”</article-title> <source>EMBO reports</source> <volume>18</volume>:<fpage>1493</fpage>–<lpage>1496</lpage>. doi:<pub-id pub-id-type="doi">10.15252/embr.201744876</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ioannidis</surname> <given-names>JPA</given-names></string-name>, <string-name><surname>Allison</surname> <given-names>DB</given-names></string-name>, <string-name><surname>Ball</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Coulibaly</surname> <given-names>I</given-names></string-name>, <string-name><surname>Cui</surname> <given-names>X</given-names></string-name>, <string-name><surname>Culhane</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Falchi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Furlanello</surname> <given-names>C</given-names></string-name>, <string-name><surname>Game</surname> <given-names>L</given-names></string-name>, <string-name><surname>Jurman</surname> <given-names>G</given-names></string-name>, <string-name><surname>Mangion</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mehta</surname> <given-names>T</given-names></string-name>, <string-name><surname>Nitzberg</surname> <given-names>M</given-names></string-name>, <string-name><surname>Page</surname> <given-names>GP</given-names></string-name>, <string-name><surname>Petretto</surname> <given-names>E</given-names></string-name>, <string-name><surname>van Noort</surname> <given-names>V.</given-names></string-name></person-group> <year>2009</year>. <article-title>Repeatability of published microarray gene expression analyses</article-title>. <source>Nature Genetics</source> <volume>41</volume>:<fpage>149</fpage>–<lpage>155</lpage>. doi:<pub-id pub-id-type="doi">10.1038/ng.295</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kannan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gowri</surname> <given-names>S</given-names></string-name></person-group>. <year>2014</year>. <article-title>Contradicting/negative results in clinical research: Why (do we get these)? Why not (get these published)?</article-title> <source>Where (to publish)? Perspect Clin Res</source> <volume>5</volume>:<fpage>151</fpage>–<lpage>153</lpage>. doi:<pub-id pub-id-type="doi">10.4103/2229-3485.140546</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kulal</surname> <given-names>A N A</given-names></string-name>, <string-name><surname>Shareena</surname> <given-names>P</given-names></string-name>, <string-name><surname>Dinesh</surname> <given-names>S</given-names></string-name></person-group>. <year>2025</year>. <article-title>Unmasking Favoritism and Bias in Academic Publishing: An Empirical Study on Editorial Practices</article-title>. <source>Public Integrity</source> <fpage>1</fpage>–<lpage>22</lpage>. doi:<pub-id pub-id-type="doi">10.1080/10999922.2024.2448875</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lemaitre</surname> <given-names>B.</given-names></string-name></person-group> <year>2016</year>. <source>An Essay on Science and NarcissismC: Why do high-ego personalities drive research in life sciences?</source>, <publisher-name>EPFL press</publisher-name>. <publisher-loc>Lausanne</publisher-loc>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lemaitre</surname> <given-names>B</given-names></string-name></person-group>. <year>2004</year>. <article-title>The road to Toll</article-title>. <source>Nature Reviews Immunology</source> <volume>4</volume>:<fpage>521</fpage>–<lpage>527</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nri1390</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lesperance</surname> <given-names>DNA</given-names></string-name>, <string-name><surname>Broderick</surname> <given-names>NA.</given-names></string-name></person-group> <year>2021</year>. <article-title>Gut Bacteria Mediate Nutrient Availability in Drosophila Diets</article-title>. <source>Applied and Environmental Microbiology</source> <volume>87</volume>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Macleod</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Michie</surname> <given-names>S</given-names></string-name>, <string-name><surname>Roberts</surname> <given-names>I</given-names></string-name>, <string-name><surname>Dirnagl</surname> <given-names>U</given-names></string-name>, <string-name><surname>Chalmers</surname> <given-names>I</given-names></string-name>, <string-name><surname>Ioannidis</surname> <given-names>JPA</given-names></string-name>, <string-name><surname>Salman</surname> <given-names>RA-S</given-names></string-name>, <string-name><surname>Chan</surname> <given-names>A-W</given-names></string-name>, <string-name><surname>Glasziou</surname> <given-names>P</given-names></string-name></person-group>. <year>2014</year>. <article-title>Biomedical research: increasing value, reducing waste</article-title>. <source>The Lancet</source> <volume>383</volume>:<fpage>101</fpage>–<lpage>104</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0140-6736(13)62329-6</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meng</surname> <given-names>X-L</given-names></string-name></person-group>. <year>2020</year>. <article-title>Reproducibility, Replicability, and Reliability</article-title>. <source>Harvard Data Science Review</source> <volume>2</volume>. doi:<pub-id pub-id-type="doi">10.1162/99608f92.dbfce7f9</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nissen</surname> <given-names>SB</given-names></string-name>, <string-name><surname>Magidson</surname> <given-names>T</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bergstrom</surname> <given-names>CT</given-names></string-name></person-group>. <year>2016</year>. <article-title>Publication bias and the canonization of false facts</article-title>. <source>eLife</source> <volume>5</volume>:<elocation-id>e21451</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.21451</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname> <given-names>R</given-names></string-name></person-group>. <year>2015</year>. <article-title>The reproducibility crisis in science: A statistical counterattack</article-title>. <source>Significance</source> <volume>12</volume>:<fpage>30</fpage>–<lpage>32</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1740-9713.2015.00827.x</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Piller</surname> <given-names>C</given-names></string-name></person-group>. <year>2022</year>. <article-title>Blots on a field?</article-title> <source>Science</source> <volume>377</volume>:<fpage>358</fpage>–<lpage>363</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Plesser</surname> <given-names>HE</given-names></string-name></person-group>. <year>2018</year>. <article-title>Reproducibility vs. Replicability: A Brief History of a Confused Terminology</article-title>. <source>Frontiers in Neuroinformatics</source> <volume>11</volume>. doi:<pub-id pub-id-type="doi">10.3389/fninf.2017.00076</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prinz</surname> <given-names>F</given-names></string-name>, <string-name><surname>Schlange</surname> <given-names>T</given-names></string-name>, <string-name><surname>Asadullah</surname> <given-names>K</given-names></string-name></person-group>. <year>2011</year>. <article-title>Believe it or not: how much can we rely on published data on potential drug targets?</article-title> <source>Nature Reviews Drug Discovery</source> <volume>10</volume>:<fpage>712</fpage>–<lpage>712</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrd3439-c1</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quan</surname> <given-names>W</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>B</given-names></string-name>, <string-name><surname>Shu</surname> <given-names>F</given-names></string-name></person-group>. <year>2017</year>. <article-title>Publish or impoverish: An investigation of the monetary reward system of science in China (1999-2016)</article-title>. <source>Ajim</source> <volume>69</volume>:<fpage>486</fpage>–<lpage>502</lpage>. doi:<pub-id pub-id-type="doi">10.1108/ajim-01-2017-0014</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rosenthal</surname> <given-names>R</given-names></string-name></person-group>. <year>1979</year>. <article-title>The file drawer problem and tolerance for null results</article-title>. <source>Psychological Bulletin</source> <volume>86</volume>:<fpage>638</fpage>–<lpage>641</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0033-2909.86.3.638</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shiffrin</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Börner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Stigler</surname> <given-names>SM</given-names></string-name></person-group>. <year>2018</year>. <article-title>Scientific progress despite irreproducibility: A seeming paradox</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>115</volume>:<fpage>2632</fpage>–<lpage>2639</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1711786114</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steneck</surname> <given-names>NH</given-names></string-name></person-group>. <year>2011</year>. <article-title>The dilemma of the honest researcher</article-title>. <source>EMBO reports</source> <volume>12</volume>:<fpage>745</fpage>–<lpage>745</lpage>. doi:<pub-id pub-id-type="doi">10.1038/embor.2011.134</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stern</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Casadevall</surname> <given-names>A</given-names></string-name>, <string-name><surname>Steen</surname> <given-names>RG</given-names></string-name>, <string-name><surname>Fang</surname> <given-names>FC</given-names></string-name></person-group>. <year>2014</year>. <article-title>Financial costs and personal consequences of research misconduct resulting in retracted publications</article-title>. <source>eLife</source> <volume>3</volume>:<elocation-id>e02956</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.02956</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Twenge</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Campbell</surname> <given-names>WK</given-names></string-name></person-group>. <year>2009</year>. <source>The narcissism epidemic: Living in the age of entitlement</source>. <publisher-name>Simon and Schuster</publisher-name>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Udesky</surname> <given-names>L</given-names></string-name></person-group>. <year>2025</year>. <article-title>‘Publish or perish’ culture blamed for reproducibility crisis</article-title>. <source>Nature</source>. doi:<pub-id pub-id-type="doi">10.1038/d41586-024-04253-w</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Wesel</surname> <given-names>M.</given-names></string-name></person-group> <year>2016</year>. <article-title>Evaluation by Citation: Trends in Publication Behavior, Evaluation Criteria, and the Strive for High Impact Publications</article-title>. <source>Sci Eng Ethics</source> <volume>22</volume>:<fpage>199</fpage>–<lpage>225</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s11948-015-9638-0</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wesolowska-Andersen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bahl</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Carvalho</surname> <given-names>V</given-names></string-name>, <string-name><surname>Kristiansen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Sicheritz-Pontén</surname> <given-names>T</given-names></string-name>, <string-name><surname>Gupta</surname> <given-names>R</given-names></string-name>, <string-name><surname>Licht</surname> <given-names>TR</given-names></string-name></person-group>. <year>2014</year>. <article-title>Choice of bacterial DNA extraction method from fecal material influences community structure as evaluated by metagenomic analysis</article-title>. <source>Microbiome</source> <volume>2</volume>:<fpage>19</fpage>. doi:<pub-id pub-id-type="doi">10.1186/2049-2618-2-19</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Westlake</surname> <given-names>H</given-names></string-name>, <string-name><surname>David</surname> <given-names>FPA</given-names></string-name>, <string-name><surname>Tian</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Krakovic</surname> <given-names>K</given-names></string-name>, <string-name><surname>Dolgikh</surname> <given-names>A</given-names></string-name>, <string-name><surname>Juravlev</surname> <given-names>L</given-names></string-name>, <string-name><surname>Esmangart de Bournonville</surname> <given-names>T</given-names></string-name>, <string-name><surname>Carboni</surname> <given-names>A</given-names></string-name>, <string-name><surname>Melcarne</surname> <given-names>C</given-names></string-name>, <string-name><surname>Shan</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Mu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Kotwal</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pirko</surname> <given-names>N</given-names></string-name>, <string-name><surname>Boquete</surname> <given-names>J-P</given-names></string-name>, <string-name><surname>Schüpfer</surname> <given-names>F</given-names></string-name>, <string-name><surname>Rommelaere</surname> <given-names>S</given-names></string-name>, <string-name><surname>Poidevin</surname> <given-names>M</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Kondo</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ratnaparkhi</surname> <given-names>GS</given-names></string-name>, <string-name><surname>Chakrabarti</surname> <given-names>S</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>G</given-names></string-name>, <string-name><surname>Masson</surname> <given-names>F</given-names></string-name>, <string-name><surname>Xiaoxue</surname> <given-names>Hanson MA</given-names></string-name>, <string-name><surname>Haobo</surname> <given-names>J</given-names></string-name>, <string-name><surname>Di Cara</surname> <given-names>F</given-names></string-name>, <string-name><surname>Kurant</surname> <given-names>E</given-names></string-name>, <string-name><surname>Lemaitre</surname> <given-names>B.</given-names></string-name></person-group> <year>2025</year>. <article-title>Reproducibility of Scientific Claims in Drosophila Immunity: A Retrospective Analysis of 400 Publications</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Westlake</surname> <given-names>H</given-names></string-name>, <string-name><surname>Hanson</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Lemaitre</surname> <given-names>B.</given-names></string-name></person-group> <year>2024</year>. <source>The Drosophila Immunity Handbook</source>. <publisher-name>EPFL press</publisher-name>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Youyou</surname> <given-names>W</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Uzzi</surname> <given-names>B</given-names></string-name></person-group>. <year>2023</year>. <article-title>A discipline-wide investigation of the replicability of Psychology papers over the past two decades</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>120</volume>:<fpage>e2208863120</fpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.2208863120</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shen</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fan</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Di</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Stanley</surname> <given-names>HE</given-names></string-name>, <string-name><surname>Havlin</surname> <given-names>S.</given-names></string-name></person-group> <year>2019</year>. <article-title>Increasing trend of scientists to switch between topics</article-title>. <source>Nat Commun</source> <volume>10</volume>:<fpage>3439</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41467-019-11401-8</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108403.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Rodgers</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8332-936X</contrib-id>
<aff>
<institution-wrap>
<institution>eLife</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study presents an impressive large-scale effort to assess the reproducibility of published findings in the field of Drosophila immunity. The authors analyse 400 papers published between 1959 and 2011, and assess how many of the claims in these papers have been tested in subsequent publications. In a companion article they report the results of experiments to test a subset of the claims that, according to the literature, have not been tested. The present article also explores if various factors related to authors, institutions and journals influence reproducibility in this field. The evidence supporting the claims is <bold>solid</bold>, but there is considerable scope for strengthening and extending the analysis. The limitations inherent to evaluating reproducibility based on the published literature should also be acknowledged.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108403.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors set out on the ambitious task of establishing the reproducibility of claims from the Drosophila immunity literature. Starting out from a corpus of 400 articles from 1959 and 2011, the authors sought to determine whether their claims were confirmed or contradicted by previous or subsequent publications. Additionally, they actively sought to replicate a subset of the claims for which no previous replications were available (although this set was not representative of the whole sample, as the authors focused on suspicious and/or easily testable claims). The focus of the article is on inferential reproducibility; thus, methods don't necessarily map exactly to the original ones.</p>
<p>The authors present a large-scale analysis of the individual replication findings, which are presented in a companion article (Westlake et al., 2025. DOI 10.1101/2025.07.07.663442). In their retrospective analysis of reproducibility, the authors find that 61% of the original claims were verified by the literature, 7.5% were partialy verified, and only 6.8% were challenged, with 23.8% having no replication available. This is in stark contrast with the result of their prospective replications, in which only 16% of claims were successfully reproduced.</p>
<p>The authors proceed to investigate correlates of replicability, with the most consistent finding being that findings stemming from higher-ranked universities (and possibly from very high impact journals) were more likely to be challenged.</p>
<p>Strengths:</p>
<p>(1) The work presents a large-scale, in-depth analysis of a particular field of science that includes authors with deep domain expertise of the field. This is a rare endeavour to establish the reproducibility of a particular subfield of science, and I'd argue that we need many more of these in different areas.</p>
<p>(2) The project was built on a collaborative basis (<ext-link ext-link-type="uri" xlink:href="https://ReproSci.epfl.ch/">https://ReproSci.epfl.ch/</ext-link>), using an online database (<ext-link ext-link-type="uri" xlink:href="https://ReproSci.epfl.ch/">https://ReproSci.epfl.ch/</ext-link>), which was used to organize the annotations and comments of the community about the claims. The website remains online and can be a valuable resource to the Drosophila immunity community.</p>
<p>(3) Data and code are shared in the authors' GitHub repository, with a Jupyter notebook available to reproduce the results.</p>
<p>Main concerns:</p>
<p>(1) Although the authors claim that &quot;Drosophila immunity claims are mostly replicable&quot;, this conclusion is strictly based on the retrospective analysis - in which around 84% of the claims for which a published verification attempt was found. This is in very stark contrast with the findings that the authors replicate prospectively, of which only 16% are verified.</p>
<p>Although this large discrepancy may be explained by the fact that the authors focused on unchallenged and suspicious claims (which seems to be their preferred explanation), an alternative hypothesis is that there is a large amount of confirmation bias in the Drosophila immunity literature, either because attempts to replicate previous findings tend to reach similar results due to researcher bias, or because results that validate previous findings are more likely to be published.</p>
<p>Both explanations are plausible (and, not being an expert in the field, I'd have a hard time estimating their relative probability), and in the absence of prospective replication of a systematic sample of claims - which could determine whether the replication rate for a random sample of claims is as high as that observed in the literature -, both should be considered in the manuscript.</p>
<p>(2) The fact that the analysis of factors correlating with reproducibility includes both prospective and retrospective replications also leads to the possibility of confusion bias in this analysis. If most of the challenged claims come from the authors' prospective replications, while most of the verified ones come from those that were replicated by the literature, it becomes unclear whether the identified factors are correlated with actual reproducibility of the claims or with the likelihood that a given claim will be tested by other authors and that this replication will be published.</p>
<p>(3) The methods are very brief for a project of this size, and many of the aspects in determining whether claims were conceptually replicated and how replications were set up are missing.</p>
<p>Some of these - such as the PubMed search string for the publications and a better description of the annotation process - are described in the companion article, but this could be more explicitly stated. Others, however, remain obscure. Statements such as &quot;Claims were cross-checked with evidence from previous, contemporary and subsequent publications and assigned a verification category&quot; summarize a very complex process for which more detail should be given - in particular because what constitutes inferential reproducibility is not a self-evident concept. And although I appreciate that what constitutes a replication is ultimately a case-by-case decision, a general description of the guidelines used by the authors to determine this should be provided. As these processes were done by one author and reviewed by another, it would also be useful to know the agreement rates between them to have a general sense of how reproducible the annotation process might be.</p>
<p>The same gap in methods descriptions holds for the prospective replications. How were labs selected, how were experimental protocols developed, and how was the validity of the experiments as a conceptual replication assessed? I understand that providing the methods for each individual replication is beyond the scope of the article, but a general description of how they were developed would be important.</p>
<p>(4) As far as I could tell, the large-scale analysis of the replication results was not preregistered, and many decisions seem somewhat ad hoc. In particular, the categorization of journals (e.g. low impact, high impact, &quot;trophy&quot;) and universities (e.g. top 50, 51-100, 101+) relies on arbitrary thresholds, and it is unclear how much the results are dependent on these decisions, as no sensitivity analyses are provided.</p>
<p>Particularly, for analyses that correlate reproducibility with continuous variable (such as year of publication, impact factor or university ranking, I'd strongly favor using these variables as continuous variables in the analysis (e.g. using logistic regression) rather than performing pairwise comparisons between categories determined by arbitrary cutoffs. This would not only reduce the impact of arbitrary thresholds in the analysis, but would also increase statistical power in the univariate analyses (as the whole sample can be used in at once) and reduce the number of parameters in the multivariate model (as they will be included as a single variable rather than multiple dummy variables when there are more than two categories).</p>
<p>(5) The multivariate model used to investigate predictors of replicability includes unchallenged claims along with verified ones in the outcome, which seems like an odd decision. If the intention is to analyze which factors are correlated with reproducibility, it would make more sense to remove the unchallenged findings, as these are likely uninformative in this sense. In fact, based on the authors' own replications of unchallenged findings, they may be more likely to belong the &quot;challenged&quot; category than to the &quot;unchallenged&quot; one if they were to be verified.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108403.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Lemaitre et al. conducted an analysis of 400 publications in the Drosophila immunity field (1959-2011), performing both univariable and multivariable analyses to identify factors that correlate with or influence the irreproducibility of scientific claims. Some of the findings are unexpected, for instance, neither the career stage of the PI nor that of the first author appears to matter that much, while others, such as the influence of institutional prestige or publication in &quot;trophy journals,&quot; are more predictable. The results provide valuable insight into patterns of irreproducibility in academia and may help inform policies to improve research reproducibility in the field.</p>
<p>Strengths:</p>
<p>This study is based on a large, manually curated dataset, complemented by a companion paper (Westlake et al., 2025. DOI 10.1101/2025.07.07.663442) that provides additional details on experimentally documented cases. The statistical methods are appropriate, and the findings are both important and informative. The results are clearly presented and supported by accessible documentation through the ReproSci project.</p>
<p>Weaknesses:</p>
<p>The analysis is limited to a specific field (immunity) and model system (Drosophila). Since biological context may influence reproducibility -- for example, depending on whether mechanisms are more hardwired or variable -- and the model system itself may contribute to these effects (as the authors note), it remains unclear to what extent these findings generalize to other fields or organisms. The authors could expand the discussion to address the potential scope and limitations of the study's generalizability.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108403.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors of this paper were trying to identify how reproducible, or not, their subfield (Drosophilia immunity) was since its inception over 50 years ago. This required identifying not only the papers, but the specific claims made in the paper, assessing if these claims were followed up in the literature, and if so whether the subsequent papers supported or refuted the original claim. In addition to this large manually curated effort, the authors further investigated some claims that were left unchallenged in the literature by conducting replications themselves. This provided a rich corpus of the subfield that could be investigated into what characteristics influence reproducibility.</p>
<p>Strengths:</p>
<p>A major strength of this study is the focus on a subfield, the detailing of identifying the main, major, and minor claims - which is a very challenging manual task - and then cataloging not only their assessment of if these claims were followed up in the literature, but also what characteristics might be contributing to reproducibility, which also included more manual effort to supplement the data that they were able to extract from the published papers. While this provides a rich dataset for analysis, there is a major weakness with this approach, which is not unique to this study.</p>
<p>Weaknesses:</p>
<p>The main weakness is relying heavily on the published literature as the source for if a claim was determined to be verified or not. There are many documented issues with this stemming from every field of research - such as publication bias, selective reporting, all the way to fraud. It's understandable why the authors took this approach - it is the only way to get at a breadth of the literature - however the flaw with this approach is it takes the literature as a solid ground truth, which it is not. At the same time, it is not reasonable to expect the authors to have conducted independent replications for all of the 400 papers they identified. However, there is a big difference trying to assess the reproducibility of the literature by using the literature as the 'ground truth' vs doing this independently like other large-scale replication projects have attempted to do. This means the interpretation of the data is a bit challenging.</p>
<p>Below are suggestions for the authors and readers to consider:</p>
<p>(1) I understand why the authors prefer to mention claims as their primary means of reporting what they found, but it is nested within paper, and that makes it very hard to understand how to interpret these results at times. I also cannot understand at the high-level the relationship between claims and papers. The methods suggest there are 3-4 major claims per paper, but at 400 papers and 1,006 claims, this averages to ~2.5 claims per paper. Can the authors consider describing this relationship better (e.g., distribution of claims and papers) and/or considering presenting the data two ways (primary figures as claims and complimentary supplementary figures with papers as the unit). This will help the reader interpret the data both ways without confusion. I am also curious how the results look when presented both ways (e.g., does shifting to the paper as the unit of analysis shift the figures and interpretation?). This is especially true since the first and last author analysis shows there is varying distribution of papers and claims by authors (and thus the relationship between these is important for the reader).</p>
<p>(2) As mentioned above, I think the biggest weakness is that the authors are taking the literature at face value when assigning if a claim was validated or challenged vs gathering new independent evidence. This means the paper leans more on papers, making it more like a citation analysis vs an independent effort like other large-scale replication projects. I highly recommend the authors state this in their limitations section.</p>
<p>On top of that, I have questions that I could not figure out (though I acknowledge I did not dig super deep into the data to try). The main comment I have is How was verified (and challenged) determined? It seems from the methods it was determined by &quot;Claims were cross-checked with evidence from previous, contemporary and subsequent publications and assigned a verification category&quot;. If this is true, and all claims were done this way - are verified claims double counted then? (e.g., an original claim is found by a future claim to be verified - and thus that future claim is also considered to be verified because of the original claim).</p>
<p>Related, did the authors look at the strength of validation or challenged claims? That is, if there is a relationship mapping the authors did for original claims and follow-up claims, I would imagine some claims have deeper (i.e., more) claims that followed up on them vs others. This might be interested to look at as well.</p>
<p>(3) I recommend the authors add sample sizes when not present (e.g., Fig 4C). I also find that the sample sizes are a bit confusing, and I recommend the authors check them and add more explanation when not complete, like they did for Fig 4A. For example, Fig 7B equals to 178 labs (how did more than 156 labs get determined here?), and yet the total number of claims is 996 (opposed to 1,006). Another example, is why does Fig 8B not have all 156 labs accounted for? (related to Fig 8B, I caution on reporting a p value and drawing strong conclusions from this very small sample size - 22 authors). As a last example, Fig 8C has al 156 labs and 1,006 claims - is that expected? I guess it means authors who published before 1995 (as shown in Figure 8A continued to publish after 1995?) in that case, it's all authors? But the text says when they 'set up their lab' after 1995, but how can that be?</p>
<p>(4) Finally, I think it would help if the authors expanded on the limitations generally and potential alternative explanations and/or driving factors. For example, the line &quot;though likely underestimated' is indicated in the discussion about the low rate of challenged claims, it might be useful to call out how publication bias is likely the driver here and thus it needs to be carefully considered in the interpretation of this. Related, I caution the authors on overinterpreting their suggestive evidence. The abstract for example, states claims of what was found in their analysis, when these are suggestive at best, which the authors acknowledge in the paper. But since most people start with the abstract, I worry this is indicating stronger evidence than what the authors actually have.</p>
<p>The authors should be applauded for the monumental effort they put into this project, which does a wonderful job of having experts within a subfield engage their community to understand the connectiveness of the literature and attempt to understand how reliable specific results are and what factors might contribute to them. This project provides a nice blueprint for others to build from as well as leverage the data generated from this subfield, and thus should have an impact in the broader discussion on reproducibility and reliability of research evidence.</p>
</body>
</sub-article>
</article>