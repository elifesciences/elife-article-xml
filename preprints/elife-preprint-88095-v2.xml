<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">88095</article-id>
<article-id pub-id-type="doi">10.7554/eLife.88095</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.88095.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>An allocentric human odometer for perceiving distances on the ground plane</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhou</surname>
<given-names>Liu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wei</surname>
<given-names>Wei</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ooi</surname>
<given-names>Teng Leng</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6313-9016</contrib-id>
<name>
<surname>He</surname>
<given-names>Zijiang J.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">#</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychological and Brain Sciences, University of Louisville</institution>, Louisville, Kentucky 40292, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>College of Optometry, The Ohio State University</institution>, Columbus, Ohio 43210, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Meng</surname>
<given-names>Ming</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>#</label><email>zjhe@louisville.edu</email></corresp>
<fn id="n1"><label>*</label><p>E-mail: <email>ooi.22@osu.edu</email></p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-07-17">
<day>17</day>
<month>07</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-06-05">
<day>05</day>
<month>06</month>
<year>2024</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP88095</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-04-21">
<day>21</day>
<month>04</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-03-24">
<day>24</day>
<month>03</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.22.533725"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-07-17">
<day>17</day>
<month>07</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.88095.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.88095.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.88095.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.88095.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.88095.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Zhou et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Zhou et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-88095-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>We reliably judge locations of static objects when we walk despite the retinal images of these objects moving with every step we take. Here, we showed our brains solve this optical illusion by adopting an allocentric spatial reference frame. We measured perceived target location after the observer walked a short distance from the home base. Supporting the allocentric coding scheme, we found the intrinsic bias<sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref></sup>, which acts as a spatial reference frame for perceiving location of a dimly lit target in the dark, remained grounded at the home base rather than traveled along with the observer. The path-integration mechanism responsible for this can utilize both active and passive (vestibular) translational motion signals, but only along the horizontal direction. This anisotropic path-integration finding in human visual space perception is reminiscent of the anisotropic spatial memory finding in desert ants<sup><xref ref-type="bibr" rid="c3">3</xref></sup>, pointing to nature’s wondrous and logically simple design for terrestrial creatures.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>The revision mainly addressed helpful comments made by three eLife reviewers.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>When viewing natural scenes, we readily appreciate the visual space expanse that “fills” the scene being enveloped by the sky above and terrain below our feet (<xref rid="fig1" ref-type="fig">figure 1a</xref>). Notably, within this phenomenological visual space, our visual system employs the prevalent ground surface, where creatures and objects frequently interact, as a reference frame for coding spatial locations<sup><xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>. Empirical findings have revealed the ground-based spatial coding scheme accurately localizes objects when the horizontal ground surface is continuous and carries rich depth cues<sup><xref ref-type="bibr" rid="c6">6</xref>–<xref ref-type="bibr" rid="c14">14</xref></sup>. Attesting to the significant role of the ground surface, it has been found that object localization becomes inaccurate when the ground surface is disrupted by a gap or an occluding box<sup><xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c13">13</xref></sup>. Similarly, object localization is inaccurate in the dark when the ground is not visible<sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c15">15</xref></sup>. In fact, research conducted in total darkness to prevent the visual system from obtaining depth information from the ground reveals the visual system defaults to using an implicit curved surface representation that we refer to as the <italic><underline>intrinsic bias</underline></italic> to localize objects<sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c16">16</xref></sup>. <xref rid="fig1" ref-type="fig">Figure 1b</xref> illustrates the intrinsic bias’s curved representation in the dark (dashed white curve) that leads to an observer perceiving a dimly lit target (green ring) at the intersection (green disc) between its projection line from the eye and the intrinsic bias. Furthermore, when a parallel row of texture elements on the ground serves as the depth cue (<xref rid="fig1" ref-type="fig">figure 1c</xref>), object localization becomes more accurate. It is as if the objects are now located along a curved reference (solid white curve), which is less slanted than the intrinsic bias. This suggests the intrinsic bias, acting as the prototype spatial reference of the ground surface when visual cues are not visible, integrates with the visible depth cues such as texture gradients on the ground to form a new ground surface representation<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. In this way, the ground surface representation depends on the weighted contributions of the depth information on the ground and the intrinsic bias.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>The ground-based spatial coding scheme. a. Natural scenes are largely enveloped by the prevalent sky above us and the ground on which we stand. Evidence suggests target locations are judged with respect to the ground surface, which is our terrestrial niche. <bold>b.</bold> The visual system uses the intrinsic bias, an implicit curved surface representation in the dark (illustrated as dashed white curve), to locate a dimly-lit target (illustrated as unfilled green disc). The target is perceived (filled green disc) at the intersection between the projection line from the eye to the target and the intrinsic bias. Essentially, the intrinsic bias acts like the visual system’s internal model of the ground surface. <bold>c.</bold> When the ground becomes visible, the intrinsic bias integrates with the visible depth cues to form a ground surface representation, which serves as a reference frame to code target location. For example, the parallel rows of texture elements on the ground (illustrated as filled red circles) provide the depth cue for the visual system to construct a ground surface representation (solid white curve) that is less slanted than the intrinsic bias. This leads to a more accurate target localization than in the dark (b).</p></caption>
<graphic xlink:href="533725v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Nevertheless, current understanding of the ground-based spatial coding scheme is limited to situations where the tested observer stood still. Namely, just as in <xref rid="fig1" ref-type="fig">figure 1b</xref>, now redrawn in <xref rid="fig2" ref-type="fig">figure 2a</xref> and referred to as the <italic>baseline-stationary</italic> condition, the observer is instructed to stand still at one location during the test, which anchors his/her intrinsic bias on the ground under his/her feet. In this way, object location in visual space can be computed relative to the intrinsic bias. However, because retinal images of the stationary environment move when one moves (e.g., walk), it is unclear what additional computation the ground-based spatial coding scheme requires to construct a stable perceptual space. We proposed that during translational movements, such as walking, the brain anchors the intrinsic bias at one location on the ground surface using a <italic><underline>path</underline> <underline>integration</underline></italic>mechanism<sup><xref ref-type="bibr" rid="c18">18</xref>–<xref ref-type="bibr" rid="c25">25</xref></sup>. The path integration mechanism generates an estimate of the observer’s current position relative to their original location (home base) by integrating each step of traveled length and direction (vector). For example, when the observer walks forward in the dark, the path integration mechanism estimates the walked length by integrating each step length based on inputs from the vestibular and/or proprioception systems. The intrinsic bias accordingly shifts behind the body over a distance equaling the estimated walked distance; effectively placing the intrinsic bias at the home base should the estimated walked distance be accurate. This leads the visual system to construct the ground surface representation from the (allocentric) intrinsic bias that is fixed to the ground surface at the home base. By doing so, the visual system can obtain a stable allocentric ground surface representation. Accordingly, the ground-based reference frame can use an allocentric, i.e., world-centered, coordinate system to code locations in visual space.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Hypotheses and predictions of Experiment 1. <bold>a.</bold> <italic>Baseline-stationary</italic> condition. A non-moving, static observer perceives the dimly lit target (illustrated as unfilled green disc) at the intersection (illustrated as filled red disc) between the projection line from the eye to the target and the intrinsic bias (dashed white curve). <bold>b.</bold> The allocentric hypothesis predicts when the observer walks forward from the home base (illustrated as blue cross) to a new location (illustrated as red cross), the visual system relies on the path-integration mechanism to keep the intrinsic bias (illustrated as dashed blue curve) at the home base while monitoring the homing error vector (illustrated as white arrow). From the new location, he/she perceives the target (illustrated as unfilled green circle) at the intersection (illustrated as red disk) between the intrinsic bias anchored at the home base and the projection line from the eye to the target. <bold>c.</bold> The egocentric hypothesis predicts the intrinsic bias tags along with the observer’s body (illustrated as gray intrinsic bias) when he/she walks to the new location (illustrated as red cross). From the new location, the target (illustrated as unfilled green circle) is perceived (illustrated as red disk) at the same location as in the <italic>baseline-stationary</italic> condition (in <bold>a</bold>). <bold>c.</bold> Comparison of judged location of a target (unfilled green circle) before and after walking based (filled vs. unfilled red circles) on the allocentric hypothesis. <bold>c &amp; d.</bold> Comparison of predicted locations of a target (unfilled green circle) before and after walking (filled vs. unfilled red circles) based on the allocentric and egocentric hypotheses. Note the smaller separation between the filled vs. unfilled red circles in (c), suggesting better perceived positional stability with allocentric spatial coding.</p></caption>
<graphic xlink:href="533725v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The allocentric hypothesis predicts that during self-motion, the intrinsic bias is fixed to the ground location before the motion begins (home base) and remains at the same ground location during self-motion. This hypothesis stands in contrast to an alternative egocentric coordinate system hypothesis, which predicts the intrinsic bias moves along with the observer’s body. To distinguish between these two hypotheses, we began by measuring the intrinsic bias when the observer walked in the dark. Consider the scenario where an observer walks from his/her home base (blue cross added in figure for illustrative purpose) in the dark and stops at a new location (<xref rid="fig2" ref-type="fig">figure 2b</xref>; red cross added for illustration). According to the allocentric hypothesis, the intrinsic bias (blue) would remain at the home base. Thus, when he/she stops and is presented with a dimly lit test target (illustrated by the green ring), the observer would perceive the target to be nearer and higher (<xref rid="fig2" ref-type="fig">figure 2b</xref>) than if he/she had not walked from the home base (<xref rid="fig2" ref-type="fig">figure 2a</xref>; we shall dub this the <italic>baseline-stationary</italic> condition). In contrast, if the intrinsic bias had moved along with the observer to the new location (illustrated as the grey curve in <xref rid="fig2" ref-type="fig">figure 2c</xref>), as predicted by the alternative egocentric hypothesis, the observer would perceive the target at the same location as that in the <italic>baseline-stationary</italic> condition (<xref rid="fig2" ref-type="fig">figure 2a</xref>).</p>
<p>We have premised the allocentric hypothesis on the ground-based spatial coding scheme employing the path-integration (or spatial updating) system to maintain the intrinsic bias at the home base (blue cross in <xref rid="fig2" ref-type="fig">figure 2b</xref>). Previous studies have found that when performing spatial memory tasks, in conditions where visual cues are not visible in the dark, the path integration mechanism computes the traveled distance based on (non-visual) idiothetic distance information, such as from the vestibular, proprioceptive, and efference copy of motor control signals<sup><xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c25">25</xref></sup>. Thus, we extended this study to characterize the behaviors of the path integration mechanism in a visual spatial perception task, to investigate if the path integration mechanism behaved in a similar manner as in the spatial memory task.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Experiment 1: Testing Allocentric vs. Egocentric Hypothesis</title>
<p>We first verified the prediction of the allocentric hypothesis (<xref rid="fig2" ref-type="fig">figure 2</xref>) by testing a <italic>baseline-stationary</italic> condition (<xref rid="fig3" ref-type="fig">figure 3a</xref>) and a <italic>walking</italic> condition (<xref rid="fig3" ref-type="fig">figure 3b</xref>). In both conditions, the observer readied for the experiment by sitting on a chair in the waiting area, which was illuminated by a dimly lit LED light from the ceiling. To ready for a test trial, he/she waited for an audio tone. Upon hearing the tone, he/she stood up and aligned his/her feet at the start point (home base) that faced a black curtain in the direction of the testing area. About 30 sec later, the experimenter turned off the LED light in the waiting area and the observer drew the curtain open in the dark to begin the trial.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Experiment 1. a. <italic>Baseline-stationary</italic> condition: The observer stood at the home base (illustrated as blue cross) and judged the location of a briefly presented dimly-lit target. b. <italic>Walking</italic> condition: The observer walked blindly from the home base (illustrated as blue cross) for 1.5 m to the new location (illustrated as red cross). After a short delay (12 or 60 sec), the dimly-lit target was presented for him/her to judge the location. c. Graph plotting the average (n=8) judged target locations from Experiment 1. The black plus symbols, here and in other graphs, represent the physical target locations. The filled and open green triangle symbols represent the results for the 12 sec and 60 sec waiting periods, respectively, in the <italic>walking</italic> condition. The red circle symbols represent the <italic>baseline-stationary</italic> condition. With the 12 sec waiting period, judged locations (filled green triangles) in the <italic>walking</italic> condition were significantly nearer than in the <italic>baseline-stationary</italic> (filled red circles) condition. With the 60 sec waiting period, judged locations (open triangles) had a much smaller separation from the <italic>baseline-stationary</italic> condition. The blue and gray curves with the same shape are the intrinsic bias fitted to the data by eye. Their horizontal shift is about 1.35m. The black circle on the vertical axis represents the average eye height of the observers. d. Graph plotting the average (n=8) judged target locations from Experiment 2. The green triangle and red circle symbols represent results from the <italic>divided-attention-walking</italic> and <italic>baseline-stationary</italic> conditions, respectively. Judged locations from both conditions were similar and are fitted by the same intrinsic bias curve. Error bars represent the standard errors of the mean. e. &amp; f. Average judged angular declination as a function of the physical angular declination.</p></caption>
<graphic xlink:href="533725v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the <italic>baseline-stationary</italic> condition, after further waiting at the home base (blue cross) for either 12 or 60 sec, the observer saw a briefly presented (1 sec) dimly lit target. He/she was instructed to judge the target location and to respond by walking blindly to the remembered target location and gesturing its height after reaching the walked destination<sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c7">7</xref>–<xref ref-type="bibr" rid="c10">10</xref></sup>. (While walking, the observer’s right hand glided along a horizontal guidance rope, drawn as the yellow line in <xref rid="fig3" ref-type="fig">figure 3a</xref>). Effectively, this blind walking-gesturing task reveals the perceived target location (x: walked distance; y: gestured height).</p>
<p>In the <italic>walking</italic> condition (<xref rid="fig3" ref-type="fig">figure 3b</xref>), the observer walked blindly from the home base (blue cross) to a new location (red cross). While walking, the observer’s right hand glided along a guidance rope and stopped at the new location when he/she made contact with a soft plastic wrap on the rope (1.5m from the home base). After waiting at the new location for either 12 or 60 sec, he/she saw a briefly presented (1 sec) dimly lit target. He/she then performed the blind walking-gesturing task to indicate the perceived target location.</p>
<p><xref rid="fig3" ref-type="fig">Figure 3c</xref> shows the average results of the two conditions. With the 12 sec waiting period, judged locations in the <italic>walking</italic> condition (filled green triangles) were significantly nearer than the <italic>baseline-stationary</italic> (filled red circles) condition (<italic>P</italic>&lt;0.0001; please refer to the supplement for details of all statistical analyses). The two sets of data points are fitted by the same shaped intrinsic bias curve with about 1.35 m horizontal separation, which is close to the initially walked distance from the home base (1.5 m; in the <italic>walking</italic> condition). This result thus confirms the prediction of the allocentric hypothesis (<xref rid="fig2" ref-type="fig">figure 2b</xref>). With the 60 sec waiting period, judged locations (open triangles) in the <italic>walking</italic> condition had a smaller, though statistically significant, separation from the <italic>baseline-stationary</italic> condition (open circles) (<italic>P</italic>&lt;0.01). This suggests if the waiting period was sufficiently long, suggesting the visual memory of the home base decays over time<sup><xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c19">19</xref></sup>, the visual system automatically resets the intrinsic bias to the observer’s (new/current) location, i.e., making it the updated home base. <xref rid="fig3" ref-type="fig">Figure 3e</xref> plots the average judged angular declination as a function of the physical angular declination for all conditions. The slopes of the regression lines obtained with the least squared methods are close unity, suggesting perceived angular declinations of the targets were largely accurate.</p>
<p>The results above demonstrate the action of the path integration mechanism. It tracks the observer’s location with respect to the external world during locomotion and plays a critical role in maintaining an allocentric reference frame. In the <italic>walking</italic> condition, the path-integration mechanism computes the traveled distance relative to the home base. Experiments 2-4 below explored the characteristics of the path-integration mechanism.</p>
</sec>
<sec id="s2b">
<title>Experiment 2: Path Integration Affected by Cognitive Load</title>
<p>Since in the <italic>walking</italic> condition (<xref rid="fig3" ref-type="fig">figure 3b</xref>) observers were simply attentive to the task at hand but otherwise neutral and not subjected to excessive cognitive demands, one might assume that the path-integration mechanism operates automatically. To test this assumption, we investigated whether the path-integration mechanism requires some attentional resources to function normally<sup><xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref></sup>. We thus tested a new, <italic>divided attention walking</italic> condition, where the observer continuously performed a cognitive task, counting number backward, while walking from the home base to the new location. We predicted this would reduce the attention resources available for path-integration. For comparison, we also tested the <italic>baseline-stationary</italic> condition as in Experiment 1 (<xref rid="fig3" ref-type="fig">figure 3a</xref>).</p>
<p>The average results in <xref rid="fig3" ref-type="fig">figure 3d</xref> reveal a small, though significant, difference between the <italic>divided attention walking</italic> condition and the <italic>baseline-stationary</italic> conditions (<italic>P</italic>=0.022). Noticeably, the distance underestimation in the <italic>divided attention walking</italic> condition (<xref rid="fig3" ref-type="fig">figure 3d</xref>) was much smaller than that found in Experiment 1 (<xref rid="fig3" ref-type="fig">figure 3c</xref>). This suggests path-integration is less effective when observers were engaged in another mental task that distracted their spatial attention. <xref rid="fig3" ref-type="fig">Figure 3f</xref> plots the average judged angular declination as a function of physical angular declination for both conditions. The regression lines from the two conditions are similar.</p>
</sec>
<sec id="s2c">
<title>Experiment 3: Path-integration from Vestibular Input</title>
<p>We addressed two issues regarding the idiothetic distance cues used by the path-integration mechanism. First, does it operate only when the observer’s movement is self-initiated? Second, does it operate when observer moves in the backward direction? To answer these questions, we moved the observers passively to stimulate their vestibular system<sup><xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c28">28</xref>–<xref ref-type="bibr" rid="c29">29</xref></sup>. That is, instead of instructing the observer to walk in the dark from the home base, he/she stood upright on a rolling platform that was moved by the experimenter (insets in <xref rid="fig4" ref-type="fig">figure 4</xref>). Doing so negated the contributions of the proprioception and motor efference copy information to path-integration while keeping the vestibular information intact.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Results of Experiment 3 (a, b) and Experiment 4 (e). a. <italic>Vestibular-forward</italic> condition: The blue square and green triangle symbols represent the <italic>vestibular-forward</italic> and <italic>walking</italic> conditions, respectively, and they show that judged target locations were similar (n=8). These judged target locations were nearer than those from the <italic>baseline-stationary</italic> condition (red circles). b. <italic>Vestibular-backward</italic> condition: Backward translation caused judged target locations (blue squares) to be farther than those from the <italic>baseline-stationary</italic> condition (red circles) (n=8). c &amp; d. Average judged angular declination as a function of the physical angular declination. e. Experiment 4. Verbally reported results of <italic>vestibular-forward</italic> condition show shorter reported eye-to-target distances (blue squares) than in the baseline-stationary (red disc) condition (n=8). Error bars represent standard errors of the mean.</p></caption>
<graphic xlink:href="533725v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We first tested a new <italic>vestibular-forward</italic> condition in which the experimenter pushed the observer on the platform forward by 1.5 m to the new location. Once at the new location, the observer stepped down from the platform and stood on the floor to wait for the dimly lit test target to be presented. He/she responded with the blind walking-gesturing task. For comparison, we also tested the <italic>walking</italic> (<xref rid="fig3" ref-type="fig">figure 3b</xref>) and <italic>baseline-stationary</italic> (<xref rid="fig3" ref-type="fig">figure 3a</xref>) conditions as in Experiment 1. <xref rid="fig4" ref-type="fig">Figure 4a</xref> reveals the judged target locations in the <italic>vestibular-forward condition</italic> (blue squares) were similar to the <italic>walking</italic> condition (green triangles; <italic>P</italic>=0.718), and were significantly nearer than the <italic>baseline-stationary</italic> condition (red circles; <italic>P</italic>&lt;0.0001). This indicates the path-integration mechanism can function during passive movements that stimulate the vestibular system.</p>
<p>To answer the second question, we then tested a complementary condition wherein the experimenter pulled the rolling platform supporting the observer backward by 1.5 m from the home base (<italic>vestibular-backward</italic> condition). As expected, the backward displacement caused judged target locations to be farther than the <italic>baseline-stationary</italic> condition (<italic>P</italic>&lt;0.0001; <xref rid="fig4" ref-type="fig">figure 4b</xref>). Taken together, the experiment reveals the path-integration mechanism can sufficiently utilize the vestibular cue in both forward and backward moving directions.</p>
<p><xref rid="fig4" ref-type="fig">Figures 4c</xref> and <xref rid="fig4" ref-type="fig">4d</xref> plot the average judged angular declination as a function of physical angular declination. The judged angular declinations are similar for targets with smaller angular declinations that were located farther from the observers. However, for nearer targets, the judged angular declination for the forward and backward conditions had a small difference with respect to the baseline-stationary condition. A similar pattern is also found in <xref rid="fig4" ref-type="fig">figure 4e</xref>. Further research, beyond the scope of this paper, is required to investigate this observation.</p>
</sec>
<sec id="s2d">
<title>Experiment 4: The Allocentric Principle Is Not Task Specific</title>
<p>Previous studies of space perception with stationary observers not undergoing self-motion measured with different response tasks have shown a concordance in finding. Specifically, the perceptual effects found were similar between an action-based task (blind walking-gesturing) and a perception-based task (e.g., verbal reports and perceptual matching) <sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c30">30</xref></sup>. To confirm that this is also true for observers undergoing self-motion, we repeated the <italic>vestibular-forward</italic> and the <italic>baseline-stationary</italic> conditions in Experiment 3 and employed the verbal report task, wherein the observer verbally reported the perceived eye-to-target distances in feet or meters. We found that verbally reported distances were significantly shorter in the <italic>vestibular-forward</italic> than <italic>the baseline-stationary</italic> condition (<italic>P</italic>&lt;0.0001, <xref rid="fig4" ref-type="fig">figure 4e</xref>). Thus, both the verbal report (<xref rid="fig4" ref-type="fig">figure 4e</xref>) and blind walking-gesturing (<xref rid="fig4" ref-type="fig">figure 4a</xref>) tasks show that forward moving in the dark before seeing the dimly lit target can affect judged target distance.</p>
</sec>
<sec id="s2e">
<title>Experiment 5: Horizontal/Vertical Asymmetry in Path-integration – A Bias for the Ground Surface</title>
<p>An assumption underlying the allocentric hypothesis is that adoption of the ground-based spatial coding scheme is fitting for our terrestrial existence, where everyday activities, including navigation, are performed with respect to the horizontal ground surface<sup><xref ref-type="bibr" rid="c4">4</xref></sup>. This dependence on the ground surface, i.e., an ecological constraint, predicts when the observer travels on a sloping surface in the dark, the visual system would not be able to simultaneously maintain the intrinsic bias at the home base in the vertical and horizontal directions. To investigate this prediction of anisotropic path-integration, we tested a new, <italic>stepladder</italic> condition, where the observer descended a stepladder in the dark (<xref rid="fig5" ref-type="fig">figure 5a</xref>). As the self-motion here consisted of both horizontal forward and vertical downward vectors, we predicted the intrinsic bias would be spatially updated in the horizontal but not in the vertical direction. This prediction is illustrated in <xref rid="fig5" ref-type="fig">figure 5a</xref> where the horizontal coordinate of the intrinsic bias (blue curve) remains at the home base location on the floor while the vertical coordinate travels along with the observer. Thus, upon stepping down from the stepladder and standing on the floor, the observer underestimates the horizontal distance of a dimly lit target. We expected that the target underestimation will be similar to that of a <italic>horizontal-walking</italic> condition (<xref rid="fig5" ref-type="fig">figure 5b</xref>). In contrast, if the intrinsic bias is spatially updated in both the horizontal and vertical dimensions, the intrinsic bias will remain at the top of the staircase (orange curve, <xref rid="fig5" ref-type="fig">figure 5a</xref>). The perceived target location will then be dramatically different from the predicted horizontal-only-updating strategy (<xref rid="fig5" ref-type="fig">figure 5b</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Experiment 5: Predictions (<bold>a &amp; b</bold>) and average results (<bold>c</bold>). <bold>a.</bold> <italic>Stepladder</italic> condition where the observer descends from the stepladder in the dark. If the path-integration process only gauges the horizontally traveled distance, the intrinsic bias would be path integrated in the horizontal but not vertical direction. As such, the horizontal coordinate of the intrinsic bias (blue curve) is kept at the home base location on the floor while the vertical coordinate travels along with the observer. Thus, upon stepping down from the stepladder and standing on the floor, the observer underestimates the horizontal distance of the target. The target underestimation would be similar to that of the <italic>horizontal-walking</italic> condition depicted in <bold>b</bold>. In contrast, if the path-integration process integrates distance in both the horizontal and vertical directions, the intrinsic bias will remain at the top of the staircase (orange curve). The perceived target location will then be dramatically different from that in the <italic>horizontal-walking</italic> condition. <bold>b.</bold> <italic>Horizontal-walking</italic> condition. <bold>c</bold>. The average results of the <italic>baseline-stationary</italic>, <italic>horizontal-walking</italic> and <italic>stepladder</italic> conditions, respectively represented by the red circle, green square and blue triangle symbols (n=9). The judged horizontal distances are significantly shorter in the <italic>horizontal-walking</italic> condition than in the <italic>baseline-stationary</italic> conditions. The two sets of data points are fitted by the same intrinsic bias profile with a horizontal separation of 1.0 m, which is close to the walked distance (1.06 m) from the home base to the new location in the <italic>horizontal-walking</italic> condition. Of significance, the judged locations in the <italic>stepladder</italic> condition (blue triangles) are similar to that in the <italic>horizontal-walking</italic> condition (green squares). This confirms that the path-integration process mainly gauges the horizontal (ground) distance travelled. <bold>d.</bold> Average judged angular declination as a function of the physical angular declination.</p></caption>
<graphic xlink:href="533725v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig5" ref-type="fig">Figure 5c</xref> depicts the average judged target locations (n=9) from the <italic>stepladder</italic> (blue triangles), <italic>horizontal-walking</italic> (green square) and <italic>baseline-stationary</italic> (red circles) conditions (the <italic>baseline-stationary</italic> condition’s setup was the same as in <xref rid="fig3" ref-type="fig">figure 3a</xref>). A comparison between the <italic>horizontal-walking</italic> and the <italic>baseline-stationary</italic> conditions reveals the judged horizontal distances were significantly shorter in the <italic>horizontal-walking</italic> condition (P&lt;0.001). The two sets of data are fitted by the same intrinsic bias profile with a horizontal separation of 1.0 m, which is close to the walked distance (1.06 m) in the <italic>horizontal-walking</italic> condition.</p>
<p>The judged horizontal distances in the <italic>stepladder</italic> condition were significantly shorter than that in the <italic>baseline-stationary</italic> condition (P&lt;0.001). This confirms the prediction that while descending the stepladder, the visual system spatially updated the horizontal but not the vertical vector of the intrinsic bias (blue intrinsic bias, <xref rid="fig5" ref-type="fig">figure 5a</xref>). Further supporting this, we found the data from the <italic>stepladder</italic> and the <italic>horizontal-walking</italic> conditions overlap substantially. <xref rid="fig5" ref-type="fig">Figures 5d</xref> shows the average judged angular declination as a function of physical angular declination for all three conditions.</p>
<p>Taken together, our experiment revealed when stepping down from a stepladder, the horizontal coordinate of the intrinsic bias is kept at the home base on the floor while the vertical coordinate moves downward with the body. This suggests human’s path-integration mechanism, responsible for allocentric spatial coding, operates much more efficiently along the horizontal ground plane than in the vertical direction.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We constantly move relative to the environment, which causes our retinal images of the environment to also be in constant motion. Yet, we reliably localize objects as we navigate our environment. This raises the question of how our visual system creates a stable visual space for us to operate and plan future actions, such as walking, despite the inadequate retinal inputs. While it was known the visual system relies predominantly on the ground surface to accurately localize objects, it was unclear what coordinate system it uses for coding visual space. In this study, we proposed the visual system uses an allocentric, or world-centered, coordinate system for the ground-based reference. In this way, the visual system could alleviate the challenge of constant retinal motion during self-motion by streamlining online location computation to only the moving observer and the object of interest. The locations of surrounding objects are anchored to a fixed position on the ground negating excessive computation, thereby reducing coding redundancy. Our main finding provides support for the allocentric hypothesis. We showed during self-motion, the intrinsic bias that acts as the spatial reference of the ground surface when visual cues are not visible, is fixed to the ground location before the motion begins (home base) and remains at the same ground location during self-motion.</p>
<p>Previous studies mainly tested location judgments of observers who stood stationary at one location on the ground while viewing a stationary target (similar to our <italic>baseline-stationary</italic> condition). Because the observers need not generate significant self-motion in those studies, it was not possible to investigate the allocentric hypothesis. The previous findings with stationary observers showed that the visual system uses the ground surface representation as a reference frame for representing object locations<sup><xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c15">15</xref></sup>. One may thus claim that the ground-based spatial coordinate system is world-centered or allocentric. However, it is hard to reject the egocentric hypothesis since the origin of the spatial coordinate system is attached to the surface underneath one’s feet while standing still on the ground. In a way, the egocentric hypothesis is implicitly assumed because it is less complex than the allocentric hypothesis. After all, without body motion, the allocentric coordinate system does not have an advantage over the egocentric coordinate system in terms of processing efficiency.</p>
<p>The visual system relies on the path-integration mechanism to ground the intrinsic bias at the home base. The path-integration mechanism can function when the observer’s motion is passive, suggesting the traveled distance can be based on the vestibular signals alone. Also, the path-integration mechanism fails to operate efficiently when the observer’s attention is distracted to an unrelated cognitive demand during walking. This suggests some degree of attentional effort is required to reliably path integrate an observer’s body location relative to the environment. Lastly, we showed the path-integration mechanism is more efficient in the horizontal than the vertical direction (anisotropic with a ground-bias). This observation further highlights one of the most important design principles underlying our sensory and motor systems, namely, fitting our terrestrial niche. We inhabit a land-based environment and most of our daily activities are structured relative to the ground. The current finding on the vertical-horizontal asymmetry of the path integration mechanism for visual space perception is consistent with previous studies on spatial memory, which revealed the path-integration systems of land-dwelling animals are more efficient in estimating horizontally traveled distance<sup><xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c31">31</xref>–<xref ref-type="bibr" rid="c34">34</xref></sup>. In particular, our observation is reminiscent of the behavioral studies of the desert ants, Cataglyphis fortis. Wohlgemuth et al (2001)<sup><xref ref-type="bibr" rid="c3">3</xref></sup> found desert ants used path integration to return from foraging excursions on a shortcut way to their nests (also see Ronacher, 2020)<sup><xref ref-type="bibr" rid="c34">34</xref></sup>. Specifically, it was revealed when the ants were crawling over a slope surface with uphill and downhill trajectory, their path integration system computed only the horizontal component of the traveled distance. We note, however, further research is needed to investigate whether the same path-integration mechanism is employed for a spatial memory task (previous studies by others) and a spatial perception task (current study).</p>
<p>Overall, our study suggests the visual system can create a world-centered visual space by anchoring its spatial reference frame to the ground during navigation. One advantage of adopting this space coding strategy is that when we travel, the spatial representations of most surrounding objects, which are static relative to the ground, will remain constant (static) despite the retinal image motion. Accordingly, our visual system only needs to dynamically update the spatial representations of our bodies and the few objects that are moving relative to an allocentric origin on the ground (within a world-centered context). A consequential byproduct of operating within the framework of an allocentric, world-centered visual space is the perceived stability of our visual environment during locomotion.</p>
<p>Let us consider the following hypothetical situation of a forward walking condition in the dark. The observer sees a target briefly before walking forward over a short distance, and again after they reach the new destination. <xref rid="fig2" ref-type="fig">Figures 2d</xref> and <xref rid="fig2" ref-type="fig">2e</xref> illustrate the perceived target locations, respectively, if the visual system adopts the allocentric or egocentric coding strategy. Noticeably, the perceived target locations differ before (open red circle) and after walking (closed circle) as shown in both figures. However, the separation between the perceived target locations is smaller in <xref rid="fig2" ref-type="fig">figure 2d</xref> (allocentric coding) than <xref rid="fig2" ref-type="fig">figure 2e</xref> (egocentric coding), indicating more perceived location stability in the former. Although, this position stability has an accuracy trade-off as the perceived target locations after the short walk are more underestimated in <xref rid="fig2" ref-type="fig">figure 2d</xref> (allocentric) than <xref rid="fig2" ref-type="fig">figure 2e</xref> (egocentric). Nevertheless, there is reason to believe that the location inaccuracy from using the allocentric spatial coding strategy will be minimized in the full cue environment, where the influence of the intrinsic bias is less. This is because when the ground is not visible in the dark, the surface slant of the intrinsic bias is responsible for the distance underestimation of a target on the ground (<xref rid="fig1" ref-type="fig">figure 1b</xref>). When there is some sparse texture information on the ground (<xref rid="fig1" ref-type="fig">figure 1c</xref>), the ground surface representation becomes less slanted than the intrinsic bias and the distance underestimation error is less. Accordingly, the underestimation error due to using the allocentric spatial coding strategy will be further reduced (unpublished data). Therefore, a full cue condition with rich texture gradient information on the ground will render the ground surface representation largely veridical, resulting in accurate distance perception.</p>
<p>The observations reported here support the proposal that during observer self-motion the brain establishes an allocentric intrinsic bias by employing the path integration mechanism. By utilizing the allocentric intrinsic bias, the visual system is able to construct an allocentric ground reference frame. Accordingly, the ground-based visual space can effectively program and guide navigation of terrestrial creatures on the ground surface. Given its significance, it is natural to ask where in the brain the intrinsic bias, a prototype model of the ground surface, is implemented. In this respect, we are reminded that the 2D cognitive map (long-term spatial memory) employed by land dwelling animals is also linked to the terrain where they travel <sup><xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c38">38</xref></sup>. Therefore, it is reasonable to speculate that the intrinsic bias could be a function of the mammalian hippocampal formation that is well known to be responsible for directing navigation when spatial sensory inputs are impoverished or lacking such as in the dark. Consistent with this speculation, Nau et al (2018) pointed to growing evidence that “the mammalian hippocampal formation, extensively studied in the context of navigation and memory, mediates a representation of visual space that is stably anchored to the external world”.<sup><xref ref-type="bibr" rid="c39">39</xref></sup></p>
<p>Following this line of thinking, we further speculate that the intrinsic bias that represents the internal model of the visual ground surface is implemented in the spatial memory system. As we mentioned earlier, our visual system employs the ground surface representation as a spatial reference frame for spatial coding, fitting our terrestrial niche. It is thus tempting to adopt the idea that the spatial memory systems of land dwellers employ the same spatial coding, with the cognitive map serving as an internal model of the ground and the associated spatial features (walls and obstacles) that are supported by the ground. As such, the intrinsic bias is the basis not only for the cognitive map within the medial temporal lobe (MTL) but also for visual ground surface representation within the perceptual cortex <sup><xref ref-type="bibr" rid="c40">40</xref>–<xref ref-type="bibr" rid="c42">42</xref></sup>. Future investigations should explore the broader notion that the concept of the intrinsic bias could be genetically or epigenetically coded, serving as an important a priori knowledge for the brain to represent the ground surface and to construct our inner spatial world <sup><xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c44">44</xref></sup>.</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<p><bold>Observers.</bold> Fourteen observers (age=24.47 ± 1.45 years old; eye height= 1.60 ± 0.02 m; 7 males and 7 females) participated in Experiments 1-4 with 8 observers for each experiment. Nine observers (age=22.78 ± 1.22 years old; eye height= 1.58 ± 0.04 cm; 6 males and 3 males) participated in Experiment 5. They were naïve to the purpose of the study and signed the informed consent form at the start of the study. All observers had normal, or corrected-to-normal, visual acuity (at least 20/20) and a stereoscopic resolution of a 20 arc sec or better. They viewed the visual scene binocularly. A within-subject experimental design was used. The study protocol was approved by the Institutional Review Board and followed the tenets of the Declaration of Helsinki.</p>
<sec id="s4a">
<title>General stimulus and testing environment</title>
<p>All experiments 1-4 were performed in a dark room whose layout and dimensions were unknown to the observers. One end of the room, just before the testing area, served as the waiting area (∼3 m<sup>2</sup>) for the observer. The waiting area had a chair facing the wall for the observer to sit in between trials so that his/her back faced the testing area. Two white LED lights on the ceiling provided ambient illumination while the observer waited in between trials. The testing and waiting areas were separated by a black curtain. A long guidance rope (0.8 m above the floor) was tied to both ends of the room and served to guide the observer while walking blindly. A plastic wrap was tied to the guidance rope on the part of the rope located in the waiting area near the curtain to mark the start point (home base). To ready for a trial, the observer walked to this start point and faced the test area while holding onto the plastic wrap on the rope and called out “ready”. The curtain was then drawn open for the trial to begin.</p>
<p>The dimly lit test target used in all experiments was a diffused green LED (0.16 cd m<sup>-2</sup>). The LED was placed in the center of a ping-pong ball that was encased in a small opaque box. An adjustable iris-diaphragm aperture was placed at the front of the ping-pong ball to keep its visual angular size constant at 0.22° when measured at the eye level. During testing, the target was displayed with a 5 Hz flicker for 1 sec. Music was played aloud during the entire experimental session to mask extraneous auditory information during the experiments.</p>
<p><bold>Observer’s response tasks.</bold> The main task used in all experiments was the <italic>blind walking-gesturing task</italic> <sup><xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref></sup>. For each trial, the observer stood by the guidance rope in the dark and judged the location of the 1 sec flickering target. After which he/she then put on the blindfold and called out “ready to walk”. This signaled the experimenter to quickly remove the target and shook the guidance rope to indicate it was safe to walk. The observer walked while sliding his/her right hand along the guidance rope until he/she reached the remembered target location. Once there, he/she indicated the remembered target height with his/her left hand and called out “done”. The experimenter turned on the flashlight, marked the observer’s feet location, measured the gestured height, and asked the observer to turn around and walk back to the start point using the guidance rope. When the observer arrived at the start point, the experimenter turned on the ceiling LED lights in the waiting area. The observer then removed the blindfold, sat down, and waited for the next trial.</p>
<p>An additional task used, in Experiment 4, was the <italic>verbal reporting task</italic> <sup><xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c37">37</xref></sup>. Here, the observer stood next to the guidance rope in the dark and viewed the target for 5 sec to judge its absolute distance between the target and his/her eyes. Once the target was turned off, an audio tone was presented to signal the observer to immediately report the estimated distance either in meters or feet. For both this and the blind walking-gesturing task, the observer was provided five practice trials before each test session. No feedback regarding performance was provided to the observer during the practice or test session.</p>
</sec>
<sec id="s4b">
<title>Experiment 1</title>
<sec id="s4b1">
<title>Design</title>
<p>Two viewing conditions were tested: <italic>walking</italic> and <italic>baseline-stationary</italic>. Each condition was tested separately with a 12 sec and a 60 sec waiting period. The target was placed at one of six locations in both conditions. Four locations were on the floor at 3, 4.25, 5.25, or 6.5 m from the observer, the fifth location was 0.5 m above the floor at 6.5 m from the observer, and the sixth location was at the observer’s eye level and 6.5 m from the observer. Testing of the 4.25, 5.25, and 6.5 m targets on the floor were repeated three times while testing of the remaining targets was repeated twice. A total of 60 trials were run over two days. The order of stimulus presentation was randomized.</p>
</sec>
<sec id="s4b2">
<title>Procedure</title>
<p>While the observer sat at the waiting area before each trial, he/she was informed of the upcoming test condition (<italic>baseline-stationary</italic> or <italic>walking</italic>) and of the waiting period (12 sec or 60 sec). After that, an audio tone was presented to signal to the observer to walk to the start point (home base) and face the black curtain in the direction of the testing area. About 30 sec later, the experimenter turned off the ambient LED lights in the waiting area and the observer drew the curtain open in the dark.</p>
<p>For the <italic>baseline-stationary</italic> condition, the observer stood in the dark at the start point (home base) over the predetermined waiting duration (12 or 60 sec). He/she was instructed to stand upright with minimal head motion during the waiting period, and to expect hearing a pure tone at the end of the waiting period. Roughly, two sec after hearing the tone, the test target was presented at one of the six predetermined locations. The observer’s task was to judge its location and perform the blind walking-gesturing task. For the <italic>walking</italic> trial, the observer stood at the start point until a white noise (instead of pure tone) was heard. He/she then walked forward until his/her right hand touched a plastic wrap tied on the guidance rope at the new location (1.5m from the start point). He/she then stopped walking, called out “ready” and waited there for either 12 sec or 60 sec before the test target was presented. The remaining procedural steps were the same as in the <italic>baseline-stationary</italic> condition.</p>
</sec>
</sec>
<sec id="s4c">
<title>Experiment 2</title>
<sec id="s4c1">
<title>Design and Procedure</title>
<p>Both the <italic>baseline-stationary</italic> and <italic>walking</italic> conditions with the 12 sec waiting period were tested but with one modification. The modification was that during the 12 second waiting period at the start point (home base) (for <italic>baseline-stationary</italic> condition), or while walking to, and waiting at the new location (for <italic>walking</italic> condition), the observer performed a counting task. The experimenter would provide a random number between 50 and 99 for the observer to count backward. The same six target locations tested in Experiment 1 were tested here. A total of 30 trials were run. The order of stimulus presentation was randomized.</p>
</sec>
</sec>
<sec id="s4d">
<title>Experiment 3</title>
<sec id="s4d1">
<label>3.1</label>
<title>Vestibular-forward condition</title>
<sec id="s4d1a">
<title>Design</title>
<p>The two conditions from Experiment 1 (<italic>walking</italic> and <italic>baseline-stationary</italic> conditions with the 12 seconds waiting period) and a new, <italic>vestibular-forward</italic> condition were tested. The latter <italic>vestibular-forward</italic> condition was the same as in the <italic>walking</italic> condition, except that the observer now stood on a rolling-platform that was pushed forward by the experimenter for 1.5 m. The rolling-platform had four wheels that was 0.30 m above the floor. The same six target locations tested in Experiment 1 were tested here. There were 60 trials in total with 30 trials tested per day. The order of stimulus presentation was randomized.</p>
</sec>
<sec id="s4d1b">
<title>Procedure</title>
<p>Two experimenters (A and B) conducted the experiment. Before each trial, experimenter A instructed the observer of the upcoming test condition and to prepare for an audio signal. Once the signal was heard, the observer stepped onto the rolling-platform at the start point (home base) behind the curtain with eyes opened and facing the test area. He/she then called out “ready”. A second audio signal was presented after 30 seconds. A pure tone indicated to the observer <underline>to stay</underline> (<italic>baseline-stationary</italic> condition), while a white noise indicated <underline>to walk</underline> (<italic>walking</italic> condition) or <underline>to be pushed forward</underline> (<italic>vestibular-forward</italic> condition). At the same time, experimenter A turned off the LED lights in the waiting area, and the observer drew the curtain open. For the <italic>baseline-stationary</italic> condition trial, the observer stepped down from the rolling-platform, stood at the start point, and called out “ready to view”. For the <italic>walking</italic> condition trial, the observer stepped down from the rolling-platform, and walked forward until he/she touched the plastic wrap tied onto the guidance rope at the new location (1.5 m), and then stopped and called out “ready to view”. For the <italic>vestibular-forward</italic> condition trial, the observer kept standing on the rolling-platform. Experimenter B verbally informed the observer the trial was starting and pushed the rolling platform forward by 1.5 m to the new location. Upon arrival, Experiment B instructed the observer to step down from the rolling platform and to call out “ready to view”. For all three trial types, after the observer called out “ready to view”, Experimenter A turned on the test target after a 12 sec waiting period for the observer to judge its location. He/she then responded by performing the blind walking-gesturing task.</p>
</sec>
</sec>
<sec id="s4d2">
<label>3.1</label>
<title>Vestibular-backward condition</title>
<sec id="s4d2a">
<title>Design and procedure</title>
<p>The <italic>baseline-stationary</italic> condition was the same as in 3.1 above. The <italic>vestibular-backward</italic> condition was modified from the <italic>vestibular-forward</italic> in 3.1 above, by pulling the rolling-platform backward for 1.5 m. The same six target locations as in 3.1 were tested. A total 30 of trials were tested in one session. The order of stimulus presentation was randomized.</p>
</sec>
</sec>
</sec>
<sec id="s4e">
<title>Experiment 4</title>
<sec id="s4e1">
<title>Design</title>
<p>The two conditions tested were the <italic>vestibular-forward</italic> and <italic>baseline-stationary</italic> conditions as in 3.1 above. But instead of the six test target locations, only five target locations on the floor were tested. The target location at 6.5 m distance and 0.5m above the ground was not tested. All targets were presented for 5 sec. Twenty-eight trials in total were tested in one session. The order of stimulus presentation was randomized.</p>
</sec>
<sec id="s4e2">
<title>Procedure</title>
<p>The procedure was the same as in 3.1 above, except the observers performed the verbal report task. To do so, the observer judged the absolute distance between the target and his/her eyes. Immediately after the target was turned off, an audio signal was presented to prompt the observer to report the estimated distance aloud either in meters or feet.</p>
</sec>
</sec>
<sec id="s4f">
<title>Experiment 5</title>
<sec id="s4f1">
<title>Design</title>
<p>Three conditions (<italic>baseline-stationary</italic>, <italic>horizontal-walking</italic> and <italic>stepladder</italic>) were tested in two different blocks. The <italic>horizontal-walking</italic> and <italic>baseline-stationary</italic> conditions were mixed in test block-A, while the <italic>stepladder</italic> and <italic>baseline-stationary</italic> conditions in test block-B. Each block consisted of 20 test trials. The order of test conditions within each block was randomized. Each block was tested in a daily session and ran twice. The testing order of the four blocks was alternated between observers.</p>
</sec>
<sec id="s4f2">
<title>Procedure</title>
<p>Before each trial, the experimenter informed the observer of the condition to be tested. The <italic>baseline-stationary</italic> and <italic>horizontal-walking</italic> conditions were conducted as in the main experiment (Experiment 1). For the <italic>stepladder</italic> condition, the observer first ascended the stepladder, and waited for an audio tone that signaled to descend the stepladder with eyes opened and looking at the invisible horizon (in the dark). Upon reaching the foot of the stepladder, he/she held onto the guidance rope and walked forward until he/she felt the plastic wrap on the rope at the new location. The observer then stood still and waited for 12 s before the test target was presented. After judging the target’s location, he/she put on the blindfold and called out “ready” to begin the blindfolded walking-gesturing task.</p>
</sec>
<sec id="s4f3">
<title>Statistical tests</title>
<p>Data were analyzed using analysis of variance with repeated measures. The Mauchly’s test was applied to verify the assumption of sphericity.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>He</surname>, <given-names>Z. J</given-names></string-name>. <article-title>Distance determined by the angular declination below the horizon</article-title>. <source>Nature</source> <volume>414</volume>, <fpage>197</fpage>–<lpage>200</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>He</surname>, <given-names>Z. J</given-names></string-name>. <article-title>Perceptual space in the dark affected by the intrinsic bias of the visual system</article-title>. <source>Perception</source> <volume>35</volume>, <fpage>605</fpage>–<lpage>624</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Wohlgemuth</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ronacher</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Wehner</surname>, <given-names>R</given-names></string-name>. <article-title>Ant odometry in the third dimension</article-title>. <source>Nature</source> <volume>411</volume>, <fpage>795</fpage>–<lpage>798</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="book"><string-name><surname>Gibson</surname>, <given-names>J. J</given-names></string-name>. <source>The Perception of the Visual World</source> (<publisher-name>Houghton Mifflin</publisher-name>, <year>1950</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="book"><string-name><surname>Sedgwick</surname>, <given-names>H. A.</given-names></string-name> <collab>Space perception</collab>. in <source>Handbook of Perception and Human Performance</source> (eds <string-name><surname>Boff</surname>, <given-names>K. R.</given-names></string-name> <etal>et al.</etal>) <fpage>21.1</fpage>–<lpage>21.57</lpage> (<publisher-name>New York Wiley</publisher-name>, <year>1986</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Sinai</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name> &amp; <string-name><surname>He</surname>, <given-names>Z. J</given-names></string-name>. <article-title>Terrain influences the accurate judgement of distance</article-title>. <source>Nature</source> <volume>395</volume>, <fpage>497</fpage>–<lpage>500</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Thomson</surname>, <given-names>J. A</given-names></string-name>. <article-title>Is continuous visual monitoring necessary in visually guided locomotion?</article-title> <source>J. Exp. Psychol. Hum. Percept. Perform</source> <volume>9</volume>, <fpage>427</fpage>–<lpage>443</lpage> (<year>1983</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Rieser</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Ashmead</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Talor</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Youngquist</surname>, <given-names>G</given-names></string-name>. <article-title>Visual perception and the guidance of locomotion without vision to previously seen targets</article-title>. <source>Perception</source> <volume>19</volume>, <fpage>675</fpage>– <lpage>689</lpage> (<year>1990</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Da Silva</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Fujita</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Fukusima</surname>, <given-names>S. S</given-names></string-name>. <article-title>Visual space perception and visually directed action</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source> <volume>18</volume>, <fpage>906</fpage>–<lpage>921</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>DaSilva</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Philbeck</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Fukusima</surname>, <given-names>S. S</given-names></string-name>. <article-title>Visual perception of location and distance</article-title>. <source>Curr. Dir. Psychol. Sci</source> <volume>5</volume>, <fpage>72</fpage>–<lpage>77</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Meng</surname>, <given-names>J. C.</given-names></string-name> &amp; <string-name><surname>Sedgwick</surname>, <given-names>H. A</given-names></string-name>. <article-title>Distance perception mediated through nested contact relations among surfaces</article-title>. <source>Percept. Psychophys</source>. <volume>63</volume>, <fpage>1</fpage>–<lpage>15</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name> &amp; <string-name><surname>He</surname>, <given-names>Z. J</given-names></string-name>. <article-title>Perceiving distance accurately by a directional process of integrating ground information</article-title>. <source>Nature</source> <volume>428</volume>, <fpage>73</fpage>–<lpage>77</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>He</surname>, <given-names>Z. J.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ooi</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Yarbrough</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Wu</surname>, <given-names>J</given-names></string-name>. <article-title>Judging egocentric distance on the ground: Occlusion and surface integration</article-title>. <source>Perception</source>, <volume>33</volume>, <fpage>789</fpage>–<lpage>806</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Bian</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Braunstein</surname>, <given-names>M. L.</given-names></string-name> &amp; <string-name><surname>Andersen</surname>, <given-names>G. J</given-names></string-name>. <article-title>The ground dominance effect in the perception of 3-D layout</article-title>. <source>Percept. Psychophys</source> <volume>67</volume>, <fpage>802</fpage>–<lpage>815</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Philbeck</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Loomis</surname>, <given-names>J. M</given-names></string-name>. <article-title>Comparison of two indicators of perceived egocentric distance under full-cue and reduced-cue conditions</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>23</volume>, <fpage>72</fpage>–<lpage>85</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Gogel</surname>, <given-names>W. C.</given-names></string-name> &amp; <string-name><surname>Tietz</surname>, <given-names>J. D</given-names></string-name>. <article-title>A comparison of oculomotor and motion parallax cues of egocentric distance</article-title>. <source>Vision Res</source>. <volume>19</volume>, <fpage>1161</fpage>–<lpage>1170</lpage> (<year>1979</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Shi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>Z. J.</given-names></string-name> &amp; <string-name><surname>Ooi</surname>, <given-names>T. L</given-names></string-name>. <article-title>The visible ground surface as a reference frame for scaling binocular depth of a target in midair</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>. <volume>41</volume>, <fpage>111</fpage>–<lpage>126</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Mittelstaedt</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Mittelstaedt</surname> <given-names>H</given-names></string-name>. <article-title>Homing by path integration in a mammal</article-title>. <source>Naturwissenschaften</source>. <volume>67</volume>, <fpage>566</fpage>–<lpage>567</lpage> (<year>1980</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Wehner</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Srinivasan</surname>, <given-names>M. V</given-names></string-name>. <article-title>Searching behavior of desert ants, genusCataglyphis (Formicidae</article-title>, <source>Hymenoptera). J. Comp. Physiol</source>. <volume>142</volume>, <fpage>315</fpage>–<lpage>318</lpage> (<year>1981</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Rieser</surname>, <given-names>J. J</given-names></string-name>. <article-title>Access to knowledge of spatial structure at novel points of observation</article-title>. <source>J. Exp. Psychol. Learn. Mem. Cogn</source>. <volume>15</volume>, <fpage>1157</fpage>–<lpage>1165</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Etienne</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Maurer</surname> <given-names>R</given-names></string-name>, <string-name><surname>Séguinot</surname> <given-names>V</given-names></string-name>. <article-title>Path integration in mammals and its interaction with visual landmarks</article-title>. <source>J Exper Biol</source>. <volume>199</volume>, <fpage>201</fpage>–<lpage>209</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>McNaughton</surname>, <given-names>B. L.</given-names></string-name> <etal>et al.</etal> <article-title>Deciphering the hippocampal polyglot: the hippocampus as a path integration system</article-title>. <source>J. Exp. Biol</source>. <volume>199</volume>, <fpage>173</fpage>–<lpage>185</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="book"><string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Klatzky</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Golledge</surname>, <given-names>R. G.</given-names></string-name> &amp; <string-name><surname>Philbeck</surname>, <given-names>J. W.</given-names></string-name> <chapter-title>Human navigation by path integration</chapter-title>. In <source>Wayfinding: Cognitive mapping and other spatial processes</source> (eds <string-name><surname>Golledge</surname>, <given-names>R. G</given-names></string-name>) <fpage>125</fpage>–<lpage>151</lpage>, (<publisher-name>Baltimore Johns Hopkins</publisher-name>, <year>1999</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Mittelstaedt</surname>, <given-names>M. L.</given-names></string-name> &amp; <string-name><surname>Mittelstaedt</surname>, <given-names>H</given-names></string-name>. <article-title>Idiothetic navigation in humans: estimation of path length</article-title>. <source>Exp. Brain Res</source>. <volume>139</volume>, <fpage>318</fpage>–<lpage>332</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="book"><string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Klatzky</surname>, <given-names>R. L.</given-names></string-name> &amp; <string-name><surname>Giudice</surname>, <given-names>N. A.</given-names></string-name> <chapter-title>Representing 3D space in working memory: Spatial images from vision, hearing, touch, and language</chapter-title>., in <source>Multisensory imagery</source> (eds <string-name><surname>Lacey</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Lawson</surname>, <given-names>R.</given-names></string-name>) <fpage>131</fpage>–<lpage>155</lpage> (<publisher-name>New York Springer</publisher-name>, <year>2013</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Amorim</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Glasauer</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Corpinot</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Berthoz</surname>, <given-names>A</given-names></string-name>. <article-title>Updating an object’s orientation and location during nonvisual navigation: A comparison between two processing modes</article-title>. <source>Percept. Psychophys</source>. <volume>59</volume>, <fpage>404</fpage>–<lpage>418</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Cavanagh</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hunt</surname> <given-names>A. R</given-names></string-name>, <string-name><surname>Afraz</surname>. <given-names>A.</given-names></string-name> &amp; <string-name><surname>Rolfs</surname>, <given-names>M.</given-names></string-name> <article-title>Visual stability based on remapping of attention pointers</article-title>. <source>Trends Cogn Sci</source>.<volume>14</volume>, <fpage>147</fpage>–<lpage>153</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Israël</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Berthoz</surname>, <given-names>A</given-names></string-name>. <article-title>Contribution of the otoliths to the calculation of linear displacement</article-title>. <source>J. Neurophysiol</source>. <volume>62</volume>, <fpage>247</fpage>–<lpage>263</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Campos</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Mohler</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Bülthoff</surname>, <given-names>H. H</given-names></string-name>. <article-title>Measurement of instantaneous perceived self-motion using continuous pointing</article-title>. <source>Experimental Brain Research</source>, <volume>195</volume>, <fpage>429</fpage>–<lpage>444</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>Z. J.</given-names></string-name> &amp; <string-name><surname>Ooi</surname>, <given-names>T. L</given-names></string-name>. <article-title>The visual system’s intrinsic bias and knowledge of size mediate perceived size and location in the dark</article-title>. <source>J. Exp. Psychol. Learn. Mem. Cogn</source>. <volume>39</volume>, <fpage>1930</fpage>–<lpage>1942</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Jovalekic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hayman</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Becares</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Reid</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Thomas</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>J. J.</given-names></string-name>, <etal>et al.</etal> <article-title>Horizontal biases in rats’ use of three-dimensional space</article-title>. <source>Behavioural Brain Research</source>, <volume>222</volume>(<issue>2</issue>), <fpage>279</fpage>–<lpage>288</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Hayman</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Verriotis</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Jovalekic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fenton</surname>, <given-names>A. A.</given-names></string-name>, &amp; <string-name><surname>Jeffery</surname>, <given-names>K. J</given-names></string-name>. <article-title>Anisotropic encoding of three-dimensional space by place cells and grid cells</article-title>. <source>Nature Neuroscience</source>, <volume>14</volume>(<issue>9</issue>), <fpage>1182</fpage>–<lpage>1188</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Zwergal</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schoberl</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Xiong</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Pradhan</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Covic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Werner</surname>, <given-names>P.</given-names></string-name>, <etal>et al.</etal> <article-title>Anisotropy of human horizontal and vertical navigation in real space: Behavioral and PET correlates</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>(<issue>11</issue>), <fpage>4392</fpage>–<lpage>4404</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Ronacher</surname>, <given-names>B</given-names></string-name>. <article-title>Path integration in a three-dimensional world: the case of desert ants</article-title>. <source>J Comp Physiol A</source>, <volume>206</volume>, <fpage>379</fpage>–<lpage>387</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Tolman</surname>, <given-names>E. C</given-names></string-name>. <article-title>Cognitive maps in rats and men</article-title>. <source>Psychological Review</source>, <volume>55</volume>, <fpage>189</fpage>–<lpage>208</lpage> (<year>1948</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>O’Keefe</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dostrovsky</surname> <given-names>J</given-names></string-name>. <article-title>The Hippocampus as a spatial map. preliminary evidence from unit activity in the freely-moving rat</article-title>. <source>Brain Research</source> <volume>34</volume>, <fpage>171</fpage>–<lpage>175</lpage>(<year>1971</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>O’Keefe</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Nadel</surname>, <given-names>L.</given-names></string-name> <article-title>The Hippocampus as a Cognitive Map</article-title> <source>Clarendon Oxford</source> (<year>1978</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Moser</surname>, <given-names>E. I.</given-names></string-name>, <etal>et al.</etal> <article-title>Grid cells and cortical representation</article-title>. <source>Nature Rev. Neurosci</source>. <volume>15</volume>, <fpage>466</fpage>–<lpage>481</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Nau</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Julian</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>Doeller</surname>, <given-names>C. F</given-names></string-name>. <article-title>How the brain’s navigation system shapes our visual experience</article-title>. <source>Trends in cognitive sciences</source>, <volume>22</volume>(<issue>9</issue>), <fpage>810</fpage>–<lpage>825</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Byrne</surname> <given-names>P</given-names></string-name>, <string-name><surname>Becker</surname> <given-names>S</given-names></string-name>, <string-name><surname>Burgess</surname> <given-names>N</given-names></string-name>. <article-title>2007. Remembering the past and imagining the future: a neural model of spatial memory and imagery</article-title>. <source>Psychological Review</source>, <volume>114</volume>, <fpage>340</fpage>–<lpage>375</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Bottini</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Doeller</surname>, <given-names>C</given-names></string-name>. <article-title>Knowledge across reference frames: Cognitive maps and image spaces</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>24</volume>(<issue>8</issue>), <fpage>606</fpage>–<lpage>619</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Knierim</surname>, <given-names>J. J</given-names></string-name>. <article-title>Egocentric and allocentric representations of space in the rodent brain</article-title>. <source>Current opinion in neurobiology</source>, <volume>60</volume>, <fpage>12</fpage>–<lpage>20</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="book"><string-name><surname>Kant</surname>, <given-names>I.</given-names></string-name> <source>Critique of pure reason</source> (<edition>2nd</edition> ed.). <string-name><given-names>N. K.</given-names> <surname>Smith</surname></string-name>, Trans. (<publisher-loc>London</publisher-loc>, <publisher-name>MacMillan</publisher-name>, <year>1963</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="book"><string-name><surname>Von Uexküll</surname>, <given-names>J.</given-names></string-name> <source>A foray into the worlds of animals and humans: With a theory of meaning</source> (Vol. <volume>12</volume>) (<publisher-name>U of Minnesota Press</publisher-name>, <year>2013</year>).</mixed-citation></ref>
</ref-list>
<ack>
<title>Acknowledgements</title>
<p>We thank Dr. Jack Loomis for the helpful discussions and feedback on an earlier draft of the current paper.</p>
</ack>
<sec id="s5">
<title>Funding</title>
<p>The work was supported by a grant from the National Institutes of Health (EY033190) to Z.J.H. and T.L.O. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
</sec>
<sec id="s6">
<title>Author contributions</title>
<p>T.L.O. and Z.J.H. conceived of the project. L.Z, T.L.O. and Z.J.H. designed experiments and analyses. L.Z. and W.W. performed the experiments and analyzed the data. L.Z. contributed to the manuscript writing; T.L.O. and Z.J.H. wrote the manuscript.</p>
</sec>
<sec id="s7">
<title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<sec id="s8">
<title>Data and materials availability</title>
<p>All data are available in the main text or the supplementary materials. The data sets generated are available from the corresponding authors upon reasonable request.</p>
<p><bold>Supplementary information</bold> is available for this paper.</p>
</sec>
<sec id="s9">
<title>Supplementary Information</title>
<sec id="s9a">
<title>Statistical Analysis of Data</title>
<sec id="s9a1">
<title>Experiment 1 (<xref rid="fig3" ref-type="fig">figure 3c</xref>)</title>
<p><bold><italic>1. Baseline-stationary condition vs. walking condition with a 12 sec waiting period</italic></bold></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>baseline-stationary vs. walking</italic>): <italic>F(1,7)=439.133, p&lt;0.001</italic></p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=158.676, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(5, 35)=13.255, p&lt;0.001</italic></p></list-item>
</list>
<p><bold><italic>2. Baseline-stationary condition vs. walking condition with a 60 sec waiting period</italic></bold></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>baseline-stationary vs. walking)</italic>: <italic>F(1,7)=13.406, p&lt;0.01</italic></p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=133.358, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(5, 35)=1.555, p=0.198</italic></p></list-item>
</list>
</sec>
<sec id="s9a2">
<title><italic>Experiment 2 (</italic><xref rid="fig3" ref-type="fig">figure 3d</xref><italic>)</italic></title>
<p><bold><italic>Baseline-stationary condition vs. divided attention walking condition</italic></bold></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>baseline-stationary vs. divided attention walking</italic>): <italic>F(1,7)=8.529, p=0.022</italic></p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=63.780, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(5, 35)=1.072, p=0.392</italic></p></list-item>
</list>
</sec>
<sec id="s9a3">
<title><italic>Experiment 3 (</italic><xref rid="fig4" ref-type="fig">figure 4</xref><italic>)</italic></title>
<p><bold><italic>1. Vestibular-forward condition vs. walking condition (<xref rid="fig4" ref-type="fig">figure 4a</xref>)</italic></bold></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>vestibular-forward vs. walking)</italic>: <italic>F(1,7)=0.142, p=0.718</italic></p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=99.788, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(5, 35)=0.714, p=0.617</italic></p></list-item>
</list>
<p><bold><italic>2. Vestibular-forward condition vs. baseline-stationary condition (<xref rid="fig4" ref-type="fig">figure 4a</xref>)</italic></bold></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>baseline-stationary vs. vestibular-forward)</italic>: <italic>F(1,7)=598.551, p&lt;0.001</italic></p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=168.855, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(5, 35)=19.422, p&lt;0.001</italic></p></list-item>
</list>
<p><bold><italic>3. Baseline-stationary condition vs. walking condition (<xref rid="fig4" ref-type="fig">figure 4a</xref>)</italic></bold></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>baseline-stationary vs. walking</italic>): <italic>F(1,7)=299.648, p&lt;0.001</italic></p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=160.078, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(5, 35)=12.607, p&lt;0.001</italic></p></list-item>
</list>
<p><bold><italic>4. Vestibular-backward condition vs. baseline-stationary condition (<xref rid="fig4" ref-type="fig">figure 4b</xref>)</italic></bold></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 6 horizontal distances) to the walked horizontal distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>vestibular-backward vs. baseline-stationary)</italic>: <italic>F(1,7)=651.863, p&lt;0.001</italic></p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(5, 35)=51.558, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(5, 35)=10.848, p&lt;0.001</italic></p></list-item>
</list>
</sec>
<sec id="s9a4">
<title><italic>Experiment 4 (</italic><xref rid="fig4" ref-type="fig">figure 4e</xref><italic>)</italic></title>
<p><bold><italic>Vestibular-forward condition vs. baseline-stationary condition (verbal reports of eye-to-target distances)</italic></bold></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 horizontal distances) to the verbally reported distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>vestibular-forward vs. baseline-stationary)</italic>: <italic>F(1,7)=217.749, p&lt;0.001</italic></p></list-item>
<list-item><p>Main effect of distance: <italic>F(4, 28)=36.945, p&lt;0.001</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(4, 28)=8.973, p&lt;0.001</italic></p></list-item>
</list>
</sec>
<sec id="s9a5">
<title><italic>Experiment 5 (</italic><xref rid="fig5" ref-type="fig">figure 5c</xref><italic>)</italic></title>
<p><bold><italic>1. Baseline-stationary condition vs. stepladder condition</italic></bold></p>
<p><italic><underline>Horizontal distances:</underline></italic></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 horizontal distances) to the walked horizontal distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>baseline-stationary vs. stepladder</italic>): <italic>F(1,8)=153.541, p=0.000</italic></p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(4, 32)=35.954, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(2.58, 20.69)= 3.89, p=0.028</italic></p></list-item>
</list>
<p><italic><underline>Heights:</underline></italic></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 heights) to the judged height data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>baseline-stationary vs stepladder</italic>): <italic>F(1,8)=0.112, p=0.747</italic></p></list-item>
<list-item><p>Main effect of height: <italic>F(4, 32)=181.666, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(4, 32)= 0.288, p=0.883</italic></p></list-item>
</list>
<p><bold><italic>2. Baseline-stationary condition vs. horizontal-walking condition</italic></bold></p>
<p><italic><underline>Horizontal distances:</underline></italic></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 horizontal distances) to the walked horizontal distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>baseline-stationary vs. horizontal-walking</italic>): <italic>F(1,8)=104.198, p=0.000</italic></p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(4, 32)=45.768, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(1.892, 15.137)= 8.461, p=0.004</italic></p></list-item>
</list>
<p><italic><underline>Heights:</underline></italic></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 heights) to the judged height data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>baseline-stationary vs. horizontal-walking</italic>): <italic>F(1,8)=7.695, p=0.024</italic></p></list-item>
<list-item><p>Main effect of height distance: <italic>F(4, 32)=236.530, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(2.636, 21.088)= 2.472, p=0.096</italic></p></list-item>
</list>
<p><bold><italic>3. Stepladder condition vs. horizontal-walking condition</italic></bold></p>
<p><italic><underline>Horizontal distances:</underline></italic></p>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 horizontal distances) to the walked horizontal distance data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>stepladder vs. horizontal-walking</italic>): <italic>F(1,8)=0.532, p=0.487</italic></p></list-item>
<list-item><p>Main effect of horizontal distance: <italic>F(4, 32)=32.372, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(2.967, 23.737)=0.874, p=0.467 <underline>Heights</underline>:</italic></p></list-item>
</list>
<p>We applied 2-way ANOVA with repeated measures (2 test conditions x 5 heights) to the judged height data. The analysis reveals:</p>
<list list-type="bullet">
<list-item><p>Main effect of test condition (<italic>stepladder vs. horizontal-walking</italic>): <italic>F(1,8)=0.864, p=0.380</italic></p></list-item>
<list-item><p>Main effect of height: <italic>F(4, 32)=185.248, p=0.000</italic></p></list-item>
<list-item><p>Interaction effect: <italic>F(2.864, 22.908)=0.829, p=0.487</italic></p></list-item>
</list>
</sec>
</sec>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88095.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Meng</surname>
<given-names>Ming</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study reveals the use of an allocentric spatial reference frame in the updating perception of the location of a dimly lit target during locomotion. The evidence supporting this claim is <bold>compelling</bold>, based on a series of cleverly and carefully designed behavioral experiments. The results will be of interest not only to scientists who study perception, action and cognition but also to engineers who work on developing visually guided robots and self-driving vehicles.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88095.2.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study conducted a series of experiments to comprehensively support the allocentric rather than egocentric visual spatial reference updating for the path-integration mechanism in the control of target-oriented locomotion. Authors firstly manipulated the waiting time before walking to tease apart the influence from spatial working memory in guiding locomotion. They demonstrated that the intrinsic bias in perceiving distance remained constant during walking and that the establishment of a new spatial layout in the brain took a relatively longer time beyond the visual-spatial working memory. In the following experiments, the authors then uncovered that the strength of the intrinsic bias in distance perception along the horizontal direction is reduced when participants' attention is distracted, implying that world-centered path integration requires attentional effort. This study also revealed horizontal-vertical asymmetry in a spatial coding scheme that bears a resemblance to the locomotion control in other animal species such as desert ants.</p>
<p>The revised version of the study effectively situates the research within the broader context of terrestrial navigation, focusing on the movement of land-based creatures and offers a clearer explanation for the potential neurological basis of the human brain's allocentric odometer. Previous feedback has been thoroughly considered, and additional details have been incorporated into the presentation of the results.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88095.2.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study investigated what kind of reference (allocentric or egocentric) frame we used for perception in darkness. This question is essential and was not addressed much before. The authors compared the perception in the walking condition with that in the stationary condition, which successfully separated the contribution of self-movement to the spatial representation. In addition, the authors also carefully manipulated the contribution of the waiting period, attentional load, vestibular input, testing task, and walking direction (forward or backward) to examine the nature of the reference frame in darkness systematically.</p>
<p>I am a bit confused by Figure 2b. Allocentric coordinate refers to the representation of the distance and direction of an object relative to other objects but not relative to the observer. In Figure 2, however, the authors assumed that the perceived target was located on the interception between the intrinsic bias curve and the viewing line from the NEW eye position to the target. This suggests that the perceived object depends on the observer's new location, which seems odd with the allocentric coordinate hypothesis.</p>
<p>According to Fig 2b, the perceived size should be left-shifted and lifted up in the walking condition compared to that in the stationary condition. However, in Figure 3C and Fig 4, the perceived size was the same height as that in the baseline condition.</p>
<p>Is the left-shifted perceived distance possibly reflecting a kind of compensation mechanism? Participants could not see the target's location but knew they had moved forward. Therefore, their brain automatically compensates for this self-movement when judging the location of a target. This would perfectly predict the left-shifted but not upward-shifted data in Fig 3C. A similar compensation mechanism exists for size constancy in which we tend to compensate for distance in computing object size.</p>
<p>According to Fig 2a, the target, perceived target, and eye should be aligned in one straight line. This means that connecting the physical targets and the corresponding perceived target results in straight lines that converge at the eye position. This seems, however, unlikely in Figure 3c.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88095.2.sa3</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhou</surname>
<given-names>Liu</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wei</surname>
<given-names>Wei</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ooi</surname>
<given-names>Teng Leng</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>He</surname>
<given-names>Zijiang J.</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6313-9016</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer 1:</bold></p>
<p>(1) Authors need to acknowledge the physical effort in addition to visual information for the spatial coding and may consider the manipulation of physical efforts in the future to support the robustness of constant intrinsic bias in ground-based spatial coding during walking.</p>
</disp-quote>
<p>Whether one’s physical effort can affect spatial coding for visual perception is not a settled issue.  Several empirical studies have not been able to obtain evidence to support the claim.  For example, empirical studies by Hutchison &amp; Loomis (2009) and Durgin et al. (2009) did not find wearing a heavy backpack significantly influenced distance perception, in contrast to the findings by Proffitt et al (2003).  We respectfully request not to discuss this issue in our revision since it is not closely related to the focus of the current study.</p>
<disp-quote content-type="editor-comment">
<p>(2) Furthermore, it would be more comprehensive and fit into the Neuroscience Section if the authors can add in current understandings of the spatial reference frames in neuroscience in the introduction and discussion, and provide explanations on how the findings of this study supplement the physiological evidence that supports our spatial perception as well.  For instance, world-centered representations of the environment, or cognitive maps, are associated with hippocampal formation while self-centered spatial relationships, or image spaces, are associated with the parietal cortex (see Bottini, R., &amp; Doeller, C. F. (2020). Knowledge Across Reference Frames: Cognitive Maps and Image Spaces. Trends in Cognitive Sciences, 24(8),606-619. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2020.05.008">https://doi.org/10.1016/j.tics.2020.05.008</ext-link> for details)</p>
</disp-quote>
<p>We have now added this important discussion in the revision on pages 12-13.</p>
<p>We thank the reviewer for the helpful comments.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2:</bold></p>
<p>(1) ….As a result, it is unclear to what extent this &quot;allocentric&quot; intrinsic bias is involved in our everyday spatial perception. To provide more context for the general audience, it would be beneficial for the authors to address this issue in their discussion.</p>
</disp-quote>
<p>We have clarified this on pages 3-4.  In brief, our hypothesis is that during self-motion, the visual system constructs an allocentric ground surface representation (reference frame) by integrating the allocentric intrinsic bias with the external depth cues on the natural ground surface.  Supporting this hypothesis, we recently found that when there is texture cue on the ground, the representation of the ground surface is influenced by the allocentric intrinsic bias (Zhou et al, unpublished results).</p>
<disp-quote content-type="editor-comment">
<p>(2) The current findings on the &quot;allocentric&quot; coding scheme raise some intriguing questions as to why such a mechanism would be developed and how it could be beneficial. The finding that the &quot;allocentric&quot; coding scheme results in less accurate object localization and requires attentional resources seems counterintuitive and raises questions about its usefulness. However, this observation presents an opportunity for the manuscript to discuss the potential evolutionary advantages or trade-offs associated with this coding mechanism.</p>
</disp-quote>
<p>The revision has discussed these important issues on page 12.</p>
<disp-quote content-type="editor-comment">
<p>(3) The manuscript lacks a thorough description of the data analysis process, particularly regarding the fitting of the intrinsic bias curve (e.g., the blue and gray dashed curve in Figure 3c) and the calculation of the horizontal separation between the curves. It would be beneficial for the authors to provide more detailed information on the specific function and parameters used in the fitting process and the formula used for the separation calculation to ensure the transparency and reproducibility of the study's results.</p>
</disp-quote>
<p>The results of the statistical analysis were presented in the supplementary materials.  We had stated in the original manuscript that we fitted the intrinsic bias curve by eye (obtained by drawing the curve to transcribe the data points as closely as possible) (page 26).  This is because we do not yet have a formula for the intrinsic bias. A challenge is the measured intrinsic bias in the dark can be affected by multiple factors.  One factor is related to individual differences as the intrinsic bias is shaped by the observer’s past experiences and their eye height relative to the ground surface.  However, it is certainly our goal to develop a quantitative model of the intrinsic bias in the future.</p>
<p>We thank the reviewer for the helpful comments.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 3:</bold></p>
<p>(1) I am a bit confused by Figure 2b. Allocentric coordinate refers to the representation of the distance and direction of an object relative to other objects but not relative to the observer. In Figure 2, however, the authors assumed that the perceived target was located on the interception between the intrinsic bias curve and the viewing line from the NEW eye position to the target. This suggests that the perceived object depends on the observer's new location, which seems odd with the allocentric coordinate hypothesis.</p>
</disp-quote>
<p>We respectively disagree with the Reviewer’s statement that “Allocentric coordinate refers to the representation of the distance and direction of an object relative to other objects but not relative to the observer.”  The statement conflates the definitions of allocentric representation with exocentric representation.  We respectfully maintain that the observer’s body location, as well as observer-object distance, can be represented with the allocentric coordinate system.</p>
<disp-quote content-type="editor-comment">
<p>(2) According to Fig 2b, the perceived size should be left-shifted and lifted up in the walking condition compared to that in the stationary condition. However, in Figure 3C and Fig 4, the perceived size was the same height as that in the baseline condition.</p>
</disp-quote>
<p>We assume by “target size”, the Reviewer actually meant, “target location”.  It is correct that figure 3c and figure 4 showed judged distance changed as predicted, while the change in judged height was not significant.  One explanation for this is that the magnitude of the height change was much smaller than the distance change and could not be revealed by our blind walking-gesturing method.  Please also note our figures used difference scales for the vertical height and horizontal distance.</p>
<disp-quote content-type="editor-comment">
<p>(3) Is the left-shifted perceived distance possibly reflecting a kind of compensation mechanism?  Participants could not see the target's location but knew they had moved forward.  Therefore, their brain automatically compensates for this self-movement when judging the location of a target.  This would perfectly predict the left-shifted but not upward-shifted data in Fig 3C.  A similar compensation mechanism exists for size constancy in which we tend to compensate for distance in computing object size.</p>
</disp-quote>
<p>We assume the Reviewer suggested that the path-integration mechanism first estimates the traveled distance in the dark, and then the brain subtracts the estimated distance from the perceived target distance.  We respectfully maintain that this explanation is unlikely because it does not account for our empirical findings.  We found that walking in the dark did not uniformly affect perceived target distance, as the Reviewer’s explanation would predict.  As shown in figures 3 and 4, walking affected the near targets less than the far targets (i.e., the horizontal distance difference between walking and baseline-stationary conditions was smaller for the near target than far target).</p>
<disp-quote content-type="editor-comment">
<p>(4) According to Fig 2a, the target, perceived target, and eye should be aligned in one straight line. This means that connecting the physical targets and the corresponding perceived target results in straight lines that converge at the eye position. This seems, however, unlikely in Figure 3c.</p>
</disp-quote>
<p>We have added in the revision, the averaged eye positions on the y-axes of figures 3 and 4.  To reveal the impact of the judged angular declination, we also added graphs that plotted the estimated angular declination as a function of the physical declination of the target.  In general, the slopes are close to unity.</p>
<p>We thank the reviewer for the helpful comments.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer 1 (Recommendations For The Authors):</bold></p>
<p>(1) This study is very well-designed and written. One minor comment is that anisotropy usually refers to the perceptual differences along cardinal (horizontal + vertical) and oblique directions. It might be clearer if the authors changed the &quot;horizontal-vertical anisotropy&quot; to &quot;horizontal/vertical asymmetry”.</p>
</disp-quote>
<p>The Reviewer is correct, and we have changed it to horizontal/vertical asymmetry (pages 8 and 11).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2 (Recommendations For The Authors):</bold></p>
<p>(1) Providing more details about the &quot;path integration mechanism&quot; when it is first introduced in line 44 would be helpful for readers to better understand the concept.</p>
</disp-quote>
<p>The revision has expanded on the path integration mechanism (page 4).</p>
<disp-quote content-type="editor-comment">
<p>Adding references for the statement starting with &quot;In fact, previous findings&quot; in lines 218 and would be helpful to provide readers with a basis for comparison between the current study and previous studies that reported an egocentric coding system.</p>
</disp-quote>
<p>We have added the references and elaborated on this important issue (pages 10-11).</p>
<disp-quote content-type="editor-comment">
<p>(2) There appears to be a discrepancy between the Materials and Methods section, which states that 14 observers participated in Experiments 1-4, and the legends of Figures 3 and 4, which indicates a sample size of &quot;n=8.&quot; It would be helpful if the authors could clarify this discrepancy and provide an explanation for the difference in the sample size reported.</p>
</disp-quote>
<p>We have clarified the number of observers on page 14.</p>
<disp-quote content-type="editor-comment">
<p>(3) While reporting statistical significance is essential in the Results section, there are several instances where the manuscript only mentions a &quot;statistically significant separation&quot; with it p-value without providing the mean and standard deviation of the separation values (e.g., line 100 and 120). This can make it difficult for readers to fully grasp the quantitative nature of the results.</p>
</disp-quote>
<p>The statistical analysis and outcomes were presented in the supplementary information document in our original submission.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 3 (Recommendations For The Authors):</bold></p>
<p>(1) Figure 1 is not significantly related to the current manuscript.</p>
</disp-quote>
<p>We feel that retaining figure 1 in the manuscript would help readers to quickly grasp the background literature without having to refer extensively to our previous publications.</p>
<disp-quote content-type="editor-comment">
<p>(2) Add eye position to the results figures.</p>
</disp-quote>
<p>We have added eye positions in the figures.</p>
<disp-quote content-type="editor-comment">
<p>(3) Fig 4c requires a more detailed explanation. The authors stated that Figures 4a and 4c showed consistent results.  However, because 4a and 4c used different horizontal axis, it is different to compare them directly.</p>
</disp-quote>
<p>We have modified the sentence in the revision (page 8).</p>
</body>
</sub-article>
</article>