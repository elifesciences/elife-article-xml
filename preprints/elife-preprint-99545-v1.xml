<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99545</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99545</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99545.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Structure, dynamics, coding and optimal biophysical parameters of efficient excitatory-inhibitory spiking networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2920-2717</contrib-id>
<name>
<surname>Koren</surname>
<given-names>Veronika</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4467-5988</contrib-id>
<name>
<surname>Malerba</surname>
<given-names>Simone Blanco</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5422-3723</contrib-id>
<name>
<surname>Schwalger</surname>
<given-names>Tilo</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1700-8909</contrib-id>
<name>
<surname>Panzeri</surname>
<given-names>Stefano</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Institute of Neural Information Processing, Center for Molecular Neurobiology (ZMNH), University Medical Center Hamburg-Eppendorf (UKE)</institution>, 20251 Hamburg, <country>Germany</country></aff>
<aff id="a2"><label>2</label><institution>Institute of Mathematics, Technische Universität Berlin</institution>, 10623 Berlin, <country>Germany</country></aff>
<aff id="a3"><label>3</label><institution>Bernstein Center for Computational Neuroscience Berlin</institution>, 10115 Berlin, <country>Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Naud</surname>
<given-names>Richard</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Ottawa</institution>
</institution-wrap>
<city>Ottawa</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label> Corresponding author; email: <email>koren.veronika@gmail.com</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-07-26">
<day>26</day>
<month>07</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99545</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-28">
<day>28</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-04-27">
<day>27</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.24.590955"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Koren et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Koren et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99545-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>The principle of efficient coding posits that sensory cortical networks are designed to encode maximal sensory information with minimal metabolic cost. Despite the major influence of efficient coding in neuro-science, it has remained unclear whether fundamental empirical properties of neural network activity can be explained solely based on this normative principle. Here, we rigorously derive the structural, coding, biophysical and dynamical properties of excitatory-inhibitory recurrent networks of spiking neurons that emerge directly from imposing that the network minimizes an instantaneous loss function and a time-averaged performance measure enacting efficient coding. The optimal network has biologically-plausible biophysical features, including realistic integrate-and-fire spiking dynamics, spike-triggered adaptation, and a non-stimulus-specific excitatory external input regulating metabolic cost. The efficient network has excitatory-inhibitory recurrent connectivity between neurons with similar stimulus tuning implementing feature-specific competition, similar to that recently found in visual cortex. Networks with unstructured connectivity cannot reach comparable levels of coding efficiency. The optimal biophysical parameters include 4 to 1 ratio of excitatory vs inhibitory neurons and 3 to 1 ratio of mean inhibitory-to-inhibitory vs. excitatory-to-inhibitory connectivity that closely match those of cortical sensory networks. The efficient network has biologically-plausible spiking dynamics, with a tight instantaneous E-I balance that makes them capable to achieve efficient coding of external stimuli varying over multiple time scales. Together, these results explain how efficient coding may be implemented in cortical networks and suggests that key properties of biological neural networks may be accounted for by efficient coding.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Neural coding</kwd>
<kwd>Efficient coding</kwd>
<kwd>Spiking neural networks</kwd>
<kwd>Recurrent Neural Networks</kwd>
<kwd>Population coding</kwd>
<kwd>Spike-triggered adaptation</kwd>
<kwd>Integrate-and-fire neuron</kwd>
<kwd>Excitatory-Inhibitory balance</kwd>
<kwd>Connectivity</kwd>
<kwd>Optimality</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Information about the sensory world is represented in the brain through the dynamics of neural population activity<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup>. One prominent theory about the principles that may guide the design of neural computations for sensory function is efficient coding<sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c5">5</xref></sup>. This theory posits that neural computations are optimized to maximize the information that neural systems encode about features of sensory stimuli while at the same time limiting the metabolic cost. Efficient coding has been highly influential, especially in visual neuroscience and computational vision<sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c9">9</xref></sup>, and has been developed to become a normative theory of how networks are organized and designed to optimally process natural sensory stimuli in visual <sup><xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c11">11</xref></sup>, auditory<sup><xref ref-type="bibr" rid="c12">12</xref></sup> and olfactory sensory pathways<sup><xref ref-type="bibr" rid="c13">13</xref></sup>.</p>
<p>The first normative neural network models<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c11">11</xref></sup> designed with efficient coding principles had at least two major levels of abstractions. First, information was assumed to be processed in a purely feedforward manner, whereas information processing in real neural circuits often involves recurrent or feedback computations. Second, neural dynamics was greatly simplified, ignoring the spiking nature of neural activity. Instead, in biological networks considerable amount of information are encoded or transmitted only through the millisecond-precise timing of spikes<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>. Also, these earlier works mostly considered encoding of static sensory stimuli, whereas the sensory environment changes continuously at multiple timescales and the dynamics of neural networks encodes these temporal variations of the environment <sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c24">24</xref></sup>.</p>
<p>Recent years have witnessed a considerable effort and success in laying down the mathematical tools and methodology to understand how to formulate efficient coding theories of neural networks with much more biological realism<sup><xref ref-type="bibr" rid="c25">25</xref></sup>. This work has established the incorporation of recurrent connectivity<sup><xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>, of spiking neurons, and of time-varying stimulus inputs<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>. In these models, the efficient coding principle has been implemented by designing networks whose activity minimizes the encoding accuracy (the error between a desired representation and a linear readout of network’s activity) subject to a constraint on the metabolic cost of processing (proportional to the total number of spikes fired by a population of neurons). This double objective is captured by a loss function that trades-off encoding accuracy and metabolic cost. The minimization of the loss function is performed through a greedy approach, by assuming that a neuron will emit a spike only if this will decrease the loss. This, in turn, yields a set of leaky integrate-and-fire (LIF) neural equations which govern the network dynamics<sup><xref ref-type="bibr" rid="c28">28</xref></sup>, which can also include biologically plausible non-instantaneous synaptic delays<sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>. These previous implementations, however, had neurons that did not respect Dale’s law. In recent work<sup><xref ref-type="bibr" rid="c37">37</xref></sup>, we further extended the biological plausibility of these models by analytically deriving how to implement efficient coding in networks of spiking neurons that respect Dale’s law. These networks take the form of generalized leaky integrate-and-fire (gLIF) models of excitatory (E) and inhibitory (I) neurons endowed with spike-triggered adaptation<sup><xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c40">40</xref></sup>, which can provide highly accurate predictions of spike times in biological networks<sup><xref ref-type="bibr" rid="c41">41</xref></sup>. Efficient spiking models have thus the potential to provide a unifying theory of neural coding through spiking dynamics of E-I circuits <sup><xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup> with elements that are fully biologically plausible and potentially interpretable as biophysical variables.</p>
<p>However, despite the major progress described above, as well the progress provided by other studies of efficient coding with spikes <sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>, we still lack a thorough characterization of which structural, coding, biophysical and dynamical properties of excitatory-inhibitory recurrent networks of spiking neurons are directly related to efficient coding principles. Previous studies only rarely made predictions that could be quantitatively compared against experimentally measurable properties of biological neural networks. As a consequence, we still do not know which, if any, fundamental properties of cortical networks emerge directly from imposing efficient coding.</p>
<p>To address the above questions, we analyze systematically our biologically plausible efficient coding model of E and I neurons that respect Dale’s law<sup><xref ref-type="bibr" rid="c37">37</xref></sup> to make concrete predictions about experimentally measurable structural, coding and dynamical features of neural activity that arise from efficient coding. We systematically investigated how experimentally measurable emergent dynamical properties, such as firing rates, trial-to-trial spiking variability of single neurons, E-I balance<sup><xref ref-type="bibr" rid="c45">45</xref></sup> and noise correlations, relate to optimally-efficient coding. We further analyze how the organization of the connectivity arising by imposing efficient coding relates to the anatomical and effective connectivity recently reported in visual cortex, which suggests competition between excitatory neurons with similar stimulus tuning. We found that several key and robustly found empirical properties of cortical circuits match the predictions of our efficient coding network, lending support to the notion that efficient coding may be a design principle that has shaped the evolution of cortical circuits and that may be used to conceptually understand and interpret them.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Assumptions and emergent properties of the efficient E-I network derived from first principles</title>
<p>We study the properties of a spiking neural network in which the dynamics and structure of the network are analytically derived starting from first principles of efficient coding of sensory stimuli. The model relies on a number of assumptions, described next.</p>
<p>The network responds to <italic>M</italic> time-varying features of a sensory stimulus, <italic>s</italic><sub><italic>k</italic></sub>(<italic>t</italic>) (e.g., for a visual stimulus, contrast, orientation, etc) received as inputs from an earlier sensory area (e.g., retina). We model features as independent Ornstein–Uhlenbeck (OU) processes (see Methods). The network’s objective is to compute a leaky integration of sensory features, a relevant computation of cortical sensory areas<sup><xref ref-type="bibr" rid="c46">46</xref></sup>. The target representations of the network, <italic>x</italic><sub><italic>k</italic></sub>(<italic>t</italic>), are defined as
<disp-formula id="eqn1">
<graphic xlink:href="590955v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>τ</italic> a characteristic integration time-scale (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Structural and dynamical properties of the efficient E-I spiking network.</title>
<p><bold>(A)</bold> Encoding of a target signal representing the evolution of a stimulus feature (top) with one E (middle) and one I spiking neuron (bottom). The target signal <italic>x</italic>(<italic>t</italic>) integrates the input signal <italic>s</italic>(<italic>t</italic>). The readout of the E neuron tracks the target signal and the readout of the I neuron tracks the readout of the E neuron. Neurons spike to bring the readout of their activity closer to their respective target. Each spike causes a jump of the readout, with the sign and the amplitude of the jump being determined by neuron’s tuning parameters.</p>
<p><bold>(B)</bold> Schematic of the matrix of tuning parameters. Every neuron is selective to all stimulus features (columns of the matrix), and all neurons participate in encoding of every feature (rows).</p>
<p><bold>(C)</bold> Schematic of the network with E (red) and I (blue) cell type. E neurons are driven by the stimulus features while I neurons are driven by the activity of E neurons. E and I neurons are connected through recurrent connectivity matrices.</p>
<p><bold>(D)</bold> Schematic of E (red) and I (blue) synaptic interactions. Arrows represent the direction of the tuning vector of each neuron. Only neurons with similar tuning are connected.</p>
<p><bold>(E)</bold> Schematic of similarity of tuning vectors (tuning similarity) in a 2-dimensional space of stimulus features.</p>
<p><bold>(F)</bold> Synaptic strength as a function of tuning similarity.</p>
<p><bold>(G)</bold> Coding and dynamics in a simulation trial. Top three rows show the signal (black), the E estimate (red) and the I estimate (blue) in each of the three stimulus dimensions. Below are the spike trains. In the bottom row, we show the average instantaneous firing rate (in Hz).</p>
<p><bold>(H)</bold> Top: Example of the target signal (black) and the E estimate in 3 simulation trials (colors) in one signal dimension. Bottom: Distribution (across time) of the time-dependent bias of estimates in E and I cell type.</p>
<p><bold>(I)</bold> Left: Distribution of time-averaged firing rates in E (top) and I neurons (bottom). Black traces are fits with log-normal distribution. Right: Distribution of coefficients of variation of interspike intervals for E and I neurons.</p>
<p><bold>(J)</bold> Distribution (across neurons) of time-averaged synaptic inputs to E (left) and I neurons (right). In E neurons, the distribution of inhibitory and of net synaptic inputs overlap.</p>
<p><bold>(K)</bold> Sum of synaptic inputs over time in a single E (top) and I neuron (bottom) in a simulation trial.</p>
<p><bold>(L)</bold> Distribution (across neurons) of Pearson’s correlation coefficients measuring the correlation of synaptic inputs in single E (red) and I (blue) neurons. For model parameters, see <xref rid="tbl1" ref-type="table">Table 1</xref>.</p></caption>
<graphic xlink:href="590955v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The network is composed of two neural populations of excitatory (E) and inhibitory (I) neurons, defined by their postsynaptic action which respects Dale’s law. For each population, <italic>y</italic> ∈ {<italic>E, I</italic>}, we define a population readout of each feature, <inline-formula><inline-graphic xlink:href="590955v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, as a filtered weighted sum of spiking activity of neurons in the population,</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Table of default model parameters for the efficient E-I network</title>
<p>Parameters above the double horizontal line are the minimal set of parameters needed to simulate model equations (<xref ref-type="disp-formula" rid="eqn30a">Eqs. 30a</xref>-<xref ref-type="disp-formula" rid="eqn30h">30h</xref> in Methods). Parameters below the double horizontal line are biophysical parameters, derived from the same model equations and from model parameters listed above the horizontal line. Parameters <italic>N</italic><sup><italic>E</italic></sup>, <italic>M, τ</italic> and <inline-formula><inline-graphic xlink:href="590955v1_inline151.gif" mimetype="image" mime-subtype="gif"/></inline-formula> were chosen for their biological plausibility and computational simplicity. Parameters <inline-formula><inline-graphic xlink:href="590955v1_inline152.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, ratio of mean E-I to I-I synaptic connectivity and <italic>β</italic> are parameters that maximize network efficiency (see the section “Criterion for determining model parameters” in Methods). The metabolic constant <italic>β</italic> and the noise intensity <italic>σ</italic> are interpreted as global network parameters and are for this reason assumed to be the same across the E and I population, e.g., <italic>β</italic><sup><italic>E</italic></sup> = <italic>β</italic><sup><italic>I</italic></sup> = <italic>β</italic> and <italic>σ</italic><sup><italic>E</italic></sup> = <italic>σ</italic><sup><italic>I</italic></sup> = <italic>σ</italic> (see <xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>). The connection probability of <italic>p</italic><sup><italic>xy</italic></sup> = 0.5 is the consequence of rectification of the connectivity (see <xref ref-type="disp-formula" rid="eqn25">Eq. 25</xref> in Methods).</p></caption>
<graphic xlink:href="590955v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>
<disp-formula id="eqn2">
<graphic xlink:href="590955v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v1_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the spike train of neuron <italic>i</italic> of type <italic>y</italic> and <inline-formula><inline-graphic xlink:href="590955v1_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are its decoding weights for features <italic>k</italic> = 1, …, <italic>M</italic>. As a result of the optimization, the decoding weights of the neurons are equivalent to neuron’s tuning parameters to the stimulus features (see Methods;<sup><xref ref-type="bibr" rid="c42">42</xref></sup>). We draw tuning parameters from a normal distribution with zero mean and SD <inline-formula><inline-graphic xlink:href="590955v1_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for E neurons and <inline-formula><inline-graphic xlink:href="590955v1_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for I neurons. We assume that every neuron encodes multiple (<italic>M</italic> &gt; 1) stimulus features<sup><xref ref-type="bibr" rid="c47">47</xref></sup> and define the vector <inline-formula><inline-graphic xlink:href="590955v1_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> as the tuning vector of neuron <italic>i</italic> (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>).</p>
<p>Unlike previous approaches<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup>, we hypothesize that E and I neurons have distinct normative objectives and define cell-type specific loss functions relative to the activity of the E and I neuron types. To implement at the same time, as requested by efficient coding, the constraints of faithful stimulus representation with limited computational resources<sup><xref ref-type="bibr" rid="c49">49</xref></sup>, we define the loss functions of E and I population as a weighted sum of a time-dependent encoding error and time-dependent metabolic cost:
<disp-formula id="eqn3">
<graphic xlink:href="590955v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We refer to <italic>β</italic>, the parameter controlling the relative importance of the metabolic cost over the encoding error, as the metabolic constant of the network. We hypothesize that population readouts of E neurons, <inline-formula><inline-graphic xlink:href="590955v1_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, track the target representations, <italic>x</italic><sub><italic>k</italic></sub>(<italic>t</italic>), and the population readouts of I neurons, <inline-formula><inline-graphic xlink:href="590955v1_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, track the population readouts of E neurons <inline-formula><inline-graphic xlink:href="590955v1_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, by minimizing the squared error between these quantities<sup><xref ref-type="bibr" rid="c37">37</xref></sup> (see also<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup> for related approaches). Furthermore, we hypothesize the time-resolved metabolic cost to be proportional to the estimate of a momentary firing rate of the neural population. We thus define the variables of loss functions in <xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref> as
<disp-formula id="eqn4">
<graphic xlink:href="590955v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v1_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the low-pass filtered spike train of neuron <italic>i</italic> (single neuron readout) with time constant <inline-formula><inline-graphic xlink:href="590955v1_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We then impose the following condition for spiking: a neuron shall emit a spike only if this decreases the loss function of its population in the immediate future. The condition for spiking also includes a noise term accounting for sources of stochasticity in spike generation<sup><xref ref-type="bibr" rid="c50">50</xref></sup>, including the effect of unspecific inputs from the rest of the brain.</p>
<p>By assuming that each neuron emits a spike at time <italic>t</italic> only if this decreases the loss function of its population (<xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>), we derived the dynamics and network structure of a spiking network that instantiates efficient coding (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>, see Methods). The derived dynamics of the subthreshold membrane potential <inline-formula><inline-graphic xlink:href="590955v1_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> obey the equations of the generalized leaky integrate and fire (gLIF) neuron
<disp-formula id="eqn5">
<graphic xlink:href="590955v1_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v1_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="590955v1_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are synaptic current, spike-triggered adaptation current and non-specific external current, respectively, <italic>R</italic><sub><italic>m</italic></sub> is the membrane resistance and <inline-formula><inline-graphic xlink:href="590955v1_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the resting potential. This dynamics is complemented with a fire-and-reset rule: when the membrane potential reaches the firing threshold <italic>ϑ</italic><sup><italic>y</italic></sup>, a spike is fired and <inline-formula><inline-graphic xlink:href="590955v1_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is set to the reset potential <italic>V</italic><sup>reset,<italic>y</italic></sup>. The analytical solution in <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> holds for any number of neurons (with at least 1 neuron in each population) and predicts an optimal spike pattern to encode the presented external stimulus.</p>
<p>The synaptic currents in E neurons, <inline-formula><inline-graphic xlink:href="590955v1_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, consist of feedforward currents, obtained as stimulus features <italic>s</italic><sub><italic>k</italic></sub>(<italic>t</italic>) weighted by the tuning weights of the neuron, and of recurrent inhibitory currents. Synaptic currents in I neurons, <inline-formula><inline-graphic xlink:href="590955v1_inline19.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, consist of recurrent excitatory and inhibitory currents (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>).</p>
<p>The optimization of the loss function also yields structured recurrent connectivity (<xref rid="fig1" ref-type="fig">Fig. 1D</xref>). The synaptic strength between two neurons is proportional to their tuning similarity if the tuning similarity is positive; otherwise the synaptic weight is set to zero (<xref rid="fig1" ref-type="fig">Fig. 1E,F</xref>) to ensure that Dale’s law is respected. This also sets the overall connection probability to 0.5. (For a study of how efficient coding would be implemented if the above Dale’s law constraint was removed and each neuron is free to have either an inhibitory or excitatory effect depending on the postsynaptic target, see Supplementary Text 1). Neurons with opposite tuning have low connection probability, consistent with experimental results<sup><xref ref-type="bibr" rid="c51">51</xref>,<xref ref-type="bibr" rid="c52">52</xref>,<xref ref-type="bibr" rid="c53">53</xref></sup> (<xref rid="fig1" ref-type="fig">Fig. 1D</xref>). Note that the structured recurrent connectivity leads to both E and I cells being stimulus-tuned, even though I cells do not receive feedforward inputs (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>). The spike-triggered adaptation current of neuron <italic>i</italic> in population <italic>y</italic>, <inline-formula><inline-graphic xlink:href="590955v1_inline20.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, has the dynamics of its low-pass filtered spike train <inline-formula><inline-graphic xlink:href="590955v1_inline21.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This current realizes spike-frequency adaptation or facilitation depending on the difference between the time constants of population and single neuron readout (see Results section “Weak spike-triggered adaptation optimizes network efficiency”). Finally, external currents have a constant mean, that depends on the parameter <italic>β</italic>, plus fluctuations that depend on the noise in the condition for spiking with intensity <italic>σ</italic>. Importantly, the relative weight of the metabolic cost over the encoding error controls the operating regime of the network biophysically plausibly, by modulating the mean of the external current. (Previous studies interpreted changes of the metabolic constant <italic>β</italic> as changes to the firing thresholds, which has less biophysical plausibility<sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup>) (see section “State-dependent coding and dynamics are controlled by the metabolic cost on spiking”).</p>
<p>To summarize, the analytical derivation of an optimally efficient network includes gLIF neurons<sup><xref ref-type="bibr" rid="c54">54</xref>,<xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c40">40</xref>,<xref ref-type="bibr" rid="c55">55</xref>,<xref ref-type="bibr" rid="c56">56</xref></sup>, a distributed code with mixed selectivity to the input stimuli, spike-triggered adaptation current, structured synaptic connectivity, and an operating regime controlled by the metabolic constant <italic>β</italic>.</p>
<p>The equations for the E-I network of gLIF neurons in <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> optimize the loss functions at any given time and for any set of parameters. In particular, the network equations have the same analytical form for any positive value of the metabolic constant <italic>β</italic>. To find a set of parameters that optimizes the overall performance, we defined a performance measure as the average over time and trials of the loss function. We then optimized the parameters by setting the metabolic constant <italic>β</italic> such that the encoding error weights 70 % and the metabolic error weights 30 % of the total performance, and by choosing all other parameters such as to minimize numerically our network performance measure (see Methods). The numerical optimization was performed by simulating a model of 400 E and 100 I units, a network size relevant for computations within one layer of a cortical microcolumn<sup><xref ref-type="bibr" rid="c57">57</xref></sup>. The set of model parameters that optimized network efficiency is detailed in <xref rid="tbl1" ref-type="table">Table 1</xref>. Unless otherwise stated, in all simulations we will use the optimal parameters of <xref rid="tbl1" ref-type="table">Table 1</xref> and only vary those parameters detailed in the figure axes.</p>
<p>With optimally efficient parameters, population readouts closely tracked the target signals (<xref rid="fig1" ref-type="fig">Fig. 1G</xref>, M=3, <italic>R</italic><sup><xref ref-type="bibr" rid="c2">2</xref></sup> = [0.95, 0.97] for E and I neurons, respectively). When stimulated by our 3-dimensional time-varying feedforward input, the optimal E-I network provided a precise and unbiased estimator of the multi-dimensional and time-dependent target signal (<xref rid="fig1" ref-type="fig">Fig. 1H</xref>).</p>
<p>Next, we examined the emergent dynamical properties of an optimally efficient E-I network. The distribution of firing rates was well described by a log-normal distribution (<xref rid="fig1" ref-type="fig">Fig. 1I</xref>, left). Neurons fired irregularly, with mean coefficient of variation (CV) slightly smaller than 1 (<xref rid="fig1" ref-type="fig">Fig. 1I</xref>, right; CV= [0.97, 0.95] for E and I neurons, respectively). We assessed E-I balance in single neurons through two complementary measures. First, we calculated the <italic>average</italic> (global) balance of E-I currents by taking the time-average of the net sum of currents<sup><xref ref-type="bibr" rid="c58">58</xref></sup>. Second, we evaluated the <italic>instantaneous</italic><sup><xref ref-type="bibr" rid="c59">59</xref></sup> (also termed detailed<sup><xref ref-type="bibr" rid="c45">45</xref></sup>) E-I balance using the Pearson correlation (<italic>ρ</italic>) of E and I currents received by a single neuron over time (see Methods).</p>
<p>We observed a strong average E-I balance (indicated by a net sum of synaptic inputs close to zero, with only a weak residual of inhibition in both E and I cells (<xref rid="fig1" ref-type="fig">Fig. 1J</xref>). Furthermore, we found a moderate instantaneous balance, stronger in I compared to E cell type (<xref rid="fig1" ref-type="fig">Fig. 1K-L</xref>, <italic>ρ</italic> = [0.44, 0.25], for I and E neurons, respectively). The presence of instantaneous balance between E and I synaptic currents within single neurons has been reported in cortical data<sup><xref ref-type="bibr" rid="c59">59</xref>,<xref ref-type="bibr" rid="c60">60</xref></sup>.</p>
</sec>
<sec id="s2b">
<title>Competition across neurons with similar stimulus tuning emerging in efficient spiking networks</title>
<p>We next explored coding properties emerging from recurrent synaptic interactions between E and I populations in the optimally efficient networks.</p>
<p>An approach that has recently provided empirical insight into local recurrent interactions between neurons is measuring the effective connectivity with cellular resolution, by photostimulating individual neurons and measuring the effect of such perturbation on other neurons in the network. Recent effective connectivity experiments photostimulated single E neurons in primary visual cortex and measured its effect on neighbouring neurons, finding that the photostimulation of an E neuron led to a decrease in firing rate of similarly tuned close-by neurons<sup><xref ref-type="bibr" rid="c61">61</xref></sup>. This effective lateral inhibition<sup><xref ref-type="bibr" rid="c26">26</xref></sup> between E neurons with similar tuning to the stimulus implements competition between neurons for the representation of stimulus features (termed feature-specific competition<sup><xref ref-type="bibr" rid="c61">61</xref></sup>).</p>
<p>To assess how E-I interactions shape coding in efficient networks, we simulated photostimulation experiments in these networks. We performed such experiments in the absence of the feedforward input to insure all effects are only due to the recurrent processing and not to feedforward processing. We stimulated a randomly selected single “target” E neuron and measured the change in the instantaneous firing rate from the baseline firing rate, Δ<italic>z</italic><sub><italic>i</italic></sub>(<italic>t</italic>), in all the other I and E neurons (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>, left). The photo-stimulation was modeled as an application of a constant depolarising current with a strength parameter, <italic>a</italic><sub><italic>p</italic></sub>, proportional to the distance between the resting potential and the firing threshold (<italic>a</italic><sub><italic>p</italic></sub> = 0 means no stimulation, while <italic>a</italic><sub><italic>p</italic></sub> = 1 indicates photostimulation at the firing threshold). We quantified the effect of the simulated photostimulation of a target E neuron on other E and I neurons, distinguishing neurons with either similar or different tuning with respect to the target neuron (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>, right; <xref rid="figS2" ref-type="fig">Supplementary Fig. S2</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Mechanism of lateral excitation/inhibition in the efficient spiking network.</title>
<p><bold>(A)</bold> Left: Schematic of the E-I network and of the stimulation and measurement in a perturbation experiment. Right: Schematic of the propagation of the neural activity between E and I neurons with similar tuning.</p>
<p><bold>(B)</bold> Trial and neuron-averaged deviation of the firing rate from the baseline, for the population of I (top) and E (bottom) neurons with similar (magenta) and different tuning (gray) to the target neuron. The stimulation strength corresponded to an increase in the firing rate of the stimulated neuron by 28.0 Hz.</p>
<p><bold>(C)</bold> Scatter plot of the tuning similarity vs. effective connectivity to the target neuron. Red line marks zero effective connectivity and magenta line is the least-squares line. Stimulation strength was <italic>a</italic><sub><italic>p</italic></sub> = 1.</p>
<p><bold>(D)</bold> Top: Firing rate of the photostimulated neuron as a function of the photostimulation strength. Middle: Effective connectivity with I neurons with similar and different tuning to the target neuron. Bottom: Effective connectivity with E neurons.</p>
<p><bold>(E)</bold> Effective connectivity with I (top) and E neurons (bottom) while varying the length of the stimulation window. The window for measuring the effective connectivity was always 50 ms longer than the stimulation window.</p>
<p><bold>(F)</bold> Correlation of membrane potentials vs. the tuning similarity in E (top) and I cell type (bottom), for the efficient E-I network (left), for the network where each E neuron receives independent instead of shared stimulus features (middle), and for the network with unstructured connectivity (right). In the model with unstructured connectivity, elements of each connectivity matrix were randomly shuffled. We quantified voltage correlation using the (zero-lag) Pearson’s correlation coefficient, denoted as <inline-formula><inline-graphic xlink:href="590955v1_inline140.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, for each pair of neurons.</p>
<p><bold>(G)</bold> Average cross-correlogram (CCG) of spike timing with strongly similar (orange), weakly similar (green) and different tuning (black).</p>
<p><bold>(H)</bold> Distribution of noise correlations across neuronal pairs. The correlation coefficient was measured in bins of 30 ms.</p></caption>
<graphic xlink:href="590955v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The photostimulation of the target E neuron increased the instantaneous firing rate of similarly-tuned I neurons and reduced that of other similarly-tuned E neurons (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, <xref rid="figS2" ref-type="fig">Supplementary Fig. S2</xref>). We quantified the effective connectivity as the difference between the time-averaged firing rate of the recorded cell in presence or absence of the photostimulation of the targeted cell, measured during perturbation and up to 50 ms after. We found positive effective connectivity on I and negative effective connectivity on E neurons with similar tuning to the stimulated neuron, with a positive correlation between tuning similarity and effective connectivity on I neurons and a negative correlation on E neurons (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). As we varied the strength of the photostimulation, the firing rate of the target neuron increased proportionally to the photostimulation strength, as did the effect of the perturbation on I and E neurons with similar tuning to the target neuron (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>, <xref rid="figS2" ref-type="fig">Supplementary Fig. S2</xref>). As we varied the time window of photostimulation, we found that the effective connectivity converges within a time window of about 300 ms (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>). We confirmed these effects of photostimulation in presence of a weak feedforward input (<xref rid="figS2" ref-type="fig">Supplementary Fig. S2</xref>), similar to the experiments of Ref<sup><xref ref-type="bibr" rid="c61">61</xref></sup> in which photostimulation was applied during the presentation of visual stimuli with weak contrast.</p>
<p>In summary, lateral excitation of I neurons and lateral inhibition of E neurons with similar tuning is an emerging coding property of the efficient E-I network. Lateral excitation and inhibition leads to competition between neurons with similar tuning to stimulus features, comparable to that found in the visual cortex<sup><xref ref-type="bibr" rid="c61">61</xref>,<xref ref-type="bibr" rid="c62">62</xref></sup>. An intuitive summary of how this mechanism is implemented is that the E neuron that fires first activates I neurons with similar tuning. In turn, these I neurons inhibit all similarly tuned E neurons (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>, right), preventing them to generate redundant spikes and encoding the sensory information that has already been encoded by the first spike. Suppression of redundant spiking allows efficient coding because it reduces the metabolic cost without compromising on encoded information<sup><xref ref-type="bibr" rid="c36">36</xref></sup>.</p>
<p>To explore further the consequences of E-I interactions for stimulus encoding, we next investigated the dynamics of lateral inhibition in a network driven by the feed-forward sensory input but without perturbing neurons. In this case, shared feedforward inputs <italic>s</italic><sub><italic>k</italic></sub>(<italic>t</italic>) create a particular pattern of voltage correlations in E-E neuronal pairs, where voltage correlations linearly depend on the tuning similarity (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>, left). The feedforward inputs are shared across neurons and weighted by the tuning parameters of E neurons. For this reason, they cause strong positive voltage correlations between E-E neuronal pairs with very similar tuning and strong negative correlations between pairs with very different (opposite) tuning (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>, top-left). Voltage correlations between E-E pairs vanished regardless of tuning similarity when we made the inputs independent across neurons (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>, top-middle), showing the relation between tuning similarity and voltage correlation occurs because of shared feedforward inputs. In contrast to E neurons, I neurons do not receive feedforward inputs and are driven only by similarly tuned E neurons (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>, right). This causes positive voltage correlations in I-I neuronal pairs with similar tuning and vanishing correlations in neurons with different tuning (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>, bottom-left). Such dependence of voltage correlations on tuning similarity disappears when removing the structure from the E-I synaptic connectivity (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>, bottom-right).</p>
<p>Although membrane potentials could be strongly correlated or anti-correlated depending on tuning similarity (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>, left), the coordination of spike timing of pairs of E neurons (measured with cross-correlograms or CCGs) was very weak (<xref rid="fig2" ref-type="fig">Fig. 2G-H</xref>). For I-I neuronal pairs, the peaks of CCGs were stronger than those observed in E-E pairs, but they were present only at very short lags (lags &lt; 1 ms), and the same was true for E-I pairs. Additionally, noise correlations measured as Pearson correlation on spike counts in trials with the same stimulus (<italic>r</italic><sub><italic>SC</italic></sub>) had values distributed around zero (<xref rid="fig2" ref-type="fig">Fig. 2H</xref>). These findings lead to two conclusions. First, recurrent interactions of the efficient E-I network wipe away the effect of membrane potential correlations to produce largely uncorrelated spiking output, consistently with the efficient coding hypothesis of reducing redundancy in cases of low noise<sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c6">6</xref></sup>. Second, such precise cancelling of correlations between voltages and the spiking output reflects the millisecond precision of information processing in efficient E-I networks.</p>
</sec>
<sec id="s2c">
<title>The effect of structured connectivity on coding efficiency and neural dynamics</title>
<p>The analytical solution of the optimally efficient E-I network predicts that recurrent synaptic weights are proportional to the tuning similarity between neurons. We here investigated the role of such efficient connectivity structure by comparing the behavior of an efficiently structured network with a similar but randomly structured E-I network of the type studied in previous works<sup><xref ref-type="bibr" rid="c63">63</xref>,<xref ref-type="bibr" rid="c64">64</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>. We removed the connectivity structure by randomly permuting synaptic weights across neuronal pairs. We either randomized connections within a single connectivity type (E-I, I-I or I-E) or within all these three connectivity types at once (“all”). Such procedure destroys the relationship between tuning similarity and synaptic strength as in <xref rid="fig1" ref-type="fig">Fig. 1F</xref> while it preserves Dale’s law and the overall distribution of connectivity weights. We found that randomizing the connectivity structure significantly altered neural dynamics and coding (<xref rid="fig3" ref-type="fig">Fig. 3A-H</xref>). The structure in E-I and in I-E connectivity has a major effect on efficient coding. Randomizing E-I and I-E connectivity led to several-fold increases in the encoding error as well as to significant increases in the metabolic cost (<xref rid="fig3" ref-type="fig">Fig. 3A-B</xref>). In particular, with unstructured E-I connectivity the network failed completely to encode the target with I population (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Effects of connectivity structure on coding efficiency, neural dynamics and lateral inhibition.</title>
<p><bold>(A)</bold> Relative error of networks with unstructured (shuffled) recurrent connectivity. The relative error is the RMSE of the unstructured network, relative to the RMSE of the structured network (dashed line). From left to right, we show the relative error for the unstructured E-I, I-I, I-E and all connectivities. <bold>(B</bold> Same as in <bold>A</bold>, showing the metabolic cost (MC) of unstructured networks relative to the metabolic cost of the structured network.</p>
<p><bold>(C)</bold> Target signal (black), E estimate (red) and I estimate (blue) in one particular input dimension, for networks with unstructured connectivity.</p>
<p><bold>(D)</bold> Standard deviation of the membrane potential (in mV) for networks with unstructured connectivity. Distributions are across neurons. The black vertical line marks the average SD of the structured network.</p>
<p><bold>(E)</bold> Average firing rate of E neurons (top) and I neurons (bottom), for different cases of unstructured networks. Dashed lines show the same measures for the structured case.</p>
<p><bold>(F)</bold> Same as in <bold>E</bold>, showing the average net synaptic input.</p>
<p><bold>(G)</bold> Same as in <bold>E</bold>, showing the time-dependent correlation of synaptic inputs.</p>
<p><bold>(H)</bold> Voltage correlation in E-E (top) and I-I neuronal pairs (bottom) for the four cases of unstructured connectivity (colored dots) and the equivalent result in the structured network (grey dots). We show the results for pairs with similar tuning.</p>
<p><bold>(I)</bold> Scatter plot of effective connectivity in I (top) and E neurons (bottom) versus tuning similarity to the stimulated (“target”) E neuron, for networks with unstructured connectivity. The magenta line is the least-squares regression line. The strength of the photostimulation is at threshold (<italic>a</italic><sub><italic>p</italic></sub> = 1.0). Other parameters for all plots are in <xref rid="tbl1" ref-type="table">Table 1</xref>.</p></caption>
<graphic xlink:href="590955v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Unstructured E-I and I-E connectivity also yielded an increase of the variance in the membrane potentials (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>) and firing rate in E neurons (<xref rid="fig3" ref-type="fig">Fig. 3E</xref>), while pulling the average net synaptic inputs towards inhibition (<xref rid="fig3" ref-type="fig">Fig. 3F</xref>) and removing the instantaneous balance (<xref rid="fig3" ref-type="fig">Fig. 3G</xref>). Together, these findings suggest a shift from mean-driven to fluctuation-driven spiking activity as the connectivity structure is removed. The structure of E-I connectivity was also found to be crucial for the linear relation between voltage correlations and tuning similarity in pairs of I neurons (<xref rid="fig3" ref-type="fig">Fig. 3H</xref>, magenta). Interestingly, we found no effect of connectivity structure on the variability of spiking of single neurons, with both structured and unstructured networks showing strong variability (<xref rid="figS3" ref-type="fig">Supplementary Fig. S3</xref>), suggesting that the variability of spiking is independent of the connectivity structure.</p>
<p>Randomizing I-I connectivity was less detrimental to the coding efficiency as it led to a slightly higher encoding error, but to a lower metabolic cost, and still allowed for a relatively good tracking of target signals in both cell types (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>, “permuted I to I”). Contrary to randomization of the E-I and I-E connectivity, shuffling I-I connectivity decreased the variance of the membrane potential, decreased the firing rate in E neurons and increased instantaneous balance in E neurons. Thus it had opposite effects compared to shuffling of E-I and I-E connectivity. To understand if there was a minimal connectivity structure necessary for efficient coding, we also removed the connectivity structure only partially, keeping like-to-like connectivity structure and removing all structure beyond like-to-like. This manipulation only had very modest effects on network’s coding and almost no effect on neural dynamics (<xref rid="figS3" ref-type="fig">Supplementary Fig. S3</xref>), thus showing that like-to-like structure of connectivity is largely sufficient to achieve efficient coding.</p>
<p>Finally, we analyzed how the structure in recurrent connectivity influences lateral inhibition that we observed in efficient (structured) networks (see <xref rid="fig2" ref-type="fig">Fig. 2A-E</xref>). We found that the dependence of lateral inhibition on tuning similarity vanish when the connectivity structure is fully removed (<xref rid="fig3" ref-type="fig">Fig. 3I</xref>, right), thus showing that connectivity structure is necessary for lateral inhibition. While networks with unstructured E-I and I-E connectivity still show inhibition in E neurons upon single neuron optostimulation (because of the net inhibitory effect of recurrent connectivity; <xref rid="figS4" ref-type="fig">Supplementary Fig. S4</xref>), this inhibition was largely unspecific to tuning similarity. Unstructured connectivity decreased the correlation between tuning similarity and effective connectivity from <italic>r</italic> = [0.31, − 0.54] in E and I neurons in a structured network to <italic>r</italic> = [0.02, − 0.13] and <italic>r</italic> = [0.57, 0.11] in networks with unstructured E-I and I-E connectivity, respectively (<xref rid="fig3" ref-type="fig">Fig. 3I</xref>, first and third from the left). Removing the structure in I-I connectivity, in contrast, increased the correlation between effective connectivity and tuning similarity in E neurons (<italic>r</italic> = [0.30, − 0.65], <xref rid="fig3" ref-type="fig">Fig. 3I</xref>, second from the left), showing that lateral inhibition takes place irrespective of the I-I connectivity structure. Furthermore, a partial removal of connectivity structure where we only removed the connectivity structure beyond like-to-like had smaller effects on lateral inhibition (<xref rid="figS4" ref-type="fig">Supplementary Fig. S4</xref>), thus confirming that like-to-like connectivity pattern is sufficient for lateral excitation/inhibition in I and E neurons.</p>
<p>While optimally structured connectivity predicted by efficient coding is biologically plausible, it may be difficult to realise it exactly on a synapse-by-synapse basis in biological networks. We verified the robustness of the model to small deviations from the optimal synaptic weights by adding a random jitter, proportional to the synaptic strength, to all synaptic connections (see Methods). The encoding performance and neural dynamics were barely affected by such perturbation, demonstrating that the network is robust against random perturbations of the optimal synaptic weights (<xref rid="figS3" ref-type="fig">Supplementary Fig. S3</xref>).</p>
<p>In summary, we found that some aspects of recurrent connectivity structure, such as the like-to-like organization of E-I and I-E connectivity, are crucial to achieve efficient coding. Instead, for other aspects there is considerable flexibility; the organization of I-I connectivity is less crucial, as is the connectivity structure beyond like-to-like, and adding small perturbations to optimal weights has only minor effects. Structured E-I and I-E, but not I-I connectivity, is necessary for a robust dependence of lateral inhibition on tuning similarity.</p>
</sec>
<sec id="s2d">
<title>Weak spike-triggered adaptation optimizes network efficiency</title>
<p>We next investigated the role of spike-triggered adaptation current,<inline-formula><inline-graphic xlink:href="590955v1_inline22.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, that emerges from the optimally efficient solution (<xref ref-type="disp-formula" rid="eqn5">Eq. 5</xref>). This current provides a within-neuron feedback triggered by each spike, with time constant equal to that of the single neuron readout <inline-formula><inline-graphic xlink:href="590955v1_inline23.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (E neurons) and <inline-formula><inline-graphic xlink:href="590955v1_inline24.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (I neurons). The strength of the current is proportional to the difference in inverse time constants of single neuron and population readouts, <inline-formula><inline-graphic xlink:href="590955v1_inline25.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and it is thus absent in previous studies assuming that these time constants are equal<sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c42">42</xref></sup>.</p>
<p>Depending on the sign of the difference of time constants, this spike-triggered current is negative, giving spike-triggered adaptation<sup><xref ref-type="bibr" rid="c39">39</xref></sup>, if the single-neuron readout has longer time constant than the population readout <inline-formula><inline-graphic xlink:href="590955v1_inline26.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, or positive, giving spike-triggered facilitation, if the opposite is true <inline-formula><inline-graphic xlink:href="590955v1_inline27.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref rid="tbl2" ref-type="table">Table 2</xref>). We expected that network efficiency would benefit from spike-triggered adaptation (in short, adaptation), because accurate encoding requires fast temporal dynamics of the population readouts, to capture fast fluctuations in the target signal, while we expect a slower dynamics in the readout of single neuron’s firing frequency,<inline-formula><inline-graphic xlink:href="590955v1_inline28.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, a process that could be related to homeostatic regulation of single neuron’s firing rate<sup><xref ref-type="bibr" rid="c65">65</xref>,<xref ref-type="bibr" rid="c66">66</xref></sup>. Measuring performance of a simulated E-I network, we indeed found that optimal coding efficiency is achieved with weak adaptation in both cell types, and in particular in regimes where the adaptation is stronger in E compared to I neurons (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>). We note that adaptation in E neurons promotes efficient coding because it enforces every spike to be error-correcting, while a spike-triggered facilitation in E neurons would lead to additional spikes that might be redundant and reduce network efficiency. Contrary to previously proposed model of adaptation in LIF neurons<sup><xref ref-type="bibr" rid="c38">38</xref></sup>, here strength and the time constant of adaptation are not independent, but they both depend on <inline-formula><inline-graphic xlink:href="590955v1_inline29.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with larger <inline-formula><inline-graphic xlink:href="590955v1_inline30.gif" mimetype="image" mime-subtype="gif"/></inline-formula> yielding both longer and stronger adaptation.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Relation of time constants of single-neuron and population readout set an adaptation or a facilitation current.</title>
<p>The population readout that evolves on a faster (slower) time scale than the single neuron readout determines a spike-triggered adaptation (facilitation) in its own cell type.</p></caption>
<graphic xlink:href="590955v1_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Adaptation, network coding efficiency and excitation-inhibition balance.</title>
<p><bold>(A)</bold> The encoding error (left), metabolic cost (middle) and average loss (right) as a function of single neuron time constants <inline-formula><inline-graphic xlink:href="590955v1_inline141.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (E neurons) and <inline-formula><inline-graphic xlink:href="590955v1_inline142.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (I neurons), in units of ms. These parameters set the sign, the strength, as well as the time constant of the feedback current in E and I neurons. Best performance is obtained in the top right quadrant, where the feedback current is spike-triggered adaptation in both E and I neurons. The performance measures are computed as a weighted sum of the respective measures across the E and I populations with equal weighting for E and I. All measures are plotted on the scale of the natural logarithm for better visibility.</p>
<p><bold>(B)</bold> Top: Log-log plot of the RMSE of the E (red) and the I (blue) estimates as a function of the time constant of the single neuron readout of E neurons,<inline-formula><inline-graphic xlink:href="590955v1_inline143.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Feedback current in I neurons is set to 0. Bottom: Same as on the top, as a function of <inline-formula><inline-graphic xlink:href="590955v1_inline144.gif" mimetype="image" mime-subtype="gif"/></inline-formula> while the feedback current in E neurons is set to 0.</p>
<p><bold>(C)</bold> Firing rate in E (left) and I neurons (right), as a function of <inline-formula><inline-graphic xlink:href="590955v1_inline145.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline146.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the regime with spike-triggered adaptation.</p>
<p><bold>(D)</bold> Same as in <bold>(C)</bold>, showing the coefficient of variation.</p>
<p><bold>(E)</bold> Average net synaptic input in E neurons (left) and in I neurons (right) as a function of <inline-formula><inline-graphic xlink:href="590955v1_inline147.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline148.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
<p><bold>(F)</bold> Correlation coefficient of synaptic inputs to E (left) and I neurons (right) as a function of <inline-formula><inline-graphic xlink:href="590955v1_inline149.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline150.gif" mimetype="image" mime-subtype="gif"/></inline-formula></p></caption>
<graphic xlink:href="590955v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To gain further insights on how adaptation influences network performance, we set the adaptation in one cell type to 0 and vary the strength of adaptation in the other cell type by varying the time constant of the single neuron readout. In the absence of adaptation in I neurons <inline-formula><inline-graphic xlink:href="590955v1_inline31.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, adaptation in E neurons resulted in an increase of the encoding error in E neurons and a decrease in I neurons (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>, top). Conversely, adaptation in I neurons (with no adaptation in E neurons) was harmful for the efficiency of the model, as it led to an increase in the encoding error in both cell types (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>, bottom).</p>
<p>Firing rates and variability of spiking were sensitive to the strength of adaptation. As expected, adaptation in E neurons caused a decrease in the firing levels in both cell types (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>). In contrast, adaptation in I neurons decreased the firing rate in I neurons, but increased the firing rate in E neurons, due to a decrease in the level of inhibition. Furthermore, adaptation decreased the variability of spiking, in particular in the cell type with strong adaptation (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>), a well-known effect of spike-triggered adaptation in single neurons<sup><xref ref-type="bibr" rid="c67">67</xref></sup>.</p>
</sec>
<sec id="s2e">
<title>Instantaneous balance of synaptic currents predicts network efficiency better than the average E-I balance</title>
<p>Next, we tested the capability of instantaneous and average E-I balance to predict the efficiency of the network. Measuring average balance and instantaneous balance of synaptic inputs from electrophysiology recordings is possible<sup><xref ref-type="bibr" rid="c59">59</xref>,<xref ref-type="bibr" rid="c60">60</xref>,<xref ref-type="bibr" rid="c58">58</xref></sup>, while measuring efficiency from empirical data is challenging. The estimation of network efficiency requires the comparison between typically unknown network’s target representations and the population readouts. The estimation of the population readout, in turn, requires an estimation of decoding weights and the knowledge of spiking dynamics from a complete neural network.</p>
<p>We focused the analysis on regimes with adaptation, because these regimes gave better performance. In regimes with adaptation, time constants of single neuron readout influenced the average imbalance (<xref rid="fig4" ref-type="fig">Fig. 4E</xref>) as well as the instantaneous balance (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>) in E and I cell type. The average balance was precise (with the net synaptic current close to 0) with strong adaptation in E neurons, and it got weaker when increasing the adaptation in I neurons (<xref rid="fig4" ref-type="fig">Fig. 4E</xref>). However, regimes with precise average balance in both cell types coincided with suboptimal efficiency (compare <xref rid="fig4" ref-type="fig">Fig. 4A</xref>, right and E).</p>
<p>To test how well the average imbalance and the instantaneous balance of synaptic inputs predict network efficiency, we concatenated the column-vectors of the measured average loss and of the average imbalance in each cell type and computed the Pearson correlation between these quantities. The correlation between the average imbalance and the average loss was weak in the E cell type (<italic>R</italic> = 0.16) and close to zero in the I cell type (<italic>R</italic> = 0.02), suggesting almost no relation between efficiency and average imbalance in the E cell type. In contrast, the average loss was negatively correlated with the instantaneous balance in both E (<italic>R</italic> = − 0.35) and in I cell type (<italic>R</italic> = − 0.45), showing that instantaneous balance of synaptic inputs is positively correlated with network efficiency.</p>
<p>When measured for varying levels of spike-triggered adaptation, unlike the average balance of synaptic inputs, the instantaneous balance is therefore a reliable predictor of network efficiency.</p>
</sec>
<sec id="s2f">
<title>State-dependent coding and dynamics are controlled by the metabolic cost on spiking</title>
<p>In our derivation of efficiency objectives, we obtained non-specific external current (in the following, non-specific current), described by the term <inline-formula><inline-graphic xlink:href="590955v1_inline32.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and comprising mean and fluctuations (see Methods). Non-specific current captures the ensemble of all synaptic currents that are unrelated and un-specific with respect to the stimulus features. This non-specific term collates effects of synaptic currents from neurons untuned to the stimulus<sup><xref ref-type="bibr" rid="c68">68</xref>,<xref ref-type="bibr" rid="c69">69</xref></sup>, as well as synaptic currents from other brain areas. This term can also be conceptualized as the “background” synaptic activity that is thought to provide a large fraction of all synaptic inputs to both E and I neurons in cortical networks<sup><xref ref-type="bibr" rid="c70">70</xref></sup>, and which may modulate feedforward-driven responses by controlling how far is typically the membrane potential from the firing threshold<sup><xref ref-type="bibr" rid="c71">71</xref></sup>. Likewise, in our model, the external current does not directly convey information about the feedforward input features, but influences the operating regime of the network. The mean of the non-specific external currents is proportional to the metabolic constant <italic>β</italic> and its fluctuations reflect the noise that we assumed in the condition for spiking. Since <italic>β</italic> governs the trade-off between encoding error and metabolic cost (<xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>), higher values of <italic>β</italic> imply that more importance is assigned to the metabolic efficiency than to coding accuracy, yielding a reduction in firing rates. In the expression for the non-specific synaptic current, we found that the mean of the current is negatively proportional to the metabolic constant <italic>β</italic> (see Methods). The non-specific current is typically depolarizing, meaning that increasing <italic>β</italic> yields a weaker non-specific current and increases the distance between mean membrane potential and the firing threshold. Thus, an increase of the metabolic constant is expected to create a network state that is less responsive to the feedforward signal.</p>
<p>We found the metabolic constant <italic>β</italic> to significantly influence the spiking dynamics (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>). The optimal efficiency was achieved for non-zero levels of the metabolic constant (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>). The metabolic constant modulated the firing rate as expected, with the firing rate decreasing with the increasing of the metabolic constant (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>, top). It also modulated the variability of spiking, as increasing the metabolic constant decreased the variability of spiking in single neurons (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>, bottom). Furthermore, it modulated the average imbalance and the instantaneous balance in opposite ways: larger values of <italic>β</italic> led to regimes that had stronger average balance, but weaker instantaneous balance (<xref rid="fig5" ref-type="fig">Fig. 5D</xref>). We note that, even with suboptimal values of the metabolic constant, the neural dynamics remained within biologically relevant ranges.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>State-dependent coding and dynamics are controlled by non-specific currents.</title>
<p><bold>(A)</bold> Spike trains of the efficient E-I network in one simulation trial, with different values of the metabolic constant <italic>β</italic>. The network received identical stimulus across trials.</p>
<p><bold>(B)</bold> Top: RMSE of E (red) and I (blue) estimates as a function of the metabolic constant. Bottom: Normalized average metabolic cost and average loss as a function of the metabolic constant. Black arrow indicates the minimum loss and therefore the optimal metabolic constant.</p>
<p><bold>(C)</bold> Average firing rate (top) and the coefficient of variation of the spiking activity (bottom), as a function of the metabolic constant. Black arrow marks the metabolic constant leading to optimal network efficiency in <bold>B</bold>.</p>
<p><bold>(D)</bold> Average imbalance (top) and instantaneous balance (bottom) balance as a function of the metabolic constant.</p>
<p><bold>(E)</bold> Same as in <bold>A</bold>, but for different values of the noise intensity <italic>σ</italic>.</p>
<p><bold>(F)</bold> Same as in <bold>B</bold>, as a function of the noise intensity. The noise is a Gaussian random process, independent over time and across neurons.</p>
<p><bold>(G)</bold> Same as <bold>C</bold>, as a function of the noise intensity.</p>
<p><bold>(H)</bold> Top: Same as in <bold>D</bold>, as a function of the noise intensity. For plots in <bold>B-D</bold> and <bold>F-H</bold>, we computed and averaged results over 100 simulation trials with 1 second of simulation time. For other parameters, see <xref rid="tbl1" ref-type="table">Table 1</xref>.</p></caption>
<graphic xlink:href="590955v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The fluctuation part of the non-specific current, modulated by the noise intensity <italic>σ</italic>, that we added in the definition of spiking rule for biological plausibility (see Methods), strongly affected the neural dynamics as well (<xref rid="fig5" ref-type="fig">Fig. 5E</xref>). The optimal performance was achieved with non-vanishing noise levels (<xref rid="fig5" ref-type="fig">Fig. 5F</xref>) and the beneficial effect of the noise in the non-specific current arose from its impact on the instantaneous E-I balance. While the average firing rate of both cell types, as well as the variability of spiking in E neurons, increased with noise variance (<xref rid="fig5" ref-type="fig">Fig. 5G</xref>), the average and instantaneous balance of synaptic currents exhibited a non-linear behavior as a function of noise variance (<xref rid="fig5" ref-type="fig">Fig. 5H</xref>). Due to decorrelation of membrane potentials by the noise, instantaneous balance decreased with increasing noise variance (<xref rid="fig5" ref-type="fig">Fig. 5H</xref>, bottom). Some level of noise in the non-specific inputs is therefore necessary to establish the optimal level of instantaneous E-I balance. Interestingly, single neurons manifest significant levels of spiking variability already in the absence of noise in the non-specific inputs (<xref rid="fig5" ref-type="fig">Fig. 5H</xref>, bottom), indicating that the recurrent network dynamics generates substantial variability even in absence of variability in the external current. Variability in absence of noise demonstrates the intrinsic chaotic behavior of the network<sup><xref ref-type="bibr" rid="c72">72</xref></sup>.</p>
<p>In summary, non-specific external currents derived in our optimal solution have a major effect on coding efficiency and on neural dynamics. The noise in the external current is particularly important to obtain optimal levels of the instantaneous E-I balance in I neurons.</p>
</sec>
<sec id="s2g">
<title>Optimal ratio of E-I neuron numbers and of the mean I-I to E-I synaptic efficacy coincide with biophysical measurements</title>
<p>Next, we investigated how coding efficiency and neural dynamics depend on the ratio of the number of E and I neurons (<italic>N</italic><sup><italic>E</italic></sup> : <italic>N</italic><sup><italic>I</italic></sup> or E-I ratio) and on the relative synaptic strengths between E-I and I-I connections.</p>
<p>Efficiency objectives (<xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>) are based on population, rather than single-neuron activity. Our efficient E-I network thus realizes a computation of the target representation that is distributed across multiple neurons (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>). We predict that, if number of neurons within the population decreases, neurons have to fire more spikes to achieve an optimal population readout because the task of tracking the target signal is distributed among fewer neurons. To test this prediction, we varied the number of I neurons while keeping the number of E neurons constant. As predicted, a decrease of the number of I neurons (and thus an increase in the ratio of the number of E to I neurons) caused a linear increase in the firing rate of I neurons, while the firing rate of E neurons stayed constant (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>, top). However, the variability of spiking and the average synaptic inputs remained relatively constant in both cell types as we varied these ratios (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>, bottom, C), indicating a compensation for the change in the ratio of E-I neuron numbers through adjustment in the firing rates. These results are consistent with the observation in neuronal cultures of a linear change in the rate of postsynaptic events but unchanged postsynaptic current in either E and I neurons for variations in the E-I neuron number ratio<sup><xref ref-type="bibr" rid="c73">73</xref></sup>.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Optimal ratios of E-I neuron numbers and of mean I-I to E-I efficacy.</title>
<p><bold>(A)</bold> Schematic of the effect of changing the number of I neurons on firing rates of I neurons. As encoding of the stimulus is distributed among more I neurons, the number of spikes per I neuron decreases.</p>
<p><bold>(B)</bold> Average firing rate as a function of the ratio of the number of E to I neurons. Black arrow marks the optimal ratio.</p>
<p><bold>(C)</bold> Average net synaptic currents in E neurons (top) and in I neurons (bottom).</p>
<p><bold>(D)</bold> Top: Encoding error (RMSE) of the E (red) and I (blue) estimates, as a function of the ratio of E-I neuron numbers. Bottom: Same as on top, showing the cost and the average loss. Black arrow shows the minimum of the loss, indicating the optimal parameter.</p>
<p><bold>(E)</bold> Top: Optimal ratio of the number of E to I neurons as a function of the weighting of the average loss of E and I cell type (using the weighting of the error and cost of 0.7 and 0.3, respectively). Bottom: Same as on top, measured as a function of the weighting of the error and the cost when computing the loss. (The weighting of the losses of E and I neurons is 0.5.) Black triangles mark weightings that we typically used.</p>
<p><bold>(F)</bold> Schematic of the readout of the spiking activity of an E neuron (red) and an I neuron (blue) with equal amplitude of decoding weight (left) and with stronger decoding weight in the I neuron (right). Stronger decoding weight in the I neuron results in a stronger effect of spikes of the I neuron on the readout, leading to less spikes by the I neuron.</p>
<p><bold>(G)</bold> Same as in (D), as a function of the ratio of mean I-I to E-I efficacy.</p>
<p><bold>(H)</bold> Same as in <bold>B</bold>, as a function of the ratio of mean I-I to E-I efficacy.</p>
<p><bold>(I)</bold> Average imbalance (top) and instantaneous balance (bottom) balance, as a function of the ratio of mean I-I to E-I efficacy. For other parameters, see <xref rid="tbl1" ref-type="table">Table 1</xref>.</p></caption>
<graphic xlink:href="590955v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The ratio of the number of E to I neurons had a significant influence on coding efficiency. We found a unique minimum of the encoding error of each cell type, while the metabolic cost increased linearly with the ratio of the number of E and I neurons (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>). We found the optimal ratio of E to I neuron numbers to be in range observed experimentally in cortical circuits (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>, bottom, black arrow, <italic>N</italic><sup><italic>E</italic></sup> : <italic>N</italic><sup><italic>I</italic></sup> = 3.75 : 1;<sup><xref ref-type="bibr" rid="c74">74</xref></sup>). Due to the linear increase of the cost with the ratio of the number of E and I neurons (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>, bottom, green), strong weighting of the error predicted higher ratios (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>, bottom). Also the encoding error (RMSE) alone, without considering the metabolic cost, predicted optimal ratio of the number of E to I neurons within a plausible physiological range, <italic>N</italic><sup><italic>E</italic></sup> : <italic>N</italic><sup><italic>I</italic></sup> = [3.75 : 1, 5.25 : 1], with stronger weightings of the encoding error by I neurons predicting higher ratios (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>, top).</p>
<p>Next, we investigated the impact of the strength of excitatory and inhibitory synaptic efficacy (EPSPs and IPSPs). In our model, the mean synaptic efficacy is fully determined by the distribution of tuning parameters (see Methods). As evident from the expression for the population readouts (<xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref>), the amplitude of tuning parameters (which are also decoding weights) determines the amplitude of jumps of the population readout caused by spikes (<xref rid="fig6" ref-type="fig">Fig. 6F</xref>). The stronger the amplitude of these weights, the larger is the average impact of spikes on the population signals.</p>
<p>We parametrized the distribution of decoding weights as a normal distributions centered at zero, but allowed the standard deviation (SD) of distributions relative to E and I neurons (<inline-formula><inline-graphic xlink:href="590955v1_inline33.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline34.gif" mimetype="image" mime-subtype="gif"/></inline-formula>) to vary across E and I cell type. With such parametrization, we were able to analytically evaluate the mean E-I, I-I and I-E synaptic efficacy (see Methods). We found that in the optimally efficient network, the mean E-I and I-E synaptic efficacy is exactly balanced.</p>
<p>We next searched for the optimal ratio of the mean I-I to E-I efficacy as the parameter setting that maximizes network efficiency. Network efficiency was maximized when such ratio was about 3 to 1 (<xref rid="fig6" ref-type="fig">Fig. 6G</xref>). Our results predict the maximum E-I and I-E synaptic efficacy, averaged across neuronal pairs, of 0.75 mV, and the maximal I-I efficacy of 2.25 mV, values that are consistent with empirical measurements in the primary sensory cortex<sup><xref ref-type="bibr" rid="c75">75</xref>,<xref ref-type="bibr" rid="c52">52</xref>,<xref ref-type="bibr" rid="c53">53</xref></sup>.</p>
<p>Similarly to the ratio of E-I neuron numbers, a change in the ratio of mean E-I to I-E synaptic efficacy was compensated for by a change in firing rates, with stronger I-I synapses leading to a decrease in the firing rate of I neurons (<xref rid="fig6" ref-type="fig">Fig. 6H</xref>). Conversely, weakening the E-I and I-E synapses resulted in an increase in the firing rate in E neurons (<xref rid="figS5" ref-type="fig">Supplementary Fig. S5</xref>). This is easily understood by considering that weakening the E-I and I-E synapses activates less strongly the lateral inhibition in E neurons (<xref rid="fig2" ref-type="fig">Fig. 2</xref>) and thus leads to an increase in the firing rate of E neurons. We also found that single neuron variability remained almost unchanged when varying the ratio of mean I-I to E-I efficacy (<xref rid="fig6" ref-type="fig">Fig. 6H</xref>, bottom) and the optimal ratio corresponded with previously found optimal levels of average and instantaneous balance of synaptic inputs (<xref rid="fig6" ref-type="fig">Fig. 6I</xref>). The instantaneous E-I balance monotonically decreased with increasing ratio of I-I to E-I efficacy (<xref rid="fig6" ref-type="fig">Fig. 6I</xref>, bottom, <xref rid="figS5" ref-type="fig">Supplementary Fig. S5</xref>).</p>
<p>In summary, our analysis suggests that optimal coding efficiency is achieved with four times more E neurons than I neurons and with mean I-I synaptic efficacy about 3 times stronger than the E-I and I-E efficacy. The optimal network has less I than E neurons, but the impact of spikes of I neurons on the population readout is stronger, also suggesting that spikes of I neurons convey more information.</p>
</sec>
<sec id="s2h">
<title>Dependence of efficient coding and neural dynamics on the timescales and dimensionality of the stimulus</title>
<p>We finally investigated how the network’s behavior depends on the timescales and dimensionality of the input stimulus features. We manipulated the stimulus timescales by changing the time constant of the Ornstein-Uhlenbeck (O-U) process. The network efficiently encoded stimulus features when their time constants varied between 1 and 200 ms, with stable encoding error, metabolic cost (<xref rid="fig7" ref-type="fig">Fig. 7A</xref>) and neural dynamics (<xref rid="figS6" ref-type="fig">Supplementary Fig. S6</xref>).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Dependence of efficient coding and neural dynamics on stimulus parameters and advantages of E-I versus one cell type model architecture.</title>
<p><bold>(A)</bold> Top: Root mean squared error (RMSE) of E estimates (red) and I estimates (blue), as a function of the time constant of stimulus features. Bottom: Same as on top, showing the metabolic cost (MC) of E and I cell type. The time constant <italic>τ</italic><sub><italic>s</italic></sub> is the same for all stimulus features.</p>
<p><bold>(B)</bold> Top: Same as in <bold>A</bold> top, measured as a function of the number of stimulus features <italic>M</italic>. Bottom: Normalized cost and the average loss as a function of the number of input features. Black arrow marks the minimum loss and the optimal parameter <italic>M</italic>.</p>
<p><bold>(C)</bold> Root mean squared error (top) and metabolic cost (bottom) in E and I populations in the E-I model and in the 1CT model. The distribution is across simulation trials.</p>
<p><bold>(D)</bold> Average loss in the E-I and 1CT models with weighting <italic>g</italic><sub><italic>L</italic></sub> = 0.7 for the error (and 0.3 for the cost).</p>
<p><bold>(E)</bold> Firing rate in the 1CT model as a function of the metabolic constant. For other parameters of the E-I model see <xref rid="tbl1" ref-type="table">Table 1</xref>, and for the 1CT model see <xref rid="tblS1" ref-type="table">Supplementary Table S1</xref>.</p></caption>
<graphic xlink:href="590955v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Finally, we tested how the network’s behavior changed when we varied the number of stimulus features <italic>M</italic> processed by the network. The encoding error of E (RMSE<sup><italic>E</italic></sup>) and I neurons (RMSE<sup><italic>I</italic></sup>) had a minimum at 3 and 4 stimulus features, respectively (<xref ref-type="fig" rid="fig7">Fig.7B</xref>, top), while the metabolic cost increased monotonically with the number of features (<xref ref-type="fig" rid="fig7">Fig.7B</xref>, bottom). The number of features that optimized network efficiency (the average loss) ranged between <italic>M</italic> = [1, 4]. With strong weighting of the error (<italic>g</italic><sub><italic>L</italic></sub> ≥ 0.89), the optimal number of features was <italic>M</italic> = 4, and with strong weighting of the cost, (<italic>g</italic><sub><italic>L</italic></sub> &lt; 0.27), the optimal number of features was <italic>M</italic> = 1. It is intriguing that the optimal encoding performance, when assuming the weighting for the error is stronger than for the cost, is achieved not for a single stimulus feature, but for 3 or 4 independent features. Increasing the number of features beyond the optimal number resulted in a monotonic increase in firing rates for both cell types and in a contrasting effect on average and instantaneous balance, as it increased the average E-I balance and weakened the instantaneous balance (<xref rid="figS6" ref-type="fig">Supplementary Fig. S6</xref>).</p>
<p>In sum, we found the optimal network efficiency in presence of several (3 or 4) stimulus features, and a surprising ability of the network to accurately encode stimuli on a wide range of timescales.</p>
</sec>
<sec id="s2i">
<title>Advantages of E-I versus one cell type model architecture for coding efficiency and robustness to parameter variations</title>
<p>Neurons in the brain are either excitatory or inhibitory. To understand how differentiating E and I neurons benefits efficient coding, we compared the properties of our efficient E-I network with an efficient network with a single cell type (1CT). The 1CT model is a simplification of the E-I model (see Supplementary Text 1) and has been derived and analyzed in previous studies<sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c42">42</xref></sup>. We compared the average encoding error (RMSE), the average metabolic cost (MC), and the average loss (see Supplementary Text 2) of the E-I model against the one cell type (1CT) model. Compared to the 1CT model, the E-I model exhibited a higher encoding error and metabolic cost in the E population, but a lower encoding error and metabolic cost in the I population (<xref rid="fig7" ref-type="fig">Fig. 7C</xref>). The average loss of the E-I model was significantly smaller than that of the 1CT model when using the typical weighting of the error and the cost of <italic>g</italic><sub><italic>L</italic></sub> = 0.7 (<xref rid="fig7" ref-type="fig">Fig. 7D</xref>), as well as for the vast majority of other weightings (<italic>g</italic><sub><italic>L</italic></sub> ≤ 0.95; <xref rid="figS1" ref-type="fig">Supplementary Fig. S1</xref>).</p>
<p>We further compared the 1CT and E-I models in terms of the robustness of firing rates to changes in the metabolic constant. Consistently with previous studies<sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>, firing rates in the 1CT model were highly sensitive to variations in the metabolic constant (<xref rid="fig7" ref-type="fig">Fig. 7E</xref>, note the logarithmic scale on the y-axis), with a superexponential growth of the firing rate with the inverse of the metabolic constant in regimes with metabolic cost lower than optimal. This is in contrast to the E-I model, whose firing rates exhibited lower sensitivity to the metabolic constant, and never exceeded physiological limits (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>). Because our E-I model does not incorporate a saturating input-output function as in<sup><xref ref-type="bibr" rid="c34">34</xref></sup> that would constrain the range of firing rates, the ability of the E-I model to maintain firing rates within biologically plausible limits emerges as a highly desirable dynamic property.</p>
<p>In summary, we found that the optimal E-I model is more efficient than the 1CT model. Beyond the performance of optimal models, the E-I model is advantageous with respect to the 1CT model also because it does not enter into states of physiologically unrealistic firing rates.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We analyzed comprehensively the structural, dynamical and coding properties that emerge in networks of spiking neurons that implement optimally the principle of efficient coding. We demonstrated that efficient recurrent E-I networks form highly accurate and unbiased representations of stimulus features with biologically plausible parameters, biologically plausible neural dynamics, instantaneous E-I balance and like-to-like lateral inhibition. The network can implement efficient coding with stimulus features varying over a wide range of timescales and when encoding even multiple such features. Here we discussed the implications of these findings.</p>
<p>By a systematic study of the model, we determined the model parameters that optimize network efficiency. Strikingly, the optimal parameters (including the ratio between the number of E and I neurons, the ratio of I-I to E-I synaptic efficacy and parameters of non-specific currents) were consistent with parameters measured empirically in cortical circuits, and generated plausible spiking dynamics. This result lends credibility to the hypothesis that cortical networks might be designed for efficient coding and may operate close to optimal efficiency, as well as provides a solid intuition about what specific parameter ranges (e.g. higher numbers of E and than I neurons) may be good for. Efficient networks still exhibited realistic dynamics and reasonably efficient coding in the presence of moderate deviations from the optimal parameters, suggesting that the optimal operational point of such networks is relatively robust. We also found that optimally efficient analytical solution derives generalized LIF (gLIF) equations for neuron models<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. While gLIF<sup><xref ref-type="bibr" rid="c67">67</xref>,<xref ref-type="bibr" rid="c40">40</xref></sup> and LIF<sup><xref ref-type="bibr" rid="c63">63</xref>,<xref ref-type="bibr" rid="c64">64</xref></sup> models are reasonably biologically plausible and are widely used to model and study spiking neural network dynamics, it was unclear how their parameters affect network-level information coding. Our study provides a principled way to determine uniquely the parameter values of gLIF networks that are optimal for efficient information encoding. Studying the dynamics of gLIF networks with such optimal parameters thus provides a direct link between optimal coding and neural dynamics. Moreover, our formalism provides a framework for the optimization of neural parameters that can in principles be used not only for neural network models that study brain function but also for the design of artificial neuromorphic circuits that perform information coding computations<sup><xref ref-type="bibr" rid="c76">76</xref>,<xref ref-type="bibr" rid="c77">77</xref></sup>.</p>
<p>Unlike in previous randomly-connected recurrent networks of LIF and gLIF spiking neurons,<sup><xref ref-type="bibr" rid="c63">63</xref>,<xref ref-type="bibr" rid="c64">64</xref></sup> in our efficient-coding solution, a highly structured E-I, I-I and I-E synaptic connectivity emerges as an optimal structural solution to support efficient coding. Our model generates a number of insights about the role of structured connectivity in efficient information processing. A first insight is that I neurons develop stimulus feature selectivity because of the structured recurrent connectivity. This is in line with recent reports of stimulus feature selectivity of inhibitory neurons, including in primary visual cortex<sup><xref ref-type="bibr" rid="c78">78</xref>,<xref ref-type="bibr" rid="c79">79</xref>,<xref ref-type="bibr" rid="c80">80</xref></sup>. A second insight is that a network with structured connectivity shows stronger average and instantaneous E-I balance, as well as significantly lower variance in membrane potentials compared to an equivalent network with the same connections organized randomly. This implies that the connectivity structure determines the operating regime of the network. In particular, a network structured as in our efficient coding solution operates in a dynamical regime that is more stimulus-driven, compared to an unstructured network that is more fluctuation driven. A third insight is that the structured network exhibits a several-fold lower encoding error compared to unstructured networks and achieves this precision with lower firing rates. Network with structured recurrent connectivity creates more precise representations with less spikes and is therefore significantly more efficient compared to unstructured networks. Our analysis of the effective connectivity created by the efficient connectivity structure shows that this structure sharpens stimulus representations, reduces redundancy and increases metabolic efficiency by implementing feature-specific competition, that is a negative effective connectivity between E neurons with similar stimulus tuning, as proposed by recent theories<sup><xref ref-type="bibr" rid="c30">30</xref></sup> and experiments <sup><xref ref-type="bibr" rid="c61">61</xref>,<xref ref-type="bibr" rid="c62">62</xref></sup> of computations in visual cortex.</p>
<p>Our perturbation experiments on single E neurons predict a negative like-to-like effective connectivity between E neurons with similar tuning, as found experimentally in the mouse primary visual cortex with 2-photon optogenetic perturbations of E neurons<sup><xref ref-type="bibr" rid="c61">61</xref>,<xref ref-type="bibr" rid="c62">62</xref></sup>. This suggests that the effective connectivity found in mouse visual cortex could reflect efficient coding in visual cortex. Comparing effective connectivity in models and experiments is also useful for ruling in and out different theories of how efficient coding may be implemented in primary visual cortex. Earlier theories<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c11">11</xref></sup> found evidence for efficient coding in visual cortex and proposed that such efficient computations relied only on feedforward connectivity; thus they predicted null effective connectivity between visual neurons and were ruled out by the empirical effective connectivity measures<sup><xref ref-type="bibr" rid="c61">61</xref>,<xref ref-type="bibr" rid="c62">62</xref></sup>. Our model, instead, implements efficient coding with recurrent interactions, suggesting a mechanism that is compatible with these empirical measures. Importantly, we made predictions for further optogenetics experiments that could better constraints models of visual cortical efficient coding. Previous studies<sup><xref ref-type="bibr" rid="c61">61</xref></sup> optogenetically stimulated E neurons but did not determine whether the recorded neurons where excitatory or inhibitory. Our model predicts that stimulation of E neurons would increase firing in similarly tuned I neurons and decrease firing in similarly tuned E neurons. Our analysis confirms earlier model predictions<sup><xref ref-type="bibr" rid="c81">81</xref></sup> that like-to-like connectivity between E and I neurons is necessary for lateral inhibition and competition between E neurons. Beyond like-to-like connectivity, our model predicts an optimally efficient connectivity where synaptic strength positively correlates with pair-wise tuning similarity, a connectivity pattern that was recently observed experimentally <sup><xref ref-type="bibr" rid="c82">82</xref></sup>.</p>
<p>Our study determines how structured E-I connectivity affects the dynamics of E-I balancing and how this relates to information coding. Previous work<sup><xref ref-type="bibr" rid="c32">32</xref></sup> proposed that the E-I balance in efficient spiking networks operates on a finer time scale than in classical balanced E-I networks with random connectivity<sup><xref ref-type="bibr" rid="c64">64</xref></sup>. However, a theory to determine the exact levels of instantaneous E-I balance that is optimal for coding was lacking. Consistent with the general idea put forth in<sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup>, we here showed that moderate levels of E-I balance are optimal for coding, and that too strong levels of instantaneous E-I balance are detrimental to coding efficiency. Our results predict that like-to-like structured E-I-E connectivity is necessary for optimal levels of temporal E-I balance. Finally, the E-I-E structured connectivity that we derived supports optimal levels of instantaneous E-I balance and causes desynchronization of the spiking output. Such intrinsically generated desynchronization is a desirable network property that in previously proposed models could only be achieved by the less plausible addition of strong noise to each neuron<sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>.</p>
<p>We found that our efficient network, optimizing the representation of a leaky integration of stimulus features, does not require recurrent E-E connections. Supporting this prediction, recurrent E-E connections were reported to be sparse in primary visual cortex<sup><xref ref-type="bibr" rid="c83">83</xref></sup>), and the majority of E-E synapses in the visual cortex were suggested to be long-range<sup><xref ref-type="bibr" rid="c84">84</xref></sup>. However, future studies could address the role of recurrent excitatory synapses, that were shown to emerge in efficient coding networks implementing computations beyond leaky integration such as linear mixing of features<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. Efficient networks with E-E connectivity show neural dynamics that goes well beyond the canonical case analyzed here and can potentially describe persistent network dynamics<sup><xref ref-type="bibr" rid="c44">44</xref></sup>. Such networks would also allow to address whether biologically plausible efficient networks exhibit criticality, as suggested by<sup><xref ref-type="bibr" rid="c85">85</xref></sup>. Finally, we note that efficient encoding might be the primary normative objective in sensory areas, while areas supporting high-level cognitive tasks such as decision-making might include other computational objectives such as efficient transmission of information downstream to generate reliable behavioral outputs<sup><xref ref-type="bibr" rid="c86">86</xref>,<xref ref-type="bibr" rid="c87">87</xref>,<xref ref-type="bibr" rid="c88">88</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>V.K. and T.S. thank Tatiana Engel for her contribution to the discussion of results and for her comments on an earlier version of the manuscript. This project was supported by funding from Technische Universität Berlin (“Equal Opportunity Program” to VK), by Internal Research Funding of Technische Universität Berlin (to TS), by NIH Brain Initiative (grants U19 NS107464, R01 NS109961, R01 NS108410 to SP), and the Simons Foundation for Autism Research Initiative (SFARI; grant 982347 to SP).</p>
</ack>
<sec id="s4">
<title>Code availability</title>
<p>The complete computer code for reproducing the results is available as a Github repository [will be shared upon acceptance].</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name>, <string-name><surname>DePasquale</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Memmesheimer</surname>, <given-names>R.-M.</given-names></string-name> <article-title>Building functional networks of spiking model neurons</article-title>. <source>Nature neuroscience</source> <volume>19</volume>, <fpage>350</fpage>–<lpage>355</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Thalmeier</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Uhlmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kappen</surname>, <given-names>H. J.</given-names></string-name> &amp; <string-name><surname>Memmesheimer</surname>, <given-names>R.-M.</given-names></string-name> <article-title>Learning universal computations with spikes</article-title>. <source>PLoS computational biology</source> <volume>12</volume>, <fpage>e1004895</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Barlow</surname>, <given-names>H. B.</given-names></string-name> <etal>et al.</etal> <article-title>Possible principles underlying the transformation of sensory messages</article-title>. <source>Sensory communication</source> <volume>1</volume>, <fpage>217</fpage>–<lpage>233</lpage> (<year>1961</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> &amp; <string-name><surname>Field</surname>, <given-names>D. J.</given-names></string-name> <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source> <volume>381</volume>, <fpage>607</fpage>–<lpage>609</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Chalk</surname>, <given-names>M.</given-names></string-name> <article-title>Efficiency turns the table on neural encoding, decoding and noise</article-title>. <source>Current Opinion in Neurobiology</source> <volume>37</volume>, <fpage>141</fpage>–<lpage>148</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name> &amp; <string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> <article-title>Natural image statistics and neural representation</article-title>. <source>Annual review of neuroscience</source> <volume>24</volume>, <fpage>1193</fpage>–<lpage>1216</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> &amp; <string-name><surname>Field</surname>, <given-names>D. J.</given-names></string-name> <article-title>Sparse coding with an overcomplete basis set: A strategy employed by v1?</article-title> <source>Vision research</source> <volume>37</volume>, <fpage>3311</fpage>–<lpage>3325</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Vinje</surname>, <given-names>W. E.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name> <article-title>Sparse coding and decorrelation in primary visual cortex during natural vision</article-title>. <source>Science</source> <volume>287</volume>, <fpage>1273</fpage>–<lpage>1276</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="book"><string-name><surname>Li</surname>, <given-names>Z.</given-names></string-name> <source>Understanding vision: theory, models, and data</source> (<publisher-name>Oxford University Press</publisher-name>, <publisher-loc>USA</publisher-loc>, <year>2014</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Atick</surname>, <given-names>J. J.</given-names></string-name> <article-title>Could information theory provide an ecological theory of sensory processing?</article-title> <source>Network: Computation in neural systems</source> <volume>3</volume>, <fpage>213</fpage>–<lpage>251</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> &amp; <string-name><surname>Field</surname>, <given-names>D. J.</given-names></string-name> <article-title>Sparse coding of sensory inputs</article-title>. <source>Current opinion in neurobiology</source> <volume>14</volume>, <fpage>481</fpage>–<lpage>487</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Lewicki</surname>, <given-names>M. S.</given-names></string-name> <article-title>Efficient coding of natural sounds</article-title>. <source>Nature neuroscience</source> <volume>5</volume>, <fpage>356</fpage>–<lpage>363</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Koulakov</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Rinberg</surname>, <given-names>D.</given-names></string-name> <article-title>Sparse incomplete representations: A potential role of olfactory granule cells</article-title>. <source>Neuron</source> <volume>72</volume>, <fpage>124</fpage>–<lpage>136</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Rieke</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>de Ruyter van Steveninck</surname>, <given-names>R. R.</given-names></string-name> &amp; <string-name><surname>Warland</surname>, <given-names>D.</given-names></string-name> <article-title>Reading a neural code</article-title>. <source>Science</source> <volume>252</volume>, <fpage>1854</fpage>–<lpage>1857</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Rieke</surname>, <given-names>F.</given-names></string-name> <article-title>Reliability and information transmission in spiking neurons</article-title>. <source>Trends in neurosciences</source> <volume>15</volume>, <fpage>428</fpage>–<lpage>434</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Schultz</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Lebedev</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Diamond</surname>, <given-names>M. E.</given-names></string-name> <article-title>The role of spike timing in the coding of stimulus location in rat somatosensory cortex</article-title>. <source>Neuron</source> <volume>29</volume>, <fpage>769</fpage>–<lpage>777</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Nemenman</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Lewen</surname>, <given-names>G. D.</given-names></string-name>, <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>de Ruyter van Steveninck</surname>, <given-names>R. R.</given-names></string-name> <article-title>Neural coding of natural stimuli: information at sub-millisecond resolution</article-title>. <source>PLoS computational biology</source> <volume>4</volume>, <fpage>e1000025</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> &amp; <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name> <article-title>Millisecond encoding precision of auditory cortex neurons</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>107</volume>, <fpage>16976</fpage>–<lpage>16981</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Ince</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name> <article-title>Neural codes formed by small and temporally precise populations in auditory cortex</article-title>. <source>Journal of Neuroscience</source> <volume>33</volume>, <fpage>18277</fpage>–<lpage>18287</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> &amp; <string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name> <article-title>Sensory neural codes using multiplexed temporal scales</article-title>. <source>Trends in neurosciences</source> <volume>33</volume>, <fpage>111</fpage>–<lpage>120</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Fairhall</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Lewen</surname>, <given-names>G. D.</given-names></string-name>, <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>de Ruyter van Steveninck</surname>, <given-names>R. R.</given-names></string-name> <article-title>Efficiency and ambiguity in an adaptive neural code</article-title>. <source>Nature</source> <volume>412</volume>, <fpage>787</fpage>–<lpage>792</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Wark</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Fairhall</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Rieke</surname>, <given-names>F.</given-names></string-name> <article-title>Timescales of inference in visual adaptation</article-title>. <source>Neuron</source> <volume>61</volume>, <fpage>750</fpage>–<lpage>761</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Mazzoni</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> &amp; <string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name> <article-title>Encoding of naturalistic stimuli by local field potential spectra in networks of excitatory and inhibitory neurons</article-title>. <source>PLoS computational biology</source> <volume>4</volume>, <fpage>e1000239</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Mlynarski</surname>, <given-names>W. F.</given-names></string-name> &amp; <string-name><surname>Hermundstadt</surname>, <given-names>A. M.</given-names></string-name> <article-title>Efficient andadaptive sensory codes</article-title>. <source>Nature Neuroscience</source> <volume>24</volume>, <fpage>998</fpage>–<lpage>1009</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Koren</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Bondanelli</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name> <article-title>Computational methods to study information processing in neural circuits</article-title>. <source>Computational and Structural Biotechnology Journal</source> <volume>21</volume>, <fpage>910</fpage>–<lpage>922</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Lochmann</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Ernst</surname>, <given-names>U. A.</given-names></string-name> &amp; <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name> <article-title>Perceptual inference predicts contextual modulations of sensory responses</article-title>. <source>Journal of Neuroscience</source> <volume>32</volume>, <fpage>4179</fpage>–<lpage>4195</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Zhu</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Rozell</surname>, <given-names>C. J.</given-names></string-name> <article-title>Visual nonclassical receptive field effects emerge from sparse coding in a dynamical system</article-title>. <source>PLoS computational biology</source> <volume>9</volume>, <fpage>e1003191</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Boerlin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name> &amp; <string-name><surname>Denève</surname>, <given-names>S.</given-names></string-name> <article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>, <fpage>e1003258</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Bourdoukan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Barrett</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name> <article-title>Learning optimal spike-based representations</article-title>. <source>Advances in neural information processing systems</source> <volume>25</volume>, <fpage>2285</fpage>–<lpage>2293</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Moreno-Bote</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Drugowitsch</surname>, <given-names>J.</given-names></string-name> <article-title>Causal inference and explaining away in a spiking network</article-title>. <source>Scientific Reports</source> <volume>5</volume>, <fpage>17531</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Chalk</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gutkin</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name> <article-title>Neural oscillations as a signature of efficient coding in the presence of synaptic delays</article-title>. <source>Elife</source> <volume>5</volume>, <fpage>e13824</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Denève</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name> <article-title>Efficient codes and balanced networks</article-title>. <source>Nature neuroscience</source> <volume>19</volume>, <fpage>375</fpage>–<lpage>382</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Gutierrez</surname>, <given-names>G. J.</given-names></string-name> &amp; <string-name><surname>Denève</surname>, <given-names>S.</given-names></string-name> <article-title>Population adaptation in efficient balanced networks</article-title>. <source>Elife</source> <volume>8</volume>, <fpage>e46926</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="book"><string-name><surname>Kadmon</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Timcheck</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name> <chapter-title>Predictive coding in balanced neural networks with noise, chaos and delays</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Larochelle</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ranzato</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hadsell</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Balcan</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Lin</surname>, <given-names>H</given-names></string-name></person-group>. (eds.) <source>Advances in Neural Information Processing Systems</source>, vol. <volume>33</volume>, <fpage>16677</fpage>–<lpage>16688</lpage> (<publisher-name>Curran Associates, Inc</publisher-name>., <year>2020</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Buxó</surname>, <given-names>C. E. R.</given-names></string-name> &amp; <string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name> <article-title>Poisson balanced spiking networks</article-title>. <source>PLoS computational biology</source> <volume>16</volume>, <fpage>e1008261</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Koren</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Denève</surname>, <given-names>S.</given-names></string-name> <article-title>Computational account of spontaneous activity as a signature of predictive coding</article-title>. <source>PLoS computational biology</source> <volume>13</volume>, <fpage>e1005355</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="book"><string-name><surname>Koren</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name> <chapter-title>Biologically plausible solutions for spiking networks with efficient coding</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Koyejo</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> (eds.) <source>Advances in Neural Information Processing Systems</source>, vol. <volume>35</volume>, <fpage>20607</fpage>–<lpage>20620</lpage> (<publisher-name>Curran Associates, Inc</publisher-name>., <year>2022</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Brette</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name> <article-title>Adaptive exponential integrate-and-fire model as an effective description of neuronal activity</article-title>. <source>Journal of neurophysiology</source> <volume>94</volume>, <fpage>3637</fpage>–<lpage>3642</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Mensi</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Parameter extraction and classification of three cortical neuron types reveals two distinct adaptation mechanisms</article-title>. <source>Journal of neurophysiology</source> <volume>107</volume>, <fpage>1756</fpage>–<lpage>1775</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="book"><string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Kistler</surname>, <given-names>W. M.</given-names></string-name>, <string-name><surname>Naud</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Paninski</surname>, <given-names>L.</given-names></string-name> <source>Neuronal dynamics: From single neurons to networks and models of cognition</source> (<publisher-name>Cambridge University Press</publisher-name>, <year>2014</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Jolivet</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal> <article-title>The quantitative single-neuron modeling competition</article-title>. <source>Biological cybernetics</source> <volume>99</volume>, <fpage>417</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Brendel</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Bourdoukan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Vertechi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name> &amp; <string-name><surname>Denéve</surname>, <given-names>S.</given-names></string-name> <article-title>Learning to represent signals spike by spike</article-title>. <source>PLoS computational biology</source> <volume>16</volume>, <fpage>e1007692</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Barrett</surname>, <given-names>D. G.</given-names></string-name>, <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name> <article-title>Optimal compensation for neuron loss</article-title>. <source>Elife</source> <volume>5</volume>, <fpage>e12454</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="confproc"><string-name><surname>Alemi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Machens</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Slotine</surname>, <given-names>J.-J.</given-names></string-name> <article-title>Learning nonlinear dynamics in efficient, balanced spiking networks using local plasticity rules</article-title>. <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>, vol. <volume>32</volume>, doi: <pub-id pub-id-type="doi">10.1609/aaai.v32i1.11320</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Vogels</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Sprekeler</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Zenke</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name> <article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title>. <source>Science</source> <volume>334</volume>, <fpage>1569</fpage>–<lpage>1573</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Amatrudo</surname>, <given-names>J. M.</given-names></string-name> <etal>et al.</etal> <article-title>Influence of highly distinctive structural properties on the excitability of pyramidal neurons in monkey visual and prefrontal cortices</article-title>. <source>Journal of Neuroscience</source> <volume>32</volume>, <fpage>13644</fpage>–<lpage>13660</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Rigotti</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>The importance of mixed selectivity in complex cognitive tasks</article-title>. <source>Nature</source> <volume>497</volume>, <fpage>585</fpage>–<lpage>590</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Denève</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Alemi</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Bourdoukan</surname>, <given-names>R.</given-names></string-name> <article-title>The brain as an efficient and robust adaptive learner</article-title>. <source>Neuron</source> <volume>94</volume>, <fpage>969</fpage>–<lpage>977</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Tavoni</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Balasubramanian</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Gold</surname>, <given-names>J. I.</given-names></string-name> <article-title>What is optimal in optimal inference?</article-title> <source>Current Opinion in Behavioral Sciences</source> <volume>29</volume>, <fpage>117</fpage>–<lpage>126</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Faisal</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Selen</surname>, <given-names>L. P.</given-names></string-name> &amp; <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name> <article-title>Noise in the nervous system</article-title>. <source>Nature reviews neuroscience</source> <volume>9</volume>, <fpage>292</fpage>–<lpage>303</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Ko</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal> <article-title>Functional specificity of local synaptic connections in neocortical networks</article-title>. <source>Nature</source> <volume>473</volume>, <fpage>87</fpage>–<lpage>91</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Pala</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Petersen</surname>, <given-names>C.</given-names></string-name> <article-title>In-vivo measurement of cell-type-specific synaptic connectivity and synaptic transmission in layer 2/3 mouse barrel cortex</article-title>. <source>Neuron</source> <volume>85</volume>, <fpage>68</fpage>–<lpage>75</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Campagnola</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal> <article-title>Local connectivity and synaptic dynamics in mouse and human neocortex</article-title>. <source>Science</source> <volume>375</volume>, <fpage>eabj5861</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Burkitt</surname>, <given-names>A. N.</given-names></string-name> <article-title>A review of the integrate-and-fire neuron model: I. homogeneous synaptic input</article-title>. <source>Biological cybernetics</source> <volume>95</volume>, <fpage>1</fpage>–<lpage>19</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Schwalger</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Deger</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name> <article-title>Towards a theory of cortical columns: From spiking neurons to interacting neural populations of finite size</article-title>. <source>PLoS Comput. Biol</source>. <volume>13</volume>, <fpage>e1005507</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="book"><string-name><surname>Harkin</surname>, <given-names>E. F.</given-names></string-name>, <string-name><surname>Béïque</surname>, <given-names>J.-C.</given-names></string-name> &amp; <string-name><surname>Naud</surname>, <given-names>R.</given-names></string-name> <chapter-title>A user’s guide to generalized integrate-and-fire models</chapter-title>. <source>Computational Modelling of the Brain: Modelling Approaches to Cells, Circuits and Networks</source>, <fpage>69</fpage>–<lpage>86</lpage> (<publisher-name>Springer</publisher-name>, <year>2021</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Lefort</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tomm</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sarria</surname>, <given-names>J.-C. F.</given-names></string-name> &amp; <string-name><surname>Petersen</surname>, <given-names>C. C.</given-names></string-name> <article-title>The excitatory neuronal network of the C2 barrel column in mouse primary somatosensory cortex</article-title>. <source>Neuron</source> <volume>61</volume>, <fpage>301</fpage>–<lpage>316</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><surname>Ahmadian</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Miller</surname>, <given-names>K. D.</given-names></string-name> <article-title>What is the dynamical regime of cerebral cortex?</article-title> <source>Neuron</source> <volume>109</volume>, <fpage>3373</fpage>–<lpage>3391</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>Okun</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Lampl</surname>, <given-names>I.</given-names></string-name> <article-title>Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities</article-title>. <source>Nature neuroscience</source> <volume>11</volume>, <fpage>535</fpage>–<lpage>537</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><string-name><surname>Xue</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Atallah</surname>, <given-names>B. V.</given-names></string-name> &amp; <string-name><surname>Scanziani</surname>, <given-names>M.</given-names></string-name> <article-title>Equalizing excitation–inhibition ratios across visual cortical neurons</article-title>. <source>Nature</source> <volume>511</volume>, <fpage>596</fpage>–<lpage>600</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><string-name><surname>Chettih</surname>, <given-names>S. N.</given-names></string-name> &amp; <string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name> <article-title>Single-neuron perturbations reveal feature-specific competition in V1</article-title>. <source>Nature</source> <volume>567</volume>, <fpage>334</fpage>–<lpage>340</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><string-name><surname>Oldenburg</surname>, <given-names>I. A.</given-names></string-name> <etal>et al.</etal> <article-title>The logic of recurrent circuits in the primary visual cortex</article-title>. <source>Nature Neuroscience</source> <volume>27</volume>, <fpage>1</fpage>–<lpage>11</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name> <article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>Journal of computational neuroscience</source> <volume>8</volume>, <fpage>183</fpage>–<lpage>208</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><string-name><surname>Renart</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title>The asynchronous state in cortical circuits</article-title>. <source>Science</source> <volume>327</volume>, <fpage>587</fpage>–<lpage>590</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name> &amp; <string-name><surname>Nelson</surname>, <given-names>S. B.</given-names></string-name> <article-title>Synaptic plasticity: taming the beast</article-title>. <source>Nature neuroscience</source> <volume>3</volume>, <fpage>1178</fpage>–<lpage>1183</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name> &amp; <string-name><surname>Nelson</surname>, <given-names>S. B.</given-names></string-name> <article-title>Homeostatic plasticity in the developing nervous system</article-title>. <source>Nature reviews neuroscience</source> <volume>5</volume>, <fpage>97</fpage>–<lpage>107</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><surname>Schwalger</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Lindner</surname>, <given-names>B.</given-names></string-name> <article-title>Patterns of interval correlations in neural oscillators with adaptation</article-title>. <source>Front. Comput. Neurosci</source>. <volume>7</volume>, <fpage>164</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><string-name><surname>Levy</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>MacLean</surname>, <given-names>J. N.</given-names></string-name> <article-title>Network analysis of murine cortical dynamics implicates untuned neurons in visual stimulus coding</article-title>. <source>Cell Reports</source> <volume>31</volume>, <fpage>107483</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="preprint"><string-name><surname>Zylberberg</surname>, <given-names>J.</given-names></string-name> <article-title>The role of untuned neurons in sensory information coding</article-title>. <source>BioRxiv</source> <volume>134379</volume> (<year>2017</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><string-name><surname>Destexhe</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Paré</surname>, <given-names>D.</given-names></string-name> <article-title>Impact of network activity on the integrative properties of neocortical pyramidal neurons in vivo</article-title>. <source>Journal of neurophysiology</source> <volume>81</volume>, <fpage>1531</fpage>–<lpage>1547</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><string-name><surname>Destexhe</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rudolph</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Paré</surname>, <given-names>D.</given-names></string-name> <article-title>The high-conductance state of neocortical neurons in vivo</article-title>. <source>Nature reviews neuroscience</source> <volume>4</volume>, <fpage>739</fpage>–<lpage>751</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><string-name><surname>Van Vreeswijk</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Sompolinsky</surname>, <given-names>H.</given-names></string-name> <article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title>. <source>Science</source> <volume>274</volume>, <fpage>1724</fpage>–<lpage>1726</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><string-name><surname>Sukenik</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal> <article-title>Neuronal circuits overcome imbalance in excitation and inhibition by adjusting connection numbers</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>118</volume>, <fpage>e2018459118</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><string-name><surname>Markram</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal> <article-title>Interneurons of the neocortical inhibitory system</article-title>. <source>Nature reviews neuroscience</source> <volume>5</volume>, <fpage>793</fpage>–<lpage>807</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><string-name><surname>Cossell</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal> <article-title>Functional organization of excitatory synaptic strength in primary visual cortex</article-title>. <source>Nature</source> <volume>518</volume>, <fpage>399</fpage>–<lpage>403</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><string-name><surname>Roy</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jaiswal</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Panda</surname>, <given-names>P.</given-names></string-name> <article-title>Towards spike-based machine intelligence with neuromorphic computing</article-title>. <source>Nature</source> <volume>575</volume>, <fpage>607</fpage>–<lpage>617</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><string-name><surname>Schuman</surname>, <given-names>C. D.</given-names></string-name> <etal>et al.</etal> <article-title>Opportunities for neuromorphic computing algorithms and applications</article-title>. <source>Nature Computational Science</source> <volume>2</volume>, <fpage>10</fpage>–<lpage>19</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><string-name><surname>Najafi</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal> <article-title>Excitatory and inhibitory subnetworks are equally selective during decision-making and emerge simultaneously during learning</article-title>. <source>Neuron</source> <volume>105</volume>, <fpage>165</fpage>–<lpage>179</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><string-name><surname>Runyan</surname>, <given-names>C. A.</given-names></string-name> <etal>et al.</etal> <article-title>Response features of parvalbumin-expressing interneurons suggest precise roles for subtypes of inhibition in visual cortex</article-title>. <source>Neuron</source> <volume>67</volume>, <fpage>847</fpage>–<lpage>857</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><string-name><surname>Kuan</surname>, <given-names>A. T.</given-names></string-name> <etal>et al.</etal> <article-title>Synaptic wiring motifs in posterior parietal cortex support decision-making</article-title>. <source>Nature</source> <volume>627</volume>, <fpage>367</fpage>–<lpage>373</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><string-name><surname>Sadeh</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name> <article-title>Theory of neuronal perturbome in cortical networks</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>117</volume>, <fpage>26966</fpage>–<lpage>26976</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><string-name><surname>Znamenskiy</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title>Functional specificity of recurrent inhibition in visual cortex</article-title>. <source>Neuron</source> <volume>112</volume>, <fpage>991</fpage>–<lpage>1000</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><string-name><surname>Seeman</surname>, <given-names>S. C.</given-names></string-name> <etal>et al.</etal> <article-title>Sparse recurrent excitatory connectivity in the microcircuit of the adult mouse and human cortex</article-title>. <source>Elife</source> <volume>7</volume>, <fpage>e37349</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><string-name><surname>Stepanyants</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Martinez</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Ferecskó</surname>, <given-names>A. S.</given-names></string-name> &amp; <string-name><surname>Kisvárday</surname>, <given-names>Z. F.</given-names></string-name> <article-title>The fractions of short-and long-range connections in the visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>106</volume>, <fpage>3555</fpage>–<lpage>3560</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="preprint"><string-name><surname>Safavi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Chalk</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Logothetis</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Levina</surname>, <given-names>A.</given-names></string-name> <article-title>Signatures of criticality in efficient coding networks</article-title>. <source>bioRxiv</source> <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2023/02/14/2023.02.14.528465">https://www.biorxiv.org/content/early/2023/02/14/2023.02.14.528465</ext-link> (<year>2023</year>).</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><string-name><surname>Valente</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Correlations enhance the behavioral readout of neural population activity in association cortex</article-title>. <source>Nature neuroscience</source> <volume>24</volume>, <fpage>975</fpage>–<lpage>986</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="journal"><string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Moroni</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Safaai</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name> <article-title>The structures and functions of correlations in neural population codes</article-title>. <source>Nature Reviews Neuroscience</source> <volume>23</volume>, <fpage>551</fpage>–<lpage>567</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="journal"><string-name><surname>Manning</surname>, <given-names>T. S.</given-names></string-name> <etal>et al.</etal> <article-title>Transformations of sensory information in the brain suggest changing criteria for optimality</article-title>. <source>PLOS Computational Biology</source> <volume>20</volume>, <fpage>e1011783</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="book"><string-name><surname>Whittaker</surname>, <given-names>V.</given-names></string-name> <chapter-title>What is Dale’s principle</chapter-title>? In <person-group person-group-type="editor"><string-name><surname>Osborne</surname>, <given-names>N. N.</given-names></string-name></person-group> (ed.) <source>Dale’s Principle and Communication Between Neurones</source>, <fpage>1</fpage>–<lpage>5</lpage> (<publisher-loc>Pergamon</publisher-loc>, <year>1983</year>).</mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="journal"><string-name><surname>Muller</surname>, <given-names>M. E.</given-names></string-name> <article-title>A note on a method for generating points uniformly on n-dimensional spheres</article-title>. <source>Communications of the ACM</source> <volume>2</volume>, <fpage>19</fpage>–<lpage>20</lpage> (<year>1959</year>).</mixed-citation></ref>
<ref id="c91"><label>91.</label><mixed-citation publication-type="confproc"><string-name><surname>Luo</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal> <article-title>Cosine normalization: Using cosine similarity instead of dot product in neural networks</article-title>. <source>Artificial Neural Networks and Machine Learning–ICANN 2018: 27th International Conference on Artificial Neural Networks</source>, vol. I <volume>27</volume>, <fpage>382</fpage>–<lpage>391</lpage> (<publisher-name>Springer</publisher-name>, <year>2018</year>).</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<title>Overview of the current approach and of differences with previous approaches</title>
<p>In the following, we present a detailed derivation of the E-I spiking network implementing the efficient coding principle. The analytical derivation is based on previous works on efficient coding with spikes<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>, and in particular on our recent work<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. While these previous works analytically derived feedforward and recurrent transmembrane currents in leaky integrate-and fire neuron models, these models did not contain any synaptic current unrelated to feedforward and recurrent processing. Non-specific synaptic current was suggested to be important for an accurate description of coding and dynamics in cortical networks<sup><xref ref-type="bibr" rid="c71">71</xref></sup>. In the model derivation that follows, we also derived non-specific external current from efficiency objectives.</p>
<p>As we mapped the efficient coding objective on biologically plausible neural implementations, we found that such implementations (with plausible biophysical parameters) requires a transmembrane current that is independent of feedforward and recurrent processing. We interpreted this current as non-specific external current (shortly, non-specific current), collating the ensemble of synaptic projections from other brain areas that are not directly involved in processing of feedforward stimulus features<sup><xref ref-type="bibr" rid="c70">70</xref></sup>, as well as synaptic inputs from the local network from neurons that are not tuned to feedforward stimulus features<sup><xref ref-type="bibr" rid="c69">69</xref></sup>. The mechanistic effect of the non-specific current is to regulate the distance to firing threshold, a role that is close to the notion of “background” synaptic activity in cortical neurons<sup><xref ref-type="bibr" rid="c71">71</xref></sup>.</p>
<p>Moreover, previous models on efficient coding did not thoroughly consider physical units of variables that were interpreted as biophysical quantities (such as membrane potentials, firing thresholds, etc.). As these biophysical variables were derived from computational variables (such as target signals and population readouts), it remained unclear how biophysical variables might acquire their physical units. Here, we assigned physical units to the computational variables and thus naturally endowed the model with physical units. The network developed here allows for a better compatibility of efficient spiking models with neurobiology compared to previous works on efficient coding with spikes. With this model, we aim to describe neural dynamics and computation in early sensory cortices such as the primary visual cortex in rodents, even though many principles of the model developed here could be relevant throughout the brain.</p>
</sec>
<sec id="s5b">
<title>Introducing variables of the model</title>
<p>We consider two types of neurons, excitatory neurons <italic>E</italic> and inhibitory neurons <italic>I</italic>. We denote as <italic>N</italic><sup><italic>E</italic></sup> and <italic>N</italic><sup><italic>I</italic></sup> the number of <italic>E</italic>-cells and <italic>I</italic>-cells, respectively. The spike train of neuron <italic>i</italic> of type <italic>y</italic> ∈ {<italic>E, I</italic>}, <italic>i</italic> = 1, 2, …, <italic>N</italic><sup><italic>y</italic></sup>, is defined as a sum of Dirac delta functions,
<disp-formula id="eqn6">
<graphic xlink:href="590955v1_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v1_inline35.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the time of the <italic>α</italic>-th spike of that neuron, defined as a time point at which the membrane potential of neuron <italic>i</italic> crosses the firing threshold.</p>
<p>We define the readout of the spiking activity of neuron <italic>i</italic> of type <italic>y</italic> (in the following, “single neuron readout”) as a leaky integration of its spike train,
<disp-formula id="eqn7">
<graphic xlink:href="590955v1_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>λ</italic><sub><italic>r</italic></sub> denoting the inverse time constant. This way, the quantity <inline-formula><inline-graphic xlink:href="590955v1_inline36.gif" mimetype="image" mime-subtype="gif"/></inline-formula> represents an estimate of the instantaneous firing rate of neuron <italic>i</italic>.</p>
<p>We denote as <italic>s</italic><sub><italic>k</italic></sub>(<italic>t</italic>), <italic>k</italic> = 1, 2, …, <italic>M</italic> the set of <italic>M</italic> dynamical features of the external stimulus (in the following, stimulus features) which are transmitted to the network through a feedforward sensory pathway. The stimulus features have the unit of the square root of millivolt, <inline-formula><inline-graphic xlink:href="590955v1_inline37.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The <italic>k</italic>-th dimension of the target signal is then obtained through a leaky integration of the feedforward variable, <italic>s</italic><sub><italic>k</italic></sub>(<italic>t</italic>)<sup><xref ref-type="bibr" rid="c29">29</xref></sup>, with inverse time constant <italic>λ</italic>, as
<disp-formula id="eqn8">
<graphic xlink:href="590955v1_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Furthermore, we define a linear population readout of the spiking activity of E and I neurons
<disp-formula id="eqn9">
<graphic xlink:href="590955v1_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="590955v1_inline38.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in units of <inline-formula><inline-graphic xlink:href="590955v1_inline39.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Here, each neuron <italic>i</italic> of type <italic>y</italic> is associated with a vector <inline-formula><inline-graphic xlink:href="590955v1_inline40.gif" mimetype="image" mime-subtype="gif"/></inline-formula> of <italic>M</italic> tuning parameters representing the readout weight of neuron <italic>i</italic> with respect to the <italic>M</italic> population readouts in <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref>. These readout weights can be combined in the <italic>M</italic> × <italic>N</italic><sup><italic>y</italic></sup> matrix <inline-formula><inline-graphic xlink:href="590955v1_inline41.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The rows of this matrix define the patterns of readout weights <inline-formula><inline-graphic xlink:href="590955v1_inline42.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for each signal dimension <italic>k</italic> = 1, …, <italic>M</italic>.</p>
</sec>
<sec id="s5c">
<title>Loss functions</title>
<p>We assume that the activity of a population <italic>y</italic> ∈ {<italic>E, I</italic>} is set so as to minimize a time-dependent encoding error and a time-dependent metabolic cost:
<disp-formula id="eqn10">
<graphic xlink:href="590955v1_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>β</italic><sup><italic>y</italic></sup> &gt; 0 in units of mV the Lagrange multiplier which controls the weight of the metabolic cost relative to the encoding error. The time-dependent encoding error is defined as the squared distance between the targets and their estimates, and the role of estimates is assigned to the population readouts <inline-formula><inline-graphic xlink:href="590955v1_inline43.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. In E neurons, the targets are defined as the target signals <italic>x</italic><sub><italic>k</italic></sub>(<italic>t</italic>), and their estimators are the population readouts of the spiking activity of E neurons,<inline-formula><inline-graphic xlink:href="590955v1_inline44.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. In I neurons, the targets are defined as the population readouts of E neurons <inline-formula><inline-graphic xlink:href="590955v1_inline45.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and their estimators are the population readouts of I neurons <inline-formula><inline-graphic xlink:href="590955v1_inline46.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Furthermore, the time-dependent metabolic cost is proportional to the squared estimate of the instantaneous firing rate, summed across neurons from the same population. Following these assumptions, we define the variables of loss functions in <xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref> as
<disp-formula id="eqn11">
<graphic xlink:href="590955v1_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We use a quadratic metabolic cost because it promotes the distribution of spiking across neurons<sup><xref ref-type="bibr" rid="c28">28</xref></sup>. In particular, the loss function of I neurons, L<sup><italic>I</italic></sup> (<italic>t</italic>) implies the relevance of the approximation: <inline-formula><inline-graphic xlink:href="590955v1_inline47.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (see <italic>ϵ</italic><sup><italic>I</italic></sup> in the <xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>), which will be used in what follows.</p>
</sec>
<sec id="s5d">
<title>When shall a neuron spike?</title>
<p>We minimize the loss function by positing that neuron <italic>i</italic> of type <italic>y</italic> ∈ {<italic>E, I</italic>} emits a spike as soon as its spike decreases the loss function of its population <italic>y</italic> in the immediate future<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. We also define <italic>t</italic><sup><italic>−</italic></sup> and <italic>t</italic><sup>+</sup> as the left- and right-sided limits of a spike time <inline-formula><inline-graphic xlink:href="590955v1_inline48.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, respectively. Thus, at the spike time, the following jump condition must hold:
<disp-formula id="eqn12">
<graphic xlink:href="590955v1_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="590955v1_inline49.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in units of mV. Here, the arguments <italic>t</italic><sup><italic>−</italic></sup> and <italic>t</italic><sup>+</sup> denote the left- and right-sided limits of the respected functions at time <italic>t</italic>. Furthermore, we added a noise term on the right-hand side of the <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref> in order to consider the stochastic nature of spike generation in biological networks<sup><xref ref-type="bibr" rid="c50">50</xref></sup>. A convenient choice for the noise <inline-formula><inline-graphic xlink:href="590955v1_inline50.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the Ornstein-Uhlenbeck process obeying
<disp-formula id="eqn13">
<graphic xlink:href="590955v1_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v1_inline51.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is a Gaussian white noise with auto-covariance function ⟨<italic>η</italic><sub><italic>i</italic></sub>(<italic>t</italic>)<italic>η</italic><sub><italic>j</italic></sub>(<italic>t</italic>′)⟩ = <italic>δ</italic><sub><italic>ij</italic></sub><italic>δ</italic>(<italic>t</italic> − <italic>t</italic>′). The process <inline-formula><inline-graphic xlink:href="590955v1_inline52.gif" mimetype="image" mime-subtype="gif"/></inline-formula> has zero mean and auto-covariance function <inline-formula><inline-graphic xlink:href="590955v1_inline53.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="590955v1_inline54.gif" mimetype="image" mime-subtype="gif"/></inline-formula> the variance of the noise.</p>
<p>By applying the condition for spiking in <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref> using <italic>y</italic> = <italic>E</italic> and <italic>y</italic> = <italic>I</italic>, respectively, we get
<disp-formula id="eqn14">
<graphic xlink:href="590955v1_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
According to the definitions in <xref ref-type="disp-formula" rid="eqn7">Eqs. (7)</xref> and <xref ref-type="disp-formula" rid="eqn9">(9)</xref>, if neuron <italic>i</italic> fires a spike at time <inline-formula><inline-graphic xlink:href="590955v1_inline55.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, it causes a jump of its own filtered spike train (but not of other neurons <italic>j ≠ i</italic>), as well as of the population readout of the population it belongs to. Therefore, when neuron <italic>i</italic> fires a spike, we have for a given neuron <italic>j</italic> and a given population readout <italic>k</italic>:
<disp-formula id="eqn15a">
<graphic xlink:href="590955v1_eqn15a.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn15b">
<graphic xlink:href="590955v1_eqn15b.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
By inserting <xref ref-type="disp-formula" rid="eqn15a">Eq. (15a)</xref>-<xref ref-type="disp-formula" rid="eqn15b">(15b)</xref> in <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref>, we find that neuron <italic>i</italic> of type <italic>y</italic> should fire a spike if the following condition holds:
<disp-formula id="eqn16a">
<graphic xlink:href="590955v1_eqn16a.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
These equations tell us when the neuron <italic>i</italic> of type <italic>E</italic> and <italic>I</italic>, respectively, emits a spike, and are similar to the ones derived in previous works<sup><xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>. In addition to what has been found in these previous works, we here also find that each term on the left- and right-hand side in the <xref ref-type="disp-formula" rid="eqn16a">Eq 16a</xref> has the physical units of millivolts.</p>
<p>We note that the expression derived from the minimization of the loss function of E neurons in the top row of <xref ref-type="disp-formula" rid="eqn16a">Eq. (16a)</xref> is independent of the activity of I neurons, and would thus lead to the E population being unconnected with the I population. In order to derive a recurrently connected E-I network, the activity of E neurons must depend on the activity of I neurons. We impose this property by using the approximation of estimates that holds under the assumption of efficient coding in I neurons (see <italic>ϵ</italic><sup><italic>I</italic></sup> in the <xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>),<inline-formula><inline-graphic xlink:href="590955v1_inline56.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This yields the following conditions:
<disp-formula id="eqn16b">
<graphic xlink:href="590955v1_eqn16b.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We now define new variables <inline-formula><inline-graphic xlink:href="590955v1_inline57.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline58.gif" mimetype="image" mime-subtype="gif"/></inline-formula> as proportional to the left- and the right-hand side of these expressions,
<disp-formula id="eqn17">
<graphic xlink:href="590955v1_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The variables <inline-formula><inline-graphic xlink:href="590955v1_inline59.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline60.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are interpreted as the membrane potential and the firing threshold of neuron <italic>i</italic> of cell type <italic>y</italic> ∈ {<italic>E, I</italic>}.</p>
</sec>
<sec id="s5e">
<title>Dynamic equations for the membrane potentials</title>
<p>In this section we develop the exact dynamic equations of the membrane potentials <inline-formula><inline-graphic xlink:href="590955v1_inline61.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for <italic>y</italic> ∈ {<italic>E, I</italic>} according to the efficient coding assumption. It is practical to use the vector notation and rewrite variables in <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref> as
<disp-formula id="eqn18">
<graphic xlink:href="590955v1_eqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="590955v1_inline62.gif" mimetype="image" mime-subtype="gif"/></inline-formula> the squared length of the tuning vector of neuron <italic>i</italic> of type <italic>y</italic>. We also rewrite <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>-<xref ref-type="disp-formula" rid="eqn9">(9)</xref> in vector notation as
<disp-formula id="eqn19">
<graphic xlink:href="590955v1_eqn19.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <bold><italic>x</italic></bold>(<italic>t</italic>) := [<italic>x</italic><sub>1</sub>(<italic>t</italic>), …, <italic>x</italic><sub><italic>M</italic></sub> (<italic>t</italic>)]<sup>⊤</sup> the vector of <italic>M</italic> target signals, <inline-formula><inline-graphic xlink:href="590955v1_inline63.gif" mimetype="image" mime-subtype="gif"/></inline-formula> the vector of estimates of cell type <italic>y</italic>, and <inline-formula><inline-graphic xlink:href="590955v1_inline64.gif" mimetype="image" mime-subtype="gif"/></inline-formula> the vector of spike trains for <italic>N</italic><sup><italic>y</italic></sup> cell type <italic>y</italic> ∈ {<italic>E, I</italic>}.</p>
<p>In the case of E neurons, the time-derivative of the membrane potential <inline-formula><inline-graphic xlink:href="590955v1_inline65.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>, is obtained as
<disp-formula id="eqn20">
<graphic xlink:href="590955v1_eqn20.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
By inserting the dynamic equations of the target signal <inline-formula><inline-graphic xlink:href="590955v1_inline66.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, its estimate <inline-formula><inline-graphic xlink:href="590955v1_inline67.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn19">Eq. 19</xref>) and of the single neuron readout <inline-formula><inline-graphic xlink:href="590955v1_inline68.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn7">Eq. 7</xref> in the case <italic>y</italic> = <italic>E</italic>), we get
<disp-formula id="eqn21">
<graphic xlink:href="590955v1_eqn21.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where in the last line we used the definition of <inline-formula><inline-graphic xlink:href="590955v1_inline69.gif" mimetype="image" mime-subtype="gif"/></inline-formula> from the <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>.</p>
<p>In the case of I neurons, the time derivative of the membrane potential <inline-formula><inline-graphic xlink:href="590955v1_inline70.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref> is
<disp-formula id="eqn22">
<graphic xlink:href="590955v1_eqn22.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
By inserting the dynamic equations of the population readouts of E neurons <inline-formula><inline-graphic xlink:href="590955v1_inline71.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and of the I neurons <inline-formula><inline-graphic xlink:href="590955v1_inline72.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn19">Eq. 19</xref>) and of the single neuron readout <inline-formula><inline-graphic xlink:href="590955v1_inline73.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn7">Eq. 7</xref> in the case <italic>y</italic> = <italic>I</italic>), we get
<disp-formula id="eqn23">
<graphic xlink:href="590955v1_eqn23.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where in the last line we used the definition of <inline-formula><inline-graphic xlink:href="590955v1_inline74.gif" mimetype="image" mime-subtype="gif"/></inline-formula> from <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>.</p>
</sec>
<sec id="s5f">
<title>Leaky integrate-and-fire neurons</title>
<p>The terms on the right-hand-side in <xref ref-type="disp-formula" rid="eqn21">Eqs. (21)</xref> and <xref ref-type="disp-formula" rid="eqn23">(23)</xref> can be interpreted as transmembrane currents. The last term in these equations, <inline-formula><inline-graphic xlink:href="590955v1_inline75.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, can be interpreted as a current instantaneously resetting the membrane potential upon reaching the firing threshold<sup><xref ref-type="bibr" rid="c28">28</xref></sup>. Indeed, when the membrane potential reaches the threshold, it triggers a spike and causes a jump of the membrane potential by an amount −<italic>β</italic><sup><italic>y</italic></sup>; this realizes resetting of the membrane potential which is equivalent to the resetting rule of integrate-and-fire neurons<sup><xref ref-type="bibr" rid="c54">54</xref>,<xref ref-type="bibr" rid="c56">56</xref></sup>. Thus, by taking into account the resetting mechanism and defining the time constants of population and single neuron readout <italic>τ</italic> := <italic>λ</italic><sup><italic>−</italic>1</sup> and <inline-formula><inline-graphic xlink:href="590955v1_inline76.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, we can rewrite <xref ref-type="disp-formula" rid="eqn21">Eqs. (21)</xref> and <xref ref-type="disp-formula" rid="eqn23">(23)</xref> as a leaky integrate-and-fire neuron model,
<disp-formula id="eqn24">
<graphic xlink:href="590955v1_eqn24.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In the <xref ref-type="disp-formula" rid="eqn24">Eq. 24</xref> we wrote explicitly the terms <inline-formula><inline-graphic xlink:href="590955v1_inline77.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which correspond to the synaptic projections of <italic>N</italic><sup><italic>x</italic></sup> presynaptic neurons of type <italic>x</italic> to the postsynaptic neuron <italic>i</italic> of type <italic>y</italic>, with the quantity <inline-formula><inline-graphic xlink:href="590955v1_inline78.gif" mimetype="image" mime-subtype="gif"/></inline-formula> denoting the synaptic weight. We note that, in the case of I neurons, the element with <italic>j</italic> = <italic>i</italic> describes an autapse, i.e., a projection of a neuron with itself; this term is equal to <inline-formula><inline-graphic xlink:href="590955v1_inline79.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and thus contributes to the resetting of the neuron <italic>i</italic>.</p>
</sec>
<sec id="s5g">
<title>Imposing Dale’s principle on synaptic connectivity</title>
<p>We now examine the synaptic terms in <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref>. As a first remark, we see that synaptic weights depend on tuning parameters <inline-formula><inline-graphic xlink:href="590955v1_inline80.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. For the sake of generality we drew tuning parameters <inline-formula><inline-graphic xlink:href="590955v1_inline81.gif" mimetype="image" mime-subtype="gif"/></inline-formula> from a normal distribution with vanishing mean, which yielded both positive and negative values of <inline-formula><inline-graphic xlink:href="590955v1_inline82.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This has the desirable consequence that a spike of a neuron with a positive tuning parameter in signal dimension <italic>k</italic>,<inline-formula><inline-graphic xlink:href="590955v1_inline83.gif" mimetype="image" mime-subtype="gif"/></inline-formula> pulls the estimate, <inline-formula><inline-graphic xlink:href="590955v1_inline84.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, up, while a spike of a neuron with <inline-formula><inline-graphic xlink:href="590955v1_inline85.gif" mimetype="image" mime-subtype="gif"/></inline-formula> pulls the estimate down, allowing population readouts to track both positive and negative fluctuations of the target signal on a fast time scale.</p>
<p>Another consequence of synaptic connectivity in the <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref> is that the synaptic weight between a presynaptic neuron <italic>j</italic> of type <italic>x</italic> and a postsynaptic neuron <italic>i</italic> of type <italic>y</italic> is symmetric and depends on the similarity of tuning vectors of the presynaptic and the postsynaptic neuron: <inline-formula><inline-graphic xlink:href="590955v1_inline86.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The sign of this scalar product is positive between neurons with similar tuning and negative between neurons with different tuning (and zero when the two tuning vectors are orthogonal). Thus, for a presynaptic neuron <italic>j</italic> of type <italic>x</italic>, the synaptic weights of its outgoing connections can be both positive and negative, because some of its postsynaptic neurons have similar tuning to the neuron <italic>j</italic> while others have different tuning. This is inconsistent with Dale’s principle<sup><xref ref-type="bibr" rid="c89">89</xref></sup>, which postulates that a particular neuron can only have one type of effect on postsynaptic neurons (excitatory or inhibitory), but never both. To impose this constraint in our model, we set synaptic weights between neurons with different tuning <inline-formula><inline-graphic xlink:href="590955v1_inline87.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to zero. To this end, we define the rectified connectivity matrices,
<disp-formula id="eqn25">
<graphic xlink:href="590955v1_eqn25.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>x, y</italic> ∈{<italic>E, I</italic>} and [<italic>a</italic>]<sub>+</sub> ≡ max(0, <italic>a</italic>) a rectified linear function. This manipulation is also plausible from a biological point of view, because in the cortex, the connection probability of neurons with very different (e.g. opposite) tuning is typically close to 0<sup><xref ref-type="bibr" rid="c51">51</xref></sup>. Since the elements of the matrix <italic>J</italic><sup><italic>yx</italic></sup> are all non-negative, it is the sign in front of the synaptic term in the <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref> that determines the sign of the synaptic current between neurons <italic>i</italic> and <italic>j</italic>. The synaptic current is excitatory if the sign is positive, and inhibitory if the sign is negative.</p>
<p>It is also interesting to note that rectification affects the rank of connectivity matrices. Without rectification, the product in <xref ref-type="disp-formula" rid="eqn25">Eq. (25)</xref> yields a connectivity matrix with rank smaller or equal to the number of input features to the network, <italic>M</italic>, similarly as in previous works<sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c44">44</xref></sup>. Since typically the number of input features is much smaller than the number of neurons, i.e., <italic>M</italic> &lt;&lt; <italic>N</italic><sup><italic>y</italic></sup>, this would give a low-rank connectivity matrix. However, rectification in <xref ref-type="disp-formula" rid="eqn25">Eq. (25)</xref>, necessary to ensure Dale’s principle in presence of positive and negative tuning parameters, typically results in a substantial increase of the rank of the connectivity matrix.</p>
<p>Using the synaptic connectivity defined in <xref ref-type="disp-formula" rid="eqn25">Eq. (25)</xref>, we rewrite the network dynamics from <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref> as:
<disp-formula id="eqn26">
<graphic xlink:href="590955v1_eqn26.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
These equations express the neural dynamics which minimizes the loss functions (<xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref>) in terms of a generalized leaky integrate-and-fire model with E and I cell types, and are consistent with Dale’s principle.</p>
<p>In principle, it is possible to use the same strategy as for the E-I network to enforce Dale’s principle in model with one cell type (introduced by<sup><xref ref-type="bibr" rid="c28">28</xref></sup>). To do so, we constrained the recurrent connectivity of the model with a single cell type from<sup><xref ref-type="bibr" rid="c36">36</xref></sup> by keeping only connections between neurons with similar tuning vectors and setting other connections to 0 (see Supplementary text). This led to a network of only inhibitory neurons, a type of network model which is less relevant for the description of biological networks.</p>
</sec>
<sec id="s5h">
<title>Model with resting potential and an external current</title>
<p>In the model given by the <xref ref-type="disp-formula" rid="eqn26">Eq. (26)</xref> the resting potential is equal to zero. In order to account for biophysical values of the resting potential and to introduce an implementation of the metabolic constant that is consistent with neurobiology, we add a constant value to the dynamical equations of the membrane potentials <inline-formula><inline-graphic xlink:href="590955v1_inline88.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the firing thresholds <inline-formula><inline-graphic xlink:href="590955v1_inline88a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and the reset potentials <inline-formula><inline-graphic xlink:href="590955v1_inline89.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This does not change the spiking dynamics of the model, as what matters to correctly infer the efficient spiking times of neurons is the distance between the membrane potential and the threshold.</p>
<p>Furthermore, in the same equations, the role of the metabolic constant <italic>β</italic><sup><italic>y</italic></sup> as a biophysical quantity is questionable. The metabolic constant <italic>β</italic><sup><italic>y</italic></sup> is an important parameter that weights the metabolic cost over the encoding error in the objective functions (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>). On the level of computational objectives, the metabolic constant naturally controls firing rates, as it allows the network to fire more or less spikes to correct for a certain encoding error. A flexible control of the firing rates is a desirable property, as gives the possibility to potentially capture different operating regimes of efficient spiking networks<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. In the spiking model we developed thus far (<xref ref-type="disp-formula" rid="eqn26">Eq. 26</xref>), similarly to previous efficient spiking models<sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup>, the metabolic constant <italic>β</italic><sup><italic>y</italic></sup> controls the firing threshold. In neurobiology, however, strong changes to the firing threshold that would reflect metabolic constraints of the network are not plausible. We thus searched for an implementation of the metabolic constant <italic>β</italic><sup><italic>y</italic></sup> that is consistent with neurobiology.</p>
<p>The condition for threshold crossing of the neuron <italic>i</italic> can be written by <xref ref-type="disp-formula" rid="eqn26">Eq. (26)</xref> as
<disp-formula id="eqn27">
<graphic xlink:href="590955v1_eqn27.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>c</italic> an arbitrary constant in units of millivolts. In <xref ref-type="disp-formula" rid="eqn27">Eq. (27)</xref> we added a constant <italic>c/</italic>2 and a resting potential <inline-formula><inline-graphic xlink:href="590955v1_inline90.gif" mimetype="image" mime-subtype="gif"/></inline-formula> on the left- and right-hand side of the firing rule. Moreover, we shifted the noise and the dependency on the parameter <italic>β</italic> from the firing threshold to the membrane potential. Thus, we assumed that the firing threshold is independent of the metabolic constant and the noise, and we instead assumed the dependence on the metabolic constant and noise in the membrane potentials.</p>
<p>We now define new variables for <italic>y</italic> ∈ {<italic>E, I</italic>}:
<disp-formula id="eqn28">
<graphic xlink:href="590955v1_eqn28.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and rewrite the model in <xref ref-type="disp-formula" rid="eqn26">Eq. 26</xref> in these new variables
<disp-formula id="eqn29">
<graphic xlink:href="590955v1_eqn29.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v1_inline91.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline92.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are the independent Gaussian white noise processes defined in <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref> above. We note that all terms on the right-hand side of <xref ref-type="disp-formula" rid="eqn29">Eq. (29)</xref> have the desired units of mV. The model in <xref ref-type="disp-formula" rid="eqn29">Eq. (29)</xref> is an efficient E-I spiking network with improved compatibility with neurobiology. We have expressed two new terms in the membrane potentials of E and I neurons, one dependent on the metabolic constant <italic>β</italic><sup><italic>y</italic></sup> and one on the noise that we assumed in the condition for spiking (see <xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref>). We will group these two terms to define an external current, a current that is well known in spiking models of neural dynamics<sup><xref ref-type="bibr" rid="c40">40</xref></sup>.</p>
</sec>
<sec id="s5i">
<title>Efficient generalized leaky integrate-and-fire neuron model</title>
<p>Finally, we rewrite the model from <xref ref-type="disp-formula" rid="eqn29">Eq. (29)</xref> in a compact form in terms of transmembrane currents, and discuss their biological interpretation. The efficient coding with spikes is realized by the following model for the neuron <italic>i</italic> of type <italic>y</italic> ∈ {<italic>E, I</italic>}:
<disp-formula id="eqn30a">
<graphic xlink:href="590955v1_eqn30a.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>R</italic><sub><italic>m</italic></sub> the current resistance. The leak current,
<disp-formula id="eqn30b">
<graphic xlink:href="590955v1_eqn30b.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>τ</italic> = <italic>R</italic><sub><italic>m</italic></sub><italic>C</italic><sub><italic>m</italic></sub> and <italic>C</italic><sub><italic>m</italic></sub> the capacitance of the neural membrane<sup><xref ref-type="bibr" rid="c54">54</xref></sup>, arose by assuming the same time constant for the target signals <italic>x</italic><sub><italic>k</italic></sub> and estimates <inline-formula><inline-graphic xlink:href="590955v1_inline93.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline94.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (see <xref ref-type="disp-formula" rid="eqn19">Eq. 19</xref>). We see that the passive membrane time constant <italic>τ</italic> = <italic>λ</italic><sup><italic>−</italic>1</sup> can be traced back to the time constant of the population read-out in <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>. The synaptic currents are defined as
<disp-formula id="eqn30c">
<graphic xlink:href="590955v1_eqn30c.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we note the presence of a feedforward current to E neurons,
<disp-formula id="eqn30d">
<graphic xlink:href="590955v1_eqn30d.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
which consist in a linear combination of the stimulus features <bold><italic>s</italic></bold>(<italic>t</italic>) weighted by the readout weights <inline-formula><inline-graphic xlink:href="590955v1_inline95.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The stimulus features can be traced back to the definition of the target signals in <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>. This current emerges in E neurons, as a consequence of having the target signals <italic>x</italic><sub><italic>k</italic></sub>(<italic>t</italic>) in the loss function of the E population (see <xref ref-type="disp-formula" rid="eqn10">Eqs. 10</xref>-<xref ref-type="disp-formula" rid="eqn11">11</xref>). I neurons do not receive the feedforward current because their loss function does not contain the target signal.</p>
<p>The current providing within-neuron feedback triggered by each spike,
<disp-formula id="eqn30e">
<graphic xlink:href="590955v1_eqn30e.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
was recently recovered<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. This current has the kinetics of the single neuron readout <inline-formula><inline-graphic xlink:href="590955v1_inline96.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (i.e., low-pass filtered spike train). Its sign depends on the relation between the time constant of the population readout <italic>τ</italic> = <italic>λ</italic><sup><italic>−</italic>1</sup> and single neuron readout <inline-formula><inline-graphic xlink:href="590955v1_inline97.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, because the metabolic constant <italic>β</italic><sup><italic>y</italic></sup> is non-negative by definition (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>). If the single neuron readout is slower than the population readout, <inline-formula><inline-graphic xlink:href="590955v1_inline98.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, within-neuron feedback is negative, and can thus be interpreted as spike-triggered <italic>adaptation</italic>. On the contrary, if the single neuron readout is faster than the population readout, <inline-formula><inline-graphic xlink:href="590955v1_inline99.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the within-neuron feedback is positive and can thus be interpreted as spike-triggered <italic>facilitation</italic>. In a special case where the time constant of the single neuron and population readout are assumed to be equal, within-neuron feedback vanishes.</p>
<p>Finally, we here derived the non-specific external current:
<disp-formula id="eqn30f">
<graphic xlink:href="590955v1_eqn30f.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
that captures the ensemble of non-specific synaptic currents received by each single neuron. The non-specific current has a homogeneous mean across all neurons of the same cell type, and a neuron-specific fluctuation. The mean of the non-specific current can be traced back to the weighting of the metabolic cost over the encoding error in model objectives (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>), while the fluctuation can be traced back to the noise intensity that we assumed in the condition for spiking (<xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref>). The non-specific external current might arise because of synaptic inputs from other brain areas than the brain area that delivers feedforward projections to the E-I network we consider here, or it might result from synaptic activity of neurons that are part of the local network, but are not tuned to the feedforward input<sup><xref ref-type="bibr" rid="c69">69</xref></sup>.</p>
<p>We also recall the fast and slower time scales of single neuron activity:
<disp-formula id="eqn30g">
<graphic xlink:href="590955v1_eqn30g.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and the connectivity matrices
<disp-formula id="eqn30h">
<graphic xlink:href="590955v1_eqn30h.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The structure of synaptic connectivity is fully determined by the similarity of tuning vectors of the presynaptic and the postsynaptic neurons (<inline-formula><inline-graphic xlink:href="590955v1_inline100.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline101.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, respectively), while the distribution of synaptic connectivity weights is fully determined by the distribution of tuning parameters <inline-formula><inline-graphic xlink:href="590955v1_inline102.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
</sec>
<sec id="s5j">
<title>Stimulus features</title>
<p>We define stimulus features as a set of <italic>k</italic> = 1, …, <italic>M</italic> independent Ornstein-Uhlenbeck processes with vanishing mean, standard deviation <italic>σ</italic><sub><italic>s</italic></sub> and the correlation time <italic>τ</italic><sub><italic>s</italic></sub>,
<disp-formula id="eqn31">
<graphic xlink:href="590955v1_eqn31.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
If not mentioned otherwise, we use the following parameters: <italic>σ</italic><sub><italic>s</italic></sub> = 2 (mV)<sup>1<italic>/</italic>2</sup> and <italic>τ</italic><sub><italic>s</italic></sub> = 10 ms. Variables <italic>η</italic><sub><italic>k</italic></sub>(<italic>t</italic>) are independent Gaussian white noise processes with zero mean and covariance function ⟨<italic>η</italic><sub><italic>k</italic></sub>(<italic>t</italic>)<italic>η</italic><sub><italic>l</italic></sub>(<italic>t</italic>′)⟩ = <italic>δ</italic><sub><italic>kl</italic></sub><italic>δ</italic>(<italic>t</italic> − <italic>t</italic>′). These variables should not be confused with the Gaussian white noises <inline-formula><inline-graphic xlink:href="590955v1_inline103.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in <xref ref-type="disp-formula" rid="eqn29">Eq. (29)</xref>.</p>
</sec>
<sec id="s5k">
<title>Parametrization of synaptic connectivity</title>
<p>In the efficient E-I model, synaptic weights <inline-formula><inline-graphic xlink:href="590955v1_inline104.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are parametrized by tuning parameters <inline-formula><inline-graphic xlink:href="590955v1_inline105.gif" mimetype="image" mime-subtype="gif"/></inline-formula> through <xref ref-type="disp-formula" rid="eqn25">Eq. (25)</xref>. The total number of synapses in the E-I, I-I and I-E connectivity matrices (including silent synapses with zero synaptic weight) is <italic>n</italic><sub>syn</sub> = 2<italic>N</italic><sup><italic>E</italic></sup><italic>N</italic><sup><italic>I</italic></sup> + (<italic>N</italic><sup><italic>I</italic></sup>)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, while the number of tuning parameters is <italic>n</italic><sub><italic>w</italic></sub> = <italic>M</italic> (<italic>N</italic><sup><italic>E</italic></sup> + <italic>N</italic><sup><italic>I</italic></sup>). Because the number of stimulus features <italic>M</italic> is expected to be much smaller than the number of E or I neurons, the number of tuning parameters <italic>n</italic><sub><italic>w</italic></sub> is much smaller than the number of synapses <italic>n</italic><sub>syn</sub>.</p>
<p>We can achieve a further substantial decrease in the number of free parameters by using a parametric distribution of tuning parameters <inline-formula><inline-graphic xlink:href="590955v1_inline106.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We have set the tuning parameters following a normal distribution and found that excellent performance can be achieved with random draws of tuning parameters from the normal distribution, thus without searching for a specific set of tuning parameters. This drastically decreased the number of free parameters relative to synaptic weights to only a handful of parameters that determine the distributions of tuning parameters.</p>
<p>Given <italic>M</italic> features, we sample tuning parameters,<inline-formula><inline-graphic xlink:href="590955v1_inline107.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with <italic>i</italic> = 1, …, <italic>N</italic><sup><italic>y</italic></sup>, <italic>y</italic> ∈ {<italic>E, I</italic>}, as random points uniformly distributed on a <italic>M</italic> -dimensional sphere of radius <inline-formula><inline-graphic xlink:href="590955v1_inline108.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We obtain this by sampling, for each neuron, a vector of <italic>M</italic> i.i.d. standard Gaussian random variables,<inline-formula><inline-graphic xlink:href="590955v1_inline109.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="590955v1_inline110.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and normalizing the vector such as to have length equal to <inline-formula><inline-graphic xlink:href="590955v1_inline111.gif" mimetype="image" mime-subtype="gif"/></inline-formula>,
<disp-formula id="eqn32">
<graphic xlink:href="590955v1_eqn32.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This ensures that the length of tuning vectors <inline-formula><inline-graphic xlink:href="590955v1_inline112.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in <xref ref-type="disp-formula" rid="eqn32">Eq. (32)</xref> is homogeneous across neurons of the same cell type, i.e., <inline-formula><inline-graphic xlink:href="590955v1_inline113.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Parameters <inline-formula><inline-graphic xlink:href="590955v1_inline114.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline115.gif" mimetype="image" mime-subtype="gif"/></inline-formula> determine the heterogeneity (spread) of tuning parameters.</p>
<p>By combining <xref ref-type="disp-formula" rid="eqn25">Eq. (25)</xref> and <xref ref-type="disp-formula" rid="eqn32">Eq. (32)</xref>, we obtain the synaptic weights, <inline-formula><inline-graphic xlink:href="590955v1_inline116.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, as a function of the angle, <inline-formula><inline-graphic xlink:href="590955v1_inline117.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, between the tuning vectors of presynaptic neurons, <inline-formula><inline-graphic xlink:href="590955v1_inline118.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and postsynaptic neurons,<inline-formula><inline-graphic xlink:href="590955v1_inline119.gif" mimetype="image" mime-subtype="gif"/></inline-formula>,
<disp-formula id="eqn33">
<graphic xlink:href="590955v1_eqn33.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In the <italic>M</italic> = 3 dimensional case, we have that the distribution of the angle between two vectors is <inline-formula><inline-graphic xlink:href="590955v1_inline120.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="590955v1_inline121.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Thus, the average strength of synaptic weights between the pre- and the postsynaptic population can be calculated as
<disp-formula id="eqn34">
<graphic xlink:href="590955v1_eqn34.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Thus, the upper bound for the synaptic weight between cell types <italic>x</italic> and <italic>y</italic> is simply
<disp-formula id="eqn35">
<graphic xlink:href="590955v1_eqn35.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
From the <xref ref-type="disp-formula" rid="eqn34">Eq. (34)</xref>, we have that the mean E-I connectivity is equal to the mean I-E connectivity <inline-formula><inline-graphic xlink:href="590955v1_inline122.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. As we consider the ratio of the mean connectivity between E-I and I-I neurons, we find that it is given by the following:
<disp-formula id="eqn36">
<graphic xlink:href="590955v1_eqn36.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s5l">
<title>Performance measures</title>
<sec id="s5l1">
<title>Average encoding error and average metabolic cost</title>
<p>The definition of the time-dependent loss functions (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>) induces a natural choice for the performance measure: the mean squared error (MSE) between the targets and their estimators for each cell type. In the case of the E population, the time-dependent encoding error is captured by the variable <italic>ϵ</italic><sup><italic>E</italic></sup>(<italic>t</italic>) in the <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref> and in case of I population it is captured by <italic>ϵ</italic><sup><italic>I</italic></sup> (<italic>t</italic>) defined in the same equation. We used the root MSE (RMSE), a standard measure for the performance of an estimator<sup><xref ref-type="bibr" rid="c40">40</xref></sup>. For the cell type <italic>y</italic> ∈ {<italic>E, I</italic>} in trial <italic>q</italic>, the RMSE is measured as
<disp-formula id="eqn37">
<graphic xlink:href="590955v1_eqn37.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with ⟨<italic>z</italic><sub><italic>q</italic></sub>(<italic>t</italic>)⟩<sub><italic>t,q</italic></sub> denoting the time- and trial-average.</p>
<p>Following the definition of the time-dependent metabolic cost in the loss functions (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>), we measured the average metabolic cost in a trial <italic>q</italic> for the cell type <italic>y</italic> ∈ {<italic>E, I</italic>} as
<disp-formula id="eqn38">
<graphic xlink:href="590955v1_eqn38.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with time-dependent metabolic cost <italic>κ</italic><sup><italic>y</italic></sup>(<italic>t</italic>) as in model’s objectives (<xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>) and ⟨<italic>z</italic><sub><italic>q</italic></sub>(<italic>t</italic>)⟩ <sub><italic>t,q</italic></sub> the time- and trial-average. The square root was taken to have the same scale as for the RMSE (see <xref ref-type="disp-formula" rid="eqn37">Eq. 37</xref>).</p>
</sec>
<sec id="s5l2">
<title>The bias of the estimator</title>
<p>The MSE can be decomposed into the bias and the variance of the estimator. The time-dependent bias of estimates<inline-formula><inline-graphic xlink:href="590955v1_inline123.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, <italic>y</italic> ∈ {<italic>E, I</italic>}, were evaluated for each time point over <italic>q</italic> = 1, …, <italic>Q</italic> trials. The time-dependent bias in input dimension <italic>k</italic> = 1, …, <italic>M</italic> is defined as
<disp-formula id="eqn39a">
<graphic xlink:href="590955v1_eqn39a.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with ⟨<italic>z</italic><sub><italic>q</italic></sub>(<italic>t</italic>)⟩ <sub><italic>t,q</italic></sub> the trial-averaged realization at time <italic>t</italic>. To have an average measure of the encoding bias, we averaged the bias of estimators over time and over input dimensions:
<disp-formula id="eqn39b">
<graphic xlink:href="590955v1_eqn39b.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The averaging over time and input dimensions is justified because <italic>s</italic><sub><italic>k</italic></sub>(<italic>t</italic>) are independent realizations of the Ornstein-Uhlenbeck process (see <xref ref-type="disp-formula" rid="eqn31">Eq.31</xref>) with vanishing mean and with the same time constant, and variance across input dimensions.</p>
</sec>
<sec id="s5l3">
<title>Criterion for determining optimal model parameters</title>
<p>The equations of the E-I spiking network in <xref ref-type="disp-formula" rid="eqn30a">Eqs. 30a</xref>-<xref ref-type="disp-formula" rid="eqn30h">30h</xref> (Methods), derived from the instantaneous loss functions, give efficient coding solutions valid for any set of parameter values. However, to choose parameters values in simulated data in a principled way, we performed a numerical optimization of the performance function detailed below. Numerical optimization gave the set of optimal parameters listed in <xref rid="tbl1" ref-type="table">Table 1</xref>. When testing the efficient E-I model with simulations, we used the optimal parameters in <xref rid="tbl1" ref-type="table">Table 1</xref> and changed only the parameters plotted in the figure axes on a figure-by-figure basis.</p>
<p>To estimate the optimal set of parameters <italic>θ</italic> = <italic>θ</italic><sup><italic>*</italic></sup>, we performed a grid search on each parameter <italic>θ</italic><sub><italic>i</italic></sub> while keeping all other parameters fixed as specified in <xref rid="tbl1" ref-type="table">Table 1</xref>. While varying the parameters, we measured a weighted sum of the time- and trial-averaged encoding error and metabolic cost. For each cell type <italic>y</italic> ∈ {<italic>E, I</italic>}, we computed
<disp-formula id="eqn40a">
<graphic xlink:href="590955v1_eqn40a.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with ⟨<italic>z</italic><sub><italic>q</italic></sub>(<italic>t</italic>)⟩ <sub><italic>t,q</italic></sub> the average over time and over trials and with <italic>ϵ</italic><sup><italic>y</italic></sup>(<italic>t</italic>) and <italic>κ</italic><sup><italic>y</italic></sup>(<italic>t</italic>) as in model’s objectives (<xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>).</p>
<p>To optimize the performance measure, we used a value of <italic>g</italic><sub><italic>L</italic></sub> = 0.7. The parameter <italic>g</italic><sub><italic>L</italic></sub> in the <xref ref-type="disp-formula" rid="eqn40a">Eq. (40a)</xref> regulates the relative importance of the average encoding error over the average metabolic cost. Since the performance measure in <xref ref-type="disp-formula" rid="eqn40a">Eq. (40a)</xref> is closely related to the average over time and trials of the instantaneous loss function (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>) where the parameter <italic>β</italic> regulates the relative weight of instantaneous encoding error over the metabolic cost, setting <italic>g</italic><sub><italic>L</italic></sub> is effectively achieved by setting <italic>β</italic>.</p>
<p>The optimal parameter set <italic>θ</italic> = <italic>θ</italic><sup><italic>*</italic></sup> reported in <xref rid="tbl1" ref-type="table">Table 1</xref> is the parameter set that minimizes the sum of losses across E and I cell type
<disp-formula id="eqn40b">
<graphic xlink:href="590955v1_eqn40b.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For visualization of the behavior of the average metabolic cost (<xref ref-type="disp-formula" rid="eqn38">Eq. 38</xref>) and average loss (<xref ref-type="disp-formula" rid="eqn40a">Eq. 40a</xref>) across a range of a specific parameter <italic>θ</italic><sub><italic>i</italic></sub>, we summed these measures across the E and I cell type and normalized them across the range of tested parameters.</p>
<p>The exact dynamic and performance of our model depends on the realizations of random variables which describe the the tuning parameters <inline-formula><inline-graphic xlink:href="590955v1_inline124.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the Gaussian noise in the non-specific currents <inline-formula><inline-graphic xlink:href="590955v1_inline125.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and the initial conditions of the membrane potential <inline-formula><inline-graphic xlink:href="590955v1_inline126.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, that were randomly drawn from a normal distribution in each simulation trial. To capture the performance of a “typical” network, we iterated the performance measures across trials with different realizations of these random variables, and averaged the performance measures across trials. We used 100 simulation trials for each parameter search.</p>
</sec>
</sec>
<sec id="s5m">
<title>Functional activity measures</title>
<sec id="s5m1">
<title>Tuning similarity</title>
<p>The pair-wise tuning similarity was measured as the cosine similarity<sup><xref ref-type="bibr" rid="c91">91</xref></sup>, defined as:
<disp-formula id="eqn41">
<graphic xlink:href="590955v1_eqn41.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="590955v1_inline127.gif" mimetype="image" mime-subtype="gif"/></inline-formula> the length of the tuning vector in Euclidean space and <italic>α</italic> the angle between the tuning vectors <inline-formula><inline-graphic xlink:href="590955v1_inline128.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v1_inline129.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
</sec>
</sec>
<sec id="s5n">
<title>Cross-correlograms of spike timing</title>
<p>The time-dependent coordination of spike timing was measured with the cross-correlogram (CCG) of spike trains, corrected for stimulus-driven coincident spiking. The raw cross-correlogram (CCG) for neuron <italic>i</italic> of cell type <italic>y</italic> and neuron <italic>j</italic> of cell type <italic>x</italic> was measured as follows:
<disp-formula id="eqn42a">
<graphic xlink:href="590955v1_eqn42a.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>q</italic> = 1, …, <italic>Q</italic> simulation trials with identical stimulus and <italic>T</italic> the duration of the trial. We subtracted from the raw CCG the CCG of trial-invariant activity. To evaluate the trial-invariant cross-correlogram, we first computed the peri-stimulus time histogram (PSTH) for each neuron as follows:
<disp-formula id="eqn42b">
<graphic xlink:href="590955v1_eqn42b.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The trial-invariant CCG was then evaluated as the cross-correlation function of PSTHs between neurons <italic>i</italic> and <italic>j</italic>,
<disp-formula id="eqn42c">
<graphic xlink:href="590955v1_eqn42c.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Finally, the temporal coordination of spike timing was computed by subtracting the correction term from the raw CCG:
<disp-formula id="eqn42d">
<graphic xlink:href="590955v1_eqn42d.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<sec id="s5n1">
<title>Average imbalance of synaptic inputs</title>
<p>We considered time and trial-averaged synaptic inputs to each E and I neuron <italic>i</italic> in trial <italic>q</italic>, evaluated as:
<disp-formula id="eqn43">
<graphic xlink:href="590955v1_eqn43.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with synaptic currents to E neurons <inline-formula><inline-graphic xlink:href="590955v1_inline130.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and to I neurons <inline-formula><inline-graphic xlink:href="590955v1_inline131.gif" mimetype="image" mime-subtype="gif"/></inline-formula> as in <xref ref-type="disp-formula" rid="eqn30c">Eq. (30c)</xref>. Synaptic inputs were measured in units of mV. We reported trial-averages of the net synaptic inputs from the <xref ref-type="disp-formula" rid="eqn43">Eq. (43)</xref>.</p>
</sec>
<sec id="s5n2">
<title>Instantaneous balance of synaptic inputs</title>
<p>We measured the instantaneous balance of synaptic inputs as the Pearson correlation of time-dependent synaptic inputs incoming to the neuron <italic>i</italic>. For those synaptic inputs that are defined as weighted delta-spikes (for which the Pearson correlation is not well defined; see <xref ref-type="disp-formula" rid="eqn30c">Eq. 30c</xref>), we convolved spikes with a synaptic filter <inline-formula><inline-graphic xlink:href="590955v1_inline132.gif" mimetype="image" mime-subtype="gif"/></inline-formula>,
<disp-formula id="eqn44">
<graphic xlink:href="590955v1_eqn44.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we used the expression for the feedforward synaptic current from the <xref ref-type="disp-formula" rid="eqn30d">Eq. (30d)</xref>. Note that the feedforward synaptic current is already already low-pass filtered (see <xref ref-type="disp-formula" rid="eqn31">Eq. 31</xref>). Using synaptic inputs from the <xref ref-type="disp-formula" rid="eqn44">Eq. 44</xref>, we computed the Pearson correlation of synaptic inputs incoming to single E neurons, <inline-formula><inline-graphic xlink:href="590955v1_inline133.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for <italic>i</italic> = 1, …, <italic>N</italic><sup><italic>E</italic></sup>, and to single I neurons, <inline-formula><inline-graphic xlink:href="590955v1_inline134.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for <italic>i</italic> = 1, …, <italic>N</italic><sup><italic>I</italic></sup>. The coefficients were then averaged across trials.</p>
</sec>
<sec id="s5n3">
<title>Perturbation of connectivity</title>
<p>To test the robustness of the model to random perturbations of synaptic weights, we applied a random jitter to optimally efficient recurrent synaptic connectivity weights. The random jitter was proportional to the synaptic weight, <inline-formula><inline-graphic xlink:href="590955v1_inline135.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>σ</italic><sub><italic>J</italic></sub> is the strength of the perturbation and <inline-formula><inline-graphic xlink:href="590955v1_inline136.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are independent standard normal random variables. All three recurrent connectivity matrices (E-I, I-I and I-E) were randomly perturbed at once.</p>
</sec>
</sec>
<sec id="s5o">
<title>Computer simulations</title>
<p>We run computer simulations with Matlab R2023b (Mathworks). The membrane equation for each neuron was integrated with Euler integration scheme with the time step of <italic>dt</italic> = 0.02 ms.</p>
<p>The simulation of the E-I network with 400 E units and 100 I units for an equivalent of 1 second of neural activity lasted approximately 1.65 seconds on a laptop.</p>
</sec>
</sec>
<sec id="s6">
<title>Supplementary material</title>
<sec id="s6a">
<title>Supplementary text 1: Derivation of the one cell type model</title>
<p>An efficient spiking model network with one cell type (1CT) has been developed previously<sup><xref ref-type="bibr" rid="c28">28</xref></sup>, and properties of the 1CT model where the computation is assumed to be the leaky integration of inputs has been addressed in a number of previous studies<sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c42">42</xref></sup>. Compared to the efficient E-I model, the 1CT model can be seen as a simplification, and can be treated similarly to the E-I model, which is what we demonstrate in this section.</p>
<p>As the name of the model suggests, all neurons in the 1CT model are of the same cell type, and we have <italic>i</italic> = 1, …, <italic>N</italic> such neurons. We can then use the definitions in <xref ref-type="disp-formula" rid="eqn6">Eqs. (6)</xref> - <xref ref-type="disp-formula" rid="eqn9">(9)</xref> (now without the index <italic>y</italic>) and a loss function similar to the one in<sup><xref ref-type="bibr" rid="c36">36</xref></sup>, but with only one (quadratic) regularizer
<disp-formula id="eqnS_1">
<graphic xlink:href="590955v1_eqnS_1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>β</italic><sub>1</sub> &gt; 0. The encoding error of the one cell type model minimizes the squared distance between the target signal <italic>x</italic><sub><italic>k</italic></sub>(<italic>t</italic>) and the estimate <inline-formula><inline-graphic xlink:href="590955v1_inline137.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. As we apply the condition for spiking as for the E-I network (<xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref> without the index <italic>y</italic>) and follow the same steps as for the E-I network, we get
<disp-formula id="eqnS_2">
<graphic xlink:href="590955v1_eqnS_2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>ξ</italic><sub><italic>i</italic></sub>(<italic>t</italic>) the noise at the condition for spiking. Same as in the E-I model, we define the noise as an Ornstein-Uhlenbeck process with zero mean, obeying
<disp-formula id="eqnS_3">
<graphic xlink:href="590955v1_eqnS_3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>η</italic><sub><italic>i</italic></sub> is a Gaussian white noise and <italic>λ</italic> = <italic>τ</italic><sup><italic>−</italic>1</sup> is the inverse time constant of the process. We now define proxies of the membrane potential and the firing threshold as
<disp-formula id="eqnS_4">
<graphic xlink:href="590955v1_eqnS_4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Differentiating the proxy of the membrane potential <italic>u</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and rewriting the model as an integrate-and-fire neuron, we get
<disp-formula id="eqnS_5">
<graphic xlink:href="590955v1_eqnS_5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We now proceed in the same way as with the E-I model and define new variables
<disp-formula id="eqnS_6">
<graphic xlink:href="590955v1_eqnS_6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In these new variables, we can rewrite the membrane equation of the 1CT model as follows:
<disp-formula id="eqnS_7">
<graphic xlink:href="590955v1_eqnS_7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Finally, we rewrite the model with a more compact notation of a leaky integrate-and-fire neuron model with transmembrane currents,
<disp-formula id="eqnS_8a">
<graphic xlink:href="590955v1_eqnS_8a.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with currents
<disp-formula id="eqnS_8b">
<graphic xlink:href="590955v1_eqnS_8b.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Note that the model with one cell type does not obey Dale’s law, since the same neuron sends to its postsynaptic targets excitatory and inhibitory currents, depending on the tuning similarity of the presynaptic and the postsynaptic neuron <italic>w</italic><sub><italic>i</italic></sub> and <italic>w</italic><sub><italic>j</italic></sub> (<xref ref-type="disp-formula" rid="eqnS_8b">Eq. S.8b</xref>). In particular, if the pre- and postsynaptic neurons have similar selectivity <inline-formula><inline-graphic xlink:href="590955v1_inline138.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the recurrent interaction is inhibitory, and if the neurons have different selectivity <inline-formula><inline-graphic xlink:href="590955v1_inline139.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the interaction is excitatory. Simply put, neurons with similar selectivity inhibit each other while neurons with different selectivity excite each other<sup><xref ref-type="bibr" rid="c36">36</xref></sup>.</p>
<p>Dale’s law can be imposed to the 1CT model the same way as in the E-I model. To do so, we removed synaptic interactions between neurons with different selectivity by rectifying the connectivity matrix,
<disp-formula id="eqnS_9">
<graphic xlink:href="590955v1_eqnS_9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
However, this manipulation results in a network with only inhibitory recurrent synaptic interactions, and thus a network of only inhibitory neurons. Network with only inhibitory interactions is less relevant for the description of recurrently connected biological networks.</p>
</sec>
<sec id="s6b">
<title>Supplementary text 2: Analysis of the one cell type model and comparison with the E-I model</title>
<p>We re-derived the 1CT model as a simplification of the E-I network (Supplementary Text 1, <xref rid="figS1" ref-type="fig">Supplementary Fig. S1A-B</xref>), with objective function of the same form as <italic>L</italic><sup><italic>E</italic></sup> and by allowing a single type of neurons sending both excitatory and inhibitory synaptic currents to their post-synaptic targets (<xref rid="figS1" ref-type="fig">Supplementary Fig. S1C</xref>). Similarly to the E-I model, also the 1CT model exhibits structured connectivity, with synaptic strength depending on the tuning similarity between the presynaptic and the postsynaptic neuron. Pairs of neurons with stronger tuning similarity (dissimilarity) have stronger mutual inhibition (excitation); see <xref rid="figS1" ref-type="fig">Supplementary Fig. S1D</xref>.</p>
<p>We compared the coding performance of the E-I model with that of a fully connected 1CT model. Both models received the same set of stimulus features and performed the same computation. In the 1CT model, tuning parameters were drawn from the same distribution as used for the E neurons in the E-I model. We used the same membrane time constant <italic>τ</italic> in both models, while the metabolic constants (<italic>β</italic> of the E-I model and <italic>β</italic><sub>1</sub> of the 1CT model) and the noise intensity (<italic>σ</italic> of the E-I model and <italic>σ</italic><sub>1</sub> of the 1CT model) were chosen such as to optimize the average loss for each model (<xref rid="fig5" ref-type="fig">Fig. 5B</xref> for E-I model, <xref rid="figS1" ref-type="fig">Supplementary Fig. S1F-G</xref> for 1CT model). Parameters of the 1CT model are listed in the <xref rid="tblS1" ref-type="table">Supplementary Table S1</xref>. A qualitative comparison of the E-I and the 1CT model showed that with optimal parameters, both models accurately tracked multiple target signals (<xref rid="fig1" ref-type="fig">Fig. 1G</xref> and <xref rid="figS1" ref-type="fig">Supplementary Fig. S1E</xref>).</p>
<p>To compare the performance of the E-I and the 1CT models also quantitatively, we measured the average encoding error (RMSE), metabolic cost (MC) and loss of each model. The RMSE and the MC in the 1CT model were measured as in <xref ref-type="disp-formula" rid="eqn37">Eq. 37</xref> and <xref ref-type="disp-formula" rid="eqn38">38</xref>, while the average loss of each model was evaluated as follows:
<disp-formula id="eqnS_10">
<graphic xlink:href="590955v1_eqnS_10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Unless mentioned otherwise, we weighted stronger the encoding error compared to the metabolic cost and used <italic>g</italic><sub><italic>L</italic></sub> = 0.7. Note that our comparison of the losses is conservative, because the metabolic cost is defined as a sum of activities across neurons (<xref ref-type="disp-formula" rid="eqn38">Eq. 38</xref>) and the total number of neurons in the E-I model (<italic>N</italic><sup><italic>E</italic></sup> + <italic>N</italic><sup><italic>I</italic></sup>) is larger than the number of neurons in the 1CT model (<italic>N</italic><sup>1CT</sup> = <italic>N</italic><sup><italic>E</italic></sup>).</p>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>Table of default model parameters for the efficient network with one cell type.</title>
<p>The parameters <italic>N, M, τ</italic> and <italic>σ</italic><sub><italic>w</italic></sub> are chosen identical to the E-I network (see <xref rid="tbl1" ref-type="table">Table 1</xref> in the main text). Parameters <italic>σ</italic><sub>1</sub> and <italic>β</italic><sub>1</sub> are determined as values that maximize network efficiency (see section “Performance measures” in the main text).</p></caption>
<graphic xlink:href="590955v1_tblS1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</sec>
<sec id="s7">
<title>Supplementary Figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Efficient spiking model with one cell type.</title>
<p><bold>(A)</bold> Schematic of efficient coding with a single spiking neuron with positive weight. The target signal (bottom, black) integrates the input signal (top). The neuron spikes to keep the readout of its activity (magenta) close to the target signal.</p>
<p><bold>(B)</bold> Schematic of the efficient 1CT model. Target signal <bold><italic>x</italic></bold>(<italic>t</italic>) is computed from stimulus features <bold><italic>s</italic></bold>(<italic>t</italic>). The network generates the estimate of the target signal with the population readouts of the spiking activity.</p>
<p><bold>(C)</bold> Schematic of excitatory (red) and inhibitory (blue) synaptic interactions in 1CT model. Neurons with similar selectivity inhibit each other (blue), while neurons with different selectivity excite each other (red). The same neuron is sending excitatory and inhibitory synaptic outputs.</p>
<p><bold>(D)</bold> Strength of recurrent synapses as a function of the tuning similarity.</p>
<p><bold>(E)</bold> Simulation of the network with 1CT. Top three rows show the signal (black), and the estimate (magenta) in each of the 3 input dimensions.</p>
<p><bold>(F)</bold> Left: Root mean squared error (RMSE) as a function of the metabolic constant <italic>β</italic><sub>1</sub>. Right: Normalized metabolic cost (green) and normalized average loss (black) as a function of the metabolic constant <italic>β</italic><sub>1</sub>. The black arrow denotes the minimum of the loss and thus the optimal parameter <italic>β</italic><sub>1</sub>.</p>
<p><bold>(G)</bold> Same as in <bold>F</bold>, measured as a function of the noise intensity <italic>σ</italic><sub>1</sub>.</p>
<p><bold>(H)</bold> Average loss as a function of the weighting of the encoding error and the metabolic cost, <italic>g</italic><sub><italic>L</italic></sub>, in the E-I model (black) and in the 1CT model (magenta). For plots F-H, results were computed in 100 simulation trials of duration of 1 second of simulated time. For other parameters, see <xref rid="tbl1" ref-type="table">Table 1</xref> (E-I model) and <xref rid="tblS1" ref-type="table">Table S1</xref> (1CT model).</p></caption>
<graphic xlink:href="590955v1_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Tuning similarity and its relation to lateral excitation/inhibition.</title>
<p><bold>(A)</bold> Pair-wise tuning similarity for all pairs of E neurons. Tuning similarity between pairs of neurons is measured as the similarity of normalized tuning vectors.</p>
<p><bold>(B)</bold> Histogram of tuning similarity across all E-E pairs shown in <bold>A</bold>.</p>
<p><bold>(C)</bold> Tuning similarity to a single, randomly selected target neuron. Tuning similarity to a single neuron corresponds to a vector from the tuning similarity matrix in <bold>A</bold>. We sorted the tuning similarity to a single neuron from smallest to biggest value. Neurons with negative similarity are grouped as neurons with different tuning, while neurons with positive tuning similarity are grouped as neurons with similar tuning.</p>
<p><bold>(D)</bold> Histogram of tuning similarity of E neurons to the target neuron shown in <bold>C</bold>. With distribution of tuning parameters symmetric around zero as used in our study, any choice of target neuron gives approximately the same number of neurons with similar and different selectivity.</p>
<p><bold>(E)</bold> Top: Trial and neuron-averaged deviation of the instantaneous firing rate from the baseline firing rate, for the population of I (top) and E (bottom) neurons with similar tuning (magenta) and different tuning (gray). The baseline firing rates were 6.8 Hz and 12.7 Hz in the E and I cell types, respectively. The stimulation intensity is <italic>a</italic><sub><italic>p</italic></sub> = 0.4. Figure shows the mean ± standard error of the mean (SEM), with SEM capturing the variance across neurons and across trials. Bottom: Scatter plot of the tuning similarity versus effective connectivity in I (top) and E neurons (bottom). Tuning similarity and effective connectivity are measured with respect to the (same) target neuron. Red line marks zero effective connectivity and magenta line marks the least-squares line.</p>
<p><bold>(F)</bold> Same as in <bold>E</bold>, for stimulation intensity of <italic>a</italic><sub><italic>p</italic></sub> = 0.8.</p>
<p><bold>(G)</bold> Same as in <bold>E</bold>, in presence of weak feedforward stimulus, showing the activity of neurons with similar tuning (orange) and different tuning (gray) to the stimulated neuron. We used the stimulation intensity at threshold (<italic>a</italic><sub><italic>p</italic></sub> = 1.0). The feedforward stimulus was received by all E neurons and it induced, together with the external current, the mean firing rates of 7.3 Hz and 13.5 Hz in E and I neurons, respectively. For model parameters, see <xref rid="tbl1" ref-type="table">Table 1</xref>. This figure is related to the <xref rid="fig2" ref-type="fig">Fig. 2</xref> in the main paper.</p></caption>
<graphic xlink:href="590955v1_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Effect of complete and partial removal of connectivity structure and of minimal perturbation of synaptic weights.</title>
<p><bold>(A)</bold> Average coefficient of variation in networks with fully unstructured connectivity. The dashed line marks the same measure in a structured network.</p>
<p><bold>(B)</bold> Mean firing rate in E (top) and I neurons (bottom) in networks with partial removal of connectivity structure in recurrent connectivity. Partial removal of connectivity structure is achieved by limiting the permutation of synaptic connectivity to neuronal pairs with similar tuning.</p>
<p><bold>(C)</bold> Same as in <bold>B</bold>, showing the coefficient of variation of spiking activity.</p>
<p><bold>(D)</bold> Same as in <bold>B</bold>, showing the average net synaptic current, neural correlate of the average E-I balance.</p>
<p><bold>(E)</bold> Same as in <bold>B</bold>, showing the correlation coefficient of synaptic currents, neural correlate of the instantaneous E-I balance.</p>
<p><bold>(F)</bold> Encoding error in networks with partially unstructured recurrent connectivity, relative to the encoding error of the structured network (dashed line). From left to right: we perturb synaptic weights in E-I, I-I, I-E and in all three recurrent connectivities at once.</p>
<p><bold>(G)</bold> Same as in <bold>F</bold>, showing the metabolic cost on spiking in E and I populations, relative to the metabolic cost in the structured network (dashed line).</p>
<p><bold>(H)</bold> The RMSE (top) and the normalized metabolic cost (green) and average loss (black) average firing rate (bottom) in E and I cell type, as a function of the strength of perturbation of the synaptic connectivity.</p>
<p><bold>(I)</bold> Average firing rate (top) and the coefficient of variation (bottom) as a function of the strength of random perturbation of all recurrent connectivities.</p>
<p><bold>(J)</bold> Target signals, E estimates and I estimates in three input dimensions (three top rows), spike trains (fourth row) and the instantaneous estimate of the firing rate of E and I populations (bottom) in a single simulation trial, with significant perturbation of recurrent connectivity (perturbation strength of 0.5, see Methods). In spite of a relatively strong perturbation, the network shows excellent encoding of the target signal. Other parameters are in <xref rid="tbl1" ref-type="table">Table 1</xref>. This figure is related to the <xref rid="fig3" ref-type="fig">Fig. 3</xref> in the main paper.</p></caption>
<graphic xlink:href="590955v1_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4.</label>
<caption><title>Lateral excitation/inhibition in models with full and partial removal of connectivity structure.</title>
<p><bold>(A)</bold> Average deviation of the instantaneous firing rate from the baseline for the population of I (top) and E (bottom) neurons in networks with fully removed structure in E-I (left), I-E (middle) and in all connectivity matrices (right). We show the mean ± SEM for neurons with similar (ochre) and different (green) tuning to the stimulated neuron. The mean traces of the network with structured connectivity is shown for comparison, magenta and gray for similar and different tuning, respectively.</p>
<p><bold>(B)</bold> Same as in <bold>A</bold>, for partial (fine-grained) removal of connectivity structure. Partial removal of connectivity structure is achieved by limiting the permutation of synaptic weights among neurons with similar tuning. Such manipulation maintains the like-like connectivity structure, but removes any structure beyond the like-like.</p>
<p><bold>(C)</bold> Scatter plot of tuning similarity versus effective connectivity for networks with partial removal of connectivity structure. In such networks, the specificity of effective connectivity with respect to tuning similarity is largely preserved, in particular in E neurons. For all results, we iterated simulations in 200 trials, where we varied randomly the membrane potential noise and initial conditions of the membrane potentials in each trial, while tuning and synaptic parameters were kept fixed. In all cases, we used stimulation intensity at threshold (<italic>a</italic><sub><italic>p</italic></sub> = 1.0). For model parameters, see <xref rid="tbl1" ref-type="table">Table 1</xref>. This figure is related to the <xref rid="fig3" ref-type="fig">Fig. 3</xref> in the main paper.</p></caption>
<graphic xlink:href="590955v1_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5.</label>
<caption><title>Dependence of coding efficiency and neural dynamics on the ratio of mean I-I to E-I connectivity, computed by changing the mean E-I connectivity.</title>
<p><bold>(A)</bold> Top: Encoding error (RMSE) of the E (red) and I (blue) estimates. Bottom: Normalized metabolic cost and average loss.</p>
<p><bold>(B)</bold> Average firing rate (top), and average coefficient of variation (bottom) in E and I cell type.</p>
<p><bold>(C)</bold> Average imbalance and instantaneous balance of synaptic currents in E and I neurons.</p>
<p><bold>(D)</bold> Top: Optimal ratio of mean I-I to E-I connectivity as a function of the weighting of the average loss of E and I cell type. Bottom: Same as on top, as a function of the weighting between the error and the cost. Black triangles mark weightings that are typically used to estimate optimal efficiency. For other parameters, see <xref rid="tbl1" ref-type="table">Table 1</xref>. This figure is related to the <xref rid="fig6" ref-type="fig">Fig. 6</xref> in the main paper.</p></caption>
<graphic xlink:href="590955v1_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6.</label>
<caption><title>Effect of stimulus properties on efficient neural coding and dynamics.</title>
<p><bold>(A)</bold> Average firing rate (top), and average coefficient of variation (bottom) in E and I cell type, as a function of the time constant of the stimulus <italic>τ</italic><sub><italic>s</italic></sub>.</p>
<p><bold>(B)</bold> Average imbalance (top) and instantaneous balance (bottom) as a function of the time constant of the stimulus <italic>τ</italic><sub><italic>s</italic></sub>.</p>
<p><bold>(C-D)</bold> Same as in <bold>A-B</bold>, as a function of the number of encoded variables. For parameters, see <xref rid="tbl1" ref-type="table">Table 1</xref>. This figure is related to the <xref rid="fig7" ref-type="fig">Fig. 7</xref> in the main paper.</p></caption>
<graphic xlink:href="590955v1_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99545.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Naud</surname>
<given-names>Richard</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Ottawa</institution>
</institution-wrap>
<city>Ottawa</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study offers a <bold>useful</bold> treatment of how the population of excitatory and inhibitory neurons integrates principles of energy efficiency in their coding strategies. The analysis provides a comprehensive characterisation of the model, highlighting the structured connectivity between excitatory and inhibitory neurons. However, the manuscript provides an <bold>incomplete</bold> motivation for parameter choices. Furthermore, the work is insufficiently contextualized within the literature, and some of the findings appear overlapping and incremental given previous work.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99545.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Koren et al. derive and analyse a spiking network model optimised to represent external signals using the minimum number of spikes. Unlike most prior work using a similar setup, the network includes separate populations of excitatory and inhibitory neurons. The authors show that the optimised connectivity has a like-to-like structure, leading to the experimentally observed phenomenon of feature competition. They also characterise the impact of various (hyper)parameters, such as adaptation timescale, ratio of excitatory to inhibitory cells, regularisation strength, and background current. These results add useful biological realism to a particular model of efficient coding. However, not all claims seem fully supported by the evidence. Specifically, several biological features, such as the ratio of excitatory to inhibitory neurons, which the authors claim to explain through efficient coding, might be contingent on arbitrary modelling choices. In addition, earlier work has already established the importance of structured connectivity for feature competition. A clearer presentation of modelling choices, limitations, and prior work could improve the manuscript.</p>
<p>Major comments:</p>
<p>(1) Much is made of the 4:1 ratio between excitatory and inhibitory neurons, which the authors claim to explain through efficient coding. I see two issues with this conclusion: (i) The 4:1 ratio is specific to rodents; humans have an approximate 2:1 ratio (see Fang &amp; Xia et al., Science 2022 and references therein); (ii) the optimal ratio in the model depends on a seemingly arbitrary choice of hyperparameters, particularly the weighting of encoding error versus metabolic cost. This second concern applies to several other results, including the strength of inhibitory versus excitatory synapses. While the model can, therefore, be made consistent with biological data, this requires auxiliary assumptions.</p>
<p>(2) A growing body of evidence supports the importance of structured E-I and I-E connectivity for feature selectivity and response to perturbations. For example, this is a major conclusion from the Oldenburg paper (reference 62 in the manuscript), which includes extensive modelling work. Similar conclusions can be found in work from Znamenskiy and colleagues (experiments and spiking network model; bioRxiv 2018, Neuron 2023 (ref. 82)), Sadeh &amp; Clopath (rate network; eLife, 2020), and Mackwood et al. (rate network with plasticity; eLife, 2021). The current manuscript adds to this evidence by showing that (a particular implementation of) efficient coding in spiking networks leads to structured connectivity. The fact that this structured connectivity then explains perturbation responses is, in the light of earlier findings, not new.</p>
<p>(3) The model's limitations are hard to discern, being relegated to the manuscript's last and rather equivocal paragraph. For instance, the lack of recurrent excitation, crucial in neural dynamics and computation, likely influences the results: neuronal time constants must be as large as the target readout (Figure 4), presumably because the network cannot integrate the signal without recurrent excitation. However, this and other results are not presented in tandem with relevant caveats.</p>
<p>(4) On repeated occasions, results from the model are referred to as predictions claimed to match the data. A prediction is a statement about what will happen in the future - but most of the &quot;predictions&quot; from the model are actually findings that broadly match earlier experimental results, making them &quot;postdictions&quot;. This distinction is important: compared to postdictions, predictions are a much stronger test because they are falsifiable. This is especially relevant given (my impression) that key parameters of the model were tweaked to match the data.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99545.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this work, the authors present a biologically plausible, efficient E-I spiking network model and study various aspects of the model and its relation to experimental observations. This includes a derivation of the network into two (E-I) populations, the study of single-neuron perturbations and lateral-inhibition, the study of the effects of adaptation and metabolic cost, and considerations of optimal parameters. From this, they conclude that their work puts forth a plausible implementation of efficient coding that matches several experimental findings, including feature-specific inhibition, tight instantaneous balance, a 4 to 1 ratio of excitatory to inhibitory neurons, and a 3 to 1 ratio of I-I to E-I connectivity strength. It thus argues that some of these observations may come as a direct consequence of efficient coding.</p>
<p>Strengths:</p>
<p>While many network implementations of efficient coding have been developed, such normative models are often abstract and lacking sufficient detail to compare directly to experiments. The intention of this work to produce a more plausible and efficient spiking model and compare it with experimental data is important and necessary in order to test these models.</p>
<p>In rigorously deriving the model with real physical units, this work maps efficient spiking networks onto other more classical biophysical spiking neuron models. It also attempts to compare the model to recent single-neuron perturbation experiments, as well as some long-standing puzzles about neural circuits, such as the presence of separate excitatory and inhibitory neurons, the ratio of excitatory to inhibitory neurons, and E/I balance. One of the primary goals of this paper, to determine if these are merely biological constraints or come from some normative efficient coding objective, is also important.</p>
<p>Though several of the observations have been reported and studied before (see below), this work arguably studies them in more depth, which could be useful for comparing more directly to experiments.</p>
<p>Weaknesses:</p>
<p>Though the text of the paper may suggest otherwise, many of the modeling choices and observations found in the paper have been introduced in previous work on efficient spiking models, thereby making this work somewhat repetitive and incremental at times. This includes the derivation of the network into separate excitatory and inhibitory populations, discussion of physical units, comparison of voltage versus spike-timing correlations, and instantaneous E/I balance, all of which can be found in one of the first efficient spiking network papers (Boerlin et al. 2013), as well as in subsequent papers. Metabolic cost and slow adaptation currents were also presented in a previous study (Gutierrez &amp; Deneve 2019). Though it is perfectly fine and reasonable to build upon these previous studies, the language of the text gives them insufficient credit.</p>
<p>Furthermore, the paper makes several claims of optimality that are not convincing enough, as they are only verified by a limited parameter sweep of single parameters at a time, are unintuitive and may be in conflict with previous findings of efficient spiking networks. This includes the following. Coding error (RMSE) has a minimum at intermediate metabolic cost (Figure 5B), despite the fact that intuitively, zero metabolic cost would indicate that the network is solely minimizing coding error and that previous work has suggested that additional costs bias the output. Coding error also appears to have a minimum at intermediate values of the ratio of E to I neurons (effectively the number of I neurons) and the number of encoded variables (Figures 6D, 7B). These both have to do with the redundancy in the network (number of neurons for each encoded variable), and previous work suggests that networks can code for arbitrary numbers of variables provided the redundancy is high enough (e.g., Calaim et al. 2022). Lastly, the performance of the E-I variant of the network is shown to be better than that of a single cell type (1CT: Figure 7C, D). Given that the E-I network is performing a similar computation as to the 1CT model but with more neurons (i.e., instead of an E neuron directly providing lateral inhibition to its neighbor, it goes through an interneuron), this is unintuitive and again not supported by previous work. These may be valid emergent properties of the E-I spiking network derived here, but their presentation and description are not sufficient to determine this.</p>
<p>Alternatively, the methodology of the model suggests that ad hoc modeling choices may be playing a role. For example, an arbitrary weighting of coding error and metabolic cost of 0.7 to 0.3, respectively, is chosen without mention of how this affects the results. Furthermore, the scaling of synaptic weights appears to be controlled separately for each connection type in the network (Table 1), despite the fact that some of these quantities are likely linked in the optimal network derivation. Finally, the optimal threshold and metabolic constants are an order of magnitude larger than the synaptic weights (Table 1). All of these considerations suggest one of the following two possibilities. One, the model has a substantial number of unconstrained parameters to tune, in which case more parameter sweeps would be necessary to definitively make claims of optimality. Or two, parameters are being decoupled from those constrained by the optimal derivation, and the optima simply corresponds to the values that should come out of the derivation.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99545.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In their paper the authors tackle three things at once in a theoretical model: how can spiking neural networks perform efficient coding, how can such networks limit the energy use at the same time, and how can this be done in a more biologically realistic way than previous work?</p>
<p>They start by working from a long-running theory on how networks operating in a precisely balanced state can perform efficient coding. First, they assume split networks of excitatory (E) and inhibitory (I) neurons. The E neurons have the task to represent some lower dimensional input signal, and the I neurons have the task to represent the signal represented by the E neurons. Additionally, the E and I populations should minimize an energy cost represented by the sum of all spikes. All this results in two loss functions for the E and I populations, and the networks are then derived by assuming E and I neurons should only spike if this improves their respective loss. This results in networks of spiking neurons that live in a balanced state, and can accurately represent the network inputs.</p>
<p>They then investigate in-depth different aspects of the resulting networks, such as responses to perturbations, the effect of following Dale's law, spiking statistics, the excitation (E)/inhibition (I) balance, optimal E/I cell ratios, and others. Overall, they expand on previous work by taking a more biological angle on the theory and showing the networks can operate in a biologically realistic regime.</p>
<p>Strengths:</p>
<p>(1) The authors take a much more biological angle on the efficient spiking networks theory than previous work, which is an essential contribution to the field.</p>
<p>(2) They make a very extensive investigation of many aspects of the network in this context, and do so thoroughly.</p>
<p>(3) They put sensible constraints on their networks, while still maintaining the good properties these networks should have.</p>
<p>Weaknesses:</p>
<p>(1) The paper has somewhat overstated the significance of their theoretical contributions, and should make much clearer what aspects of the derivations are novel. Large parts were done in very similar ways in previous papers. Specifically: the split into E and I neurons was also done in Boerlin et al (2008) and in Barrett et al (2016). Defining the networks in terms of realistic units was already done by Boerlin et al (2008). It would also be worth it to discuss Barrett et al (2016) specifically more, as there they also use split E/I networks and perform biologically relevant experiments.</p>
<p>(2) It is not clear from an optimization perspective why the split into E and I neurons and following Dale's law would be beneficial. While the constraints of Dale's law are sensible (splitting the population in E and I neurons, and removing any non-Dalian connection), they are imposed from biology and not from any coding principles. A discussion of how this could be done would be much appreciated, and in the main text, this should be made clear.</p>
<p>(3) Related to the previous point, the claim that the network with split E and I neurons has a lower average loss than a 1 cell-type (1-CT) network seems incorrect to me. Only the E population coding error should be compared to the 1-CT network loss, or the sum of the E and I populations (not their average). In my author recommendations, I go more in-depth on this point.</p>
<p>(4) While the paper is supposed to bring the balanced spiking networks they consider in a more experimentally relevant context, for experimental audiences I don't think it is easy to follow how the model works, and I recommend reworking both the main text and methods to improve on that aspect.</p>
<p>Assessment and context:</p>
<p>Overall, although much of the underlying theory is not necessarily new, the work provides an important addition to the field. The authors succeeded well in their goal of making the networks more biologically realistic, and incorporating aspects of energy efficiency. For computational neuroscientists, this paper is a good example of how to build models that link well to experimental knowledge and constraints, while still being computationally and mathematically tractable. For experimental readers, the model provides a clearer link between efficient coding spiking networks to known experimental constraints and provides a few predictions.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99545.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Koren</surname>
<given-names>Veronika</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2920-2717</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Malerba</surname>
<given-names>Simone Blanco</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4467-5988</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Schwalger</surname>
<given-names>Tilo</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5422-3723</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Panzeri</surname>
<given-names>Stefano</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1700-8909</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>eLife assessment</bold></p>
<p>This study offers a useful treatment of how the population of excitatory and inhibitory neurons integrates principles of energy efficiency in their coding strategies. The analysis provides a comprehensive characterisation of the model, highlighting the structured connectivity between excitatory and inhibitory neurons. However, the manuscript provides an incomplete motivation for parameter choices. Furthermore, the work is insufficiently contextualized within the literature, and some of the findings appear overlapping and incremental given previous work.</p>
</disp-quote>
<p>We thank the Reviewers and the Reviewing Editor for taking time to provide extremely valuable suggestions and comments, which will help us to substantially improve our paper. In what follows we summarize our current plan to improve the paper taking up on their suggestions.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary: Koren et al. derive and analyse a spiking network model optimised to represent external signals using the minimum number of spikes. Unlike most prior work using a similar setup, the network includes separate populations of excitatory and inhibitory neurons. The authors show that the optimised connectivity has a like-to-like structure, leading to the experimentally observed phenomenon of feature competition. They also characterise the impact of various (hyper)parameters, such as adaptation timescale, ratio of excitatory to inhibitory cells, regularisation strength, and background current. These results add useful biological realism to a particular model of efficient coding. However, not all claims seem fully supported by the evidence. Specifically, several biological features, such as the ratio of excitatory to inhibitory neurons, which the authors claim to explain through efficient coding, might be contingent on arbitrary modelling choices. In addition, earlier work has already established the importance of structured connectivity for feature competition. A clearer presentation of modelling choices, limitations, and prior work could improve the manuscript.</p>
</disp-quote>
<p>Thanks for these insights and for this summary of our work.</p>
<disp-quote content-type="editor-comment">
<p>Major comments:</p>
<p>(1) Much is made of the 4:1 ratio between excitatory and inhibitory neurons, which the authors claim to explain through efficient coding. I see two issues with this conclusion: (i) The 4:1 ratio is specific to rodents; humans have an approximate 2:1 ratio (see Fang &amp; Xia et al., Science 2022 and references therein); (ii) the optimal ratio in the model depends on a seemingly arbitrary choice of hyperparameters, particularly the weighting of encoding error versus metabolic cost. This second concern applies to several other results, including the strength of inhibitory versus excitatory synapses. While the model can, therefore, be made consistent with biological data, this requires auxiliary assumptions.</p>
</disp-quote>
<p>We will describe better the ratio of numbers of E and I neurons found in real data, as suggested. The first submission already contained an analysis of how this ratio of neuron numbers depends on the weighting of the loss of E and I neurons and on the relative weighting of the encoding error vs the metabolic cost in the loss function (see Fig 6E). We will make sure that these results are suitably expanded and better emphasized in revision. We will also include new analysis of dependence of optimal parameters on the relative weighting of encoding error vs metabolic cost in the loss function when studying other parameters (namely: noise intensity, metabolic constant, ratio of mean I-I to E-I connectivity, time constants of single E and I neurons).</p>
<disp-quote content-type="editor-comment">
<p>(2) A growing body of evidence supports the importance of structured E-I and I-E connectivity for feature selectivity and response to perturbations. For example, this is a major conclusion from the Oldenburg paper (reference 62 in the manuscript), which includes extensive modelling work. Similar conclusions can be found in work from Znamenskiy and colleagues (experiments and spiking network model; bioRxiv 2018, Neuron 2023 (ref. 82)), Sadeh &amp; Clopath (rate network; eLife, 2020), and Mackwood et al. (rate network with plasticity; eLife, 2021). The current manuscript adds to this evidence by showing that (a particular implementation of) efficient coding in spiking networks leads to structured connectivity. The fact that this structured connectivity then explains perturbation responses is, in the light of earlier findings, not new.</p>
</disp-quote>
<p>We agree that the main contribution of our manuscript in this respect is to show how efficient coding in spiking networks can lead to structured connectivity similar to those proposed in the above papers. We apologize if this was not clear enough in the previous version. We will make it clearer in revision.  We nevertheless think it useful to report the effects of perturbations within this network because the structure derived in our network is not identical to those studied in the above paper, and because these results give information about how lateral inhibition works in this network. Thus, we will keep presenting it in the revised version, although we will de-emphasize and simplify its presentation to give more emphasis to the novelty of the derivation of this connectivity rule from the principles of efficient coding.</p>
<disp-quote content-type="editor-comment">
<p>(3) The model's limitations are hard to discern, being relegated to the manuscript's last and rather equivocal paragraph. For instance, the lack of recurrent excitation, crucial in neural dynamics and computation, likely influences the results: neuronal time constants must be as large as the target readout (Figure 4), presumably because the network cannot integrate the signal without recurrent excitation. However, this and other results are not presented in tandem with relevant caveats.</p>
</disp-quote>
<p>We will improve the Limitations paragraph in Discussion, and also anticipate caveats in tandem with results when needed, as suggested.</p>
<disp-quote content-type="editor-comment">
<p>(4) On repeated occasions, results from the model are referred to as predictions claimed to match the data. A prediction is a statement about what will happen in the future - but most of the &quot;predictions&quot; from the model are actually findings that broadly match earlier experimental results, making them &quot;postdictions&quot;.</p>
</disp-quote>
<p>This distinction is important: compared to postdictions, predictions are a much stronger test because they are falsifiable. This is especially relevant given (my impression) that key parameters of the model were tweaked to match the data.</p>
<p>We will better distinguish between pre- and post-dictions  in revision.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary: In this work, the authors present a biologically plausible, efficient E-I spiking network model and study various aspects of the model and its relation to experimental observations. This includes a derivation of the network into two (E-I) populations, the study of single-neuron perturbations and lateral-inhibition, the study of the effects of adaptation and metabolic cost, and considerations of optimal parameters. From this, they conclude that their work puts forth a plausible implementation of efficient coding that matches several experimental findings, including feature-specific inhibition, tight instantaneous balance, a 4 to 1 ratio of excitatory to inhibitory neurons, and a 3 to 1 ratio of I-I to E-I connectivity strength. It thus argues that some of these observations may come as a direct consequence of efficient coding.</p>
<p>Strengths:</p>
<p>While many network implementations of efficient coding have been developed, such normative models are often abstract and lacking sufficient detail to compare directly to experiments. The intention of this work to produce a more plausible and efficient spiking model and compare it with experimental data is important and necessary in order to test these models.</p>
<p>In rigorously deriving the model with real physical units, this work maps efficient spiking networks onto other more classical biophysical spiking neuron models. It also attempts to compare the model to recent single-neuron perturbation experiments, as well as some long-standing puzzles about neural circuits, such as the presence of separate excitatory and inhibitory neurons, the ratio of excitatory to inhibitory neurons, and E/I balance. One of the primary goals of this paper, to determine if these are merely biological constraints or come from some normative efficient coding objective, is also important.</p>
<p>Though several of the observations have been reported and studied before (see below), this work arguably studies them in more depth, which could be useful for comparing more directly to experiments.</p>
</disp-quote>
<p>Thanks for these insights and for the kind words of appreciation of the strengths of our work.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>Though the text of the paper may suggest otherwise, many of the modeling choices and observations found in the paper have been introduced in previous work on efficient spiking models, thereby making this work somewhat repetitive and incremental at times. This includes the derivation of the network into separate excitatory and inhibitory populations, discussion of physical units, comparison of voltage versus spike-timing correlations, and instantaneous E/I balance, all of which can be found in one of the first efficient spiking network papers (Boerlin et al. 2013), as well as in subsequent papers. Metabolic cost and slow adaptation currents were also presented in a previous study (Gutierrez &amp; Deneve 2019). Though it is perfectly fine and reasonable to build upon these previous studies, the language of the text gives them insufficient credit.</p>
</disp-quote>
<p>We will improve the text to make sure that credit to previous studies is more precisely and more clearly given.</p>
<disp-quote content-type="editor-comment">
<p>Furthermore, the paper makes several claims of optimality that are not convincing enough, as they are only verified by a limited parameter sweep of single parameters at a time, are unintuitive and may be in conflict with previous findings of efficient spiking networks. This includes the following. Coding error (RMSE) has a minimum at intermediate metabolic cost (Figure 5B), despite the fact that intuitively, zero metabolic cost would indicate that the network is solely minimizing coding error and that previous work has suggested that additional costs bias the output. Coding error also appears to have a minimum at intermediate values of the ratio of E to I neurons (effectively the number of I neurons) and the number of encoded variables (Figures 6D, 7B). These both have to do with the redundancy in the network (number of neurons for each encoded variable), and previous work suggests that networks can code for arbitrary numbers of variables provided the redundancy is high enough (e.g., Calaim et al. 2022). Lastly, the performance of the E-I variant of the network is shown to be better than that of a single cell type (1CT: Figure 7C, D). Given that the E-I network is performing a similar computation as to the 1CT model but with more neurons (i.e., instead of an E neuron directly providing lateral inhibition to its neighbor, it goes through an interneuron), this is unintuitive and again not supported by previous work. These may be valid emergent properties of the E-I spiking network derived here, but their presentation and description are not sufficient to determine this.</p>
</disp-quote>
<p>We are addressing this issue in two ways. First, we will present results of joint sweeps of variations of pairs of parameters whose joint variations are expected to influence optimality in a way that cannot be understood varying one parameter at a time. Namely we plan to vary jointly the noise intensity and the metabolic constant, as well as the ratio of E to I neuron numbers and the ratio of mean I-I to E-I connectivity. Second, we will individuate a reasonable/realistic range of possible variations of each individual parameter and then perform a Monte Carlo search for the optimal point within this range, and compare the so-obtained results with those obtained from the understanding gained from varying one or two parameters at a time.  We will also add the suggested citation to Calaim et al. 2022 in regard to the points discussed above.</p>
<p>We will improve the comparison between the Excitatory-Inhibitory and the 1-Cell-Type model (see reply to the suggestions of Referee 3 for more details).</p>
<disp-quote content-type="editor-comment">
<p>Alternatively, the methodology of the model suggests that ad hoc modeling choices may be playing a role. For example, an arbitrary weighting of coding error and metabolic cost of 0.7 to 0.3, respectively, is chosen without mention of how this affects the results. Furthermore, the scaling of synaptic weights appears to be controlled separately for each connection type in the network (Table 1), despite the fact that some of these quantities are likely linked in the optimal network derivation. Finally, the optimal threshold and metabolic constants are an order of magnitude larger than the synaptic weights (Table 1). All of these considerations suggest one of the following two possibilities. One, the model has a substantial number of unconstrained parameters to tune, in which case more parameter sweeps would be necessary to definitively make claims of optimality. Or two, parameters are being decoupled from those constrained by the optimal derivation, and the optima simply corresponds to the values that should come out of the derivation.</p>
</disp-quote>
<p>In the previously submitted manuscript we presented both the encoding error and the metabolic cost separately as a function of the parameters, so that readers could get an understanding of how stable optimal parameters would be to the change of the relative weighting of encoding error and metabolic cost. We will improve this work by adding the suggested calculations to provide quantitative measures of the dependence of the optimal network parameters and configurations on this relative weighting.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary: In their paper the authors tackle three things at once in a theoretical model: how can spiking neural networks perform efficient coding, how can such networks limit the energy use at the same time, and how can this be done in a more biologically realistic way than previous work?</p>
<p>They start by working from a long-running theory on how networks operating in a precisely balanced state can perform efficient coding. First, they assume split networks of excitatory (E) and inhibitory (I) neurons. The E neurons have the task to represent some lower dimensional input signal, and the I neurons have the task to represent the signal represented by the E neurons. Additionally, the E and I populations should minimize an energy cost represented by the sum of all spikes. All this results in two loss functions for the E and I populations, and the networks are then derived by assuming E and I neurons should only spike if this improves their respective loss. This results in networks of spiking neurons that live in a balanced state, and can accurately represent the network inputs.</p>
<p>They then investigate in-depth different aspects of the resulting networks, such as responses to perturbations, the effect of following Dale's law, spiking statistics, the excitation (E)/inhibition (I) balance, optimal E/I cell ratios, and others. Overall, they expand on previous work by taking a more biological angle on the theory and showing the networks can operate in a biologically realistic regime.</p>
<p>Strengths:</p>
<p>(1) The authors take a much more biological angle on the efficient spiking networks theory than previous work, which is an essential contribution to the field.</p>
<p>(2) They make a very extensive investigation of many aspects of the network in this context, and do so thoroughly.</p>
<p>(3) They put sensible constraints on their networks, while still maintaining the good properties these networks should have.</p>
</disp-quote>
<p>Thanks for this summary and for these kind words of appreciation of the strengths of our work.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1) The paper has somewhat overstated the significance of their theoretical contributions, and should make much clearer what aspects of the derivations are novel. Large parts were done in very similar ways in previous papers. Specifically: the split into E and I neurons was also done in Boerlin et al (2008) and in Barrett et al (2016). Defining the networks in terms of realistic units was already done by Boerlin et al (2008). It would also be worth it to discuss Barrett et al (2016) specifically more, as there they also use split E/I networks and perform biologically relevant experiments.</p>
</disp-quote>
<p>We will improve the text to make sure that credit to previous studies is more precisely and more clearly given.</p>
<disp-quote content-type="editor-comment">
<p>(2) It is not clear from an optimization perspective why the split into E and I neurons and following Dale's law would be beneficial. While the constraints of Dale's law are sensible (splitting the population in E and I neurons, and removing any non-Dalian connection), they are imposed from biology and not from any coding principles. A discussion of how this could be done would be much appreciated, and in the main text, this should be made clear.</p>
</disp-quote>
<p>We indeed removed non-Dalian connections because having only connections respecting Dale’s law is a major constraint for biological plausibility. Our logic was to consider efficient coding within the space of networks that satisfy this (and other) biological plausibility constraints. We did not intend to claim that removing the non-Dalian connections was the result of an analytical optimization. However, to get better insights into how Dale’s Law constrains or influences the design of efficient networks, we added a comparison of the coding properties of networks that either do or do not satisfy Dale’s law. We apologize if this was not sufficiently clear in the previous version and we will clarify this in revision.</p>
<disp-quote content-type="editor-comment">
<p>(3) Related to the previous point, the claim that the network with split E and I neurons has a lower average loss than a 1 cell-type (1-CT) network seems incorrect to me. Only the E population coding error should be compared to the 1-CT network loss, or the sum of the E and I populations (not their average). In my author recommendations, I go more in-depth on this point.</p>
</disp-quote>
<p>We will perform the suggested detailed comparisons between the network loss in the 1CT-model and E-I model and then revise or refine conclusions if and as needed, according to the results we will obtain.</p>
<disp-quote content-type="editor-comment">
<p>(4) While the paper is supposed to bring the balanced spiking networks they consider in a more experimentally relevant context, for experimental audiences I don't think it is easy to follow how the model works, and I recommend reworking both the main text and methods to improve on that aspect.</p>
</disp-quote>
<p>We will try to make the presentation of the model more accessible to a non-computational audience.</p>
<disp-quote content-type="editor-comment">
<p>Assessment and context: Overall, although much of the underlying theory is not necessarily new, the work provides an important addition to the field. The authors succeeded well in their goal of making the networks more biologically realistic, and incorporating aspects of energy efficiency. For computational neuroscientists, this paper is a good example of how to build models that link well to experimental knowledge and constraints, while still being computationally and mathematically tractable. For experimental readers, the model provides a clearer link between efficient coding spiking networks to known experimental constraints and provides a few predictions.</p>
</disp-quote>
<p>Thanks for these kind words. We will make sure that these points emerge more clearly and in a more accessible way from the revised paper.</p>
</body>
</sub-article>
</article>