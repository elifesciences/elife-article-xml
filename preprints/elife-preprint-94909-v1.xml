<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">94909</article-id>
<article-id pub-id-type="doi">10.7554/eLife.94909</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.94909.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Mining the neuroimaging literature</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Dockès</surname>
<given-names>Jérôme</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4087-5402</contrib-id>
<name>
<surname>Oudyk</surname>
<given-names>Kendra</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Torabi</surname>
<given-names>Mohammad</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>de la Vega</surname>
<given-names>Alejandro I</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9794-749X</contrib-id>
<name>
<surname>Poline</surname>
<given-names>Jean-Baptiste</given-names>
</name>
<email>jean-baptiste.poline@mcgill.ca</email>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>National Institute for Research in Digital Science and Technology (INRIA)</institution>, Paris, <country>France</country></aff>
<aff id="a2"><label>2</label><institution>Montreal Neurological Institute, McGill University</institution>, Montreal, <country>Canada</country></aff>
<aff id="a3"><label>3</label><institution>University of Texas at Austin</institution>, Austin, Texas, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Yan</surname>
<given-names>Chaogan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Chinese Academy of Sciences</institution>
</institution-wrap>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="others"><label>*</label><p>co-first authors</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-04-09">
<day>09</day>
<month>04</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP94909</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-12-22">
<day>22</day>
<month>12</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-11-23">
<day>23</day>
<month>11</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.30.564783"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Dockès et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Dockès et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-94909-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Automated analysis of the biomedical literature (<italic>literature-mining</italic>) offers a rich source of insights. However, such analysis requires collecting a large number of articles and extracting and processing their content. This task is often prohibitively difficult and time-consuming. Here, we provide tools to easily collect, process and annotate the biomedical literature. In particular, <ext-link ext-link-type="uri" xlink:href="https://neuroquery.github.io/pubget/">pubget</ext-link> is an efficient and reliable command-line tool for downloading articles in bulk from PubMed Central, extracting their contents and meta-data into convenient formats, and extracting and analyzing information such as stereotactic brain coordinates. <ext-link ext-link-type="uri" xlink:href="https://jeromedockes.github.io/labelbuddy/labelbuddy/current/">Labelbuddy</ext-link> is a lightweight local application for annotating text, which facilitates the extraction of complex information or the creation of ground-truth labels to validate automated information extraction methods. Further, we describe repositories where researchers can share their analysis code and their manual annotations in a format that facilitates re-use. These resources can help streamline text-mining and meta-science projects and make text-mining of the biomedical literature more accessible, effective, and reproducible. We describe a typical workflow based on these tools and illustrate it with several example projects.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>1) Clarifying co-first authorship between JD and KO by adding the author names to the pdf. 2) Adding an author that was forgotten in the first version. 3) updating results after fixing a bug; this is pointed out in the manuscript.</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/neurodatascience/literature_mining_paper">https://github.com/neurodatascience/literature_mining_paper</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/jeromedockes/fmri_participant_demographics">https://github.com/jeromedockes/fmri_participant_demographics</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<sec id="s1a">
<label>1.1</label>
<title>The need for literature mining is growing fast</title>
<p>More than 1 million papers are indexed by PubMed each year, about 2 papers per minute [<xref ref-type="bibr" rid="c6">Landhuis, 2016</xref>]. This vast and fast-growing collection of knowledge has the potential to accelerate scientific insight and discovery. However, this information is represented as unstructured natural language texts across heterogeneous sources, making it difficult for researchers to access and analyze. As the number of publications continues to grow, there is an increasing need for tools and frameworks to systematically extract information from the scientific literature.</p>
<p>To understand the state of science, researchers and practitioners rely on syntheses in the form of reviews, meta-analyses, and other types of meta-research (i.e., research on research). These can be thought of as forms of ‘literature-mining’, which is a special form of text mining where the goal is to extract information from the scientific literature. This type of research requires finding and collecting articles, systematically annotating information, and analyzing the resulting data. However, this painstaking work is traditionally performed manually, making it difficult to scale to the fast-growing literature. This process can be facilitated with user-friendly tools, but manual work is not easily reproduced. This is important because there is growing interest in continuously updating summaries of the literature [<xref ref-type="bibr" rid="c11">Simmonds et al., 2022</xref>], as well as growing concern over reproducibility of science in general [<xref ref-type="bibr" rid="c1">Baker, 2016</xref>].</p>
<p>Advances in natural language processing promise to automate many aspects of literature mining, potentially increasing scalability and reproducibility of meta-research. For example, specialized biomedical text mining models are able to automatically recognize key entities in natural language, such as proteins, genes, and drugs (for a review, see <xref ref-type="bibr" rid="c5">Huang et al., 2020</xref>). More recently, there has been a growing interest in domain general large language models, which promise to perform well on a variety of language tasks, including scientific literature mining (e.g., <xref ref-type="bibr" rid="c12">Tinn et al., 2023</xref>).</p>
<p>However, even as we move towards more highly-automated literature mining, there is still a need for manual work: we need ground-truth labels for training and evaluating these automated methods. Further, these projects require large datasets, which are difficult to collect and curate. This is more of a software-engineering task than a research task, for which researchers are generally not well-prepared. Thus, right now we need tools to automate as much of the literature mining process as we can, while making the remaining manual steps as easy and efficient as possible.</p>
<p>Here, we present <italic>litmining</italic>, an ecosystem to optimize the scalability, reproducibility, and accessibility of literature mining tasks, from collecting documents, to transforming them into usable formats, to manually and automatically extracting information. We offer a flexible environment that can be used for projects that have various needs. To illustrate this, we describe a variety of projects that use our ecosystem, from meta-analysis, to model evaluation, to automated and manual meta-research projects. In what follows, we will further expand on current approaches to literature mining, outline problems with those approaches, and describe our proposed workflow and set of tools. While this article focuses on the neuroimaging field, we demonstrate that our ecosystem may be used in a variety of contexts.</p>
</sec>
<sec id="s1b">
<label>1.2</label>
<title>Limitations of current approaches to literature data collection</title>
<p>We can divide approaches to literature data collection into three broad categories, each with its own disadvantages in terms accessibility, reproducibility, and scalability.</p>
<p>The first, simplest approach is to rely on an existing and open corpus of published articles and extracted information. In neuroimaging, there are two such corpora: NeuroSynth [<xref ref-type="bibr" rid="c15">Yarkoni et al., 2011</xref>] and NeuroQuery [<xref ref-type="bibr" rid="c3">Dockès et al., 2020</xref>]. They have been used extensively (especially NeuroSynth), reflecting the interest in using this type of data. However, these static article collections are limited in their coverage of the literature over time and topics, and they do not provide the full texts of papers, hence not suitable for many types of meta-research projects.</p>
<p>A second approach is to manually collect articles and extract information. In this case, a researcher typically performs a search on PubMed, downloads PDF files or reads the articles online, and then enters the relevant information into a spreadsheet. This is the most accessible approach, but it is a massive undertaking when many articles are involved; it is not scalable. Moreover, it is challenging to report this manual process completely and transparently, presenting a barrier to reproducibility.</p>
<p>The third and most scalable approach is to automate the data collection and curation. This involves writing and validating software to query a web repository’s Application Programmer Interface (API), download articles, parse the output, extract the metadata and content, and prepare the text for annotation and analysis. This often represents a prohibitive amount of work, especially if this step is to be reproducible. Redoing this work for each new project is an extremely inefficient use of time and energy for the research community in addition to being error-prone and variably reproducible.</p>
<p><xref rid="fig1" ref-type="fig">Figure 1</xref> summarizes the advantages and disadvantages of these three approaches, as well as our approach.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Advantages and disadvantages of different approaches data collection for literature mining. The first two approaches are not scalable, while the third is not accessible to researchers with lower technical expertise. We have aimed to make our approach as scalable, reproducible, and accessible as possible.</p></caption>
<graphic xlink:href="564783v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s1c">
<label>1.3</label>
<title>Our proposed workflow and ecosystem</title>
<p>In contrast to these workflows, we present both an ecosystem of well-tested tools and repositories, as well as a set of recommendations for easier, more scalable, and more reproducible literature-mining and meta-research projects. <xref rid="fig2" ref-type="fig">Figure 2</xref> shows our proposed open ecosystem and workflow for literature mining projects.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Our suggested workflow and litmining ecosystem of tools for efficient, reproducible meta-research. Our tool pubget performs the tasks of collecting documents and extracting content; this corpus can be stored in a dedicated OSF project. Our tools labelbuddy, pubget, and pubextract can be used to manually and automatically extract information. We have an open repository of labelbuddy annotations, where researchers can re-use, update, and add new annotations. For the step of analyzing the data, each project would have its own code, which we hope would be tracked and shared in its own repository on GitHub or elsewhere.</p></caption>
<graphic xlink:href="564783v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>For the first step of collecting documents and extracting content, we introduce <ext-link ext-link-type="uri" xlink:href="https://neuroquery.github.io/pubget/">pubget</ext-link>. This tool downloads open-access articles from PubMed Central and extracts their metadata and text. Further, for neuroimaging articles, it extracts the stereotactic coordinates of results in the 3-dimensional space of the brain. The extracted information is stored in a way that facilitates its subsequent use and increases interoperability with other tools, such as the NiMARE library for neuroimaging meta-analysis [<xref ref-type="bibr" rid="c10">Salo et al., 2018</xref>].</p>
<p>To extract information from papers, we introduce a text-annotation software, <ext-link ext-link-type="uri" xlink:href="https://jeromedockes.github.io/labelbuddy/labelbuddy/current/">labelbuddy</ext-link>. This tool is critical as many meta-research projects require some manual annotation, for example, to provide validation data for an automatic extraction algorithm, or when automation is not feasible because the corpus is too small. While there already are existing tools for annotation, most are meant for a large team of annotators and generally not adapted to the annotation of research articles. labelbuddy is a lightweight solution and is easy to integrate in any workflow, making manual annotations as reproducible and scalable as possible, as well as interoperable with other tools in the <italic>limining</italic> ecosystem (such as pubget, see below).</p>
<p>When information may be extracted automatically, without the need for manual annotation, pubget is easily extensible through plugins, allowing extension with additional automated processing steps. To illustrate this, we also provide <ext-link ext-link-type="uri" xlink:href="https://github.com/neurodatascience/pubextract">pubextract</ext-link>, a collection of pubget plugins for extracting additional information from downloaded articles. Currently, pubextract offers functions for automatically extracting sample sizes, participant demographics, author locations, and author genders.</p>
<p>We further provide <ext-link ext-link-type="uri" xlink:href="https://litmining.github.io/labelbuddy-annotations/">labelbuddy-annotations</ext-link>, an open repository of annotations (created with labelbuddy on documents collected with pubget) that can be verified and re-used across projects and updated over time.</p>
<p>We have created a landing page for all these tools and repositories at <ext-link ext-link-type="uri" xlink:href="https://litmining.github.io/">litmining.github.io</ext-link>, which explains the basic workflow in text and instructional videos.</p>
</sec>
<sec id="s1d">
<label>1.4</label>
<title>Goals and scope</title>
<p>While researchers have a clear idea of the analyses they want to perform, the vast majority of time and energy is spent on the data collection and curation steps. We hope that the tools and recommendations we present here will make this process easier, more scalable, and more reproducible, and will therefore facilitate meta-research and literature mining.</p>
<p>In what follows, we illustrate how our flexible set of tools can be used to perform a variety of meta-research tasks. We highlight projects that demonstrate the use of the ecosystem for reproducing or extending previous important meta-research papers, as well as for asking new questions that were previously challenging to address. We describe not only neuroimaging but also meta-science examples to demonstrate the benefit of the proposed ecosystem.</p>
<p>These tools are already being used by other large funded projects, such as <ext-link ext-link-type="uri" xlink:href="https://neurostuff.github.io/">Neurosynth Compose</ext-link>, an online platform for performing PRISMA-compliant [<xref ref-type="bibr" rid="c8">Page et al., 2021</xref>] meta-analyses in neuroimaging, as well as <ext-link ext-link-type="uri" xlink:href="https://neurocausal.github.io/">neurocausal</ext-link>, which aims to meta-analyze neuropsychology (lesion-based) studies.</p>
</sec>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<p>To demonstrate the use of the <italic>litmining</italic> tools we chose examples diverse in their nature, requirements, and scale (from a few dozen documents to tens of thousands).</p>
<sec id="s2a">
<label>2.1</label>
<title>Large-scale meta-analyses</title>
<p>For the first time, pubget enables running a complete large-scale meta-analysis as performed by NeuroSynth and NeuroQuery in a fully automated and reproducible manner. <xref rid="fig3" ref-type="fig">Figure 3</xref> presents example results obtained running a single pubget command, which performs the full pipeline from data collection to the final statistical analysis and interactive exploration of the resulting brain maps.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Meta-analytic maps produced by pubget and by the original NeuroSynth and NeuroQuery platforms for some example terms. We note that pubget-NeuroSynth (i.e., the top row) has higher statistical power and better face validity for rare terms than the original NeuroSynth (second row). On the other hand, the original NeuroQuery (fourth row) was trained on 13,000 full-text articles and therefore performs better than pubget-NeuroQuery (third row). From these and other examples we suggest the following rule of thumb: (i) for frequent, well-defined terms such as “auditory” or “parietal”, all methods produce adequate results; (ii) for formal meta-analysis of a single term, pubget-NeuroSynth produces the best results; (iii) for multi-term queries, <ext-link ext-link-type="uri" xlink:href="https://neuroquery.org/">neuroquery.org</ext-link> or Text2Brain [<xref ref-type="bibr" rid="c7">Ngo et al., 2022</xref>] produce the best results. There is no <ext-link ext-link-type="uri" xlink:href="http://neurosynth.org">neurosynth.org</ext-link> map for “prosopagnosia” because this term is too rare to be included in NeuroSynth vocabulary.</p></caption>
<graphic xlink:href="564783v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Individual neuroimaging studies often lack statistical power and are likely to report spurious findings [<xref ref-type="bibr" rid="c9">Poldrack et al., 2017</xref>]. More reliable associations between brain structures and behavior can be extracted through meta-analysis [<xref ref-type="bibr" rid="c13">Wager et al., 2007</xref>], by aggregating the <italic>stereotactic coordinates</italic> (coordinates in a standard spatial referential for the brain) reported in these individual studies and performing a statistical analysis to identify consistent brain-behavior associations.</p>
<p>NeuroSynth [<xref ref-type="bibr" rid="c15">Yarkoni et al., 2011</xref>] made it possible to run such meta-analyses at a large scale by automating the collection of articles and coordinate extraction. Neuro-Query [<xref ref-type="bibr" rid="c3">Dockès et al., 2020</xref>] followed a similar approach but introduced a new article collection, an improved coordinate-extraction method, and a different statistical model. Importantly, these projects each share a static corpus, the result of a data collection that has been performed once, but do not provide an easy and reliable way to run the data collection. NeuroSynth does provide some code, but it is slow and difficult to use, and in practice users prefer to rely on the provided static corpus. These corpora are not updated with new publications, and they only contain a subset of the literature, so the practice of re-using these corpora is not sustainable.</p>
<p>Pubget downloads full-text articles from PubmedCentral (PMC) in a fast and reliable way (see <xref ref-type="sec" rid="s4a">section 4.1</xref>) and extracts existing brain coordinates using the method that generated the NeuroQuery coordinates. This makes it possible to extract coordinates from new sets of publications, enabling large-scale meta-analysis applications and development to move forward without being tied to the static NeuroQuery or NeuroSynth corpora. Moreover, pubget can run large-scale meta-analyses on the data it downloads, either by performing the NeuroSynth statistical test, or by fitting the NeuroQuery model. The results can be explored through an interactive (local) web application.</p>
<p>We first ran pubget with a query covering many neuroimaging journals as well as the terms “fMRI” (functional magnetic resonance imaging) and “VBM” (voxel-based morphometry). The full query can be found in the <ext-link ext-link-type="uri" xlink:href="https://github.com/neuroquery/pubget/blob/main/docs/example_queries/journal_list_fmri_vbm.txt">pubget repository</ext-link>. This resulted in over 10,000 articles with x, y, z stereotactic coordinates. Although that is a smaller number of publications than the NeuroSynth corpus, which contains over 14,000 abstracts, it represents a much larger amount of text because it also contains the articles full text. Therefore, any given term of interest such as “language” or “aphasia” is seen in more publications, associated with more coordinates, resulting in meta-analyses with higher statistical power. We used pubget to run meta-analyses on this corpus, using the <monospace>--fit_neurosynth</monospace> and <monospace>--fit_neuroquery</monospace> options provided by pubget. Some examples are shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Participants demographics</title>
<p>This second example reproduces and extends the investigation of sample size and statistical power presented in Poldrack et al. [2017]. In the original paper, the authors point out that small sample sizes pose a threat to neuroimaging studies’ replicability and reliability. Poldrack et al. [2017] investigate the evolution of the median sample size in the literature through time, exploiting 2 sources of data: (i) sample sizes for 583 studies manually extracted from published meta-analyses provided by David et al. [2013], and (ii) new semi-automatic annotations of 548 abstracts in the NeuroSynth corpus. The authors find that the median sample size increased steadily from 1995 to 2015, as well as the number of studies with more than 100 participants.</p>
<p>We used the <italic>litmining</italic> ecosystem tools pubget and labelbuddy to update this study with data from the most recent literature. We further characterized the articles’ co-horts and extracted, where possible, participants’ age, sex and whether participants were patient or healthy control. Using pubget, we downloaded papers from PubMed Central that mention the term “fMRI” in their abstracts. This yielded 4,230 articles, which we have shared on <ext-link ext-link-type="uri" xlink:href="https://osf.io/kfmdp">on OSF</ext-link>.</p>
<p>We then used labelbuddy to explore and annotate a first set of 20 papers, and implement a plugin as a simple set of rules to automatically extract the sample size and participants demographics from the text, shared <ext-link ext-link-type="uri" xlink:href="https://github.com/jeromedockes/fmri_participant_demographics">on GitHub</ext-link>. With labelbuddy we annotated a new batch of 100 articles and obtained a 10% mean absolute percentage error for the automatic extraction. As this is satisfactory for this illustration, we then ran the extraction on the full set of 4,230 articles to obtain the results presented below.</p>
<p>To illustrate how our ecosystem facilitates the evaluation newly emerging language models, we also extracted sample sizes using zero-shot learning using Generative Pre-trained Transformer 3.5 (GPT-3.5), a commercially available large language model. We first prototyped chat-completion prompts to extract sample size from a scientific text sample as well as pre- and post-processing pipeline using a set of 188 manually-annotated papers. We then applied the GPT pipeline to an unseen set of 103 annotated papers to evaluate model performance.</p>
<p>We found that GPT-3.5 performed better than our heuristic approach, with the heuristic finding the exact sample size in 36% of papers, compared to 50% for GPT-3.5. The two approaches had similar error rates when they did make a guess; the heuristic had a median absolute percent error of 0% (median absolute error of 0 participants), and GPT-3.5 had a median absolute percent error of 0.9% (1 participant). However, GPT-3.5 had better recall; it made a guess for 100% of evaluation papers, whereas pubextract made a guess in 54%. This is likely due to the fact that GPT-3.5 is able to extract sample sizes from more complex sentences than our heuristic, which is limited to simple sentences. The overall median sample size in the validation set was 32 participants, and the median sample sizes extracted by pubextract and GPT-3.5 were 24 and 34, respectively. <xref rid="fig4" ref-type="fig">Figure 4A</xref> shows the performance of both methods. Note that we identified a bug in GPT-3.5 embedding search implementation after the evaluation was completed; these present results reflect the revised implementation.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p><bold>A)</bold> Performance of the sample size extraction plugin in pubextract as well as GPT-3.5. The x-axis shows the sample size extracted from the text, and the y-axis shows the sample size reported in the article. The dashed line shows the identity line. The stars represent the median values of each extraction method, compared to the ground-truth median. <bold>B)</bold> Distribution of ages for different participant groups, extracted with pubextract. “Unknown” is chosen when the tool fails to detect whether a participant group corresponds to patients or healthy controls – in most cases when this is not specified explicitly the participants are healthy. We note that the distribution of healthy participants’ ages has a large peak around the age of university students, who are often recruited in their own university’s studies. Patients tend to be older on average, with a long tail likely due to studies on aging or neurodegenerative diseases. <bold>C)</bold> Median sample size through time. Error bars show 95% bootstrap confidence intervals. Following Poldrack et al. [2017], for sample sizes extracted from pubget-downloaded articles, we only consider single-group studies.</p></caption>
<graphic xlink:href="564783v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The purpose of this illustration is not to maximize the performance of these methods, but to show that their comparison was extremely easy using our tools; this example of model comparison was completed in a couple of weeks. While the accuracy of sample size extractors could be improved, we replicated and extended the results presented in Poldrack et al. [2017], and we found them sufficiently accurate for estimating the median sample size in the literature.</p>
<p>In <xref rid="fig4" ref-type="fig">Figure 4C</xref>, we show the median sample size evolution’s through time according to the four available sources: data from David et al. [2013], semi-automatic annotations of NeuroSynth abstracts from Poldrack et al. [2017], as well as automatic extractions from papers collected with pubget in the present study, using pubextract and GPT-3. We note that the median sample size estimated from the new data seems slightly smaller in 2015 than what was extracted from NeuroSynth abstracts. This may be due to errors in the automatic extraction, as well as the fact that papers with large sample sizes are more likely to mention it in the abstract.</p>
<p>We also investigated the distribution of participant ages reported in articles. We automatically extracted the age range, the mean age, and the age standard deviation when they were reported. In <xref rid="fig4" ref-type="fig">Figure 4</xref> (right), we plot the distribution of the mean age reported in individual studies. We note that healthy participants very often have the typical age of university students, possibly reflecting the declaimed practice of some studies that recruit students from their own department as participants [<xref ref-type="bibr" rid="c4">Henrich et al., 2010</xref>].</p>
<p>As high-quality annotations about participants are useful for many projects (e.g. as validation data for efforts to extract Population, Intervention, Comparison, Outcome (PICO) data with machine learning), our annotations are shared online, version controlled, and may be expanded in the future. The annotation <ext-link ext-link-type="uri" xlink:href="https://litmining.github.io/labelbuddy-annotations/projects/participant_demographics.html">project</ext-link> provides rich information about the (often complex) group structure of participants and demographic information. Annotating this information is made easy by labelbuddy’s interface and by a utility tool provided by <italic>litmining</italic> located in the annotations repository that infers the participant group structure and information about each subgroup and displays it as annotations are being added (<xref rid="fig5" ref-type="fig">Figure 5</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Screen capture of annotating the participants demographic in one document. The window on the right is labelbuddy, displaying an article, the available labels and their shortcuts (right column), and the existing annotations for the current document (right column). The left window is the dedicated tool for participant demographics, showing the inferred group structure and information about each group. This tool is not part of labelbuddy itself; it is distributed as part of the labelbuddy-annotations repository.</p></caption>
<graphic xlink:href="564783v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>At the time of writing this annotation repository contains over 3K annotations for 188 documents.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Dynamic Functional Connectivity methods</title>
<p>This third example illustrates the use of <italic>litmining</italic> on a fully manual project, that relies only on pubget and labelbuddy without adding any custom code beyond simple plotting.</p>
<p>In recent years, dynamic Functional Connectivity (dFC) has emerged as a popular approach for characterizing the time-varying patterns of functional connectivity in the brain. Numerous methods have been proposed for assessing dFC, but the selection of a specific method for a given study is often arbitrary and lacks a clear rationale. Additionally, despite being frequently cited in the literature, some dFC assessment methods are seldom applied in practice.</p>
<p>In this study, we investigated six different dFC assessment methods and their usage in the literature. We download 385 papers from PubMed Central using pubget and the search query <monospace>“(dynamic functional connectivity[Abstract]) OR time-varying functional connectivity[Abstract]”</monospace>. The resulting corpus is available <ext-link ext-link-type="uri" xlink:href="https://osf.io/td3cm">on OSF</ext-link>. These papers cover a range of topics, including clinical, cognitive, and methodological studies.</p>
<p>We then used labelbuddy to manually annotate a subset of papers with the dFC assessment method(s) used in the study and the application of the study (e.g., clinical, cognitive, etc.). This information provides valuable insights into the current state of the field, as it helps identify which dFC assessment methods are most commonly used and in which types of studies. The annotations are available <ext-link ext-link-type="uri" xlink:href="https://litmining.github.io/labelbuddy-annotations/overview.html">online</ext-link>.</p>
<p>The scatter plot in <xref rid="fig6" ref-type="fig">Figure 6</xref> presents the number of times each method was applied across the annotated papers versus the number of citations received. A total of 70 papers were annotated for this analysis. This figure serves to examine the relationship between the practical adoption of each method and its citation count. Essentially, it explores whether a method that receives frequent citations is also commonly employed in practice.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><p>The scatter plot displays the number of times each dFC assessment method was applied across the annotated papers, versus the number of times it was cited. The year of the first publication of each method is also shown besides its name. <italic>Clustering</italic> was both highly cited and highly applied, while <italic>Time-Frequency</italic> was highly cited but not highly applied. <italic>Sliding Window</italic> was highly applied, although not as frequently cited as <italic>Clustering. Co-Activation Patterns</italic> and <italic>Window-less</italic> were not highly applied, although <italic>Co-Activation Patterns</italic> was as frequently cited as <italic>Sliding Window</italic>.</p></caption>
<graphic xlink:href="564783v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>We have introduced an ecosystem of simple tools that help make text-mining of the biomedical literature more accessible, transparent and reproducible. This ecosystem, when used with recommended platforms GitHub and OSF, fosters collaborative, reproducible research. The ecosystem and recommendations include:</p>
<list list-type="roman-lower">
<list-item><p><bold>pubget</bold>, a tool for downloading and processing articles from PMC,</p></list-item>
<list-item><p><bold>an OSF repository</bold> for corpora and meta-analytic results created with pubget,</p></list-item>
<list-item><p><bold>labelbuddy</bold>, a flexible and effective text labelling application,</p></list-item>
<list-item><p><bold>labelbuddy-annotations</bold>, a repository of manual annotations of the neuroimaging literature made with labelbuddy, and</p></list-item>
<list-item><p><bold>pubextract</bold>, a Python package to extract information from the output of pubget.</p></list-item>
</list>
<p>These tools allowed us to run for the first time fully-automated, large-scale meta-analyses whose provenance can be tracked. Further, we were able to reproduce, update, and automate a systematic analysis of sample sizes in fMRI studies, and further demonstrated the usefulness of our ecosystem for model comparison, which is an essential component of literature mining. Finally, we could efficiently review dFC methods use in a systematic and transparent way. These examples illustrate the potential of the proposed ecosystem to facilitate meta-research and literature mining.</p>
<p>There are wide-ranging applications of these tools, from reviews and meta-analyses that summarize the literature, to meta-research evaluating and improving research itself, to the creation of new corpora for text-mining, as well as evaluation of different natural language processing models. These tools can be used to generate new corpora as well as to check the performance of existing tools that work with text data. The accessibility, scalability, and reproducibility of these tools are essential for updating past studies; as the literature grows with new information and evolves with new methods and practices, it is critical to make updates of reviews, meta-analyses, and other meta-research as seamless as possible. Our tools will make it easier to evaluate new models as they are developed, opening the doors of literature mining to researchers with a wider range of technical expertise and domain knowledge. Because pubget gathers biomedical literature provided by PubMed Central, these tools may be used in any biomedical field. Further, labelbuddy has even greater flexibility; it could be used to annotate any kind of text data, not just research articles.</p>
<p>While this ecosystem constitutes an important first step and facilitate a wide variety of projects, it has several limitations. A limitation of pubget is that it is restricted to the Open-Access subset of PMC, and this excludes a large portion of the biomedical literature. This choice was made to make pubget maintainable and is unlikely to be revisited in the near future. Because it relies on a single, stable and open resource, pubget is less likely to be affected by changes in the services it depends on, to fail for some users due to authentication issues, or to face high maintenance costs due to the need to handle multiple, potentially evolving formats chosen by individual publishers. It also makes it possible to regularly test pubget without needing an account for a paid resource. Finally, it allows users to conduct their analyses without worrying about infringing complex copyright laws, as all articles in the Open Access subset have Creative Commons or similar licenses and are available for text-mining purposes according to the PMC <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/">online documentation</ext-link>. While the restriction to PMC Open Access is an important limitation for reviews or manual meta-analyses that aim to be exhaustive, it is a smaller problem for large-scale meta-analysis or meta-science projects that only need a large, representative sample of the literature. Moreover, when the Open Access is insufficient, pubget can still be useful by automatically handling a large part of the articles, leaving the researchers with fewer articles to collect from other sources and process. Finally, there is hope that PMC will cover an even larger portion of the literature in the future as there is a trend towards more open-access papers (e.g., White <xref ref-type="bibr" rid="c14">House, 2022</xref>). Another potential issue, as with all recent tools that do not yet have a mature community of users and contributors, is the perennity of pubget, labelbuddy, and of the annotations repository. We have alleviated this risk through design choices and the distributed nature of these tools. Indeed, pubget and labelbuddy are local applications, that run on the users’ machines, lifting the high maintenance costs of a web service. Moreover, they both rely on a small set of widely used and well-maintained libraries, and intentionally have a restricted scope and feature set, to reduce the maintenance cost. Perhaps more importantly, they store their output in standard formats (CSV and JSON) in files or directories chosen by the user on their local disk and in Git repositories. This makes it easy to use the data without the tool, or to switch to a different tool. Even labelbuddy’s internal, binary format is the universally supported SQLite file format. By contrast, when relying on tools that store data online or even in local databases, or those that use complex or proprietary file formats, the user needs to worry about losing their work or spending significant time extracting and reformatting it if they decide to stop using the application. Similarly, due to the distributed nature of Git, annotations in labelbuddy-annotations repository will not be lost if the original authors of the repository stop contributing or even if the online repository is deleted – all contributors have a local copy and a new online repository can easily be created on several platforms such as GitHub, GitLab or BitBucket. Finally, although they are interoperable, the <italic>litmining</italic> tools are completely independent of each other (beyond pubget’s option to store result in labelbuddy’s JSON format) and from the annotations repository, and they are only linked by project development guidelines. We expect that many users will opt to use only part of the ecosystem.</p>
<p>One downside of this distributed and local design is a less-integrated experience for users. Compared to a solution where a researcher would perform all tasks on one online platform, they have to take the extra steps of installing the tools (which is seamless on Linux, MacOS and Windows) and managing their local files. In exchange, researchers gain full control of the work they produce and flexibility in how they store, analyze and share their work.</p>
<p>The ecosystem can be used from the command line or Python (e.g., when using pubget to download articles). However, we have made an effort to make these tools as accessible as possible, with minimal need for programming, while maintaining excellent reproducibility and scalability. To mitigate this challenge, we have created step-by-step instructional videos on how to use our ecosystem to perform an entire literature-mining project. These can be found at our central website, <ext-link ext-link-type="uri" xlink:href="http://litmining.github.io/">litmining.github.io</ext-link>.</p>
<p>Overall, despite these limitations and the intentionally restricted scope of the tools introduced, the proposed ecosystem constitutes an important step toward a more accessible open, reproducible and collaborative biomedical – and in particular neuroimaging – meta-science.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Materials and methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Pubget</title>
<p>Pubget is a command-line tool for downloading and processing publications from PMC. It is written in Python. This section provides a brief overview, and more information on using pubget can be found in the <ext-link ext-link-type="uri" xlink:href="https://neuroquery.github.io/pubget/">documentation</ext-link>.</p>
<p>The user provides either a PMC query, such as <monospace>“fMRI[Abstract]”</monospace>, or a list of PMC IDs. Pubget downloads all the corresponding publications that are in PMC’s Open Access subset, and processes them to extract their content. Outputs are stored in CSV files, making them very easy to use for subsequent analysis steps with scientific software such as Python, R, or Matlab.</p>
<p>A pubget execution proceeds in several steps, detailed below. The first steps consist of downloading the articles and extracting their content, and these are always executed. Then, several optional steps can be added, that prepare the downloaded data for more specific uses – for example extracting Term Frequency - Inverse Document Frequency (TF-IDF) features for use in natural language processing, or preparing documents in labelbuddy’s format for easy annotation.</p>
<p>Each step stores its output in a separate directory. When invoking pubget again with the same query, steps that have already been completed are not re-run. Most steps can process different articles in parallel.</p>
<sec id="s4a1">
<title>Downloading articles</title>
<p>The first step is to download the articles matching a PMC query. This is done with the <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/books/NBK25497/">Entrez E-utilities</ext-link>. This collection of web services provides a programmatic interface to the National Library of Medicine (NLM) resources. Pubget strives to provide a client that is reliable and robust to network and server failures. As an example, the NLM servers often send responses with a 200 (success) status code, despite the fact that an internal server error has occurred and the response content is an error message. Pubget handles these cases by checking the content of the responses and retrying the download when necessary.</p>
<p>Pubget also respects the request rate limits mentioned in the E-Utilities’ documentation to avoid overloading these resources. pubget includes in requests the API key when provided by the user, which is not required but encouraged by the NLM.</p>
<p>Given a query, pubget first uses E-Search to query the PMC database and build a set of matching PMCIDs. The result is stored on the Entrez history server, and pubget stores the keys that identify the result set on the history server. Then, pubget uses E-Fetch to download the XML documents, in batches of 500 articles each. Failed downloads are attempted several times.</p>
<p>If pubget is executed again with a query for which a partial download exists, only missing batches of articles are downloaded. This is possible due to the Entrez history server keys stored by pubget, which identify a fixed set of articles.</p>
</sec>
<sec id="s4a2">
<title>Extracting the articles’ content</title>
<p>The next step is extracting content in a usable format from the XML batches. Pubget first extracts articles from the downloaded batches and stores them in separate files. Then, each article’s XML is parsed and useful information is extracted. This is done in parallel across articles, using a number of processes chosen by the user. Metadata such as authors or publication year are extracted with XPath. The articles are transformed with an XSLT stylesheet to obtain the text, free of XML markup.</p>
<p>Tables require special attention. The XML language used by PMC, JATS, allows two different table formats – XHTML or OASIS. Pubget relies on the DocBook stylesheets to transform articles to the XHTML format and extracts and processes the tables from there. Tables are stored in pubget’s output in their original XML format, in the XHTML format, and as CSV files.</p>
<p>Stereotactic coordinates are extracted from tables with the approach that was used to create the neuroquery corpus [<xref ref-type="bibr" rid="c3">Dockès et al., 2020</xref>]. The coordinate space (MNI or Talairach) is detected with the heuristic used by NeuroSynth [<xref ref-type="bibr" rid="c15">Yarkoni et al., 2011</xref>].</p>
</sec>
<sec id="s4a3">
<title>Meta-analyses</title>
<p>Pubget can optionally extract TF-IDF features from articles and run the same Chi-square test as NeuroSynth, or train a neuroquery model. A small Python script is added to the output directory for a NeuroSynth or a NeuroQuery meta-analysis, which, when executed, allows a user to explore the meta-analytic maps interactively, with an interface similar to that of <ext-link ext-link-type="uri" xlink:href="https://neurosynth.org/">neurosynth.org</ext-link>.</p>
</sec>
<sec id="s4a4">
<title>Integration with other tools</title>
<p>Pubget aims to make it as easy as possible to use the data it generates for subsequent analysis steps. Part of this effort consists in formatting this data to target specific tools that are used by the community or wellsuited for common analysis steps. At the time of writing, such integrations exist only for 2 external tools (labelbuddy and NiMARE), but we are planning to add more, depending on user feedback.</p>
<p>Labelbuddy is a desktop application for manually annotating documents, described in more detail below. Pubget can format its output in the JSONLines format used by labelbuddy. Therefore, the pubget output can be directly imported in a labelbuddy database and the user can start labeling these documents without any effort.</p>
<p>NiMARE [<xref ref-type="bibr" rid="c10">Salo et al., 2018</xref>] is the established Python library for neuroimaging meta-analysis. It provides implementations of most of the widely-used meta-analysis methods. Pubget can format its output in the JSON format used by NiMARE to represent coordinate-based neuroimaging datasets and run meta-analyses. As a result, pubget output can directly be loaded with NiMARE and processed by any meta-analysis nimare method. For rigorous manual meta-analysis, additional steps to manually filter articles, select contrasts and check the extracted coordinates will need to be applied. However, having the pubget output in NiMARE format both facilitates this task and enables obtaining quickly a crude approximation of the final meta-analytic maps.</p>
</sec>
<sec id="s4a5">
<title>Plugins</title>
<p>A core design element of pubget is that it easily allows defining plugins. This means that additional processing steps can be developed and distributed independently of pubget, and still become part of the pubget command when they are installed. Such plugins are discovered at runtime and provide additional commands or options to the pubget command-line interface. One possible use for this plugin mechanism is to distribute processing steps that depend on libraries that are not installed by pubget, or that are too specific to be directly integrated in the main tool. These plugins foster re-use and an opportunity for projects to collaborate and progress at a faster pace or with less strict coding standards than pubget itself. A collection of such plugins is provided in the <ext-link ext-link-type="uri" xlink:href="https://github.com/neurodatascience/pubextract">pubextract</ext-link> package. Among other plugins, it adds to pubget the <monospace>--participants</monospace> option, which extracts participant counts and demographic information. This plugin is described in more detail below, where we present the methods used for the participant demographics example.</p>
</sec>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Labelbuddy</title>
<p>Although many annotation tools exist, we did not find a good solution when we searched for one adapted to our needs for extracting information from the neuroimaging literature. Most available tools are web-based and annotators collaboratively contribute to a centralized database of annotations. This is necessary when many annotators are involved, which is typically the case when labeling text for machine learning, as many annotations are then needed. However, this makes the setup and integration in a Git-based workflow more difficult. As a result, many projects prefer to use a simple spreadsheet, at the cost of relying on a much less convenient interface and losing the provenance and location of the exact portions of text that support the conclusions.</p>
<p>For small research projects with a few annotators, labelbuddy can compete both with web-based annotation tools running on a local server and with more ad-hoc local tools such as text editors or spreadsheets. It has an intentionally minimal feature set and focuses only on creating the annotations – for example it does not create reports, compare annotators etc. It can seamlessly be installed on Linux, Windows or MacOS and does not require any setup or configuration. Labelbuddy is also distributed as standalone binaries for those 3 platforms, so it can be used by downloading and running a single executable without any specific installation.</p>
<p>Internally the documents, labels and annotations are stored in a single ordinary file. They can be exported to JSON, which is easy to parse in any programming language. Any analysis or further manipulation of the annotations can be done with the researchers’ preferred tool, such as Python or R. Aggregating annotations across annotators and tracking their evolution can be done with the preferred tool for collaboration and version control – Git and a remote repository. Although it was designed with small projects in mind, labelbuddy can easily handle dozens of thousands of documents, large documents, and hundreds of thousands of annotations in a single file even on a small machine, making it a lightweight tool.</p>
<p>Pubget is well integrated with labelbuddy and can store the extracted text in JSON files that can be directly imported into labelbuddy for annotation, providing a core element of the <italic>litmining</italic> ecosystem.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>labelbuddy-annotations: A Git repository of manual annotations</title>
<p>To store and showcase annotations from several projects, we created the <ext-link ext-link-type="uri" xlink:href="https://litmining.github.io/labelbuddy-annotations/">labelbuddy-annotations</ext-link> Git repository. It stores documents, labels and annotations from several projects and annotators. The documents have typically been downloaded with pubget, although that is not a requirement. To start adding annotations, contributors simply need to create annotations in labelbuddy, export them, commit the resulting JSON and open a Pull Request. A Jupyter book showcases the contents of the repository, provides documentation for users and contributors, and presents simple analyses of the available data. It is hosted on GitHub Pages and updated by Continuous Integration (GitHub Actions). Moreover, the repository also contains a small Python package to facilitate the analysis of existing annotations. <xref rid="tbl1" ref-type="table">Table 1</xref> shows a summary of the projects currently included in the repository.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Number of documents, labels, annotators and annotations for each project in the labelbuddy-annotations repository.</title></caption>
<graphic xlink:href="564783v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Participant demographics</title>
<p>To extend the study of sample sizes presented in Poldrack et al. [2017], we used two approaches to extract the number of participant from the text of a study: a manually defined heuristic, and a workflow using OpenAI’s pre-trained GPT large language model.</p>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>pubextract heuristic</title>
<p>The heuristic approach proceeds in two steps. The first step recognizes mentions of participant groups in the text and extracts their different components (such as count and mean age). This is done with simple string matching and hard-coded rules – it does not involve any statistical learning. These rules are described by an Extended Backus–Naur Form (EBNF) grammar and parsing is performed by the <ext-link ext-link-type="uri" xlink:href="https://lark-parser.readthedocs.io/en/latest/index.html">Lark</ext-link> Python library. The second step aggregates the different group mentions detected in the first step and infers the participant group structure. The full implementation can be found on GitHub as part of the <ext-link ext-link-type="uri" xlink:href="https://github.com/neurodatascience/pubextract">pubextract package</ext-link>.</p>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>GPT workflow</title>
<p>The GPT approach consists of two steps: a semantic search to identify the most relevant section of each paper, and a zero-shot prompt to extract participant count from a given section. First, we separated each paper into chunks of less than 2000 characters, using Markdown headers from the Body of each paper. If after exhausting all headers a section was above 2000 characters, we further subdivided it using newline characters, to reach a chunk under the desired length. Next, using the OpenAI API, we generated a 1536 dimensional embedding for all chunks in each article using the pre-trained “text-embedding-ada-002” model. To find the chunk most likely to contain participant demographic information for each article, we embedded a string search query using the same embedding model, and computed the euclidean distance between each chunk and the search embedding. After piloted various search queries, we settled on the following search query: ‘How many participants or subjects were recruited for this study?’. Finally, for each paper we attempted to extract participant demographics using a zeroshot prompt. Using OpenAI Chat Completion API, we prompted “GPT-3.5-turbo” using a template that inserted text from the nearest chunk from each article, and the instruction to “identify groups of participants that participated in the study, and underwent MRI.”. To ensure that GPT provided a structured response, we included a JSON Schema for the desired output to the model as part of the prompt, using the “function calling” parameter. If the model responded with a null object for a chunk, we iteratively re-attempted extraction on the next-nearest chunk until a response was generated. Finally, we applied a manually derived set of cleaning results to the final output of the model, such as dropping rows with missing values or predicted counts of “0”. The full implementation can be found on GitHub as part of the <ext-link ext-link-type="uri" xlink:href="https://github.com/adelavega/publang">publang package</ext-link>.</p>
</sec>
<sec id="s4g">
<label>4.7</label>
<title>Validation</title>
<p>To validate the sample size extraction methods, we manually annotated 291 papers, which we divided into a training set of 188 and final validation set of 103 papers. We used the training example to iteratively improve the heuristic extraction rules, and the search query and extraction prompt using in the GPT approach (i.e. “prompt engineering”).</p>
<p>The code for performing this validation and the plots shown in this paper, as well as the manual annotations, can be found on <ext-link ext-link-type="uri" xlink:href="https://github.com/jeromedockes/fmri_participant_demographics/">GitHub</ext-link>. As detailed annotations about study participants demographics are useful on their own and for other projects, the annotation effort continues in the <ext-link ext-link-type="uri" xlink:href="https://litmining.github.io/labelbuddy-annotations/projects/participant_demographics.html">labelbuddy-annotations</ext-link> repository.</p>
<p>To validate the age extraction, we opened the automatically generated annotations in labelbuddy and checked them manually. In these papers, we found no errors in the age extraction.</p>
<p>Automatically extracting information about study participants from text is a difficult task and beyond the scope of the current paper. Our heuristic approach leaves much room for improvement, and statistical methods, for example based on pre-trained deep language models, would probably yield better results. What we aim to illustrate with this example project is that by using pubget we were able to focus on the task of interest (finding and analyzing the participants’ information), without spending time and effort on obtaining the relevant text. Moreover, the ability to easily annotate the documents with labelbuddy, and to visualize automatically generated annotations with the same tool, was key to an efficient validation step.</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Baker</surname></string-name>. <article-title>Reproducibility crisis</article-title>. <source>Nature</source>, <volume>533</volume>(<issue>26</issue>):<fpage>353</fpage>–<lpage>66</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><given-names>S. P.</given-names> <surname>David</surname></string-name>, <string-name><given-names>J. J.</given-names> <surname>Ware</surname></string-name>, <string-name><given-names>I. M.</given-names> <surname>Chu</surname></string-name>, <string-name><given-names>P. D.</given-names> <surname>Loftus</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Fusar-Poli</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Radua</surname></string-name>, <string-name><given-names>M. R.</given-names> <surname>Munafo</surname></string-name>, and <string-name><given-names>J. P.</given-names> <surname>Ioannidis</surname></string-name>. <article-title>Potential reporting bias in fmri studies of the brain</article-title>. <source>PloS one</source>, <volume>8</volume>(<issue>7</issue>):<fpage>e70104</fpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Dockès</surname></string-name>, <string-name><given-names>R. A.</given-names> <surname>Poldrack</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Primet</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Gözükan</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Yarkoni</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Suchanek</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Thirion</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Varoquaux</surname></string-name>. <article-title>Neuroquery, comprehensive meta-analysis of human brain mapping</article-title>. <source>eLife</source>, <volume>9</volume>, <month>Mar</month> <year>2020</year>. ISSN <issn>2050-084X</issn>. doi: <pub-id pub-id-type="doi">10.7554/elife.53385</pub-id>. URL <pub-id pub-id-type="doi">10.7554/elife.53385</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Henrich</surname></string-name>, <string-name><given-names>S. J.</given-names> <surname>Heine</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Norenzayan</surname></string-name>. <article-title>Most people are not weird</article-title>. <source>Nature</source>, <volume>466</volume> (<issue>7302</issue>):<fpage>29</fpage>–<lpage>29</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><given-names>M.-S.</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>P.-T.</given-names> <surname>Lai</surname></string-name>, <string-name><given-names>P.-Y.</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>Y.-T.</given-names> <surname>You</surname></string-name>, <string-name><given-names>R. T.-H.</given-names> <surname>Tsai</surname></string-name>, and <string-name><given-names>W.-L.</given-names> <surname>Hsu</surname></string-name>. <article-title>Biomedical named entity recognition and linking datasets: survey and our recent development</article-title>. <source>Briefings in Bioinformatics</source>, <volume>21</volume>(<issue>6</issue>):<fpage>2219</fpage>–<lpage>2238</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><given-names>E.</given-names> <surname>Landhuis</surname></string-name>. <article-title>Scientific literature: Information overload</article-title>. <source>Nature</source>, <volume>535</volume>(<issue>7612</issue>):<fpage>457</fpage>–<lpage>458</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><given-names>G. H.</given-names> <surname>Ngo</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>N. F.</given-names> <surname>Chen</surname></string-name>, and <string-name><given-names>M. R.</given-names> <surname>Sabuncu</surname></string-name>. <article-title>A transformer-based neural language model that synthesizes brain activation maps from free-form text queries</article-title>. <source>Medical Image Analysis</source>, <volume>81</volume>:<fpage>102540</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><given-names>M. J.</given-names> <surname>Page</surname></string-name>, <string-name><given-names>J. E.</given-names> <surname>McKenzie</surname></string-name>, <string-name><given-names>P. M.</given-names> <surname>Bossuyt</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Boutron</surname></string-name>, <string-name><given-names>T. C.</given-names> <surname>Hoffmann</surname></string-name>, <string-name><given-names>C. D.</given-names> <surname>Mulrow</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Shamseer</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Tetzlaff</surname></string-name>, <string-name><given-names>E. A.</given-names> <surname>Akl</surname></string-name>, <string-name><given-names>S. E.</given-names> <surname>Brennan</surname></string-name>, <etal>et al.</etal> <article-title>The prisma 2020 statement: an updated guideline for reporting systematic reviews</article-title>. <source>International journal of surgery</source>, <volume>88</volume>:<fpage>105906</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><given-names>R. A.</given-names> <surname>Poldrack</surname></string-name>, <string-name><given-names>C. I.</given-names> <surname>Baker</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Durnez</surname></string-name>, <string-name><given-names>K. J.</given-names> <surname>Gorgolewski</surname></string-name>, <string-name><given-names>P. M.</given-names> <surname>Matthews</surname></string-name>, <string-name><given-names>M. R.</given-names> <surname>Munafò</surname></string-name>, <string-name><given-names>T. E.</given-names> <surname>Nichols</surname></string-name>, <string-name><given-names>J.-B.</given-names> <surname>Poline</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Vul</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Yarkoni</surname></string-name>. <article-title>Scanning the horizon: towards transparent and reproducible neuroimaging research</article-title>. <source>Nature reviews neuroscience</source>, <volume>18</volume>(<issue>2</issue>):<fpage>115</fpage>–<lpage>126</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Salo</surname></string-name>, <string-name><given-names>K. L.</given-names> <surname>Bottenhorn</surname></string-name>, <string-name><given-names>T. E.</given-names> <surname>Nichols</surname></string-name>, <string-name><given-names>M. C.</given-names> <surname>Riedel</surname></string-name>, <string-name><given-names>M. T.</given-names> <surname>Sutherland</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Yarkoni</surname></string-name>, and <string-name><given-names>A. R.</given-names> <surname>Laird</surname></string-name>. <article-title>Nimare: a neuroimaging meta-analysis research environment</article-title>. <source>F1000Research</source>, <volume>7</volume>, <year>2018</year>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>Simmonds</surname></string-name>, <string-name><given-names>J. H.</given-names> <surname>Elliott</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Synnot</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Turner</surname></string-name>. <article-title>Living systematic reviews</article-title>. <source>Meta-Research: Methods and Protocols</source>, pages <fpage>121</fpage>–<lpage>134</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Tinn</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Cheng</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Gu</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Usuyama</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Naumann</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Gao</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Poon</surname></string-name>. <article-title>Fine-tuning large neural language models for biomedical natural language processing</article-title>. <source>Patterns</source>, <volume>4</volume>(<issue>4</issue>), <year>2023</year>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><given-names>T. D.</given-names> <surname>Wager</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Lindquist</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Kaplan</surname></string-name>. <article-title>Meta-analysis of functional neuroimaging data: current and future directions</article-title>. <source>Social cognitive and affective neuroscience</source>, <volume>2</volume> (<issue>2</issue>):<fpage>150</fpage>–<lpage>158</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="web"><string-name><given-names>T. White</given-names> <surname>House</surname></string-name>. <source>Ostp issues guidance to make federally funded research freely available without delay</source>. <ext-link ext-link-type="uri" xlink:href="https://www.whitehouse.gov/ostp/news-updates/2022/08/25/ostp-issues-guidance-to-make-federally-funded-research-freely-available-with">https://www.whitehouse.gov/ostp/news-updates/2022/08/25/ostp-issues-guidance-to-make-federally-funded-research-freely-available-with</ext-link> mAug <year>2022</year>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Yarkoni</surname></string-name>, <string-name><given-names>R. A.</given-names> <surname>Poldrack</surname></string-name>, <string-name><given-names>T. E.</given-names> <surname>Nichols</surname></string-name>, <string-name><given-names>D. C.</given-names> <surname>Van Essen</surname></string-name>, and <string-name><given-names>T. D.</given-names> <surname>Wager</surname></string-name>. <article-title>Large-scale automated synthesis of human functional neuroimaging data</article-title>. <source>Nature methods</source>, <volume>8</volume>(<issue>8</issue>):<fpage>665</fpage>, <year>2011</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94909.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Yan</surname>
<given-names>Chaogan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Chinese Academy of Sciences</institution>
</institution-wrap>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>The study presents an <bold>important</bold> ecosystem designed to support literature mining in biomedical research, showcasing a methodological framework that includes tools like Pubget for article collection and labelbuddy for text annotation. The <bold>solid</bold> evidence presented for these tools suggests they could streamline the analysis and annotation of scientific literature, potentially benefiting research across a range of biomedical disciplines. While the primary focus is on neuroimaging literature, the applicability of these methods and tools might extend further, offering an advance in the practices of meta-research and literature mining.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94909.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this paper, the authors present new tools to collect and process information from the biomedical literature that could be typically used in a meta-analytic framework. The tools have been specifically developed for the neuroimaging literature. However, many of their functions could be used in other fields. The tools mainly enable to downloading of batches of paper from the literature, extracting relevant information along with meta-data, and annotating the data. The tools are implemented in an open ecosystem that can be used from the command line or Python.</p>
<p>Strengths:</p>
<p>The tools developed here are really valuable for the future of large-scale analyses of the biomedical literature. This is a very well-written paper. The presentation of the use of the tools through several examples corresponding to different scientific questions really helps the readers to foresee the potential application of these tools.</p>
<p>Weaknesses:</p>
<p>The tools are command-based and store outcomes locally. So users who prefer to work only with GUI and web-based apps may have some difficulties. Furthermore, the outcomes of the tools are constrained by inherent limitations in the scientific literature, in particular, here the fact that only a small portion of the publications have full text openly available.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94909.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this manuscript, the authors described the litmining ecosystem that can flexibly combine automatic and manual annotation for meta-research.</p>
<p>Strengths:</p>
<p>Software development is crucial for cumulative science and of great value to the community. However, such works are often greatly under-valued in the current publish-or-perish research culture. Thus, I applaud the authors' efforts devoted to this project. All the tools and repositories are public and can be accessed or installed without difficulty. The results reported in the manuscript are also compelling that the ecosystem is relatively mature.</p>
<p>Weaknesses:</p>
<p>First and foremost, the logic flow of the current manuscript is difficult to follow.</p>
<p>The second issue is the results from the litmining ecosystem were not validated and the efficiency of using litmining was not quantified. To validate the results, it would be better to directly compare the results of litmining with recognized ground truth in each of the examples. To prove the efficiency of the current ecosystem, it would be better to use quantitative indices for comparing the litmining and the other two approaches (in terms of time and/or other costs in a typical meta-research).</p>
<p>The third family of issues is about the functionality of the litmining ecosystem. As the authors mentioned, the ecosystem can be used for multiple purposes, however, the description here is not sufficient for researchers to incorporate the litmining ecosystem into their meta-research project. Imagine that a group of researchers are interested in using the litmining ecosystem to facilitate their meta-analyses, how should they incorporate litmining into their workflow? I have this question because, in a complete meta-analysis, researchers are required to (1) search in more than one database to ensure the completeness of their literature search; (2) screen the articles from the searched articles, which requires inspection of the abstract and the pdf; (3) search all possible pdf file of included articles instead of only relying on the open-access pdf files on PMC database. That said, if researchers are interested in using litmining in a meta-analysis that follows reporting standards such as PRISMA, the following functionalities are crucial:</p>
<p>
(a) How to incorporate the literature search results from different databases;</p>
<p>
(b) After downloading the meta-data of articles from databases, how to identify whose pdf files can be downloaded from PMC and whose pdf files need to be searched from other resources;</p>
<p>
(c) Is it possible to also annotate pdf files that were not downloaded by pubget?</p>
<p>
(d) How to maintain and update the meta-data and intermediate data for a meta-analysis by using litmining? For example, after searching in a database using a specific command and conducting their meta-analysis, researchers may need to update the search results and include items after a certain period.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94909.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors aimed to develop an automated tool to easily collect, process, and annotate the biomedical literature for higher efficiency and better reproducibility.</p>
<p>Strengths:</p>
<p>Two charms coming with the efforts made by the team are Pubget (for efficient and reliable grabbing articles from PubMed) and labelbuddy (for annotating text). They make text-mining of the biomedical literature more accessible, effective, and reproducible for streamlined text-mining and meta-science projects. The data were collected and analyzed using solid and validated methodology and demonstrated a very promising direction for meta-science studies.</p>
<p>Weaknesses:</p>
<p>More developments are needed for different resources of literature and strengths of AI-powered functions.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94909.1.sa4</article-id>
<title-group>
<article-title>Author Response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Dockès</surname>
<given-names>Jérôme</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Oudyk</surname>
<given-names>Kendra</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4087-5402</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Torabi</surname>
<given-names>Mohammad</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>de la Vega</surname>
<given-names>Alejandro I</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Poline</surname>
<given-names>Jean-Baptiste</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9794-749X</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>Thank you for the reviews and the eLife assessment. We want to take this opportunity to acknowledge the weaknesses pointed out by the reviewers and we will make small changes to the manuscript to account for these as part of the Version of Record.</p>
<p><bold>The tools are command-based and store outcomes locally</bold></p>
<p>We consider this to be an advantage of our ecosystem, which is intended for the case of individuals or small groups of authors. These features facilitate easy installation and integration with other tools. Further, our tool labelbuddy is a graphic user interface. Our tools may also be integrated into web-based systems as backends. Pubget is already being used in this way in the NeuroSynth Compose platform for semi-automated neuroimaging meta-analyses.</p>
<p><bold>pubget only gathers open-access papers from PubMed Central</bold></p>
<p>We recognize this as a limitation, and we acknowledge it in the original manuscript (in the discussion section, starting with &quot;A limitation of Pubget is that it is restricted to the Open-Access subset of PMC&quot;). We chose to limit the scope of our tools in order to ensure maintainability. Further, we are currently expanding pubget so it will also be able to access the abstracts and meta-data from closed-access papers indexed on PubMed. Future research could build other tools to work alongside pubget, to access other databases.</p>
<p><bold>Logic flow is difficult to follow</bold></p>
<p>We thank the reviewer for this feedback. Our paper describes an ecosystem of literature mining tools which does not lend itself to narrative flow nor does readily fit into the standard &quot;Intro, Results, Discussion, Methods&quot; structure that is typical in the scientific literature. We have done our best to conform to this expected format, but we have also provided detailed section and subsection headings to enable the reader to digest the paper nonlinearly. Each of the tools we describe also has detailed documentation on github that we update continuously.</p>
<p><bold>Results were not validated</bold></p>
<p>For the example where we automatically extracted participant demographics from papers, we validated the results on a held-out dataset of 100 manually-annotated papers. For the example with automatic large-scale meta-analyses (neuroquery and neurosynth), these methods are described together with their validation in the original papers. If this ecosystem of tools is integrated into other workflows, it should be validated in those contexts. We recognize that validating meta-analyses is a difficult problem because we do not have ground truth maps of the brain.</p>
<p><bold>Efficiency was not quantified</bold></p>
<p>Creators of tools do not always do experiments to quantify their efficiency and other qualities. We have chosen not to do this here, first because it is outside the scope of this paper as it would necessitate to specify very precise tasks and how efficiency is measured, and second because at least for the data collection part, the benefit of using an automated tool over manually downloading papers one by one is clear even without quantifying it. Compared to the approach of re-using existing datasets, our ecosystem is not necessarily more or less efficient. But it has other advantages, such as providing datasets that contain the latest literature, whereas the existing datasets are static and quickly out-of-date.</p>
<p><bold>We do not highlight the strength of AI functions</bold></p>
<p>We provide an example of using our tools to gather data and manually annotate a validation set for use with large language models (in our case, GPT). We are further exploring this domain in other projects; for example, for performing semi-automated meta-analyses using the NeuroSynth Compose platform. However, we did not deem it necessary to include more AI examples in the current paper;  we only wanted to provide enough examples to demonstrate the scope of possible use cases of our ecosystem.</p>
<p>We thank the reviewers for their time and valuable feedback, which we will keep in mind in our future research.</p>
</body>
</sub-article>
</article>