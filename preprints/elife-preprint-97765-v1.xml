<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">97765</article-id>
<article-id pub-id-type="doi">10.7554/eLife.97765</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97765.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Uncertainty-based causal inference modulates audiovisual temporal recalibration</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0182-3952</contrib-id>
<name>
<surname>Li</surname>
<given-names>Luhe</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1890-1977</contrib-id>
<name>
<surname>Hong</surname>
<given-names>Fangfang</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Badde</surname>
<given-names>Stephanie</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Landy</surname>
<given-names>Michael S.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology, New York University</institution></aff>
<aff id="a2"><label>2</label><institution>Department of Psychology, University of Pennsylvania</institution></aff>
<aff id="a3"><label>3</label><institution>Department of Psychology, Tufts University</institution></aff>
<aff id="a4"><label>4</label><institution>Center for Neural Science, New York University</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label> Corresponding author; email: <email>luhe.li@nyu.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-06-24">
<day>24</day>
<month>06</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP97765</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-15">
<day>15</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-11">
<day>11</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.08.584189"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Li et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Li et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-97765-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Cross-modal temporal recalibration is crucial for maintaining coherent perception in a multimodal environment. The classic view suggests that cross-modal temporal recalibration aligns the perceived timing of sensory signals from different modalities, such as sound and light, to compensate for physical and neural latency differences. However, this view cannot fully explain the nonlinearity and asymmetry observed in audiovisual recalibration effects: the amount of re-calibration plateaus with increasing audiovisual asynchrony and varies depending on the leading modality of the asynchrony during exposure. To address these discrepancies, our study examines the mechanism of audiovisual temporal recalibration through the lens of causal inference, considering the brain’s capacity to determine whether multimodal signals come from a common source and should be integrated, or else kept separate. In a three-phase recalibration paradigm, we manipulated the adapter stimulus-onset asynchrony in the exposure phase across nine sessions, introducing asynchronies up to 0.7 s of either auditory or visual lead. Before and after the exposure phase in each session, we measured participants’ perception of audiovisual relative timing using a temporal-order-judgment task. We compared models that assumed observers re-calibrate to approach either the physical synchrony or the causal-inference-based percept, with uncertainties specific to each modality or comparable across them. Modeling results revealed that a causal-inference model incorporating modality-specific uncertainty captures both the nonlinearity and asymmetry of audiovisual temporal recalibration. Our results indicate that human observers employ causal-inference-based percepts to recalibrate cross-modal temporal perception.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Perception is flexible and should change in response to perceptual error (<xref ref-type="bibr" rid="c6">Bedford, 1999</xref>). In a multimodal environment, systematic discrepancies between modalities indicate perceptual errors and thus the need for recalibration. Signals in different modalities from the same event can arrive with different physical and neural delays in the relevant brain areas (<xref ref-type="bibr" rid="c13">Fain, 2019</xref>; <xref ref-type="bibr" rid="c31">Pöppel, 1988</xref>; <xref ref-type="bibr" rid="c39">Spence &amp; Squire, 2003</xref>). Cross-modal temporal recalibration has been considered a compensatory mechanism that attempts to realign the perceived timing between modalities to maintain perceptual synchrony across changes in the perceptual systems and the environment (reviewed by <xref ref-type="bibr" rid="c22">King, 2005</xref>; <xref ref-type="bibr" rid="c47">Vroomen and Keetels, 2010</xref>). This phenomenon is exemplified by audiovisual temporal recalibration, where consistent exposure to audiovisual asynchrony shifts the audiovisual temporal bias (i.e., point of subjective simultaneity) between auditory and visual stimuli in the direction of the asynchrony to which one has been exposed (<xref ref-type="bibr" rid="c11">Di Luca et al., 2009</xref>; <xref ref-type="bibr" rid="c14">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="c17">Hanson et al., 2008</xref>; <xref ref-type="bibr" rid="c18">Harrar &amp; Harris, 2008</xref>; <xref ref-type="bibr" rid="c19">Heron et al., 2007</xref>; <xref ref-type="bibr" rid="c21">Keetels &amp; Vroomen, 2007</xref>; <xref ref-type="bibr" rid="c29">Navarra et al., 2005</xref>; <xref ref-type="bibr" rid="c32">Roach et al., 2011</xref>; <xref ref-type="bibr" rid="c41">Tanaka et al., 2011</xref>; <xref ref-type="bibr" rid="c45">Vatakis et al., 2007</xref>, <xref ref-type="bibr" rid="c46">2008</xref>; <xref ref-type="bibr" rid="c47">Vroomen &amp; Keetels, 2010</xref>; <xref ref-type="bibr" rid="c48">Vroomen et al., 2004</xref>; <xref ref-type="bibr" rid="c53">Yamamoto et al., 2012</xref>).</p>
<p>The classic compensatory view is that recalibration serves to offset the physical and neural latency differences between modalities (<xref ref-type="bibr" rid="c14">Fujisaki et al., 2004</xref>), aiming for external accuracy, the agreement between perception and the environment (<xref ref-type="bibr" rid="c57">Zaidel et al., 2011</xref>). When external feedback is not available, recalibration may target internal consistency (<xref ref-type="bibr" rid="c8">Burge et al., 2010</xref>). In the context of the recalibration of relative timing, both theories predict similar behavior regardless of whether the goal is accuracy or consistency: the perceptual system will attempt to recalibrate for any amount of asynchrony so as to homeostatically restore a physical or perceived synchrony. Here, we formalize this hypothesis as the fixed-update model.</p>
<p>The fixed-update model predicts a linearly increasing amount of temporal recalibration as with increases in the asynchrony one is exposed to. However, empirical observations do not fully align with this model. The amount of audiovisual temporal recalibration as a function of adapted stimulus-onset asynchrony (SOA) exhibits two crucial characteristics: nonlinearity and asymmetry. The amount of recalibration is not proportional to the adapted SOA, but instead plateaus at SOAs of approximately 100–300 ms (<xref ref-type="bibr" rid="c14">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="c48">Vroomen et al., 2004</xref>). The amount of recalibration can also be asymmetrical: the magnitude of recalibration differs when the visual stimulus leads during the exposure phase compared to when the auditory stimulus leads (<xref ref-type="bibr" rid="c14">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="c30">O’Donohue et al., 2022</xref>; <xref ref-type="bibr" rid="c42">Van der Burg et al., 2013</xref>). These observations suggest that while the fixed-update model might capture the general purpose behind temporal recalibration, it falls short in fully capturing the nuanced ways in which the brain adjusts to varying SOAs. This prompts consideration of additional mechanisms to account for previously observed nonlinearity and asymmetry.</p>
<p>Notably, the fixed-update model overlooks the causal relationship between multimodal stimuli by implicitly assuming that they originate from a single source. However, that’s not always the case in a multimodal environment. For instance, in a dubbed movie with a noticeable delay between the video and the audio, this delay can indicate that the sound is provided by a different actor, not the character on screen. To address this challenge, the brain must perform causal inference to determine whether multimodal signals come from a common source and should be integrated, or else kept separate. Indeed, numerous studies support the hypothesis that humans consider the causal structure of cross-modal stimuli when making perceptual decisions (<xref ref-type="bibr" rid="c3">Aller &amp; Noppeney, 2019</xref>; <xref ref-type="bibr" rid="c9">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="c12">Dokka et al., 2019</xref>; <xref ref-type="bibr" rid="c23">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="c26">Locke &amp; Landy, 2017</xref>; <xref ref-type="bibr" rid="c28">McGovern et al., 2016</xref>; <xref ref-type="bibr" rid="c34">Rohe &amp; Noppeney, 2015</xref>; <xref ref-type="bibr" rid="c36">Samad et al., 2015</xref>; <xref ref-type="bibr" rid="c38">Sato et al., 2007</xref>; <xref ref-type="bibr" rid="c49">Wei &amp; Körding, 2009</xref>; <xref ref-type="bibr" rid="c50">Wozny et al., 2010</xref>). Drawing on this framework, we formulate a causal-inference model of temporal recalibration based on previous models that have successfully predicted visual-auditory (<xref ref-type="bibr" rid="c20">Hong et al., 2021</xref>; <xref ref-type="bibr" rid="c38">Sato et al., 2007</xref>) and visual-tactile (<xref ref-type="bibr" rid="c5">Badde, Navarro, &amp; Landy, 2020</xref>) spatial recalibration.</p>
<p>Although models incorporating causal inference are promising in capturing the observed nonlinearities, they predict an identical amount of temporal recalibration for audiovisual stimulus pairs that have the same SOA but with opposite sign (i.e., lead vs. lag). This suggests that additional factors are required to explain the observed asymmetry. In previous studies, the asymmetry has been attributed to different factors, such as the physical and neural latency differences between sensory signals (<xref ref-type="bibr" rid="c30">O’Donohue et al., 2022</xref>; <xref ref-type="bibr" rid="c42">Van der Burg et al., 2013</xref>) or more frequent exposure to visual-lead events in natural environments (<xref ref-type="bibr" rid="c14">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="c42">Van der Burg et al., 2013</xref>). These factors can explain the audiovisual temporal bias most humans developed through early sensory experience (Badde, Ley, et al., 2020). Yet, this bias would again equally affect the amount of recalibration resulting from the same SOAs on either side of the observer’s bias. In contrast to bias, sensory uncertainty has been shown to affect the degree of cross-modal recalibration in a complex fashion (<xref ref-type="bibr" rid="c5">Badde, Navarro, &amp; Landy, 2020</xref>; <xref ref-type="bibr" rid="c20">Hong et al., 2021</xref>; <xref ref-type="bibr" rid="c44">van Beers et al., 2002</xref>). We hypothesize that different degrees of auditory and visual uncertainty play a critical role in the asymmetry of cross-modal temporal recalibration.</p>
<p>To examine the mechanism underlying cross-modal temporal recalibration, we used a classic three-phase recalibration paradigm, in which participants completed a pre-test, exposure, and post-test. We manipulated the adapter SOA (i.e., the audiovisual asynchrony presented in the exposure phase) across sessions, introducing SOAs up to 0.7 s of either auditory or visual lead. Before and after the exposure phase in each session, we measured participant’s perception of audiovisual relative timing using a temporal-order-judgement (TOJ) task. To preview the empirical results, we confirmed the nonlinearity as well as idiosyncratic asymmetry of the recalibration effect. To scrutinize the factors that might drive these two main characteristics, we fitted four models to the data, using either causal inference or a fixed update. Despite previous empirical evidence challenging the fixed-update model, it doesn’t mean we should discount its relevance without a statistical comparison to alternative models. The causal-inference and the fixed-update models were combined with either modality-specific or modality-independent uncertainty.</p>
<p>Model comparison revealed that causal inference combined with modality-specific uncertainty is essential to accurately capture the nonlinearity and idiosyncratic asymmetry of temporal recalibration. Our results indicate that human observers employ causal-inference-based percepts to recalibrate cross-modal temporal perception. This finding suggests that cross-modal temporal recalibration, typically considered an early-stage, low-level perceptual process, involves higher cognitive functions in the adjustment of perception.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>Behavioral results</title>
<p>We adopted a classical three-phase recalibration paradigm in which participants completed a pre-test, an exposure phase, and a post-test in each session. In pre- and post-tests, we measured participants’ perception of audiovisual relative timing using a TOJ task: participants reported the perceived order (“visual first,” “auditory first,” or “simultaneous”) of audiovisual stimulus pairs with varying SOAs (range: from -0.5 to 0.5 s with 15 levels; <xref rid="fig1" ref-type="fig">Figure 1A</xref>). In the exposure phase, participants were exposed to a series of audiovisual stimuli with a consistent SOA (250 trials; <xref rid="fig1" ref-type="fig">Figure 1B</xref>). To ensure that participants were attentive to the stimuli, they performed an oddball-detection task. Specifically, we inserted oddball stimuli with slightly greater intensity in either one or both modalities (5% of total trials independently sampled for each modality). Participants were instructed to respond whenever they detected such stimuli. The high <italic>d</italic><sup><italic>′</italic></sup> of oddball-detection performance (auditory <italic>d</italic><sup><italic>′</italic></sup> = 3.34 <italic>±</italic> 0.54, visual <italic>d</italic><sup><italic>′</italic></sup> = 2.44 <italic>±</italic> 0.72) showed that participants paid attention to both modalities (Figure S2). The post-test was almost identical to the pre-test, except that before every temporal-order judgment, there were three top-up exposure trials to maintain the recalibration effect. In total, participants completed nine sessions on separate days. The adapter SOA (range: -0.7 to 0.7 s) varied across but not within sessions.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Task timing. (A) Temporal-order-judgment task administered in the pre- and post-tests. In each trial, participants made a temporal-order judgment in response to an audiovisual stimulus pair with a varying stimulus-onset asynchrony (SOA). Negative values: auditory lead; positive values: visual lead. (B) Oddball-detection task used in the exposure phase and post-test top-up trials. Participants were repeatedly presented with an audiovisual stimulus pair with a SOA that was fixed within each session but varied across sessions. Occasionally, the intensity of either or both of the auditory and the visual stimuli was increased. Participants were instructed to press a key whenever such an oddball stimulus occurred.</p></caption>
<graphic xlink:href="584189v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We compared the temporal-order judgments between the pre- and post-tests to examine the amount of audiovisual temporal recalibration induced by the SOA of audiovisual stimuli during the exposure phase. Specifically, we fitted the data from the pre- and post-tests jointly assuming different points of subjective simultaneity (PSS) between the two tests while assuming fixed arrival-latency distributions and fixed response criteria (<xref rid="fig2" ref-type="fig">Figure 2A</xref>; see Supplement S1 for an alternative model assuming a shift in the response criteria due to recalibration). The amount of audiovisual temporal recalibration was defined as the difference between the two PSSs. At the group level, we observed a nonlinear pattern of recalibration as a function of the adapter SOA: the amount of recalibration in the direction of the adapter SOA first increased but then plateaued with increasing magnitude of the adapter SOA presented during the exposure phase (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Additionally, we observed an asymmetry between auditory-lead and visual-lead adapter SOAs in the magnitude of recalibration at the group level, with auditory-lead adapter SOAs inducing a greater amount of recalibration (<xref rid="fig2" ref-type="fig">Figure 2B</xref>; see Figure S6 for individual participants’ data). To quantify this asymmetry for each participant, we calculated an asymmetry index, defined as the sum of the recalibration effects across all sessions (zero: no evidence for asymmetry; positive values: greater recalibration given visual-lead adapters; negative: greater recalibration given auditory-lead adapters). For each participant, we bootstrapped the temporal-order judgments to obtain a 95% confidence interval for the asymmetry index. All participants’ confidence intervals excluded zero, suggesting that all of them showed audiovisual asymmetry in temporal recalibration (Figure S3).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Behavioral results. (A) The probability of reporting that the auditory stimulus came first, the two arrived at the same time, or the visual stimulus came first as a function of SOA for a representative participant in a single session. The adapter SOA was -0.3 s for this session. Curves: best-fitting functions estimated jointly using the data from the pre-test (dashed) and post-test (solid). Shaded areas: 95% bootstrapped confidence intervals. (B) Mean recalibration effects (shifts in the point of subjective simultaneity from the pre- to the post-test phase) averaged across all participants as a function of adapter SOA. Error bars: <italic>±</italic>SEM.</p></caption>
<graphic xlink:href="584189v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Modeling results</title>
<p>In the following sections, we describe our models for cross-modal temporal recalibration by first laying out the general assumptions of these models, and then explaining the differences between them. Then, we provide a comparison in terms of model performance and illustrate how well the models capture the observed data by generating model predictions.</p>
<sec id="s2b1">
<label>2.2.1</label>
<title>General model assumptions</title>
<p>We formulated four models of cross-modal temporal recalibration (<xref rid="fig3" ref-type="fig">Figure 3</xref>). These models share several common assumptions. First, when an auditory and a visual signal are presented, the corresponding neural signals arrive in the relevant brain areas with a variable latency due to internal and external noise. The probability distribution of arrival latency is an exponential distribution (<xref ref-type="bibr" rid="c15">García-Pérez &amp; Alcalá-Quintana, 2012</xref>) (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). A simple derivation shows that the resulting measurements of SOA follow a double-exponential distribution (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). The mode reflects the physical SOA plus the participant’s audiovisual temporal bias. The slopes of the distribution reflect the uncertainties of the arrival latency; the steeper the slope, the less variable the latency, and the less uncertainty a Bayesian observer would have in a single trial. Second, these models define temporal recalibration as accumulating updates of the audiovisual bias after each encounter with a SOA. The accumulated update of the audiovisual bias at the end of the exposure phase is then carried over to the post-test and persists throughout that phase. Lastly, the bias is assumed to be reset to the same initial value in the pre-test across all nine sessions, reflecting the stability of the audiovisual temporal bias across time (<xref rid="c4" ref-type="bibr">Badde, Ley, et al., 2020</xref>; <xref ref-type="bibr" rid="c16">Grabot &amp; van Wassenhove, 2017</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Illustration of the model for cross-modal temporal recalibration. (A) The probability density function of the arrival latency of the auditory and visual signals relative to the physical onset of each stimulus. (B) The resulting probability density function of the measured SOA, <italic>m</italic>, before (dashed) and after (solid) recalibration. The measurement distribution peaks at the physical SOA plus an audiovisual temporal bias. Temporal recalibration is modeled as cumulative changes in the audiovisual bias, Δ, across the exposure phase. (C) Two recalibration models. The fixed-update model updates the audiovisual bias so that subsequent measurements of the audiovisual SOA approach zero. The causal-inference model updates the audiovisual bias based on the perceived SOA, <italic>ŝ</italic>, i.e., taking different causal scenarios into account. The percept <italic>ŝ</italic> is computed as a weighted average of estimates inferred from the scenarios of a common cause, <italic>C</italic> = 1, and separate causes, <italic>C</italic> = 2. <italic>α</italic>: learning rate. See text for details.</p></caption>
<graphic xlink:href="584189v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b2">
<label>2.2.2</label>
<title>Models of cross-modal temporal recalibration</title>
<p>The four models we tested differed in the mechanism governing the updates of the audiovisual bias during the exposure phase as well as the modality specificity of the arrival latency uncertainty.</p>
<p>We formulated a Bayesian causal-inference model (<xref ref-type="bibr" rid="c23">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="c28">McGovern et al., 2016</xref>; <xref ref-type="bibr" rid="c38">Sato et al., 2007</xref>) to describe the recalibration of the relative timing between cross-modal signals. When an observer is presented with an adapter SOA during the exposure phase, they infer the causal relationship between the auditory and visual stimulus. Specifically, the observer computes two intermediate estimates of the SOA, one for each causal scenario (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). In the common-cause scenario, the estimated SOA of the stimuli is smaller than the measurement as it is combined with a prior distribution over SOA that reflects synchrony. In the separate-causes scenario, the estimated SOA is approximately equal to the measurement. The two estimates are then averaged with each one weighted by the inferred probability of the corresponding causal scenario. The audiovisual bias is then updated to reduce the difference between the measurement and the combined estimate of SOA. In other words, causal inference regulates the recalibration process by shifting the measured SOA to more closely match the percept, which in turn is computed based on the inferred causal structure.</p>
<p>We also considered a fixed-update model. The major distinction between the causal-inference and the fixed-update model is that, according to the latter, the measured SOA is shifted toward zero rather than toward the inferred SOA. Essentially, whenever the observer detects a SOA, they recalibrate by shifting the audiovisual bias in the opposite direction so that the measured SOA will be closer to zero.</p>
<p>We additionally varied a second model element: we assumed either modality-specific or modality-independent uncertainty of arrival latency. The auditory system typically has higher temporal precision than the visual system. Hence, the arrival latency of visual signals can be more variable than auditory-signal latency, resulting in an asymmetrical probability density of measured SOA (<italic>m</italic>). A Bayesian observer will take this modality-specific sensory uncertainty into account to derive an estimate of SOA (<italic>ŝ</italic>). However, temporal precision might not be due to the variability of arrival latency. The auditory and visual systems might share a common, modality-independent timing mechanism (<xref ref-type="bibr" rid="c40">Stauffer et al., 2012</xref>), predicting modality-independent uncertainty.</p>
</sec>
<sec id="s2b3">
<label>2.2.3</label>
<title>Model comparison</title>
<p>We fitted four models to each participant’s data. Each model was constrained jointly by the temporal-order judgments from the pre- and post-tests of all nine sessions. To quantify model performance, we computed the Akaike information criterion (AIC) for each model and each participant (<xref ref-type="bibr" rid="c2">Akaike, 1998</xref>). The model with the lowest AIC value was considered the best-fitting model. For all participants, the causal-inference model with modality-specific uncertainty outperformed the other three models. We then computed the AIC values of the other models relative to the best-fitting model, Δ<italic>AIC</italic>, with higher Δ<sub><italic>AIC</italic></sub> values indicating stronger evidence for the best-fitting model. The results of model comparison revealed robust evidence for the causal-inference model with modality-specific uncertainty (Δ<sub><italic>AIC</italic></sub> = 55.68 <italic>±</italic> 21.45 for the fixed-update model with modality-specific uncertainty; Δ<sub><italic>AIC</italic></sub> = 48.71 <italic>±</italic> 18.51 for the fixed-update model with modality-independent uncertainty; Δ<sub><italic>AIC</italic></sub> = 12 <italic>±</italic> 5.94 for the causal-inference model with modality-independent uncertainty).</p>
</sec>
<sec id="s2b4">
<label>2.2.4</label>
<title>Model prediction</title>
<p>We predicted the recalibration effect per adapter SOA using the estimated parameters based on each of the four models. The nonlinearity in audiovisual temporal recalibration was only captured by models that rely on causal inference during the exposure phase (<xref rid="fig4" ref-type="fig">Figure 4A</xref>; see Figure S5 for other variants of the causal-inference model; see Figures S6 and S7 for model predictions for individual participants’ recalibration effects and TOJ responses). On the other hand, the models that assume a fixed update based on the measured SOA were unable to capture the data, as they predict a linear increase of recalibration with greater adapter SOA. We derived the asymmetry index (i.e., the recalibration effect summed across sessions) for the predictions of each model and compared these indices with those computed directly from the data. To capture participants’ idiosyncratic asymmetry in temporal recalibration, the model not only requires modality-specific uncertainty of arrival latency, it also needs to account for causal inference during the exposure phase (<xref rid="fig4" ref-type="fig">Figure 4B</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Model predictions. (A) Data and model predictions of recalibration as a function of adapter SOA. (B) Model prediction of the asymmetry index, the summed recalibration effect across adapter SOA. Dots: individual participants. Error bars: 68% bootstrapped confidence intervals. Identity line: perfect model prediction.</p></caption>
<graphic xlink:href="584189v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b5">
<label>2.2.5</label>
<title>Model simulation</title>
<p>Simulations with the best-fitting model revealed key factors that determine the degree of non-linearity and asymmetry of cross-modal temporal recalibration to different adapter SOAs. The belief that the auditory and visual stimuli share a common cause plays a crucial role in adjudicating between these two causal scenarios (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). When the observer infers that the audiovisual stimuli share the same cause, they recalibrate by a proportion of the perceived asynchrony no matter how large the measured asynchrony is, identical to the fixed-update model. On the contrary, when the observer infers that the audiovisual stimuli have separate causes, they treat the audiovisual stimuli as independent of each other and do not recalibrate. Estimates of the common-cause prior for all participants range between these two extremes. Thus, all observers weighted the estimates from these two scenarios based on the scenarios’ probability, resulting in the nonlinear pattern of recalibration (see Table S1 for parameter estimates for individual participants).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Model simulations. (A) Effect of the prior probability of a common cause on cross-modal temporal recalibration. (B) Asymmetry due to differing uncertainty of auditory vs. visual arrival latency. Left panel: When auditory uncertainty is smaller than visual uncertainty, reducing auditory uncertainty leads to less recalibration in response to visual-lead adapter SOA. Right panel: when visual uncertainty is smaller than auditory uncertainty, the opposite effect results. Top-left insets: corresponding SOA likelihood functions for a measured SOA of zero as auditory or visual uncertainty is varied.</p></caption>
<graphic xlink:href="584189v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Differences in arrival-time uncertainty between audition and vision result in an asymmetry of audiovisual temporal recalibration across adapter SOAs (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). The amount of recalibration is attenuated when the modality with less uncertainty lags during the exposure phase. When the lagging stimulus is less uncertain, the perceptual system is more likely to attribute the SOA to separate causes and thus recalibrate less. In addition, the initial audiovisual bias does not affect asymmetry, but shifts the recalibration function horizontally and determines the SOA for which no recalibration occurs (Figure S8).</p>
</sec>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>In this study, we examined audiovisual temporal recalibration by repeatedly exposing participants to various stimulus-onset asynchronies and measured perceived audiovisual relative timing before and after exposure. To further understand the mechanisms underlying audiovisual temporal recalibration, we assessed the efficacy of different models of the recalibration process in predicting the amount of recalibration as a function of the audiovisual asynchrony to which one is exposed. Our findings indicate that a Bayesian causal-inference model with modality-specific uncertainty best captured the two key features of cross-modal temporal recalibration: the non-linear increase of recalibration magnitude with increasing adapted audiovisual asynchrony, and the asymmetrical recalibration magnitude between auditory- and visual-lead adapters with the same absolute asynchrony. Our results indicate that human observers employ causal-inference-based percepts to recalibrate cross-modal temporal perception.</p>
<p>In cross-modal recalibration, causal inference effectively serves as a credit-assignment mechanism, evaluating to what extent the source of discrepancy is external (i.e., the stimuli from the two modalities occurred at different times in the world) or internal (i.e., the measurement of asynchrony resulted from internal miscalibration). The perceptual system should correct for errors if they are due to misalignment between the senses. It shouldn’t recalibrate if two in-dependent events, such as your TV screen and the neighbors’ conversation, exhibit audiovisual asynchrony. The same principle also applies to other cross-modal domains. The relevance of causal inference extends beyond temporal recalibration, influencing cross-modal spatial recalibration (<xref ref-type="bibr" rid="c5">Badde, Navarro, &amp; Landy, 2020</xref>; <xref ref-type="bibr" rid="c20">Hong et al., 2021</xref>; <xref ref-type="bibr" rid="c38">Sato et al., 2007</xref>; <xref ref-type="bibr" rid="c51">Wozny &amp; Shams, 2011a</xref>, <xref ref-type="bibr" rid="c52">2011b</xref>). Similarly, in sensorimotor adaptation, humans correct for motor errors that are more likely due to the motor system, but not due to the environment (<xref ref-type="bibr" rid="c7">Berniker &amp; Kording, 2008</xref>; <xref ref-type="bibr" rid="c49">Wei &amp; Körding, 2009</xref>).</p>
<p>Previous investigations into the mechanisms behind audiovisual temporal recalibration have proposed various models. These models describe recalibration as a selective reduction of response gain of the adapted asynchrony in a population code (<xref ref-type="bibr" rid="c32">Roach et al., 2011</xref>), a shift of latency or response criteria (<xref ref-type="bibr" rid="c55">Yarrow et al., 2015</xref>), changes in temporal discriminability (<xref ref-type="bibr" rid="c35">Roseboom et al., 2015</xref>), or the update of prior and likelihood function (<xref ref-type="bibr" rid="c37">Sato &amp; Aihara, 2011</xref>). However, a common feature of the experimental methods in these studies is to examine the recalibration process within a relatively narrow range of adapted audiovisual asynchrony. This is based on the assumption that the audiovisual stimuli are perceived as originating from the same source, which holds for small asynchronies. Our model seeks to go beyond this limitation by incorporating causal inference, which extends the model applicability across a wider range of audiovisual asynchrony.</p>
<p>In addition to the nonlinear pattern of temporal recalibration, our results revealed significant asymmetry in how much participants recalibrated to visual- vs. auditory-lead stimuli. The majority of our participants showed larger recalibration effects in response to auditory-than visual-lead asynchrony, in line with previous studies (<xref ref-type="bibr" rid="c30">O’Donohue et al., 2022</xref>). Simulation results supported the idea that this asymmetry could be due to less uncertainty in auditory arrival latency, in line with psychophysical studies (reviewed by <xref ref-type="bibr" rid="c40">Stauffer et al., 2012</xref>) that found audition has better temporal sensitivity than vision. However, our findings also highlighted individual differences: a few participants showed the opposite pattern, which was also revealed before (<xref ref-type="bibr" rid="c14">Fujisaki et al., 2004</xref>). We speculate that in certain conditions, visual arrival-latency might be less variable than auditory latency if the auditory signal is influenced by environmental factors such as echoes. Accordingly, our model explains how temporal uncertainty, based on the precision of the perceptual system and temporal variability of the physical stimulus, can lead to different directions of asymmetry in audiovisual temporal recalibration.</p>
<p>The principle of causal inference in audiovisual temporal recalibration is likely to extend to rapid cross-modal temporal recalibration, which occurs following the exposure to a single and brief audiovisual asynchrony (<xref ref-type="bibr" rid="c42">Van der Burg et al., 2013</xref>). However, it is an open question whether modality-specific uncertainty can explain the asymmetry of rapid cross-modal temporal recalibration. The pattern of asymmetry for rapid temporal recalibration differs from that of cumulative recalibration; in rapid recalibration, recalibration magnitude and the range in which recalibration occurs is larger when vision leads than when audition leads (<xref ref-type="bibr" rid="c42">Van der Burg et al., 2013</xref>, <xref ref-type="bibr" rid="c43">2015</xref>). Such findings suggest that the mechanisms behind rapid and cumulative temporal recalibration may differ fundamentally. Supporting this, recent neuroimaging research has revealed distinct underlying neurophysiological processes. Cumulative temporal recalibration induces gradual phase shifts of entrained neural oscillations in the auditory cortex (<xref ref-type="bibr" rid="c24">Kösem et al., 2014</xref>), whereas rapid recalibration relies on phase-frequency coupling that happens at a faster time scale (<xref ref-type="bibr" rid="c25">Lennert et al., 2021</xref>).</p>
<p>In sum, we found that causal inference with modality-specific uncertainty modulates audiovisual temporal recalibration. This finding suggests that cross-modal temporal recalibration is more complex than a compensatory mechanism for maintaining accuracy or consistency. It relies on causal inference that considers both the sensory and causal uncertainty of multisensory inputs. Cross-modal temporal recalibration is typically viewed as an early-stage, low-level perceptual process. Our findings refine this view, suggesting that it is deeply intertwined with higher cognitive functions.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Participants</title>
<p>Ten students from New York University (three males; age: 24.4 <italic>±</italic> 1.77; all right-handed) participated in the experiment. They all reported normal or corrected-to-normal vision. All participants provided informed written consent before the experiment and received $15/hr as monetary compensation. The study was conducted in accordance with the guidelines laid down in the Declaration of Helsinki and approved by the New York University institutional review board. Data of one of the participants was identified as an outlier and therefore excluded from further data analysis (Figure S4).</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Apparatus and stimuli</title>
<p>Participants completed the experiments in a dark and semi sound-attenuated room. They were seated 1 m from an acoustically transparent, white screen (1.36 × 1.02 m, 68 × 52° visual angle) and placed their head on a chin rest. An LCD projector (Hitachi CP-X3010N, 1024 × 768 pixels, 60 Hz) was mounted above and behind participants to project visual stimuli on the screen. The visual stimulus was a high-contrast (36.1 cd<italic>/</italic>m<sup>2</sup>) Gaussian blob (SD: 3.6°) on a gray background (10.2 cd<italic>/</italic>m<sup>2</sup>) projected onto the screen. The auditory stimulus was a 500 Hz beep (50 dB SPL) played by a loudspeaker behind and located at the center of the screen. The visual and auditory stimulus durations were 33.33 ms. We adjusted the timing of audiovisual stimulus presentations and verified the timing using an oscilloscope (PICOSCOPE 2204A).</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Procedure</title>
<p>The experiment consisted of nine sessions, which took place on nine separate days. In each session, participants completed a pre-test, an exposure phase, and a post-test in sequence. The adapter SOA was fixed within a session, but varied across sessions (<italic>±</italic>700, <italic>±</italic>300, <italic>±</italic>200, <italic>±</italic>100, 0 ms). The intensities of the oddball stimuli were determined prior to the experiment for each participant using an intensity-discrimination task to equate the difficulty of detecting oddball stimuli between participants and across modalities.</p>
<sec id="s4c1">
<label>4.3.1</label>
<title>Pre-test phase</title>
<p>Participants completed a TOJ task during the pre-test phase. Each trial started with the display of a fixation cross (0.1–0.2 s, uniform distribution), followed by a blank screen (0.4–0.6 s, uniform distribution). Then, an auditory and a visual stimulus (0.033 s) were presented with a variable SOA. There were a total of 15 possible test SOAs (from -0.5 to 0.5 s in steps of 0.05 s), with positive values representing visual lead and negative values representing auditory lead. Following stimulus presentation there was another blank screen (0.4–0.6 s, uniform distribution), and then a response probe appeared on the screen. Participants indicated by button press whether the auditory stimulus occurred before the visual stimulus, occurred after, or the two were simultaneous. There was no time limit for the response, and response feedback was not provided. The inter-trial interval (ITI) was 0.2–0.4 s. Each test SOA was presented 20 times in pseudo-randomized order, resulting in 300 trials in total, divided into five blocks. Participants usually took around 15 minutes to finish the pre-test phase.</p>
</sec>
<sec id="s4c2">
<label>4.3.2</label>
<title>Exposure phase</title>
<p>Participants completed an oddball-detection task during the exposure phase. In each trial, participants were presented with an audiovisual stimulus pair with a fixed SOA (adapter SOA). In 10% of trials, the intensity of either the visual or the auditory component (or both) was greater than in the other trials. Participants were instructed to press a button as soon as possible when there was an auditory oddball, a visual oddball, or both stimuli were oddballs. The task timing was almost identical to the TOJ task, except that there was a response time limit of 1.4 s. The visual and auditory oddball stimuli were presented to participants prior to the exposure phase and they practiced as much as they needed to familiarize themselves with the task. There were a total of 250 trials, divided into five blocks. At the end of each block, we presented a performance summary with the hit and false alarm rates for each modality. Participants usually took 15 minutes to complete the exposure phase.</p>
</sec>
<sec id="s4c3">
<label>4.3.3</label>
<title>Post-test phase</title>
<p>Participants completed the TOJ task as well as the oddball-detection task during the post-test phase. Specifically, each temporal-order judgment was preceded by three top-up (oddball-detection) trials. The adapter SOA in the top-up trials was the same as that in the exposure phase to prevent dissipation of temporal recalibration (<xref ref-type="bibr" rid="c27">Machulla et al., 2012</xref>). To facilitate task switching, the ITI between the last top-up trial and the following TOJ trial was longer (with the additional time jittered around 1 s). Additionally, the fixation cross became red to signal the start of a TOJ trial. As in the pre-test phase, there were 300 TOJ trials (15 test SOAs <italic>×</italic> 20 repetitions) with the addition of 900 top-up trials, grouped into six blocks. At the end of each block, we provided a summary of the oddball-detection performance. Participants usually took around 1 hour to complete the post-test phase.</p>
</sec>
<sec id="s4c4">
<label>4.3.4</label>
<title>Intensity-discrimination task</title>
<p>This task was conducted to estimate the just-noticeable-difference (JND) in intensity for a standard visual stimulus with a luminance of 36.1 cd<italic>/</italic>m<sup>2</sup>, and a standard auditory stimulus with a volume of 40 dB SPL. The task was two-interval, forced-choice. The trial started with a fixation (0.1–0.2 s) and a blank screen (0.4–0.6 s). Participants were presented with a standard stimulus in one randomly selected interval (0.4–0.6 s) and a comparison stimulus in the other interval (0.4–0.6 s), temporally separated by an inter-stimulus interval (0.6–0.8 s). They indicated which interval contained the brighter/louder stimulus without time constraint. Seven test stimulus levels (luminance range: 5%–195%; volume range: 50%–150% of the standard) were repeated 20 times, resulting in 140 trials for each task. We fit a Gaussian cumulative distribution function to these data and defined the JND as the intensity difference for which the test stimulus was chosen 90% of the time as more intense than the standard. An oddball was defined as an auditory or visual stimulus with an intensity 1 JND above the standard intensity.</p>
</sec>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Modeling</title>
<p>In this section, we use the best-fitting model that combines causal inference with modality-specific uncertainty as a template, and then describe how the alternative models differ from this. We start by describing how the arrival latencies of auditory and visual stimuli lead to noisy internal measurements of audiovisual SOA, followed by how we modeled the process of audiovisual temporal recalibration. Then, we provide a formalization of the TOJ task administered in the pre- and the post-test phases, data from which were used to constrain the model parameters. Finally, we describe how the models were fit to the data.</p>
<sec id="s4d1">
<label>4.4.1</label>
<title>Measurements of audiovisual stimulus-onset-asynchrony</title>
<p>When an audiovisual stimulus pair with a stimulus onset asynchrony, <italic>s</italic> = <italic>t</italic><sub><italic>A</italic></sub> <italic>− t</italic><sub><italic>V</italic></sub>, is presented, it triggers auditory and visual signals that are registered with different latency in the region of cortex where audiovisual comparisons are made. This leads to two internal measurements in an observer’s brain. As in previous work (Garcí a-Pé rez &amp; Alcalá -Quintana, 2012), we model the probability of the latency of auditory and visual signals across repetitions, relative to the physical onset of each stimulus, as shifted exponential distributions (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). These distributions may be shifted relative to the physical stimulus onset due to internal signal delays (denoted <italic>β</italic><sub><italic>V</italic></sub> and <italic>β</italic><sub><italic>A</italic></sub>). The arrival latency of the auditory signal relative to onset <italic>t</italic><sub><italic>A</italic></sub> is the sum of the fixed delay, <italic>β</italic><sub><italic>A</italic></sub>, and an additional delay that is exponentially distributed with time constant <italic>τ</italic><sub><italic>A</italic></sub>, and similarly for the visual arrival latency (with delay <italic>β</italic><sub><italic>V</italic></sub> and time constant <italic>τ</italic><sub><italic>V</italic></sub>).</p>
<p>The measured SOA of the audiovisual stimulus pair is modeled as the difference in the arrival latency of both stimuli. Thus, the measured audiovisual SOA <italic>m</italic> includes the physical SOA <italic>s</italic>, the fixed latency (i.e., the difference between the auditory and visual fixed latency) <italic>β</italic> = <italic>β</italic><sub><italic>A</italic></sub> <italic>− β</italic><sub><italic>V</italic></sub>, and a stochastic component. Given that both latency distributions are shifted exponential distributions, a noisy sensory measurement of SOA <italic>m</italic> given a physical SOA <italic>s</italic> has a probability density that is an asymmetric double-exponential (<xref rid="fig6" ref-type="fig">Figure 6A</xref>):
<disp-formula id="eqn1">
<graphic xlink:href="584189v1_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><p>Causal-inference model of the temporal-order-judgment task. (A) An example measurement distribution for a SOA of zero. The measurement distribution peaks at the audiovisual bias <italic>β</italic>, whose left and right slopes reflect the visual and auditory uncertainty, respectively. The point of subjective simultaneity is marked by the dashed line. (B) Simulated estimate distribution for a SOA of zero. The dashed lines represent the criteria placed symmetrically around zero, forming a temporal window of SOA estimates treated as simultaneous. The areas under the estimate distribution partitioned by the criteria indicate the probabilities of the three possible responses for a stimulus pair with a SOA of zero. (C) Simulated psychometric function computed by repeatedly calculating the probability of each possible response for all SOAs.</p></caption>
<graphic xlink:href="584189v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The mode of this measurement distribution is the physical SOA plus the fixed latency <italic>s</italic> + <italic>β</italic>. A negative value of <italic>β</italic> indicates faster auditory processing. The left and right spread of this measurement distribution depends on the uncertainty of the visual latency <italic>τ</italic><sub><italic>V</italic></sub> and auditory latency <italic>τ</italic><sub><italic>A</italic></sub>, respectively.</p>
</sec>
<sec id="s4d2">
<label>4.4.2</label>
<title>The perceptual inference process</title>
<p>To infer the stimulus SOA <italic>s</italic> from the measurement <italic>m</italic>, the ideal observer initially computes the posterior distribution of the SOA <italic>s</italic> by multiplying the likelihood function and the prior for two causal scenarios. The auditory and visual stimuli can arise from a single cause (<italic>C</italic> = 1) or two independent causes (<italic>C</italic> = 2).</p>
<p>The ideal observer holds two prior distributions of audiovisual SOA, one for each causal scenario. In the case of a common cause (<italic>C</italic> = 1), the prior distribution of the SOA between sound and light is a narrow Gaussian distribution (<xref ref-type="bibr" rid="c28">McGovern et al., 2016</xref>),
<disp-formula id="eqn2">
<graphic xlink:href="584189v1_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
When there are two separate causes (<italic>C</italic> = 2), the prior distribution of the audiovisual SOA is a broad Gaussian distribution (<xref ref-type="bibr" rid="c28">McGovern et al., 2016</xref>), assigning almost equal probability to each audiovisual SOA
<disp-formula id="eqn3">
<graphic xlink:href="584189v1_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The observer obtains intermediate estimates of the stimulus SOA by combining the measured SOA with the prior over SOA corresponding to the two causal scenarios, <italic>ŝ</italic><sub><italic>C</italic> =1</sub> and <italic>ŝ</italic><sub><italic>C</italic> = 2</sub>. In this model, we assume that this observer doesn’t have access to, or chooses to ignore, the current temporal bias <italic>β</italic>.</p>
<p>The likelihood functions under the two causal scenarios are identical:
<disp-formula id="eqn4">
<graphic xlink:href="584189v1_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the left and right spreads depend on auditory and visual uncertainties of the arrival latency. Because the likelihood function is non-Gaussian, there is no closed form for the intermediate estimate. We computed the posterior numerically and used a maximum-a-posteriori (MAP) estimator, i.e., <italic>ŝ</italic> was the mode of the posterior over stimulus SOA in each scenario.</p>
<p>The final estimate of the stimulus SOA <italic>ŝ</italic> depends on the posterior probability of each causal scenario. By Bayes Rule, the posterior probability that an audiovisual stimulus pair with the measured SOA shares a common cause is
<disp-formula id="eqn5">
<graphic xlink:href="584189v1_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The likelihood of a common source/separate sources for a fixed SOA measurement is calculated by numerically integrating the protoposterior (i.e., the unnormalized posterior),
<disp-formula id="eqn6">
<graphic xlink:href="584189v1_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The posterior probability of a common cause additionally depends on the observer’s prior belief of a common cause for the auditory and visual stimuli, <italic>P</italic> (<italic>C</italic> = 1) = <italic>p</italic><sub>common</sub>.</p>
<p>The final estimate of SOA is derived by model averaging, so that the final estimate is the average of the scenario-specific SOA estimates above weighted by the posterior probability of the corresponding causal scenario,
<disp-formula id="eqn7">
<graphic xlink:href="584189v1_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4d3">
<label>4.4.3</label>
<title>Formalization of recalibration in the exposure phase</title>
<p>We model the recalibration process as a shift of the audiovisual fixed latency <italic>β</italic>, audiovisual temporal bias, after encountering an audiovisual stimulus pair (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). The internal value of <italic>β</italic> reflects the observed point of subjective simultaneity (PSS), which is the stimulus SOA <italic>s</italic> that leads to a median measurement of audiovisual synchrony equal to <italic>m</italic> = 0. That is, it is the value of <italic>s</italic> such that <italic>P</italic> (<italic>m &lt;</italic> 0|<italic>s</italic> = PSS, <italic>β</italic>) = 0.5. A simple derivation yields
<disp-formula id="eqn8">
<graphic xlink:href="584189v1_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The shift of the audiovisual bias <italic>β</italic> also moves the measurement distribution. We assume the exponential time constants (<italic>τ</italic><sub><italic>A</italic></sub>, <italic>τ</italic><sub><italic>V</italic></sub>) remain unchanged across phases and sessions.</p>
<p>At the end of every exposure trial <italic>i</italic>, a discrepancy between the measured SOA, <italic>m</italic><sub><italic>i</italic></sub> and the final estimate of the stimulus SOA <italic>ŝ</italic><sub><italic>i</italic></sub> signals the need for recalibration. In each session, we assume the participant arrives with a default bias <italic>β</italic>. We define Δ<sub><italic>β,i</italic></sub> as the cumulative shift of audiovisual bias after exposure trial <italic>i</italic>,
<disp-formula id="eqn9">
<graphic xlink:href="584189v1_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>α</italic> is the learning rate. At the end of the exposure phase, the predicted audiovisual bias is thus shifted by the accumulated shifts across the exposure phase, that is, <italic>β</italic><sub>post</sub> = <italic>β</italic> + Δ<sub><italic>β</italic>,250</sub>.</p>
</sec>
<sec id="s4d4">
<label>4.4.4</label>
<title>Formalization of the temporal-order-judgement task</title>
<p>In the TOJ task administered in the pre- and post-test phases, the observer makes a perceptual judgment by comparing the final estimate of stimulus SOA <italic>ŝ</italic> to two internal criteria (<xref ref-type="bibr" rid="c10">Cary et al., 2024</xref>; <xref ref-type="bibr" rid="c15">García-Pérez &amp; Alcalá-Quintana, 2012</xref>). We assume that the observer has a symmetric pair of criteria, <italic>±c</italic>, centered on the stimulus SOA corresponding to perceptual simultaneity (<italic>ŝ</italic> = 0). In addition, the observer may lapse or make an error when responding. The probabilities of reporting visual lead, Ψ<sub><italic>V</italic></sub>, auditory lead, Ψ<sub><italic>A</italic></sub> or that the two stimuli were simultaneous, Ψ<sub><italic>S</italic></sub>, are thus
<disp-formula id="eqn10">
<graphic xlink:href="584189v1_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>λ</italic> is the lapse rate. <xref rid="fig6" ref-type="fig">Figure 6C</xref> shows an example of the resulting psychometric functions.</p>
<p>The probability distribution of causal-inference-based stimulus SOA estimates <italic>P</italic> (<italic>ŝ</italic>|<italic>s</italic>) has no closed form and can only be simulated. For each simulation we sampled 10,000 SOA measurements from the corresponding double-exponential probability distribution (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). For each sampled measurement, we simulated the process by which the observer carries out causal inference and produced an estimate of the stimulus SOA (fixing the values of a few additional causal-inference model parameters). This process resulted in a Monte-Carlo approximation of the probability distribution of the causal-inference-based stimulus SOA estimates (<xref rid="fig6" ref-type="fig">Figure 6B</xref>).</p>
</sec>
<sec id="s4d5">
<label>4.4.5</label>
<title>Alternative models</title>
<p>In the fixed-update model, observers measure the audiovisual SOA <italic>s</italic> by comparing the arrival latency of the auditory and visual signals (<xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>). They do not perform causal inference to estimate the SOA. Instead, the measured SOA is shifted toward zero by recalibrating the audiovisual bias <italic>β</italic>. Hence, the update of the audiovisual bias in trial <italic>i</italic> is defined by
<disp-formula id="eqn11">
<graphic xlink:href="584189v1_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The update of audiovisual bias is accumulated across the exposure phase, <italic>β</italic><sub>post</sub> = <italic>β</italic> + Δ<sub><italic>β</italic>,250</sub>. In TOJ tasks, observers make the temporal-order decision by applying the criteria to the measurement of SOA <italic>m</italic> (see psychometric functions in Supplement Eq. S1).</p>
<p>In models with modal-independent uncertainty, <italic>τ</italic><sub><italic>A</italic></sub> = <italic>τ</italic><sub><italic>V</italic></sub>, resulting in a symmetrical measurement distribution (<xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>).</p>
</sec>
<sec id="s4d6">
<label>4.4.6</label>
<title>Model fitting</title>
<sec id="s4d6a">
<title>Model log-likelihood</title>
<p>The model was fitted by maximizing likelihood. We fit the model to the TOJ data collected during the pre- and post-test phases of all sessions together. We did not collect temporal-order judgments in the exposure phase. However, to model the post-test data, we needed to estimate the distribution of shifts of audiovisual bias resulting from the exposure phase (Δ<sub><italic>β</italic>,250</sub>). We did this using Monte Carlo simulation of the 250 exposure trials to estimate the probability distribution of the cumulative shifts.</p>
<p>The set of model parameters Θ is listed in <xref rid="tbl1" ref-type="table">Table 1</xref>. There are <italic>I</italic> sessions, each including <italic>K</italic> trials in the pre-test phase and <italic>K</italic> trials in the post-test phase. We denote the full dataset of pre-test data as <italic>X</italic><sub>pre</sub> and for the post-test data as <italic>X</italic><sub>post</sub>. On a given trial, the observer responds either auditory-first (A), visual-first (V) or simultaneous (S). We denote a single response using indicator variables that are equal to 1 if that was the response in that trial and 0 otherwise. These variables for trial <italic>k</italic> in session <italic>i</italic> are <inline-formula><inline-graphic xlink:href="584189v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="584189v1_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for the pre-test trials, and <inline-formula><inline-graphic xlink:href="584189v1_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, etc., for the post-test trials. The log-likelihood of all pre-test responses <italic>X</italic><sub>pre</sub> given the model parameters given is
<disp-formula id="eqn12">
<graphic xlink:href="584189v1_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Model parameters. Check marks signify that the parameter is used for determining the likelihood of the data from the temporal-order judgment task in the pre- and post-test phase and/or for the Monte Carlo simulation of recalibration in the exposure phase.</p></caption>
<graphic xlink:href="584189v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>The psychometric functions for the pre-test (e.g., Ψ<sub><italic>A</italic>,pre</sub>) are defined in <xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>, and are the same across all sessions as we assumed that the audiovisual delay <italic>β</italic> was the same before recalibration in every session.</p>
<p>The log-likelihood of responses in the post-test depends on the audiovisual bias after re-calibration <italic>β</italic><sub>post</sub> = <italic>β</italic> + Δ<sub><italic>β</italic>,250,<italic>i</italic></sub> for session <italic>i</italic>. To determine the log-likelihood of the post-test data requires us to integrate out the unknown value of the cumulative shift Δ<sub><italic>β</italic>,250,<italic>i</italic></sub>. We approximated this integral in two steps based on our previous work (<xref ref-type="bibr" rid="c20">Hong et al., 2021</xref>). First, we simulated the 250 exposure-phase trials 1,000 times for a given set of parameters Θ and session <italic>i</italic>. This resulted in 1,000 values of Δ<sub><italic>β</italic>,250,<italic>i</italic></sub>. The distribution of these values was well fit by a Gaussian whose parameters were determined by the empirical mean and standard deviation of the sample distribution, resulting in the distribution <inline-formula><inline-graphic xlink:href="584189v1_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Second, we approximated the integral of the log-likelihood of the data over possible values of Δ<sub><italic>β</italic>,250,<italic>i</italic></sub> by numerical integration. We discretized the approximated distribution <inline-formula><inline-graphic xlink:href="584189v1_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> into 100 equally spaced bins centered on values Δ<sub><italic>β</italic>,250,<italic>i</italic></sub>(<italic>n</italic>) (<italic>n</italic> = 1, …, 100). The range of the bins was triple the range of the values from the Monte Carlo sample, so that the lower bound was <inline-formula><inline-graphic xlink:href="584189v1_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and the upper bound was <inline-formula><inline-graphic xlink:href="584189v1_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
<p>The log-likelihood of the post-test data is approximated as
<disp-formula id="eqn13">
<graphic xlink:href="584189v1_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where
<disp-formula id="eqn14">
<graphic xlink:href="584189v1_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The psychometric functions in the post-test (e.g., Ψ<sub><italic>A</italic>,post,<italic>in</italic></sub>) differ across sessions and bins because the simulated bias after recalibration <italic>β</italic><sub><italic>i</italic>,post</sub> depends on the adapter SOA fixed in session <italic>i</italic> and the simulation bin <italic>n</italic>.</p>
</sec>
<sec id="s4d6b">
<title>Parameter estimation</title>
<p>We used the BADS toolbox (<xref ref-type="bibr" rid="c1">Acerbi &amp; Ma, 2017</xref>) in MATLAB to optimize the set of parameters for the models because it outperforms fmincon when parameter numbers increase. We repeated each search 80 times with a different and random starting point to address the possibility of reporting a local minimum, and chose the parameter estimates with the maximum likelihood across the repeated searches.</p>
</sec>
</sec>
</sec>
</sec>
<sec id="d1e1435" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1520">
<label>Supplementary Materials</label>
<media xlink:href="supplements/584189_file03.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="confproc"><string-name><surname>Acerbi</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Ma</surname>, <given-names>W. J.</given-names></string-name> (<year>2017</year>). <article-title>Practical bayesian optimization for model fitting with bayesian adaptive direct search</article-title>. <source>Proceedings of the 31st International Conference on Neural Information Processing Systems</source>, <fpage>1834</fpage>–<lpage>1844</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="book"><string-name><surname>Akaike</surname>, <given-names>H.</given-names></string-name> (<year>1998</year>). <chapter-title>Information theory and an extension of the maximum likelihood principle</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>E.</given-names> <surname>Parzen</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Tanabe</surname></string-name>, &amp; <string-name><given-names>G.</given-names> <surname>Kitagawa</surname></string-name></person-group> (Eds.), <source>Selected papers of hirotugu akaike</source> (pp. <fpage>199</fpage>–<lpage>213</lpage>). <publisher-name>Springer</publisher-name> <publisher-loc>New York</publisher-loc>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Aller</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U.</given-names></string-name> (<year>2019</year>). <article-title>To integrate or not to integrate: Temporal dynamics of hierarchical bayesian causal inference</article-title>. <source>PLoS Biol</source>., <volume>17</volume> (<issue>4</issue>), <fpage>e3000210</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Badde</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ley</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rajendran</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Shareef</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Kekunnaya</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Röder</surname>, <given-names>B.</given-names></string-name> (<year>2020</year>). <article-title>Sensory experience during early sensitive periods shapes cross-modal temporal biases</article-title>. <source>Elife</source>, <volume>9</volume>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Badde</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Navarro</surname>, <given-names>K. T.</given-names></string-name>, &amp; <string-name><surname>Landy</surname>, <given-names>M. S.</given-names></string-name> (<year>2020</year>). <article-title>Modality-specific attention attenuates visual-tactile integration and recalibration effects by reducing prior expectations of a common source for vision and touch</article-title>. <source>Cognition</source>, <volume>197</volume>, <fpage>104170</fpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Bedford</surname>, <given-names>F. L.</given-names></string-name> (<year>1999</year>). <article-title>Keeping perception accurate</article-title>. <source>Trends Cogn. Sci</source>., <volume>3</volume> (<issue>1</issue>), <fpage>4</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Berniker</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Kording</surname>, <given-names>K.</given-names></string-name> (<year>2008</year>). <article-title>Estimating the sources of motor errors for adaptation and generalization</article-title>. <source>Nat. Neurosci</source>., <volume>11</volume> (<issue>12</issue>), <fpage>1454</fpage>–<lpage>1461</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Burge</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Girshick</surname>, <given-names>A. R.</given-names></string-name>, &amp; <string-name><surname>Banks</surname>, <given-names>M. S.</given-names></string-name> (<year>2010</year>). <article-title>Visual-haptic adaptation is determined by relative reliability</article-title>. <source>J. Neurosci</source>., <volume>30</volume> (<issue>22</issue>), <fpage>7714</fpage>–<lpage>7721</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Cao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Giordano</surname>, <given-names>B. L.</given-names></string-name>, &amp; <string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name> (<year>2019</year>). <article-title>Causal inference in the multisensory brain</article-title>. <source>Neuron</source>, <volume>102</volume> (<issue>5</issue>), <fpage>1076</fpage>–<lpage>1087.e8</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Cary</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lahdesmaki</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Badde</surname>, <given-names>S.</given-names></string-name> (<year>2024</year>). <article-title>Audiovisual simultaneity windows reflect temporal sensory uncertainty</article-title>. <source>Psychon. Bull. Rev</source>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Di Luca</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Machulla</surname>, <given-names>T.-K.</given-names></string-name>, &amp; <string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name> (<year>2009</year>). <article-title>Recalibration of multisensory simultaneity: Cross-modal transfer coincides with a change in perceptual latency</article-title>. <source>J. Vis</source>., <volume>9</volume> (<issue>12</issue>), <fpage>7.1</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Dokka</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Jansen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>DeAngelis</surname>, <given-names>G. C.</given-names></string-name>, &amp; <string-name><surname>Angelaki</surname>, <given-names>D. E.</given-names></string-name> (<year>2019</year>). <article-title>Causal inference accounts for heading perception in the presence of object motion</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>., <volume>116</volume> (<issue>18</issue>), <fpage>9060</fpage>–<lpage>9065</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="book"><string-name><surname>Fain</surname>, <given-names>G. L.</given-names></string-name> (<year>2019</year>, <month>December</month>). <source>Sensory transduction</source>. <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Fujisaki</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Shimojo</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kashino</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Nishida</surname>, <given-names>S.</given-names></string-name> (<year>2004</year>). <article-title>Recalibration of audiovisual simultaneity</article-title>. <source>Nat. Neurosci</source>., <volume>7</volume> (<issue>7</issue>), <fpage>773</fpage>–<lpage>778</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>García-Pérez</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Alcalá-Quintana</surname>, <given-names>R.</given-names></string-name> (<year>2012</year>). <article-title>On the discrepant results in synchrony judgment and temporal-order judgment tasks: A quantitative model</article-title>. <source>Psychon. Bull. Rev</source>., <volume>19</volume> (<issue>5</issue>), <fpage>820</fpage>–<lpage>846</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Grabot</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>van Wassenhove</surname>, <given-names>V.</given-names></string-name> (<year>2017</year>). <article-title>Time order as psychological bias</article-title>. <source>Psychol. Sci</source>., <volume>28</volume> (<issue>5</issue>), <fpage>670</fpage>–<lpage>678</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Hanson</surname>, <given-names>J. V. M.</given-names></string-name>, <string-name><surname>Heron</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Whitaker</surname>, <given-names>D.</given-names></string-name> (<year>2008</year>). <article-title>Recalibration of perceived time across sensory modalities</article-title>. <source>Exp. Brain Res</source>., <volume>185</volume> (<issue>2</issue>), <fpage>347</fpage>–<lpage>352</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Harrar</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Harris</surname>, <given-names>L. R.</given-names></string-name> (<year>2008</year>). <article-title>The effect of exposure to asynchronous audio, visual, and tactile stimulus combinations on the perception of simultaneity</article-title>. <source>Exp. Brain Res</source>., <volume>186</volume> (<issue>4</issue>), <fpage>517</fpage>–<lpage>524</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Heron</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Whitaker</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>McGraw</surname>, <given-names>P. V.</given-names></string-name>, &amp; <string-name><surname>Horoshenkov</surname>, <given-names>K. V.</given-names></string-name> (<year>2007</year>). <article-title>Adaptation minimizes distance-related audiovisual delays</article-title>. <source>J. Vis</source>., <volume>7</volume> (<issue>13</issue>), <fpage>5.1</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Hong</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Badde</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Landy</surname>, <given-names>M. S.</given-names></string-name> (<year>2021</year>). <article-title>Causal inference regulates audiovisual spatial recalibration via its influence on audiovisual perception</article-title>. <source>PLoS Comput. Biol</source>., <volume>17</volume> (<issue>11</issue>), <fpage>e1008877</fpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Keetels</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Vroomen</surname>, <given-names>J.</given-names></string-name> (<year>2007</year>). <article-title>No effect of auditory-visual spatial disparity on temporal recalibration</article-title>. <source>Exp. Brain Res</source>., <volume>182</volume> (<issue>4</issue>), <fpage>559</fpage>–<lpage>565</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>King</surname>, <given-names>A. J.</given-names></string-name> (<year>2005</year>). <article-title>Multisensory integration: Strategies for synchronization</article-title>. <source>Curr. Biol</source>., <volume>15</volume> (<issue>9</issue>), <fpage>R339</fpage>–<lpage>41</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Körding</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Beierholm</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>W. J.</given-names></string-name>, <string-name><surname>Quartz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name> (<year>2007</year>). <article-title>Causal inference in multisensory perception</article-title>. <source>PLoS One</source>, <volume>2</volume> (<issue>9</issue>), <fpage>e943</fpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Kösem</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>van Wassenhove</surname>, <given-names>V.</given-names></string-name> (<year>2014</year>). <article-title>Encoding of event timing in the phase of neural oscillations</article-title>. <source>Neuroimage</source>, <volume>92</volume>, <fpage>274</fpage>–<lpage>284</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Lennert</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Samiee</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Baillet</surname>, <given-names>S.</given-names></string-name> (<year>2021</year>). <article-title>Coupled oscillations enable rapid temporal recalibration to audiovisual asynchrony</article-title>. <source>Commun Biol</source>, <volume>4</volume> (<issue>1</issue>), <fpage>559</fpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Locke</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Landy</surname>, <given-names>M. S.</given-names></string-name> (<year>2017</year>). <article-title>Temporal causal inference with stochastic audio-visual sequences</article-title>. <source>PLoS One</source>, <volume>12</volume> (<issue>9</issue>), <fpage>e0183776</fpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Machulla</surname>, <given-names>T.-K.</given-names></string-name>, <string-name><surname>Di Luca</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Froehlich</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name> (<year>2012</year>). <article-title>Multisensory simultaneity recalibration: Storage of the aftereffect in the absence of counterevidence</article-title>. <source>Exp. Brain Res</source>., <volume>217</volume> (<issue>1</issue>), <fpage>89</fpage>–<lpage>97</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>McGovern</surname>, <given-names>D. P.</given-names></string-name>, <string-name><surname>Roudaia</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Newell</surname>, <given-names>F. N.</given-names></string-name>, &amp; <string-name><surname>Roach</surname>, <given-names>N. W.</given-names></string-name> (<year>2016</year>). <article-title>Perceptual learning shapes multisensory causal inference via two distinct mechanisms</article-title>. <source>Sci. Rep</source>., <volume>6</volume>, <fpage>24673</fpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Navarra</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Vatakis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zampini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Soto-Faraco</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Humphreys</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Spence</surname>, <given-names>C.</given-names></string-name> (<year>2005</year>). <article-title>Exposure to asynchronous audiovisual speech extends the temporal window for audiovisual integration</article-title>. <source>Brain Res. Cogn. Brain Res</source>., <volume>25</volume> (<issue>2</issue>), <fpage>499</fpage>–<lpage>507</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>O’Donohue</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lacherez</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Yamamoto</surname>, <given-names>N.</given-names></string-name> (<year>2022</year>). <article-title>Musical training refines audiovisual integration but does not influence temporal recalibration</article-title>. <source>Sci. Rep</source>., <volume>12</volume> (<issue>1</issue>), <fpage>15292</fpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="book"><string-name><surname>Pöppel</surname>, <given-names>E.</given-names></string-name> (<year>1988</year>). <source>Mindworks: Time and conscious experience</source>. <publisher-name>Harcourt Brace Jovanovich</publisher-name>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Roach</surname>, <given-names>N. W.</given-names></string-name>, <string-name><surname>Heron</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Whitaker</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>McGraw</surname>, <given-names>P. V.</given-names></string-name> (<year>2011</year>). <article-title>Asynchrony adaptation reveals neural population code for audio-visual timing</article-title>. <source>Proc. Biol. Sci</source>., <volume>278</volume> (<issue>1710</issue>), <fpage>1314</fpage>–<lpage>1322</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Rohde</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Greiner</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name> (<year>2014</year>). <article-title>Asymmetries in visuomotor recalibration of time perception: Does causal binding distort the window of integration?</article-title> <source>Acta Psychol</source>., <volume>147</volume>, <fpage>127</fpage>–<lpage>135</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Rohe</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U.</given-names></string-name> (<year>2015</year>). <article-title>Sensory reliability shapes perceptual inference via two mechanisms</article-title>. <source>J. Vis</source>., <volume>15</volume> (<issue>5</issue>), <fpage>22</fpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Roseboom</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Linares</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Nishida</surname>, <given-names>S.</given-names></string-name> (<year>2015</year>). <article-title>Sensory adaptation for timing perception</article-title>. <source>Proc. Biol. Sci</source>., <volume>282</volume> (<issue>1805</issue>).</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Samad</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chung</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name> (<year>2015</year>). <article-title>Perception of body ownership is driven by bayesian sensory inference</article-title>. <source>PLoS One</source>, <volume>10</volume> (<issue>2</issue>), <fpage>e0117178</fpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Sato</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Aihara</surname>, <given-names>K.</given-names></string-name> (<year>2011</year>). <article-title>A bayesian model of sensory adaptation</article-title>. <source>PLoS One</source>, <volume>6</volume> (<issue>4</issue>), <fpage>e19377</fpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Sato</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Toyoizumi</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Aihara</surname>, <given-names>K.</given-names></string-name> (<year>2007</year>). <article-title>Bayesian inference explains perception of unity and ventriloquism aftereffect: Identification of common sources of audiovisual stimuli</article-title>. <source>Neural Comput</source>., <volume>19</volume> (<issue>12</issue>), <fpage>3335</fpage>–<lpage>3355</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Spence</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Squire</surname>, <given-names>S.</given-names></string-name> (<year>2003</year>). <article-title>Multisensory integration: Maintaining the perception of synchrony</article-title>. <source>Current Biology</source></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Stauffer</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Haldemann</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Troche</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name><surname>Rammsayer</surname>, <given-names>T. H.</given-names></string-name> (<year>2012</year>). <article-title>Auditory and visual temporal sensitivity: Evidence for a hierarchical structure of modality-specific and modality-independent levels of temporal information processing</article-title>. <source>Psychol. Res</source>., <volume>76</volume> (<issue>1</issue>), <fpage>20</fpage>–<lpage>31</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Tanaka</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Asakawa</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Imai</surname>, <given-names>H.</given-names></string-name> (<year>2011</year>). <article-title>The change in perceptual synchrony between auditory and visual speech after exposure to asynchronous speech</article-title>. <source>Neuroreport</source>, <volume>22</volume> (<issue>14</issue>), <fpage>684</fpage>–<lpage>688</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Van der Burg</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Alais</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Cass</surname>, <given-names>J.</given-names></string-name> (<year>2013</year>). <article-title>Rapid recalibration to audiovisual asynchrony</article-title>. <source>J. Neurosci</source>., <volume>33</volume> (<issue>37</issue>), <fpage>14633</fpage>–<lpage>14637</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Van der Burg</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Alais</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Cass</surname>, <given-names>J.</given-names></string-name> (<year>2015</year>). <article-title>Audiovisual temporal recalibration occurs independently at two different time scales</article-title>. <source>Sci. Rep</source>., <volume>5</volume>, <fpage>14526</fpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>van Beers</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Haggard</surname>, <given-names>P.</given-names></string-name> (<year>2002</year>). <article-title>When feeling is more important than seeing in sensorimotor adaptation</article-title>. <source>Curr. Biol</source>., <volume>12</volume> (<issue>10</issue>), <fpage>834</fpage>–<lpage>837</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Vatakis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Navarra</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Soto-Faraco</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Spence</surname>, <given-names>C.</given-names></string-name> (<year>2007</year>). <article-title>Temporal recalibration during asynchronous audiovisual speech perception</article-title>. <source>Exp. Brain Res</source>., <volume>181</volume> (<issue>1</issue>), <fpage>173</fpage>–<lpage>181</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Vatakis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Navarra</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Soto-Faraco</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Spence</surname>, <given-names>C.</given-names></string-name> (<year>2008</year>). <article-title>Audiovisual temporal adaptation of speech: Temporal order versus simultaneity judgments</article-title>. <source>Exp. Brain Res</source>., <volume>185</volume> (<issue>3</issue>), <fpage>521</fpage>–<lpage>529</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Vroomen</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Keetels</surname>, <given-names>M.</given-names></string-name> (<year>2010</year>). <article-title>Perception of intersensory synchrony: A tutorial review</article-title>. <source>Atten. Percept. Psychophys</source>., <volume>72</volume> (<issue>4</issue>), <fpage>871</fpage>–<lpage>884</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Vroomen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Keetels</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>de Gelder</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Bertelson</surname>, <given-names>P.</given-names></string-name> (<year>2004</year>). <article-title>Recalibration of temporal order perception by exposure to audio-visual asynchrony</article-title>. <source>Brain Res. Cogn. Brain Res</source>., <volume>22</volume> (<issue>1</issue>), <fpage>32</fpage>–<lpage>35</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Wei</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Körding</surname>, <given-names>K.</given-names></string-name> (<year>2009</year>). <article-title>Relevance of error: What drives motor adaptation?</article-title> <source>J. Neurophysiol</source>., <volume>101</volume> (<issue>2</issue>), <fpage>655</fpage>–<lpage>664</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Wozny</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Beierholm</surname>, <given-names>U. R.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name> (<year>2010</year>). <article-title>Probability matching as a computational strategy used in perception</article-title>. <source>PLoS Comput. Biol</source>., <volume>6</volume> (<issue>8</issue>).</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Wozny</surname>, <given-names>D. R.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name> (<year>2011a</year>). <article-title>Computational characterization of visually induced auditory spatial adaptation</article-title>. <source>Front. Integr. Neurosci</source>., <volume>5</volume>, <fpage>75</fpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Wozny</surname>, <given-names>D. R.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name> (<year>2011b</year>). <article-title>Recalibration of auditory space following milliseconds of cross-modal discrepancy</article-title>. <source>J. Neurosci</source>., <volume>31</volume> (<issue>12</issue>), <fpage>4607</fpage>–<lpage>4612</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Yamamoto</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Miyazaki</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Iwano</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Kitazawa</surname>, <given-names>S.</given-names></string-name> (<year>2012</year>). <article-title>Bayesian calibration of simultaneity in audiovisual temporal order judgments</article-title>. <source>PLoS One</source>, <volume>7</volume> (<issue>7</issue>), <fpage>e40379</fpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Yarrow</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jahn</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Durant</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Arnold</surname>, <given-names>D. H.</given-names></string-name> (<year>2011</year>). <article-title>Shifts of criteria or neural timing? the assumptions underlying timing perception studies</article-title>. <source>Conscious. Cogn</source>., <volume>20</volume> (<issue>4</issue>), <fpage>1518</fpage>–<lpage>1531</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Yarrow</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Minaei</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Arnold</surname>, <given-names>D. H.</given-names></string-name> (<year>2015</year>). <article-title>A model-based comparison of three theories of audiovisual temporal recalibration</article-title>. <source>Cogn. Psychol</source>., <volume>83</volume>, <fpage>54</fpage>–<lpage>76</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Yarrow</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Solomon</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Arnold</surname>, <given-names>D. H.</given-names></string-name>, &amp; <string-name><surname>Roseboom</surname>, <given-names>W.</given-names></string-name> (<year>2023</year>). <article-title>The best fitting of three contemporary observer models reveals how participants’ strategy influences the window of subjective synchrony</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>., <volume>49</volume> (<issue>12</issue>), <fpage>1534</fpage>–<lpage>1563</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Zaidel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>A. H.</given-names></string-name>, &amp; <string-name><surname>Angelaki</surname>, <given-names>D. E.</given-names></string-name> (<year>2011</year>). <article-title>Multisensory calibration is independent of cue reliability</article-title>. <source>J. Neurosci</source>., <volume>31</volume> (<issue>39</issue>), <fpage>13949</fpage>–<lpage>13962</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97765.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>In this <bold>valuable</bold> study, Li et al., set out to understand the mechanisms of audiovisual temporal recalibration - the brain's ability to adjust to the latency differences that emerge due to different (distance-dependent) transduction latencies of auditory and visual signals - through psychophysical measurements and modelling. The analysis supports a role for causal inference in recalibration, though the evidence is <bold>incomplete</bold>.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97765.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study asks whether the phenomenon of crossmodal temporal recalibration, i.e. the adjustment of time perception by consistent temporal mismatches across the senses, can be explained by the concept of multisensory causal inference. In particular, they ask whether the explanation offered by causal inference better explains temporal recalibration better than a model assuming that crossmodal stimuli are always integrated, regardless of how discrepant they are.</p>
<p>The study is motivated by previous work in the spatial domain, where it has been shown consistently across studies that the use of crossmodal spatial information is explained by the concept of multisensory causal inference. It is also motivated by the observation that the behavioral data showcasing temporal recalibration feature nonlinearities that, by their nature, cannot be explained by a fixed integration model (sometimes also called mandatory fusion).</p>
<p>To probe this the authors implemented a sophisticated experiment that probed temporal recalibration in several sessions. They then fit the data using the two classes of candidate models and rely on model criteria to provide evidence for their conclusion. The study is sophisticated, conceptually and technically state-of-the-art, and theoretically grounded. The data clearly support the authors' conclusions.</p>
<p>I find the conceptual advance somewhat limited. First, by design, the fixed integration model cannot explain data with a nonlinear dependency on multisensory discrepancy, as already explained in many studies on spatial multisensory perception. Hence, it is not surprising that the causal inference model better fits the data. Second, and again similar to studies on spatial paradigms, the causal inference model fails to predict the behavioral data for large discrepancies. The model predictions in Figure 5 show the (expected) vanishing recalibration for large delta, while the behavioral data don't' decay to zero. Either the range of tested SOAs is too small to show that both the model and data converge to the same vanishing effect at large SOAs, or the model's formula is not the best for explaining the data. Again, the studies using spatial paradigms have the same problem, but in my view, this poses the most interesting question here.</p>
<p>In my view there is nothing generally wrong with the study, it does extend the 'known' to another type of paradigm. However, it covers little new ground on the conceptual side.</p>
<p>On that note, the small sample size of n=10 is likely not an issue, but still, it is on the very low end for this type of study.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97765.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Li et al.'s goal is to understand the mechanisms of audiovisual temporal recalibration. This is an interesting challenge that the brain readily solves in order to compensate for real-world latency differences in the time of arrival of audio/visual signals. To do this they perform a 3-phase recalibration experiment on 9 observers that involves a temporal order judgment (TOJ) pretest and posttest (in which observers are required to judge whether an auditory and visual stimulus were coincident, auditory leading or visual leading) and a conditioning phase in which participants are exposed to a sequence of AV stimuli with a particular temporal disparity. Participants are required to monitor both streams of information for infrequent oddballs, before being tested again in the TOJ, although this time there are 3 conditioning trials for every 1 TOJ trial. Like many previous studies, they demonstrate that conditioning stimuli shift the point of subjective simultaneity (pss) in the direction of the exposure sequence.</p>
<p>These shifts are modest - maxing out at around -50 ms for auditory leading sequences and slightly less than that for visual leading sequences. Similar effects are observed even for the longest offsets where it seems unlikely listeners would perceive the stimuli as synchronous (and therefore under a causal inference model you might intuitively expect no recalibration, and indeed simulations in Figure 5 seem to predict exactly that which isn't what most of their human observers did). Overall I think their data contribute evidence that a causal inference step is likely included within the process of recalibration.</p>
<p>Strengths:</p>
<p>The manuscript performs comprehensive testing over 9 days and 100s of trials and accompanies this with mathematical models to explain the data. The paper is reasonably clearly written and the data appear to support the conclusions.</p>
<p>Weaknesses:</p>
<p>While I believe the data contribute evidence that a causal inference step is likely included within the process of recalibration, this to my mind is not a mechanism but might be seen more as a logical checkpoint to determine whether whatever underlying neuronal mechanism actually instantiates the recalibration should be triggered.</p>
<p>The authors' causal inference model strongly predicts that there should be no recalibration for stimuli at 0.7 ms offset, yet only 3/9 participants appear to show this effect. They note that a significant difference in their design and that of others is the inclusion of longer lags, which are unlikely to originate from the same source, but don't offer any explanation for this key difference between their data and the predictions of a causal inference model.</p>
<p>I'm also not completely convinced that the causal inference model isn't 'best' simply because it has sufficient free parameters to capture the noise in the data. The tested models do not (I think) have equivalent complexity - the causal inference model fits best, but has more parameters with which to fit the data. Moreover, while it fits 'best', is it a good model? Figure S6 is useful in this regard but is not completely clear - are the red dots the actual data or the causal inference prediction? This suggests that it does fit the data very well, but is this based on predicting held-out data, or is it just that by having more parameters it can better capture the noise? Similarly, S7 is a potentially useful figure but it's not clear what is data and what are model predictions (what are the differences between each row for each participant; are they two different models or pre-test post-test or data and model prediction?!).</p>
<p>I'm not an expert on the implementation of such models but my reading of the supplemental methods is that the model is fit using all the data rather than fit and tested on held-out data. This seems problematic.</p>
<p>I would have liked to have seen more individual participant data (which is currently in the supplemental materials, albeit in a not very clear manner as discussed above).</p>
<p>The way that S3 is described in the text (line 141) makes it sound like everyone was in the same direction, however, it is clear that 2 /9 listeners show the opposite pattern, and 2 have confidence intervals close to zero (albeit on the -ve side).</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97765.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Li et al. describe an audiovisual temporal recalibration experiment in which participants perform baseline sessions of ternary order judgments about audiovisual stimulus pairs with various stimulus-onset asynchronies (SOAs). These are followed by adaptation at several adapting SOAs (each on a different day), followed by post-adaptation sessions to assess changes in psychometric functions. The key novelty is the formal specification and application/fit of a causal-inference model for the perception of relative timing, providing simulated predictions for the complete set of psychometric functions both pre and post-adaptation.</p>
<p>Strengths:</p>
<p>(1) Formal models are preferable to vague theoretical statements about a process, and prior to this work, certain accounts of temporal recalibration (specifically those that do not rely on a population code) had only qualitative theoretical statements to explain how/why the magnitude of recalibration changes non-linearly with the stimulus-onset asynchrony of the adaptor.</p>
<p>(2) The experiment is appropriate, the methods are well described, and the average model prediction is a fairly good match to the average data (Figure 4). Conclusions may be overstated slightly, but seem to be essentially supported by the data and modelling.</p>
<p>(3) The work should be impactful. There seems a good chance that this will become the go-to modelling framework for those exploring non-population-code accounts of temporal recalibration (or comparing them with population-code accounts).</p>
<p>(4) A key issue for the generality of the model, specifically in terms of recalibration asymmetries reported by other authors that are inconsistent with those reported here, is properly acknowledged in the discussion.</p>
<p>Weaknesses:</p>
<p>(1) The evidence for the model comes in two forms. First, two trends in the data (non-linearity and asymmetry) are illustrated, and the model is shown to be capable of delivering patterns like these. Second, the model is compared, via AIC, to three other models. However, the main comparison models are clearly not going to fit the data very well, so the fact that the new model fits better does not seem all that compelling. I would suggest that the authors consider a comparison with the atheoretical model they use to first illustrate the data (in Figure 2). This model fits all sessions but with complete freedom to move the bias around (whereas the new model constrains the way bias changes via a principled account). The atheoretical model will obviously fit better, but will have many more free parameters, so a comparison via AIC/BIC or similar should be informative.</p>
<p>(2) It does not appear that some key comparisons have been subjected to appropriate inferential statistical tests. Specifically, lines 196-207 - presumably this is the mean (and SD or SE) change in AIC between models across the group of 9 observers. So are these differences actually significant, for example via t-test?</p>
<p>(3) The manuscript tends to gloss over the population-code account of temporal recalibration, which can already provide a quantitative account of how the magnitude of recalibration varies with adaptor SOA. This could be better acknowledged, and the features a population code may struggle with (asymmetry?) are considered.</p>
<p>(4) The engagement with relevant past literature seems a little thin. Firstly, papers that have applied causal inference modelling to judgments of relative timing are overlooked (see references below). There should be greater clarity regarding how the modelling here builds on or differs from these previous papers (most obviously in terms of additionally modelling the recalibration process, but other details may vary too). Secondly, there is no discussion of previous findings like that in Fujisaki et al.'s seminal work on recalibration, where the spatial overlap of the audio and visual events didn't seem to matter (although admittedly this was an N = 2 control experiment). This kind of finding would seem relevant to a causal inference account.</p>
<p>References:</p>
<p>
Magnotti JF, Ma WJ and Beauchamp MS (2013) Causal inference of asynchronous audiovisual speech. Front. Psychol. 4:798. doi: 10.3389/fpsyg.2013.00798</p>
<p>
Sato, Y. (2021). Comparing Bayesian models for simultaneity judgement with different causal assumptions. J. Math. Psychol., 102, 102521.</p>
<p>(5) As a minor point, the model relies on simulation, which may limit its take-up/application by others in the field.</p>
<p>(6) There is little in the way of reassurance regarding the model's identifiability and recoverability. The authors might for example consider some parameter recovery simulations or similar.</p>
<p>(7) I don't recall any statements about open science and the availability of code and data.</p>
</body>
</sub-article>
</article>