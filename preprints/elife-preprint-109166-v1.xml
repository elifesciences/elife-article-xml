<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">109166</article-id>
<article-id pub-id-type="doi">10.7554/eLife.109166</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.109166.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>A differentiable model for optimizing the genetic drivers of synaptogenesis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Boccato</surname>
<given-names>Tommaso</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<email xlink:href="mailto:tommaso.boccato@uniroma2.it">tommaso.boccato@uniroma2.it</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ferrante</surname>
<given-names>Matteo</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Toschi</surname>
<given-names>Nicola</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02p77k626</institution-id><institution>Department of Biomedicine and Prevention, University of Rome Tor Vergata</institution></institution-wrap>, <city>Rome</city>, <country country="IT">Italy</country></aff>
<aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>A.A. Martinos Center for Biomedical Imaging and Harvard Medical School</institution></institution-wrap>, <city>Boston</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Barab√°si</surname>
<given-names>D√°niel</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Harvard University</institution>
</institution-wrap>
<city>Cambridge</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country country="GR">Greece</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2025-12-29">
<day>29</day>
<month>12</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP109166</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2025-10-21">
<day>21</day>
<month>10</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-09-07">
<day>07</day>
<month>09</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2402.07242"/>
</event>
</pub-history>
<permissions>
<copyright-statement>¬© 2025, Boccato et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Boccato et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-109166-v1.pdf"/>
<abstract>
<p>There is growing consensus among neuroscientists that neural circuits critical for survival are the result of genomic decompression processes. We introduce SynaptoGen, a novel computational framework-member of the Connectome Models family-bringing synthetic biological intelligence closer, facilitating neural biological agent development through precise genetic control of synaptogenesis. SynaptoGen is the first model of its kind offering mechanistic explanation of synaptic multiplicity based on genetic expression and protein interaction probabilities. The framework connects genetic factors through a differentiable function, working as a neural network where synaptic weights equal average numbers of synapses between neurons, multiplied by conductance, derived from genetic profiles. Differentiability enables gradient-based optimization, allowing generation of genetic expression patterns producing pre-wired biological agents for specific tasks. Validation in simulated synaptogenesis scenarios shows agents successfully solving four reinforcement learning benchmarks, consistently surpassing control baselines. Despite gaps in biological realism requiring mitigation, this framework has potential to accelerate synthetic biological intelligence research.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p>Let us consider a scenario in which, during brain development, <bold>hypothesis 1 - h1</bold>. <italic>it becomes possible to manipulate, prior to the formation of synapses, the gene expression profiles of individual neurons</italic>. Furthermore, let us assume that <bold>h2</bold>. <italic>we possess the capability to influence these expression profiles in a manner that directs synaptogenesis towards a specific neuronal network topology</italic>. In this context, <bold>h3</bold>. <italic>we might be able to obtain the optimal computational graph, expressed as a composition of functions that represent the behaviour of neurons, required to solve a task of interest</italic>. Small living organisms, or organoids (<xref ref-type="bibr" rid="c4">Bhaduri et al., 2020</xref>), could be, in principle, genetically programmed to develop into neuronal networks capable of solving pre-specified tasks. Such technology would lead to disruptive applications-e.g., extreme low-power computing, micro-devices for the control of biological systems, or novel biological testing platforms capable of accelerating drug discovery. To date, hypothesis <bold>h1</bold>. seems to be verified, (<xref ref-type="bibr" rid="c26">Nishikawa et al., 2014</xref>) while in the case of <bold>h3</bold>. we can partially rely on artificial neural networks and optimization techniques (<xref ref-type="bibr" rid="c21">Kingma and Ba, 2015</xref>; <xref ref-type="bibr" rid="c14">Graves, 2014</xref>).</p>
<p>In this work, we take a step toward the realization of the joint technology conceptualized in <bold>h2</bold>. and <bold>h3</bold>. by proposing SynaptoGen<fn id="FN1"><label>1</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/BoCtrl-C/synaptogen">https://github.com/BoCtrl-C/synaptogen</ext-link></p></fn> (<xref ref-type="fig" rid="fig1">Figure 1</xref>), a model that links, by means of differentiable functions, vector representations of gene expression profiles (i.e., the pattern of genes actively being transcribed into RNA in a cell) and genetic rules (i.e., interaction probabilities of protein pairs involved in synaptogenesis) to the average number of synaptic connections between pairs of neurons as well as their synaptic conductance values. We substantiate our work through theoretical development, which hinges on novel propositions and related mathematical proofs. SynaptoGen is compatible with backpropagation and can be inserted in learning frameworks where optimization is performed through gradient descent, enabling management of network sizes and task complexities beyond the capabilities of other optimization techniques. Finally, SynaptoGen is designed with flexibility in mind, allowing practitioners to choose which biological quantities to optimize (e.g., genetic rules, expression profiles, or both).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><title>Multi-scale overview of the SynaptoGen model.</title>
<p><bold>A</bold>. In neural networks, synaptogenesis-the formation of synapses-can be approximated as the outcome of interactions (e.g., molecular binding) between proteins translated from gene pairs. SynaptoGen models this process as the realization <inline-formula id="ID1">
<alternatives>
<mml:math display="inline" id="I1"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>Àú</mml:mo></mml:mover><mml:mrow><mml:mi>u</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
<inline-graphic xlink:href="2402.07242v3_ieq1.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula> of a random variable defined as the sum of multiple binomial random variables (e.g., <italic>B</italic><sup><italic>AB</italic></sup> + <italic>B</italic><sup><italic>BC</italic></sup>), one for each potentially interacting gene pair. In the notation used, the <italic>uv</italic> subscript indicates the neuronal pair taken into consideration. <bold>B</bold>. The use of binomial random variables stems from the idea of modeling synapse formation for each gene pair, as a process akin to flipping <italic>n<sub>ij</sub></italic> biased coins, each representing a Bernoulli(<italic>p<sub>ij</sub></italic>) random variable. <bold>C</bold>. When working with matrix representations of the genetic factors in the panel, it is mathematically provable (see Section Methods) that a series of matrix multiplications and point-wise operations yields a matrix where the entry in the <italic>u</italic>-th row and <italic>v</italic>-th column contains the expected number of synapses formed between neurons <italic>u</italic> and <italic>v</italic>, multiplied by their average synaptic conductance. <bold>D</bold>. This resulting matrix, <italic>W</italic> (<inline-formula id="ID2">
<alternatives>
<mml:math display="inline" id="I2"><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover></mml:math>
<inline-graphic xlink:href="2402.07242v3_ieq2.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula> in Section Methods), can be interpreted as a weight matrix and integrated into architectures such as multilayer perceptrons (MLPs). Here, <italic><bold>x</bold></italic> denotes an MLP layer‚Äôs input, while <italic><bold>a</bold></italic> the resulting activations.</p></caption>
<graphic xlink:href="2402.07242v3_fig1.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<sec id="s1-1">
<title>Related Work</title>
<p>Interest in leveraging organoid production for computational purposes has surged since 2022, following the introduction of DishBrain (<xref ref-type="bibr" rid="c17">Kagan et al., 2022</xref>) by Cortical Labs. DishBrain combines in-vitro neural networks, cultured from human or rodent sources, with a simulated environment-specifically the game ‚ÄúPong‚Äù-via a high-density multielectrode array. The system employs the <italic>free energy principle</italic>, which suggests that neural networks adapt by minimizing the unpredictability of their sensory inputs through belief updating and environmental interactions. Although the neuron cultures in DishBrain exhibited statistically significant improvements in gameplay performance (e.g., <italic>average rally length</italic>, <italic>% of aces</italic> and <italic>% of long-rallies</italic>) compared to the control conditions defined by the authors (i.e., random agents), it is difficult to conclude that the neuronal networks fully mastered the task.</p>
<p>In contrast to DishBrain, our research aims to ‚Äútrain‚Äù networks of neurons by influencing synap-togenesis through genetic manipulations at the individual neuron level, a largely unexplored task in the literature, with only a limited number of closely related studies available. The basis for our work is outlined in (<xref ref-type="bibr" rid="c3">Barab√°si and Cz√©gel, 2021</xref>; <xref ref-type="bibr" rid="c1">Barab√°si and Barab√°si, 2020</xref>), which introduces methods for constructing networks based on genetic encodings inspired by the wiring rules of the brain. These methods were further elaborated in the Connectome Model (CM) (<xref ref-type="bibr" rid="c23">Kov√°cs et al., 2020</xref>), where the authors decomposed the adjacency matrix of a connectome into the product of three matrices representing specific genetic quantities. Another development was presented in (<xref ref-type="bibr" rid="c2">Barab√°si et al., 2023</xref>), where the CM‚Äôs matrix entries were treated as learnable parameters, resulting in the weight matrix of a Multilayer Perceptron (MLP) within the context of training neural networks. While this methodology has proven effective in producing parameter-efficient neural networks, it maintains a notable distance from the biological intricacies of real neuronal networks. A distinct generalization of the CM has also been proposed for the computational inference of synaptic polarities (<xref ref-type="bibr" rid="c15">Harris et al., 2022</xref>), a quantity not considered in the original CM.</p>
<p>Similarly, our work draws inspiration from the CM but is geared towards a more bio-plausible computational modeling of synaptogenesis, with the novel elements extensively discussed in Section Methods.</p>
</sec>
</sec>
<sec id="s2" sec-type="methods">
<title>Methods</title>
<p>In 2020, Kov√°cs et al. proposed the CM, a novel strategy to link a brain‚Äôs connectome (<italic>B</italic>) to the expression patterns of individual neurons (<italic>X</italic>)and existing biological mechanisms- or genetic rules- <italic>O</italic>:
<disp-formula id="FD1">
<alternatives>
<mml:math id="M1" display="block"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mi>O</mml:mi><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math>
<graphic xlink:href="2402.07242v3_eqn1.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(1)</label>
</disp-formula>
</p>
<p>In the CM‚Äôs first interpretation, each row of <italic>X</italic> referred to a specific neuron while the <italic>i</italic>-th entry of the row described the binary expression (1-‚Äùexpressed‚Äù-or 0-‚Äùnot expressed‚Äù) of gene <italic>i</italic>, one of the genes involved in synapse formation. Matrix <italic>O</italic>, instead, represented interaction compatibility, in the synapse formation process, for proteins translated from all gene pairs. Hence, <italic>X</italic> ‚àà {0, 1}<sup><italic>N √ó G</italic></sup> and <italic>O</italic> ‚àà {0, 1}<sup><italic>G√ó3</italic></sup> were defined as binary matrices while the entries of <italic>B</italic>, of shape <italic>N</italic> √ó <italic>N</italic>, belonged to <bold>‚Ñ§</bold><sup>+</sup>; with <italic>N</italic> and <italic>G</italic> denoting the number of neurons and genes, respectively. When <italic>G ‚â™ N</italic> however, a very common scenario in nature (<xref ref-type="bibr" rid="c22">Koulakov et al., 2022</xref>), not all possible connectomes can be decomposed through <xref ref-type="disp-formula" rid="FD1">(1)</xref>. For this reason, the authors of the CM went on to relaxing the genetic rules matrix to <italic>O</italic> ‚àà [0, 1]<sup><italic>G√óG</italic></sup>, interpreting its entries as probabilities, which generates the following approximation:
<disp-formula id="FD2">
<alternatives>
<mml:math id="M2" display="block"><mml:mi>B</mml:mi><mml:mo>‚âÉ</mml:mo><mml:mi>X</mml:mi><mml:mi>O</mml:mi><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:math>
<graphic xlink:href="2402.07242v3_eqn2.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(2)</label>
</disp-formula>
<disp-formula id="FD3">
<alternatives>
<mml:math id="M3" display="block"><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>¬†</mml:mo><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:msup><mml:mi>O</mml:mi><mml:mo>‚Ä≤</mml:mo></mml:msup></mml:munder><mml:msup><mml:mrow><mml:mo>‚Äñ</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>‚àí</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mi>O</mml:mi><mml:mo>‚Ä≤</mml:mo></mml:msup><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>‚Äñ</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math>
<graphic xlink:href="2402.07242v3_eqn3.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(3)</label>
</disp-formula>
</p>
<p>where || ‚Ä¢ || is intended as the Frobenius norm.</p>
<p>In this paper, we formulate a more general alternative to this framework by building a model that takes into account synaptic multiplicity and conductance. Starting from <xref ref-type="disp-formula" rid="FD1">(1)</xref>, we design two novel interpretations tightly linked to the formalism with which the quantities of interest (i.e., the number of synapses between neurons and their conductances) have been represented in our model. Our theoretical framework is as follows.</p>
<p>Let the number of synaptic connections between two neurons be represented by the following:
<disp-formula id="FD4">
<alternatives>
<mml:math id="M4" display="block"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math>
<graphic xlink:href="2402.07242v3_eqn4.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(4)</label>
</disp-formula>
</p>
<p>where <italic>B</italic><sup><italic>ij</italic></sup> is a binomial random variable that expresses the contribution of the (<italic>i,j</italic>) gene pair to the total synaptic count:
<disp-formula id="FD5">
<alternatives>
<mml:math id="M5" display="block"><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2402.07242v3_eqn5.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(5)</label>
</disp-formula>
</p>
<p>And let <italic><bold>x</bold></italic> ‚àà ‚Ñù<sup>+<italic>G</italic></sup> and <italic><bold>y</bold></italic> ‚àà ‚Ñù<sup>+<italic>G</italic></sup> be vector representations of gene expression in the pre- and post- synaptic neurons, respectively.</p>
<sec id="s2-1">
<sec id="s2-1-1">
<title>Proposition 1</title>
<p><italic>If the product between the i-th entry of <bold>x</bold> and the j-th entry of <bold>y</bold> denotes the number of independent experiments that characterizes B<sup>ij</sup>-i.e., x<sub>i</sub>y<sub>j</sub> = n<sub>ij</sub>-and entry O<sub>ij</sub> corresponds to probability p<sub>ij</sub>, then the expected number of synapses between two neurons can be calculated as:</italic>
<disp-formula id="FD6">
<alternatives>
<mml:math id="M6" display="block"><mml:mo>ùîº</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>O</mml:mi><mml:mi>y</mml:mi></mml:math>
<graphic xlink:href="2402.07242v3_eqn6.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(6)</label>
</disp-formula>
</p>
<p><italic>Proof.</italic> From probability theory,
<disp-formula id="FD7">
<alternatives>
<mml:math id="M7" display="block"><mml:mo>ùîº</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>
<graphic xlink:href="2402.07242v3_eqn7.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
</p>
<p>and due to the linearity of expectation, we have
<disp-formula id="FD8">
<alternatives>
<mml:math id="M8" display="block"><mml:mo>ùîº</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math>
<graphic xlink:href="2402.07242v3_eqn8.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
</p>
<p>On the other hand,
<disp-formula id="FD9">
<alternatives>
<mml:math id="M9" display="block"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>O</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>‚Ä¶</mml:mn><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>‚Ä¶</mml:mn></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>‚ÄÉ‚ÄÉ‚Äâ‚Äâ</mml:mtext><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>‚ÄÉ‚ÄÉ‚Äâ‚Äâ</mml:mtext><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math>
<graphic xlink:href="2402.07242v3_eqn9.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
</p>
<p>Recalling that <italic>x<sub>i</sub>y<sub>j</sub> = n<sub>ij</sub></italic> and <italic>O<sub>ij</sub> = p<sub>ij</sub></italic>, the proof is concluded.</p>
<p>In different terms, if the hypotheses of Proposition 1 are verified, gene expression in a pair of genes tells us how many attempts we can make to place a synapse between a pre- and a post-synaptic neuron; the genetic rule, instead, describes the probability of success, conditioned on the interaction between the proteins translated from the corresponding genes, for each attempt. It is worth noting that <xref ref-type="disp-formula" rid="FD6">(6)</xref> represents the average number of links between two specific nodes of the connectome. Keeping in mind that genetic rules are shared across neurons, the equation can easily be generalized to the whole connectome:
<disp-formula id="FD10">
<alternatives>
<mml:math id="M10" display="block"><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>ùîº</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mi>O</mml:mi><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math>
<graphic xlink:href="2402.07242v3_eqn10.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(7)</label>
</disp-formula>
</p>
<p>where <italic>X</italic> is obtained by stacking the expression profiles of all neurons (e.g., <italic>X<sup>T</sup></italic> = [‚Ä¶, <italic><bold>x, ‚Ä¶ ,y,</bold></italic>‚Ä¶]).</p>
<p>In order to model synaptic conductances, a slightly more complex formalism is required. We restrict ourselves to chemical synapses, which are the result of the interplay between neurotransmitters released by pre-synaptic neurons and receptors in post-synaptic neurons. We also account for the fact that a chemical synapse can also have an excitatory or inhibitory effect depending on the nature of the receptor that receives a specific neurotransmitter (<xref ref-type="bibr" rid="c13">Fenyves et al., 2020</xref>; <xref ref-type="bibr" rid="c15">Harris et al., 2022</xref>). The way in which synapses work in our framework is described by the following equation:
<disp-formula id="FD11">
<alternatives>
<mml:math id="M11" display="block"><mml:msub><mml:mi>I</mml:mi><mml:mi>œÖ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mi>u</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>œÖ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:msub><mml:mi>V</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math>
<graphic xlink:href="2402.07242v3_eqn11.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(8)</label>
</disp-formula>
</p>
<p>where <italic>I<sub>v</sub></italic> is the current injected into post-synaptic neuron <italic>v</italic> while <italic>V<sub>u</sub></italic> is an input voltage from pre-synaptic neuron <italic>u</italic>; <italic>G<sub>uv</sub></italic>, is the equivalent conductance that takes care of all the synapses formed between <italic>u</italic> and <italic>v</italic>. To model the possibility of characterizing synapses by different neurotransmitterreceptor pairs, we rely again on random variables as follows:</p>
<p>Let <italic>œÑ</italic> be a multinomial random variable representing the process of randomly picking, from <italic>u</italic>, a synaptic vesicle filled with a specific neurotransmitter. And let <italic>R</italic> be a multinomial random variable representing the process of randomly selecting a specific receptor from the membrane of <italic>v</italic>. We define vectors <italic><bold>q</bold></italic> ‚àà [0, 1]<sup><italic>L</italic></sup> and <italic><bold>r</bold></italic> ‚àà [0, 1]<sup><italic>M</italic></sup> as the probability distributions associated with <italic>œÑ</italic> and <italic>R</italic>; where <italic>L</italic> denotes the total number of neurotransmitters while <italic>M</italic> the number of receptors. We also define <italic>A</italic> ‚àà {‚àí1, 0, 1}<sup><italic>L√óM</italic></sup> as the polarity matrix (for further details, refer to Appendix The Polarity Matrix) and <italic>K</italic> ‚àà ‚Ñù<sup>+<italic>L√óM</italic></sup> as the conductance matrix. In detail, entry <italic>A<sub>ij</sub></italic> defines the polarity of synapses derived from the interaction of the <italic>i</italic>-th neurotransmitter with the <italic>j</italic>-th receptor (<italic>A<sub>ij</sub></italic> = 0 if the considered neurotransmitter and receptor are not compatible) while <italic>K<sub>ij</sub></italic> stores its linked conductance. We finally set <italic>ùí¢</italic> = <italic>f (œÑ, R)</italic>, with <italic>f</italic> (<italic>i, j</italic>) = <italic>A<sub>ij</sub>K<sub>ij</sub></italic>. In other words, <italic>ùí¢</italic> represents the ‚Äúsigned‚Äù conductance of a synapse randomly selected from the ones that connect neurons <italic>u</italic> and <italic>v</italic>.</p>
</sec>
<sec id="s2-1-2">
<title>Proposition 2</title>
<p><italic>If œÑ and R are independent (i.e., the distribution of receptors in the post-synaptic neuron does not depend on the neurotransmitters synthesized by the pre-synaptic neuron), the expected ‚Äúsigned‚Äù conductance of a randomly picked synapse can be calculated as:</italic>
<disp-formula id="FD12">
<alternatives>
<mml:math id="M12" display="block"><mml:mo>ùîº</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>G</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>‚äô</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:math>
<graphic xlink:href="2402.07242v3_eqn12.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(9)</label>
</disp-formula>
</p>
<p><italic>Proof.</italic> By expanding the matrix multiplications in <xref ref-type="disp-formula" rid="FD12">(9)</xref>, we have:
<disp-formula id="FD13">
<alternatives>
<mml:math id="M13" display="block"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>q</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>‚äô</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>‚Ä¶</mml:mn><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>‚Ä¶</mml:mn></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ</mml:mtext><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ</mml:mtext><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math>
<graphic xlink:href="2402.07242v3_eqn13.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
</p>
<p>Thanks to the independence of <italic>œÑ</italic> and <italic>R</italic>:
<disp-formula id="FD14">
<alternatives>
<mml:math id="M14" display="block"><mml:mo>‚Ñô</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>œÑ</mml:mo><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math>
<graphic xlink:href="2402.07242v3_eqn14.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
</p>
<p>where ‚Ñô[‚Ä¢] stands for ‚Äúprobability of‚Äù. Hence,
<disp-formula id="FD15">
<alternatives>
<mml:math id="M15" display="block"><mml:msup><mml:mi>q</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>‚äô</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mo>‚Ñô</mml:mo></mml:mstyle><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>œÑ</mml:mo><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2402.07242v3_eqn15.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
</p>
<p>that corresponds exactly to the definition of ùîº[ùí¢].</p>
<p>As for <xref ref-type="disp-formula" rid="FD10">(7)</xref>, also <xref ref-type="disp-formula" rid="FD12">(9)</xref> can be generalized by stacking the neurotransmitter distributions of all pre-synaptic neurons in <italic>Q</italic> = [‚Ä¶, <italic><bold>q</bold></italic>,‚Ä¶]<sup><italic>T</italic></sup> and the receptor distributions of post-synaptic neurons in <italic>R</italic> = [‚Ä¶,<italic><bold>r</bold></italic>,‚Ä¶]<sup><italic>T</italic></sup>:
<disp-formula id="FD16">
<alternatives>
<mml:math id="M16" display="block"><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>ùîº</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>G</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>‚Äã</mml:mtext><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>‚äô</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math>
<graphic xlink:href="2402.07242v3_eqn16.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(10)</label>
</disp-formula>
</p>
<p>As a next step, <xref ref-type="disp-formula" rid="FD10">(7)</xref> and <xref ref-type="disp-formula" rid="FD16">(10)</xref> can be inserted into the core equation of our model, which follows:
<disp-formula id="FD17">
<alternatives>
<mml:math id="M17" display="block"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mo>‚äô</mml:mo><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>‚ÄÉ</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mi>O</mml:mi><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>‚äô</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>‚äô</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
<graphic xlink:href="2402.07242v3_eqn17.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(11)</label>
</disp-formula>
</p>
<p>Summarizing, through <xref ref-type="disp-formula" rid="FD17">(11)</xref> we are able to express the average equivalent conductance between all pairs of neurons as a differentiable function of their gene expression profiles and distributions of synthesized neurotransmitters and receptors, which, in turn, depend on gene expression. Furthermore, in our formalism, synaptogenesis can be simulated by sampling from the random variables we have defined. For instance, the simplest approximation of synaptogenesis can be obtained as follows (further details in Appendices Simulating Synaptogenesis, A Problem of Quantization):
<disp-formula id="FD18">
<alternatives>
<mml:math id="M18" display="block"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>Àú</mml:mo></mml:mover><mml:mo>‚äô</mml:mo><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover></mml:math>
<graphic xlink:href="2402.07242v3_eqn18.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(12)</label>
</disp-formula>
</p>
<p>with
<disp-formula id="FD19">
<alternatives>
<mml:math id="M19" display="block"><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>Àú</mml:mo></mml:mover><mml:mo>‚àº</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>‚Ä¶</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>‚Ä¶</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>‚Ä¶</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>‚Ä¶</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>œÖ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>‚Ä¶</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>‚Ä¶</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>‚Ä¶</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>‚Ä¶</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2402.07242v3_eqn19.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(13)</label>
</disp-formula>
</p>
<p>where ~ stands for ‚Äúsampled from‚Äù.</p>
</sec>
</sec>
</sec>
<sec id="s3" sec-type="results">
<title>Results</title>
<p>To validate the proposed framework and assess its applicability in real-world scenarios, we conducted a series of experiments that involved simulating synaptogenesis in small populations of neurons. In simple terms, we simulated the formation of synapses in neuron populations where genes and gene expression were manipulated at the level of individual neurons. These manipulations followed the genetic rules and expression profiles optimized by our model with the aim of enabling the resulting fully-developed neuronal networks to perform effectively in a pre-specified task.</p>
<p>Our approach begins with a simplified setup, where a neuronal population comprises spatially-separated layers. We assumed that neurons in one layer could only attempt to form synapses with neurons in adjacent layers. This restriction allowed us to focus on multipartite network topologies and implement SynaptoGen as a customized MLP, where its weight matrices are decomposed according to <xref ref-type="disp-formula" rid="FD17">(11)</xref>, with genetic rules shared across layers.</p>
<p>Regarding the task for our proof-of-concept experiments, we chose reinforcement learning (RL) as a bio-plausible benchmark. Biological organisms equipped with neuronal networks, indeed, exist immersed in an external environment from which they receive stimuli and upon which they can perform actions. These same characteristics are also inherent to the tasks addressed by RL. Our goal was to create virtual neural agents capable of solving the control tasks defined by the CartPole-v1, MountainCar-v0, LunarLander-v2 and Acrobot-v1 environments from the OpenAI Gym library (<xref ref-type="bibr" rid="c8">Brockman et al., 2016</xref>), among the most famous benchmarks in the RL community. In Cart Pole, a pole is attached to a cart that moves along a frictionless track. The objective is to balance the pole by applying forces to move the cart left or right. In Mountain Car, instead, the goal is to strategically accelerate a car to climb a sinusoidal valley-and-hill terrain. In Lunar Lander, agents must operate the main and orientation engines ofa lander to ensure it lands smoothly on a designated landing pad without damage. Finally, in the Acrobot game, the aim is to apply torques to the actuated joint of a two-link chain to make the free end of the chain surpass a given height.</p>
<p>Initially, we separately trained SynaptoGen-16/32/64 genes, 3 neurotransmitters, neuronal population sizes of ~128 (see Appendix Training Details for the training details)-on the four selected environments using the DQN algorithm (<xref ref-type="bibr" rid="c24">Mnih et al., 2015</xref>). The training provided the learned genetic rules, gene expression profiles, neurotransmitter and receptor distributions, and synaptic conductances that enabled the agents to perform the tasks. Notably, SynaptoGen is built on the average equivalent conductances introduced in <xref ref-type="disp-formula" rid="FD16">(10)</xref>, and can be interpreted as an average agent that reflects the effects of the underlying genetically-derived quantities.</p>
<p>Successively, for each environment, we sampled 100 neural networks from the trained Synap- toGen models, simulating the development, based on the computed synaptogenesis rules, of multiple populations of neurons into neuronal networks. We then measured their performance and compared the obtained metrics with those from two carefully designed baselines. The first baseline (Appendix The Separable Natural Evolution Strategy (SNES) Baseline) adopts the optimization technique employed in (<xref ref-type="bibr" rid="c28">St√∂ckl et al., 2022</xref>), which has been applied to optimize related models, the <italic>probabilistic skeletons</italic>, for similar tasks, i.e., the optimization of connection probabilities between neuronal types. This technique, known as SNES (<xref ref-type="bibr" rid="c27">Schaul, 2012</xref>), is an evolutionary algorithm specifically designed for medium-to-large problem dimensions. Given the structure of Synapto- Gen, which is explicitly designed for gradient-based optimization but remains general and flexible enough to accommodate other optimizers, SNES could serve as both a benchmark we aim to surpass and a potential alternative worth exploring. The second baseline (Appendix The Bio-Plausible Baseline) leveraged the SynaptoGen‚Äôs biology, using the optimized genetic rules <italic>O</italic> and conductance matrix <italic>K</italic>, while initializing gene expression profiles in a bio-plausible manner following the procedure identified by Kerstjens et al. (<xref ref-type="bibr" rid="c19">Kerstjens et al., 2022</xref>, <xref ref-type="bibr" rid="c18">2024</xref>). As before, 100 neuronal networks were sampled from each baseline.</p>
<p>The results obtained from experiments with agents characterized by a genetic profile of 16 genes are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. <xref ref-type="table" rid="tbl1">Table 1</xref> provides summary metrics of the outcomes of each experiment. The first metric is the average reward calculated across groups of 100 sampled agents. The second metric is also the average reward, but calculated for the top-10 best-performing agents in each group. This metric aims to assess the models‚Äô effectiveness in scenarios where the entity responsible for synthesizing biological agents prioritizes achieving a small subset of highly performing agents, even at the cost of the overall process yield-the number of agents capable of solving their task divided by the total number of agents. Finally, the third metric describes the percentage of simulated agents capable of solving the tasks defined by the four selected environments (see Appendix Reward Thresholds for Considering a Task Solved for further discussion). This metric represents the global yield of each methodology under consideration.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><title>Mean reward distributions from the model families, characterized by a 16-gene profile, tested on the four selected RL environments.</title>
<p>The green crosses represent the mean rewards, averaged over 10 episodes, achieved by the trained SynaptoGen models. Each <italic>scatterplot</italic> point represents the mean reward obtained by a specific agent. The black crosses denote instead the distribution means. Model families are color-coded. The dashed horizontal lines indicate the reward threshold beyond which the task associated with an environment is considered solved.</p></caption>
<graphic xlink:href="2402.07242v3_fig2.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl1" position="float" orientation="portrait">
<label>Table 1.</label>
<caption><title>Summary metrics computed over the reward distributions obtained from the model families characterized by a 16-gene profile.</title>
<p>Three metrics are reported: the distribution mean, a mean computed over the top-10 agents, and the percentage of simulated agents that solved the task. Each group of rows refers to one of the selected RL environments and the best scores are highlighted in bold.</p></caption>
<alternatives>
<graphic xlink:href="2402.07242v3_tbl1.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="bottom">environment</th>
<th align="left" valign="bottom">model</th>
<th align="left" valign="top">mean</th>
<th align="left" valign="top">std</th>
<th align="left" valign="top">top-10 mean</th>
<th align="left" valign="top">top-10 std</th>
<th align="left" valign="top">% solved</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="middle" rowspan="3">CartPole-v1</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">456.05</td>
<td align="right" valign="top">83.38</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">13.56</td>
<td align="right" valign="top">21.12</td>
<td align="right" valign="top">51.12</td>
<td align="right" valign="top">56.24</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">MountainCar-v0</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">‚àí<bold>135.26</bold></td>
<td align="right" valign="top">11.17</td>
<td align="right" valign="top">‚àí<bold>115.75</bold></td>
<td align="right" valign="top">3.31</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.05</td>
<td align="right" valign="top">-199.95</td>
<td align="right" valign="top">0.16</td>
<td align="right" valign="top">1.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">LunarLander-v2</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top"><bold>197.92</bold></td>
<td align="right" valign="top">32.37</td>
<td align="right" valign="top"><bold>250.12</bold></td>
<td align="right" valign="top">7.60</td>
<td align="right" valign="top"><bold>47.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-88.92</td>
<td align="right" valign="top">31.67</td>
<td align="right" valign="top">-28.71</td>
<td align="right" valign="top">17.71</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-530.68</td>
<td align="right" valign="top">255.51</td>
<td align="right" valign="top">-115.53</td>
<td align="right" valign="top">36.15</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">Acrobot-v1</td>
<td align="right" valign="top">SynaptoGen</td>
<td align="right" valign="top">‚àí<bold>309.37</bold></td>
<td align="right" valign="top">179.32</td>
<td align="right" valign="top">‚àí<bold>80.38</bold></td>
<td align="right" valign="top">2.39</td>
<td align="right" valign="top">63.00%</td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-388.44</td>
<td align="right" valign="top">21.69</td>
<td align="right" valign="top">-350.77</td>
<td align="right" valign="top">9.95</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-495.89</td>
<td align="right" valign="top">41.07</td>
<td align="right" valign="top">-458.93</td>
<td align="right" valign="top">129.87</td>
<td align="right" valign="top">1.00%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In terms of average reward SynaptoGen (trained via gradient descent) emerged as the bestperforming model, outperforming agents optimized with SNES in three out of four environments. It also consistently surpassed the baseline characterized by biologically plausible initialization of gene expression. The SNES agents also performed well, exceeding the bio-plausible baseline-the one characterized by the bio-plausible expression initialization-in three out of four environments. When examining the average rewards of the top-10 agents, the superiority of SynaptoGen and its derived agents becomes even more evident. They achieved the highest performance across all environments studied, sharing the leading position only in Cart Pole. By contrast, the SNES top-10 agents achieved average rewards indicative of task resolution in only 50% of the cases. For the bio-plausible baseline, nearly all top-10 agents failed to achieve sufficient performance. Finally, regarding the percentage of agents capable of solving the tasks their genetic profiles were optimized/initialized for, SynaptoGen was the only approach to achieve non-zero percentages in all four environments. Moreover, it achieved the highest percentage in three out of four cases. SNES agents also performed reasonably well, reaching a 100% resolution rate in half the environments, though failing entirely in the other half. For the bio-plausible baseline, only 2 out of 400 sampled agents successfully solved their respective tasks. Notably, in 10 out of 12 comparisons, the differences between reward distributions were statistically significant based on the Mann-Whitney test with Bonferroni correction (for the multiple environment-specific comparisons performed). For brevity and clarity, results for agents derived from genetic profiles with 32 or 64 genes are presented in Appendix Additional Results. These results exhibit minimal differences from those reported here and lead to equivalent conclusions.</p>
<p>To provide additional evidence of SynaptoGen‚Äôs effectiveness, we designed an additional set of simulations aimed at validating the framework in a context closer to the biological reality of neuronal networks. In these experiments, we used genetic rules derived from experimental data collected on the nerve ring of the nematode <italic>C. elegans</italic>. This region was selected due to the availability of both transcriptomic data (<xref ref-type="bibr" rid="c29">Taylor et al., 2021</xref>) and connectomic data-including membrane contacts and synapses (<xref ref-type="bibr" rid="c7">Brittin et al., 2021</xref>; <xref ref-type="bibr" rid="c11">Cook et al., 2019</xref>; <xref ref-type="bibr" rid="c30">Witvliet et al., 2020</xref>)-collected from it. The genetic rules were constructed based on a restricted set of 141 cell adhesion molecules (CAMs) with a documented role in synapse formation (<xref ref-type="bibr" rid="c10">Col√≥n-Ramos et al., 2007</xref>; <xref ref-type="bibr" rid="c20">Kim and Emmons, 2017</xref>), following a procedure adapted from the Network Differential Gene Expression Analysis (nDGE) framework proposed in (<xref ref-type="bibr" rid="c29">Taylor et al., 2021</xref>). The procedure is described in Appendix nDGE-based Bio-Plausible Rules and summarized in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Briefly, we first identified pairs of genes responsible for CAM production that were co-expressed in nerve ring neurons with synaptic connections but not co-expressed in cells sufficiently close to form synapses yet lacking connections. We then constrained the (learnable) genetic linked to these pairs to assume high probabilities (&gt;0.5), while assigning low probabilities (&lt;0.5) to all other rules.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p><bold>A</bold>. Co-expression data computed following our extended nDGE variant from the expression patterns released with (<xref ref-type="bibr" rid="c29">Taylor et al., 2021</xref>). In the matrix, rows correspond to genes expressed in pre-synaptic neurons, while columns represent genes utilized in post-synaptic neurons. The green circles indicate pairs of genes that are co-expressed in neurons that are connected but not co-expressed in neurons that could be connected but lack synapses. Genes involved in co-expressed pairs are highlighted in bold. <bold>B</bold>. Visualization of the two sigmoids used to map the learnable parameters associated with the genetic rules into the probabilities in <italic>O</italic>. The green sigmoid is applied to the parameters corresponding to co-expressed pairs, while the pink sigmoid is applied to the remaining ones. We also show in blue the parameter distribution after initialization and in green (co-expressed pairs) and pink (non-co-expressed pairs) the distributions of the probabilities obtainable in an example scenario where 50% of the gene pairs are co-expressed. <bold>C</bold>. Genetic rules learned by SynaptoGen in our bio-plausible validations. The emerging patterns are fully consistent with the co-expression data in A. (correlation coefficients: 0.92, 0.74, 0.44, 0.84; same order as in C.) and demonstrate how the model assigned high probabilities (~1) specifically to the genetic rules corresponding to the pairs of co-expressed genes in the <italic>C. elegans</italic> nerve ring.</p></caption>
<graphic xlink:href="2402.07242v3_fig3.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>The results of these experiments are presented in <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="table" rid="tbl2">Table 2</xref>. In this context, the SNES agents improved their competitiveness in terms of average reward, surpassing SynaptoGen in Acrobot. Nevertheless, SynaptoGen maintained the highest average reward in Mountain Car and Lunar Lander. As in the previous set of experiments, the bio-plausible baseline consistently ranked last across all environments. The average reward calculated for the top-10 agents followed a similar trend, with SynaptoGen managing to achieve a tie in Cart Pole. Finally, in terms of resolution percentages, agents derived from SynaptoGen once again were the only ones to achieve non-zero percentages across all tested environments. Results for SNES agents remained polarized, with all simulated agents resolving the tasks in two environments but none performing adequately in the remaining two. The bio-plausible baseline again failed to produce agents capable of solving any task.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Mean reward distributions from the model families, characterized by a 39-gene profile which includes the genetic rules derived from <italic>C. elegans</italic>, tested on the four selected RL environments.</title>
<p>The green crosses represent the mean rewards, averaged over 10 episodes, achieved by the trained SynaptoGen models. Each <italic>scatterplot</italic> point represents the mean reward obtained by a specific agent. The black crosses denote instead the distribution means. Model families are color-coded. The dashed horizontal lines indicate the reward threshold beyond which the task associated with an environment is considered solved.</p></caption>
<graphic xlink:href="2402.07242v3_fig4.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl2" position="float" orientation="portrait">
<label>Table 2.</label>
<caption><title>Summary metrics computed over the reward distributions obtained from the model families characterized by a 39-gene profile which includes the genetic rules derived from <italic>C. elegans</italic>.</title>
<p>Three metrics are reported: the distribution mean, a mean computed over the top-10 agents, and the percentage of simulated agents that solved the task. Each group of rows refers to one of the selected RL environments and the best scores are highlighted in bold.</p></caption>
<alternatives>
<graphic xlink:href="2402.07242v3_tbl2.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="bottom">environment</th>
<th align="left" valign="bottom">model</th>
<th align="right" valign="top">mean</th>
<th align="right" valign="top">std</th>
<th align="right" valign="top">top-10 mean</th>
<th align="right" valign="top">top-10 std</th>
<th align="right" valign="top">% solved</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="middle" rowspan="3">CartPole-v1</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">494.39</td>
<td align="right" valign="top">18.27</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top"><bold>497.25</bold></td>
<td align="right" valign="top">8.09</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">11.26</td>
<td align="right" valign="top">15.16</td>
<td align="right" valign="top">28.44</td>
<td align="right" valign="top">46.52</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">MountainCar-v0</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">‚àí<bold>136.53</bold></td>
<td align="right" valign="top">18.06</td>
<td align="right" valign="top">‚àí<bold>115.21</bold></td>
<td align="right" valign="top">4.08</td>
<td align="right" valign="top"><bold>97.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">LunarLander-v2</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">‚àí<bold>45.59</bold></td>
<td align="right" valign="top">146.52</td>
<td align="right" valign="top"><bold>184.01</bold></td>
<td align="right" valign="top">47.31</td>
<td align="right" valign="top"><bold>4.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-120.70</td>
<td align="right" valign="top">16.98</td>
<td align="right" valign="top">-91.96</td>
<td align="right" valign="top">7.96</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-499.67</td>
<td align="right" valign="top">253.46</td>
<td align="right" valign="top">-116.52</td>
<td align="right" valign="top">12.17</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">Acrobot-v1</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">-441.75</td>
<td align="right" valign="top">126.19</td>
<td align="right" valign="top">-128.74</td>
<td align="right" valign="top">33.49</td>
<td align="right" valign="top">21.00%</td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">‚àí<bold>121.59</bold></td>
<td align="right" valign="top">16.50</td>
<td align="right" valign="top">‚àí<bold>102.17</bold></td>
<td align="right" valign="top">1.84</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-498.37</td>
<td align="right" valign="top">16.34</td>
<td align="right" valign="top">-483.66</td>
<td align="right" valign="top">51.67</td>
<td align="right" valign="top">1.00%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="s4" sec-type="discussion">
<title>Discussion</title>
<p>In this section, we will discuss the contributions of SynaptoGen to neuroscience, with a particular focus on the ‚Äúgenomic bottleneck‚Äù field (<xref ref-type="bibr" rid="c31">Zador, 2019</xref>), and its potential impact on technologies aimed at creating synthetic biological intelligence. We will also address the limitations of SynaptoGen in its current form and propose strategies to mitigate these challenges.</p>
<p>Firstly, SynaptoGen represents the first model capable of rigorously explaining synaptic multiplicity based on genes, their interaction probabilities, and gene expression, achieving (by design) unprecedented granularity in modeling synaptogenesis. Secondly, SynaptoGen establishes a direct relationship between the average number of synapses between neurons, their average synaptic weights, and vector representations of genetic expression and protein interaction probabilities through a differentiable function. This is a key feature, as it ensures compatibility with gradientbased optimization techniques, bringing several advantages discussed further below. Notably, since SynaptoGen can be deployed as a standard neural network, it broadens the accessibility of the simulation tools used in computational neuroscience to deep learning practitioners, whose contributions could significantly advance the field. Additionally, SynaptoGen can be integrated with any differentiable AI model.</p>
<p>Arguably, one of the most significant features of SynaptoGen is its performance. Simulations confirm that SynaptoGen generates genetic profiles capable of guiding the development of biological agents that perform effectively, in terms of mean reward and yield, in the environments for which they have been pre-wired. Agents derived from SynaptoGen consistently outperform the bio-plausible baseline across all experiments. This is not surprising, as populations of neurons left to interact without regulation are unlikely to form the connections necessary for task-specific behavior. More unexpectedly, SynaptoGen also outperforms agents optimized through SNES in most scenarios. Unlike SNES, SynaptoGen does not rely on evaluating the simulated agents‚Äô performance during optimization, highlighting the strategic advantage of gradient-based methods in our framework. We hypothesize that gradient descent scales better with problem size and task complexity. Supporting this hypothesis are results from the Mountain Car and Lunar Lander environments. In these environments, generally regarded by the RL community as more complex than inverted pendulum tasks, the performance gap between the SynaptoGen agents and the SNES-derived ones was greater. Nonetheless, SynaptoGen remains flexible and general enough to support alternative optimization techniques if advantageous. Notably, SynaptoGen demonstrates strong effectiveness in ‚Äúbiologically realistic‚Äù simulations (i.e., simulations in which rules derived from co-expression data are utilized), maintaining its superiority despite the additional constraints introduced by biological genetic rules.</p>
<p>Despite these advances, there remains a considerable gap between our model and the biological reality of neuronal networks before it can be applied to real-world contexts. One major difference lies in the event-based nature of processing in biological neural networks. SynaptoGen can be adapted into a spiking neural network without sacrificing differentiability by using libraries like <italic>snnTorch</italic> (<xref ref-type="bibr" rid="c12">Eshraghian et al., 2023</xref>), which leverages surrogate gradients. This does not preclude exploring more complex neuron models, although such complexity may be unnecessary, as recent studies indicate that networks trained with simplified neuron models can be effectively deployed on neuromorphic hardware with negligible performance loss (<xref ref-type="bibr" rid="c9">Cakal et al., 2024</xref>). This result indeed hints at the fact that even simplified neuron models could represent the essential computational dynamics of biological networks. Another significant assumption in our model is that synapses are modeled as simple resistors. Whether this level of abstraction suffices for developing functional biological networks remains an open question.</p>
<p>Moreover, the current implementation of SynaptoGen assumes a feedforward multipartite network topology, which can be achieved by genetically inhibiting, through an additional set of genes, all synapses that could form between compatible neurons in non-adjacent layers. Another possiblity would be to employ an external pruning process. However, biological neuronal networks exhibit more complex, often cyclic wiring. SynaptoGen can be extended to account for this complexity by integrating its weight matrix decomposition into frameworks like <italic>4Ward</italic> (<xref ref-type="bibr" rid="c5">Boccato et al., 2024a</xref>,<xref ref-type="bibr" rid="c6">b</xref>), which are designed to convert arbitrary graphs into neural networks trainable via backpropagation. Incorporating more complex topologies would also enable compliance with constraints derived from contactomic data (i.e., information about the physical contacts between neurons), as synapses can only form between neurons in close proximity. Using such topologies, SynaptoGen could learn the average number of synapses only for neuron pairs which are actually in contact. Another phenomenon which his not modeled in the current version of SynaptoGen is the modulation of genetic rules based on the distance between neurons. As highlighted in (<xref ref-type="bibr" rid="c28">St√∂ckl et al., 2022</xref>), connection probabilities decay exponentially with increasing distance. This behavior can be incorporated into SynaptoGen without compromising differentiability by assigning each neuron a positional embedding to calculate distances. This embedding could be learnable, given as a prior, or derived from genetic expression. We recall that a function such as <inline-formula id="ID3">
<alternatives>
<mml:math display="inline" id="I3"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>‚àí</mml:mo><mml:mrow><mml:mo>‚Äñ</mml:mo><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mi>u</mml:mi></mml:msup><mml:mo>‚àí</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>œÖ</mml:mi></mml:msup></mml:mrow><mml:mo>‚Äñ</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math>
<inline-graphic xlink:href="2402.07242v3_ieq3.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula>, where <italic><bold>p</bold><sup>u</sup></italic> and <italic><bold>p</bold><sup>œÖ</sup></italic> are the positions of neurons <italic>u</italic> and <italic>œÖ</italic>, is differentiable w.r.t. the positions except at coincident points.</p>
<p>Finally, the bio-plausibility of SynaptoGen could be enhanced by integrating experimental data on the circuit-level properties of synapses formed by specific gene interactions-parameters - these are currently treated as learnable. While the data available for integration is limited, promising progress by various research groups suggests this limitation may soon be alleviated (<xref ref-type="bibr" rid="c29">Taylor et al., 2021</xref>; <xref ref-type="bibr" rid="c23">Kov√°cs et al., 2020</xref>; <xref ref-type="bibr" rid="c13">Fenyves et al., 2020</xref>; <xref ref-type="bibr" rid="c15">Harris et al., 2022</xref>). Additionally, developing more faithful synaptogenesis simulation techniques-such as replacing average conductances with further sampling from the learned distributions-could make SynaptoGen even more biologically accurate.</p>
<sec id="s5" sec-type="conclusions">
<title>Conclusions</title>
<p>In this work, we introduced SynaptoGen, a novel computational framework aimed at advancing the field of synthetic biological intelligence by addressing a critical challenge: controlling the genetic factors underlying synaptogenesis to develop neural agents pre-wired for task-specific behavior. By modeling synaptic multiplicity through gene expression and protein interaction probabilities, SynaptoGen extends the existing frameworks by offering a granular approach to understanding neural connectivity. SynaptoGen is supported by an innovative use of the formalism of random variables and complemented by rigorous mathematical proves.</p>
<p>The differentiable nature of SynaptoGen makes it uniquely suited for integration with gradientbased optimization techniques. This capability enables the joint optimization of genetic quantities of interest and the performance of resulting agents in selected tasks, thus bridging a critical gap between biological plausibility and computing. Out validation experiments demonstrated the effectiveness of SynaptoGen across multiple RL benchmarks, with agents derived from the framework consistently outperforming the considered baselines. These results highlight the potential of SynaptoGen as a foundational tool for generating task-specific biological agents with unparalleled precision.</p>
<p>While SynaptoGen marks significant advancements over the existing methodologies, important challenges remain before its application in real-world contexts can be conceptualized. Key gaps include the event-driven nature of biological neural networks, the employed synapse model, and the reliance on feedforward topologies that do not fully capture the complexity of biological wiring. We proposed several mitigations to address these limitations, such as adapting the framework for spiking neural networks, incorporating positional embeddings to account for distance-dependent connectivity, and leveraging experimental data to refine circuit-level properties. These extensions will not only enhance the biological plausibility of SynaptoGen but also expand its applicability to more complex and realistic scenarios.</p>
<p>By enabling the precise manipulation of genetic rules governing neural circuit formation, we expect SynaptoGen to lay the foundation for groundbreaking advancements in the development of biological agents and the realization of applications that are currently beyond our reach.</p>
</sec>
</sec>
</body>
<back>
<app-group>
<app id="s6">
<title>Appendix 1</title>
<sec id="s6-1">
<title>The Polarity Matrix</title>
<p>As outlined in (<xref ref-type="bibr" rid="c13">Fenyves et al., 2020</xref>), synapse polarity in <italic>C. elegans</italic>, a well-studied small nematode, is described by the interplay among 3 neurotransmitters-glutamate, acetylcholine, and GABA-and their corresponding receptors. Specifically, each neurotransmitter can be associated with receptors capable of exerting both excitatory and inhibitory effects on synaptic connections. This relationship can be represented by a 3 √ó (2 ¬∑ 3) polarity matrix:
<disp-formula id="FD20">
<alternatives>
<mml:math id="M20" display="block"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2402.07242v3_eqn20.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(14)</label>
</disp-formula>
</p>
<p>Here, each neurotransmitter synthesized in pre-synaptic neurons can be bind to either a positive (+) or negative (‚àí) receptor in post-synaptic neurons. The 0s in the matrix signify that receptors attuned to a specific neurotransmitter are incapable of receiving different ones. This formalism readily extends to accommodate an arbitrary number of neurotransmitters by setting <italic>M</italic> = 2<italic>L</italic> and expanding <italic>A</italic> to an <italic>L</italic> √ó 2<italic>L</italic> block diagonal matrix, where each block is represented as [1,-1]. It is worth noting that, in the experiments described in Section Results, the entries of <italic>A</italic> do not belong to the set of learnable parameters.</p>
</sec>
<sec id="s6-2">
<title>Simulating Synaptogenesis</title>
<p><xref ref-type="fig" rid="fig5">Algorithm 1</xref> outlines one possible implementation of the synaptogenesis simulation introduced at the end of Section Methods, which we used in our experiments.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Algorithm 1:</label>
<caption><title>Our synaptogenesis simulation procedure.</title></caption>
<graphic xlink:href="2402.07242v3_fig7.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>In this algorithm, <italic>G</italic> represents the number of genes involved, notation ¬∑<sub>:</sub>,<italic><sub>i</sub></italic> represents selection of the <italic>i</italic>-th column, ‚äó denotes the outer product, the method <italic>round</italic>() performs pointwise rounding to the nearest integer, while <inline-formula id="ID4">
<alternatives>
<mml:math display="inline" id="I4"><mml:msubsup><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>Àú</mml:mo></mml:mover><mml:mrow><mml:mi>u</mml:mi><mml:mi>œÖ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:math>
<inline-graphic xlink:href="2402.07242v3_ieq4.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula> is the contribution of gene pair (<italic>i, j</italic>) to the number of synapses of neuron pair (<italic>u,œÖ</italic>). All quantities mentioned in <xref ref-type="fig" rid="fig5">Algorithm 1</xref> are restricted to a specific neural layer.</p>
<p>The coefficient <italic>Œ±</italic> is a general correction factor used to control the average degree of nodes in the generated networks. The necessity of this correction is discussed in Appendix A Problem of Quantization. Here, we show that this correction does not impact the average weight matrices learned by SynaptoGen:</p>
<sec id="s6-2-1">
<title>Proposition 3</title>
<p><italic>The average weight matrix </italic><italic><inline-formula id="ID5">
<alternatives>
<mml:math display="inline" id="I5"><mml:msup><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msup></mml:math>
<inline-graphic xlink:href="2402.07242v3_ieq5.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula></italic><italic> resulting from the correction shown in </italic><xref ref-type="fig" rid="fig5">Algorithm 1</xref><italic>, with Œ±&gt; 0, coincides with the average weight matrix learned by SynaptoGen, </italic><italic><inline-formula id="ID6">
<alternatives>
<mml:math display="inline" id="I6"><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover></mml:math>
<inline-graphic xlink:href="2402.07242v3_ieq6.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula></italic>.</p>
<p><italic>Proof.</italic></p>
<disp-formula id="FD21">
<alternatives>
<mml:math id="M21" display="block"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mi>Œ±</mml:mi></mml:msqrt><mml:mi>X</mml:mi><mml:mi>O</mml:mi><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msqrt><mml:mi>Œ±</mml:mi></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>‚äô</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Œ±</mml:mi></mml:mfrac><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>‚ÄÉ‚ÄÉ</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mi>O</mml:mi><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mtext>‚Äâ</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>‚äô</mml:mo><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>‚ÄÉ‚ÄÉ</mml:mtext><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover></mml:mtd></mml:mtr></mml:mtable></mml:math>
<graphic xlink:href="2402.07242v3_eqn21.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
<p>‚ñ´</p>
<p>In all our simulations, we chose Œ± such that the resulting networks had an average degree of 10<sup>5</sup>. While this value is quite high (the typical number of synapses per neuron in the human brain is on the order of magnitude of 10<sup>4</sup> (<xref ref-type="bibr" rid="c32">Zhang, 2019</xref>)), it aligns with the connectivity levels observed in certain types of cells in specific brain regions, e.g., Purkinje cells in the cerebellar cortex (<xref ref-type="bibr" rid="c25">Napper and Harvey, 1988</xref>).</p>
</sec>
</sec>
<sec id="s6-3">
<title>A Problem of Quantization</title>
<p>The synaptogenesis simulation introduces a subtle ‚Äúquantization‚Äù issue. When the order of magnitude of the input provided to a SynaptoGen network and the average conductances associated with each pair of neurons are fixed, the number of synapses determines the range of values each weight can take. Conversely, when the number of synapses is fixed, the average conductances dictate the granularity with which a given range can be represented. Thus, the interplay between these factors significantly affects the degree of synaptic weight quantization. Additionally, the role of rounding, introduced in <xref ref-type="fig" rid="fig5">Algorithm 1</xref> to ensure that the parameters <italic>n<sub>ij</sub></italic> of the binomial random variables are integers, cannot be overlooked. The inherently discrete nature of these random variables also plays a fundamental role in the quantization process.</p>
<p>It is therefore interesting to study the error <inline-formula id="ID7">
<alternatives>
<mml:math display="inline" id="I7"><mml:mi>W</mml:mi><mml:mo>‚àí</mml:mo><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover></mml:math>
<inline-graphic xlink:href="2402.07242v3_ieq7.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula>, specifically the contribution to this error made by a single gene pair for a specific pair of neurons. It can be shown that:</p>
<sec id="s6-3-1">
<title>Proposition 4</title>
<p><italic>When considering an optimal simulated agent-characterized by a number of synapses as close as possible to the mean learned by SynaptoGen-and assuming the correction introduced in </italic><xref ref-type="fig" rid="fig5">Algorithm 1</xref><italic> is applied, the error e between the mean number of synapses predicted by SynaptoGen and the actual number of synapses in the agent is bounded by</italic> <inline-formula id="ID8">
<alternatives>
<mml:math display="inline" id="I8"><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mrow><mml:mi>u</mml:mi><mml:mi>œÖ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>Œ±</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2402.07242v3_ieq8.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula>.</p>
<p><italic>Proof.</italic></p>
<disp-formula id="FD22">
<alternatives>
<mml:math id="M22" display="block"><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtext>‚Äã</mml:mtext><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mrow><mml:mi>u</mml:mi><mml:mi>œÖ</mml:mi></mml:mrow></mml:msub><mml:mo>‚àí</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Œ±</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Œ±</mml:mi></mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mrow><mml:mi>u</mml:mi><mml:mi>œÖ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2402.07242v3_eqn22.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(15)</label>
</disp-formula>
<disp-formula id="FD23">
<alternatives>
<mml:math id="M23" display="block"><mml:mtext>‚Äâ‚Äâ‚Äâ</mml:mtext><mml:mo>‚â§</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Œ±</mml:mi></mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mrow><mml:mi>u</mml:mi><mml:mi>œÖ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>Œ±</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>‚àí</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Œ±</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2402.07242v3_eqn23.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(16)</label>
</disp-formula>
<disp-formula id="FD24">
<alternatives>
<mml:math id="M24" display="block"><mml:mtext>‚Äâ‚Äâ‚Äâ</mml:mtext><mml:mo>‚â§</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Œ±</mml:mi></mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mrow><mml:mi>u</mml:mi><mml:mi>œÖ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>Œ±</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>‚àí</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Œ±</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2402.07242v3_eqn24.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(17)</label>
</disp-formula>
<disp-formula id="FD25">
<alternatives>
<mml:math id="M25" display="block"><mml:mtext>‚Äâ‚Äâ‚Äâ</mml:mtext><mml:mo>‚â§</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Œ±</mml:mi></mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mrow><mml:mi>u</mml:mi><mml:mi>œÖ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>Œ±</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>‚àí</mml:mo><mml:mtext>‚Äã</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Œ±</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2402.07242v3_eqn25.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(18)</label>
</disp-formula>
<disp-formula id="FD26">
<alternatives>
<mml:math id="M26" display="block"><mml:mtext>‚Äâ‚Äâ‚Äâ</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>¬Ø</mml:mo></mml:mover><mml:mrow><mml:mi>u</mml:mi><mml:mi>œÖ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>Œ±</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2402.07242v3_eqn26.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(19)</label>
</disp-formula>
<p>In this proof, <inline-formula id="ID9">
<alternatives>
<mml:math display="inline" id="I9"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>ùîº</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2402.07242v3_ieq9.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula>, the outer round in step <xref ref-type="disp-formula" rid="FD22">(15)</xref> helps account for the nearest integer approximation ofthe mean number of synapses learned by SynaptoGen, and steps <xref ref-type="disp-formula" rid="FD24">(17)</xref> and <xref ref-type="disp-formula" rid="FD25">(18)</xref> are derived from bounds applied to the two rounding operations.</p>
<p>‚ñ´</p>
<p>It is evident that choosing a sufficiently large Œ±-and thus achieving a sufficiently high average neuronal degree-can bring the weight matrices ofthe best simulated agents closer to the optimal matrices identified by SynaptoGen. This reduces the performance loss of these agents on the task for which they were pre-wired.</p>
</sec>
</sec>
<sec id="s6-4">
<title>Training Details</title>
<p>This section provides all the details related to the training of the SynaptoGen models that were not explicitly mentioned in Section Methods Neural networks were implemented as custom MLPs featuring a single hidden layer with 128 neurons, a number that allowed us to easily conduct experiments with the computational resources available. The input and output layers were configured with a number of neurons equal to the dimensionality of the observation space and the number of possible actions in the selected RL environments, respectively. Consequently, the size of the input layer ranged from 2 to 8, while the size of the output layer ranged from 2 to 4. Details about the RL environments can be found in the Gym library documentation: <ext-link ext-link-type="uri" xlink:href="https://www.gymlibrary.dev/index.html">https://www.gymlibrary.dev/index.html</ext-link>. Learnable parameters associated with genetic rules and conductances were initialized following the procedure described in (<xref ref-type="bibr" rid="c2">Barab√°si et al., 2023</xref>). All other parameters were initialized using a Kaiming normal distribution (<xref ref-type="bibr" rid="c16">He et al., 2015</xref>).</p>
<p>Trainings were conducted using the Adam optimizer (<xref ref-type="bibr" rid="c21">Kingma and Ba, 2015</xref>), with its default parameters unchanged except for the learning rate. Training consisted of 500000 <italic>environment steps</italic>. To minimize randomness in the results and mitigate sub-optimality associated with specific hyperparameter choices, the genetic profiles used in simulations were selected through a hyperparameter grid-search. This search explored three different learning rates (0.03, 0.003, 0.0003) and three random seeds for various sampling processes. Specifically, after hyperparameter optimization, we retained the checkpoint corresponding to the best validation performance achieved by the models. Validation was conducted every 10000 training steps. During both validation and subsequent simulations, agent rewards were computed as the average scores over 10 different episodes. The remaining hyperparameters related to the DQN algorithm were sourced from: <ext-link ext-link-type="uri" xlink:href="https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml">https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml</ext-link>. Additional information can be found in the released code.</p>
</sec>
<sec id="s6-5">
<title>The Separable Natural Evolution Strategy (SNES) Baseline</title>
<p>As mentioned in Section Results, the <italic>SNES baseline</italic> was constructed by replacing gradient descent in our SynaptoGen framework with SNES, an optimization technique introduced in (<xref ref-type="bibr" rid="c27">Schaul, 2012</xref>) and employed by the authors of (<xref ref-type="bibr" rid="c28">St√∂ckl et al., 2022</xref>) for a related task, namely the optimization of <italic>probabilistic skeletons</italic>. The core idea behind SNES is to sample <italic>Œª offsprings</italic> from a normal distribution of the type ∆ù(<italic><bold>Œº, IœÉ</bold></italic>), where <italic><bold>Œº</bold></italic> is built by concatenating the flattened learnable matrices of SynaptoGen. These offsprings are evaluated based on their <italic>fitness</italic>, allowing the distribution to adapt and better capture those regions of the parameter space where offspring fitness is higher. The implementation details of SNES are provided in the pseudo-code of <xref ref-type="fig" rid="fig6">Algorithm 2</xref>.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Algorithm 2:</label>
<caption><title>The SNES optimization technique.</title></caption>
<graphic xlink:href="2402.07242v3_fig8.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Inside the algorithm, the vectors <italic><bold>s</bold><sub>k</sub></italic> contain the ‚Äúnoise‚Äù used in generating the offsprings, <italic><bold>œÉ</bold></italic> is a vector of variances initialized in accordance with (<xref ref-type="bibr" rid="c28">St√∂ckl et al., 2022</xref>), <italic>F</italic> is the fitness function, <italic><bold>Œ∑<sub>Œº</sub></bold></italic> and <italic><bold>Œ∑<sub>œÉ</sub></bold></italic> are two learning rates &gt; 0, and the method <italic>steps_performed</italic>() returns the number of actions taken during the current iteration on the RL environment being optimized. Specifically, we defined the fitness function <italic>F</italic>(¬∑) as the average performance achieved by <italic>m</italic> simulated biological agents based on the genetic profile provided as input to the function. Individual scores were calculated by averaging the rewards obtained by an agent over 10 different episodes.</p>
<p>To ensure fair comparisons with the profiles obtained through gradient descent, we limited the number of actions executable on a RL environment to 500000 in the context of a single optimization. Additionally, for this baseline, we conducted a hyperparameter search similar to the one described in Appendix Training Details, varying <italic>m</italic> in the set {10,20, 30} and assigning <italic>Œª</italic>, during the search, the values 8,16, and the default value of the SNES implementation used (<ext-link ext-link-type="uri" xlink:href="https://github.com/pybrain/pybrain">https://github.com/pybrain/pybrain</ext-link>).</p>
</sec>
<sec id="s6-6">
<title>The Bio-Plausible Baseline</title>
<p>As a control, we designed an additional baseline, referred to as <italic>bio-plausible</italic> in the main text, intended to model a scenario where a neural population with a predefined number of neurons is left free to form synapses without any manipulation of genes or gene expression in individual neurons tailored to a specific task. In other words, this baseline was designed to assess whether there is a performance difference between agents produced by SynaptoGen and neuronal networks that develop without guidance, or equivalently, to determine whether the scores achieved by SynaptoGen agents are due to chance.</p>
<p>To do this, we fixed the genetic rules learned by SynaptoGen for each environment as the biological ground truth and initialized the gene expression of the individual neurons in the tested agents according to the <italic>lineal model</italic> proposed in (<xref ref-type="bibr" rid="c19">Kerstjens et al., 2022</xref>). This model is simple yet profound and elegant, and it is capable of predicting phenomena later validated by existing experimental data (<xref ref-type="bibr" rid="c18">Kerstjens et al., 2024</xref>). According to the lineal model, the expression inherited by two cells <italic>u</italic> and <italic>œÖ</italic> resulting from mitosis is calculated by adding a differential expression vector, drawn from a normal distribution, to the expression of the parent cell <italic>p</italic>. Formally:
<disp-formula id="FD27">
<alternatives>
<mml:math id="M27" display="block"><mml:msub><mml:mi>c</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Œ¥</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math>
<graphic xlink:href="2402.07242v3_eqn27.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(20)</label>
</disp-formula>
<disp-formula id="FD28">
<alternatives>
<mml:math id="M28" display="block"><mml:msub><mml:mi>c</mml:mi><mml:mi>œÖ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Œ¥</mml:mi><mml:mi>œÖ</mml:mi></mml:msub></mml:math>
<graphic xlink:href="2402.07242v3_eqn28.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(21)</label>
</disp-formula>
<disp-formula id="FD29">
<alternatives>
<mml:math id="M29" display="block"><mml:msub><mml:mi>Œ¥</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Œ¥</mml:mi><mml:mi>œÖ</mml:mi></mml:msub><mml:mo>‚àº</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2402.07242v3_eqn29.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(22)</label>
</disp-formula>
</p>
<p>Here, the genetic expression pattern <italic><bold>c</bold></italic> = [<italic><bold>x</bold><sup>T</sup>, <bold>q</bold><sup>T</sup>, <bold>r</bold><sup>T</sup></italic>]<italic><sup>T</sup></italic> of a generic neuron is understood as the concatenation of genetic expression responsible for synaptic multiplicity, neurotransmitter distribution, and receptor distribution. With a slight abuse of notation, we are actually using these variables to refer to the previously mentioned vectors before they are normalized or otherwise mapped into the domain of the genetic quantities of interest.</p>
<p>In more detail, for each agent, we sampled the genetic expression profile of a virtual zygote from a normal distribution and simulated a lineage tree with as many leaves as the number of neurons in the agent. This allowed us to assign gene expression profiles to individual cells following the equations above. We finalized the procedure by randomly assigning each neural layer expression profiles corresponding to leaves from the same complete subtree, simulating spatial contiguity for neurons within the same layer. The procedure we implemented is detailed in <xref ref-type="fig" rid="fig7">Algorithm 3</xref>. In the pseudo-code, the first line initializes the zygote‚Äôs genetic expression, <italic>N</italic> corresponds to the target number of cells to be produced, the notation ¬∑<sub>1‚Ä¶<italic>N</italic></sub> selects the first <italic>N</italic> columns ofthe respective matrix, function <italic>rand_roll</italic> () circularly permutes the columns ofthe input matrix, and <italic>C<sup>in</sup></italic>, <italic>c<sup>hidden</sup></italic> and <italic>C<sup>out</sup></italic> represent the matrices associated with the genetic expressions of the various layers of the agent being initialized.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Algorithm 3:</label>
<caption><title>Our <italic>lineal model</italic>-based initialization procedure for gene expression.</title></caption>
<graphic xlink:href="2402.07242v3_fig9.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s6-7">
<title>Reward Thresholds for Considering a Task Solved</title>
<p>There is no consensus within the RL community on the definition of ‚Äúsolving‚Äù when it comes to the most popular benchmark tasks used for evaluating new algorithms. Therefore, we derived reward thresholds, based on the guidelines provided at <ext-link ext-link-type="uri" xlink:href="https://www.gymlibrary.dev">https://www.gymlibrary.dev</ext-link>, to serve as a reference for distinguishing agents capable of solving their respective RL environments from those unable to achieve satisfactory performance.</p>
<p>For Cart Pole, the threshold was set to 195, which represents the number of consecutive time steps an agent must keep the pole balanced. The environment provides a reward of +1 for every step the pole remains upright. For Mountain Car, we used a threshold of ‚àí200. The objective of this task is to propel the car to the top of a hill within the environment. We consider the task solved in all cases where the car successfully reaches the goal, and consider it unsolved only when the car fails to reach the goal within the simulation time, which results in a reward of-200. In Lunar Lander, the threshold is set to 200, a reward that Gym documentation associates with a soft landing on the designated landing pad, without any crashes. Finally, for Acrobot, we consider the task solved if the agent manages to swing the chain above the target height before the simulation ends. This corresponds to achieving a reward greater than ‚àí500.</p>
</sec>
<sec id="s6-8">
<title>Additional Results</title>
<p>We provide here, for the sake of completeness, the results obtained by optimizing models characterized by genetic profiles containing 32 and 64 genes. These results are shown in <xref ref-type="fig" rid="fig1">Figures 1</xref>, <xref ref-type="fig" rid="fig2">2</xref> and <xref ref-type="table" rid="tbl1">Tables 1</xref>,<xref ref-type="table" rid="tbl2">2</xref>.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Appendix 1‚Äîfigure 1.</label>
<caption><title>Mean reward distributions from the model families, characterized by a 32-gene profile, tested on the four selected RL environments.</title>
<p>The green crosses represent the mean rewards, averaged over 10 episodes, achieved by the trained SynaptoGen models. Each <italic>scatterplot</italic> point represents the mean reward obtained by a specific agent. The black crosses denote instead the distribution means. Model families are color-coded. The dashed horizontal lines indicate the reward threshold beyond which the task associated with an environment is considered solved.</p></caption>
<graphic xlink:href="2402.07242v3_fig5.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl3" position="float" orientation="portrait">
<label>Appendix 1‚Äîtable 1.</label>
<caption><title>Summary metrics computed over the reward distributions obtained from the model families characterized by a 32-gene profile.</title>
<p>Three metrics are reported: the distribution mean, a mean computed over the top-10 agents, and the percentage of simulated agents that solved the task. Each group of rows refers to one of the selected RL environments and the best scores are highlighted in bold.</p></caption>
<alternatives>
<graphic xlink:href="2402.07242v3_tbl3.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="bottom">environment</th>
<th align="left" valign="bottom">model</th>
<th align="right" valign="top">mean</th>
<th align="right" valign="top">std</th>
<th align="right" valign="top">top-10 mean</th>
<th align="right" valign="top">top-10 std</th>
<th align="right" valign="top">% solved</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="middle" rowspan="3">CartPole-v1</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">493.31</td>
<td align="right" valign="top">27.04</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">13.19</td>
<td align="right" valign="top">20.32</td>
<td align="right" valign="top">47.28</td>
<td align="right" valign="top">55.74</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">MountainCar-vO</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">‚àí<bold>190.63</bold></td>
<td align="right" valign="top">20.93</td>
<td align="right" valign="top">‚àí<bold>136.09</bold></td>
<td align="right" valign="top">9.09</td>
<td align="right" valign="top"><bold>22.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">LunarLander-v2</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top"><bold>154.50</bold></td>
<td align="right" valign="top">39.34</td>
<td align="right" valign="top"><bold>217.98</bold></td>
<td align="right" valign="top">8.39</td>
<td align="right" valign="top"><bold>17.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-81.45</td>
<td align="right" valign="top">24.69</td>
<td align="right" valign="top">-36.21</td>
<td align="right" valign="top">6.18</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-583.41</td>
<td align="right" valign="top">293.58</td>
<td align="right" valign="top">-128.53</td>
<td align="right" valign="top">5.40</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">Acrobot-v1</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">‚àí<bold>311.14</bold></td>
<td align="right" valign="top">180.04</td>
<td align="right" valign="top">‚àí<bold>80.78</bold></td>
<td align="right" valign="top">1.56</td>
<td align="right" valign="top">61.00%</td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-354.02</td>
<td align="right" valign="top">49.73</td>
<td align="right" valign="top">-268.30</td>
<td align="right" valign="top">21.28</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-496.89</td>
<td align="right" valign="top">31.06</td>
<td align="right" valign="top">-468.94</td>
<td align="right" valign="top">98.22</td>
<td align="right" valign="top">1.00%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="fig9" position="float" fig-type="figure">
<label>Appendix 1‚Äîfigure 2.</label>
<caption><title>Mean reward distributions from the model families, characterized by a 64-gene profile, tested on the four selected RL environments.</title>
<p>The green crosses represent the mean rewards, averaged over 10 episodes, achieved by the trained SynaptoGen models. Each <italic>scatterplot</italic> point represents the mean reward obtained by a specific agent. The black crosses denote instead the distribution means. Model families are color-coded. The dashed horizontal lines indicate the reward threshold beyond which the task associated with an environment is considered solved.</p></caption>
<graphic xlink:href="2402.07242v3_fig6.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl4" position="float" orientation="portrait">
<label>Appendix 1‚Äîtable 2.</label>
<caption><title>Summary metrics computed over the reward distributions obtained from the model families characterized by a 64-gene profile.</title>
<p>Three metrics are reported: the distribution mean, a mean computed over the top-10 agents, and the percentage of simulated agents that solved the task. Each group of rows refers to one ofthe selected RL environments and the best scores are highlighted in bold.</p></caption>
<alternatives>
<graphic xlink:href="2402.07242v3_tbl4.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="bottom">environment</th>
<th align="left" valign="bottom">model</th>
<th align="right" valign="top">mean</th>
<th align="right" valign="top">std</th>
<th align="right" valign="top">top-10 mean</th>
<th align="right" valign="top">top-10 std</th>
<th align="right" valign="top">% solved</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="middle" rowspan="3">CartPole-vl</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">498.84</td>
<td align="right" valign="top">6.63</td>
<td align="right" valign="top"><bold>500.00</bold></td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top"><bold>100.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">15.93</td>
<td align="right" valign="top">27.03</td>
<td align="right" valign="top">75.00</td>
<td align="right" valign="top">61.07</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">MountainCar-v0</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">‚àí<bold>199.80</bold></td>
<td align="right" valign="top">1.43</td>
<td align="right" valign="top">‚àí<bold>197.99</bold></td>
<td align="right" valign="top">4.30</td>
<td align="right" valign="top"><bold>2.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">-200.00</td>
<td align="right" valign="top">0.00</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">LunarLander-v2</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top"><bold>151.21</bold></td>
<td align="right" valign="top">40.20</td>
<td align="right" valign="top"><bold>215.45</bold></td>
<td align="right" valign="top">13.43</td>
<td align="right" valign="top"><bold>13.00%</bold></td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-127.01</td>
<td align="right" valign="top">10.55</td>
<td align="right" valign="top">-109.01</td>
<td align="right" valign="top">3.63</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="top">bio-plausible</td>
<td align="right" valign="top">-552.04</td>
<td align="right" valign="top">270.15</td>
<td align="right" valign="top">-129.60</td>
<td align="right" valign="top">12.52</td>
<td align="right" valign="top">0.00%</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="3">Acrobot-vl</td>
<td align="left" valign="top">SynaptoGen</td>
<td align="right" valign="top">‚àí<bold>451.23</bold></td>
<td align="right" valign="top">118.92</td>
<td align="right" valign="top">‚àí<bold>121.76</bold></td>
<td align="right" valign="top">40.33</td>
<td align="right" valign="top">18.00%</td>
</tr>
<tr>
<td align="left" valign="top">SNES</td>
<td align="right" valign="top">-462.01</td>
<td align="right" valign="top">37.41</td>
<td align="right" valign="top">-385.67</td>
<td align="right" valign="top">21.67</td>
<td align="right" valign="top"><bold>68.00%</bold></td>
</tr>
<tr>
<td align="right" valign="top">bio-plausible</td>
<td align="right" valign="top">-489.99</td>
<td align="right" valign="top">57.73</td>
<td align="right" valign="top">-399.88</td>
<td align="right" valign="top">163.21</td>
<td align="right" valign="top">4.00%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="s6-9">
<title>nDGE-based Bio-Plausible Rules</title>
<p>To produce the genetic rules used in the bio-plausible validation in Section Results, we first computed the gene co-expressions of the <italic>C. elegans</italic> nerve ring by performing a generalized nDGE analysis on the dataset provided in (<xref ref-type="bibr" rid="c29">Taylor et al., 2021</xref>). This generalization was necessary because the nDGE implementation available at <ext-link ext-link-type="uri" xlink:href="https://github.com/cengenproject/connectivity_analysis">https://github.com/cengenproject/connectivity_analysis</ext-link> is designed to calculate local co-expressions, specifically limited to a set of cells consisting of a pre-synaptic neuron and its corresponding post-synaptic partners.</p>
<p>To compute global co-expressions, we identified all pairs of neurons that shared synapses and all the pairs that were in contact without being connected. Then, for each possible pair of genes, we calculated co-expressions for both sets and conducted a T-test with Bonferroni correction. Pairs with a <italic>p</italic>-value &lt; 0.05 were considered co-expressed. The detailed procedure is outlined in <xref ref-type="fig" rid="fig10">Algorithm 4</xref>. In this pseudo-code, <italic>B</italic> and <italic>C</italic> represent the connectome and the contactome ofthe neuronal network under analysis, respectively, ‚äïdenotes a pointwise XOR operation, and <italic>G</italic> corresponds to the number of genes analyzed. The third-level ‚Äúfor‚Äù loops, instead, separate the indices of pre- and post-synaptic neurons involved in the pairs being evaluated in each iteration. The co-expression of a specific pair of genes for all neuron pairs in a given set is calculated as the pointwise product <inline-formula id="ID10">
<alternatives>
<mml:math display="inline" id="I10"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>‚äô</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>œÖ</mml:mi><mml:mo>‚Ä≤</mml:mo></mml:msup></mml:mrow></mml:msub></mml:math>
<inline-graphic xlink:href="2402.07242v3_ieq10.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula>, where <italic>C</italic> is the matrix containing gene expression data for all neurons, and the notation ¬∑<sub><italic>i,<bold>u</bold></italic></sub> selects the <italic>i</italic>-th row (corresponding to gene <italic>i</italic>) and the columns associated with the neurons in <italic><bold>u</bold></italic>. The method <italic>ttest</italic>() performs the T-test, while <italic>p<sub>ij</sub></italic> and <italic>g<sub>ij</sub></italic> represent the <italic>p</italic>-value for the gene pair (<italic>i,j</italic>) and its corresponding binary co-expression, respectively.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Algorithm 4:</label>
<caption><title>Our global nDGE variant.</title></caption>
<graphic xlink:href="2402.07242v3_fig10.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>After obtaining the co-expressions (illustrated in <xref ref-type="fig" rid="fig3">Figure 3</xref>), and to avoid constructing overly artificial rules-since co-expression reflects correlation while genetic rules imply causation-while still maintaining sufficient bio-plausibility, we allowed the models to learn the matrix <italic>O</italic> but constrained the domain of its entries. Specifically, starting from Synap- toGen‚Äôs learnable parameters, we computed the genetic rules matrix <italic>O</italic> differently for coexpressed and non-co-expressed gene pairs. The approach involved pushing the rules for co-expressed pairs toward probabilities close to 1, while those for non-co-expressed pairs were pushed toward probabilities close to 0. This was implemented using two sigmoid functions, also shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>:
<disp-formula id="FD30">
<alternatives>
<mml:math id="M30" display="block"><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>sigmoid</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>O</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:msub><mml:mi>œÉ</mml:mi><mml:mover accent="true"><mml:mi>O</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mtext>‚Äã‚ÄÉ‚ÄÉ</mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>‚Äâ</mml:mtext><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>sigmoid</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>O</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:msub><mml:mi>œÉ</mml:mi><mml:mover accent="true"><mml:mi>O</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext>‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ</mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>‚Äâ</mml:mtext><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mtext>‚Äã‚Äã‚ÄÉ</mml:mtext></mml:math>
<graphic xlink:href="2402.07242v3_eqn30.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
<label>(23)</label>
</disp-formula>
</p>
<p>Here, <italic>√î</italic> represents the parameter matrix from which the genetic rules <italic>O</italic> are derived, <italic>œÉ<sub>√î</sub></italic> is the standard deviation of the <italic>√î</italic> values‚Äô distribution, and <italic>T</italic> is the temperature used. The learned genetic rule matrices shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> confirm that SynaptoGen assigned high probabilities (i.e., approximately 1) to co-expressed pairs, supporting the utility of coexpression data for optimization purposes.</p>
</sec>
</app>
</app-group>
<ack>
<title>Acknowledgements</title>
<p>This work is supported and funded by: #NEXTGENERATIONEU (NGEU); the Ministry of University and Research (MUR); the National Recovery and Resilience Plan (NRRP); project MNESYS (PE0000006, to NT) - <italic>A Multiscale integrated approach to the study of the nervous system in health and disease</italic> (DN. 1553 11.10.2022); the MUR-PNRR M4C2I1.3 PE6 project PE00000019 Heal Italia (to NT); the NATIONAL CENTRE FOR HPC, BIG DATA AND QUANTUM COMPUTING, within the spoke <italic>‚ÄúMultiscale Modeling and Engineering Applications‚Äù</italic> (to NT); the European Innovation Council (Project CROSSBRAIN - Grant Agreement 101070908, Project BRAINSTORM - Grant Agreement 101099355); the Horizon 2020 research and innovation Programme (Project EXPERIENCE - Grant Agreement 101017727). Tommaso Boccato is a PhD student enrolled in the National PhD in Artificial Intelligence, XXXVII cycle, course on Health and Life Sciences, organized by Universit√† Campus BioMedico di Roma.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barab√°si</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Barab√°si</surname> <given-names>AL</given-names></string-name></person-group>. <article-title>A Genetic Model of the Connectome</article-title>. <source>Neuron</source>. <year>2020</year>; <volume>105</volume>(<issue>3</issue>):<elocation-id>435‚Äì445.e5</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0896627319309262">https://www.sciencedirect.com/science/article/pii/S0896627319309262</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.031</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barab√°si</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Beynon</surname> <given-names>T</given-names></string-name>, <string-name><surname>Katona</surname> <given-names>√Å</given-names></string-name>, <string-name><surname>Perez-Nieves</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Complex computation from developmental priors</article-title>. <source>Nature Communications</source>. <year>2023</year> <month>Apr</month>; <volume>14</volume>(<issue>1</issue>):<fpage>2226</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-023-37980-1">https://www.nature.com/articles/s41467-023-37980-1</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-023-37980-1</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barab√°si</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Cz√©gel</surname> <given-names>D</given-names></string-name></person-group>. <article-title>Constructing graphs from genetic encodings</article-title>. <source>Scientific Reports</source>. <year>2021</year> <month>Dec</month>; <volume>11</volume>(<issue>1</issue>):<fpage>13270</fpage>. <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/articles/s41598-021-92577-2">http://www.nature.com/articles/s41598-021-92577-2</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41598-021-92577-2</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bhaduri</surname> <given-names>A</given-names></string-name>, <string-name><surname>Andrews</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Kriegstein</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Nowakowski</surname> <given-names>TJ</given-names></string-name></person-group>. <article-title>Are Organoids Ready for Prime Time?</article-title> <source>Cell Stem Cell</source>. <year>2020</year>; <volume>27</volume>(<issue>3</issue>):<fpage>361</fpage>‚Äì<lpage>365</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1934590920304094">https://www.sciencedirect.com/science/article/pii/S1934590920304094</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.stem.2020.08.013</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boccato</surname> <given-names>T</given-names></string-name>, <string-name><surname>Ferrante</surname> <given-names>M</given-names></string-name>, <string-name><surname>Duggento</surname> <given-names>A</given-names></string-name>, <string-name><surname>Toschi</surname> <given-names>N</given-names></string-name></person-group>. <article-title>4Ward: A relayering strategy for efficient training of arbitrarily complex directed acyclic graphs</article-title>. <source>Neurocomputing</source>. <year>2024</year>; <volume>568</volume>:<fpage>127058</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0925231223011815">https://www.sciencedirect.com/science/article/pii/S0925231223011815</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neucom.2023.127058</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boccato</surname> <given-names>T</given-names></string-name>, <string-name><surname>Ferrante</surname> <given-names>M</given-names></string-name>, <string-name><surname>Duggento</surname> <given-names>A</given-names></string-name>, <string-name><surname>Toschi</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Beyond multilayer perceptrons: Investigating complex topologies in neural networks</article-title>. <source>Neural Networks</source>. <year>2024</year>; <volume>171</volume>:<fpage>215</fpage>‚Äì<lpage>228</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0893608023007177">https://www.sciencedirect.com/science/article/pii/S0893608023007177</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neunet.2023.12.012</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brittin</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Cook</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Hall</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Emmons</surname> <given-names>SW</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>N</given-names></string-name></person-group>. <article-title>A multi-scale brain map derived from whole-brain volumetric reconstructions</article-title>. <source>Nature</source>. <year>2021</year> <month>Mar</month>; <volume>591</volume>(<issue>7848</issue>):<fpage>105</fpage>‚Äì<lpage>110</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41586-021-03284-x">https://www.nature.com/articles/s41586-021-03284-x</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41586-021-03284-x</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Brockman</surname> <given-names>G</given-names></string-name>, <string-name><surname>Cheung</surname> <given-names>V</given-names></string-name>, <string-name><surname>Pettersson</surname> <given-names>L</given-names></string-name>, <string-name><surname>Schneider</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schulman</surname> <given-names>J</given-names></string-name>, <string-name><surname>Tang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zaremba</surname> <given-names>W</given-names></string-name></person-group>, <source>OpenAI Gym</source>; <year>2016</year>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cakal</surname> <given-names>U</given-names></string-name>, <string-name><surname>Maryada</surname></string-name>, <string-name><surname>Wu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ulusoy</surname> <given-names>I</given-names></string-name>, <string-name><surname>Muir</surname> <given-names>DR</given-names></string-name></person-group>. <article-title>Gradient-descent hardware-aware training and deployment for mixed-signal neuromorphic processors</article-title>. <source>Neuromorphic Computing and Engineering</source>. <year>2024</year> <month>Mar</month>; <volume>4</volume>(<issue>1</issue>):<fpage>014011</fpage>. <ext-link ext-link-type="uri" xlink:href="https://iopscience.iop.org/article/10.1088/2634-4386/ad2ec3">https://iopscience.iop.org/article/10.1088/2634-4386/ad2ec3</ext-link>, doi: <pub-id pub-id-type="doi">10.1088/2634-4386/ad2ec3</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Col√≥n-Ramos</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Margeta</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Shen</surname> <given-names>K</given-names></string-name></person-group>. <article-title>Glia Promote Local Synaptogenesis Through UNC-6 (Netrin) Signaling in &lt;i&gt;C. elegans&lt;/i&gt;</article-title>. <source>Science</source>. <year>2007</year>; <volume>318</volume>(<issue>5847</issue>):<fpage>103</fpage>‚Äì<lpage>106</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/abs/10.1126/science.1143762">https://www.science.org/doi/abs/10.1126/science.1143762</ext-link>, doi: <pub-id pub-id-type="doi">10.1126/science.1143762</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cook</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Jarrell</surname> <given-names>TA</given-names></string-name>, <string-name><surname>Brittin</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Bloniarz</surname> <given-names>AE</given-names></string-name>, <string-name><surname>Yakovlev</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>KCQ</given-names></string-name>, <string-name><surname>Tang</surname> <given-names>LTH</given-names></string-name>, <string-name><surname>Bayer</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Duerr</surname> <given-names>JS</given-names></string-name>, <string-name><surname>B√ºlow</surname> <given-names>HE</given-names></string-name>, <string-name><surname>Hobert</surname> <given-names>O</given-names></string-name>, <string-name><surname>Hall</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Emmons</surname> <given-names>SW</given-names></string-name></person-group>. <article-title>Whole-animal connectomes of both Caenorhabditis elegans sexes</article-title>. <source>Nature</source>. <year>2019</year> <month>Jul</month>; <volume>571</volume>(<issue>7763</issue>):<fpage>63</fpage>‚Äì<lpage>71</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41586-019-1352-7">https://www.nature.com/articles/s41586-019-1352-7</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41586-019-1352-7</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eshraghian</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>M</given-names></string-name>, <string-name><surname>Neftci</surname> <given-names>E</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Lenz</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dwivedi</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bennamoun</surname> <given-names>M</given-names></string-name>, <string-name><surname>Jeong</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>WD</given-names></string-name></person-group>. <article-title>Training spiking neural networks using lessons from deep learning</article-title>. <source>Proceedings of the IEEE</source>. <year>2023</year>; <volume>111</volume>(<issue>9</issue>):<fpage>1016</fpage>‚Äì<lpage>1054</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fenyves</surname> <given-names>BG</given-names></string-name>, <string-name><surname>Szil√°gyi</surname> <given-names>GS</given-names></string-name>, <string-name><surname>Vassy</surname> <given-names>Z</given-names></string-name>, <string-name><surname>S√∂ti</surname> <given-names>C</given-names></string-name>, <string-name><surname>Csermely</surname> <given-names>P</given-names></string-name></person-group>. <article-title>Synaptic polarity and sign-balance prediction using gene expression data in the Caenorhabditis elegans chemical synapse neuronal connectome network</article-title>. <source>PLOS Computational Biology</source>. <year>2020</year> <day>12</day>; <volume>16</volume>(<issue>12</issue>):<fpage>1</fpage>‚Äì<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1007974</pub-id>, doi: <pub-id pub-id-type="doi">10.1371/jour-nal.pcbi.1007974</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Graves</surname> <given-names>A</given-names></string-name></person-group>, <article-title>Generating Sequences With Recurrent Neural Networks</article-title>; <year>2014</year>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Wytock</surname> <given-names>TP</given-names></string-name>, <string-name><surname>Kov√°cs</surname> <given-names>IA</given-names></string-name></person-group>. <article-title>Computational Inference of Synaptic Polarities in Neuronal Networks</article-title>. <source>Advanced Science</source>. <year>2022</year>; <volume>9</volume>(<issue>16</issue>):<fpage>2104906</fpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/abs/10.1002/advs.202104906">https://onlinelibrary.wiley.com/doi/abs/10.1002/advs.202104906</ext-link>, doi: <pub-id pub-id-type="doi">10.1002/advs.202104906</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</article-title>. In: <conf-name>2015 IEEE International Conference on Computer Vision (ICCV)</conf-name>; <year>2015</year>. p. <fpage>1026</fpage>‚Äì<lpage>1034</lpage>. doi: <pub-id pub-id-type="doi">10.1109/ICCV.2015.123</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kagan</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Kitchen</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Tran</surname> <given-names>NT</given-names></string-name>, <string-name><surname>Habibollahi</surname> <given-names>F</given-names></string-name>, <string-name><surname>Khajehnejad</surname> <given-names>M</given-names></string-name>, <string-name><surname>Parker</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Bhat</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rollo</surname> <given-names>B</given-names></string-name>, <string-name><surname>Razi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>KJ</given-names></string-name></person-group>. <article-title>In vitro neurons learn and exhibit sentience when embodied in a simulated game-world</article-title>. <source>Neuron</source>. <year>2022</year>; <volume>110</volume>(<issue>23</issue>):<elocation-id>3952‚Äì3969.e8</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0896627322008066">https://www.sciencedirect.com/science/article/pii/S0896627322008066</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2022.09.001</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Kerstjens</surname> <given-names>S</given-names></string-name>, <string-name><surname>Engert</surname> <given-names>F</given-names></string-name>, <string-name><surname>Zador</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Douglas</surname> <given-names>RJ</given-names></string-name></person-group>. <article-title>Eigengene reveals invariant global spatial patterns across mouse and fish brain development</article-title>. <source>bioRxiv</source>. <year>2024</year>; <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2024/09/20/2024.09.19.613507">https://www.biorxiv.org/content/early/2024/09/20/2024.09.19.613507</ext-link>, doi: <pub-id pub-id-type="doi">10.1101/2024.09.19.613507</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kerstjens</surname> <given-names>S</given-names></string-name>, <string-name><surname>Michel</surname> <given-names>G</given-names></string-name>, <string-name><surname>Douglas</surname> <given-names>RJ</given-names></string-name></person-group>. <article-title>Constructive connectomics: How neuronal axons get from here to there using gene-expression maps derived from their family trees</article-title>. <source>PLOS Computational Biology</source>. <year>2022</year> <month>Aug</month>; <volume>18</volume>(<issue>8</issue>):<elocation-id>e1010382</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://dx.plos.org/10.1371/journal.pcbi.1010382">https://dx.plos.org/10.1371/journal.pcbi.1010382</ext-link>, doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1010382</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname> <given-names>B</given-names></string-name>, <string-name><surname>Emmons</surname> <given-names>SW</given-names></string-name></person-group>. <article-title>Multiple conserved cell adhesion protein interactions mediate neural wiring of a sensory circuit in <italic>C. elegans</italic></article-title>. <source>eLife</source>. <year>2017</year> <month>sep</month>; <volume>6</volume>:<elocation-id>e29257</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.29257</pub-id>, doi: <pub-id pub-id-type="doi">10.7554/eLife.29257</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Kingma</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Ba</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Adam: A Method for Stochastic Optimization</article-title>. <conf-name>3rd International Conference on Learning Representations, ICLR 2015</conf-name>, <conf-loc>San Diego, CA, USA</conf-loc>, <conf-date>May 7-9, 2015</conf-date>; <year>2015</year>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Koulakov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shuvaev</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lachi</surname> <given-names>D</given-names></string-name>, <string-name><surname>Zador</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Encoding innate ability through a genomic bottleneck</article-title>. <source>bioRxiv</source>. <year>2022</year>; <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2022/05/26/2021.03.16.435261">https://www.biorxiv.org/content/early/2022/05/26/2021.03.16.435261</ext-link>, doi: <pub-id pub-id-type="doi">10.1101/2021.03.16.435261</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kov√°cs</surname> <given-names>IA</given-names></string-name>, <string-name><surname>Barab√°si</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Barab√°si</surname> <given-names>AL</given-names></string-name></person-group>. <article-title>Uncovering the genetic blueprint of the &lt;i&gt;C. elegans&lt;/i&gt; nervous system</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2020</year>; <volume>117</volume>(<issue>52</issue>):<fpage>33570</fpage>‚Äì<lpage>33577</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/abs/10.1073/pnas.2009093117">https://www.pnas.org/doi/abs/10.1073/pnas.2009093117</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.2009093117</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mnih</surname> <given-names>V</given-names></string-name>, <string-name><surname>Kavukcuoglu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Silver</surname> <given-names>D</given-names></string-name>, <string-name><surname>Rusu</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Veness</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bellemare</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Graves</surname> <given-names>A</given-names></string-name>, <string-name><surname>Riedmiller</surname> <given-names>M</given-names></string-name>, <string-name><surname>Fidjeland</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Ostrovski</surname> <given-names>G</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Beattie</surname> <given-names>C</given-names></string-name>, <string-name><surname>Sadik</surname> <given-names>A</given-names></string-name>, <string-name><surname>Antonoglou</surname> <given-names>I</given-names></string-name>, <string-name><surname>King</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kumaran</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wierstra</surname> <given-names>D</given-names></string-name>, <string-name><surname>Legg</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hassabis</surname> <given-names>D</given-names></string-name></person-group>. <article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source>. <year>2015</year> <month>Feb</month>; <volume>518</volume>(<issue>7540</issue>):<fpage>529</fpage>‚Äì<lpage>533</lpage>. <pub-id pub-id-type="doi">10.1038/nature14236</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/nature14236</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Napper</surname> <given-names>RMA</given-names></string-name>, <string-name><surname>Harvey</surname> <given-names>RJ</given-names></string-name></person-group>. <article-title>Number of parallel fiber synapses on an individual Purkinje cell in the cerebellum of the rat</article-title>. <source>Journal of Comparative Neurology</source>. <year>1988</year> <month>Aug</month>; <volume>274</volume>(<issue>2</issue>):<fpage>168</fpage>‚Äì<lpage>177</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/10.1002/cne.902740204">https://onlinelibrary.wiley.com/doi/10.1002/cne.902740204</ext-link>, doi: <pub-id pub-id-type="doi">10.1002/cne.902740204</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nishikawa</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hirashima</surname> <given-names>N</given-names></string-name>, <string-name><surname>Tanaka</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Optimization of Single-Cell Electroporation Protocol for Forced Gene Expression in Primary Neuronal Cultures</article-title>. <source>Molecular Biotechnology</source>. <year>2014</year> <month>Sep</month>; <volume>56</volume>(<issue>9</issue>):<fpage>824</fpage>‚Äì<lpage>832</lpage>. <pub-id pub-id-type="doi">10.1007/s12033-014-9761-1</pub-id>, doi: <pub-id pub-id-type="doi">10.1007/s12033-014-9761-1</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Schaul</surname> <given-names>T</given-names></string-name></person-group>. <article-title>Benchmarking separable natural evolution strategies on the noiseless and noisy black-box optimization testbeds</article-title>. In: <conf-name>Proceedings of the 14th Annual Conference Companion on Genetic and Evolutionary Computation GECCO ‚Äò12</conf-name>, <conf-loc>New York, NY, USA</conf-loc>: <conf-sponsor>Association for Computing Machinery</conf-sponsor>; <year>2012</year>. p. <fpage>205</fpage>‚Äì<lpage>212</lpage>. <pub-id pub-id-type="doi">10.1145/2330784.2330815</pub-id>, doi: <pub-id pub-id-type="doi">10.1145/2330784.2330815</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>St√∂ckl</surname> <given-names>C</given-names></string-name>, <string-name><surname>Lang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Maass</surname> <given-names>W</given-names></string-name></person-group>. <article-title>Structure induces computational function in networks with diverse types of spiking neurons</article-title>. <source>bioRxiv</source>. <year>2022</year>; <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2022/07/06/2021.05.18.444689">https://www.biorxiv.org/content/early/2022/07/06/2021.05.18.444689</ext-link>, doi: <pub-id pub-id-type="doi">10.1101/2021.05.18.444689</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taylor</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Santpere</surname> <given-names>G</given-names></string-name>, <string-name><surname>Weinreb</surname> <given-names>A</given-names></string-name>, <string-name><surname>Barrett</surname> <given-names>A</given-names></string-name>, <string-name><surname>Reilly</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Varol</surname> <given-names>E</given-names></string-name>, <string-name><surname>Oikonomou</surname> <given-names>P</given-names></string-name>, <string-name><surname>Glenwinkel</surname> <given-names>L</given-names></string-name>, <string-name><surname>McWhirter</surname> <given-names>R</given-names></string-name>, <string-name><surname>Poff</surname> <given-names>A</given-names></string-name>, <string-name><surname>Basavaraju</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rafi</surname> <given-names>I</given-names></string-name>, <string-name><surname>Yemini</surname> <given-names>E</given-names></string-name>, <string-name><surname>Cook</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Abrams</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vidal</surname> <given-names>B</given-names></string-name>, <string-name><surname>Cros</surname> <given-names>C</given-names></string-name>, <string-name><surname>Tavazoie</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sestan</surname> <given-names>N</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Molecular topography of an entire nervous system</article-title>. <source>Cell</source>. <year>2021</year>; <volume>184</volume>(<issue>16</issue>):<elocation-id>4329‚Äì4347.e23</elocation-id>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0092867421007583">https://www.sciencedirect.com/science/article/pii/S0092867421007583</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cell.2021.06.023</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Witvliet</surname> <given-names>D</given-names></string-name>, <string-name><surname>Mulcahy</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mitchell</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Meirovitch</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Berger</surname> <given-names>DR</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Koh</surname> <given-names>WX</given-names></string-name>, <string-name><surname>Parvathala</surname> <given-names>R</given-names></string-name>, <string-name><surname>Holmyard</surname> <given-names>D</given-names></string-name>, <string-name><surname>Schalek</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Shavit</surname> <given-names>N</given-names></string-name>, <string-name><surname>Chisholm</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Lichtman</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Samuel</surname> <given-names>ADT</given-names></string-name>, <string-name><surname>Zhen</surname> <given-names>M</given-names></string-name></person-group>, <article-title>Connectomes across development reveal principles of brain maturation</article-title>; <year>2020</year>. <ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/lookup/doi/10.1101/2020.04.30.066209">http://biorxiv.org/lookup/doi/10.1101/2020.04.30.066209</ext-link>, doi: <pub-id pub-id-type="doi">10.1101/2020.04.30.066209</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zador</surname> <given-names>AM</given-names></string-name></person-group>. <article-title>A critique of pure learning and what artificial neural networks can learn from animal brains</article-title>. <source>Nature Communications</source>. <year>2019</year> <month>Aug</month>; <volume>10</volume>(<issue>1</issue>):<fpage>3770</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-019-11786-6">https://www.nature.com/articles/s41467-019-11786-6</ext-link>, doi: <pub-id pub-id-type="doi">10.1038/s41467-019-11786-6</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Zhang</surname> <given-names>J</given-names></string-name></person-group>, <article-title>Basic Neural Units of the Brain: Neurons, Synapses and Action Potential</article-title>; <year>2019</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.01703">https://arxiv.org/abs/1906.01703</ext-link>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109166.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Barab√°si</surname>
<given-names>D√°niel</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Harvard University</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents SynaptoGen, a differentiable extension of connectome models that links gene expression, protein-protein interaction probabilities, synaptic multiplicity, and synaptic weights, and demonstrates its use in reinforcement learning agents and a C. elegans-inspired case study. The work is a <bold>valuable</bold> contribution to computational connectomics and neuro-inspired machine learning, with <bold>solid</bold> mathematical and computational evidence supporting the proposed optimization framework. However, the broader biological and synthetic-biology claims - particularly genomic control of synaptogenesis and drug-discovery applications - are currently overstated and would benefit from a more tempered framing and clearer articulation of biological limitations.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109166.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors address a set of important and challenging questions at the interface of (developmental) neuroscience, genetics, and computation. They ask how complex neural circuits could emerge from compact genomic information, and they outline a bold vision in which this process might eventually be harnessed to design synthetic biological intelligence through genetic control of synaptogenesis. These are significant and stimulating ideas that merit rigorous theoretical and experimental exploration.</p>
<p>However, the present work does not convincingly engage with these questions at a mechanistic level. Most of the circuit formation aspects appear to be adopted from prior models, and it is not clear how the main methodological modifications-introducing synaptic conductance and stochastic formalisms-provide new conceptual insight into genomic specification of neural circuitry. The manuscript does not include significant biological data or validation to support the proposed framework, and the results provided instead use artificial reinforcement learning benchmarks, which do not appear informative with respect to the biological claims.</p>
<p>Overall, while the manuscript raises intriguing themes and ambitions, the proposed model is conceptually disconnected from the biological problem it purports to address. The strength of evidence does not support the strong interpretative or translational claims, and substantial rethinking of the modeling framework, in particular its validation strategy, would be required for the work to match the claims of our improved understanding of the genomic basis of neural circuit formation and our ability to engineer it.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109166.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this manuscript, the authors built upon the Connectome Model literature and proposed SynaptoGen, a differentiable model that explicitly takes into account multiplicity and conductance in neural connectivity. The authors evaluated SynaptoGen through simulated reinforcement learning tasks and established its performance as often superior to two considered baselines. This work is a valuable addition to the field, supported by a solid methodology with some details and limitations missing.</p>
<p>Major points:</p>
<p>(1) The genetic features in the X and Y matrices in the CM were originally introduced as combinatorial gene expression patterns that correspond to the presence and even absence of a subset of genes. The authors oversimplify this original scope by only considering single-gene expression features. While this was arguably a reasonable first approximation for a case study of gap junctions in C. elegans, it is by no means expected to be a plausible expectation for chemical synapses. As the authors appear to motivate their model by chemical synapses that have polarities, they should either consider combinatorial rules in the model or at least present this explicitly as a key limitation of the model. Omitting combinatorial effects also renders the presented &quot;bioplausible&quot; baseline much less bioplausible, likely calling for a different name.</p>
<p>(2) It is not fully explained how Equation (11) is obtained, even conceptually. It is unclear why \bar{B} and \bar{G} should be element-wise multiplied together, both already being expected values. Moreover, the authors acknowledged in lines 147-149 that the components of \bar{G} actually depend on gene expression X, which is a component in \bar{B}, so the logic here seems circular.</p>
<p>(3) The authors considered two baselines, namely SNES and a bioplausible control. However, it would be of interest to also investigate:
a) Vanilla DQN with the same size trained on the same MLP, to judge whether the biological insights behind SynaptoGen parameterization add value to performance.
b) Using Equation (7) instead of Equation (11) to construct the weight matrices, to judge whether incorporating the conductance adds value to performance.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109166.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>Boccato et al. present an ambitious and thoughtfully developed framework, SynaptoGen, which proposes a differentiable model of synaptogenesis grounded in gene-expression vectors, protein interaction probabilities, and conductance rules. The authors aim to bridge the gap between computational connectomics and synthetic biological intelligence by enabling gradient-based optimization of genetically encoded circuit architectures. They support this goal with mathematical derivations, simulation experiments across several RL benchmarks, and a biologically grounded validation using C. elegans adhesion-molecule co-expression data. The paper is timely and conceptually compelling, offering a unified formulation of synaptic multiplicity and synaptic weight formation that can be integrated directly into learning systems.</p>
<p>Strengths</p>
<p>(1) Well-motivated framework with clear conceptual contributions.</p>
<p>(2) Rigorous mathematical development.</p>
<p>(3) Compelling empirical validation.</p>
<p>(4) Excellent framing and discussion of future impact.</p>
<p>Weaknesses</p>
<p>(1) Overstated claims in the abstract and discussion.</p>
<p>(2) Ambiguity in &quot;first of its kind&quot; assertions.</p>
</body>
</sub-article>
</article>