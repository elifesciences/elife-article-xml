<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">96566</article-id>
<article-id pub-id-type="doi">10.7554/eLife.96566</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.96566.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Homeostatic synaptic normalization optimizes learning in network models of neural population codes</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-2237-3880</contrib-id>
<name>
<surname>Mayzel</surname>
<given-names>Jonathan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8653-9848</contrib-id>
<name>
<surname>Schneidman</surname>
<given-names>Elad</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Brain Sciences, Weizmann Institute of Science</institution>, Rehovot 76100, <country>Israel</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Sharpee</surname>
<given-names>Tatyana O</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Salk Institute for Biological Studies</institution>
</institution-wrap>
<city>La Jolla</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence: <email>elad.schneidman@weizmann.ac.il</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-07-01">
<day>01</day>
<month>07</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP96566</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-03-11">
<day>11</day>
<month>03</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-12-18">
<day>18</day>
<month>12</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.05.530392"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Mayzel &amp; Schneidman</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Mayzel &amp; Schneidman</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-96566-v1.pdf"/>
<abstract>
<p>Studying and understanding the code of large neural populations hinge on accurate statistical models of population activity. A novel class of models, based on learning to weigh sparse nonlinear Random Projections (RP) of the population, has demonstrated high accuracy, efficiency, and scalability. Importantly, these RP models have a clear and biologically-plausible implementation as shallow neural networks. We present a new class of RP models that are learned by optimizing the randomly selected sparse projections themselves. This “reshaping” of projections is akin to changing synaptic connections in just one layer of the corresponding neural circuit model. We show that Reshaped RP models are more accurate and efficient than the standard RP models in recapitulating the code of tens of cortical neurons from behaving monkeys. Incorporating more biological features and utilizing synaptic normalization in the learning process, results in even more efficient and accurate models. Remarkably, these models exhibit homeostasis in firing rates and total synaptic weights of projection neurons. We further show that these sparse homeostatic reshaped RP models outperform fully connected neural network models. Thus, our new scalable, efficient, and highly accurate population code models are not only biologically-plausible but are actually optimized due to their biological features. These findings suggest a dual functional role of synaptic normalization in neural circuits: maintaining spiking and synaptic homeostasis while concurrently optimizing network performance and efficiency in encoding information and learning.</p>
</abstract>
<kwd-group kwd-group-type="author">
<kwd>Population Codes</kwd>
<kwd>Neural Networks</kwd>
<kwd>Homeostatic Plasticity</kwd>
<kwd>Random Projections</kwd>
<kwd>Efficient Coding</kwd>
<kwd>Learning</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Additional analyses, text and figure changes.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The potential “vocabulary” of spiking patterns of a population of neurons scales exponentially with the size of the population, and so, the mapping the rules of neural population codes and their semantic organization, cannot rely on direct sampling of the vocabulary for more than a handful of neurons. Moreover, the stochastic nature of neural activity implies that the characterization of neural codes must rely on probability distributions over population activity patterns. Therefore, to describe and analyze the structure and content of the code with which neural circuits respond to stimuli, process information, and direct action – we must learn statistical models of their activity. Such models have been used to study neural population codes in different systems: Models of the directional coupling between neurons, such as Generalized Linear Models, have been used to replicate the stimulusdependent rates of populations of tens of neurons (<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c4">4</xref>). Maximum entropy models have accurately captured the joint activity patterns of more than 100 neurons, using simple statistical features of the population, like firing rates, pairwise correlations, synchrony, and other low-order statistics (5– 13). These models have further been used to characterize the semantic organization of population codes (<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>). Autoencoder models have been employed to replicate the detailed structure of population activity – yielding generative models that can be used to study the code, but their design is difficult to interpret (<xref ref-type="bibr" rid="c16">16</xref>–<xref ref-type="bibr" rid="c18">18</xref>). Importantly, scaling of these models to hundreds of neurons is computationally challenging (<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>), which has been a major challenge in modeling large neural systems.</p>
<p>While statistical models are invaluable for describing and studying neural codes, it is not clear whether the brain relies on such models or implements them when representing or processing information (<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>). Consequently, much of the analysis of neural codes has focused on decoding population activity, typically using simple decoders (2, 23– 27), or metrics over the structure of population activity patterns (<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>). Yet, if neural circuits do implement such statistical models, and in particular, ones that compute the likelihood of their inputs – this would present a realizable mechanism for real neural circuits to carry Bayesian computation and decision making (<xref ref-type="bibr" rid="c30">30</xref>–<xref ref-type="bibr" rid="c32">32</xref>). Such network models are, therefore, of interest not only as a way to study neural codes, but also as a potential way for biological neural networks to implement efficient learning and overcome the credit assignment problem. In addition, they may be useful for improving learning in artificial neural networks using biological features (<xref ref-type="bibr" rid="c33">33</xref>–<xref ref-type="bibr" rid="c38">38</xref>).</p>
<p>Both structured architectural features of neural circuits and random connectivity patterns have been suggested to shape the computation carried out by neural circuits (<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c39">39</xref>–<xref ref-type="bibr" rid="c43">43</xref>). These computations rely on the nature of synaptic connectivity and the coupling between synapses in terms of how they change during learning. Competition mechanisms between synapses or other regularization mechanisms have also been suggested to be important components of computation and learning in artificial neural networks as well as in cortical circuits (<xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>). One such mechanism is the homeostatic scaling of synaptic plasticity, which has been observed <italic>in vitro</italic> and <italic>in vivo</italic> at the level of incoming synapses to a neuron and outgoing ones (<xref ref-type="bibr" rid="c46">46</xref>–<xref ref-type="bibr" rid="c49">49</xref>). This mechanism has been commonly attributed to the regulation of firing rates, while its computational implications remain mostly unclear, but of interest computationally and mechanistically (<xref ref-type="bibr" rid="c50">50</xref>–<xref ref-type="bibr" rid="c54">54</xref>). A related computational feature has been presented by network models that include divisive normalization, suggested as an important component of computations performed by cortical circuits (<xref ref-type="bibr" rid="c55">55</xref>).</p>
<p>Here, we bring these ideas together to present a biologicallyinspired variant of a new family of statistical models for large neural population codes. Adding biological features to these population models enabled us to improve the models, and to explore designs that real neural circuits could employ to implement such models. Specifically, we expand the Random Projections (RP) model (<xref ref-type="bibr" rid="c30">30</xref>), which was shown to be highly accurate in recapitulating the detailed spiking patterns of more than 100 neurons in different neural systems. Importantly, in addition to being accurate and requiring little amounts of training data, these RP models can be readily implemented by a simple neural circuit model – suggesting how real neural circuits can learn a statistical model of their own inputs and compute the likelihood of the inputs. We show that we can make these models better by “reshaping” the randomly chosen sparse non-linear projections that they rely on, achieving highly accurate models using significantly fewer projections. We further show that reshaping of projections that incorporates normalization of synaptic weights during learning, results in more accurate models that are also more efficient, and makes the models homeostatic in terms of neural activity and total synaptic weights. Thus, we present a new class of accurate and efficient statistical models for large neural population codes that also suggests a clear computational benefit of homeostatic synaptic normalization and its potential role in biological neural networks and artificial ones.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>The Random Projections (RP) model is a class of highly accurate, scalabale, and efficient statistical models of the joint activity patterns of large populations of neurons (<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>). These models are based on random and sparse nonlinear functions, or “projections”, of the population: Given a recording of the spiking activity of a population of neuorns, the model is a probability distribution over discrete activity patterns (quantized into small time bins, e.g. 10-20 ms), that relies on a set of random non-linear functions of the population activity,
<disp-formula id="eqn1">
<graphic xlink:href="530392v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>a</italic><sub><italic>ij</italic></sub> are randomly sampled coefficients such that most of them for any <italic>i</italic> are zero (i.e., the set is sparse), <italic>θ</italic><sub><italic>j</italic></sub> are thresholds, and <italic>σ</italic>(<italic>·</italic>) are nonlinear functions, (e.g., the Heaviside step function). The RP model is the maximum entropy distribution <inline-formula><inline-graphic xlink:href="530392v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref ref-type="bibr" rid="c56">56</xref>), which is consistent with the observed average values of the random projections <italic>⟨f</italic><sub><italic>i</italic></sub><italic>⟩</italic><sub><italic>p</italic></sub> = <italic>⟨f</italic><sub><italic>i</italic></sub><italic>⟩</italic><sub><italic>data</italic></sub> (See Methods). Thus, it is the least structured distribution that retains the average values of the projections, is mathematically unique, and is given by
<disp-formula id="eqn2">
<graphic xlink:href="530392v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>λ</italic><sub><italic>i</italic></sub> are Lagrange multipliers, and <italic>Z</italic> is a normalization factor or the “partition function”, which can be found numerically. Applied to cortical data from multiple areas (see, e.g., <xref rid="fig1" ref-type="fig">Figure 1A</xref>), this model proved to be highly accurate in predicting individual activity patterns, using small amounts of training data (<xref ref-type="bibr" rid="c30">30</xref>). Importantly, unlike many other statistical models of population activity, RP models have a simple, biologically plausible neural circuit that can implement them (<xref ref-type="bibr" rid="c30">30</xref>): <xref rid="fig1" ref-type="fig">Figure 1B</xref> shows such a feed-forward circuit with one intermediate layer and an output neuron, where the random coefficients of the sparse projections, <italic>a</italic><sub><italic>ij</italic></sub>, are the synaptic weights connecting the input neurons <inline-formula><inline-graphic xlink:href="530392v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to an intermediate layer of neurons <italic>f</italic><sub><italic>i</italic></sub>. Each intermediate neuron implements one projection of the input population. The Lagrange multipliers, <italic>λ</italic><sub><italic>i</italic></sub>, are the synaptic weights connecting the intermediate layer to the output neuron, whose membrane potential or output gives the log-likelihood of the activity pattern of <italic>_x</italic>, up to a normalization factor.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>Reshaped Random Projections models outperform Random Projections models.</title>
<p>(A) A short segment of the spiking activity of 100 cortical neurons used for the analysis and comparison of different statistical models of population activity (see Methods). (B) Schematics of the neural circuits that implement the different random projections models we compared: The “standard” Random Projections model, where the coefficients <italic>a</italic><sub><italic>ij</italic></sub> that define the projections are randomly selected and fixed whereas the factors <italic>λ</italic><sub><italic>i</italic></sub> are learned (see text). The Reshaped Random Projections model, where the coefficients <italic>a</italic><sub><italic>ij</italic></sub> that define the projections are tuned and the factors <italic>λ</italic><sub><italic>i</italic></sub>s are fixed. The backpropagation model, where we tune both the <italic>a</italic><sub><italic>ij</italic></sub> s and <italic>λ</italic><sub><italic>i</italic></sub>s. (C) The predicted probability of individual populations’ activity patterns as a function of the observed probability for RP, Reshaped and Backpropagation models. Gray funnels denote 99% confidence interval. (D) Average performance of models of the three classes is shown as a function of the number of projections, measured by log-likelihood, over 100 sets of randomly selected groups of 50 neurons. Reshaped models outperform RP models and are on par with backpropagation; the shaded area denotes the standard error over 100 models. (E-F) Mean firing rates of projection neurons and mean correlation between projections. Reshaped models show lower correlations and lower firing rates compared to RP and backpropagation models. Standard errors are smaller than marker’s size, hence invisible.</p></caption>
<graphic xlink:href="530392v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The model in <xref ref-type="disp-formula" rid="eqn2">eq. 2</xref> harbors a duality between the projections, <italic>f</italic><sub><italic>i</italic></sub>, and their coefficients, <italic>λ</italic><sub><italic>i</italic></sub>: In the maximum entropy formalism of the model, the projections are randomly sampled and then fixed, and their corresponding weights, <italic>λ</italic><sub><italic>i</italic></sub>’s, are tuned to maximize the entropy and satisfy the constraints. Alternatively, we may consider the case of training the model by keeping the <italic>λ</italic><sub><italic>i</italic></sub>’s fixed and changing or tuning the projections <italic>f</italic><sub><italic>i</italic></sub> to maximize the likelihood. In the corresponding neural circuit, this would imply that we would learn a circuit that implements the statistical model by training the sparse set of synaptic connections, <italic>a</italic><sub><italic>ij</italic></sub>, which define the projections, instead of training the synapses that weigh the projections, <italic>λ</italic><sub><italic>i</italic></sub> (<xref rid="fig1" ref-type="fig">Figure 1B</xref>).</p>
<p>Notably, a variant of the RP model in which projections that were weighted by a low value of <italic>λ</italic><sub><italic>i</italic></sub> are pruned and replaced with new random projections proved to be more accurate than the original RP model, while using fewer projections (<xref ref-type="bibr" rid="c30">30</xref>). This procedure of pruning and replacement is a crude form of learning of the model through changing the projections, and finding more efficient ones. We, therefore, asked here whether instead of the heuristic pruning and replacement, we can directly learn more accurate and efficient models by tuning the projections.</p>
<sec id="s2a">
<title>Reshaping random projections gives more accurate and compact models</title>
<p>We first learned a new class of RP models for populations of tens of cortical neurons from the prefrontal cortex of monkeys performing a visual classification task (<xref ref-type="bibr" rid="c57">57</xref>) by tuning their randomly selected projections. Specifically, given an initial draw of sparse projections, the random weights that define the projections, <italic>a</italic><sub><italic>ij</italic></sub>, are then changed to maximize the likelihood of the model:
<disp-formula id="eqn3">
<graphic xlink:href="530392v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>η</italic> is the learning rate. We note that unlike the RP model presented in (<xref ref-type="bibr" rid="c30">30</xref>), here we used a sigmoid function for the nonlinearity of the projections,
<disp-formula id="eqn4">
<graphic xlink:href="530392v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>β</italic> sets the slope of the sigmoid. In this formulation, the model ranges from an independent model of the popula-tion for <italic>β →</italic> 0, to the original RP model of (<xref ref-type="bibr" rid="c30">30</xref>) for <italic>β → ∞</italic>. The rule for changing the projections (<xref ref-type="disp-formula" rid="eqn3">eq. 3</xref>) means that the specific set of inputs to each projection neuron is retained, but their relative weights are changed, and so the projections are “reshaped”. We focus henceforth on the case of all <italic>λ</italic><sub><italic>i</italic></sub> = 1, and so the Reshaped Random Projections (Reshaped RP) model, is given by
<disp-formula id="eqn5">
<graphic xlink:href="530392v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We compared the RP and the Reshaped RP models by quantifying their performance on the same set of initial projections. We first learned the RP model as in (<xref ref-type="bibr" rid="c30">30</xref>), using a Heaviside non-linearity for the projections, and RP models that used a sigmoid non-linearity, where both models used the same set of random projections, and found the latter models to be be more accurate (see <xref rid="figS1" ref-type="fig">supp. Figure 1</xref>). We then learned Reshaped RP models in which we optimize the same initial projections while keeping <italic>λ</italic><sub><italic>i</italic></sub> = 1. We note that while in its maximum entropy formulation, the RP model is the unique so-lution to a convex optimization problem, the Reshaped RP models are not guaranteed to reach a global optimum. We also considered yet another class of models, in which the projections and the Lagrange multipliers <italic>λ</italic><sub><italic>i</italic></sub> are optimized simultaneously, similar to backpropagation-based learning used to train feed-forward neural networks (see Methods). <xref rid="fig1" ref-type="fig">Figure 1C</xref> shows an example of the accuracy of the sigmoid RP models, Reshaped Random Projections models, and backpropagation-based models in predicting the probability of individual activity patterns for one group of 20 neurons, recorded from the cortex of behaving monkeys (<xref ref-type="bibr" rid="c57">57</xref>). The activity patterns are predicted by the reshaped RP model to an accuracy that is within the sampling noise (denoted by the 99% confidence interval funnel), and is similar to the performance of the full backpropagation model. The standard RP model, in comparison, has many more patterns that are outside the 99% confidence interval funnel. We quantified the performance of the three classes of models by calculating the mean log-likelihood of the models over 100 groups of 50 neurons on held out datasets, as a function of the number of projections that we used (<xref rid="fig1" ref-type="fig">Fig. 1D</xref>). The reshaped models outperform the RP ones for a low number of projections, whereas the performances of all three models converge to a similar value for large number of projections.</p>
<p>To compare the “mechanistic” nature of these different models, we calculated the mean correlation between the projections within each model class, and the average values of each projection (where the average is over the population activity patterns), which correspond to the mean firing rates of the neurons in the intermediate layer. Interestingly, the firing rates of the neurons in the intermediate layer are considerably lower for the reshaped models, and this sparseness in activity becomes more pronounced as a function of the number of projections (<xref rid="fig1" ref-type="fig">Figure 1E</xref>). We further find that the correlations between the projections in the reshaped models are considerably lower compared to RP and backpropagation models (<xref rid="fig1" ref-type="fig">Figure 1F</xref>).</p>
<p>Thus, the reshaped projection models suggest a way to learn more accurate models of population activity, by tuning of projections. These models are also more efficient, requiring fewer projections. These projections also have lower firing rates (i.e., reshaped projections use fewer spikes), and they are less correlated. Given their accuracy and efficiency, we next asked how adding biological features or constraints to a Reshaped RP circuit may affect its performance and efficiency.</p>
</sec>
<sec id="s2b">
<title>Normalized reshaping of random projections gives more accurate and efficient models</title>
<p>We studied the effect of adding two classes of biological features or constraints on the performance and nature of the Reshaped RP circuit model. The first constraint stems from the biophysical limits on individual synapses, and so we bound the maximal strength of individual synapses such that the strength of all synaptic weights are smaller than a “ceiling” value: |<italic>a</italic><sub><italic>ij</italic></sub>| <italic>&lt; θ</italic>. The other is a normalization of the synaptic weights during the reshaping, inspired by the synaptic re-scaling that has been observed experimentally (<xref ref-type="bibr" rid="c49">49</xref>), and divisive normalization of synaptic weights (<xref ref-type="bibr" rid="c44">44</xref>).We consider multiple mechanisms of this kind later, but begin here with fixing the total sum of the incoming synaptic strength of each projection such that Σ <italic>j</italic> |<italic>a</italic><sub><italic>ij</italic></sub>| = <italic>ϕ</italic>. Thus, when the strength of one synapse increases (decreases), the strength of the rest of the incoming synapses decreases (increases) such that the total synaptic weight incoming into the projection is kept constant. We term this constraint “homeostatic synaptic normalization”. We emphasize that the notion of homeostatic mechanisms is commonly reserved for designating regulation processes that retain a functional property of neurons, whereas normalization of synaptic weights might seem more mechanistic than functional. But, as we show later, learning with synaptic normalization also regulates the firing rate of the projection neurons, and so, we use this name henceforth.</p>
<p>To compare the effect of these constraints, we used the same set of initial random projections, and then learn by reshaping them, each time with a different value of their corresponding parameters, <italic>ϕ</italic> or <italic>θ</italic>. We estimated the likelihood of each of the models on 100 groups of 50 neurons, over 100 random sets of 150 projections. To quantify the “synaptic budget” of each model, we measured the total sum of the absolute values of synaptic weights available to each model in units of the total synaptic strength of the initial set of projections (this is equivalent to defining the total sum of the synaptic weights of the initial set of projections as “1”, and then measuring total synaptic weights in these units). For the models with bounded synapses, the total available synaptic budget is given by the number of synapses times <italic>θ</italic>, whereas for the homeostatic constraint, it equals <italic>ϕ</italic> times the number of pro-jections in the model. <xref rid="fig2" ref-type="fig">Figure 2B</xref> shows the log-likelihood of each model class vs. the total available synaptic budget of the different models: For a wide range of synaptic budgets, the homeostatic models outperform the bounded models, and only for very high values of available synaptic budget, the performance of the bounded models is on par with the homeostatic models.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2.</label>
<caption><title>Reshaped RP models that use homeostatic synaptic normalization outperform RP models and bounded RP models.</title>
<p>(A) Schematic drawing of the different models we studied: standard RP model, unconstrained reshaped model, and two types of constrained reshaped models: Bounded models in which each synapse separately obeys Σ <sub><italic>ij</italic></sub> |<italic>a</italic><sub><italic>ij</italic></sub> | <italic>&lt; θ</italic> during learning, and normalized input reshaped models, where we fix the total synaptic weight of incoming synapses Σ <sub><italic>ij</italic></sub> |<italic>a</italic><sub><italic>ij</italic></sub> | = <italic>ϕ</italic>. (B) The mean log-likelihood of the models is shown as a function of the total available budget. The normalized input reshaped RP models give optimal results for a wide range of values of available synaptic budget, outperforming the bounded models and the RP model. (C) The mean log-likelihood of the models is shown as a function of the total used budget. Aside from the bounded models, all other models are the same as in (B) by construction. For high available budget values, bounded models show better performance while utilizing a lower synaptic budget, similar to the unconstrained reshape model. (D) Comparison of the performance of 100 individual examples of each model class and their corresponding RP model, where all models relied on the same set of initial projections. Normalized input models outperformed the RP models in all cases (all points are above the diagonal), while all bounded models were worse (points below the diagonal). (E-F) The mean correlation and firing rates of projections as a function of the model’s cost. Normalized reshape models show low correlations and mean firing rates, similar to unconstrained reshaped models. Note that in panels B, C, E, and F, the standard errors are smaller than the marker size, and are therefore invisible.</p></caption>
<graphic xlink:href="530392v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The differences between the homeostatic normalization models and the bounded synaptic strength models are further reflected in <xref rid="fig2" ref-type="fig">Figure 2C</xref>, which shows the performance of each model class as a function of the total sum of synaptic weights that is used by that model at the end of the training Σ <italic>ij</italic> |<italic>a</italic><sub><italic>ij</italic></sub>|. We note that the curve of the homeostatic model is identi-cal to the one from <xref rid="fig2" ref-type="fig">Figure 2B</xref> by definition; the curve of the bounded models shows that at a certain value of <italic>θ</italic> the sum of the synaptic weights starts to decrease and converges to the unconstrained reshaped model. The poor performance of the bounded models compared to the homeostatic ones suggests that the coupled changes in the synaptic weights improve learning. Specifically, during reshaping, the homeostatic models move synaptic “mass” from less important synapses to more important ones. This redistribution of resources results in accurate models even for relatively low values of synaptic weights – making them more efficient in terms of the total synaptic weight needed.</p>
<p>The dominance of the homeostatic learning over the bounded synaptic weights is clear not just for the average over models, but also at the level of individual models: <xref rid="fig2" ref-type="fig">Figure 2D</xref> shows the performance of the homeostatic and bounded models that are initialized with the same set of random projections; all bounded constraint models are inferior to the corresponding RP ones, whereas all the homeostatic constraint models are superior to the RP models (and clearly all the homeostatic models are superior to the corresponding bounded models).</p>
<p>We further find that the mean firing rates of the reshaped projection neurons, as well as the correlations between them, are lower in the homeostatic models compared to the bounded models (<xref rid="fig2" ref-type="fig">Figure 2E-F</xref>), making them more energetically efficient (in terms of spiking activity). We recall that this is consistent with the notions of efficient coding by decorrelated neural populations (<xref ref-type="bibr" rid="c58">58</xref>, <xref ref-type="bibr" rid="c59">59</xref>).</p>
</sec>
<sec id="s2c">
<title>Normalized reshaping of random projections results in more efficient codes and homeostasis of firing rates</title>
<p>The experimental characterization of synaptic rescaling has shown it to be a homeostatic mechanism that regulates the firing rates of neurons (<xref ref-type="bibr" rid="c49">49</xref>). We therefore asked whether the synaptic normalization we employ for the Reshaped RP models has a similar effect. <xref rid="fig3" ref-type="fig">Figure 3A</xref> shows that the overall performance of the model in terms of capturing the population codebook is similar between the “free” reshape model and different values of synaptic normalization. Similarly, reshaping with normalization or without it drives the projection neurons to converge to similar average firing rate values (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). However, the distribution of firing rates over the different neurons becomes narrower with tighter normalization values (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). Importantly, while different normalization values imply very different initial firing rates of the projection neurons, after reshaping the values converge to similar average values (<xref rid="fig3" ref-type="fig">Figure 3D</xref>). Moreover, reshaping with normalization implies lower firing rates as well as smaller changes in the reshaping process (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). Thus, normalized reshaping results in homeostatic regulation of the firing rates, which validates the naming of these models as homeostatic normalization reshaping of random projections.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>Homeostatic models firing rates.</title>
<p>(A) Likelihood values of the different models we consider here. Error bars denote standard error over 100 different models. (B) Projections mean firing rates during reshape. Standard error is smaller than marker size, hence cannot be seen (C) A histogram of the projections firing rates of unconstrained models and different homeostatic input models. (D) Projections mean firing rates after reshaping vs. the projection initial firing rate, for different homeostatic models and unconstrained model. Error bars denote standard deviation over 100 different models. (E) The cumulative density function of the projections firing rate relative change during reshape, |<italic>FR</italic><sub><italic>final</italic></sub> <italic>− FR</italic><sub><italic>initial</italic></sub>| <italic>/F R</italic><sub><italic>initial</italic></sub>.</p></caption>
<graphic xlink:href="530392v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Having established the computational benefits and efficiency of the homeostatic reshaped projection models that rely on synaptic normalization, we turned to ask how the connectivity itself, rather than the synaptic weights, may affect the performance of the models.</p>
</sec>
<sec id="s2d">
<title>Optimal sparseness of Reshaped Projections models under homeostatic constraints</title>
<p>The benefits of reshaping a given set of projections, reflected in the figures above, raise the question of the importance of the nature of the random projections we choose (which are then reshaped). We, therefore, asked how the initial random “wiring” of the projections affects the performance of the model, and whether non-random projections would result in even better models. To quantify the effects of the projections’ connectivity on the performance and efficiency of reshaped models, we used simulated population activity that we generated using RP models that were trained on real data. By using synthetic data that was generated by a known model, we can compare the learned models to the “ground truth” in terms of connectivity, as well as extensively sampling of activity patterns from the model.</p>
<p>We learned homeostatic reshaped models for the synthetic data, using different initial connectivity structures (<xref rid="fig4" ref-type="fig">Figure 4A-B</xref>): (i) A “true” connectivity model in which we reshaped a random projections model that has the same connectivity as the projections of the model that generated the data. (ii) A Random connectivity model in which we reshaped projections with sparse and connectivity that is randomly sampled and is independent of the model that generated the synthetic data. (iii) A full connectivity model in which we reshaped random projections with full connectivity, i.e., all input neurons are connected to all the projections, but with random initial weights. We carried out homeostatic reshaping of the projections in all three models with different values of <italic>θ</italic>. Surprisingly, the true and random connectivity models performed very similarly (<xref rid="fig4" ref-type="fig">Figure 4C</xref>). Although the full connectivity model contains the “ground truth” connectivity, and could recreate the true connectivity by canceling out unnecessary synapses during reshaping – we find that the full connectivity models are inferior to the other models, except for the case of high model costs.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4.</label>
<caption><title>Models that rely on projections that use random connectivity show similar performance to models that use the correct connectivity.</title>
<p>(A) Synthetic population activity data is sampled from an RP model with known connectivity (i.e., the “ground truth” model; see Methods). (B) Homeostatic reshaped random projections models that differ in their connectivity are learned to fit the synthetic data. The “True connectivity” model uses projections whose connectivity is identical to the “ground truth” model. The “Random connectivity” model uses projections that are randomly sampled using sparse random connectivity. The “Full connectivity” model is a homeostatic reshaped model that uses projections with full connectivity. (C) The mean log-likelihood of the models is shown as a function of the model’s cost. The true connectivity model is only slightly better than the random connectivity model, with both outperforming the full connectivity model for low model budget values. (D) The mean correlation between the activity of the projection neurons, shown as a function of the model cost. We note that true and random connectivity models are indistinguishable. (E) The firing rates of the projection neurons, shown as a function of the model cost. (F) The performance of homeostatic reshaped RP models, shown as a function of their normalized in-degree of the projections (0–disconnected, 1–fully connected), for different normalization values, shown by the model’s synaptic cost. (G) The performance of the homeostatic reshaped RP models, shown as a function of the synaptic cost normalized by the in-degree of the projections. Curves of different cost values coincide, suggesting a fixed optimal cost/activity ratio. Note that in panels D-G the standard errors over 100 models are smaller than the size of markers and are, therefore, invisible.</p></caption>
<graphic xlink:href="530392v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The mean correlations between projections at the end of reshaping and the mean firing rates of the models that use the true and random connectivity were also very similar (<xref rid="fig4" ref-type="fig">Figure 4D-E</xref>), whereas the full connectivity models showed, again, very different behavior. These results reflect another computational benefit of homeostatic reshaping: there is no need to know the optimal circuit connectivity, and there is no apparent benefit to all-to-all connectivity, which would be expensive in terms of the energetic cost, the space needed, and the biological construction. Thus, starting from random connectivity and optimizing the circuit under homeostatic constraints seems to provide optimal results.</p>
<p>Given the inefficiency of the fully connected reshaped projections model, we also quantified the effect of the sparseness of the projections on reshaped RP models. We recall that for the standard RP model, sparse projections were optimal for a wide range of network sizes (<xref ref-type="bibr" rid="c30">30</xref>)), and so we measured the performance of homeostatic reshaped RP models for different values of in-degree of the projections, while keeping the total synaptic budget of the models fixed. We found that different synaptic budgets have a different optimal in-degree (<xref rid="fig4" ref-type="fig">Figure 4F</xref>), and that the value of the optimal in-degree seems to grow with the total synaptic budget.</p>
<p>We further estimated the efficiency of the models by the synaptic cost per connection in the projections (<xref rid="fig4" ref-type="fig">Figure 4G</xref>). We find that curves for different total synaptic costs seem to coincide and have a similar peak value – suggesting an optimal ratio between the total available resources and the number of synapses.</p>
</sec>
<sec id="s2e">
<title>Different homeostatic mechanisms for reshaping random projections models result in different projection sets</title>
<p>We explored two other forms of synaptic normalization rules for the reshaping of projections (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). In the first, we fixed and normalized the outgoing synapses from each neuron, such that Σ <italic>i</italic> |<italic>a</italic><sub><italic>ij</italic></sub>| = <italic>ϕ</italic>. In the second, we kept the total synaptic weight of the whole circuit fixed, namely, Σ <sub><italic>ij</italic></sub> |<italic>a</italic><sub><italic>ij</italic></sub>| = <italic>ϕ</italic>. <xref rid="fig5" ref-type="fig">Figure 5B</xref> shows that the performance of the models that use these other homeostatic mechanisms is surprisingly similar in terms of the model’s likelihood over the test data, as well as the firing rates of the projection neurons (Supp. Figure 2A), and correlations between them (<xref rid="figS2" ref-type="fig">Supp. Figure 2B</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5.</label>
<caption><title>A variety of homeostatic synaptic normalization mechanisms produce similar model behavior using a wide range of random projection parameters.</title>
<p>(A) Schematic drawings of the different homeostatic models we compared: Homeostatic input models in which we fixed the total synaptic weight of the incoming synapses Σ<sub><italic>j</italic></sub> |<italic>a</italic><sub><italic>ij</italic></sub> | = <italic>ϕ</italic>; Homeostatic output models in which we fixed the total synaptic weight of the outgoing synapses Σ<sub><italic>i</italic></sub> |<italic>a</italic><sub><italic>ij</italic></sub> | = <italic>ϕ</italic>; and Homeostatic circuit models in which we fixed the total synaptic weight of the whole synaptic circuit Σ <sub><italic>ij</italic></sub> |<italic>a</italic><sub><italic>ij</italic></sub> | = <italic>ϕ</italic>. (B) The mean log-likelihood of models, shown as a function of the total used synaptic cost. All three homeostatic model variants show similar behavior. (C) Schematic drawing of how projections rotate during reshaping: starting from the initial projections (grey lines), they rotate to their reshaped orientation (black lines) by angle <italic>θ</italic><sub><italic>i</italic></sub>. (D) Rotation angles after reshaping, shown for different pairs of models. All four panels show models that initialized with the same set of projections. The different labels specify the constraint type and strength, namely, the specific value of <italic>ϕ</italic> and <italic>θ</italic>. (E) The mean rotation angle of the projections due to reshaping, shown as a function of the model synaptic budget.</p></caption>
<graphic xlink:href="530392v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As the homeostatic reshaping of random projections proved to be similarly accurate and efficient for the three homeostatic model variants, we asked which features of normalized reshaping might differentiate between homeostatic models in terms of their performance. Since each projection defines a hyperplane in the space of population activity patterns, reshaping can be interpreted as a rotation or a change of the angle of these hyperplanes, depicted schematically in <xref rid="fig5" ref-type="fig">Figure 5C</xref>. We, therefore, compared the different homeostatic variants of the reshaped projections models by initializing them from the same set of random projections, and evaluating the corresponding rotation angles, <italic>θ</italic>, of all of the projections due to the reshaping. <xref rid="fig5" ref-type="fig">Figure 5D</xref> shows an example of the rotations of the same initial projections for one model under different reshaping constraints, highlighting the substantial differences between them.</p>
<p><xref rid="fig5" ref-type="fig">Figure 5E</xref> shows the mean rotation angle over 100 homeostatic models as a function of synaptic cost – reflecting that the different forms of homeostatic regulation results in different reshaped projections. <xref rid="figS2" ref-type="fig">Supp. Figure 2C-D</xref> shows the histogram of the rotation angles of several homeostatic models, as well as the unconstrained reshape model.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We presented a new family of statistical models for large neural populations that is based on sparse and random non-linear projections of the population, which are adapted during learning. This new family of models proved to be more accurate than the highly accurate Random Projections class of models, using fewer projections and incurring a lower “synaptic cost” in terms of the total sum of synaptic weights of the model. Moreover, we found that reshaping of the projections gave even more accurate and efficient models in terms of synaptic weights of the neural circuit that implements the model, and was optimal for random and sparse initial connectivity, surpassing fully connected network models. The synaptic normalization mechanism resulted in homeostatic regulation of the firing rates of neurons in the model.</p>
<p>Our results suggest a computational role for the experimentally observed scaling or normalization of synapses during learning: In addition to “regularizing” the firing rates in neural circuits, in our Reshaped RP models, homeostatic plasticity optimizes the performance of network models and their efficiency in scenarios of limited resources and random connectivity. Moreover, the similarity of the performance of models that use different homeostatic synaptic mechanisms suggests a possible universal role for homeostatic mechanisms in computation.</p>
<p>We note that while homeostatic synaptic scaling regulates the firing rates of neurons (<xref ref-type="bibr" rid="c49">49</xref>), it is not immediately clear what “sets” the desired firing rate of each neuron. The synaptic normalization constraints we used here offer a simple solution: a universal value of the total incoming synaptic weights for the neurons in the circuit (or outgoing ones), results in a widely distributed firing rates of neurons (which may change considerably during the learning), but converge to a similar average value. Thus, rather than requiring some mechanism to define and balance the firing rates of individual neurons, our model suggest a single global synaptic feature that would set this for the random projections.</p>
<p>The shallowness of the circuit implementation of the Reshaped Random Projections model implies that the learning of these models does not require the backpropagation of information over many layers, which distinguishes deep artificial networks from biological ones. Moreover, the locality of the reshaping process itself points to the feasibility of this model in terms of real biological circuits. The biological plausibility is further supported by the robustness of the model to the specific connectivity used for the reshaped models, and to the specific choice of the homeostatic mechanism we used.</p>
<p>A key remaining issue for the biological feasibility of the RP family of models is the feedback signal from the readout neuron to the intermediate neurons. The noise-dependent learning mechanism for RP models presented in (<xref ref-type="bibr" rid="c30">30</xref>) and for other local feedback and synaptic learning mechanisms that approximate backprogapation (<xref ref-type="bibr" rid="c35">35</xref>) offers clear directions for future study. Our results may also be relevant for learning in artificial neural networks, whose training relies on nonconvex approaches that necessitate different regularization techniques (<xref ref-type="bibr" rid="c60">60</xref>). The homeostatic mechanism we focused on here is a form of “hard” L1 regularization, but on the sum of the weights. This approach limits the search space, compared to regularization over the weights themselves, but defines coupled changes in weights, in a manner highly effective for the cortical data we studied. We, therefore, hypothesize that homeostatic normalization may be beneficial for artificial architectures (see, e.g., (<xref ref-type="bibr" rid="c37">37</xref>)).</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Experimental Data</title>
<p>Extra-cellular recordings were performed using Utah arrays from populations of neurons in the prefrontal cortex of macaque monkeys performing a direction discrimination task with random dots. For more details see (<xref ref-type="bibr" rid="c57">57</xref>).</p>
</sec>
<sec id="s4b">
<title>Data Pre-processing</title>
<p>Neural activity was discretized using 20 ms bins, such that in each time bin a neuron was active (‘1’) if it emitted a spike in that bin and silent (‘0’) if not. Recorded data was split randomly into training sets and heldout test sets: 100 different random splits were generated for each model setup, consisting of 160,000 samples in the training set and 40,000 in the test set.</p>
</sec>
<sec id="s4c">
<title>Constructing Sparse Random Projections</title>
<p>Following (<xref ref-type="bibr" rid="c30">30</xref>), the coefficients <italic>a</italic><sub><italic>ij</italic></sub> of the random projections are set using a two stage process. First, the connectivity of the projections is set such that the average in-degree of the projections matches a predetermined sparsity value: each input neuron connects to each projection with a probability <italic>p</italic> = <italic>indegree/n</italic>, where <italic>n</italic> is the number of neurons in the input layer. The corresponding <italic>a</italic><sub><italic>ij</italic></sub> coefficients are then sampled from a Gaussian distribution, <italic>a</italic><sub><italic>ij</italic></sub> <italic>∼ N</italic> (<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c1">1</xref>), and the remaining <italic>a</italic><sub><italic>ij</italic></sub> values are set to zero. The threshold of each projection, <italic>θ</italic><sub><italic>i</italic></sub>, was set to 1.</p>
<p>The average in-degree of sparse models used here was 5, unless specified otherwise in the text. For the fully connected models <italic>indegree</italic> = <italic>n</italic> (i.e., sparsity=0).</p>
</sec>
<sec id="s4d">
<title>Training RP models</title>
<p>Given empirical data <bold>X</bold> and a set of projections defined by <italic>a</italic><sub><italic>ij</italic></sub>, we train the RP models by searching for the parameters <italic>λ</italic><sub><italic>i</italic></sub> that maximize the log-likelihood of the model given the data, arg max<sub><italic>λi</italic></sub> (<italic>L</italic>(X)), where <inline-formula><inline-graphic xlink:href="530392v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This is a convex function whose gradient is given by
<disp-formula id="eqn6">
<graphic xlink:href="530392v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We found the values <italic>λ</italic><sub><italic>i</italic></sub> that maximize the log-likelihood by gradient descent with momentum or ADAM algorithms. We computed the empirical expectation in <italic>⟨f</italic><sub><italic>i</italic></sub><italic>⟩</italic>X by summing over the training data, and the expectation over the probability model <italic>⟨f</italic><sub><italic>i</italic></sub><italic>⟩pRP</italic> by summing over synthetic data generated from <italic>p</italic><sub><italic>RP</italic></sub> using Metropolis–Hasting sampling.</p>
<p>For each of the empirical marginals <italic>⟨f</italic><sub><italic>i</italic></sub><italic>⟩</italic>X, we used the Clopper–Pearson method to estimate the distribution of possible values for the real marginal given the empirical observation. We set the convergence threshold of the numerical solver such that each of the marginals in the model distribution falls within a CI of one SD under this distribution, from its empirical marginal.</p>
</sec>
<sec id="s4e">
<title>Reshaping RP models</title>
<p>Given empirical data <bold>X</bold>, we optimize the RP models by modifying the coefficients <italic>a</italic><sub><italic>ij</italic></sub> such that the log-likelihood of the model is maximized, arg max<sub><italic>a</italic></sub><italic>ij</italic> (<italic>L</italic>(X)). Starting from an initial set of projec-tions, <inline-formula><inline-graphic xlink:href="530392v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, using the update rule of <xref ref-type="disp-formula" rid="eqn3">equation 3</xref>, we optimize the projections by applying the gradient descent with momentum algorithm. Importantly, only non-zero elements of <inline-formula><inline-graphic xlink:href="530392v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are optimized.</p>
</sec>
<sec id="s4f">
<title>Optimizing backpropagation models</title>
<p>Full backpropagation models are optimized using the learning rules of the trained RP models and the reshaped models simultaneously in each gradient descent step, i.e., <xref ref-type="disp-formula" rid="eqn3">eqs. 3</xref> and <xref ref-type="disp-formula" rid="eqn6">6</xref>.</p>
</sec>
<sec id="s4g">
<title>Homeostatic reshaping of RP models</title>
<p>The homeostatic RP models are reshaped as follows: We first define a set of unconstrained projections where the coefficients <italic>ã</italic><sub><italic>ij</italic></sub> are randomly sampled. Each of the projections is then normalized homeostatically, such that <italic>a</italic><sub><italic>ij</italic></sub> are a function of this unconstrained set: <italic>a</italic><sub><italic>ij</italic></sub> = <italic>ϕ · ã</italic><sub><italic>ij</italic></sub><italic>/Σ</italic><sub><italic>k</italic></sub> |<italic>ã</italic><sub><italic>ik</italic></sub>|, where <italic>ϕ</italic> is the available synaptic budget for each projection. We then optimize <italic>ã</italic><sub><italic>ij</italic></sub> to maximize the log-likelihood of the model given the empirical data <bold>X</bold>: arg max<italic>ãij</italic> (<italic>L</italic>(<bold>X</bold>)). The computed constrained projections <italic>a</italic><sub><italic>ij</italic></sub> are then used in the resulting homeostatic RP model.</p>
</sec>
<sec id="s4h">
<title>Bounded reshaping of RP models</title>
<p>Similar to reshaping homeostatic RP models, we define a set of unconstrained projections <italic>ã</italic><sub><italic>ij</italic></sub>, where the projections are a function of this unconstrained set: <italic>a</italic><sub><italic>ij</italic></sub> = min (max (<italic>ã</italic><sub><italic>ij</italic></sub>, <italic>−θ</italic>), <italic>θ</italic>), where <italic>θ</italic> is the “ceiling” value of each synapse.</p>
</sec>
<sec id="s4i">
<title>Generating synthetic data from RP models with known connectivity</title>
<p>Synthetic neural activity patterns were obtained by training RP models on real neural recordings as described above and then generating data from these models using Metropolis-Hastings sampling.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Adam Haber, Tal Tamir, Udi Karpas, and the rest of the Schneidman lab members for discussions, comments, and ideas. This work was supported by Simons Collaboration on the Global Brain grant 542997 (ES), Israel Science Foundation grant 137628 (ES), Israeli Council for Higher Education/Weizmann Data Science Research Center (ES), Martin Kushner Schnur, and Mr. &amp; Mrs. Lawrence Feis. ES is the incumbent of the Joseph and Bessie Feinberg Chair.</p>
</ack>
<ref-list>
<title>Bibliography</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><given-names>Wilson</given-names> <surname>Truccolo</surname></string-name>, <string-name><given-names>Uri T.</given-names> <surname>Eden</surname></string-name>, <string-name><given-names>Matthew R.</given-names> <surname>Fellows</surname></string-name>, <string-name><given-names>John P.</given-names> <surname>Donoghue</surname></string-name>, and <string-name><given-names>Emery N.</given-names> <surname>Brown</surname></string-name>. <article-title>A Point Process Framework for Relating Neural Spiking Activity to Spiking History, Neural Ensemble, and Extrinsic Covariate Effects</article-title>. <source>Journal of Neurophysiology</source>, <volume>93</volume>(<issue>2</issue>):<fpage>1074</fpage>–<lpage>1089</lpage>, <month>February</month> <year>2005</year>. ISSN <issn>0022-3077, 1522-1598</issn>. doi: <pub-id pub-id-type="doi">10.1152/jn.00697.2004</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>Jonathan W.</given-names> <surname>Pillow</surname></string-name>, <string-name><given-names>Jonathon</given-names> <surname>Shlens</surname></string-name>, <string-name><given-names>Liam</given-names> <surname>Paninski</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Sher</surname></string-name>, <string-name><given-names>Alan M.</given-names> <surname>Litke</surname></string-name>, <string-name><given-names>E. J.</given-names> <surname>Chichilnisky</surname></string-name>, and <string-name><given-names>Eero P.</given-names> <surname>Simoncelli</surname></string-name>. <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source>, <volume>454</volume>(<issue>7207</issue>):<fpage>995</fpage>–<lpage>999</lpage>, <month>August</month> <year>2008</year>. ISSN <issn>0028-0836, 1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature07140</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><given-names>Ana</given-names> <surname>Calabrese</surname></string-name>, <string-name><given-names>Joseph W.</given-names> <surname>Schumacher</surname></string-name>, <string-name><given-names>David M.</given-names> <surname>Schneider</surname></string-name>, <string-name><given-names>Liam</given-names> <surname>Paninski</surname></string-name>, and <string-name><given-names>Sarah M. N.</given-names> <surname>Woolley</surname></string-name>. <article-title>A Generalized Linear Model for Estimating Spectrotemporal Receptive Fields from Responses to Natural Sounds</article-title>. <source>PLoS ONE</source>, <volume>6</volume>(<issue>1</issue>):<fpage>e16104</fpage>, <month>January</month> <year>2011</year>. ISSN <issn>1932-6203</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0016104</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><given-names>Franz</given-names> <surname>Weber</surname></string-name>, <string-name><given-names>Christian K.</given-names> <surname>Machens</surname></string-name>, and <string-name><given-names>Alexander</given-names> <surname>Borst</surname></string-name>. <article-title>Disentangling the functional consequences of the connectivity between optic-flow processing neurons</article-title>. <source>Nature Neuro-science</source>, <volume>15</volume>(<issue>3</issue>):<fpage>441</fpage>–<lpage>448</lpage>, <month>March</month> <year>2012</year>. ISSN <issn>1097-6256, 1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn.3044</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>, <string-name><given-names>Michael J.</given-names> <surname>Berry</surname></string-name>, <string-name><given-names>Ronen</given-names> <surname>Segev</surname></string-name>, and <string-name><given-names>William</given-names> <surname>Bialek</surname></string-name>. <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title>. <source>Nature</source>, <volume>440</volume> (<issue>7087</issue>):<fpage>1007</fpage>–<lpage>1012</lpage>, <month>April</month> <year>2006</year>. ISSN <issn>0028-0836, 1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature04701</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Shlens</surname></string-name>, <string-name><given-names>G. D.</given-names> <surname>Field</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Gauthier</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Grivich</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Petrusca</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Sher</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Litke</surname></string-name>, and <string-name><given-names>E. J.</given-names> <surname>Chichilnisky</surname></string-name>. <article-title>The Structure of Multi-Neuron Firing Patterns in Primate Retina</article-title>. <source>Journal of Neuroscience</source>, <volume>26</volume>(<issue>32</issue>):<fpage>8254</fpage>–<lpage>8266</lpage>, <month>August</month> <year>2006</year>. ISSN <issn>0270-6474, 1529-2401</issn>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1282-06.2006</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Tang</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Jackson</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Hobbs</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Smith</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Patel</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Prieto</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Petrusca</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Grivich</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Sher</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Hottowy</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Dabrowski</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Litke</surname></string-name>, and <string-name><given-names>J. M.</given-names> <surname>Beggs</surname></string-name>. <article-title>A Maximum Entropy Model Applied to Spatial and Temporal Correlations from Cortical Networks In Vitro</article-title>. <source>Journal of Neuroscience</source>, <volume>28</volume>(<issue>2</issue>):<fpage>505</fpage>–<lpage>518</lpage>, <month>January</month> <year>2008</year>. ISSN <issn>0270-6474, 1529-2401</issn>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3359-07.2008</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><given-names>Gašper</given-names> <surname>Tkačik</surname></string-name>, <string-name><given-names>Olivier</given-names> <surname>Marre</surname></string-name>, <string-name><given-names>Dario</given-names> <surname>Amodei</surname></string-name>, <string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>, <string-name><given-names>William</given-names> <surname>Bialek</surname></string-name>, and <string-name><given-names>Michael J.</given-names> <surname>Berry</surname></string-name>. <article-title>Searching for Collective Behavior in a Large Network of Sensory Neurons</article-title>. <source>PLoS Computational Biology</source>, <volume>10</volume>(<issue>1</issue>):<fpage>e1003408</fpage>, <month>January</month> <year>2014</year>. ISSN <issn>1553-7358</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003408</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><given-names>Elad</given-names> <surname>Ganmor</surname></string-name>, <string-name><given-names>Ronen</given-names> <surname>Segev</surname></string-name>, and <string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>. <article-title>Sparse low-order interaction network underlies a highly correlated and learnable neural population code</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>108</volume>(<issue>23</issue>):<fpage>9679</fpage>–<lpage>9684</lpage>, <month>June</month> <year>2011</year>. ISSN <issn>0027-8424, 1091-6490</issn>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1019641108</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><given-names>O.</given-names> <surname>Marre</surname></string-name>, <string-name><given-names>S.</given-names> <surname>El Boustani</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Frégnac</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Destexhe</surname></string-name>. <article-title>Prediction of Spatiotemporal Patterns of Neural Activity from Pairwise Correlations</article-title>. <source>Physical Review Letters</source>, <volume>102</volume>(<issue>13</issue>): <fpage>138101</fpage>, <month>April</month> <year>2009</year>. doi: <pub-id pub-id-type="doi">10.1103/PhysRevLett.102.138101</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><given-names>Ifije E.</given-names> <surname>Ohiorhenuan</surname></string-name>, <string-name><given-names>Ferenc</given-names> <surname>Mechler</surname></string-name>, <string-name><given-names>Keith P.</given-names> <surname>Purpura</surname></string-name>, <string-name><given-names>Anita M.</given-names> <surname>Schmid</surname></string-name>, <string-name><given-names>Qin</given-names> <surname>Hu</surname></string-name>, and <string-name><given-names>Jonathan D.</given-names> <surname>Victor</surname></string-name>. <article-title>Sparse coding and high-order correlations in fine-scale cortical networks</article-title>. <source>Nature</source>, <volume>466</volume>(<issue>7306</issue>):<fpage>617</fpage>–<lpage>621</lpage>, <month>July</month> <year>2010</year>. ISSN <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature09178</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><given-names>Einat</given-names> <surname>Granot-Atedgi</surname></string-name>, <string-name><given-names>Gašper</given-names> <surname>Tkačik</surname></string-name>, <string-name><given-names>Ronen</given-names> <surname>Segev</surname></string-name>, and <string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>. <article-title>Stimulus-dependent Maximum Entropy Models of Neural Population Codes</article-title>. <source>PLOS Computational Biology</source>, <volume>9</volume>(<issue>3</issue>):<fpage>e1002922</fpage>, <month>March</month> <year>2013</year>. ISSN <issn>1553-7358</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002922</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><given-names>Leenoy</given-names> <surname>Meshulam</surname></string-name>, <string-name><given-names>Jeffrey L.</given-names> <surname>Gauthier</surname></string-name>, <string-name><given-names>Carlos D.</given-names> <surname>Brody</surname></string-name>, <string-name><given-names>David W.</given-names> <surname>Tank</surname></string-name>, and <string-name><given-names>William</given-names> <surname>Bialek</surname></string-name>. <article-title>Collective Behavior of Place and Non-place Neurons in the Hippocampal Network</article-title>. <source>Neuron</source>, <volume>96</volume>(<issue>5</issue>):<fpage>1178</fpage>–<lpage>1191</lpage>.e4, <month>December</month> <year>2017</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2017.10.027</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><given-names>Elad</given-names> <surname>Ganmor</surname></string-name>, <string-name><given-names>Ronen</given-names> <surname>Segev</surname></string-name>, and <string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>. <article-title>A thesaurus for a neural population code</article-title>. <source>eLife</source>, <volume>4</volume>:<fpage>e06134</fpage>, <month>September</month> <year>2015</year>. ISSN <issn>2050-084X</issn>. doi: <pub-id pub-id-type="doi">10.7554/eLife.06134</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><given-names>Gašper</given-names> <surname>Tkačik</surname></string-name>, <string-name><given-names>Einat</given-names> <surname>Granot-Atedgi</surname></string-name>, <string-name><given-names>Ronen</given-names> <surname>Segev</surname></string-name>, and <string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>. <article-title>Retinal Metric: A Stimulus Distance Measure Derived from Population Neural Responses</article-title>. <source>Physical Review Letters</source>, <volume>110</volume>(<issue>5</issue>):<fpage>058104</fpage>, <month>January</month> <year>2013</year>. ISSN <issn>0031-9007, 1079-7114</issn>. doi: <pub-id pub-id-type="doi">10.1103/PhysRevLett.110.058104</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>Chethan</given-names> <surname>Pandarinath</surname></string-name>, <string-name><given-names>Daniel J.</given-names> <surname>O’Shea</surname></string-name>, <string-name><given-names>Jasmine</given-names> <surname>Collins</surname></string-name>, <string-name><given-names>Rafal</given-names> <surname>Jozefowicz</surname></string-name>, <string-name><given-names>Sergey D.</given-names> <surname>Stavisky</surname></string-name>, <string-name><given-names>Jonathan C.</given-names> <surname>Kao</surname></string-name>, <string-name><given-names>Eric M.</given-names> <surname>Trautmann</surname></string-name>, <string-name><given-names>Matthew T.</given-names> <surname>Kaufman</surname></string-name>, <string-name><given-names>Stephen I.</given-names> <surname>Ryu</surname></string-name>, <string-name><given-names>Leigh R.</given-names> <surname>Hochberg</surname></string-name>, <string-name><given-names>Jaimie M.</given-names> <surname>Henderson</surname></string-name>, <string-name><given-names>Krishna V.</given-names> <surname>Shenoy</surname></string-name>, <string-name><given-names>L. F.</given-names> <surname>Abbott</surname></string-name>, and <string-name><given-names>David</given-names> <surname>Sussillo</surname></string-name>. <article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title>. <source>Nature Methods</source>, <volume>15</volume>(<issue>10</issue>):<fpage>805</fpage>–<lpage>815</lpage>, <month>October</month> <year>2018</year>. ISSN <issn>1548-7105</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><given-names>David GT</given-names> <surname>Barrett</surname></string-name>, <string-name><given-names>Ari S</given-names> <surname>Morcos</surname></string-name>, and <string-name><given-names>Jakob H</given-names> <surname>Macke</surname></string-name>. <article-title>Analyzing biological and artificial neural networks: challenges with opportunities for synergy?</article-title> <source>Current Opinion in Neurobiology</source>, <volume>55</volume>:<fpage>55</fpage>–<lpage>64</lpage>, <month>April</month> <year>2019</year>. ISSN <issn>0959-4388</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.conb.2019.01.007</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><given-names>Pedro J</given-names> <surname>Gonçalves</surname></string-name>, <string-name><given-names>Jan-Matthis</given-names> <surname>Lueckmann</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Deistler</surname></string-name>, <string-name><given-names>Marcel</given-names> <surname>Nonnenmacher KaanÖcal</surname></string-name>, <string-name><given-names>Giacomo</given-names> <surname>Bassetto</surname></string-name>, <string-name><given-names>Chaitanya</given-names> <surname>Chintaluri</surname></string-name>, <string-name><given-names>William F</given-names> <surname>Podlaski</surname></string-name>, <string-name><given-names>Sara A</given-names> <surname>Haddad</surname></string-name>, <string-name><given-names>Tim P</given-names> <surname>Vogels</surname></string-name>, <string-name><given-names>David S</given-names> <surname>Greenberg</surname></string-name>, and <string-name><given-names>Jakob H</given-names> <surname>Macke</surname></string-name>. <article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title>. <source>eLife</source>, <volume>9</volume>:<fpage>e56261</fpage>, <month>September</month> <year>2020</year>. ISSN <issn>2050-084X</issn>. doi: <pub-id pub-id-type="doi">10.7554/eLife.56261</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><given-names>Gašper</given-names> <surname>Tkačik</surname></string-name>, <string-name><given-names>Thierry</given-names> <surname>Mora</surname></string-name>, <string-name><given-names>Olivier</given-names> <surname>Marre</surname></string-name>, <string-name><given-names>Dario</given-names> <surname>Amodei</surname></string-name>, <string-name><given-names>Stephanie E.</given-names> <surname>Palmer</surname></string-name>, <string-name><given-names>Michael J.</given-names> <surname>Berry</surname></string-name>, and <string-name><given-names>William</given-names> <surname>Bialek</surname></string-name>. <article-title>Thermodynamics and signatures of criticality in a network of neurons</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>112</volume>(<issue>37</issue>):<fpage>11508</fpage>–<lpage>11513</lpage>, <month>September</month> <year>2015</year>. ISSN <issn>0027-8424, 1091-6490</issn>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1514188112</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><given-names>Leenoy</given-names> <surname>Meshulam</surname></string-name>, <string-name><given-names>Jeffrey L.</given-names> <surname>Gauthier</surname></string-name>, <string-name><given-names>Carlos D.</given-names> <surname>Brody</surname></string-name>, <string-name><given-names>David W.</given-names> <surname>Tank</surname></string-name>, and <string-name><given-names>William</given-names> <surname>Bialek</surname></string-name>. <article-title>Coarse Graining, Fixed Points, and Scaling in a Large Population of Neurons</article-title>. <source>Physical Review Letters</source>, <volume>123</volume>(<issue>17</issue>):<fpage>178103</fpage>, <month>October</month> <year>2019</year>. ISSN <issn>0031-9007, 1079-7114</issn>. doi: <pub-id pub-id-type="doi">10.1103/PhysRevLett.123.178103</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>. <article-title>Towards the design principles of neural population codes</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>37</volume>:<fpage>133</fpage>–<lpage>140</lpage>, <month>April</month> <year>2016</year>. ISSN <issn>0959-4388</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.conb.2016.03.001</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="preprint"><string-name><given-names>ED</given-names> <surname>Karpas</surname></string-name>, <string-name><given-names>O</given-names> <surname>Maoz</surname></string-name>, <string-name><given-names>R</given-names> <surname>Kiani</surname></string-name>, and <string-name><given-names>E</given-names> <surname>Schneidman</surname></string-name>. <article-title>Strongly correlated spatiotemporal encoding and simple decoding in the prefrontal cortex</article-title>. <source>bioRxiv</source>, <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1101/693192</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><given-names>Stefano</given-names> <surname>Panzeri</surname></string-name>, <string-name><given-names>Christopher D.</given-names> <surname>Harvey</surname></string-name>, <string-name><given-names>Eugenio</given-names> <surname>Piasini</surname></string-name>, <string-name><given-names>Peter E.</given-names> <surname>Latham</surname></string-name>, and <string-name><given-names>Tommaso</given-names> <surname>Fellin</surname></string-name>. <article-title>Cracking the Neural Code for Sensory Perception by Combining Statistics, Intervention, and Behavior</article-title>. <source>Neuron</source>, <volume>93</volume>(<issue>3</issue>):<fpage>491</fpage>–<lpage>507</lpage>, <month>February</month> <year>2017</year>. ISSN <issn>08966273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.036</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><given-names>Gašper</given-names> <surname>Tkačik</surname></string-name>, <string-name><given-names>Olivier</given-names> <surname>Marre</surname></string-name>, <string-name><given-names>Thierry</given-names> <surname>Mora</surname></string-name>, <string-name><given-names>Dario</given-names> <surname>Amodei</surname></string-name>, <string-name><given-names>Michael J.</given-names> <surname>Berry</surname> <suffix>II</suffix></string-name>, and <string-name><given-names>William</given-names> <surname>Bialek</surname></string-name>. <article-title>The simplest maximum entropy model for collective behavior in a neural network</article-title>. <source>Journal of Statistical Mechanics: Theory and Experiment</source>, <volume>2013</volume>(<issue>03</issue>):<fpage>P03011</fpage>, <month>March</month> <year>2013</year>. ISSN <issn>1742-5468</issn>. doi: <pub-id pub-id-type="doi">10.1088/1742-5468/2013/03/P03011</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><given-names>Vicente</given-names> <surname>Botella-Soler</surname></string-name>, <string-name><given-names>Stéphane</given-names> <surname>Deny</surname></string-name>, <string-name><given-names>Georg</given-names> <surname>Martius</surname></string-name>, <string-name><given-names>Olivier</given-names> <surname>Marre</surname></string-name>, and <string-name><given-names>Gašper</given-names> <surname>Tkačik</surname></string-name>. <article-title>Nonlinear decoding of a complex movie from the mammalian retina</article-title>. <source>PLOS Computational Biology</source>, <volume>14</volume>(<issue>5</issue>):<fpage>e1006057</fpage>, <month>May</month> <year>2018</year>. ISSN <issn>1553-7358</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006057</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><given-names>Qing</given-names> <surname>Shi</surname></string-name>, <string-name><given-names>Pranjal</given-names> <surname>Gupta</surname></string-name>, <string-name><given-names>Alexandra K.</given-names> <surname>Boukhvalova</surname></string-name>, <string-name><given-names>Joshua H.</given-names> <surname>Singer</surname></string-name>, and <string-name><given-names>Daniel A.</given-names> <surname>Butts</surname></string-name>. <article-title>Functional characterization of retinal ganglion cells using tailored nonlinear modeling</article-title>. <source>Scientific Reports</source>, <volume>9</volume>(<issue>1</issue>):<fpage>8713</fpage>, <month>June</month> <year>2019</year>. ISSN <issn>2045-2322</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41598-019-45048-8</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="preprint"><string-name><given-names>Matthew R.</given-names> <surname>Whiteway</surname></string-name>, <string-name><given-names>Bruno</given-names> <surname>Averbeck</surname></string-name>, and <string-name><given-names>Daniel A.</given-names> <surname>Butts</surname></string-name>. <article-title>A latent variable approach to decoding neural population activity</article-title>, <source>bioRxiv</source> <month>January</month> <year>2020</year>. Pages: 2020.01.06.896423 Section: New Results.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><given-names>Juan A.</given-names> <surname>Gallego</surname></string-name>, <string-name><given-names>Matthew G.</given-names> <surname>Perich</surname></string-name>, <string-name><given-names>Raeed H.</given-names> <surname>Chowdhury</surname></string-name>, <string-name><given-names>Sara A.</given-names> <surname>Solla</surname></string-name>, and <string-name><given-names>Lee E.</given-names> <surname>Miller</surname></string-name>. <article-title>Long-term stability of cortical population dynamics underlying consistent behavior</article-title>. <source>Nature Neuroscience</source>, <volume>23</volume>(<issue>2</issue>):<fpage>260</fpage>–<lpage>270</lpage>, <month>February</month> <year>2020</year>. ISSN <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41593-019-0555-4</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><given-names>Rishidev</given-names> <surname>Chaudhuri</surname></string-name>, <string-name><given-names>Berk</given-names> <surname>Gerçek</surname></string-name>, <string-name><given-names>Biraj</given-names> <surname>Pandey</surname></string-name>, <string-name><given-names>Adrien</given-names> <surname>Peyrache</surname></string-name>, and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name>. <article-title>The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>(<issue>9</issue>):<fpage>1512</fpage>–<lpage>1520</lpage>, <month>September</month> <year>2019</year>. ISSN <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41593-019-0460-x</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><given-names>Ori</given-names> <surname>Maoz</surname></string-name>, <string-name><given-names>Gašper</given-names> <surname>Tkačik</surname></string-name>, <string-name><given-names>Mohamad Saleh</given-names> <surname>Esteki</surname></string-name>, <string-name><given-names>Roozbeh</given-names> <surname>Kiani</surname></string-name>, and <string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>. <article-title>Learning probabilistic neural representations with randomly connected circuits</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>40</issue>):<fpage>25066</fpage>–<lpage>25073</lpage>, <month>October</month> <year>2020</year>. ISSN <issn>0027-8424, 1091-6490</issn>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1912804117</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="preprint"><string-name><given-names>Eszter</given-names> <surname>Vertes</surname></string-name> and <string-name><given-names>Maneesh</given-names> <surname>Sahani</surname></string-name>. <article-title>Flexible and accurate inference and learning for deep generative models</article-title>, <source>arXiv</source> <month>May</month> <year>2018</year>. arXiv:1805.11051 [cs, stat].</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><given-names>Richard S.</given-names> <surname>Zemel</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Dayan</surname></string-name>, and <string-name><given-names>Alexandre</given-names> <surname>Pouget</surname></string-name>. <article-title>Probabilistic Interpretation of Population Codes</article-title>. <source>Neural Computation</source>, <volume>10</volume>(<issue>2</issue>):<fpage>403</fpage>–<lpage>430</lpage>, <month>February</month> <year>1998</year>. ISSN <issn>0899-7667, 1530-888X</issn>. doi: <pub-id pub-id-type="doi">10.1162/089976698300017818</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="preprint"><string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>Dong-Hyun</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Jorg</given-names> <surname>Bornschein</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Mesnard</surname></string-name>, and <string-name><given-names>Zhouhan</given-names> <surname>Lin</surname></string-name>. <article-title>Towards Biologically Plausible Deep Learning</article-title>, <source>arXiv</source> <month>August</month> <year>2016</year>. arXiv:1502.04156 [cs].</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><given-names>Daniel L K</given-names> <surname>Yamins</surname></string-name> and <string-name><given-names>James J</given-names> <surname>DiCarlo</surname></string-name>. <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>3</issue>):<fpage>356</fpage>–<lpage>365</lpage>, <month>March</month> <year>2016</year>. ISSN <issn>1097-6256, 1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn.4244</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><given-names>Panayiota</given-names> <surname>Poirazi</surname></string-name>, <string-name><given-names>Terrence</given-names> <surname>Brannon</surname></string-name>, and <string-name><given-names>Bartlett W.</given-names> <surname>Mel</surname></string-name>. <article-title>Pyramidal Neuron as Two-Layer Neural Network</article-title>. <source>Neuron</source>, <volume>37</volume>(<issue>6</issue>):<fpage>989</fpage>–<lpage>999</lpage>, <month>March</month> <year>2003</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/S0896-6273(03)00149-1</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><given-names>Blake A.</given-names> <surname>Richards</surname></string-name>, <string-name><given-names>Timothy P.</given-names> <surname>Lillicrap</surname></string-name>, <string-name><given-names>Philippe</given-names> <surname>Beaudoin</surname></string-name>, <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>Rafal</given-names> <surname>Bogacz</surname></string-name>, <string-name><given-names>Amelia</given-names> <surname>Christensen</surname></string-name>, <string-name><given-names>Claudia</given-names> <surname>Clopath</surname></string-name>, <string-name><given-names>Rui Ponte</given-names> <surname>Costa</surname></string-name>, <string-name><given-names>Archy</given-names> <surname>de Berker</surname></string-name>, <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>, <string-name><given-names>Colleen J.</given-names> <surname>Gillon</surname></string-name>, <string-name><given-names>Danijar</given-names> <surname>Hafner</surname></string-name>, <string-name><given-names>Adam</given-names> <surname>Kepecs</surname></string-name>, <string-name><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Latham</surname></string-name>, <string-name><given-names>Grace W.</given-names> <surname>Lindsay</surname></string-name>, <string-name><given-names>Kenneth D.</given-names> <surname>Miller</surname></string-name>, <string-name><given-names>Richard</given-names> <surname>Naud</surname></string-name>, <string-name><given-names>Christopher C.</given-names> <surname>Pack</surname></string-name>, <string-name><given-names>Panayiota</given-names> <surname>Poirazi</surname></string-name>, <string-name><given-names>Pieter</given-names> <surname>Roelfsema</surname></string-name>, <string-name><given-names>João</given-names> <surname>Sacramento</surname></string-name>, <string-name><given-names>Andrew</given-names> <surname>Saxe</surname></string-name>, <string-name><given-names>Benjamin</given-names> <surname>Scellier</surname></string-name>, <string-name><given-names>Anna C.</given-names> <surname>Schapiro</surname></string-name>, <string-name><given-names>Walter</given-names> <surname>Senn</surname></string-name>, <string-name><given-names>Greg</given-names> <surname>Wayne</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Yamins</surname></string-name>, <string-name><given-names>Friedemann</given-names> <surname>Zenke</surname></string-name>, <string-name><given-names>Joel</given-names> <surname>Zylberberg</surname></string-name>, <string-name><given-names>Denis</given-names> <surname>Therien</surname></string-name>, and <string-name><given-names>Konrad P.</given-names> <surname>Kording</surname></string-name>. <article-title>A deep learning framework for neuroscience</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>(<issue>11</issue>):<fpage>1761</fpage>–<lpage>1770</lpage>, <month>November</month> <year>2019</year>. ISSN <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="confproc"><string-name><given-names>Weishun</given-names> <surname>Zhong</surname></string-name>, <string-name><given-names>Ben</given-names> <surname>Sorscher</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Lee</surname></string-name>, and <string-name><given-names>Haim</given-names> <surname>Sompolinsky</surname></string-name>. <article-title>A theory of weight distribution-constrained learning</article-title>. In <person-group person-group-type="editor"><string-name><given-names>Alice H.</given-names> <surname>Oh</surname></string-name>, <string-name><given-names>Alekh</given-names> <surname>Agarwal</surname></string-name>, <string-name><given-names>Danielle</given-names> <surname>Belgrave</surname></string-name>, and <string-name><given-names>Kyunghyun</given-names> <surname>Cho</surname></string-name></person-group>, editors, <source>Advances in Neural Information Processing Systems</source>, <year>2022</year>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><given-names>Spyridon</given-names> <surname>Chavlis</surname></string-name> and <string-name><given-names>Panayiota</given-names> <surname>Poirazi</surname></string-name>. <article-title>Drawing inspiration from biological dendrites to empower artificial neural networks</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>70</volume>:<fpage>1</fpage>–<lpage>10</lpage>, <month>October</month> <year>2021</year>. ISSN <issn>0959-4388</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.conb.2021.04.007</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><given-names>Ashok</given-names> <surname>Litwin-Kumar</surname></string-name>, <string-name><given-names>Kameron Decker</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>Richard</given-names> <surname>Axel</surname></string-name>, <string-name><given-names>Haim</given-names> <surname>Sompolinsky</surname></string-name>, and <string-name><given-names>L. F.</given-names> <surname>Abbott</surname></string-name>. <article-title>Optimal Degrees of Synaptic Connectivity</article-title>. <source>Neuron</source>, <volume>93</volume>(<issue>5</issue>):<fpage>1153</fpage>–<lpage>1164</lpage>.e7, <year>2017</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.030</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><given-names>Adam</given-names> <surname>Haber</surname></string-name> and <string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>. <article-title>Learning the Architectural Features That Predict Functional Similarity of Neural Networks</article-title>. <source>Physical Review X</source>, <volume>12</volume>(<issue>2</issue>):<fpage>021051</fpage>, <month>June</month> <year>2022</year>. ISSN <issn>2160-3308</issn>. doi: <pub-id pub-id-type="doi">10.1103/PhysRevX.12.021051</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><given-names>Sung Soo</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Ann M.</given-names> <surname>Hermundstad</surname></string-name>, <string-name><given-names>Sandro</given-names> <surname>Romani</surname></string-name>, <string-name><given-names>L. F.</given-names> <surname>Abbott</surname></string-name>, and <string-name><given-names>Vivek</given-names> <surname>Jayaraman</surname></string-name>. <article-title>Generation of stable heading representations in diverse visual scenes</article-title>. <source>Nature</source>, <volume>576</volume>(<issue>7785</issue>): <fpage>126</fpage>–<lpage>131</lpage>, <month>December</month> <year>2019</year>. ISSN <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41586-019-1767-1</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><given-names>Vladyslava</given-names> <surname>Pechuk</surname></string-name>, <string-name><given-names>Gal</given-names> <surname>Goldman</surname></string-name>, <string-name><given-names>Yehuda</given-names> <surname>Salzberg</surname></string-name>, <string-name><given-names>Aditi H.</given-names> <surname>Chaubey</surname></string-name>, <string-name><given-names>R. Aaron</given-names> <surname>Bola</surname></string-name>, <string-name><given-names>Jonathon R.</given-names> <surname>Hoffman</surname></string-name>, <string-name><given-names>Morgan L.</given-names> <surname>Endreson</surname></string-name>, <string-name><given-names>Renee M.</given-names> <surname>Miller</surname></string-name>, <string-name><given-names>Noah J.</given-names> <surname>Reger</surname></string-name>, <string-name><given-names>Douglas S.</given-names> <surname>Portman</surname></string-name>, <string-name><given-names>Denise M.</given-names> <surname>Ferkey</surname></string-name>, <string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>, and <string-name><given-names>Meital</given-names> <surname>Oren-Suissa</surname></string-name>. <article-title>Reprogramming the topology of the nociceptive circuit in C. elegans reshapes sexual behavior</article-title>. <source>Current Biology</source>, <volume>32</volume>(<issue>20</issue>):<fpage>4372</fpage>–<lpage>4385</lpage>.e7, <month>October</month> <year>2022</year>. ISSN <issn>09609822</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2022.08.038</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="preprint"><string-name><given-names>Adam</given-names> <surname>Haber</surname></string-name> and <string-name><given-names>Elad</given-names> <surname>Schneidman</surname></string-name>. <article-title>The computational and learning benefits of Daleian neural networks</article-title>, <source>arXiv</source> <month>October</month> <year>2022</year>. arXiv:2210.05961 [q-bio].</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><given-names>David J.</given-names> <surname>Heeger</surname></string-name>. <article-title>Normalization of cell responses in cat striate cortex</article-title>. <source>Visual Neuroscience</source>, <volume>9</volume>(<issue>2</issue>):<fpage>181</fpage>–<lpage>197</lpage>, <month>August</month> <year>1992</year>. ISSN <issn>0952-5238, 1469-8714</issn>. doi: <pub-id pub-id-type="doi">10.1017/S0952523800009640</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><given-names>Matteo</given-names> <surname>Carandini</surname></string-name> and <string-name><given-names>David J.</given-names> <surname>Heeger</surname></string-name>. <article-title>Normalization as a canonical neural computation</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>13</volume>(<issue>1</issue>):<fpage>51</fpage>–<lpage>62</lpage>, <month>January</month> <year>2012</year>. ISSN <issn>1471-003X, 1471-0048</issn>. doi: <pub-id pub-id-type="doi">10.1038/nrn3136</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><given-names>Gina G.</given-names> <surname>Turrigiano</surname></string-name>, <string-name><given-names>Kenneth R.</given-names> <surname>Leslie</surname></string-name>, <string-name><given-names>Niraj S.</given-names> <surname>Desai</surname></string-name>, <string-name><given-names>Lana C.</given-names> <surname>Rutherford</surname></string-name>, and <string-name><given-names>Sacha B.</given-names> <surname>Nelson</surname></string-name>. <article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons</article-title>. <source>Nature</source>, <volume>391</volume>(<issue>6670</issue>):<fpage>892</fpage>–<lpage>896</lpage>, <month>February</month> <year>1998</year>. ISSN <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/36103</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><given-names>Tara</given-names> <surname>Keck</surname></string-name>, <string-name><given-names>Georg B.</given-names> <surname>Keller</surname></string-name>, <string-name><given-names>R. Irene</given-names> <surname>Jacobsen</surname></string-name>, <string-name><given-names>Ulf T.</given-names> <surname>Eysel</surname></string-name>, <string-name><given-names>Tobias</given-names> <surname>Bonhoeffer</surname></string-name>, and <string-name><given-names>Mark</given-names> <surname>Hübener</surname></string-name>. <article-title>Synaptic Scaling and Homeostatic Plasticity in the Mouse Visual Cortex In Vivo</article-title>. <source>Neuron</source>, <volume>80</volume>(<issue>2</issue>):<fpage>327</fpage>–<lpage>334</lpage>, <month>October</month> <year>2013</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.018</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><given-names>Keith B.</given-names> <surname>Hengen</surname></string-name>, <string-name><given-names>Mary E.</given-names> <surname>Lambo</surname></string-name>, <string-name><given-names>Stephen D.</given-names> <surname>Van Hooser</surname></string-name>, <string-name><given-names>Donald B.</given-names> <surname>Katz</surname></string-name>, and <string-name><given-names>Gina G.</given-names> <surname>Turrigiano</surname></string-name>. <article-title>Firing Rate Homeostasis in Visual Cortex of Freely Behaving Rodents</article-title>. <source>Neuron</source>, <volume>80</volume>(<issue>2</issue>):<fpage>335</fpage>–<lpage>342</lpage>, <month>October</month> <year>2013</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.038</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><given-names>Gina G.</given-names> <surname>Turrigiano</surname></string-name>. <article-title>The Self-Tuning Neuron: Synaptic Scaling of Excitatory Synapses</article-title>. <source>Cell</source>, <volume>135</volume>(<issue>3</issue>):<fpage>422</fpage>–<lpage>435</lpage>, <month>October</month> <year>2008</year>. ISSN <issn>00928674</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cell.2008.10.008</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><given-names>Sami</given-names> <surname>El-Boustani</surname></string-name>, <string-name><given-names>Jacque P. K.</given-names> <surname>Ip</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Breton-Provencher</surname></string-name>, <string-name><given-names>Graham W.</given-names> <surname>Knott</surname></string-name>, <string-name><given-names>Hiroyuki</given-names> <surname>Okuno</surname></string-name>, <string-name><given-names>Haruhiko</given-names> <surname>Bito</surname></string-name>, and <string-name><given-names>Mriganka</given-names> <surname>Sur</surname></string-name>. <article-title>Locally coordinated synaptic plasticity of visual cortex neurons in vivo</article-title>. <source>Science</source>, <volume>360</volume>(<issue>6395</issue>):<fpage>1349</fpage>–<lpage>1354</lpage>, <month>June</month> <year>2018</year>. ISSN <issn>0036-8075, 1095-9203</issn>. doi: <pub-id pub-id-type="doi">10.1126/science.aao0862</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><given-names>Yue Kris</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Keith B.</given-names> <surname>Hengen</surname></string-name>, <string-name><given-names>Gina G.</given-names> <surname>Turrigiano</surname></string-name>, and <string-name><given-names>Julijana</given-names> <surname>Gjorgjieva</surname></string-name>. <article-title>Homeostatic mechanisms regulate distinct aspects of cortical circuit dynamics</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>39</issue>):<fpage>24514</fpage>–<lpage>24525</lpage>, <month>September</month> <year>2020</year>. ISSN <issn>0027-8424, 1091-6490</issn>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1918368117</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><given-names>Tara</given-names> <surname>Keck</surname></string-name>, <string-name><given-names>Taro</given-names> <surname>Toyoizumi</surname></string-name>, <string-name><given-names>Lu</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Brent</given-names> <surname>Doiron</surname></string-name>, <string-name><given-names>Daniel E.</given-names> <surname>Feldman</surname></string-name>, <string-name><given-names>Kevin</given-names> <surname>Fox</surname></string-name>, <string-name><given-names>Wulfram</given-names> <surname>Gerstner</surname></string-name>, <string-name><given-names>Philip G.</given-names> <surname>Haydon</surname></string-name>, <string-name><given-names>Mark</given-names> <surname>Hübener</surname></string-name>, <string-name><given-names>Hey-Kyoung</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>John E.</given-names> <surname>Lisman</surname></string-name>, <string-name><given-names>Tobias</given-names> <surname>Rose</surname></string-name>, <string-name><given-names>Frank</given-names> <surname>Sengpiel</surname></string-name>, <string-name><given-names>David</given-names> <surname>Stellwagen</surname></string-name>, <string-name><given-names>Michael P.</given-names> <surname>Stryker</surname></string-name>, <string-name><given-names>Gina G.</given-names> <surname>Turrigiano</surname></string-name>, and <string-name><given-names>Mark C.</given-names> <surname>van Rossum</surname></string-name>. <article-title>Integrating Hebbian and homeostatic plasticity: the current state of the field and future research directions</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>372</volume>(<issue>1715</issue>):<fpage>20160158</fpage>, <month>March</month> <year>2017</year>. ISSN <issn>0962-8436, 1471-2970</issn>. doi: <pub-id pub-id-type="doi">10.1098/rstb.2016.0158</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><given-names>Friedemann</given-names> <surname>Zenke</surname></string-name> and <string-name><given-names>Wulfram</given-names> <surname>Gerstner</surname></string-name>. <article-title>Hebbian plasticity requires compensatory processes on multiple timescales</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>372</volume>(<issue>1715</issue>):<fpage>20160259</fpage>, <month>March</month> <year>2017</year>. ISSN <issn>0962-8436, 1471-2970</issn>. doi: <pub-id pub-id-type="doi">10.1098/rstb.2016.0259</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><given-names>Taro</given-names> <surname>Toyoizumi</surname></string-name>, <string-name><given-names>Megumi</given-names> <surname>Kaneko</surname></string-name>, <string-name><given-names>Michael P.</given-names> <surname>Stryker</surname></string-name>, and <string-name><given-names>Kenneth D.</given-names> <surname>Miller</surname></string-name>. <article-title>Modeling the Dynamic Interaction of Hebbian and Homeostatic Plasticity</article-title>. <source>Neuron</source>, <volume>84</volume>(<issue>2</issue>):<fpage>497</fpage>–<lpage>510</lpage>, <month>October</month></mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><given-names>Eero P.</given-names> <surname>Simoncelli</surname></string-name> and <string-name><given-names>David J.</given-names> <surname>Heeger</surname></string-name>. <article-title>A model of neuronal responses in visual area MT</article-title>. <source>Vision Research</source>, <volume>38</volume>(<issue>5</issue>):<fpage>743</fpage>–<lpage>761</lpage>, <year>1998</year>. ISSN <issn>0042-6989</issn>. doi: <pub-id pub-id-type="doi">10.1016/S0042-6989(97)00183-1</pub-id>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><given-names>E. T.</given-names> <surname>Jaynes</surname></string-name>. <article-title>Information Theory and Statistical Mechanics</article-title>. <source>Physical Review</source>, <volume>106</volume>(<issue>4</issue>):<fpage>620</fpage>–<lpage>630</lpage>, <month>May</month> <year>1957</year>. ISSN <issn>0031-899X</issn>. doi: <pub-id pub-id-type="doi">10.1103/PhysRev.106.620</pub-id>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><given-names>Roozbeh</given-names> <surname>Kiani</surname></string-name>, <string-name><given-names>Christopher J.</given-names> <surname>Cueva</surname></string-name>, <string-name><given-names>John B.</given-names> <surname>Reppas</surname></string-name>, and <string-name><given-names>William T.</given-names> <surname>Newsome</surname></string-name>. <article-title>Dynamics of Neural Population Responses in Prefrontal Cortex Indicate Changes of Mind on Single Trials</article-title>. <source>Current Biology</source>, <volume>24</volume>(<issue>13</issue>):<fpage>1542</fpage>–<lpage>1547</lpage>, <month>July</month> <year>2014</year>. ISSN <issn>09609822</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2014.05.049</pub-id>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><given-names>Horace B</given-names> <surname>Barlow</surname></string-name>. <article-title>Possible principles underlying the transformation of sensory messages</article-title>. <source>Sensory communication</source>, <volume>1</volume>(<issue>01</issue>), <year>1961</year>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><given-names>Bruno A.</given-names> <surname>Olshausen</surname></string-name> and <string-name><given-names>David J.</given-names> <surname>Field</surname></string-name>. <article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title> <source>Vision Research</source>, <volume>37</volume>(<issue>23</issue>):<fpage>3311</fpage>–<lpage>3325</lpage>, <month>December</month> <year>1997</year>. ISSN <issn>00426989</issn>. doi: <pub-id pub-id-type="doi">10.1016/S0042-6989(97)00169-7</pub-id>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="book"><string-name><given-names>Ian</given-names> <surname>Goodfellow</surname></string-name>, <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>, and <string-name><given-names>Aaron</given-names> <surname>Courville</surname></string-name>. <source>Deep Learning</source>. <publisher-name>MIT Press</publisher-name>, <year>2016</year>.</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Supplementary Figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supp. Fig. 1.</label>
<caption><title>Sigmoid Random Projections models outperform Step Random Projections models.</title>
<p>(A) Average performance of models as a function of the number of projections, measured by log-likelihood. Sigmoid RP models outperform step RP models. (B-C) Mean firing rates of projection neurons and mean correlation between projections, step RP models show slightly lower correlations and firing rates compared to sigmoid RP models. In all panels shaded area denotes standard error over 100 models.</p></caption>
<graphic xlink:href="530392v2_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supp. Fig. 2.</label>
<caption><title>Homeostatic model variants show similar firing rates and correlation, but different rotation angles.</title>
<p>(A) and (B) Mean correlation between the projection neurons and firing rates as a function of the model cost of the three different homeostatic models we compared. All three homeostatic models show a similar behavior. Note that the standard errors over 100 models are smaller than the marker’s size and are, therefore, invisible. (C) and (D) Histograms of rotation angles of the projections after learning in the case of the unconstrained reshape models and different homeostatic input (C) and homeostatic circuit (D) models. The legend specifies the constraint type and strength, namely, the specific value of <italic>ϕ</italic>.</p></caption>
<graphic xlink:href="530392v2_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.96566.1.sa1</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sharpee</surname>
<given-names>Tatyana O</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Salk Institute for Biological Studies</institution>
</institution-wrap>
<city>La Jolla</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This work is an <bold>important</bold> contribution to the development of a biologically plausible theory of statistical modeling of spiking activity. The authors <bold>convincingly</bold> implemented the statistical inference of input likelihood in a simple neural circuit, demonstrating the relationship between synaptic homeostasis, neural representations, and computational accuracy. This work will be of interest to neuroscientists, both theoretical and experimental, who are exploring how statistical computation is implemented in neural networks. There are questions about the performance of the methods in the case where other biologically significant parameters, such as firing rate and thresholds, are optimized together with the synaptic weights.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.96566.1.sa0</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>A novel statistical model of neural population activity called the Random Projection model has been recently proposed. Not only is this model accurate, efficient, and scalable, but also is naturally implemented as a shallow neural network. This work proposes a new class of RP model called the reshaped RP model. Inheriting the virtue of the original RP model, the proposed model is more accurate and efficient than the original, as well as compatible with various biological constraints. In particular, the authors have demonstrated that normalizing the total synaptic input in the reshaped model has a homeostatic effect on the firing rates of the neurons, resulting in even more efficient representations with equivalent computational accuracy. These results suggest that synaptic normalization contributes to synaptic homeostasis as well as efficiency in neural encoding.</p>
<p>Strengths</p>
<p>
This paper demonstrates that the accuracy and efficiency of the random projection models can be improved by extending the model with reshaped projections. Furthermore, it broadens the applicability of the model under biological constraints of synaptic regularization. It also suggests the advantage of the sparse connectivity structure over the fully connected model for modeling spiking statistics. In summary, this work successfully integrates two different elements, statistical modeling of the spikes and synaptic homeostasis in a single biologically plausible neural network model. The authors logically demonstrate their arguments with clear visual presentations and well-structured text, facilitating an unambiguous understanding for readers.</p>
<p>Weaknesses</p>
<p>
It would be helpful if the following issues about the major claims of the manuscript could be expanded and/or clarified:</p>
<p>(1) We find it interesting that the reshaped model showed decreased firing rates of the projection neurons. We note that maximizing the entropy &lt;-ln p(x)&gt; with a regularizing term -\lambda &lt;\sum _i f(x_i)&gt;, which reflects the mean firing rate, results in \lambda _i = \lambda for all i in the Boltzmann distribution. In other words, in addition to the homeostatic effect of synaptic normalization which is shown in Figures 3B-D, setting all \lambda_i = 1 itself might have a homeostatic effect on the firing rates. It would be better if the contribution of these two homeostatic effects be separated. One suggestion is to verify the homeostatic effect of synaptic normalization by changing the value of \lambda.</p>
<p>(2) As far as we understand, \theta_i (thresholds of the neurons) are fixed to 1 in the article. Optimizing the neural threshold as well as synaptic weights is a natural procedure (both biologically and engineeringly), and can easily be computed by a similar expression to that of a_ij (equation 3). Do the results still hold when changing \theta _i is allowed as well? For example,</p>
<p>a. If \theta _i becomes larger, the mean firing rates will decrease. Does the backprop model still have higher firing rates than the reshaped model when \theta _i are also optimized?</p>
<p>b. Changing \theta _i affects the dynamic range of the projection neurons, thus could modify the effect of synaptic constraints. In particular, does it affect the performance of the bounded model (relative to the homeostatic input models)?</p>
<p>(3) In Figure 1, the authors claim that the reshaped RP model outperforms the RP model. This improved performance might be partly because the reshaped RP model has more parameters to be optimized than the RP model. Indeed, let the number of projections N and the in-degree of the projections K, then the RP model and the reshaped RP model have N and KN parameters, respectively. Does the reshaped model still outperform the original one when only (randomly chosen) N weights (out of a_ij) are allowed to be optimized and the rest is fixed? (or, does it still outperform the original model with the same number of optimized parameters (i.e. N/K neurons)?)</p>
<p>(4) In Figure 2, the authors have demonstrated that the homeostatic synaptic normalization outperforms the bounded model when the allowed synaptic cost is small. One possible hypothesis for explaining this fact is that the optimal solution lies in the region where only a small number of |a_ij| is large and the rest is near 0. If it is possible to verify this idea by, for example, exhibiting the distribution of a_ij after optimization, it would help the readers to better understand the mechanism behind the superiority of the homeostatic input model.</p>
<p>(5) In Figures 5D and 5E, the authors present how different reshaping constraints result in different learning processes (&quot;rotation&quot;). We find these results quite intriguing, but it would help the readers understand them if there is more explanation or interpretation. For example,</p>
<p>a. In the &quot;Reshape - Hom. circuit 4.0&quot; plot (Fig 5D, upper-left), the rotation angle between the two models is almost always the same. This is reasonable since the Homeostatic Circuit model is the least constrained model and could be almost irrelevant to the optimization process. Is there any similar interpretation to the other 3 plots of Figure 5D?</p>
<p>b. In Figure 5E, is there any intuitive explanation for why the three models take minimum rotation angle at similar global synaptic cost (~0.3)?</p>
</body>
</sub-article>
</article>