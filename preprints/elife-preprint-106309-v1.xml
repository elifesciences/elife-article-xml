<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106309</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106309</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106309.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A theory of brain-computer interface learning via low-dimensional control</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5005-8791</contrib-id>
<name>
<surname>Menéndez</surname>
<given-names>Jorge A</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>j.audi11@gmail.com</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hennig</surname>
<given-names>Jay A</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Golub</surname>
<given-names>Matthew D</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Oby</surname>
<given-names>Emily R</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sadtler</surname>
<given-names>Patrick T</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Batista</surname>
<given-names>Aaron P</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chase</surname>
<given-names>Steven M</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yu</surname>
<given-names>Byron M</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Latham</surname>
<given-names>Peter E</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Gatsby Computational Neuroscience Unit, University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Harvard University</institution></institution-wrap>, <city>Cambridge</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap>, <city>Seattle</city>, <country country="US">United States</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an3r305</institution-id><institution>University of Pittsburgh</institution></institution-wrap>, <city>Pittsburgh</city>, <country country="US">United States</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap>, <city>Pittsburgh</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-05-14">
<day>14</day>
<month>05</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106309</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-02-25">
<day>25</day>
<month>02</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-01-26">
<day>26</day>
<month>01</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.18.589952"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Menéndez et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Menéndez et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106309-v1.pdf"/>
<abstract>
<p>A remarkable demonstration of the flexibility of mammalian motor systems is primates’ ability to learn to control brain-computer interfaces (BCIs). This constitutes a completely novel motor behavior, yet primates are capable of learning to control BCIs under a wide range of conditions. BCIs with carefully calibrated decoders, for example, can be learned with only minutes to hours of practice. With a few weeks of practice, even BCIs with randomly constructed decoders can be learned. What are the biological substrates of this learning process? Here, we develop a theory based on a re-aiming strategy, whereby learning operates within a low-dimensional subspace of task-relevant inputs driving the local population of recorded neurons. Through comprehensive numerical and formal analysis, we demonstrate that this theory can provide a unifying explanation for disparate phenomena previously reported in three different BCI learning tasks, and we derive a novel experimental prediction that we verify with previously published data. By explicitly modeling the underlying neural circuitry, the theory reveals an interpretation of these phenomena in terms of biological constraints on neural activity.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Minor fixes and clarifications to text.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>A core property of mammalian motor systems is their capacity to adapt to novel environments. Through learning, mammals are able to tailor their movements to an astonishing variety of previously unexperienced tasks, often needing only minutes to hours of practice to do so.<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c7">7</xref></sup> A particularly remarkable demonstration of this is offered by brain-computer interfaces (BCIs), where the movement of a cursor on a screen is determined by cortical activity via an external decoder.<sup><xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c10">10</xref></sup> Despite the unfamiliarity of this motor task, human and non-human primates are capable of learning to control BCIs under a wide range of conditions, often with little practice. With a carefully calibrated BCI decoder, proficient control of the BCI cursor can be learned after only minutes of experience.<sup><xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c14">14</xref></sup> But even effectively random BCI decoders can be learned as well, provided the subject undergoes a more extensive training procedure (e.g. a few weeks).<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup> The purpose of this study is to develop a theory of the algorithm(s) underlying this learning process.</p>
<p>Previous models of motor cortical BCI learning have postulated that synaptic plasticity within motor cortex underlies learning a BCI.<sup><xref ref-type="bibr" rid="c17">17</xref>–<xref ref-type="bibr" rid="c20">20</xref></sup> Indeed, models of the synaptic connectivity required for a recurrent network to solve a BCI reaching task<sup><xref ref-type="bibr" rid="c19">19</xref></sup> and the plasticity rules by which that connectivity might be learned<sup><xref ref-type="bibr" rid="c20">20</xref></sup> can account for slow and fast learning of different BCI decoders. However, a fundamental limitation of synaptic plasticity is the curse of dimensionality: motor cortex contains trillions of synapses, so learning via optimization of their weights would entail solving an extremely high-dimensional optimization problem. In the best of cases – when the objective function and its gradient are explicitly known – solving such problems typically requires vast amounts of training data. In the case of BCI learning, the subject’s motor system has no explicit access to the BCI decoder, so the relationship between internal neural activity and movement – and, by extension, task performance – is unknown. This means that gradients of task performance with respect to internal biological parameters must be estimated through trial and error,<sup><xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c21">21</xref></sup> which is notoriously slow in high dimensional spaces.<sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup> Moreover, this estimation problem is made even more difficult by the biological constraints of neurons and synapses, which impose noise in the learning signals available to each synapse<sup><xref ref-type="bibr" rid="c24">24</xref></sup> and preclude synaptic plasticity rules from back-propagating gradients through the many layers of neural circuitry.<sup><xref ref-type="bibr" rid="c25">25</xref>–<xref ref-type="bibr" rid="c27">27</xref></sup> These considerations suggest that BCI learning by synaptic plasticity in motor cortex should be slow and highly limited.</p>
<p>Such slow and limited learning is inconsistent with the strikingly fast and flexible learning observed in many BCI experiments, where non-human primates are observed to achieve proficient control after only a single session of 10’s to 100’s of trials of practice.<sup><xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup> Moreover, the hypothesis that motor cortex undergoes substantial synaptic changes over learning is inconsistent with two additional observations. First, the statistical structure of motor cortical activity remains remarkably conserved after learning: the repertoire of activity patterns employed for BCI control is unchanged after training on a new decoder for a few hours,<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup> and single neuron tuning to manual reaches also remains largely unchanged after performing a BCI reaching task.<sup><xref ref-type="bibr" rid="c31">31</xref></sup> Second, learning a BCI task can occur without interfering with natural limb control<sup><xref ref-type="bibr" rid="c31">31</xref></sup> (but see<sup><xref ref-type="bibr" rid="c32">32</xref></sup>).</p>
<p>Together, these observations suggest that synaptic plasticity in motor cortex is not the primary mechanism underlying BCI learning, at least for the short timescales of learning observed in the studies cited above. Instead, they suggest that the brain might take a more parsimonious learning strategy, in which (1) learning is reduced to a low-dimensional optimization problem to enable data-efficient learning, and (2) the motor cortical machinery for natural movements is kept intact.</p>
<p>A learning strategy that satisfies these two criteria is that of “re-aiming”<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup> or “intrinsic variable learning”.<sup><xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup> Under this strategy, the animal exploits the pre-existing motor cortical circuitry by learning an association between intended BCI movements and internal motor commands that would otherwise be used during natural motor behavior. For example, if the BCI decoder were such that motor cortical activity generated during a <italic>leftward</italic> arm reach would lead to an <italic>upward</italic> BCI movement, then the animal would learn to employ the motor command usually reserved for <italic>leftward</italic> arm reaches to achieve this <italic>upward</italic> BCI movement (<xref rid="fig1" ref-type="fig">fig. 1a</xref>). This strategy satisfies criteria 1 and 2 above: the dimensionality of the learning problem is kept low because both BCI movements – typically movements of a 2D or 3D cursor – and natural motor commands<sup><xref ref-type="bibr" rid="c36">36</xref>–<xref ref-type="bibr" rid="c41">41</xref></sup> are low-dimensional, and the motor cortical circuit can be kept intact because the patterns of activity used for manual and BCI control are the same.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>The re-aiming learning strategy.</title>
<p>a. Re-aiming strategy for BCI learning. If activity evoked by imagining a leftward planar movement moves the BCI cursor right, then the animal learns to use this motor command to move the cursor to the right. Critically, the space of imagined planar movements is low-dimensional.</p>
<p>b. Proposed model of re-aiming. Upstream inputs to motor cortex, <italic>u</italic><sub><italic>j</italic></sub>, depend on a low-dimensional motor command vector, <bold><italic>θ</italic></bold> (depicted here as two-dimensional). The BCI readout, <bold>y</bold>, is a 2D linear readout of motor cortical firing rates through a decoding matrix, <bold>D</bold>. Re-aiming is formalized as identifying the motor command, <bold><italic>θ</italic></bold>, that ensures the BCI readout gets as close as possible to a given target readout, <bold>y</bold><sup>*</sup> (cf. <xref ref-type="disp-formula" rid="eqn4">equation 4</xref>).</p></caption>
<graphic xlink:href="589952v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Previous experimental results have suggested that re-aiming can account for some<sup><xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup> but not all<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup> of the changes in motor cortical activity that occur after learning a novel BCI decoder. However, this evidence has typically been interpreted through the lens of a feed-forward spatial tuning curve model of motor cortex, which does not take into account the influence of additional motor variables beyond reach direction, and omits biological constraints on the dynamics of cortical circuits. Here, we address these limitations by modeling motor cortex as a non-linear recurrently connected network of neurons and modeling re-aiming as an optimization over low-dimensional motor commands driving this network. Via simulation and analysis, we derive predictions of this theory about how neural activity and behavior should change under a pure re-aiming learning strategy, for three distinct BCI learning tasks. These predictions reveal a potentially unifying explanation of disparate phenomena observed in BCI learning.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>Re-aiming as optimization of low-dimensional inputs to motor cortex</title>
<p>We begin by modeling motor cortex as a recurrent neural network driven by an upstream population of neurons (<xref rid="fig1" ref-type="fig">fig. 1b</xref>),
<disp-formula id="eqn1a">
<graphic xlink:href="589952v2_eqn1a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn1b">
<graphic xlink:href="589952v2_eqn1b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>r</italic><sub>1</sub>, <italic>r</italic><sub>2</sub>, …, <italic>r</italic><sub><italic>N</italic></sub> and <italic>u</italic><sub>1</sub>, <italic>u</italic><sub>2</sub>, …, <italic>u</italic><sub><italic>M</italic></sub> denote the firing rates of the motor cortical and upstream neurons, respectively. A rectified linear activation function <italic>ϕ</italic>(±) is used to ensure that firing rates are strictly non-negative. We assume that firing rates are low at the start of each trial of BCI control, and thus set the initial conditions to 0, <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic> = 0) = 0. The weights <inline-formula><inline-graphic xlink:href="589952v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="589952v2_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> represent the strengths of the synaptic connections between neurons within motor cortex and from the upstream population to motor cortex, respectively. To avoid making any strong commitments about the structure of these connections, we use randomly connected networks throughout the main text; simulations with other, more realistic, connectivity patterns yield similar results (see <xref rid="figS1" ref-type="fig">Supplementary Figure S1</xref>).</p>
<p>Next, we consider the upstream inputs to motor cortex, {<italic>u</italic><sub><italic>i</italic></sub>(<italic>t</italic>)}. Inspired by recent models and theories of motor cortex,<sup><xref ref-type="bibr" rid="c42">42</xref>–<xref ref-type="bibr" rid="c46">46</xref></sup> we assume that the rich intrinsic dynamics of the local motor cortical circuit suffice to generate the complex patterns of cortical activity necessary to execute a given motor behavior. Which behavior is executed at a given time is selected by an upstream “motor command” that drives motor cortex via these upstream inputs. These inputs are therefore assumed to fluctuate on a much slower timescale than the motor cortical firing rates they drive. In the analysis presented below, we take these to be constant in time; results for more complex input dynamics are presented in Supplementary Materials <xref ref-type="sec" rid="s5a6">Section S.1.6</xref>.</p>
<p>Motivated by the fact that motor behaviors are generally low-dimensional,<sup><xref ref-type="bibr" rid="c36">36</xref>–<xref ref-type="bibr" rid="c41">41</xref></sup> we assume that the motor commands setting these inputs also have low dimensionality. We formalize this by representing the motor command as a <italic>K</italic>-dimensional vector, <bold><italic>θ</italic></bold> ∈ ℝ <sup><italic>K</italic></sup>, constituted by <italic>K ≪ N command variables θ</italic><sub>1</sub>, <italic>θ</italic><sub>2</sub>, …, <italic>θ</italic><sub><italic>K</italic></sub>. These command variables could correspond to extrinsic motor variables, such as reach speed or direction, or to more abstract motor-related information, such as parameters of prepared, observed, or imagined movements. Fundamentally, we make no commitments as to the nature of these intrinsic command variables beyond them influencing the upstream activity driving motor cortex. This assumption is formalized by having the upstream firing rates depend on the low-dimensional motor command via a set of encoding weights, <italic>U</italic><sub><italic>ij</italic></sub>,
<disp-formula id="eqn2">
<graphic xlink:href="589952v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The rectified linear activation function, <italic>ϕ</italic>(±), is again used here to enforce non-negative firing rates. For simplicity, we set the encoding weights <italic>U</italic><sub><italic>ij</italic></sub> randomly.</p>
<p>During BCI control, motor cortical firing rates, <bold>r</bold>(<italic>t</italic>) = (<italic>r</italic><sub>1</sub>(<italic>t</italic>) ···<italic>r</italic><sub><italic>N</italic></sub> (<italic>t</italic>)) ∈ ℝ <sup><italic>N</italic></sup>, are directly translated to behavior of an external effector (e.g. a cursor on a screen) through a linear readout,
<disp-formula id="eqn3">
<graphic xlink:href="589952v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
As is typically done in BCI experiments with linear decoders, we include a constant offset <bold>c</bold> to center the strictly positive firing rates (see Methods <xref ref-type="sec" rid="s4h">Section 4.8</xref>). The readout, <bold>y</bold>(<italic>t</italic>), determines behavior in the BCI task by specifying, for example, the position<sup><xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup> or velocity<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c49">49</xref></sup> of a cursor. Regardless of how exactly how the readout maps to cursor movements, performing a given task (e.g. moving the cursor towards a target) demands a particular sequence of target readouts, which we denote by <bold>y</bold><sup>*</sup>(<italic>t</italic>). A subject learning to perform a BCI task with a given decoder must therefore find a way to generate motor cortical activity patterns that will produce these target readouts.</p>
<p>Our hypothesis is that subjects do so only by optimizing the upstream motor commands, <bold><italic>θ</italic></bold>. A key feature of this learning strategy is that it reduces the dimensionality of the learning problem. That reduction can be huge: from the number of synaptic weights to the number of command variables specifying the motor command, <italic>K</italic> – a factor that can easily reach 10<sup>9</sup>. Moreover, not all <italic>K</italic> command variables need to be optimized – we will argue below that, in certain settings, subjects may be optimizing only a subset of the task-relevant command variables, sometimes as few as 2. Such a reduction in the number of optimized parameters allows efficient learning in the absence of gradient information. However, it also limits the space of available solutions to the BCI task. Here we develop a formal theory of re-aiming to understand the implications of these limitations, and, importantly, show that they are consistent with empirical data.</p>
<p>We analyze a simplified model of re-aiming in which the motor command, <bold><italic>θ</italic></bold>, is optimized to produce a target readout, <bold>y</bold><sup>*</sup>, at a single endpoint time, <italic>t</italic><sub>end</sub>,
<disp-formula id="eqn4">
<graphic xlink:href="589952v2_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The vector <bold>y</bold>(<italic>t</italic><sub>end</sub>; <bold><italic>θ</italic></bold>) is the BCI readout at time <italic>t</italic><sub>end</sub> resulting from driving the model motor cortical network with the motor command <bold><italic>θ</italic></bold>. The integer <inline-formula><inline-graphic xlink:href="589952v2_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denotes the number of command variables optimized by re-aiming; for simplicity, the remaining command variables that are not optimized, <inline-formula><inline-graphic xlink:href="589952v2_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, are set to 0. The second term on the right-hand side quantifies the metabolic cost of the upstream firing rates induced by the motor command, <bold><italic>θ</italic></bold>, included in the objective function to ensure that only biologically plausible solutions are allowed.</p>
<p>Solutions to <xref ref-type="disp-formula" rid="eqn4">equation 4</xref> constitute a concrete hypothesis about <italic>what</italic> subjects learn. In the following, we analyze these optimal motor commands to evaluate whether this hypothesis is consistent with empirical observations from BCI learning experiments. The question of <italic>how</italic> subjects might learn these optimal motor commands is left for future work. We also briefly acknowledge here that <xref ref-type="disp-formula" rid="eqn4">equation 4</xref> constitutes an incomplete description of the true BCI learning problem, since controlling the BCI effector’s movement typically requires specifying a whole sequence of readouts over time (rather than at just one target time, <italic>t</italic><sub>end</sub>) and relies on closed-loop feedback of the effector’s state.<sup><xref ref-type="bibr" rid="c50">50</xref>–<xref ref-type="bibr" rid="c52">52</xref></sup> That said, this simplified model of re-aiming will prove useful to intuit general principles of the re-aiming learning strategy, which, as we show in Supplementary Materials <xref ref-type="sec" rid="s5a6">Section S.1.6</xref>, extend to more complex settings such as closed-loop control. After all, being able to produce a target readout at a fixed future time is, loosely, a pre-requisite to solving the full closed-loop control problem.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Re-aiming implies neural constraints on short-term learning</title>
<p>We begin by modelling the BCI experiment designed by Sadtler et al. (2014).<sup><xref ref-type="bibr" rid="c14">14</xref></sup> In this task, subjects learn to perform center-out movements with a 2D cursor on a screen, with the velocity of the cursor controlled by the readout from a linear BCI decoder, as in <xref ref-type="disp-formula" rid="eqn3">equation 3</xref>. Prior to learning, subjects first engage in a “calibration task”, in which neural activity is recorded while the subject passively views center-out cursor movements to eight radial targets (<xref rid="fig2" ref-type="fig">fig. 2a</xref>). Sadtler et al. observed that neural responses to these stimuli occupy a low-dimensional subspace, termed the “intrinsic manifold”. This subspace – identified via linear dimensionality reduction – is subsequently used to construct three types of BCI decoders.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>BCI learning task of Sadtler et al. (2014).</title>
<p>a. Schematic of task structure. Subjects first engage in a “calibration task” whereby they passively observe center-out cursor movements on a screen. Recorded neural activity in motor cortex is used to construct the baseline decoder and estimate the intrinsic manifold. Subjects are then instructed to perform center-out cursor movements under BCI control, first using the baseline decoder and then with a perturbed decoder, constructed by perturbing the baseline decoder. This perturbation can either preserve the baseline decoder’s alignment with the intrinsic manifold (a within-manifold perturbation, or WMP) or disrupt it (an outside-manifold perturbation, or OMP).</p>
<p>b. Low-dimensional illustration of the intrinsic manifold and its relationship to the decoders (defined in <xref ref-type="disp-formula" rid="eqn3">equation 3</xref>) used in this task. Colored dots represent activity patterns recorded during different trials of the calibration task, colored by the cursor velocity presented on that trial. The cursor velocities of these stimuli are depicted by color-matched arrows in the inset in the top right, with the cursor targets used in the subsequent cursor control task depicted by green diamonds. The evoked neural activity patterns reside predominantly within the two-dimensional plane depicted by the gray rectangle, the so-called intrinsic manifold. Three hypothetical one-dimensional decoders are depicted by colored arrows, labelled baseline decoder, WMP, and OMP. The corresponding component of the linear readouts, <italic>y</italic><sub>1</sub>, from these decoders can be visualized by projecting individual activity patterns onto the corresponding decoder vector. This is illustrated for one activity pattern marked in green, whose projections onto each of the three decoders is shown. Because this activity pattern resides close to the intrinsic manifold, it yields a large readout (i.e. far from the origin, at the intersection of the three decoders) from the baseline decoder and WMP, which are both well aligned with the intrinsic manifold. In contrast, this activity pattern’s readout through the OMP is much weaker (i.e. its projection onto this decoder is much closer to the origin), since this decoder is oriented away from the intrinsic manifold. It is important to keep in mind that this illustration is a simplified cartoon of the true task, in which the intrinsic manifold is higher-dimensional (8-12D instead of 2D) and the BCI task depends on two readouts (<italic>y</italic><sub>1</sub>, <italic>y</italic><sub>2</sub>) rather than one.</p></caption>
<graphic xlink:href="589952v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>First, a “baseline decoder” is constructed by fitting the decoding matrix, <bold>D</bold>, to the neural responses from the calibration task such that these activity patterns suffice to move the cursor towards the corresponding target in each trial. By construction, the baseline decoder is well aligned with the intrinsic manifold, such that activity patterns within this subspace can produce large readouts through this decoder (<xref rid="fig2" ref-type="fig">fig. 2b</xref>). Sadtler et al. found that, with this baseline decoder, non-human primate subjects can easily perform center-out cursor movements to the targets instantly, with no learning time required.</p>
<p>Next, the decoding matrix of the baseline decoder is perturbed and the subject is prompted to perform the same center-out cursor movements with the perturbed decoder. Two types of perturbations are used, which either preserve or disrupt the baseline decoder’s alignment with the intrinsic manifold: <italic>within</italic>-manifold perturbations (WMPs) randomly re-orient the baseline decoder <italic>within</italic> the intrinsic manifold, whereas <italic>outside</italic>-manifold perturbations (OMPs) randomly re-orient the baseline decoder <italic>outside</italic> the intrinsic manifold (<xref rid="fig2" ref-type="fig">fig. 2b</xref>). WMPs alter how neural activity <italic>within</italic> the intrinsic manifold subspace gets mapped to readouts, such that activity patterns in this subspace suffice to perform the task. Under an OMP, on the other hand, activity patterns within the intrinsic manifold are limited in the extent of readouts they can produce, so new activity patterns <italic>outside</italic> of the intrinsic manifold are needed to proficiently perform the task.</p>
<p>Sadtler et al. found that with 1-2 hours of practice (a few hundred trials), non-human primates can learn to successfully move the cursor to the targets with WMP decoders. In contrast, such short-term learning does not typically occur with OMP decoders, under which relatively little improvement is observed over this timespan. Here we argue that this limitation of short-term BCI learning is consistent with a re-aiming learning strategy. This low-dimensional learning strategy can account for the rapid learning achievable with WMPs, as well as the much slower learning exacted by OMPs.</p>
<p>To demonstrate this, we follow the experimental protocol outlined above, but with simulations of our motor cortical model (equations 1-<xref ref-type="disp-formula" rid="eqn2">2</xref>) rather than with animals. Our starting point, as with the experiments, is to estimate the intrinsic manifold from motor cortical firing rates recorded during the calibration task, in which the subject passively views center-out cursor movements to each of the eight radial targets, <inline-formula><inline-graphic xlink:href="589952v2_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.We simulated neural responses to these stimuli by driving the model network with command variables set to the cursor’s constant velocity on each trial: the first two command variables, <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub>, set to the coordinates of the given target, <inline-formula><inline-graphic xlink:href="589952v2_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,and the remaining command variables, <italic>θ</italic><sub>3</sub>, <italic>θ</italic><sub>4</sub>, …, <italic>θ</italic><sub><italic>K</italic></sub>, set to 0. We then used Principal Components Analysis (PCA) to find the minimal subspace containing 95% of the variance over the resulting firing rates, which we found to be 8-dimensional (<xref rid="fig3" ref-type="fig">fig. 3g</xref>). We then defined the intrinsic manifold to be this subspace and used it to construct the baseline decoder and the two types of perturbed decoders (WMPs and OMPs), following the procedures of Sadtler et al. (see Methods <xref ref-type="sec" rid="s4h">Section 4.8</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Re-aiming with two command variables suffices to learn good solutions for within-but not outside-manifold perturbations.</title>
<p>a. Optimal motor commands, <inline-formula><inline-graphic xlink:href="589952v2_inline202.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,for the baseline decoder and one example WMP and OMP, plotted in <italic>θ</italic><sub>1</sub>-<italic>θ</italic><sub>2</sub> space. The shade of green indexes the target readout, <inline-formula><inline-graphic xlink:href="589952v2_inline203.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,that each motor command is optimized for, corresponding to the target readouts plotted in the adjacent panel (green diamonds in <xref rid="fig3" ref-type="fig">fig. 3b</xref>).</p>
<p>b. Readouts generated at time <italic>t</italic><sub>end</sub> by the optimal motor commands shown in the previous panel (<xref rid="fig3" ref-type="fig">fig. 3a</xref>), i.e. <inline-formula><inline-graphic xlink:href="589952v2_inline204.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.Green diamonds mark the eight target readouts for the center-out cursor control task, set to the directions of the eight radial cursor targets used by Sadtler et al. (2014).</p>
<p>c. Readouts from each of the reachable manifold activity patterns plotted in <xref rid="fig3" ref-type="fig">fig. 3f</xref>, with matched marker colors and sizes. The diamonds denote the eight target readouts as in <xref rid="fig3" ref-type="fig">fig. 3b</xref>. Note that the reachable readouts closest to the targets do not necessarily match the readouts produced by the optimal motor commands (<xref rid="fig3" ref-type="fig">fig. 3b</xref>), as the optimal motor commands are optimized to minimize the metabolic cost of the upstream input as well the readout error (cf. <xref ref-type="disp-formula" rid="eqn4">equation 4</xref>).</p>
<p>d. Distribution of mean squared error achieved by the optimal motor commands for 100 randomly sampled WMP’s and OMP’s. The mean squared error achieved by the optimal motor commands for the baseline decoder from which these perturbations are derived is marked by the vertical dashed black line. Target readouts are unit norm, so a mean squared error of 1.0 is equivalent to producing readouts at the origin.</p>
<p>e. Motor commands covering a range of angles on the <italic>θ</italic><sub>1</sub> − <italic>θ</italic><sub>2</sub> plane and 5 norms, ∥<bold><italic>θ</italic></bold>∥ ∈ {0.1, 0.4, 0.7, 1.0, <italic>s</italic><sub>max</sub>}, with <italic>s</italic><sub>max</sub> <italic>≈</italic> 1.25 (see Methods <xref ref-type="sec" rid="s4d">Section 4.4</xref> for how this was chosen). The motor commands used to simulate the calibration task are indicated by the pink/purple squares. All other command variables, <italic>θ</italic><sub>3</sub>, <italic>θ</italic><sub>4</sub>, …, <italic>θ</italic><sub><italic>K</italic></sub>, are fixed to 0.</p>
<p>f. Activity patterns in the reachable manifold at endpoint time <italic>t</italic><sub>end</sub> = 1000ms. Each ring of activity patterns is generated by the corresponding ring of color- and size-matched motor commands in the previous panel. This ensemble of <italic>N</italic> - dimensional activity patterns is projected onto its top three principal components. The black line is drawn to facilitate visualization of the 3D structure of this conical manifold. Note that the points in this plot should not be thought of as spatiotemporal trajectories of activity; rather, they depict activity patterns <italic>at the same timepoint</italic> generated by different motor commands.</p>
<p>g. Purple curve: cumulative variance in reachable manifold activity patterns along each intrinsic manifold dimension (<xref ref-type="disp-formula" rid="eqn25">equation 25</xref>). Gray curve: cumulative variance in calibration task neural responses. By construction, the intrinsic manifold contains 95% of the total variance of the calibration task neural responses (Methods <xref ref-type="sec" rid="s4h">Section 4.8</xref>).</p></caption>
<graphic xlink:href="589952v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Our hypothesis is that subjects learn to control the cursor by re-aiming with the same two command variables driving the calibration task responses, <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub>. We thus model BCI learning by optimizing <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub> with respect to the re-aiming objection function (<xref ref-type="disp-formula" rid="eqn4">equation 4</xref>, with<inline-formula><inline-graphic xlink:href="589952v2_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula>), leaving the remaining command variables fixed to 0 as in the calibration task (<italic>θ</italic><sub>3</sub> = <italic>θ</italic><sub>4</sub> = … = <italic>θ</italic><sub><italic>K</italic></sub> = 0). As only two variables need to be optimized, learning should proceed very efficiently. The motor commands available for BCI control, however, are now severely constrained: only two command variables are free to change, and they are bounded by the metabolic cost incurred by the upstream firing rates (the second term in <xref ref-type="disp-formula" rid="eqn4">equation 4</xref>).</p>
<p>To see how this affects performance in this BCI learning task, we simulate re-aiming for WMP and OMP decoders. For each decoder and target readout, we solve <xref ref-type="disp-formula" rid="eqn4">equation 4</xref> with <inline-formula><inline-graphic xlink:href="589952v2_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (setting <italic>t</italic><sub>end</sub> = 1000 ms, roughly matching the ∼700-1000 ms target acquisition times observed in experiments, and setting <italic>γ</italic> to its largest possible value guaranteeing good performance with the baseline decoder, cf. Methods <xref ref-type="sec" rid="s4c">Section 4.3</xref>) and drive the motor cortical network with the resulting optimal motor commands, <inline-formula><inline-graphic xlink:href="589952v2_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.The optimal motor commands for the baseline decoder and an example WMP and OMP are shown in <xref rid="fig3" ref-type="fig">fig. 3a</xref>, as vectors in <italic>θ</italic><sub>1</sub>-<italic>θ</italic><sub>2</sub> space. The readouts produced by driving motor cortex with these motor commands, <inline-formula><inline-graphic xlink:href="589952v2_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,are shown in adjacent panels to the right (<xref rid="fig3" ref-type="fig">fig. 3b</xref>), with the corresponding target readouts underlaid. We find that, for the baseline decoder and WMP, most of these optimally driven readouts reach their targets; for the OMP, on the other hand, most of them fall far short. In <xref rid="fig3" ref-type="fig">fig. 3d</xref>, we repeat this simulation for 100 randomly sampled WMP and OMP decoders (see Methods <xref ref-type="sec" rid="s4h">Section 4.8</xref> for the sampling procedure, closely matching that used by Sadtler et al.), and in each case quantify re-aiming success using the mean squared error between the optimally driven readouts and their corresponding targets. We find that the mean squared error is consistently lower for WMP decoders than for OMP decoders, as it was for the representative examples in <xref rid="fig3" ref-type="fig">fig. 3b</xref>.</p>
<p>Why does re-aiming fail to produce good readouts through OMPs? The answer lies in the constraints the re-aiming strategy imposes on the set activity patterns available in motor cortex for BCI control. To visualize and characterize these, we consider the <italic>reachable manifold</italic> : the set of all motor cortical activity patterns at a fixed endpoint time, <bold>r</bold>(<italic>t</italic><sub>end</sub>; <bold><italic>θ</italic></bold>), that can be reached by a motor command, <bold><italic>θ</italic></bold>, which is accessible by re-aiming; that is, with <italic>θ</italic><sub><italic>k</italic></sub> = 0 for all <italic>k &gt;</italic> 2 and with its norm, ∥<bold><italic>θ</italic></bold>∥, constrained by the quadratic metabolic cost (which we enforce here with a hard upper bound, ∥<bold><italic>θ</italic></bold>∥ ≤ <italic>s</italic><sub>max</sub>, set to the maximum norm of the re-aiming solutions to all sampled decoder perturbations; cf. Methods <xref ref-type="sec" rid="s4d">Section 4.4</xref>). A large set of these accessible motor commands are shown in <xref rid="fig3" ref-type="fig">fig. 3e</xref>, and the corresponding activity patterns they generated are shown to the right in <xref rid="fig3" ref-type="fig">fig. 3f</xref>, projected down to three dimensions via PCA. Note that, despite the accessible motor commands being two-dimensional, the reachable manifold occupies more than two dimensions of state space, due to non-linearities in the dynamics of the motor cortical network. The three-dimensional projection in <xref rid="fig3" ref-type="fig">fig. 3f</xref> in fact contains about 80% of the variance over the <italic>N</italic> -dimensional activity patterns, revealing that the reachable manifold occupies in a moderately low-dimensional linear subspace – higher than that of the motor commands giving rise to it (two-dimensional) but significantly lower than that of its ambient state space (<italic>N</italic> -dimensional).</p>
<p>In fact, the reachable manifold is almost completely contained within the intrinsic manifold subspace. This is quantified in <xref rid="fig3" ref-type="fig">fig. 3g</xref>, which reveals that the eight dimensions of the intrinsic manifold subspace capture almost 100% of the variance in reachable activity patterns. This is unsurprising given that both the activity patterns in the reachable manifold and the activity patterns evoked by the calibration task – which define the intrinsic manifold – are generated by similarly low-dimensional motor commands <bold><italic>θ</italic></bold>, in which only two command variables (<italic>θ</italic><sub>1</sub>, <italic>θ</italic><sub>2</sub>) are non-zero. <xref rid="fig3" ref-type="fig">Fig. 3e</xref> shows this directly by overlaying the calibration task inputs on the accessible motor commands. Ultimately, what this entails is that there are virtually no reachable activity patterns outside of the intrsinsic manifold; no activity patterns outside of the intrinsic manifold are accessible via re-aiming. This explains why this learning strategy would fail to produce large readouts through OMPs.</p>
<p>To confirm this, in <xref rid="fig3" ref-type="fig">fig. 3c</xref> we visualize the set of readouts reachable by re-aiming, for the baseline decoder, WMP, and OMP from <xref rid="fig3" ref-type="fig">fig. 3a</xref> and <xref rid="fig3" ref-type="fig">3b</xref>. Specifically, we plot the readouts from each of the reachable activity patterns shown in <xref rid="fig3" ref-type="fig">fig. 3f</xref>, providing a comprehensive visualization of the space of readouts that can be reached through each decoder by re-aiming. As expected from the fact that the reachable manifold resides solely within the intrinsic manifold, we see that the readouts reachable under the baseline and WMP decoders cover a wider area than those reachable under the OMP decoder. The targets are thus enclosed by the baseline and WMP decoder reachable readouts, but remain out of reach of the OMP decoder. The re-aiming learning strategy therefore fails to solve the task with this OMP decoder, as none of the motor commands accessible under this learning strategy can reach the target readouts.</p>
<p>We conclude that re-aiming with only two variables (<italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub>) can lead to successful BCI control with WMP decoders but not with OMP decoders. This offers an explanation for why only WMPs are learnable on the short timescale of a single experimental session. Because such low-dimensional re-aiming can’t succeed for OMPs, subjects must resort to an alternative – and presumably higher-dimensional – learning strategy, explaining why it requires substantially more training to learn these.<sup><xref ref-type="bibr" rid="c16">16</xref></sup></p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Re-aiming predicts biases in short-term learning</title>
<p>A close look at <xref rid="fig3" ref-type="fig">fig. 3c</xref> reveals an important difference between the baseline and the WMP decoders: the readouts reachable with the baseline decoder cover the readout space symmetrically while those reachable with the WMP decoder do not (compare <xref rid="fig3" ref-type="fig">figs. 3ci</xref> and <xref rid="fig3" ref-type="fig">3cii</xref>). In other words, larger readouts are reachable in some directions than in others. Such biases in reachable readouts are not unique to this particular WMP decoder; <xref rid="fig4" ref-type="fig">fig. 4a</xref> reveals similar asymmetries in the readouts reachable through three other representative WMP decoders.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Re-aiming predicts biases in readouts after short-term learning of within-manifold perturbation (WMP) decoders.</title>
<p>a. Readouts reachable through four representative WMP decoders, using the same color conventions as in <xref rid="fig3" ref-type="fig">fig. 3c</xref>. In each case, the four loops correspond to four distinct motor command norms, chosen to aid visualization. The leftmost panel corresponds to the example WMP decoder shown in <xref rid="fig3" ref-type="fig">fig. 3cii</xref>. The projection of the reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline205.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,is overlaid as an open arrow, arbitrarily rescaled for visibility.</p>
<p>b. Maximal cursor progress in each target direction as a function of angle with <inline-formula><inline-graphic xlink:href="589952v2_inline206.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, for the four example WMP decoders in panel a.</p>
<p>c. Maximal cursor progress in each target direction as a function of angle with <inline-formula><inline-graphic xlink:href="589952v2_inline207.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,for all 100 sampled WMPs (Pearson <italic>r</italic> = −0.66, <italic>p &lt;</italic> .001, <italic>N</italic> = 8 target directions × 100 sampled WMPs = 800). As was done for the experimental data in the next panel, the reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline208.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is estimated using simulated mean firing rates during baseline decoder control (see Methods <xref ref-type="sec" rid="s4f">Section 4.6</xref>).</p>
<p>d. Maximal cursor progress in each target direction as a function of angle with <inline-formula><inline-graphic xlink:href="589952v2_inline209.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,for all 46 sessions of WMP learning across three monkeys (Pearson <italic>r</italic> = −0.41, <italic>p &lt;</italic> .001, <italic>N</italic> = 8 target directions × 46 experimental sessions with WMP control = 368). Maximal cursor progress is estimated using the average cursor progress over the 50 contiguous trials with lowest acquisition times. The reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline210.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is estimated using mean firing rates over trials of baseline decoder control (see Methods <xref ref-type="sec" rid="s4f">Section 4.6</xref>).</p></caption>
<graphic xlink:href="589952v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The direction of this bias is moreover predictable: typically, the largest reachable readouts are in the direction of <inline-formula><inline-graphic xlink:href="589952v2_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (arrow overlaid on each plot), where <inline-formula><inline-graphic xlink:href="589952v2_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the centroid of the reachable manifold. This bias arises because of the non-negativity of firing rates, which permits the population firing rate, <bold>r</bold>, to grow widely away from the origin, but shrink towards the origin only up to a point, where it is truncated by the non-negativity. It is this property that endows the reachable manifold its conical structure (<xref rid="fig3" ref-type="fig">fig. 3f</xref>), whose centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, dictates the direction in which firing rates can grow the most under the re-aiming strategy. The projection of this direction through a given decoder, <inline-formula><inline-graphic xlink:href="589952v2_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,thus determines the direction in which the largest readouts can be reached by re-aiming (see Supplementary Materials <xref ref-type="sec" rid="s5a2">Section S.1.2</xref> for a more detailed analysis). In <xref rid="figS1" ref-type="fig">Supplementary Figure S1d</xref>, we show that – as long as firing rates are constrained to be non-negative (Supplementary Materials <xref ref-type="sec" rid="s5a3">Section S.1.3</xref>) – this bias arises across a large variety of motor cortical connectivity patterns and dynamics, suggesting that it is an unavoidable consequence of the re-aiming learning strategy. The absence of such a bias in experimental data would therefore provide strong evidence against this theory of BCI learning.</p>
<p>To quantify this experimental prediction, we used the “cursor progress” metric, <italic>ρ</italic>, introduced by Golub et al. (2018) to measure the degree to which a given readout, <bold>y</bold>, pushes the BCI cursor in a given target direction, <bold>y</bold><sup>*</sup>,
<disp-formula id="eqn5">
<graphic xlink:href="589952v2_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We then predict the maximum achievable cursor progress in each target direction,
<disp-formula id="eqn6">
<graphic xlink:href="589952v2_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where, as above, <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub> are the two command variables optimized by re-aiming and <italic>s</italic><sub>max</sub> is the bound on motor command norms imposed by the metabolic constraint in <xref ref-type="disp-formula" rid="eqn4">equation 4</xref> (cf. Methods <xref ref-type="sec" rid="s4d">Section 4.4</xref>). In <xref rid="fig4" ref-type="fig">fig. 4b</xref>, we plot this maximal cursor progress for each target readout, <inline-formula><inline-graphic xlink:href="589952v2_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,as a function of the target readout’s angle from <inline-formula><inline-graphic xlink:href="589952v2_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,for each of the four example WMPs. The negative correlation in each case confirms our above observation: higher cursor progress is reachable in target directions more aligned with <inline-formula><inline-graphic xlink:href="589952v2_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.In <xref rid="fig4" ref-type="fig">fig. 4c</xref>, we plot the maximal cursor progress in each target direction for all 100 sampled WMP decoders, revealing a statistically significant negative correlation across all sampled decoders (Pearson <italic>r</italic> = −0.66, <italic>p &lt;</italic> .001).</p>
<p>Does this predicted negative correlation also hold in the empirical data? To test this, we estimated the maximal cursor progress and reachable manifold centroid in each experimental session of WMP control. Maximal cursor progress in each target direction, <inline-formula><inline-graphic xlink:href="589952v2_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,was estimated using the average cursor progress in that target direction over the 50 contiguous WMP control trials with fastest target acquisition times (see Methods <xref ref-type="sec" rid="s4f">Section 4.6</xref>). The reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,was estimated using mean motor cortical firing rates during the block of baseline decoder control (see Methods <xref ref-type="sec" rid="s4f">Section 4.6</xref>), which in our model is highly correlated with the true reachable manifold centroid. We then replicated <xref rid="fig4" ref-type="fig">fig. 4c</xref> by plotting the empirically measured maximal cursor progress for each target direction as a function of the angle between the target direction and <inline-formula><inline-graphic xlink:href="589952v2_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,using our empirical estimates of maximal cursor progress and <inline-formula><inline-graphic xlink:href="589952v2_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from each experimental session. The data over all sessions are plotted in <xref rid="fig4" ref-type="fig">fig. 4d</xref>, revealing a significant negative correlation (Pearson <italic>r</italic> = −0.41, <italic>p &lt;</italic> .001) akin to that observed in our model. This confirms the existence of a statistically significant bias in the same direction predicted by our model of re-aiming.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Long-term BCI learning by generalized re-aiming</title>
<p>Although non-human primates struggle to control OMP decoders within a single experimental session (a few hundred trials),<sup><xref ref-type="bibr" rid="c14">14</xref></sup> they can in fact learn to do so when trained over multiple days (thousands of trials).<sup><xref ref-type="bibr" rid="c16">16</xref></sup> In this long-term learning paradigm, new motor cortical activity patterns emerge that allow the subjects to achieve good performance with OMP decoders. Could re-aiming play a role in the emergence of novel activity patterns over these longer timescales?</p>
<p>Since re-aiming with the two command variables evoked by the calibration task, <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub>, is not sufficient to produce the activity patterns required for OMP control, additional command variables will be required. We refer to a learning strategy that uses additional command variables as “generalized re-aiming”, and demonstrate below that this strategy can in fact achieve good performance with OMP decoders. Moreover, it can account for why learning is slower for these decoders: the search for optimal motor commands takes place in a higher-dimensional space beyond the narrow 2D space of command variables evoked by the calibration task.</p>
<p>To simulate generalized re-aiming, we simply increase the number of command variables used for re-aiming, <inline-formula><inline-graphic xlink:href="589952v2_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,and solve the resulting <inline-formula><inline-graphic xlink:href="589952v2_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula> -dimensional optimization problem in <xref ref-type="disp-formula" rid="eqn4">equation 4</xref>. In <xref rid="fig5" ref-type="fig">fig. 5a</xref> we plot the mean squared error achieved by the re-aiming solutions for each OMP decoder for each value of <inline-formula><inline-graphic xlink:href="589952v2_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.We find that as <inline-formula><inline-graphic xlink:href="589952v2_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> increases, a lower mean squared error is achieved, demonstrating that this learning strategy can be effective for OMP learning. For this model motor cortical network, re-aiming with about 15-20 command variables suffice to achieve a mean squared error as low as that achievable with WMP decoders using <inline-formula><inline-graphic xlink:href="589952v2_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.For other motor cortical models with different connectivity, fewer than 10 command variables suffice (<xref rid="figS1" ref-type="fig">Supplementary Figure S1e</xref>). These values of <inline-formula><inline-graphic xlink:href="589952v2_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula> comfortably fall in the range of the total number of extrinsic motor variables known to influence motor cortical activity.<sup><xref ref-type="bibr" rid="c53">53</xref>–<xref ref-type="bibr" rid="c59">59</xref></sup> However, they may be too high for naïve gradient-free optimization to succeed in solving <xref ref-type="disp-formula" rid="eqn4">equation 4</xref> under biological limitations (e.g. on memory, motivation, and noise), which might explain why primates seem to only be able to learn to control OMP decoders when provided with a structured incremental training paradigm.<sup><xref ref-type="bibr" rid="c16">16</xref></sup></p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Generalized re-aiming produces good solutions for outside-manifold perturbations (OMPs).</title>
<p>a. Mean squared error achieved by generalized re-aiming solutions for all sampled OMP decoders, plotted as a function of the number of command variables used for re-aiming,<inline-formula><inline-graphic xlink:href="589952v2_inline211.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Lighter blue points show the mean squared error for individual OMP decoders, darker open circles show the median over all sampled OMPs. For reference, dotted horizontal lines show the mean squared error achieved by re-aiming solutions with <inline-formula><inline-graphic xlink:href="589952v2_inline212.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the baseline decoder (black) and for WMP decoders (red); for WMP decoders, the median over all sampled decoders is shown with shading marking the upper and lower quartiles (corresponding to the values plotted in the red histogram in <xref rid="fig3" ref-type="fig">fig. 3d</xref>).</p>
<p>b. Participation ratio of the reachable manifold covariance (a measure of the effective dimensionality of the reachable manifold; see Methods <xref ref-type="sec" rid="s4d">Section 4.4</xref>, <xref ref-type="disp-formula" rid="eqn16">equation 16</xref>) as a function of the number of command variables used for re-aiming, <inline-formula><inline-graphic xlink:href="589952v2_inline213.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>c. Convex hull of OMP readouts reachable with different number of command variables, <inline-formula><inline-graphic xlink:href="589952v2_inline214.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,for the same OMP decoder shown in <xref rid="fig3" ref-type="fig">fig. 3c</xref>. The innermost ring <inline-formula><inline-graphic xlink:href="589952v2_inline215.gif" mime-subtype="gif" mimetype="image"/></inline-formula> corresponds to the convex hull of the reachable readouts plotted in <xref rid="fig3" ref-type="fig">fig. 3ciii</xref>.</p></caption>
<graphic xlink:href="589952v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Why generalized re-aiming works can be understood by looking at how increasing <inline-formula><inline-graphic xlink:href="589952v2_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula> changes the set of activity patterns reachable by re-aiming. A larger number of learnable command variables permits a more diverse set of upstream inputs, which in turn implies that a more diverse set of activity patterns are reachable. This diversity is quantified in <xref rid="fig5" ref-type="fig">fig. 5b</xref> by the participation ratio of the covariance of the reachable manifold (see Methods <xref ref-type="sec" rid="s4d">Section 4.4</xref>, <xref ref-type="disp-formula" rid="eqn17">equation 17</xref>). The participation ratio measures the extent to which variability is spread out over many dimensions (high participation ratio) or concentrated to only a few (low participation ratio).<sup><xref ref-type="bibr" rid="c60">60</xref></sup> We find that as <inline-formula><inline-graphic xlink:href="589952v2_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula> rises, the participation ratio of the reachable manifold covariance increases, indicating it occupies more and more dimensions of state space. That said, the participation ratio does begin to saturate at around <inline-formula><inline-graphic xlink:href="589952v2_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,reflecting the fact that the reachable manifold is ultimately limited by the smooth dynamics of the motor cortical network.</p>
<p>This expansion in the reachable manifold leads to the inclusion of new activity patterns that are useful for OMP control. We can see this in <xref rid="fig5" ref-type="fig">fig. 5c</xref>, which shows the readouts reachable through the same OMP visualized in <xref rid="fig3" ref-type="fig">fig. 3ciii</xref>. The readouts reachable under different values of <inline-formula><inline-graphic xlink:href="589952v2_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are plotted with different colors, revealing how re-aiming with a larger number of command variables allows the target readouts to be reached. As the reachable manifold expands, more and more activity patterns occupying dimensions relevant to OMP control become reachable, such that a wider set of readouts become reachable.</p>
</sec>
<sec id="s2e">
<label>2.5</label>
<title>Illusory credit assignment by generalized re-aiming</title>
<p>We now turn to a different class of BCI decoder perturbation, termed the credit assignment rotation perturbation.<sup><xref ref-type="bibr" rid="c61">61</xref></sup> We can think of the readout from a linear BCI decoder (<xref ref-type="disp-formula" rid="eqn3">equation 3</xref>) as summing together the <italic>N</italic> columns of the decoding matrix <bold>D</bold> (termed the “decoding vectors”), each one weighted by the activity of the corresponding neuron (<xref rid="fig6" ref-type="fig">fig. 6a</xref>, top). Under a credit assignment rotation perturbation, the decoding vectors of a random subset of neurons (the “rotated neurons”) are rotated by a given angle (<xref rid="fig6" ref-type="fig">fig. 6a</xref>, bottom). Errors induced by this decoder perturbation can be corrected by adjusting only the responses of the rotated neurons, while leaving the responses of the “non-rotated” neurons unchanged. But doing so would require solving the so-called credit assignment problem:<sup><xref ref-type="bibr" rid="c62">62</xref></sup> identifying which neurons’ decoding vectors were rotated – a tall order given that the subject has no explicit knowledge about the BCI decoder or the few motor cortical neurons (among millions) it records from.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Generalized re-aiming solutions reproduce motor cortical tuning changes observed under credit assignment rotation perturbations.</title>
<p>a. A linear BCI readout (<xref ref-type="disp-formula" rid="eqn3">equation 3</xref>) can be interpreted as summing together columns of the decoding matrix <bold>D</bold>, each weighted by the firing rate of the corresponding neuron (the centering term <bold>c</bold> has been dropped here for simplicity). These columns are called the neurons’ decoding vectors, and they are plotted on the axes below the equation. Under a credit assignment rotation perturbation, the decoding vectors of a subset of neurons (marked in purple) are rotated by a fixed angle (in this case 75<sup><italic>o</italic></sup> counter-clockwise). The neurons’ decoding vectors under this perturbed decoder are shown by dashed green arrows. The neurons whose decoding vectors are rotated are termed “rotated” neurons (in purple), the rest of the neurons that are recorded by the BCI are termed “non-rotated” neurons (in pink). Neurons that are not recorded by the BCI (i.e. whose decoding vectors are just a vector of 0’s, not depicted here) are termed “indirect” neurons.</p>
<p>b. Tuning curve of a representative example rotated neuron of our model, during cursor control with the baseline decoder (black) and with a credit assignment rotation perturbation (green). The dots show the time-averaged activity over <italic>t</italic><sub>end</sub> = 1000ms while the motor cortical network is driven by the re-aiming solutions for each respective decoder, using <inline-formula><inline-graphic xlink:href="589952v2_inline216.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the baseline decoder and <inline-formula><inline-graphic xlink:href="589952v2_inline217.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the perturbed decoder. Curves show tuning curves fit to these responses (Methods <xref ref-type="sec" rid="s4i">Section 4.9</xref>). The vertical dotted gray lines mark the preferred direction under each decoder, with an arrow labeling the change in preferred direction.</p>
<p>c. Tuning curve of a representative example non-rotated neuron of our model, under the same two decoders. All conventions exactly as in the previous panel. Note that this neuron’s preferred direction changes less than that of the rotated neuron in the previous panel.</p>
<p>d. Mean squared error achieved by generalized re-aiming solutions for 100 random credit assignment rotation perturbations, plotted as a function of the number of command variables used for re-aiming, <inline-formula><inline-graphic xlink:href="589952v2_inline218.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.Light green dots denote individual decoder perturbations, overlaid darker open circles denote medians over all 100 sampled decoder perturbations. Black dotted horizontal line shows the mean squared error achieved by re-aiming solutions to the unperturbed baseline decoder with <inline-formula><inline-graphic xlink:href="589952v2_inline219.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>e. Average change in preferred direction of rotated, non-rotated, and indirect neurons between simulated cursor control with the baseline decoder and each perturbed decoder, plotted as a function of the number of command variables used for re-aiming, <inline-formula><inline-graphic xlink:href="589952v2_inline220.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.For each decoder perturbation, the changes in preferred direction are averaged over all neurons in each sub-population, and the median over all sampled perturbations is plotted. Error bars mark the upper and lower quartiles. Positive angles indicate a counter-clockwise rotatation, consistent with the direction of rotation of the decoding vectors of the rotated neurons.</p></caption>
<graphic xlink:href="589952v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Despite these challenges, multiple studies have shown that non-human primates can learn to control such decoder perturbations.<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c61">61</xref></sup> These studies used the same 2D cursor control task described above (<xref rid="fig2" ref-type="fig">fig. 2a</xref>), in which subjects first control a BCI cursor using a baseline decoder fit to motor cortical activity recorded during a calibration task, and then learn to control the cursor using a perturbed decoder with rotated decoding vectors. Subjects’ motor cortical activity changes after learning to control the perturbed decoder, and this can be characterized by the change in neurons’ tuning to cursor direction during BCI control with the baseline and perturbed decoders. Each of these studies found that, after learning, tuning curves of both rotated (<xref rid="fig6" ref-type="fig">fig. 6b</xref>) and non-rotated neurons (<xref rid="fig6" ref-type="fig">fig. 6c</xref>) shift in the same direction as the decoding vectors. For example, if the decoding vectors are rotated counter-clockwise, tuning curves also shift counter-clockwise. Notably, however, tuning curves of rotated neurons shift more on average than those of non-rotated neurons (compare the simulated examples in <xref rid="fig6" ref-type="fig">fig. 6b</xref> and <xref rid="fig6" ref-type="fig">fig. 6c</xref>). This observation could be interpreted to support the hypothesis that the motor system is able to solve the credit assignment problem: it has identified which neurons’ decoding vectors were rotated, and accordingly modified their responses more so than the others’ (e.g. via Hebbian plasticity<sup><xref ref-type="bibr" rid="c17">17</xref></sup>). Here we consider an alternate hypothesis: that this phenomenon could arise from generalized re-aiming, a global learning strategy entirely unconcerned with modifying individual neurons.</p>
<p>To test this, we follow the same procedure as above: we simulate motor cortical activity during the calibration task, use it to construct a baseline decoder, and then sample 100 random credit assignment rotation perturbations (Methods <xref ref-type="sec" rid="s4i">Section 4.9</xref>). Following the experimental procedures of Zhou et al. (2019), the perturbed decoders are constructed by applying a 75° counter-clockwise rotation to a random selection of 50% of the columns of the decoding matrix, <bold>D</bold>. For each decoder, we then compute the optimal motor commands for each target (<xref ref-type="disp-formula" rid="eqn4">equation 4</xref>) and drive the network with them to simulate center-out cursor movements learned by re-aiming. By comparing neurons’ directional tuning under the optimal motor commands for the baseline decoder and the perturbed decoder, we can determine how directional tuning would change after learning by re-aiming.</p>
<p>Reflecting the fact that the baseline decoder is easy to learn, we used <inline-formula><inline-graphic xlink:href="589952v2_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to compute re-aiming solutions for it. For the perturbed decoders, we simulated generalized re-aiming with 2 to 6 command variables. We find that re-aiming with about <inline-formula><inline-graphic xlink:href="589952v2_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables is necessary to achieve the same performance as with the baseline decoder (<xref rid="fig6" ref-type="fig">fig. 6d</xref>). That said, re-aiming with only <inline-formula><inline-graphic xlink:href="589952v2_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula> suffices to achieve a relatively low mean squared error (around 0.1; compare to <xref rid="fig3" ref-type="fig">fig. 3d</xref>), suggesting ordinary 2D re-aiming could still be a viable learning strategy for this task.</p>
<p>As in the experiments, we measured neurons’ preferred directions (i.e. the direction at the tuning curve peak, cf. <xref rid="fig6" ref-type="fig">fig. 6b</xref> and <xref rid="fig6" ref-type="fig">fig. 6c</xref>) under the optimal motor commands for the baseline decoder and for each perturbed decoder, and calculated each neuron’s change in preferred direction. We then averaged the change in preferred direction separately over rotated and non-rotated neurons. <xref rid="fig6" ref-type="fig">Figure 6e</xref> shows the median of this average change in preferred direction over all sampled perturbed decoders, as a function of the number of command variables used for re-aiming, <inline-formula><inline-graphic xlink:href="589952v2_inline35.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.Consistent with the experimental results, we find that re-aiming leads to a global counter-clockwise shift in motor cortical tuning curves congruent with the rotation of the decoding vectors. Importantly, we find that generalized re-aiming with <inline-formula><inline-graphic xlink:href="589952v2_inline36.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables replicates the credit assignment effects seen in the experiments, whereby the preferred directions of rotated neurons shift on average more than their non-rotated counterparts. This is true despite the fact that the credit assignment problem was never truly solved: no neuron-specific parameters were modified under this learning strategy.</p>
<p>Because we have complete access to the full population of neurons in our motor cortical model, we can also measure tuning changes in the sub-population of “indirect” neurons not recorded by the BCI (i.e. neurons whose decoding vectors in <bold>D</bold> comprise a vector of 0’s). These are plotted in <xref rid="fig6" ref-type="fig">fig. 6e</xref> with a gray line. Under generalized re-aiming, indirect neurons’ tuning curves shift less on average than rotated neurons’. Whether they shift more or less than non-rotated neurons’ tuning curves, on the other hand, depends on the specific value of <inline-formula><inline-graphic xlink:href="589952v2_inline37.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and varies considerably across different perturbed decoders.</p>
<p>These two results are roughly consistent with the observations of Zhou et al. (2019), who examined this phenomenon in two non-human primate subjects. They found that, in both subjects, the average change in tuning curves was larger for rotated neurons than for indirect neurons, as predicted by our model. But when comparing non-rotated neurons to indirect neurons, they found that the average change in tuning curves was larger for the former in one subject and larger for the latter in the second subject, consistent with the variability observed in our simulations. To our knowledge, this is the only experimental study on indirect neurons’ responses before and after learning a credit assignment rotation perturbation; more studies are needed to fully test the predictions of our model.</p>
<p>An important additional prediction of our model is that credit assignment effects do not arise under ordinary 2D re-aiming (<xref rid="fig6" ref-type="fig">fig. 6e</xref>,<inline-formula><inline-graphic xlink:href="589952v2_inline38.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). This is consistent with prior modeling work showing that two-dimensional re-aiming does not suffice to account for empirically observed changes in motor cortical tuning curves after learning a credit assignment perturbation.<sup><xref ref-type="bibr" rid="c33">33</xref></sup> Interestingly, it is also consistent with recent experimental work showing that differences between rotated and non-rotated neurons seem to arise gradually over multiple days of training.<sup><xref ref-type="bibr" rid="c61">61</xref></sup> Our model suggests that this timecourse of learning reflects a change in learning strategy, whereby subjects initially engage in low-dimensional re-aiming to rapidly reduce gross cursor movement errors, and then turn to generalized re-aiming to further refine</p>
<p>BCI control over a longer timescale, resulting in more marked credit assignment effects later in learning. We briefly remark, however, that Zhou et al. did not observe changes in the preferred directions of non-rotated neurons after the first day of training. In our simulation, on the other hand, the non-rotated neuron tuning curves shift back towards their starting values under the baseline decoder as the number of re-aimed command variables increases (see decreasing pink line in <xref rid="fig6" ref-type="fig">fig. 6e</xref>). This discrepancy between our model and the experimental data could be explained by subjects using suboptimal re-aiming solutions deviating from the optimal one predicted by our model, possibly due to the difficulty of solving this optimization problem when <inline-formula><inline-graphic xlink:href="589952v2_inline39.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,or due to the lack of motivation to find it (since even <inline-formula><inline-graphic xlink:href="589952v2_inline40.gif" mime-subtype="gif" mimetype="image"/></inline-formula> suffices to achieve relatively low error, <xref rid="fig6" ref-type="fig">fig. 6d</xref>). This could also explain why the amount of change in preferred directions is significantly larger in our simulation (30 − 60<sup><italic>o</italic></sup>) than in the experimental data (20 − 40<sup><italic>o</italic></sup>).</p>
</sec>
<sec id="s2f">
<label>2.6</label>
<title>Operant conditioning of individual neurons by re-aiming</title>
<p>The third and final BCI task we study is the operant conditioning of individual motor cortical neurons. In this task, subjects are rewarded for simply increasing the activity of one group of motor cortical neurons over another.<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c63">63</xref>–<xref ref-type="bibr" rid="c66">66</xref></sup> The fact that primates and rodents are capable of solving such tasks is often cited as evidence that the motor system can learn to specifically modulate the responses of individual neurons. Classical models of single-neuron operant conditioning have argued that these changes happen via reward-modulated plasticity at their synapses.<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c67">67</xref></sup> Here we explore the extent to which these observations could instead be explained by re-aiming.</p>
<p>We begin by considering the classic operant conditioning task of Fetz and Baker (1973). In this task, the subject is rewarded for increasing the firing rate of one neuron – termed the “target” neuron – while simultaneously decreasing that of another neuron – termed the “distractor” neuron. Remarkably, Fetz and Baker found that non-human primates are often able to do this with only minutes of practice. Moreover, the identity of the target and distractor neurons could be flipped midway through a recording session, and the subject would subsequently adapt to this new target assignment within tens of minutes, increasing the activity of the neuron whose activity was previously suppressed. Could low-dimensional re-aiming explain this behavior?</p>
<p>The answer depends on the reachable manifold. If the reachable manifold contains activity patterns in which neuron <italic>a</italic> is more active than neuron <italic>b</italic>, as well as activity patterns in which neuron <italic>b</italic> is more active than neuron <italic>a</italic>, then good re-aiming solutions will exist for both target assignments. This is illustrated in <xref rid="fig7" ref-type="fig">fig. 7a</xref>, which shows two neurons’ endpoint firing rates, <italic>r</italic><sub><italic>i</italic></sub>(<italic>t</italic><sub>end</sub>), at various points on the reachable manifold, following the same conventions as in <xref rid="fig3" ref-type="fig">fig. 3f</xref>. On this plane, activity patterns below the diagonal are rewarded when neuron <italic>a</italic> is the target neuron; activity patterns above the diagonal are rewarded when neuron <italic>b</italic> is the target neuron. Because there are reachable activity patterns on both sides of the diagonal, good re-aiming solutions exist for both target assignments. We calculated optimal re-aiming solutions for each target assignment by maximizing the firing rate difference between the target and distractor neurons, subject to a metabolic cost (<xref ref-type="disp-formula" rid="eqn29">equation 29</xref>). The activity produced by these optimal solutions are marked by the open red and green circles. These evidently satisfy each target assignment, with neuron <italic>a</italic> achieving a higher firing rate under one re-aiming solution (green circle) and neuron <italic>b</italic> achieving a higher firing rate under the other (red circle).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Operant conditioning of individual neurons by re-aiming.</title>
<p>a. Activity of two model neurons at various points on the reachable manifold, at <italic>t</italic><sub>end</sub> = 1000ms with <inline-formula><inline-graphic xlink:href="589952v2_inline40a.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, following the same conventions as <xref rid="fig3" ref-type="fig">fig. 3f</xref>. As in that figure, each ring of activity patterns is generated by the corresponding ring of color- and size-matched motor commands in <xref rid="fig3" ref-type="fig">fig. 3e</xref>. Activity patterns below the diagonal are ones where neuron <italic>a</italic> is more active than neuron <italic>b</italic>, satisfying the task demands when neuron <italic>a</italic> is the target neuron; the reverse holds for activity patterns above the diagonal. The green and orange open circles denote the activity patterns produced by the optimal re-aiming solutions for the two respective target assignments. Note that, due to the metabolic cost term in the re-aiming objective function, these do not necessarily correspond to the points on reachable manifold that are furthest away from the diagonal.</p>
<p>b. Difference in activity produced by re-aiming solutions for each target assignment (at time <italic>t</italic><sub>end</sub> = 1000 ms, the endpoint time the re-aiming solutions were optimized for), for 500 random pairs of neurons from the same model motor cortical network used in previous simulations. Each dot corresponds to one pair of neurons. The pair of neurons shown in previous panel is marked by an open circle. Two additional examples are marked by open circles. Insets show the activity of those neuron pairs at various points on the reachable manifold, following same conventions as the previous panel.</p>
<p>c. Difference in activity (at <italic>t</italic><sub>end</sub> = 1000 ms) produced by the re-aiming solutions optimized for neuron <italic>a</italic> being the target, for the same 500 random pairs of neurons, plotted as a function of the correlation between the two neurons during simulated spontaneous behavior. The three example pairs of neurons highlighted in the previous panel are again highlighted here with open circles. A quantitatively similar trend is observed for <italic>r</italic><sub><italic>b</italic></sub> − <italic>r</italic><sub><italic>a</italic></sub> with re-aiming solutions optimized for neuron <italic>b</italic> being the target (data not shown).</p>
<p>d. Activity of indirect neurons (at <italic>t</italic><sub>end</sub> = 1000 ms) for the same neuron pairs and re-aiming solutions in previous panel, plotted as a function of correlation with neuron <italic>a</italic> during simulated spontaneous behavior. A quantitatively similar trend is observed for correlations with neuron <italic>b</italic> with re-aiming solutions optimized for neuron <italic>b</italic> being the target (data not shown).</p></caption>
<graphic xlink:href="589952v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>More generally, we can think of this plot as a particular two-dimensional projection of the reachable manifold, specified by the pair of neurons <italic>a</italic> and <italic>b</italic>. A given neuron pair thus admits good re-aiming solutions for this task whenever the corresponding projection of the reachable manifold covers the appropriate side(s) of the diagonal. Framed in this way, it is easy to intuit that for most random pairs of neurons a good solution will generally exist for at least one target assignment – random two-dimensional projections of the reachable manifold are unlikely to lie exactly on the diagonal. We verify this intuition by sampling 500 random pairs of neurons from our model motor cortical network (see Methods <xref ref-type="sec" rid="s4j">Section 4.10</xref>) and checking whether the re-aiming solutions for the two target assignments produce higher firing rates for the target neuron than for the distractor neuron. The difference in the two neurons’ firing rates produced by the optimal re-aiming solutions are plotted in <xref rid="fig7" ref-type="fig">fig. 7b</xref>. For most neuron pairs, we see that at least one of the two neurons can be activated more than the other. In many cases, both neurons can be activated more than the other, meaning that both target assignments could be learned by re-aiming.</p>
<p>For some neuron pairs, however, the optimal re-aiming solutions do not produce a large difference in firing rates under either target assignment. These infelicitous neuron pairs are ones where the two neurons are highly correlated across all activity patterns on the reachable manifold, such that the relevant projection doesn’t deviate strongly from the diagonal and no good re-aiming solutions exist. One such example is shown in the inset on the right, where the reachable manifold lies largely right on the diagonal, and thus both re-aiming solutions lead to near-0 difference in firing rates. A different kind of exampe is shown in the inset on the top, where the two neurons are correlated in such a way that the reachable manifold resides on only one side of the diagonal. In this case, a good re-aiming solution exists for one target assignment but not for the other. These observations reveals a tight relationship between neural correlations and operant conditioning performance: the more correlated a pair of neurons is across reachable activity patterns, the more difficult it should be to selectively activate one more than the other by re-aiming.</p>
<p>Is this prediction of our model consistent with experimental observations? Without empirical access to the reachable manifold, we cannot directly measure correlations across reachable activity patterns. But if we assume that neural activity during a prior “calibration task” is driven by the same command variables used subsequently for re-aiming – as we did in our simulation of WMP/OMP learning –, then we should expect neural correlations during this calibration task to approximately match correlations across the reachable manifold used for re-aiming. This predicts that neural correlations during a prior calibration task should be predictive of subsequent operant conditioning performance. This prediction is in fact consistent with observations from a study by Clancy et al. (2014), in which the conditioned neurons’ correlation was measured during a period of spontaneous behavior (the “calibration task”) just prior to performing operant conditioning. Consistent with our model’s prediction, they observed that the stronger the correlation during spontaneous behavior prior to operant conditioning, the worse the mouse tended to perform the subsequent operant conditioning task.</p>
<p>To quantify this prediction, we simulated the experiment of Clancy et al. We first simulated motor cortical activity during spontaneous behavior by driving the model motor cortical network with randomly sampled motor commands in which only two command variables, <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub>, were allowed to vary (all other command variables were set to 0). We sampled 50 such motor commands to simulate 50 bouts of spontaneous activity (cf. Methods <xref ref-type="sec" rid="s4j">Section 4.10</xref>). For each conditioned pair of neurons, we measured their correlation coefficient over all activity across the 50 bouts, and then simulated re-aiming with the same two command variables, <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub>. We quantified the efficacy of re-aiming with the firing rate difference between the target and distractor neurons at the optimized endpoint time, <italic>t</italic><sub>end</sub>. Mirroring the experimental results of Clancy et al., we find that the spontaneous activity correlations are weakly but significantly predictive of re-aiming efficacy (<xref rid="fig7" ref-type="fig">fig. 7c</xref>). We briefly remark here that the operant conditioning task of Clancy et al. differs from our simulations in that pairs of ensembles of up to 11 neurons were conditioned, rather than pairs of single neurons. In Supplementary Materials <xref ref-type="sec" rid="s5a4">Section S.1.4</xref>, we show that the same results hold in this setting as well.</p>
<p>Finally, we consider what happens to the “indirect” neurons – neurons that are neither a target nor a distractor. Clancy et al. observed that, after learning, indirect neurons <italic>strongly</italic> correlated with the target neuron during spontaneous behavior remained highly active during performance of the subsequent operant conditioning task (supplementary figure 9a in<sup><xref ref-type="bibr" rid="c65">65</xref></sup>). This is consistent with our model of re-aiming, in which re-aiming tends to drive indirect neurons proportionally to their correlation with the target neuron during spontaneous behavior (<xref rid="fig7" ref-type="fig">fig. 7d</xref>). Our model is not, however, consistent with another observation by Clancy et al.: indirect neurons that were <italic>moderately</italic> correlated with the target neuron were highly active only in the early stages of learning, but by the end of learning became silent. A possible explanation for this inconsistency is that subjects re-aim with <inline-formula><inline-graphic xlink:href="589952v2_inline41.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables. If spontaneous behavior during the calibration period were driven by more than 2 command variables, we would expect subjects to re-aim with more as well. Given the complexity of spontaneous behavior, this is a reasonable explanation, but we leave for future work a more comprehensive study of generalized re-aiming with <inline-formula><inline-graphic xlink:href="589952v2_inline42.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables in operant conditioning tasks.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study, we proposed and investigated the hypothesis that motor cortical BCI learning proceeds via a learning strategy we refer to as generalized re-aiming. Under this strategy, internal motor commands are manipulated to control the BCI using the same motor cortical circuitry used during natural motor behaviors. Because only a few command variables need to be manipulated to achieve this goal, learning can proceed rapidly and flexibly, and, because the motor cortical circuitry is conserved, the operation of motor cortex during natural motor control is conserved as well.</p>
<p>To study the neural and behavioral consequences of this learning strategy, we formulated a mechanistic model of re-aiming in which the internal command variables specify upstream inputs to motor cortex. By analyzing how these inputs get transformed into motor cortical activity patterns through the circuit’s nonlinear recurrent dynamics, we were able to demonstrate that re-aiming can in fact account for a wide range of experimental observations about BCI learning. This model can explain the different timescales of learning required for different BCI decoders,<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup> selective changes in motor cortical tuning curves over learning,<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c61">61</xref></sup> and the seemingly astonishing ability of mammals to flexibly modulate the activity of single neurons via operant conditioning.<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c63">63</xref>,<xref ref-type="bibr" rid="c65">65</xref></sup> The model also makes a novel experimental prediction about behavioral biases during short-term learning (<xref rid="fig4" ref-type="fig">fig. 4</xref>), which we were able to corroborate in previously published data.<sup><xref ref-type="bibr" rid="c14">14</xref></sup> The success of this model at replicating these empirical phenomena provides an explanation in terms of the biological dynamics of neural circuits.</p>
<sec id="s3a">
<label>3.1</label>
<title>Intrinsic variable learning vs. individual neuron learning</title>
<p>An important debate in the BCI learning literature has been whether human and non-human primates are able to precisely learn and control the contribution of individual neurons to a given BCI decoder readout – the so-called “individual neuron learning” hypothesis. Several studies have been directly aimed at testing this hypothesis, leading to evidence in favor<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c61">61</xref></sup> and against it.<sup><xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup> The alternative hypothesis is often referred to as “intrinsic variable learning”,<sup><xref ref-type="bibr" rid="c9">9</xref></sup> whereby subjects learn to control the BCI using the same constrained set of activity patterns usually used for natural motor control, unable to independently control the activity of single neurons. Our model of re-aiming is a particular formalization of this latter hypothesis, with the command variables <inline-formula><inline-graphic xlink:href="589952v2_inline43.gif" mime-subtype="gif" mimetype="image"/></inline-formula> acting as the so-called intrinsic variables.</p>
<p>Our simulations of generalized re-aiming show that many experimental results traditionally attributed to some form of individual neuron learning<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c67">67</xref></sup> can be accounted for by intrinsic variable learning. In particular, even classical single neuron operant conditioning results can be reproduced by our model. Our simulations show that the dynamics of recurrently connected neural circuits are capable of generating the activity patterns required by these BCI tasks, without the need to optimize parameters specific to individual neurons or synapses.<sup><xref ref-type="bibr" rid="c68">68</xref></sup> This suggests caution in underestimating the role of macroscopic cognitive strategies<sup><xref ref-type="bibr" rid="c69">69</xref>,<xref ref-type="bibr" rid="c70">70</xref></sup> when observing what may look like highly specific, microscopic, changes to the activity of single neurons.</p>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>The role of synaptic plasticity in BCI learning</title>
<p>All the results we replicated here have been previously replicated by various models of synaptic plasticity within motor cortex. As argued in the introduction, however, learning by optimizing synaptic parameters entails solving an extremely high-dimensional optimization problem with no access to explicit gradients, which would limit learning to be slow and brittle. Several of these previously proposed models worked around this problem by using small and simplified feed-forward models of motor cortex<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c68">68</xref></sup> or biologically implausible learning rules.<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c71">71</xref></sup> A few have demonstrated that, for simple tasks like operant conditioning, biologically plausible learning rules can in fact succeed in biologically relevant regimes despite these obstacles.<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c67">67</xref></sup> However, to our knowledge, none have comprehensively accounted for all three sets of experimental results considered here, including the observed effects on non-recorded neurons and the dependence of operant conditioning performance on neural correlations.</p>
<p>That said, the present study does not by any means rule out the possibility that synaptic plasticity within motor cortex may play an important role in BCI learning; rather, it reveals the surprising capabilities of a pure re-aiming strategy. The true mechanisms underlying BCI learning most likely comprise a mixture of both re-aiming as well as synaptic plasticity, and future work will be needed to tease apart the contributions of these two learning mechanisms and understand how they are coordinated.</p>
<p>One natural possibility is that synaptic plasticity operates on a much slower timescale than reaiming.<sup><xref ref-type="bibr" rid="c72">72</xref></sup> This could help explain selective changes to motor cortical responses that only arise late in learning and are not replicated by our model. For example, Clancy et al. (2014) observed in their operant conditioning experiments that indirect neurons not strongly correlated with the target neuron become silent late in learning.<sup><xref ref-type="bibr" rid="c65">65</xref></sup> Ganguly et al. (2011) similarly observed that indirect neurons become less tuned to reach direction after days of practice with a given BCI decoder. Jarosiewicz et al. (2008) reported a similar effect in rotated neurons after a credit assignment rotation perturbation (although note that this effect seems to disappear when increasing the proportion of neurons rotated, see [12, 61]). These selective changes in tuning strength are not reproduced by our model of re-aiming (<xref rid="figS5" ref-type="fig">Supplementary Figure S5a</xref>), but previous theoretical work has demonstrated that they can be reproduced by reward-modulated Hebbian plasticity in a simplified model of motor cortex.<sup><xref ref-type="bibr" rid="c17">17</xref></sup></p>
<p>An additional set of observations that are not well accounted for by our model come from a few recent studies demonstrating long-term changes in motor cortical activity after short-term learning.<sup><xref ref-type="bibr" rid="c73">73</xref>,<xref ref-type="bibr" rid="c74">74</xref></sup> In particular, Losey et al. (2024) found that motor cortical activity during baseline decoder control changed before and after learning a WMP within a single experimetal session. Our model could account for this if the upstream population driving motor cortex encoded not only the motor commands relevant for control but also additional variables indexing the current behavioral context,<sup><xref ref-type="bibr" rid="c73">73</xref>,<xref ref-type="bibr" rid="c75">75</xref></sup> or a memory trace of the current task.<sup><xref ref-type="bibr" rid="c74">74</xref></sup></p>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>The “intrinsic manifold” of population activity</title>
<p>A simple but important takeaway from this study is that the low-dimensional structure of activity in a population depends not only on the intrinsic dynamics and connectivity within that population, but also on the structure of its upstream input. The observation that population activity is confined to a low-dimensional subspace – often termed the “intrinsic manifold”<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c76">76</xref></sup> or the “neural modes”<sup><xref ref-type="bibr" rid="c58">58</xref></sup> – does not mean that the circuit connectivity prevents it from generating activity patterns outside of this subspace. It is likely that many more activity patterns outside of this subspace are accessible, but that only a low-dimensional subset are accessed by the inputs evoked by the subjects’ behavior during the recording session.<sup><xref ref-type="bibr" rid="c60">60</xref>,<xref ref-type="bibr" rid="c77">77</xref></sup></p>
<p>This insight leads to a novel interpretation of the observation that outside-manifold perturbations require a longer time to learn than their within-manifold counterparts.<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup> Previous models of this phenomenon have assumed that the longer learning time reflects the challenge of modifying the motor cortical connectivity to permit the production of activity patterns outside of the intrinsic manifold.<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c68">68</xref>,<xref ref-type="bibr" rid="c71">71</xref></sup> Our simulations demonstrate that this isn’t necessary, and that in many cases it may suffice to simply exploit additional input dimensions beyond those evoked by the calibration task (<xref rid="fig5" ref-type="fig">fig. 5a</xref>, <xref rid="figS1" ref-type="fig">Supplementary Figure S1e</xref>). Under this model of learning, the longer learning time required for OMPs reflects the fact that these new input dimensions need to be discovered from scratch, as the calibration task provides little prior information about them.</p>
<p>Another important aspect of the intrinsic manifold that this study highlights is its nonlinear structure. Because firing rates are bounded from below by 0, activity patterns are confined to the positive orthant of state space. This constraint imposes a conical structure on population activity within the intrinsic manifold (<xref rid="fig3" ref-type="fig">fig. 3f</xref>, <xref rid="figS2" ref-type="fig">Supplementary Figure S2</xref>), which we show in Supplementary Materials <xref ref-type="sec" rid="s5a3">Section S.1.3</xref> is in fact necessary to account for experimentally observed behavioral biases in WMP learning. Given the strong behavioral repercussions this structure can have on BCI control, understanding and identifying such nonlinear structure in motor cortical activity may prove crucial both for understanding BCI learning as well as for designing better BCI decoders.</p>
</sec>
<sec id="s3d">
<label>3.4</label>
<title>The role of the calibration task in BCI learning</title>
<p>From a more practical perspective, our theory of re-aiming highlights the role of the calibration task in BCI learning. The calibration task is typically seen as a way to calibrate the decoding parameters; that is, as a source of information for constructing the BCI <italic>decoder</italic>. Here we suggest that it additionally serves as a source of information for the subject itself, that is, for the BCI <italic>learner</italic>.<sup><xref ref-type="bibr" rid="c78">78</xref>,<xref ref-type="bibr" rid="c79">79</xref></sup> For example, in modeling WMP learning, we assumed that subjects re-aimed with the two command variables modulated by the calibration task; in modeling operant conditioning, we assumed that subjects re-aimed with the same command variables driving spontaneous behavior prior to the operant conditioning task. If any other two command variables had been optimized instead, the re-aiming strategy would not have succeeded in solving the task. It is the prior information provided by the calibration task that allows efficient learning by telling the subject which command variables to re-aim with. This hypothesis is consistent with various BCI learning studies demonstrating that subjects learn to control BCIs using the same patterns of activity evoked by the task they were engaged in just prior to BCI learning.<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup></p>
<p>Importantly, it predicts that the calibration task can influence subjects’ ability to learn a given BCI decoder, and therefore that careful design of this task could help improve subjects’ learning speed. For example, the calibration task should evoke changes in as few command variables as necessary, so that subjects subsequently re-aim by optimizing only those <inline-formula><inline-graphic xlink:href="589952v2_inline44.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables and avoid wasting time exploring modifications to other command variables. A prediction of our model is that learning should be slower when the calibration task evokes changes in more command variables.</p>
</sec>
<sec id="s3e">
<label>3.5</label>
<title>How are the re-aiming solutions learned?</title>
<p>The theory presented here treats the question of <italic>what</italic> solutions subjects learn, and makes no claims about <italic>how</italic> they are learned and subsequently maintained. That said, a strong assumption we made in motivating the re-aiming learning strategy was that learning could operate within the low-dimensional space of the command variables. It is this low dimensionality that we claimed would be critical for efficient learning; if the command variables were learned by simply optimizing the connectivity of an upstream circuit, then the limitations of learning by synaptic plasticity would also apply to learning by re-aiming.</p>
<p>One intriguing resolution to this problem would be that the command variables are stored and updated in the <italic>activity</italic> – rather than the <italic>synapses</italic> – of an upstream circuit, as in the pre-frontal cortex model of Wang et al. (2018).<sup><xref ref-type="bibr" rid="c80">80</xref></sup> In this model, a recurrent neural network implicitly stores a behavioral policy in its internal state, which, through the network’s dynamics, is updated over time as it interacts with the environment and observes which actions are rewarded in which states. A similar architecture might operate upstream of motor cortex, whereby an upstream circuit continually stores and updates a sensorimotor policy for selecting low-dimensional motor commands. This learning circuit would likely encompass additional populations beyond those directly driving motor cortex, such as the basal ganglia, which are well known to play an important role in BCI learning.<sup><xref ref-type="bibr" rid="c64">64</xref>,<xref ref-type="bibr" rid="c81">81</xref>–<xref ref-type="bibr" rid="c83">83</xref></sup></p>
<p>We finally remark that the short timescale of WMP learning closely mirrors that of motor adaptation, in which subjects adapt their natural movements to a systematic environmental perturbation. Learning these tasks typically requires 100’s of trials of practice,<sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c73">73</xref></sup> similar to the time it takes non-human primates to learn WMPs. Neural recordings during these tasks have suggested that changes in neural activity during motor adaptation are driven by changes in the preparatory input from dorsal pre-motor cortex to primary motor cortex.<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c84">84</xref></sup> Moreover, changes in the preparatory state of motor cortex (presumably set by upstream inputs<sup><xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c46">46</xref></sup>) have been shown to play a critical role in motor adaptation tasks under manual control<sup><xref ref-type="bibr" rid="c73">73</xref></sup> as well as BCI control.<sup><xref ref-type="bibr" rid="c32">32</xref></sup> These results are consistent with the idea that, much like in our model of re-aiming, motor adaptation involves modifications to the inputs driving motor cortex while motor cortex itself remains unchanged. Our model of re-aiming may thus be relevant to more general and naturalistic forms of sensorimotor learning beyond BCIs.</p>
</sec>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Motor cortical dynamics</title>
<p>Motor cortical activity was simulated by integrating equation 1 using a standard 4th order Runge-Kutta method with step size 0.1ms, implemented with the torchdiffeq Python package.<sup><xref ref-type="bibr" rid="c85">85</xref></sup> Reachable activity patterns, <bold>r</bold>(<italic>t</italic><sub>end</sub>; <bold><italic>θ</italic></bold>), were computed by integrating this equation from the initial condition at time <italic>t</italic> = 0 to the endpoint time <italic>t</italic> = <italic>t</italic><sub>end</sub>, with constant inputs determined by the given motor command, <bold><italic>θ</italic></bold>, using <xref ref-type="disp-formula" rid="eqn2">equation 2</xref>. We assumed silent initial conditions, <italic>x</italic><sub><italic>i</italic></sub>(0) = 0, to enable a computationally efficient solution to the re-aiming objective function (see <xref ref-type="disp-formula" rid="eqn12">equation 12</xref>). Decoding from the reachable activity patterns using <xref ref-type="disp-formula" rid="eqn3">equation 3</xref> yields the reachable readouts, <bold>y</bold>(<italic>t</italic><sub>end</sub>; <bold><italic>θ</italic></bold>).</p>
<p>In all simulations in the main text, sparse random recurrent weights, <inline-formula><inline-graphic xlink:href="589952v2_inline45.gif" mime-subtype="gif" mimetype="image"/></inline-formula> were used: only 10% of these weights were set to non-zero values, which were independently sampled from a zero-mean Gaussian, 𝒩 (0, 1<italic>/N</italic>), where <italic>N</italic> is the total number of motor cortical neurons in the network. Input weights were all sampled from a zero-mean Gaussian, <inline-formula><inline-graphic xlink:href="589952v2_inline46.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>M</italic> is the total number of inputs. Encoding weights were sampled randomly from the standard Gaussian distribution, <italic>U</italic><sub><italic>ij</italic></sub> ∼ 𝒩 (0, 1) (any normalization is taken care of by the metabolic cost term in <xref ref-type="disp-formula" rid="eqn4">equation 4</xref> when computing re-aiming solutions). Other connectivity patterns are considered in <xref rid="figS1" ref-type="fig">Supplementary Figure S1</xref>. We used <italic>τ</italic> = 200ms, as in the motor cortical model of Hennequin et al. (2014). To enable efficient numerical simulation, network size was set to <italic>N</italic> = <italic>M</italic> = 256. Simulations with larger networks (up to <italic>N</italic> = <italic>M</italic> = 2048 neurons) produced similar results (data not shown).</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Computing re-aiming solutions</title>
<p>Concatenating the command variables into a <inline-formula><inline-graphic xlink:href="589952v2_inline47.gif" mime-subtype="gif" mimetype="image"/></inline-formula> -dimensional vector that contains only the command variables being optimized, <inline-formula><inline-graphic xlink:href="589952v2_inline48.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,we can treat <xref ref-type="disp-formula" rid="eqn4">equation 4</xref> as an optimization over all <inline-formula><inline-graphic xlink:href="589952v2_inline49.gif" mime-subtype="gif" mimetype="image"/></inline-formula> -dimensional vectors <inline-formula><inline-graphic xlink:href="589952v2_inline50.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <inline-formula><inline-graphic xlink:href="589952v2_inline51.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.We can simplify this optimization problem by first analytically solving for the optimal magnitude of <inline-formula><inline-graphic xlink:href="589952v2_inline52.gif" mime-subtype="gif" mimetype="image"/></inline-formula> given its direction. Once we have this optimal magnitude, all that remains is an optimization over its direction – an optimization over unit vectors on the <inline-formula><inline-graphic xlink:href="589952v2_inline52a.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-dimensional unit hypersphere. This is a <inline-formula><inline-graphic xlink:href="589952v2_inline53.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-dimensional manifold that, importantly, is bounded, so we can hope to find the optimal direction efficiently by brute-force search, avoiding the difficulties of non-convex gradient-based optimization in high dimensions.</p>
<p>Formally, we decompose the re-aiming optimization (<xref ref-type="disp-formula" rid="eqn4">equation 4</xref>) into an optimization of the norm, <italic>s</italic>, and direction, <inline-formula><inline-graphic xlink:href="589952v2_inline54.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,of <inline-formula><inline-graphic xlink:href="589952v2_inline55.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,
<disp-formula id="eqn7">
<graphic xlink:href="589952v2_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We can analytically solve for the optimal magnitude, ŝ, by exploiting two simplifications afforded to us by the rectified linear activation function <italic>ϕ</italic>(±) of the motor cortical dynamics (<xref ref-type="disp-formula" rid="eqn1b">equation 1b</xref>). The first is the scale-invariance of the activation function (<italic>ϕ</italic>(<italic>sx</italic>) = <italic>sϕ</italic>(<italic>x</italic>) for any <italic>s</italic> ≥ 0), which accordingly endows the motor cortical dynamics with scale invariance,
<disp-formula id="eqn8">
<graphic xlink:href="589952v2_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
whenever <italic>x</italic><sub><italic>i</italic></sub>(0) = 0 (see Supplementary Materials <xref ref-type="sec" rid="s5a1">Section S.2.1</xref> for a formal proof), which we assumed to be the case in our simulations. The second simplification is to approximate the quadratic cost term by its large <italic>M</italic> limit
<disp-formula id="eqn9">
<graphic xlink:href="589952v2_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The equality holds whenever the encoding weights (<italic>U</italic><sub><italic>ij</italic></sub> in <xref ref-type="disp-formula" rid="eqn2">equation 2</xref>) are independent and identically distributed with zero mean and unit variance, as they are here, so that the law of large numbers can be invoked to replace the sum with an expectation over the encoding weight distribution (the factor of 1<italic>/</italic>2 arises from the fact that only half of each axis counts towards the expectation due to the linear rectification, see Supplementary Materials <xref ref-type="sec" rid="s5b2">Section S.2.2</xref> for a formal proof). Inserting these two equations into <xref ref-type="disp-formula" rid="eqn7">equation 7</xref> together with the BCI readout <xref ref-type="disp-formula" rid="eqn3">equation 3</xref>, we obtain
<disp-formula id="eqn10">
<graphic xlink:href="589952v2_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
It is then straightforward to solve for ŝ in terms of <inline-formula><inline-graphic xlink:href="589952v2_inline56.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,yielding the following closed set of equations:
<disp-formula id="eqn11">
<graphic xlink:href="589952v2_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn12">
<graphic xlink:href="589952v2_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the ·notation denotes the Euclidean dot product. We have thus reduced what was an optimization over all vectors in <inline-formula><inline-graphic xlink:href="589952v2_inline57.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn4">equation 4</xref>) to an optimization over all vectors living on the <inline-formula><inline-graphic xlink:href="589952v2_inline58.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-dimensional unit hypersphere (<xref ref-type="disp-formula" rid="eqn12">equation 12</xref>).</p>
<p>We can therefore approximately solve this new optimization problem via brute-force search, by simply uniformly sampling a large number of <inline-formula><inline-graphic xlink:href="589952v2_inline59.gif" mime-subtype="gif" mimetype="image"/></inline-formula>’s on the unit hypersphere and identifying the one that produces the smallest value of the loss function in <xref ref-type="disp-formula" rid="eqn12">equation 12</xref>. Evaluating <inline-formula><inline-graphic xlink:href="589952v2_inline60.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for a large number of <inline-formula><inline-graphic xlink:href="589952v2_inline61.gif" mime-subtype="gif" mimetype="image"/></inline-formula>’s can be done efficiently by using a GPU to integrate in parallel the dynamics driven by each <inline-formula><inline-graphic xlink:href="589952v2_inline62.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,Note that, once these activity patterns have been calculated, they can be re-used to perform the brute-force search optimization for any given value of <italic>γ</italic>, without having to again integrate the dynamics.</p>
<p>For simulations with <inline-formula><inline-graphic xlink:href="589952v2_inline63.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, this brute-force search algorithm sufficed to produce good re-aiming solutions. In this case, the relevant hypersphere is the unit circle, from which it is straightforward to sample densely and uniformly. For simulations of generalized re-aiming, however, we took an additional step to ensure the obtained solutions were as good as they could be even for the larger values of <inline-formula><inline-graphic xlink:href="589952v2_inline64.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where it becomes more difficult to sample densely from the corresponding unit hypersphere. We first performed a brute-force search over 2<sup>17</sup> vectors sampled uniformly from the unit hypersphere, as just described, to obtain an initial estimate of the re-aiming solution. This initial estimate was then used as a starting point for the L-BFGS algorithm,<sup><xref ref-type="bibr" rid="c86">86</xref></sup> which we then applied to minimize the re-aiming loss function (<xref ref-type="disp-formula" rid="eqn4">equation 4</xref>) with respect to the raw command variables <inline-formula><inline-graphic xlink:href="589952v2_inline65.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We found that this additional step was essential when <inline-formula><inline-graphic xlink:href="589952v2_inline66.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>In all simulations in the main text, we used an endpoint time of <italic>t</italic><sub>end</sub> = 1000 ms, reflecting the typical time it takes for trained primates to complete center-out reaches under BCI control.<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c14">14</xref></sup> The results of simulations with other endpoint times are shown in <xref rid="figS1" ref-type="fig">Supplementary Figure S1b</xref>. To simulate a center-out reaching task, the target readouts <bold>y</bold><sup>*</sup> were set to eight equally spaced unit vectors on the unit circle (cf. <xref rid="fig3" ref-type="fig">fig. 3c</xref>). Mean squared error is calculated as
<disp-formula id="eqn13">
<graphic xlink:href="589952v2_eqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="589952v2_inline67.gif" mime-subtype="gif" mimetype="image"/></inline-formula> correspond to the eight radial target readouts. Because the targets are unit norm, a mean squared error of 1.0 corresponds to that achieved by readouts at the origin.</p>
<p>In the case of operant conditioning, there is no “target readout” per se, as subjects are simply instructed to modulate firing rates as much as possible in a given direction. In this case, a different re-aiming objective was used, see the section “Simulation of operant conditioning” for details.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Setting the metabolic cost weight</title>
<p>The metabolic cost weight parameter <italic>γ</italic> was picked to ensure that low mean squared error would be achieved under the baseline decoder with <inline-formula><inline-graphic xlink:href="589952v2_inline68.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.We calculated re-aiming solutions with <inline-formula><inline-graphic xlink:href="589952v2_inline69.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the baseline decoder under a wide range of values of <italic>γ</italic>. We took advantage of the fact that the brute-force search algorithm outlined above allows us to easily evaluate solutions for different values of <italic>γ</italic> with only a single forward pass of the model. Once we had re-aiming solutions for each value of <italic>γ</italic>, we calculated the error achieved by these re-aiming solutions for each target readout, and found the largest value of <italic>γ</italic> that permitted a squared error of less than .05 for each of the eight targets. <italic>γ</italic> was then fixed to this value for simulations with all the decoder perturbations.</p>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Characterizing the reachable manifold</title>
<p>The reachable manifold is the set of activity patterns at time <italic>t</italic><sub>end</sub> that can be generated by any motor command allowable under the re-aiming strategy. We assume that these allowable motor commands are bounded, reflecting the fact that (i) actual extrinsic motor variables are finite and bounded and (ii) upstream firing rates are bounded. Formally, we enforce this by assuming an upper bound on the motor command norm, ∥<bold><italic>θ</italic></bold>∥ ≤ <italic>s</italic><sub>max</sub>. In our simulations of short-term learning of WMP’s and OMP’s, we set the value of this bound to the maximum norm over all 2D re-aiming solutions to all decoders. Specifically, we computed re-aiming solutions for each target readout and decoder perturbation (8 target readouts × (100 WMP’s + 100 OMP’s + baseline decoder) = 1,608 re-aiming solutions) with <inline-formula><inline-graphic xlink:href="589952v2_inline70.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,calculated their norms, and set <italic>s</italic><sub>max</sub> to their maximum. For the randomly connected network presented in the main text, we found this value to be approximately 1.25.</p>
<p>In <xref rid="fig3" ref-type="fig">figure 3f</xref>, we drove the motor cortical network with motor command vectors with five distinct norms, ∥<bold><italic>θ</italic></bold>∥ ∈ {0.1, 0.4, 0.7, 1.0, <italic>s</italic><sub>max</sub>}, chosen to aid visualization of the reachable manifold. We picked 256 equally spaced angles between 0 and 2<italic>π</italic> and constructed 2D vectors with each angle and each norm to obtain the command variable pairs, <italic>θ</italic><sub>1</sub>, <italic>θ</italic><sub>2</sub>, shown in <xref rid="fig3" ref-type="fig">fig. 3e</xref> (all other command variables were set to zero). We then simulated the motor cortical network with each of these motor commands to obtain a large ensemble of activity patterns on the reachable manifold, <bold>r</bold>(<italic>t</italic><sub>end</sub>; <bold><italic>θ</italic></bold>), and projected these onto their top three principal components to obtain the visualization in <xref rid="fig3" ref-type="fig">figure 3f</xref>. <xref rid="fig3" ref-type="fig">Figure 3c</xref> plots the readouts of each of these activity patterns from three different decoders. <xref rid="fig4" ref-type="fig">Figure 4a</xref> plots the readouts from four different WMP decoders, in this case using activity patterns generated from motor commands with four different norms equally spaced between 0.1 and <italic>s</italic><sub>max</sub> (thus producing four loops of readouts instead of five). In <xref rid="fig7" ref-type="fig">figure 7a</xref>, five motor command norms equally spaced between .1 and the maximum norm of the re-aiming solutions for that pair of neurons were used. These choices were all made to aid visualization of the reachable manifold’s structure.</p>
<p>To obtain the centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline71.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,and covariance, <bold>Σ</bold><sub><italic>r</italic></sub>, of the reachable manifold for <inline-formula><inline-graphic xlink:href="589952v2_inline72.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we computed expectations over a uniform distribution on the reachable manifold. Note that an expectation over uniformly distributed activity patterns on the reachable manifold is not the same as an expectation over activity patterns generated by uniformly distributed command variables. These two distributions of activity patterns are related via the Jacobian of the mapping from command variables, <italic>θ</italic><sub>1</sub>, <italic>θ</italic><sub>2</sub>, to activity patterns, <bold>r</bold>(<italic>t</italic><sub>end</sub>; <bold><italic>θ</italic></bold>), which was used to derive the following two expressions for <inline-formula><inline-graphic xlink:href="589952v2_inline73.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <bold>Σ</bold><sub><italic>r</italic></sub> (see Supplementary Materials <xref ref-type="sec" rid="s5b3">Section S.2.3</xref> for full mathematical derivation):
<disp-formula id="eqn14">
<graphic xlink:href="589952v2_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn15">
<graphic xlink:href="589952v2_eqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>r</bold><sub>0</sub>(<italic>φ</italic>) is the population activity at time <italic>t</italic><sub>end</sub> generated by a pair of non-zero command variables <italic>θ</italic><sub>1</sub>, <italic>θ</italic><sub>2</sub> with angle <italic>φ</italic> and unit norm, <inline-formula><inline-graphic xlink:href="589952v2_inline73a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is its derivative with respect to <italic>φ</italic>, and <italic>ω</italic>(<italic>φ</italic>) is the angle between <bold>r</bold><sub>0</sub>(<italic>φ</italic>) and <inline-formula><inline-graphic xlink:href="589952v2_inline74.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We used a finite-differences approximation for the derivative <inline-formula><inline-graphic xlink:href="589952v2_inline75.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and computed these integrals numerically by summing over a dense range of values of <italic>φ</italic> ∈ [0, 2<italic>π</italic>]. This estimate of the reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline76.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,is plotted in <xref rid="fig4" ref-type="fig">figures 4a, 4b</xref>, and <xref rid="figS2" ref-type="fig">S2a</xref>. This estimate of the reachable manifold covariance, <bold>Σ</bold><sub><italic>r</italic></sub>, is used for the variance explained curve plotted in <xref rid="fig3" ref-type="fig">figure 3g</xref>.</p>
<p>Analagous calculations for <inline-formula><inline-graphic xlink:href="589952v2_inline77.gif" mime-subtype="gif" mimetype="image"/></inline-formula> quickly become numerically intractable, as the derivatives and integrals become multivariate as the number of polar coordinates increases. We thus chose to characterize the dimensionality of the reachable manifold under <italic>generalized</italic> re-aiming by the covariance over activity patterns produced by uniformly distributed motor commands, which we denote by <bold>Σ</bold><sub><italic>θ</italic></sub>. As already noted, this is not the same as the covariance over activity patterns uniformly distributed on the reachable manifold manifold, but these two covariances are strongly related. This covariance is given by (see Supplementary Materials <xref ref-type="sec" rid="s5b4">Section S.2.4</xref> for derivation)
<disp-formula id="eqn16">
<graphic xlink:href="589952v2_eqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="589952v2_inline78.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denotes an expectation over a uniform distribution on the unit-norm <inline-formula><inline-graphic xlink:href="589952v2_inline79.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-sphere. These expectations were estimated numerically by uniformly sampling 2<sup>17</sup> vectors <inline-formula><inline-graphic xlink:href="589952v2_inline80.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from the unit hypersphere, setting the <inline-formula><inline-graphic xlink:href="589952v2_inline81.gif" mime-subtype="gif" mimetype="image"/></inline-formula> non-zero command variables to these values, calculating the activity patterns generated by these motor commands at time <italic>t</italic><sub>end</sub> = 1000ms, <inline-formula><inline-graphic xlink:href="589952v2_inline82.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,and then averaging over the resulting ensemble of activity patterns.</p>
<p>This estimate of the reachable manifold covariance, <bold>Σ</bold><sub><italic>θ</italic></sub>, is used to compute the participation ratio plotted in <xref rid="fig5" ref-type="fig">figure 5b</xref> as a function of <inline-formula><inline-graphic xlink:href="589952v2_inline83.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,using the formula
<disp-formula id="eqn17">
<graphic xlink:href="589952v2_eqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>λ</italic><sub>1</sub>, <italic>λ</italic><sub>2</sub>, …, <italic>λ</italic><sub><italic>N</italic></sub> are the eigenvalues of <bold>Σ</bold><sub><italic>θ</italic></sub>.</p>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Quantifying biases in BCI readouts</title>
<p>To quantify behavioral biases, we used the maximal cursor progress metric defined in <xref ref-type="disp-formula" rid="eqn6">equation 6</xref>. This equation was solved by again exploiting the same re-parameterization of the motor commands we used for calculating re-aiming solutions (<xref ref-type="disp-formula" rid="eqn7">equation 7</xref>). Specifically, we decompose the vector of non-zero command variables, <inline-formula><inline-graphic xlink:href="589952v2_inline84.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,into a magnitude and direction, <inline-formula><inline-graphic xlink:href="589952v2_inline85.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>s &gt;</italic> 0 and <inline-formula><inline-graphic xlink:href="589952v2_inline86.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Plugging in the readout equation (<xref ref-type="disp-formula" rid="eqn3">equation 3</xref>) into the definition of cursor progress (<xref ref-type="disp-formula" rid="eqn5">equation 5</xref>) and exploiting the scale invariance of the motor cortical dynamics (<xref ref-type="disp-formula" rid="eqn8">equation 8</xref>), we have that the maximal cursor progress is given by
<disp-formula id="eqn18">
<graphic xlink:href="589952v2_eqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula id="eqn19">
<graphic xlink:href="589952v2_eqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Since <inline-formula><inline-graphic xlink:href="589952v2_inline87.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in these simulations (and thus <inline-formula><inline-graphic xlink:href="589952v2_inline88.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is just a 2D unit vector), we were able to effectively solve the optimization problem in <xref ref-type="disp-formula" rid="eqn18">equation 18</xref> by brute-force search over densely and uniformly sampled <inline-formula><inline-graphic xlink:href="589952v2_inline89.gif" mime-subtype="gif" mimetype="image"/></inline-formula>’s from the unit circle.</p>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>Re-analysis of data from Sadtler et al. (2014)</title>
<p>To quantify behavioral biases in the experimental data, we estimated the maximal cursor progress in each experimental session by using the cursor progress values observed in the window of 50 contiguous trials of WMP control with shortest average reach completion times. At each timestep in each trial, we calculated the cursor progress in the direction of the target relative to the cursor’s position at that time. We then binned the per-timestep relative target directions into 45<sup><italic>o</italic></sup> bins centered at the eight radial reach target angles, and averaged the cursor progresses in each bin to obtain an average cursor progress for each target direction. We take these averages to be estimates of the subject’s <italic>maximal</italic> cursor progress with that session’s WMP decoder, as they are taken from the 50 trials with fastest reaches.</p>
<p>To predict the maximal cursor progress in each session from the target direction angle with <inline-formula><inline-graphic xlink:href="589952v2_inline90.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,we sought an estimate of the reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline91.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,that could be empirically measured from the recorded neural activity, without access to the underlying reachable manifold. To do this, we noticed that, in our model, the time- and trial-averaged population activity generated by the re-aiming solutions for the baseline decoder – which we denote by <inline-formula><inline-graphic xlink:href="589952v2_inline92.gif" mime-subtype="gif" mimetype="image"/></inline-formula>– was highly correlated with the true reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline93.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.We therefore used <inline-formula><inline-graphic xlink:href="589952v2_inline94.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to estimate <inline-formula><inline-graphic xlink:href="589952v2_inline95.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,since <inline-formula><inline-graphic xlink:href="589952v2_inline96.gif" mime-subtype="gif" mimetype="image"/></inline-formula> can be easily estimated in the experimental data by simply averaging motor cortical activity during the baseline decoder control block in each session. We calculated target-specific means by averaging motor cortical activity over all trials and time during reaches to each target, and then averaged these target-specific means over targets to obtain <inline-formula><inline-graphic xlink:href="589952v2_inline97.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.This was the estimate of the reachable manifold centroid – and its projection through the respective WMP decoder in each session, <inline-formula><inline-graphic xlink:href="589952v2_inline98.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-used in the analysis presented in <xref rid="fig4" ref-type="fig">figure 4d</xref>.</p>
<p>To keep the analysis of the data and the model consistent, we also used an analogous estimate of the reachable manifold centroid for the analysis of the model in <xref rid="fig4" ref-type="fig">figure 4c</xref>. In this case, <inline-formula><inline-graphic xlink:href="589952v2_inline99.gif" mime-subtype="gif" mimetype="image"/></inline-formula> was measured by simulating reaches to each target by driving the motor cortical network dynamics with the re-aiming solutions for the baseline decoder, and then averaging the motor cortical firing rates over all time and over all eight reach directions. We found that the resulting negative correlation was similar regardless of whether the true reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline100.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,(data not shown) or its estimate, <inline-formula><inline-graphic xlink:href="589952v2_inline101.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,was used.</p>
</sec>
<sec id="s4g">
<label>4.7</label>
<title>Simulation of the calibration task</title>
<p>The calibration task was simulated by setting the first two command variables <italic>θ</italic><sub>1</sub>, <italic>θ</italic><sub>2</sub> to the coordinates of the reach direction <inline-formula><inline-graphic xlink:href="589952v2_inline102.gif" mime-subtype="gif" mimetype="image"/></inline-formula> being presented in each trial (a 2D unit vector pointing in one of eight equally spaced angles), and setting all other command variables to zero (<italic>θ</italic><sub>3</sub> = <italic>θ</italic><sub>4</sub> = … = <italic>θ</italic><sub><italic>K</italic></sub> = 0). To simulate noise in the neural responses, we added noise in the dynamics, in the motor commands, and in the initial conditions on each trial. At each timestep, zero mean Gaussian noise with standard deviation 0.05 was sampled and added to the single neuron potentials <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and to the two command variables <italic>θ</italic><sub>1</sub>, <italic>θ</italic><sub>2</sub>. Initial conditions in each trial were sampled randomly from a 0-mean isotropic Gaussian with standard deviation 0.1. The network was driven for 1000ms in each trial, matching the duration of each trial in the experiment.</p>
<p>For simulations with WMPs and OMPs, we simulated 10 trials of each reach direction, replicating the structure of the calibration task used by Sadtler et al. (2014). For simulations with credit assignment rotation perturbations (<xref rid="fig6" ref-type="fig">fig. 6</xref>), the calibration task was identical except that only a single trial of each reach direction was simulated, to mimic the decoder initialization procedure of Zhou et al. (2019). Note that in all cases re-aiming with <inline-formula><inline-graphic xlink:href="589952v2_inline103.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables implies re-aiming with the same two command variables driving the calibration task neural responses, <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub>.</p>
</sec>
<sec id="s4h">
<label>4.8</label>
<title>Within- and outside-manifold perturbations</title>
<p>In the BCI system used by Sadtler et al. (2014) and Oby et al. (2019), 96-channel microelectrode arrays were used to record neural activity. Spikes were detected by threshold crossings in the recorded voltage signals at each electrode, resulting in a series of spike trains at each electrode. Spike trains at each electrode could therefore contain spikes from multiple neurons near the electrode site, as no spike sorting was performed. In total, about 100 neurons were likely to have been recorded, constituting a small fraction of the total population of neurons in motor cortex. To simulate this, we ensured that the BCI decoder in our simulations only had access to a linear mixture of firing rates from <italic>N</italic><sub><italic>r</italic></sub> = 99 neurons (so as to be divisible by <italic>ℓ</italic> + 1 = 9, to group neurons by modulation depth, see below). This was done by first multiplying the firing rates with a <italic>N</italic><sub><italic>r</italic></sub> × <italic>N</italic> “recording matrix”, <bold>H</bold>, which had the following tri-diagonal structure
<disp-formula id="eqn20">
<graphic xlink:href="589952v2_eqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Thus, each “neural unit” in the vector <bold>Hr</bold> is composed of a linear mixture of seven neurons, with “neural units” with adjacent indices mixing together overlapping sets of neurons.</p>
<p>Following Sadtler et al., we next z-scored the activity recorded by each neural unit with respect to its statistics during the calibration task,
<disp-formula id="eqn21">
<graphic xlink:href="589952v2_eqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>c</bold> is an <italic>N</italic> -dimensional vector with the mean firing rate of each neuron and <bold>S</bold><sub><italic>r</italic></sub> is an <italic>N</italic><sub><italic>r</italic></sub> × <italic>N</italic><sub><italic>r</italic></sub> diagonal matrix with the standard deviation of each “neural unit”, measured from the simulated activity during the calibration task. Readouts, <bold>y</bold>, were obtained by decoding from the <italic>N</italic><sub><italic>r</italic></sub>-dimensional vectors of z-scored mixed firing rates. It is the 2 × <italic>N</italic><sub><italic>r</italic></sub> <italic>effective</italic> decoding matrix, <bold>D</bold><sub>0</sub>, that is perturbed by the WMP and OMP decoder perturbations (see below). Note that the full 2 × <italic>N</italic> decoding matrix, <bold>D</bold>, is such that only its first <italic>N</italic><sub><italic>r</italic></sub> columns are non-zero, reflecting the fact that only a subset of the full population of motor cortical neurons is recorded by the BCI.</p>
<p>For the baseline decoder, the effective decoding matrix <bold>D</bold><sub>0</sub> was constructed following the methods of Sadtler et al., with the exception that we used Principal Components Analysis instead of Factor Analysis to estimate the intrinsic manifold. This choice was made purely for the sake of numerical convenience, as Principal Components Analysis has a closed-form solution that can be computed more efficiently. The full procedure for estimating the intrinsic manifold and constructing the baseline decoder is outlined in detail in Supplementary Materials <xref ref-type="sec" rid="s5c">Section S.3</xref>. In short, the baseline decoder effective decoding matrix can be expressed as a product of a 2 × <italic>ℓ</italic> matrix <bold>K</bold> and an <italic>ℓ</italic> × <italic>N</italic><sub><italic>r</italic></sub> matrix <bold>L</bold>, where <italic>ℓ</italic> is the dimensionality of the intrinsic manifold,
<disp-formula id="eqn22">
<graphic xlink:href="589952v2_eqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The matrix <bold>L</bold> projects <italic>N</italic><sub><italic>r</italic></sub>-dimensional activity patterns down to the <italic>ℓ</italic>-dimensional intrinsic manifold; its rows span the subspace defined by the intrinsic manifold (Supplementary Materials <xref ref-type="sec" rid="s5c2">Section S.3.2</xref>, <xref ref-type="disp-formula" rid="eqn71">equation 71</xref>). In our simulations we used <italic>ℓ</italic> = 8, as we found that the top 8 principal components contained 95% of the variance in the simulated calibration task responses. The matrix <bold>K</bold> then translates the resulting <italic>ℓ</italic>-dimensional dimensionality-reduced activity patterns into 2-dimensional BCI readouts. This matrix is fit to the calibration task data, by fitting a Kalman filter that accurately decodes the calibration task stimuli from the dimensionality-reduced calibration task neural responses at each timestep and trial (Supplementary Materials <xref ref-type="sec" rid="s5c2">Section S.3.2</xref>, <xref ref-type="disp-formula" rid="eqn72">equation 72</xref>).</p>
<p>Within-manifold perturbations (WMPs) perturb the baseline decoder in such a way that the row space of <bold>L</bold> remains intact, so as to conserve the decoder’s relationship with the intrinsic manifold. This is done by simply shuffling the rows of <bold>L</bold> without modifying them, via pre-multiplication with a random <italic>ℓ</italic> × <italic>ℓ</italic> permutation matrix <bold>P</bold>,
<disp-formula id="eqn23">
<graphic xlink:href="589952v2_eqn23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Outside-manifold decoders, on the other hand, directly disrupt the row space of <bold>L</bold>. This is done by randomly shuffling the components of each of its rows, via post-multiplication with a random <italic>N</italic><sub><italic>r</italic></sub> × <italic>N</italic><sub><italic>r</italic></sub> permutation matrix <bold>P</bold>,
<disp-formula id="eqn24">
<graphic xlink:href="589952v2_eqn24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
It is important to keep in mind that both WMPs and OMPs can change the readouts in complex ways, beyond a simple rotation like that depicted by the cartoon in <xref rid="fig2" ref-type="fig">figure 2b</xref> (for examples, see <xref rid="fig4" ref-type="fig">fig. 4a</xref> here and <xref rid="figS2" ref-type="fig">supplementary figure 2</xref> in<sup><xref ref-type="bibr" rid="c29">29</xref></sup>). Once the baseline decoder was constructed, we randomly sampled 100 within-manifold and 100 outside-manifold perturbations by randomly selecting 100 <italic>ℓ</italic> × <italic>ℓ</italic> and <italic>N</italic><sub><italic>r</italic></sub> × <italic>N</italic><sub><italic>r</italic></sub> permutation matrices, respectively.</p>
<p>To minimize any differences between these two types of decoder perturbations that would go beyond their opposing relationship to the intrinsic manifold, we imposed several restrictions on the selected permutation matrices, as was done by Sadtler et al. (see Supplementary Materials <xref ref-type="sec" rid="s5c3">Section S.3.3</xref> for details). First, we enforced that the mean principal angle between the row space of the baseline effective decoding matrix and the row space of each perturbed effective decoding matrix was between 60<sup><italic>o</italic></sup> and 80<sup><italic>o</italic></sup>. Second, we enforced that population activity produced by the re-aiming solutions for the baseline decoder would produce readouts through each perturbed decoder that resulted in a mean squared error between 0.6 and 0.8. Finally, we fit tuning curves to the neural activity generated by the re-aiming solutions for the baseline decoder, and asked how much the preferred directions would need to change to produce the same readouts under the perturbed decoder, following the same procedure employed by Sadtler et al. We enforced that this change be between 30<sup><italic>o</italic></sup> and 45<sup><italic>o</italic></sup>. We typically found that about 100-200 permutations out of all possible permutation matrices satisfied these criteria. We then randomly sampled 100 of them.</p>
<p>Following the procedure used by Sadtler et al. with monkey L, we did not consider all possible <italic>N</italic><sub><italic>r</italic></sub> × <italic>N</italic><sub><italic>r</italic></sub> permutation matrices for OMPs (as there are 99! of them). Rather, we grouped all <italic>N</italic><sub><italic>r</italic></sub> neural units into <italic>ℓ</italic> groups, and then considered all <italic>ℓ</italic>-dimensional permutations of these groups (of which there are 8!). In other words, rather than permuting all <italic>N</italic><sub><italic>r</italic></sub> columns of <bold>L</bold>, <italic>ℓ</italic> groups of columns were permuted. This ensured that the total number of possible decoder perturbations was the same for WMPs and OMPs. The <italic>ℓ</italic> groups were assigned as follows: for each neural unit in <bold>Hr</bold>, we fit a cosine tuning curve to its calibration task responses to obtain its modulation depth (<xref ref-type="disp-formula" rid="eqn26">equation 26</xref>), assigned the <italic>N</italic><sub><italic>r</italic></sub><italic>/</italic>(<italic>ℓ</italic> + 1) neurons with smallest modulation depths to a small-modulation group not to be permuted, and randomly assigned the remaining neurons to <italic>ℓ</italic> high-modulation groups to be permuted; following Sadtler et al., the small-modulation group was never permuted to avoid cases in which an inactive or noisy neuron could get assigned a large decoding weight.</p>
<p>In <xref rid="fig3" ref-type="fig">figure 3g</xref>, we define the “dimensions” of the intrinsic manifold as a set of orthonormal basis vectors <bold>f</bold><sub>1</sub>, <bold>f</bold><sub>2</sub>, …, <bold>f</bold><sub><italic>ℓ</italic></sub> spanning the intrinsic manifold (Supplementary Materials <xref ref-type="sec" rid="s5c1">Section S.3.1</xref>). We then calculated the variance explained by each dimension by
<disp-formula id="eqn25">
<graphic xlink:href="589952v2_eqn25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For the gray curve, <bold>Σ</bold> was set to the sample covariance of the simulated calibration task responses. For the purple curve, <bold>Σ</bold> was set to the reachable manifold covariance <bold>Σ</bold><sub><italic>r</italic></sub> (defined in <xref ref-type="disp-formula" rid="eqn15">equation 15</xref>). In each case, the cumulative variance explained was calculated by ordering the dimensions by variance explained and then summing them in that order.</p>
</sec>
<sec id="s4i">
<label>4.9</label>
<title>Credit assignment rotation perturbations</title>
<p>In the BCI system used by Zhou et al. (2019), recorded activity was sorted by matching spike waveforms to identify spikes from single neurons, resulting in the identification of 10–12 individual neurons. Importantly, each neuron had reliable tuning to reach direction during the calibration task. In our simulation, we modelled this by randomly choosing 80 neurons, fitting tuning curves to their activity during the calibration task, and selecting only those with modulation depth greater than 0.5 (see below for how modulation depth is measured). This typically resulted in 10–15 neurons being included in the BCI decoder (i.e. being assigned non-zero weights in the decoding matrix <bold>D</bold>); no “recording matrix” <bold>H</bold> was used in these simulations. For the particular network model used in the simulations reported in the main text, this selection procedure resulted in <italic>N</italic><sub><italic>r</italic></sub> = 11 neurons being included.</p>
<p>Tuning curves were fit to time-averaged simulated firing rates in the calibration task, <inline-formula><inline-graphic xlink:href="589952v2_inline104.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,in response to presentation of each of the eight radial reach targets, <inline-formula><inline-graphic xlink:href="589952v2_inline105.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (cf. section “Simulation of the calibration task”). An <italic>N</italic> × 3 matrix of tuning weights, <bold>T</bold>, was fit to predict these average responses from the respective reach target coordinates,
<disp-formula id="eqn26">
<graphic xlink:href="589952v2_eqn26.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="589952v2_inline106.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is a 3D vector with the coordinates of the direction of the <italic>j</italic>th reach target, <inline-formula><inline-graphic xlink:href="589952v2_inline107.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,as its first two components and a constant 1 in its third component, included to model baseline tonic firing rates of each neuron. Thus, only the first two columns of the tuning weight matrix <bold>T</bold> model how the <italic>i</italic>th neuron’s firing rate depends on the reach target’s direction, whereas its third column models activity independent of the reach target. To extract from these weights the directional tuning of neuron <italic>i</italic>, we take the 2D vector comprising the first two components of the <italic>i</italic>th row of <inline-formula><inline-graphic xlink:href="589952v2_inline108.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We notate this 2D vector by <italic>m</italic><sub><italic>i</italic></sub><bold>p</bold><sub><italic>i</italic></sub>, where <bold>p</bold><sub><italic>i</italic></sub> is a unit vector pointing in its direction and <italic>m</italic><sub><italic>i</italic></sub> is its norm. The angle of <bold>p</bold><sub><italic>i</italic></sub> is neuron <italic>i</italic>’s preferred direction, and <italic>m</italic><sub><italic>i</italic></sub> is its modulation depth.</p>
<p>Following the methods of Zhou et al. (2019), the baseline decoder was constructed as follows. First, raw firing rates were baseline-subtracted and normalized by their modulation depths,
<disp-formula id="eqn27">
<graphic xlink:href="589952v2_eqn27.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>c</bold> is given by the third column of <inline-formula><inline-graphic xlink:href="589952v2_inline109.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, containing the baseline firing rates estimated from the linear regression fit (<xref ref-type="disp-formula" rid="eqn26">equation 26</xref>), and <bold>M</bold> is an <italic>N</italic><sub><italic>r</italic></sub> × <italic>N</italic> diagonal rectangular matrix containing the inverse modulation depths <italic>m</italic><sup>−1</sup> for each of the <italic>N</italic><sub><italic>r</italic></sub> neurons recorded by the BCI on the diagonal across the first <italic>N</italic><sub><italic>r</italic></sub> columns 0 everywhere else. These normalized firing rates were then transformed into 2D readouts by a 2 × <italic>N</italic><sub><italic>r</italic></sub> effective decoding matrix <inline-formula><inline-graphic xlink:href="589952v2_inline109a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> containing the preferred direction vectors <bold>p</bold><sub><italic>i</italic></sub> of each of the <italic>N</italic><sub><italic>r</italic></sub> recorded neurons in their corresponding columns. More precisely, the <italic>i</italic>th column of <inline-formula><inline-graphic xlink:href="589952v2_inline110.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,is given by
<disp-formula id="eqn28">
<graphic xlink:href="589952v2_eqn28.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the scaling constant <italic>k</italic> is chosen to minimize the mean squared error between the readouts from the calibration task activity and the target readouts. This is the classic population vector algorithm (PVA).<sup><xref ref-type="bibr" rid="c87">87</xref></sup> The full 2 × <italic>N</italic> decoding matrix of the baseline decoder was thus <inline-formula><inline-graphic xlink:href="589952v2_inline111.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,which has non-zero weights only in the <italic>N</italic><sub><italic>r</italic></sub> columns corresponding to the <italic>N</italic><sub><italic>r</italic></sub> recorded neurons.</p>
<p>Credit assignment rotation perturbations were constructed by simply picking a random subset of the columns of <inline-formula><inline-graphic xlink:href="589952v2_inline112.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and rotating them. In our simulations, we picked a random 50% of these columns and rotated them 75<sup><italic>o</italic></sup> counter-clockwise, as was done in the decoder perturbations used by Zhou et al. (2019). We sampled 100 random perturbations in this way, in each case rotating a different subset of columns. The normalization matrix <bold>M</bold> and baseline subtraction parameters <bold>c</bold> are kept the same for all decoders.</p>
<p>To measure the tuning changes predicted by re-aiming, we simulated cursor reaches with each decoder by driving the motor cortical network with the re-aiming solutions for that decoder. In each case, noise was applied to the dynamics, exactly as in the calibration task. We then fit tuning curves to each neuron’s time-averaged activity, using linear regression exactly as described in <xref ref-type="disp-formula" rid="eqn26">equation 26</xref>, and extracted the preferred direction of each rotated, non-rotated, and indirect neuron. For each perturbed decoder, we then determined each neuron’s change in preferred direction by calculating the difference between its preferred direction under the re-aiming solutions for the perturbed decoder and its preferred direction under the re-aiming solutions for the baseline decoder (cf. <xref rid="fig6" ref-type="fig">figures 6b, 6c</xref>). These changes were then averaged over all neurons in each sub-population (rotated, non-rotated, or indirect). <xref rid="fig6" ref-type="fig">Figure 6e</xref> shows the percentiles (median and quartiles), over all 100 sampled decoder perturbations, of this average tuning change. An analagous analysis of the changes in modulation depth is shown in <xref rid="figS5" ref-type="fig">Supplementary Figure S5a</xref>.</p>
</sec>
<sec id="s4j">
<label>4.10</label>
<title>Simulation of operant conditioning</title>
<p>In an operant conditioning task, there is no “target readout” per se. The objective is to simply increase the activity of one neuron over another, as much as possible. We can thus express the objective as maximizing the difference in firing rate between the two neurons, which can be thought of as a one-dimensional linear readout from the population. Formally, we calculate readouts in this task by a dot product between the firing rate vector <bold>r</bold> and a decoding vector <bold>d</bold> which has a +1 for the target neuron, a −1 for the distractor neuron, and 0’s everywhere else. This one-dimensional readout indicates how much more active the target neuron is than the distractor neuron. The goal in an operant conditioning task is to maximize this readout.</p>
<p>Adding in a metabolic cost, the objective function we use for re-aiming is
<disp-formula id="eqn29">
<graphic xlink:href="589952v2_eqn29.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Letting <inline-formula><inline-graphic xlink:href="589952v2_inline113.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denote the non-zero command variables, we again re-parameterize this optimization problem into an optimization over the magnitude, <italic>s</italic>, and direction, <inline-formula><inline-graphic xlink:href="589952v2_inline114.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,of <inline-formula><inline-graphic xlink:href="589952v2_inline115.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,
<disp-formula id="eqn30">
<graphic xlink:href="589952v2_eqn30.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn31">
<graphic xlink:href="589952v2_eqn31.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the approximation follows from the application of the scale invariance of the network dynamics (<xref ref-type="disp-formula" rid="eqn8">equation 8</xref>) and the mean-field approximation of the metabolic cost (<xref ref-type="disp-formula" rid="eqn9">equation 9</xref>). This approximation allows us to analytically solve for the optimal magnitude ŝ,
<disp-formula id="eqn32">
<graphic xlink:href="589952v2_eqn32.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
which in turn allows us to solve for the optimal direction, <inline-formula><inline-graphic xlink:href="589952v2_inline116.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,via optimization over the <inline-formula><inline-graphic xlink:href="589952v2_inline117.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-dimensional unit hypersphere,
<disp-formula id="eqn33">
<graphic xlink:href="589952v2_eqn33.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn34">
<graphic xlink:href="589952v2_eqn34.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the second equality follows from plugging in <xref ref-type="disp-formula" rid="eqn32">equation 32</xref> and simplifying. In all our operant conditioning simulations, we used <inline-formula><inline-graphic xlink:href="589952v2_inline118.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,which enabled us to easily solve this optimization problem via brute force search over the unit circle. As in other simulations, we used <italic>t</italic><sub>end</sub> = 1000ms.</p>
<p>Note that the optimal readout achieved by the re-aiming solution is
<disp-formula id="eqn35">
<graphic xlink:href="589952v2_eqn35.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Thus, changing the exact value of <italic>γ</italic> only re-scales the re-aiming solutions and the readouts they produce. We thus simply set it to <italic>γ</italic> = 1 in all our simulations of this task.</p>
<p>In classic operant conditioning experiments,<sup><xref ref-type="bibr" rid="c28">28</xref></sup> neurons selected for operant conditioning had to be active prior to the conditioning task to be identified by the recording electrode. We imposed a similar constraint in our simulation by first driving the network with 50 random <italic>K</italic> = 100-dimensional motor commands for <italic>t</italic><sub>end</sub> = 1000ms, and identifying the 50% of neurons with highest average firing rate over motor commands and time. The neuron pairs used for operant conditioning were sampled from this sub-population.</p>
<p>To simulate neural activity during a baseline period of spontaneous behavior prior to operant conditioning, we used a similar procedure but now driving the network with 50 random <inline-formula><inline-graphic xlink:href="589952v2_inline119.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-dimensional motor commands, with <inline-formula><inline-graphic xlink:href="589952v2_inline120.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.This allowed us to ask whether operant conditioning performance under re-aiming could be predicted from correlations arising during spontaneous behavior driven by the same command variables used subsequently for re-aiming. In <xref rid="fig7" ref-type="fig">figure 7c</xref>, correlations were measured by correlation coefficient between the two conditioned neurons. In <xref rid="fig7" ref-type="fig">figure 7d</xref>, correlation coefficients between each indirect neuron and the target neuron are plotted against the firing rate of the indirect neuron at <italic>t</italic><sub>end</sub> = 1000ms when driving the network with the re-aiming solution.</p>
</sec>
<sec id="s4k">
<label>4.11</label>
<title>Table of simulation parameters</title>
<p><table-wrap id="utbl1" orientation="portrait" position="float">
<graphic xlink:href="589952v2_utbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap></p>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Supplementary Matrials</title>
<sec id="s5a">
<label>S.1</label>
<title>Extended modeling results</title>
<sec id="s5a1">
<label>S.1.1</label>
<title>Structured motor cortical connectivity</title>
<p>In addition to the randomly connected network architecture used in the results presented in the main text, we simulated re-aiming in the task of Sadtler et al. (2014) with alternative motor cortical connectivity profiles, described below. In each case, we simulated the calibration task, sampled decoder perturbations, and computed re-aiming solutions as described in the main text, except for a few minor modifications noted below. Results of these simulations are shown in <xref rid="figS1" ref-type="fig">figure S1</xref>.</p>
<sec id="s5a1a">
<title>Random excitatory/inhibitory (E/I) connectivity</title>
<p>a random sparse and balanced E/I recurrent connectivity matrix was constructed following the sampling procedure described in.<sup><xref ref-type="bibr" rid="c88">88</xref></sup> In short, all ex1316 citatory weights had the same strength, all inhibitory had the same strength (re-scaled relative to the excitatory weights to account for the different number of excitatory and inhibitory neurons), and each row of the weight matrix was enforced to be 0 mean to enforce so-called E/I balance. We used a sparsity of 10% (i.e. only 10% of weights were non-zero), with 80% of the neurons in the population being excitatory. Input and encoding weights were sampled randomly as for the randomly connected network in the main text.</p>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1:</label>
<caption><title>Each column shows simulation results for a different network: (1) the randomly connected network used in the main text; (2) another randomly connected network, with weights sampled in exactly the same way; (3) a network with random E/I connectivity; (4) a network with inhibition-optimized E/I connectivity; (5) network with connectivity optimized for delayed center-out reaching.</title>
<p>a. Eigenspectra of recurrent weight matrices of each network, plotted on the complex plane. Note that the optimized network has low-rank connectivity: almost all eigenvalues are clustered at 0. Dashed vertical line marks the linear stability boundary of a real value of 1.0.</p>
<p>b. Mean squared error achieved by re-aiming solutions for different endpoint times, <italic>t</italic><sub>end</sub>, for each decoder. Lighter markers correspond to individual decoders, darker open markers (connected by lines) show medians over all decoders. Note that the metabolic cost weight, <italic>γ</italic>, is fixed to the same value, which was picked to guarantee low error under the baseline decoder for <italic>t</italic><sub>end</sub> = 1000ms only (see Methods <xref ref-type="sec" rid="s4c">Section 4.3</xref>). Thus, the error rises as the endpoint time decreases below this, as higher magnitude motor commands are necessary to achieve low error faster. However, note that the difference between baseline decoder, WMPs, and OMPs remains the same even at these lower values of <italic>t</italic><sub>end</sub>.</p>
<p>c. Calibration task response and reachable manifold variance cumulatively accounted for by each dimension of the intrinsic manifold, as in <xref rid="fig3" ref-type="fig">fig. 3g</xref>. Intrinsic manifold was found to be about 12-dimensional with inhibition-optimized connectivity and about 6-dimensional with connectivity optimized for delayed center-out reaching.</p>
<p>d. Maximal cursor progress for each target and WMP as a function of target direction angle with <inline-formula><inline-graphic xlink:href="589952v2_inline221.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,as in <xref rid="fig4" ref-type="fig">fig. 4c</xref>.</p>
<p>e. Mean squared error achieved for each OMP by re-aiming with different numbers of command variables, <inline-formula><inline-graphic xlink:href="589952v2_inline222.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, as in <xref rid="fig5" ref-type="fig">fig. 5a</xref>.</p></caption>
<graphic xlink:href="589952v2_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s5a1b">
<title>Inhibition-optimized excitatory/inhibitory connectivity</title>
<p>a sparse and balanced E/I recurrent connectivity matrix was constructed following the optimization procedure described in.<sup><xref ref-type="bibr" rid="c89">89</xref></sup> In short, the excitatory weights were initialized to be very strong, and then inhibitory weights were optimized to ensure the dynamics were stable (by minimizing the spectral abscissa of the full connectivity matrix). Half of the neurons were assigned to be excitatory, and the inhibitory weights were enforced to be on average three times stronger than the excitatory weights. The only difference with<sup><xref ref-type="bibr" rid="c89">89</xref></sup> was that the weight matrices were initialized with a spectral radius of 5, rather than 10. This was necessary as we found that an initial spectral radius of 10 lead to chaotic dynamics under constant input. Input and encoding weights were sampled randomly as for the randomly connected network in the main text.</p>
<p>Because of their highly non-normal dynamics, these networks were highly sensitive to changes in initial conditions, even with the reduced initial spectral radius. We therefore reduced the standard deviation of the initial conditions by half when simulating the calibration task (see Methods <xref ref-type="sec" rid="s4g">Section 4.7</xref>). These networks produced much higher-dimensional calibration task responses than the randomly connected network, so a 12-dimensional intrinsic manifold was used for constructing WMP’s and OMP’s (i.e. <italic>ℓ</italic> = 12).</p>
</sec>
<sec id="s5a1c">
<title>Connectivity optimized for delayed center-out reaching</title>
<p>network weights were optimized to produce joint torques for performing delayed center-out reaches with a biomechanical arm model. The architecture and optimization scheme followed that used by,<sup><xref ref-type="bibr" rid="c90">90</xref></sup> in which the recurrent network is driven by two distinct inputs. The first input is a one-dimensional signal reflecting a go cue that indicates when the reach should be performed (go time). This was built into our model by setting <italic>θ</italic><sub><italic>K</italic></sub> to 1 at the start of the trial and then setting it to 0 at go time, 1000ms after trial start. The other input is a two-dimensional signal reflecting the visual presentation of the target to reach towards, presented prior to go time to prepare the subject (or network) to perform the delayed center-out reach. This was built into our model by setting <italic>θ</italic><sub>1</sub>, <italic>θ</italic><sub>2</sub> to the coordinates of the reach target at a randomly sampled target presentation time before the go cue, and then setting it back to 0 at the same time the go cue input is shut off. All other command variables are set to 0 (<italic>θ</italic><sub>3</sub> = <italic>θ</italic><sub>4</sub> = … = <italic>θ</italic><sub><italic>K</italic>−1</sub> = 0). We chose to encode the go cude with the very last command variable, <italic>θ</italic><sub><italic>K</italic></sub>, to reflect the hypothesis that subjects would not re-aim with this non-directional command variable, neither in <inline-formula><inline-graphic xlink:href="589952v2_inline121.gif" mime-subtype="gif" mimetype="image"/></inline-formula> -dimensional re-aiming nor in generalized re-aiming with up to <inline-formula><inline-graphic xlink:href="589952v2_inline122.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables.</p>
<p>Two joint torques were read out from the network through a set of readout weights, which were optimized along with the input, recurrent, and encoding weights. The weights were optimized to produce the joint torques required to move the endpoint of a planar two-link arm model<sup><xref ref-type="bibr" rid="c91">91</xref></sup> to the cued reach target, in 500ms with a bell-shaped speed profile. Following the methods of,<sup><xref ref-type="bibr" rid="c92">92</xref></sup> these target joint torques were computed by backpropagating through the arm model dynamics to minimize mean squared error between the arm endpoint velocity and the desired velocity profile for each reach target. We then trained the network weights so that in each trial it would produce 0 torque until go time, followed by the optimal reaching torque corresponding to the reach target on that trial.</p>
<p>The loss function used to optimize the network weights was a combination of the mean squared error plus L2 regularization on all weights and on network firing rates, to encourage naturalistic solutions to this task.<sup><xref ref-type="bibr" rid="c90">90</xref></sup> This was minimized via stochastic gradient descent using the Adam optimization algorithm<sup><xref ref-type="bibr" rid="c93">93</xref></sup> with learning rate set to .001. Since only three command variables were non-zero during this task, only the three corresponding columns of the encoding weights <italic>U</italic><sub><italic>ij</italic></sub> were optimized by this procedure. The remaining columns were thus fixed to their random initialization, as for the randomly connected network in the main text.</p>
<p>As is often observed in networks trained to perform a single task,<sup><xref ref-type="bibr" rid="c94">94</xref></sup> the resulting optimized recurrent connectivity matrix had low rank (<xref rid="figS1" ref-type="fig">Supplementary Figure S1a</xref>). Its activity was consequently constrained to a much lower-dimensional subspace than that of the other networks.<sup><xref ref-type="bibr" rid="c95">95</xref></sup> This network thus produced much lower-dimensional calibration task responses than the randomly connected networks did. We therefore used a 6-dimensional intrinsic manifold for constructing WMP’s and OMP’s in simulations with this network (i.e. <italic>ℓ</italic> = 6). This meant that only 6! − 1 = 719 possible decoder perturbations existed (as opposed to 40,319), so far fewer decoder perturbations satisfied the stringent criteria outlined in Methods <xref ref-type="sec" rid="s4h">Section 4.8</xref> for sampling WMP’s and OMP’s. We thus loosened these criteria to include WMP’s and OMP’s with mean squared error going up to 1.2. These networks were also found to be highly sensitive to noise, so we reduced the standard deviation of the noise in the dynamics and in the initial conditions during simulation of the calibration task to 0.02 and 0.005, respectively.</p>
<p>Because of the low-rank recurrent connectivity, we found that re-aiming with even <inline-formula><inline-graphic xlink:href="589952v2_inline123.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables could not yield good solutions for OMP control with this network (<xref rid="figS1" ref-type="fig">Supplementary Figure S1e</xref>). In other words, the low-rank connectivity did not permit the generation of activity patterns outside of the intrinsic manifold, even when re-aiming with a large nubmer of command variables. It is important to keep in mind, however, that in reality motor cortical connectivity is likely optimized to perform a wide variety of motor behaviors, rather than a single center-out reaching task. This assumption is implicit in our choice of high-rank connectivity structure, as in several other recent models of motor cortical function.<sup><xref ref-type="bibr" rid="c89">89</xref>,<xref ref-type="bibr" rid="c92">92</xref>,<xref ref-type="bibr" rid="c96">96</xref></sup> Note also that such high-rank connectivity was necessary for our model to produce calibration task responses with dimensionality near the dimensionality of 10 observed by Sadtler et al. (2014) (<xref rid="figS1" ref-type="fig">Supplementary Figure S1c</xref>).</p>
</sec>
</sec>
<sec id="s5a2">
<label>S.1.2</label>
<title>Analysis of WMP bias</title>
<p>This section provides a more detailed analysis of why WMP reachable readouts are biased in the direction of <inline-formula><inline-graphic xlink:href="589952v2_inline124.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref rid="fig4" ref-type="fig">fig. 4c</xref>). Specifically, we show that the reachable readouts are centered away from the origin, and the direction of this displacement is dictated by the relationship between the reachable manifold and the calibration task activity that the decoders are fit to, leading to the observed bias.</p>
<p>We begin by calculating the centroid of the reachable readouts, <inline-formula><inline-graphic xlink:href="589952v2_inline125.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,which is given by the readout of the reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline126.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,
<disp-formula id="eqn36">
<graphic xlink:href="589952v2_eqn36.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>c</bold> is the decoder centering vector (<xref ref-type="disp-formula" rid="eqn3">equation 3</xref>), set to the mean population response during the calibration task (Methods <xref ref-type="sec" rid="s4h">Section 4.8</xref>). As long as the reachable readouts are somewhat symmetrically distributed around their centroid, then they will be biased in the direction of their centroid. This equation shows that the direction and magnitude of this bias thus depends on the difference between between the mean calibration task response, <bold>c</bold>, and the reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline127.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>Because the calibration task responses are driven by a subset of the motor commands used to define the reachable manifold (<xref rid="fig3" ref-type="fig">fig. 3e</xref>), these two directions are highly aligned. This is shown empirically for our simulations in <xref rid="figS2" ref-type="fig">Supplementary Figure S2a</xref>, where we overlay the network’s firing rates during individual trials of the calibration task on top of the reachable manifold, along with the mean population response and the reachable manifold centroid. Population activity during the calibration task evidently evolves along the same directions in state space occupied by the reachable manifold, and thus its mean, <bold>c</bold>, is highly aligned with the manifold’s centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline128.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>Next, note that the reachable manifold centroid has a larger norm than the calibration task mean. That is because the reachable manifold contains activity patterns generated by motor commands with larger norms than those driving the calibration task responses (<xref rid="fig3" ref-type="fig">fig. 3e</xref>), so its centroid comprises higher firing rates. The underlying reason for this is that, in our simulations, we selected the metabolic cost weight, <italic>γ</italic>, such that it guaranteed high re-aiming performance with the baseline decoder (see Methods <xref ref-type="sec" rid="s4c">Section 4.3</xref>). Because the baseline decoder fit is not perfect (due both to the noise and to the non-linear mapping from calibration task stimuli to neural activity), stronger firing rates than those evoked by the calibration task are required to achieve such high performance, so the resulting metabolic cost weight permits stronger motor commands than the ones driving the calibration task.</p>
<p>Putting these two observations together, we have that
<disp-formula id="eqn37">
<graphic xlink:href="589952v2_eqn37.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
In other words, the reachable readout centroid points in the direction of <inline-formula><inline-graphic xlink:href="589952v2_inline129.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.This explains why WMP readouts are biased in that direction.</p>
<p>An important remaining question, however, is why <italic>baseline</italic> decoder readouts are not biased in that direction. The baseline decoder shares the same centering vector, <bold>c</bold>, so, by the above logic, should inherit the same bias. The reason it does not is that the reachable manifold centroid is orthogonal to the baseline decoder, <inline-formula><inline-graphic xlink:href="589952v2_inline130.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, so the reachable readouts are centered at the origin despite <xref ref-type="disp-formula" rid="eqn37">equation 37</xref> holding true. Because the baseline decoder is fit to predict the calibration task stimuli from the neural responses they elicit (Methods <xref ref-type="sec" rid="s4h">Section 4.8</xref>), by construction it ignores any directions of calibration task activity that do not provide information about the stimulus. One such direction is their mean, <inline-formula><inline-graphic xlink:href="589952v2_inline131.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.This can be appreciated from <xref rid="figS2" ref-type="fig">Supplementary Figure S2a</xref>, where it is evident that the trajectories of activity during different trials of the calibration task all evolve identically along this direction, despite being evoked by different stimuli. Thus, decoding from this direction is useless for decoding the stimulus identity, so the baseline decoder ignores it by spanning an orthogonal subspace. However, while this direction may not contain information about the calibration task stimuli, it does contain a lot of the variance of the calibration task neural responses. Consequently, it resides within the intrinsic manifold, and thus WMP’s – which are essentially randomly oriented within the intrinsic manifold – are likely to be aligned with it by chance.</p>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2:</label>
<caption><title>Analysis of WMP readout bias in the direction of <inline-formula><inline-graphic xlink:href="589952v2_inline223.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</title>
<p>a. Simulated motor cortical responses in the calibration task, color-matched to the motor commands in <xref rid="fig3" ref-type="fig">fig. 3e</xref> driving these responses. These are plotted together with the same reachable manifold activity patterns from <xref rid="fig3" ref-type="fig">fig. 3f</xref>, projected onto the same three principal components. The open circles in the interior of this conical structure show the calibration task mean, <bold>c</bold>, in light purple and the reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline224.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, in dark purple, each connected to the origin by a line for visualization purposes. Note that, by definition, the calibration task neural responses at time <italic>t</italic><sub>end</sub> = 1000ms (the last point in each trajectory) lie almost exactly on the reachable manifold, slightly offset only because of noise in the response dynamics (see Methods <xref ref-type="sec" rid="s4g">Section 4.7</xref>).</p>
<p>b. Norm of <inline-formula><inline-graphic xlink:href="589952v2_inline225.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for each sampled WMP decoder, <bold>D</bold>, for each simulated model motor cortical network (see Supplementary Materials <xref ref-type="sec" rid="s5a1">Section S.1.1</xref>). Overlaid with an open black square is the norm of <inline-formula><inline-graphic xlink:href="589952v2_inline226.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the baseline decoder of each corresponding simulation.</p></caption>
<graphic xlink:href="589952v2_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>This is confirmed empirically in <xref rid="figS2" ref-type="fig">fig. S2b</xref>, where we plot the norm of <inline-formula><inline-graphic xlink:href="589952v2_inline132.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the baseline decoder and each of the sampled WMPs for each simulation we studied. The norm is evidently much higher for WMP decoders than for the baseline decoder in each simulation, explaining why the reachable readout bias manifests itself in WMP decoders but not in the baseline decoder.</p>
</sec>
<sec id="s5a3">
<label>S.1.3</label>
<title>Non-negative firing rates are necessary to replicate biases in WMP learning</title>
<p>Here we demonstrate that removing the non-negativity constraint on firing rates precludes our model from reproducing the behavioral biases in WMP reachable readouts. We test this by replacing the activation function with the identity function, <italic>ϕ</italic>(<italic>x</italic>) = <italic>x</italic>, and repeating the analysis of <xref rid="fig4" ref-type="fig">fig. 4c</xref> to check if the maximal cursor progress under each WMP is highest in the direction of <inline-formula><inline-graphic xlink:href="589952v2_inline133.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.We find that they in fact are not (<xref rid="figS3" ref-type="fig">Supplementary Figure S3a</xref>)</p>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3:</label>
<caption><title>Removing the non-negativity constraint fails to reproduce experimentally observed biases in WMP learning.</title>
<p>a. Maximal cursor progress in each target direction as a function of angle with <inline-formula><inline-graphic xlink:href="589952v2_inline227.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,for 20 sampled WMPs. Only 20 WMP’s were used as we found that the intrinsic manifold of the linear network was only <italic>ℓ</italic> = 5-dimensional, so we correspondingly adjusted the criteria for subsampling WMP’s and OMP’s (cf. Supplementary Materials <xref ref-type="sec" rid="s5c3">Section S.3.3</xref>) and found that only 20 WMP’s and 60 OMP’s satisfied them. As was done in <xref rid="fig4" ref-type="fig">fig. 4c</xref>, the reachable manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline228.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is estimated using simulated mean firing rates during baseline decoder control (see Methods <xref ref-type="sec" rid="s4f">Section 4.6</xref>). Maximal cursor progress was calculated exactly as in <xref ref-type="disp-formula" rid="eqn6">equation 6</xref>, following the same procedure as in the main text for selecting <italic>s</italic><sub>max</sub> (cf. Methods <xref ref-type="sec" rid="s4d">Section 4.4</xref>). A total of 8 target directions × 20 sampled WMPs = 160 points are plotted.</p>
<p>b. Activity patterns in the reachable manifold at endpoint time <italic>t</italic><sub>end</sub> = 1000ms, with <inline-formula><inline-graphic xlink:href="589952v2_inline229.gif" mime-subtype="gif" mimetype="image"/></inline-formula>non-zero command variables. Calibration task responses to each of the eight radial reach stimuli are overlaid in shades of pink, following exactly the same conventions as in <xref rid="figS2" ref-type="fig">Supplementary Figure S2a</xref>. These <italic>N</italic> -dimensional activity patterns are projected onto the top two principal components of the reachable manifold (PC1 and PC2) and the orthogonal dimension capturing the most calibration task response variance (PC3). Because the network dynamics are linear, the reachable manifold is exactly <inline-formula><inline-graphic xlink:href="589952v2_inline230.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-dimensional, so PC1 and PC2 capture 100% of the variance in activity patterns within it. The small open black circle at the center marks the origin of the state space. The light and dark purple open circles at the origin mark the calibration task mean <bold>c</bold> the reachable manifold centroid <inline-formula><inline-graphic xlink:href="589952v2_inline231.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in purple, respectively. The latter is barely visible because they overlap almost completely.</p>
<p>c. Readouts reachable through an example WMP, following the same color conventions as in <xref rid="fig3" ref-type="fig">fig. 3c</xref>. The green diamonds show the eight target readouts from the radial cursor reaching task.</p></caption>
<graphic xlink:href="589952v2_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The reason why can again be gleaned from looking at the relationship between the reachable repertoire centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline134.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,the mean calibration task response, <bold>c</bold>. In this case, because firing rates are allowed to be negative, both the reachable manifold and the calibration task neural responses are centered near the origin, and thus the reachable readouts are as well (<xref ref-type="disp-formula" rid="eqn36">equation 36</xref>),
<disp-formula id="eqn38">
<graphic xlink:href="589952v2_eqn38.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We can see this empirically in <xref rid="figS3" ref-type="fig">Supplementary Figure S3b</xref>, where we plot a projection of the reachable manifold with the calibration task neural responses overlaid (analogous to <xref rid="figS2" ref-type="fig">Supplementary Figure S2a</xref>). We see that both are centered around the origin, with their means exactly on top of each other. The reachable readouts through a representative WMP are shown in <xref rid="figS3" ref-type="fig">Supplementary Figure S3c</xref>, illustrating the fact that they are consequently also centered at the origin. The maximal cursor progress is higher in some directions than in others (in this case in the NW and SE directions), but the bias is not unidirectional as it is in the model with non-negative firing rates (<xref rid="fig4" ref-type="fig">fig. 4a</xref>, <xref rid="fig4" ref-type="fig">fig. 4c</xref>) or in the experimental data (<xref rid="fig4" ref-type="fig">fig. 4d</xref>).</p>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4:</label>
<caption><title>Operant conditioning of ensembles of 10 neurons by re-aiming.</title>
<p>a. Difference in ensembles’ summed firing rates (at <italic>t</italic><sub>end</sub> = 1000 ms) produced by the re-aiming solutions optimized for ensemble <italic>a</italic> being the target, for 500 randomly sampled ensembles of 10 neurons, plotted as a function of the correlation between the two ensembles’ summed firing rates during simulated spontaneous behavior. Here, <inline-formula><inline-graphic xlink:href="589952v2_inline232.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="589952v2_inline233.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denote the summed firing rates of neurons in ensemble <italic>a</italic> and <italic>b</italic>, respectively.</p>
<p>b. Activity of indirect neurons (at <italic>t</italic><sub>end</sub> = 1000 ms) for the same ensembles and re-aiming solutions in previous panel, plotted as a function of mean correlation coefficeint with neurons in ensemble <italic>a</italic> during simulated spontaneous behavior.</p></caption>
<graphic xlink:href="589952v2_figS4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s5a4">
<label>S.1.4</label>
<title>Operant conditioning of ensembles of neurons</title>
<p>Clancy et al. (2014) conditioned ensembles of up to 10 neurons, rather than only pairs of neurons. Here we repeat our simulations of operant conditioning but with a pair of target/distractor <italic>ensembles</italic> of 10 neurons, rather than a pair of single neurons. We simulate re-aiming in exactly the same way via <xref ref-type="disp-formula" rid="eqn29">equation 29</xref>, but where now the decoding vector <bold>d</bold> is a vector with a +1 for each target ensemble neuron and a -1 for each distractor ensemble neuron, and 0’s everywhere else. Selection of neurons used for operant conditioning and simulation of spontaneous behavior was done exactly as described in Methods <xref ref-type="sec" rid="s4j">Section 4.10</xref>.</p>
<p>We find that the main results from the main text are replicated in this setting as well: (1) the correlation coefficient of ensembles’ mean firing rates during simulated spontaneous behavior is correlated with the difference in mean firing rates achieved by the optimal re-aiming solutions (<xref rid="figS4" ref-type="fig">Supplementary Figure S4a</xref>); and (2) indirect neuron firing rates produced by these optimal re-aiming solutions are correlated with their mean correlation coefficient with target ensemble neurons during simulated spontaneous behavior (<xref rid="figS4" ref-type="fig">Supplementary Figure S4b</xref>).</p>
</sec>
<sec id="s5a5">
<label>S.1.5</label>
<title>Changes in modulation depth of motor cortical tuning curves</title>
<p>In a study employing credit assignment rotation perturbations in a 3D cursor reaching task, it was observed that both non-rotated and rotated neurons reduced their modulation depth after learning the perturbed decoder, and that rotated neurons reduced their modulation depth more<sup><xref ref-type="bibr" rid="c97">97</xref></sup> (see text surrounding Methods <xref ref-type="sec" rid="s4i">Section 4.9</xref>, <xref ref-type="disp-formula" rid="eqn26">equation 26</xref> for how modulation depth is defined and measured). <xref rid="figS5" ref-type="fig">Figure S5a</xref> reveals that our model of generalized re-aiming does not reproduce this result, at least for the values of <inline-formula><inline-graphic xlink:href="589952v2_inline135.gif" mime-subtype="gif" mimetype="image"/></inline-formula> we tested. Generalized re-aiming with up to 6 command variables seems to lead to slight <italic>increases</italic> in the modulation depths of both rotated and non-rotated neurons, with marginal differences between rotated and non-rotated neurons.</p>
<p>While we did not simulate re-aiming with <inline-formula><inline-graphic xlink:href="589952v2_inline136.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables in the context of credit assignment rotation perturbations, we did simulate this in the context of OMP learning, where we found that modulation depths of indirect neurons decreased as <inline-formula><inline-graphic xlink:href="589952v2_inline137.gif" mime-subtype="gif" mimetype="image"/></inline-formula> increased (<xref rid="figS5" ref-type="fig">figure S5b</xref>). The fact that only generalized re-aiming with a large number of command variables – but not regular re-aiming with only <inline-formula><inline-graphic xlink:href="589952v2_inline138.gif" mime-subtype="gif" mimetype="image"/></inline-formula> variables – can reproduce selective changes in modulation depth is consistent with the separate observation that indirect neurons show selective decreases in modulation depth only after days of practice with a given BCI decoder, but not within a single session.<sup><xref ref-type="bibr" rid="c98">98</xref></sup></p>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5:</label>
<caption><title>Changes in modulation depth under generalized re-aiming.</title>
<p>a. Average change in modulation depth of rotated, non-rotated, and indirect neurons between simulated reaches with the baseline decoder and each perturbed decoder, plotted as a function of the number of command variables used for re-aiming <inline-formula><inline-graphic xlink:href="589952v2_inline234.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.As in <xref rid="fig6" ref-type="fig">fig. 6e</xref>, the changes in modulation depth are averaged over all neurons in each sub-population, and the median over all 100 sampled credit assignment rotation perturbations is plotted. Error bars mark the upper and lower quartiles.</p>
<p>b. Average modulation depth of direct neurons (neurons recorded by the BCI, with non-zero decoding weights in <bold>D</bold>) and indirect neurons (neurons not recorded by the BCI) under generalized re-aiming solutions for OMP’s. Modulation depths are averaged over all neurons in each sub-population, and the median over all 100 sampled OMPs is plotted. Error bars marks the upper and lower quartiles.</p></caption>
<graphic xlink:href="589952v2_figS5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s5a6">
<label>S.1.6</label>
<title>Closed-loop feedback control</title>
<p>All of the models we have considered so far are models of <italic>open-loop</italic> control: once the optimal motor command is specified, it is used to drive the motor cortical population for the duration of the movement, unchanged until the pre-specified endpoint time <italic>t</italic><sub>end</sub>. Any errors encountered along the way – either due to noise or suboptimal motor commands – are thus ignored. A better strategy would be <italic>closed-loop</italic> control, wherein errors observed via sensory feedback are used to adaptively modify the motor command online. Under this strategy, errors that are encountered along the way can be corrected, thus improving the accuracy of the desired BCI output. Such closed-loop feedback control strategies are well known to be optimal in the presence of noise,<sup><xref ref-type="bibr" rid="c99">99</xref>,<xref ref-type="bibr" rid="c100">100</xref></sup> and substantial evidence exists that non-human primates utilize them during BCI control.<sup><xref ref-type="bibr" rid="c101">101</xref>–<xref ref-type="bibr" rid="c103">103</xref></sup></p>
<p>In this section, we consider a re-aiming-based model of closed-loop feedback control in which the motor command is continuously updated in response to sensory feedback. We evaluate this model on the Sadtler et al. (2014) BCI learning task, and confirm that closed-loop re-aiming suffers from the same limitations as open-loop re-aiming: the set of reachable activity patterns is limited by the number of command variables used for control, such that OMP’s cannot be learned with a small number of them.</p>
<p>We assume an error feedback controller architecture of the following form,
<disp-formula id="eqn39">
<graphic xlink:href="589952v2_eqn39.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the command variables, <inline-formula><inline-graphic xlink:href="589952v2_inline139.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,vary continuously in time according to an affine transformation of the instantaneous error, <bold>y</bold><sup>*</sup> − <bold>y</bold>(<italic>t</italic>). As in the open-loop control simulations, all additioanl command variables beyond the first <inline-formula><inline-graphic xlink:href="589952v2_inline140.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are fixed to 0. For simplicity, we assume a linear encoding of the motor command in the upstream inputs,
<disp-formula id="eqn40">
<graphic xlink:href="589952v2_eqn40.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For a given decoder, <bold>D</bold>, we postulate that the subject learns a feedback controller, <inline-formula><inline-graphic xlink:href="589952v2_inline141.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,that minimizes the following loss function:
<disp-formula id="eqn41">
<graphic xlink:href="589952v2_eqn41.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>y</bold> <italic>t</italic>; <bold>G, b</bold> denotes the readout produced at time <italic>t</italic> under the closed-loop dynamics <inline-formula><inline-graphic xlink:href="589952v2_inline142.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the squared Frobenius norm, and <inline-formula><inline-graphic xlink:href="589952v2_inline143.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the eight radial reach targets in the BCI cursor control task. The time window of control was set to <italic>T</italic> = 1000ms.</p>
<p>To compute <inline-formula><inline-graphic xlink:href="589952v2_inline144.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="589952v2_inline145.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,we used gradient descent on the above loss function, using the Adam optimization algorithm<sup><xref ref-type="bibr" rid="c93">93</xref></sup> with a initial learning rate of .01. To facilitate numerical optimization, deterministic dynamics were used (no noise in the dynamics or in the initial conditions, which were fixed to 0). To avoid poor local optima (which was often a problem with WMPs in particular), we ran gradient descent from five different random initializations and used the best solution from these five runs.</p>
<p>We computed <inline-formula><inline-graphic xlink:href="589952v2_inline146.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the randomly connected non-linear network analyzed in the main text (equation 1), for each of the sampled baseline/WMP/OMP decoders used in the simulations presented in the main text. For the baseline decoder we performed this optimization over multiple values of the metabolic cost weight <italic>γ</italic> so as to identify the largest value of <italic>γ</italic> that permitted a time-averaged squared error of less than .05 for all eight target readouts under this decoder (analogous to how <italic>γ</italic> was set in the open-loop simulations in the main text, Methods <xref ref-type="sec" rid="s4c">Section 4.3</xref>). We then fixed <italic>γ</italic> to this value for all the decoder perturbations.</p>
<p>Simulations of cursor control with <inline-formula><inline-graphic xlink:href="589952v2_inline147.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables are shown in <xref rid="figS6" ref-type="fig">Supplementary Figure S6a</xref>, where we plot the mean squared error over target readouts as a function of time. Each trace corresponds to the mean squared error achieved by the optimal feedback controller for a given decoder, with one trace for the baseline decoder and one for each of the 100 sampled WMPs and OMPs. We find that for almost all decoders, the mean squared error decreases to a certain level and remains low for the rest of this time window of 1000ms. However, this asymptotic error value is typically higher for OMP’s than for WMP’s (<xref rid="figS6" ref-type="fig">Supplementary Figure S6b</xref>), replicating the analogous result observed for the open-loop control model presented in the main text (<xref rid="fig3" ref-type="fig">fig. 3d</xref>).</p>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6:</label>
<caption><title>Closed-loop re-aiming reproduces the differences in WMP and OMP learning.</title>
<p>a. Mean squared error (mean over target readouts) achieved by closed loop control with <inline-formula><inline-graphic xlink:href="589952v2_inline235.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables, as a function of time. Each line corresponds to performance on a different decoder, with a correspondingly optimized feedback controller, <inline-formula><inline-graphic xlink:href="589952v2_inline236.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn41">equation 41</xref>).</p>
<p>b. Mean squared error (mean over target readouts and over time) achieved by error feedback controllers with <inline-formula><inline-graphic xlink:href="589952v2_inline237.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables. Each point corresponds to a different decoder, with medians over all decoders in each class marked by the height of the bars.</p>
<p>c. Mean squared error (mean over target readouts and over time) achieved by error feedback controllers optimized for each OMP, with <inline-formula><inline-graphic xlink:href="589952v2_inline238.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and 10 command variables. Light blue points denote this quantity for individual OMP’s, larger open circles on top show the median. For reference, dotted horizontal lines show the mean squared error achieved by optimized error feedback with <inline-formula><inline-graphic xlink:href="589952v2_inline239.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables for the baseline decoder (black) and WMP’s (red); the red dotted line shows the median over all sampled WMP’s with shading marking the upper and lower quartiles.</p></caption>
<graphic xlink:href="589952v2_figS6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>This again reflects the limitations of re-aiming with only <inline-formula><inline-graphic xlink:href="589952v2_inline148.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables. In this case, this manifests itself in restricting how the error can be fed back into the network: the error gets mapped to a <inline-formula><inline-graphic xlink:href="589952v2_inline148a.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-dimensional vector through <xref ref-type="disp-formula" rid="eqn39">equation 39</xref> before being fed back into the network. As we saw occurs for the open-loop controller, this results in a restriction of how population activity can be modulated, making it difficult to generate the patterns of activity required to produce the target readouts under OMP’s. <xref rid="figS6" ref-type="fig">Supplementary Figure S6c</xref> shows that these restrictions can be relaxed by increasing the number of command variables used for re-aiming, <inline-formula><inline-graphic xlink:href="589952v2_inline149.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.In this case, re-aiming with only <inline-formula><inline-graphic xlink:href="589952v2_inline150.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables suffices to obtain a mean squared error less than 0.1 with OMP’s. Interestingly, this is substantially less than the <inline-formula><inline-graphic xlink:href="589952v2_inline151.gif" mime-subtype="gif" mimetype="image"/></inline-formula> command variables that are necessary to achieve the same level of performance with open-loop re-aiming (<xref rid="fig5" ref-type="fig">fig. 5a</xref>).</p>
</sec>
</sec>
<sec id="s5b">
<label>S.2</label>
<title>Mathematical derivations</title>
<sec id="s5b1">
<label>S.2.1</label>
<title>Scale-invariance of RNN dynamics with rectified linear activation function</title>
<p>Here we prove that, whenever <italic>x</italic><sub><italic>i</italic></sub>(0) = 0,
<disp-formula id="eqn42">
<graphic xlink:href="589952v2_eqn42.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>r</bold>(<italic>t</italic>; <italic>s</italic><bold><italic>θ</italic></bold>) = <italic>ϕ</italic>(<bold>x</bold>(<italic>t</italic>; <italic>s</italic><bold><italic>θ</italic></bold>)) and <bold>x</bold>(<italic>t</italic>; <italic>s</italic><bold><italic>θ</italic></bold>) is the solution to equation 1 with inputs defined by <xref ref-type="disp-formula" rid="eqn2">equation 2</xref>. The function <italic>ϕ</italic>(°) is the rectified linear activation function defined in equation 1.</p>
<p>We begin by demonstrating that, when <italic>x</italic><sub><italic>i</italic></sub>(0) = 0,
<disp-formula id="eqn43">
<graphic xlink:href="589952v2_eqn43.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We prove this by showing that the dynamics of <italic>s</italic><bold>x</bold>(<italic>t</italic>; <bold><italic>θ</italic></bold>) are the same as those of <bold>x</bold>(<italic>t</italic>; <italic>s</italic><bold><italic>θ</italic></bold>):
<disp-formula id="eqn44">
<graphic xlink:href="589952v2_eqn44.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where in the second line we plugged in equation 1 and <xref ref-type="disp-formula" rid="eqn2">equation 2</xref> for the dynamics and upstream inputs, respectively, and in the third line we used the scale-invariance of the rectified linear activation function,
<disp-formula id="eqn45">
<graphic xlink:href="589952v2_eqn45.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
It is easy to see that <xref ref-type="disp-formula" rid="eqn44">equation 44</xref> exactly matches equation 1 but with <italic>s</italic><bold>x</bold> substituted in for <bold>x</bold>; that is, the dynamics of these two quantities are the same. Therefore, whenever the initial conditions match, <italic>s</italic><bold>x</bold>(0; <bold><italic>θ</italic></bold>) = <bold>x</bold>(0; <italic>s</italic><bold><italic>θ</italic></bold>), then their trajectories will too. It is easy to see that this condition holds for any <italic>s</italic> if <italic>x</italic><sub><italic>i</italic></sub>(0) = 0, thus proving <xref ref-type="disp-formula" rid="eqn43">equation 43</xref>.</p>
<p>Along with the scale invariance of the activation function (<xref ref-type="disp-formula" rid="eqn45">equation 45</xref>), <xref ref-type="disp-formula" rid="eqn43">equation 43</xref> implies <xref ref-type="disp-formula" rid="eqn42">equation 42</xref>:
<disp-formula id="eqn46">
<graphic xlink:href="589952v2_eqn46.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
thus completing our proof.</p>
</sec>
<sec id="s5b2">
<label>S.2.2</label>
<title>Large <italic>M</italic> limit of quadratic metabolic cost</title>
<p>Here we derive the large <italic>M</italic> limit of the quadratic metabolic cost term in the reaiming objective function (<xref ref-type="disp-formula" rid="eqn4">equation 4</xref>),
<disp-formula id="eqn47">
<graphic xlink:href="589952v2_eqn47.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We first note that each term in the sum depends on a sum over the randomly sampled encoding weights (<xref ref-type="disp-formula" rid="eqn2">equation 2</xref>),
<disp-formula id="eqn48">
<graphic xlink:href="589952v2_eqn48.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
If the encoding weights, <italic>U</italic><sub><italic>ij</italic></sub>, are independent and identically distributed, then each of the terms in this sum is also independent and identically distributed. By the law of large numbers, then, as <italic>M → ∞</italic> their sum will approach an expectation over this distribution,
<disp-formula id="eqn49">
<graphic xlink:href="589952v2_eqn49.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="589952v2_inline152.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denotes an expectation over the probability distribution of the encoding weights, <italic>U</italic><sub><italic>ij</italic></sub>.</p>
<p>This expectation can be evaluated by first defining the random variable <inline-formula><inline-graphic xlink:href="589952v2_inline153.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to express the expectation as an integral over <italic>z</italic>, and then exploiting the rectified linear activation function (equation 1) to simplify this integral,
<disp-formula id="eqn50">
<graphic xlink:href="589952v2_eqn50.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we simply exploited the fact that <italic>ϕ</italic>(<italic>z</italic>) = 0 when <italic>z &lt;</italic> 0 and <italic>ϕ</italic>(<italic>z</italic>) = <italic>z</italic> when <italic>z</italic> ≥ 0. If the distribution of the encoding weights <italic>U</italic><sub><italic>ij</italic></sub> is symmetric around 0, then the distribution of <italic>z</italic> is as well and we have that
<disp-formula id="eqn51">
<graphic xlink:href="589952v2_eqn51.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Finally, if the encoding weights are zero-mean and independent, we have that
<disp-formula id="eqn52">
<graphic xlink:href="589952v2_eqn52.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
If <italic>U</italic><sub><italic>ij</italic></sub> additionally have unit variance, <inline-formula><inline-graphic xlink:href="589952v2_inline154.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.Putting this all together, we arrive at the equality in <xref ref-type="disp-formula" rid="eqn9">equation 9</xref>:
<disp-formula id="eqn53">
<graphic xlink:href="589952v2_eqn53.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s5b3">
<label>S.2.3</label>
<title>Reachable manifold moments for <inline-formula><inline-graphic xlink:href="589952v2_inline155.gif" mime-subtype="gif" mimetype="image"/></inline-formula></title>
<p>In our analysis of the reachable manifold, we characterized its location and shape via its centroid and covariance, which were evaluated as expectations over a uniform distribution on the manifold. Here we derive the probability density function of this distribution and use it to calculate these expections.</p>
<p>We begin with the case of <inline-formula><inline-graphic xlink:href="589952v2_inline156.gif" mime-subtype="gif" mimetype="image"/></inline-formula> non-zero command variables, which we parameterize by their polar coordinates,
<disp-formula id="eqn54">
<graphic xlink:href="589952v2_eqn54.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We then formally define the reachable manifold as follows:
<disp-formula id="eqn55">
<graphic xlink:href="589952v2_eqn55.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>r</bold>(<italic>t</italic><sub>end</sub>; <italic>s, φ</italic>) is the motor cortical activity pattern at time <italic>t</italic><sub>end</sub> produced by a pair of command variables <italic>θ</italic><sub>1</sub>, <italic>θ</italic><sub>2</sub> with angle <italic>φ</italic> and norm <italic>s</italic>, with all other command variables set to 0 (<italic>θ</italic><sub>3</sub> = <italic>θ</italic><sub>4</sub> = … = <italic>θ</italic><sub><italic>K</italic></sub> = 0). The function <bold>r</bold>(<italic>t</italic><sub>end</sub>; <italic>s, φ</italic>) can be thought of as a function mapping 2D command variables, (<italic>s, φ</italic>) ∈ [0, <italic>s</italic><sub>max</sub>] × [0, 2<italic>π</italic>], to activity patterns, <bold>r</bold> ∈ ℝ <sub><italic>N</italic></sub>, on the 2D surface constituting the reachable manifold (the conical surface shown in <xref rid="fig3" ref-type="fig">fig. 3f</xref>).</p>
<p>The probability density function of the uniform distribution on this 2D surface in ℝ <sub><italic>N</italic></sub> is given by its area element, <italic>dV</italic> (<italic>s, φ</italic>), divided by its total area,
<disp-formula id="eqn56">
<graphic xlink:href="589952v2_eqn56.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The area element and total area are given by
<disp-formula id="eqn57">
<graphic xlink:href="589952v2_eqn57.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn58">
<graphic xlink:href="589952v2_eqn58.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where det[°] denotes the matrix determinant and <bold>J</bold> denotes the <italic>N</italic> × 2 Jacobian of the mapping from command variables to the reachable manifold,
<disp-formula id="eqn59">
<graphic xlink:href="589952v2_eqn59.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To evaluate the probability density function, we must first calculate these derivatives.</p>
<p>To do so, we again resort to the scale invariance property of the rectified linear activation function (<xref ref-type="disp-formula" rid="eqn45">equation 45</xref>),
<disp-formula id="eqn60">
<graphic xlink:href="589952v2_eqn60.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we have defined <bold>r</bold><sub>0</sub>(<italic>φ</italic>) to be the activity generated by a pair of command variables with angle <italic>φ</italic> and unit norm. The Jacobian is thus given by
<disp-formula id="eqn61">
<graphic xlink:href="589952v2_eqn61.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="589952v2_inline157.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.Plugging this into <xref ref-type="disp-formula" rid="eqn57">equation 57</xref>, we have that the area element is given by
<disp-formula id="eqn62">
<graphic xlink:href="589952v2_eqn62.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>ω</italic>(<italic>φ</italic>) is the angle between <bold>r</bold><sub>0</sub>(<italic>φ</italic>) and its derivative at <inline-formula><inline-graphic xlink:href="589952v2_inline158.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.The total area of the manifold is thus
<disp-formula id="eqn63">
<graphic xlink:href="589952v2_eqn63.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
With these two expressions in hand, we can analytically express expectations over the probability density function in <xref ref-type="disp-formula" rid="eqn56">equation 56</xref>. The mean, corresponding to the manifold centroid, <inline-formula><inline-graphic xlink:href="589952v2_inline159.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,is given by
<disp-formula id="eqn64">
<graphic xlink:href="589952v2_eqn64.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Its covariance, <bold>Σ</bold><sub><italic>r</italic></sub>, is given by
<disp-formula id="eqn65">
<graphic xlink:href="589952v2_eqn65.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="589952v2_inline160.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the matrix of second moments,
<disp-formula id="eqn66">
<graphic xlink:href="589952v2_eqn66.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Because the integrals and derivatives in these expressions are all univariate, we can estimate them accurately with discrete approximations.</p>
</sec>
<sec id="s5b4">
<label>S.2.4</label>
<title>Reachable manifold moments for <inline-formula><inline-graphic xlink:href="589952v2_inline161.gif" mime-subtype="gif" mimetype="image"/></inline-formula></title>
<p>Analagous expressions can be derived for the case of <inline-formula><inline-graphic xlink:href="589952v2_inline162.gif" mime-subtype="gif" mimetype="image"/></inline-formula> but in these cases good estimates of the integrals and derivatives quickly become numerically intractable as the number of variables increases. For these cases, we therefore resorted to moments with respect to the probability distribution of activity patterns <italic>generated by uniformly distributed motor commands</italic>, instead of the probability distribution of activity patterns <italic>uniformly distributed on the reachable manifold</italic>.</p>
<p>We can express the covariance of this simpler distribution, which we denote by <bold>Σ</bold><sub><italic>θ</italic></sub>, by parameterizing the non-zero command variables, <inline-formula><inline-graphic xlink:href="589952v2_inline163.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,in terms of a magnitude and direction, <inline-formula><inline-graphic xlink:href="589952v2_inline164.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,where 0 ≤ <italic>s</italic> ≤ <italic>s</italic><sub>max</sub> and <inline-formula><inline-graphic xlink:href="589952v2_inline165.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This allows us to factorize the uniform distribution over motor commands into a scalar uniform distribution for the magnitude, <italic>s</italic> ∼ Unif[0, <italic>s</italic><sub>max</sub>], and a uniform distribution over the unit radius <inline-formula><inline-graphic xlink:href="589952v2_inline165a.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-sphere for the direction, <inline-formula><inline-graphic xlink:href="589952v2_inline165b.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The expectations in the covariance thus factorize as follows:
<disp-formula id="eqn67">
<graphic xlink:href="589952v2_eqn67.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we used the scale invariance of the motor cortical dynamics (<xref ref-type="disp-formula" rid="eqn8">equation 8</xref>) to write <inline-formula><inline-graphic xlink:href="589952v2_inline166.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the second line, and in the third line we simply inserted expressions for the first and second moments of <italic>s</italic>. The expectations over <inline-formula><inline-graphic xlink:href="589952v2_inline167.gif" mime-subtype="gif" mimetype="image"/></inline-formula> can be approximated using Monte Carlo methods by uniformly sampling vectors from the corresponding unit radius <inline-formula><inline-graphic xlink:href="589952v2_inline168.gif" mime-subtype="gif" mimetype="image"/></inline-formula>-sphere.</p>
</sec>
</sec>
<sec id="s5c">
<label>S.3</label>
<title>Extended methods</title>
<sec id="s5c1">
<label>S.3.1</label>
<title>Estimating the intrinsic manifold</title>
<p>To estimate the intrinsic manifold, we fit a Probabilistic PCA (PPCA) model<sup><xref ref-type="bibr" rid="c104">104</xref></sup> to the mixed and z-scored calibration task responses (see Methods <xref ref-type="sec" rid="s4h">Section 4.8</xref>),
<disp-formula id="eqn68">
<graphic xlink:href="589952v2_eqn68.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here, <italic>i</italic> indexes a particular timestep and trial during the calibration task. The PPCA generative model assumes that each of these data points are generated from a corresponding set of <italic>ℓ</italic> uncorrelated latent variables <inline-formula><inline-graphic xlink:href="589952v2_inline169.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as follows,
<disp-formula id="eqn69a">
<graphic xlink:href="589952v2_eqn69a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn69b">
<graphic xlink:href="589952v2_eqn69b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The model thus assumes that the activity patterns <inline-formula><inline-graphic xlink:href="589952v2_inline170.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are concentrated within the column space of the factor loading matrix <bold>F</bold> – it is the columns of this matrix that define the intrinsic manifold. These parameters are fit to the mixed and z-scored calibration task data,<inline-formula><inline-graphic xlink:href="589952v2_inline171.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, by maximum likelihood:
<disp-formula id="ueqn1">
<graphic xlink:href="589952v2_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="589952v2_inline172.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the eigenvalues of the sample covariance of the calibration task activity, <inline-formula><inline-graphic xlink:href="589952v2_inline173.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, ordered from largest to smallest (i.e. <italic>λ</italic><sub>1</sub> is the largest eigenvalue), and <inline-formula><inline-graphic xlink:href="589952v2_inline174.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are their associated eigenvectors (i.e. the principal components, ordered from most to least variance explained).</p>
<p>Note, however, that the columns of <bold>F</bold> define the dimensions of the intrinsic manifold in mixed and z-scored neural activity space (i.e. the space defined by the coordinates of the <inline-formula><inline-graphic xlink:href="589952v2_inline175.gif" mime-subtype="gif" mimetype="image"/></inline-formula> vectors). To convert these to dimensions of the full <italic>N</italic> -dimensional state space, where each coordinate corresponds to the activity of an individual neuron (i.e. the space defined by the coordinates of the <bold>r</bold><sub><italic>i</italic></sub> vectors), we invert <xref ref-type="disp-formula" rid="eqn68">equation 68</xref> to obtain
<disp-formula id="ueqn2">
<graphic xlink:href="589952v2_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we define <bold>H</bold><sup>−1</sup> as the <italic>N</italic> × <italic>N</italic><sub><italic>r</italic></sub> matrix containing the inverse of the tri-diagonal component of <bold>H</bold> in its first <italic>N</italic><sub><italic>r</italic></sub> rows and 0’s filling all subsequent rows. We then apply this linear transformation to the columns of <bold>F</bold> to obtain an analagous <italic>N</italic> × <italic>ℓ</italic> factor loading matrix <bold>F</bold><sub><italic>r</italic></sub> defined in the full <italic>N</italic> -dimensional state space,
<disp-formula id="ueqn3">
<graphic xlink:href="589952v2_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Note that, since the bottom <italic>N</italic> − <italic>N</italic><sub><italic>r</italic></sub> rows of <bold>H</bold><sup>−1</sup> are filled with 0’s, those same rows of <bold>F</bold><sub><italic>r</italic></sub> are also filled with 0’s. This reflects the fact that the intrinsic manifold is orthogonal to the dimensions of activity corresponding to neurons not recorded in the experiment. Finally, we defined an orthonormal basis <bold>f</bold><sub>1</sub>, <bold>f</bold><sub>2</sub>, …, <bold>f</bold><sub><italic>ℓ</italic></sub> ∈ ℝ <sub><italic>N</italic></sub> for the intrinsic manifold by taking the left singular vectors of <bold>F</bold><sub><italic>r</italic></sub>. These are the vectors used in <xref ref-type="disp-formula" rid="eqn25">equation 25</xref> for <xref rid="fig3" ref-type="fig">figure 3g</xref>.</p>
<p>This method for estimating the intrinsic manifold is almost the same as that used by Sadtler et al., which differs only in that a Factor Analysis model was used instead of a PPCA model. In that case, the maximum likelihood estimates of the model parameters cannot be evaluated in closed form and must be computed via an iterative optimization algorithm (the Expectation Maximization algorithm). We found that using a Factor Analysis model instead of PPCA had no noticeable effects on our results (data not shown), so we reported only results with the more easily fit PPCA model.</p>
</sec>
<sec id="s5c2">
<label>S.3.2</label>
<title>Construction of the baseline decoder</title>
<p>As described in the Methods section, the baseline decoder has the following form
<disp-formula id="ueqn4">
<graphic xlink:href="589952v2_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We term <bold>L</bold> the <italic>dimensionality reduction matrix</italic> and <bold>K</bold> the <italic>velocity readout matrix</italic>. Here we describe in greater detail how these two matrices are fit to the calibration task data. Unless otherwise noted, these procedures are exactly as those described in<sup><xref ref-type="bibr" rid="c105">105</xref></sup> and.<sup><xref ref-type="bibr" rid="c106">106</xref></sup></p>
<p>The dimensionality reduction matrix <bold>L</bold> is derived from the mode of the posterior distribution of the PPCA model (equation 69),
<disp-formula id="eqn70a">
<graphic xlink:href="589952v2_eqn70a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn70b">
<graphic xlink:href="589952v2_eqn70b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The <italic>N</italic><sub><italic>r</italic></sub> × <italic>ℓ</italic> matrix <inline-formula><inline-graphic xlink:href="589952v2_inline176.gif" mime-subtype="gif" mimetype="image"/></inline-formula> thus yields a linear transformation from <italic>N</italic><sub><italic>r</italic></sub> dimensions to <italic>ℓ</italic> dimensions. The z-scored and mixed activity patterns <inline-formula><inline-graphic xlink:href="589952v2_inline177.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from the calibration task can thus be reduced to <italic>ℓ</italic> dimensions via multiplication with <inline-formula><inline-graphic xlink:href="589952v2_inline178.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,resulting in a corresponding set of dimensionality-reduced activity patterns <inline-formula><inline-graphic xlink:href="589952v2_inline179.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (as above, here and in the rest of this section the index <italic>i</italic> jointly indexes a timestep and trial of the calibration task).</p>
<p>To complete the construction of the dimensionality reduction matrix <bold>L</bold>, these dimensionality-reduced activity patterns are then z-scored. The standard deviations of each component of the <inline-formula><inline-graphic xlink:href="589952v2_inline180.gif" mime-subtype="gif" mimetype="image"/></inline-formula> vectors are calculated over all timesteps and trials of the calibration task, and collected in a diagonal matrix <bold>S</bold><sub><italic>z</italic></sub>. Note that mean subtraction is not necessary since the activity vectors <inline-formula><inline-graphic xlink:href="589952v2_inline181.gif" mime-subtype="gif" mimetype="image"/></inline-formula> have already been z-scored so are mean 0. The final dimensionality reduction matrix is then given by
<disp-formula id="eqn71">
<graphic xlink:href="589952v2_eqn71.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This second z-scoring step is necessary to ensure that controlling the BCI does not require neurons to produce firing rates beyond the range exhibited during the calibration task.</p>
<p>The dimensionality reduction matrix used by Sadtler et al. differed from ours in that <inline-formula><inline-graphic xlink:href="589952v2_inline182.gif" mime-subtype="gif" mimetype="image"/></inline-formula> was constructed from the posterior distribution under a Factor Analysis generative model, rather than a PPCA generative model. Like in PPCA, the mode of the posterior distribution of a Factor Analysis model can also be expressed as a linear transformation of <inline-formula><inline-graphic xlink:href="589952v2_inline183.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,yielding a very similar expression for <inline-formula><inline-graphic xlink:href="589952v2_inline184.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>The velocity readout matrix <bold>K</bold> is also chosen by maximum likelihood fit of a generative model. In this case, we assume that the z-scored dimensionality-reduced activity patterns from the calibration task, <inline-formula><inline-graphic xlink:href="589952v2_inline185.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,depend on the observed cursor velocities, <bold>y</bold><sub><italic>i</italic></sub>, via the following latent Gaussian state space model,
<disp-formula id="ueqn5">
<graphic xlink:href="589952v2_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>i</italic>−1 indexes the previous timestep in the same trial. Note that the cursor velocities <bold>y</bold><sub><italic>i</italic></sub> are constant within each trial of the calibration task, so within a given trial <bold>y</bold><sub><italic>i</italic></sub> = <bold>y</bold><sub><italic>i</italic>−1</sub>. As was done in the original experiment of Sadtler et al., we set
<disp-formula id="ueqn6">
<graphic xlink:href="589952v2_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>k</italic> = 1<italic>/</italic>.15 denotes the ratio of the cursor speeds used in our simulation (∥<bold>y</bold><sub><italic>i</italic></sub>∥ = 1) and the cursor speeds used in the original experiment (∥<bold>y</bold><sub><italic>i</italic></sub>∥ = .15 m/s). Maximum likelihood estimates of the remaining parameters are given by
<disp-formula id="ueqn7">
<graphic xlink:href="589952v2_ueqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>T</italic> denotes the total number of data points in the calibration task data: the number of timesteps in each trial times the total number of trials.</p>
<fig id="figS7" position="float" fig-type="figure">
<label>Figure S7:</label>
<caption><title>Differences between sampled decoder perturbations and the baseline decoder.</title>
<p>a. Distribution of mean principal angle between row space of baseline decoder and row space of each perturbed decoder.</p>
<p>b. Distribution of mean squared error achieved by mean calibration task responses under each perturbed decoder.</p>
<p>c. Distribution of minimal absolute change in preferred direction needed to produce the same readouts with each perturbed decoder as with the baseline decoder.</p></caption>
<graphic xlink:href="589952v2_figS7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The velocity readout matrix is then derived from the mode of the of the posterior distribution <inline-formula><inline-graphic xlink:href="589952v2_inline186.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,where the ellipses go back to the first timestep of the given trial. We use the posterior distribution at steady state, whose mode is given by
<disp-formula id="ueqn8">
<graphic xlink:href="589952v2_ueqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>K</bold> is the so-called steady-state Kalman gain matrix. This matrix is given by
<disp-formula id="eqn72">
<graphic xlink:href="589952v2_eqn72.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>Σ</bold><sub>ss</sub> is the steady-state posterior covariance, given by the solution to the discrete-time algebraic Riccatti equation
<disp-formula id="ueqn9">
<graphic xlink:href="589952v2_ueqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The 2 × <italic>ℓ</italic> velocity readout matrix used for the baseline decoder is thus set to the steady-state Kalman gain matrix, <bold>K</bold>.</p>
</sec>
<sec id="s5c3">
<label>S.3.3</label>
<title>Subsampling WMPs and OMPs</title>
<p>As mentioned in the methods, we attempted to minimize any differences between within- and outside-manifold perturbations that would go beyond their opposing relationship to the intrinsic manifold. To do this, we first calculated every possible WMP and OMP, corresponding to each <italic>ℓ</italic>-dimensional permutation. Since we set <italic>ℓ</italic> = 8, this resulted in <italic>ℓ</italic>! −1 = 40, 319 decoder perturbations of each type (minus 1 to exclude the identity permutation). We then quantified how different each of these perturbations were from the baseline decoder with three different metrics, and eliminated all decoder perturbations for which one or more of these metrics fell outside a specific range.</p>
<p>The first metric is the angle between the perturbed decoder’s row space and the baseline decoder’s.</p>
<p>For each decoder perturbation, <inline-formula><inline-graphic xlink:href="589952v2_inline187.gif" mime-subtype="gif" mimetype="image"/></inline-formula> or <inline-formula><inline-graphic xlink:href="589952v2_inline188.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,we calculated the two principal angles<sup><xref ref-type="bibr" rid="c107">107</xref></sup> between its row space and that of the baseline decoder effective decoding matrix, <inline-formula><inline-graphic xlink:href="589952v2_inline189.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,and averaged these two angles. Any decoder perturbations for which this mean principal angle was greater than 80<sup><italic>o</italic></sup> or less than 60<sup><italic>o</italic></sup> was eliminated (<xref rid="figS7" ref-type="fig">fig. S7a</xref>).</p>
<p>The second metric is the mean squared error that would be achieved if the subject were to simply reproduce the neural activity from the calibration task. Analagous to the procedure followed by Sadtler et al., we averaged the calibration task responses over time and over trials for each reach target,
<disp-formula id="ueqn10">
<graphic xlink:href="589952v2_ueqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and then computed the readouts from these time- and trial-averaged firing rate vectors under each decoder perturbation, <inline-formula><inline-graphic xlink:href="589952v2_inline190.gif" mime-subtype="gif" mimetype="image"/></inline-formula> or <inline-formula><inline-graphic xlink:href="589952v2_inline191.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.We then discarded all decoder perturbations where the mean</p>
<p>squared error between these readouts and the target readouts was greater than 0.8 or less than 0.6 (<xref rid="figS7" ref-type="fig">fig. S7b</xref>).</p>
<p>The third metric is to ask how much the mean calibration task responses would have to change to produce the same readouts under the perturbed decoder as under the baseline decoder. We first calculated the time- and trial-averaged z-scored and mixed firing rates from the calibration task
<disp-formula id="ueqn11">
<graphic xlink:href="589952v2_ueqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For each perturbed decoder, <inline-formula><inline-graphic xlink:href="589952v2_inline192.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,we then computed the activity patterns closest to <inline-formula><inline-graphic xlink:href="589952v2_inline193.gif" mime-subtype="gif" mimetype="image"/></inline-formula> that would produce the same readouts through that decoder as the original activity patterns would through the baseline decoder, <inline-formula><inline-graphic xlink:href="589952v2_inline194.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,
<disp-formula id="ueqn12">
<graphic xlink:href="589952v2_ueqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We then quantified the difference between <inline-formula><inline-graphic xlink:href="589952v2_inline195.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="589952v2_inline196.gif" mime-subtype="gif" mimetype="image"/></inline-formula> by fitting tuning curves and asking how much the preferred direction changed. Tuning curves were fit by least-squares regression, exactly as described in Methods <xref ref-type="sec" rid="s4i">Section 4.9</xref> <xref ref-type="disp-formula" rid="eqn26">equation 26</xref> (but with <inline-formula><inline-graphic xlink:href="589952v2_inline197.gif" mime-subtype="gif" mimetype="image"/></inline-formula> or <inline-formula><inline-graphic xlink:href="589952v2_inline198.gif" mime-subtype="gif" mimetype="image"/></inline-formula> plugged in for<inline-formula><inline-graphic xlink:href="589952v2_inline199.gif" mime-subtype="gif" mimetype="image"/></inline-formula>), and preferred directions were extracted from the fitted tuning weights as described in that section. For each decoder perturbation, we then computed the mean absolute difference of the preferred directions of the computed activity patterns <inline-formula><inline-graphic xlink:href="589952v2_inline200.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from those of the observed calibration task mean responses <inline-formula><inline-graphic xlink:href="589952v2_inline201.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Any perturbed decoders that resulted in a mean absolute difference of more than 45<sup><italic>o</italic></sup> or less than 30<sup><italic>o</italic></sup> were discarded (<xref rid="figS7" ref-type="fig">fig. S7c</xref>).</p>
<p>We typically found that about 100-200 permutations out all possible decoder perturbations satisfied these criteria. We then randomly sampled 100 of them. The distributions of these three merics for the 100 sampled WMPs and OMPs used in the main text are shown in <xref rid="figS7" ref-type="fig">figures S7a, S7b</xref>, and <xref rid="figS7" ref-type="fig">S7c</xref>.</p>
</sec>
</sec>
</sec>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by the Gatsby Charitable Foundation (J.A.M., P.E.L.), Wellcome Trust 110114/Z/15/Z (P.E.L.), University College London Research Excellence Scholarship (J.A.M.), NIH K99/R00 MH121533 (M.D.G.), NIH R01 NS129584 (A.P.B., S.M.C., B.M.Y.), NSF NCS DRL 2124066 and 2123911 (B.M.Y., S.M.C., A.P.B.), Simons Foundation 543065 and NC-GB-CULM-00003241-05 (B.M.Y.).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Shadmehr</surname></string-name> and <string-name><given-names>F. A.</given-names> <surname>Mussa-Ivaldi</surname></string-name></person-group>. <article-title>Adaptive Representation of Dynamics during Learning of a Motor Task</article-title>. <source>Journal of Neuroscience</source> <volume>14</volume>.<issue>5</issue> (<month>May</month> <year>1994</year>), pp. <fpage>3208</fpage>–<lpage>3224</lpage>. issn: <issn>0270-6474</issn>, <issn>1529-2401</issn>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-05-03208.1994</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Chiang-Shan Ray</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Camillo</given-names> <surname>Padoa-Schioppa</surname></string-name>, and <string-name><given-names>Emilio</given-names> <surname>Bizzi</surname></string-name></person-group>. <article-title>Neuronal Correlates of Motor Performance and Motor Learning in the Primary Motor Cortex of Monkeys Adapting to an External Force Field</article-title>. <source>Neuron</source> <volume>30</volume>.<issue>2</issue> (<month>May</month> <year>2001</year>), pp. <fpage>593</fpage>–<lpage>607</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/S0896-6273(01)00301-4</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Rony</given-names> <surname>Paz</surname></string-name> <etal>et al.</etal></person-group> <article-title>Acquisition and Generalization of Visuomotor Transformations by Nonhuman Primates</article-title>. <source>Experimental Brain Research</source> <volume>161</volume>.<issue>2</issue> (<month>Feb</month>. <year>2005</year>), pp. <fpage>209</fpage>–<lpage>219</lpage>. issn: <issn>1432-1106</issn>. doi: <pub-id pub-id-type="doi">10.1007/s00221-004-2061-4</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Gonçalo</given-names> <surname>Lopes</surname></string-name> <etal>et al.</etal></person-group> <article-title>A Robust Role for Motor Cortex</article-title>. <source>bioRxiv</source> (<month>May</month> <year>2017</year>), p. <fpage>058917</fpage>. doi: <pub-id pub-id-type="doi">10.1101/058917</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Risa</given-names> <surname>Kawai</surname></string-name> <etal>et al.</etal></person-group> <article-title>Motor Cortex Is Required for Learning but Not for Executing a Motor Skill</article-title>. <source>Neuron</source> <volume>86</volume>.<issue>3</issue> (<month>May</month> <year>2015</year>), pp. <fpage>800</fpage>–<lpage>812</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.024</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mackenzie Weygandt</given-names> <surname>Mathis</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Mathis</surname></string-name>, and <string-name><given-names>Naoshige</given-names> <surname>Uchida</surname></string-name></person-group>. <article-title>Somatosensory Cortex Plays an Essential Role in Forelimb Motor Adaptation in Mice</article-title>. <source>Neuron</source> <volume>93</volume>.<issue>6</issue> (<month>Mar</month>. <year>2017</year>), <fpage>1493</fpage>–<lpage>1503.e6</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2017.02.049</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Matthew G.</given-names> <surname>Perich</surname></string-name>, <string-name><given-names>Juan A.</given-names> <surname>Gallego</surname></string-name>, and <string-name><given-names>Lee E.</given-names> <surname>Miller</surname></string-name></person-group>. <article-title>A Neural Population Mechanism for Rapid Learning</article-title>. <source>Neuron</source> <volume>100</volume>.<issue>4</issue> (<month>Nov</month>. <year>2018</year>), <fpage>964</fpage>–<lpage>976.e7</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.030</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Matthew D</given-names> <surname>Golub</surname></string-name> <etal>et al.</etal></person-group> <article-title>Brain–Computer Interfaces for Dissecting Cognitive Processes Underlying Sensorimotor Control</article-title>. <source>Current Opinion in Neurobiology. Neurobiology of Cognitive Behavior</source> <volume>37</volume> (<month>Apr</month>. <year>2016</year>), pp. <fpage>53</fpage>–<lpage>58</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.conb.2015.12.005</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Richard A.</given-names> <surname>Andersen</surname></string-name>, <string-name><given-names>Tyson</given-names> <surname>Aflalo</surname></string-name>, and <string-name><given-names>Spencer</given-names> <surname>Kellis</surname></string-name></person-group>. <article-title>From Thought to Action: The Brain–Machine Interface in Posterior Parietal Cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>116</volume>.<issue>52</issue> (<month>Dec</month>. <year>2019</year>), pp. <fpage>26274</fpage>–<lpage>26279</lpage>. issn: <issn>0027-8424</issn>, <issn>1091-6490</issn>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1902276116</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Emily R.</given-names> <surname>Oby</surname></string-name> <etal>et al.</etal></person-group> <chapter-title>Intracortical Brain–Machine Interfaces</chapter-title>. <source>Neural Engineering</source>. Ed. by <person-group person-group-type="editor"><string-name><given-names>Bin</given-names> <surname>He</surname></string-name></person-group>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>, <year>2020</year>, pp. <fpage>185</fpage>–<lpage>221</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-030-43395-6_5</pub-id>. (Visited on 07/30/2021).</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Beata</given-names> <surname>Jarosiewicz</surname></string-name> <etal>et al.</etal></person-group> <article-title>Functional Network Reorganization during Learning in a Brain-Computer Interface Paradigm</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>105</volume>.<issue>49</issue> (<month>Dec</month>. <year>2008</year>), pp. <fpage>19486</fpage>–<lpage>19491</lpage>. issn: <issn>0027-8424</issn>, <issn>1091-6490</issn>. doi: <pub-id pub-id-type="doi">10.1073/pnas.0808113105</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Steven M.</given-names> <surname>Chase</surname></string-name>, <string-name><given-names>Robert E.</given-names> <surname>Kass</surname></string-name>, and <string-name><given-names>Andrew B.</given-names> <surname>Schwartz</surname></string-name></person-group>. <article-title>Behavioral and Neural Correlates of Visuomotor Adaptation Observed through a Brain-Computer Interface in Primary Motor Cortex</article-title>. <source>Journal of Neurophysiology</source> <volume>108</volume>.<issue>2</issue> (<month>Apr</month>. <year>2012</year>), pp. <fpage>624</fpage>–<lpage>644</lpage>. issn: <issn>0022-3077</issn>. doi: <pub-id pub-id-type="doi">10.1152/jn.00371.2011</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Vikash</given-names> <surname>Gilja</surname></string-name> <etal>et al.</etal></person-group> <article-title>A High-Performance Neural Prosthesis Enabled by Control Algorithm Design</article-title>. <source>Nature Neuroscience</source> <volume>15</volume>.<issue>12</issue> (<month>Dec</month>. <year>2012</year>), pp. <fpage>1752</fpage>–<lpage>1757</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn.3265</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Patrick T.</given-names> <surname>Sadtler</surname></string-name> <etal>et al.</etal></person-group> <article-title>Neural Constraints on Learning</article-title>. <source>Nature</source> <volume>512</volume>.<issue>7515</issue> (<month>Aug</month>. <year>2014</year>), pp. <fpage>423</fpage>–<lpage>426</lpage>. issn: <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature13665</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Karunesh</given-names> <surname>Ganguly</surname></string-name> and <string-name><given-names>Jose M.</given-names> <surname>Carmena</surname></string-name></person-group>. <article-title>Emergence of a Stable Cortical Map for Neuroprosthetic Control</article-title>. <source>PLOS Biology</source> <volume>7</volume>.<issue>7</issue> (<month>July</month> <year>2009</year>), <fpage>e1000153</fpage>. issn: <issn>1545-7885</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.1000153</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Emily R.</given-names> <surname>Oby</surname></string-name> <etal>et al.</etal></person-group> <article-title>New Neural Activity Patterns Emerge with Long-Term Learning</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>116</volume>.<issue>30</issue> (<year>2019</year>), pp. <fpage>15210</fpage>–<lpage>15215</lpage>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Robert</given-names> <surname>Legenstein</surname></string-name> <etal>et al.</etal></person-group> <article-title>A Reward-Modulated Hebbian Learning Rule Can Explain Experimentally Observed Network Reorganization in a Brain Control Task</article-title>. <source>Journal of Neuroscience</source> <volume>30</volume>.<issue>25</issue> (<month>June</month> <year>2010</year>), pp. <fpage>8400</fpage>–<lpage>8410</lpage>. issn: <issn>0270-6474</issn>, <issn>1529-2401</issn>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4284-09.2010</pub-id>..</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Ben</given-names> <surname>Engelhard</surname></string-name> <etal>et al.</etal></person-group> <article-title>Neuronal Activity and Learning in Local Cortical Networks Are Modulated by the Action-Perception State</article-title>. <source>bioRxiv</source> (<month>Feb</month>. <year>2019</year>), p. <fpage>537613</fpage>. doi: <pub-id pub-id-type="doi">10.1101/537613</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Emil</given-names> <surname>Wärnberg</surname></string-name> and <string-name><given-names>Arvind</given-names> <surname>Kumar</surname></string-name></person-group>. <article-title>Perturbing Low Dimensional Activity Manifolds in Spiking Neuronal Networks</article-title>. <source>PLOS Computational Biology</source> <volume>15</volume>.<issue>5</issue> (<month>May</month> <year>2019</year>), <fpage>e1007074</fpage>. issn: <issn>1553-7358</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1007074</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Barbara</given-names> <surname>Feulner</surname></string-name> and <string-name><given-names>Claudia</given-names> <surname>Clopath</surname></string-name></person-group>. <article-title>Neural Manifold under Plasticity in a Goal Driven Learning Behaviour</article-title>. <source>PLOS Computational Biology</source> <volume>17</volume>.<issue>2</issue> (<month>Feb</month>. <year>2021</year>), <fpage>e1008621</fpage>. issn: <issn>1553-7358</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008621</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Rodolphe</given-names> <surname>Héliot</surname></string-name> <etal>et al.</etal></person-group> <article-title>Learning in Closed-Loop Brain–Machine Interfaces: Modeling and Experimental Validation</article-title>. <source>IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</source> <volume>40</volume>.<issue>5</issue> (<month>Oct</month>. <year>2010</year>), pp. <fpage>1387</fpage>–<lpage>1397</lpage>. issn: <issn>1941-0492</issn>. doi: <pub-id pub-id-type="doi">10.1109/TSMCB.2009.2036931</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Justin</given-names> <surname>Werfel</surname></string-name>, <string-name><given-names>Xiaohui</given-names> <surname>Xie</surname></string-name>, and <string-name><given-names>H. Sebastian</given-names> <surname>Seung</surname></string-name></person-group>. <article-title>Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</article-title>. <conf-name>Advances in Neural Information Processing Systems</conf-name>. <year>2004</year>, pp. <fpage>1197</fpage>–<lpage>1204</lpage>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Naoki</given-names> <surname>Hiratani</surname></string-name> <etal>et al.</etal></person-group> <article-title>On the Stability and Scalability of Node Perturbation Learning</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>35</volume> (<month>Dec</month>. <year>2022</year>), pp. <fpage>31929</fpage>–<lpage>31941</lpage>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. Aldo</given-names> <surname>Faisal</surname></string-name>, <string-name><given-names>Luc P. J.</given-names> <surname>Selen</surname></string-name>, and <string-name><given-names>Daniel M.</given-names> <surname>Wolpert</surname></string-name></person-group>. <article-title>Noise in the Nervous System</article-title>. <source>Nat Rev Neurosci</source> <volume>9</volume>.<issue>4</issue> (<month>Apr</month>. <year>2008</year>), pp. <fpage>292</fpage>–<lpage>303</lpage>. issn: <issn>1471-0048</issn>. doi: <pub-id pub-id-type="doi">10.1038/nrn2258</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Francis</given-names> <surname>Crick</surname></string-name></person-group>. <article-title>The Recent Excitement about Neural Networks</article-title>. <source>Nature</source> <volume>337</volume>.<issue>6203</issue> (<month>Jan</month>. <year>1989</year>), pp. <fpage>129</fpage>–<lpage>132</lpage>. issn: <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/337129a0</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Sergey</given-names> <surname>Bartunov</surname></string-name> <etal>et al.</etal></person-group>., <year>2018</year>, pp. <article-title>Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures</article-title>. <conf-name>Advances in Neural Information Processing Systems 31</conf-name>. pp. <fpage>9368</fpage>–<lpage>9378</lpage>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Timothy P.</given-names> <surname>Lillicrap</surname></string-name> <etal>et al.</etal></person-group> <article-title>Backpropagation and the Brain</article-title>. <source>Nature Reviews Neuroscience</source> <volume>21</volume>.<issue>6</issue> (<month>June</month> <year>2020</year>), pp. <fpage>335</fpage>–<lpage>346</lpage>. issn: <issn>1471-0048</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41583-020-0277-3</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>EE</given-names> <surname>Fetz</surname></string-name> and <string-name><given-names>MA</given-names> <surname>Baker</surname></string-name></person-group>. <article-title>Operantly Conditioned Patterns on Precentral Unit Activity and Correlated Responses in Adjacent Cells and Contralateral Muscles</article-title>. <source>Journal of Neurophysiology</source> <volume>36</volume>.<issue>2</issue> (<month>Mar</month>. <year>1973</year>), pp. <fpage>179</fpage>–<lpage>204</lpage>. issn: <issn>0022-3077</issn>. doi: <pub-id pub-id-type="doi">10.1152/jn.1973.36.2.179</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Matthew D.</given-names> <surname>Golub</surname></string-name> <etal>et al.</etal></person-group> <article-title>Learning by Neural Reassociation</article-title>. <source>Nature Neuroscience</source> <volume>21</volume>.<issue>4</issue> (<month>Apr</month>. <year>2018</year>), pp. <fpage>607</fpage>–<lpage>616</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41593-018-0095-3</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jay A</given-names> <surname>Hennig</surname></string-name> <etal>et al.</etal></person-group> <article-title>Constraints on Neural Redundancy</article-title>. <source>eLife</source> <volume>7</volume> (<month>Aug</month>. <year>2018</year>) <elocation-id>e36774</elocation-id>. doi: <pub-id pub-id-type="doi">10.7554/eLife.36774</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Karunesh</given-names> <surname>Ganguly</surname></string-name> <etal>et al.</etal></person-group> <article-title>Reversible Large-Scale Modification of Cortical Networks during Neuroprosthetic Control</article-title>. <source>Nature Neuroscience</source> <volume>14</volume>.<issue>5</issue> (<month>May</month> <year>2011</year>), pp. <fpage>662</fpage>–<lpage>667</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn.2797</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Saurabh</given-names> <surname>Vyas</surname></string-name> <etal>et al.</etal></person-group> <article-title>Neural Population Dynamics Underlying Motor Learning Transfer</article-title>. <source>Neuron</source> <volume>97</volume>.<issue>5</issue> (<month>Mar</month>. <year>2018</year>), <fpage>1177</fpage>–<lpage>1186.e3</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.040</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Steven M.</given-names> <surname>Chase</surname></string-name>, <string-name><given-names>Andrew B.</given-names> <surname>Schwartz</surname></string-name>, and <string-name><given-names>Robert E.</given-names> <surname>Kass</surname></string-name></person-group>. <article-title>Latent Inputs Improve Estimates of Neural Encoding in Motor Cortex</article-title>. <source>Journal of Neuroscience</source> <volume>30</volume>.<issue>41</issue> (<month>Oct</month>. <year>2010</year>), pp. <fpage>13873</fpage>–<lpage>13882</lpage>. issn: <issn>0270-6474</issn>, <issn>1529-2401</issn>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2325-10.2010</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Eun Jung</given-names> <surname>Hwang</surname></string-name>, <string-name><given-names>Paul M.</given-names> <surname>Bailey</surname></string-name>, and <string-name><given-names>Richard A.</given-names> <surname>Andersen</surname></string-name></person-group>. <article-title>Volitional Control of Neural Activity Relies on the Natural Motor Repertoire</article-title>. <source>Current Biology</source> <volume>23</volume>.<issue>5</issue> (<month>Mar</month>. <year>2013</year>), pp. <fpage>353</fpage>–<lpage>361</lpage>. issn: <issn>0960-9822</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2013.01.027</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sofia</given-names> <surname>Sakellaridi</surname></string-name> <etal>et al.</etal></person-group> <article-title>Intrinsic Variable Learning for Brain-Machine Interface Control by Human Anterior Intraparietal Cortex</article-title>. <source>Neuron</source> <volume>102</volume>.<issue>3</issue> (<month>May</month> <year>2019</year>), <fpage>694</fpage>–<lpage>705.e3</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2019.02.012</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>d’Avella</surname></string-name> and <string-name><given-names>E.</given-names> <surname>Bizzi</surname></string-name></person-group>. <article-title>Low Dimensionality of Supraspinally Induced Force Fields</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>95</volume>.<issue>13</issue> (<month>June</month> <year>1998</year>), pp. <fpage>7711</fpage>–<lpage>7714</lpage>. issn: <issn>0027-8424</issn>, <issn>1091-6490</issn>. doi: <pub-id pub-id-type="doi">10.1073/pnas.95.13.7711</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andrea</given-names> <surname>d’Avella</surname></string-name>, <string-name><given-names>Philippe</given-names> <surname>Saltiel</surname></string-name>, and <string-name><given-names>Emilio</given-names> <surname>Bizzi</surname></string-name></person-group>. <article-title>Combinations of Muscle Synergies in the Construction of a Natural Motor Behavior</article-title>. <source>Nature Neuroscience</source> <volume>6</volume>.<issue>3</issue> (<month>Mar</month>. <year>2003</year>), pp. <fpage>300</fpage>–<lpage>308</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn1010</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yuri P.</given-names> <surname>Ivanenko</surname></string-name> <etal>et al.</etal></person-group> <article-title>Temporal Components of the Motor Patterns Expressed by the Human Spinal Cord Reflect Foot Kinematics</article-title>. <source>Journal of Neurophysiology</source> <volume>90</volume>.<issue>5</issue> (<month>Nov</month>. <year>2003</year>), pp. <fpage>3555</fpage>–<lpage>3565</lpage>. issn: <issn>0022-3077</issn>. doi: <pub-id pub-id-type="doi">10.1152/jn.00223.2003</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Emanuel</given-names> <surname>Todorov</surname></string-name></person-group>. <article-title>Optimality Principles in Sensorimotor Control</article-title>. <source>Nature Neuroscience</source> <volume>7</volume>.<issue>9</issue> (<month>Sept</month>. <year>2004</year>), pp. <fpage>907</fpage>–<lpage>915</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn1309</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jason J.</given-names> <surname>Kutch</surname></string-name> and <string-name><given-names>Francisco J.</given-names> <surname>Valero-Cuevas</surname></string-name></person-group>. <article-title>Challenges and New Approaches to Proving the Existence of Muscle Synergies of Neural Origin</article-title>. <source>PLOS Computational Biology</source> <volume>8</volume>.<issue>5</issue> (<month>May</month> <year>2012</year>), <fpage>e1002434</fpage>. issn: <issn>1553-7358</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002434</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Naveen</given-names> <surname>Kuppuswamy</surname></string-name> and <string-name><given-names>Christopher M.</given-names> <surname>Harris</surname></string-name></person-group>. <article-title>Do Muscle Synergies Reduce the Dimensionality of Behavior? English</article-title>. <source>Frontiers in Computational Neuroscience</source> <volume>8</volume> (<year>2014</year>). issn: <issn>1662-5188</issn>. doi: <pub-id pub-id-type="doi">10.3389/fncom.2014.00063</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mark M.</given-names> <surname>Churchland</surname></string-name> <etal>et al.</etal></person-group> <article-title>Neural Population Dynamics during Reaching</article-title>. <source>Nature</source> <volume>487</volume>.<issue>7405</issue> (<month>July</month> <year>2012</year>), pp. <fpage>51</fpage>–<lpage>56</lpage>. issn: <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature11129</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Guillaume</given-names> <surname>Hennequin</surname></string-name>, <string-name><given-names>Tim P.</given-names> <surname>Vogels</surname></string-name>, and <string-name><given-names>Wulfram</given-names> <surname>Gerstner</surname></string-name></person-group>. <article-title>Optimal Control of Transient Dynamics in Balanced Networks Supports Generation of Complex Movements</article-title>. <source>Neuron</source> <volume>82</volume>.<issue>6</issue> (<month>June</month> <year>2014</year>), pp. <fpage>1394</fpage>–<lpage>1406</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>David</given-names> <surname>Sussillo</surname></string-name> <etal>et al.</etal></person-group> <article-title>A Neural Network That Finds a Naturalistic Solution for the Production of Muscle Activity</article-title>. <source>Nature Neuroscience</source> <volume>18</volume>.<issue>7</issue> (<month>July</month> <year>2015</year>), pp. <fpage>1025</fpage>–<lpage>1033</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn.4042</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Abigail A.</given-names> <surname>Russo</surname></string-name> <etal>et al.</etal></person-group> <article-title>Motor Cortex Embeds Muscle-like Commands in an Untangled Population Response</article-title>. <source>Neuron</source> <volume>97</volume>.<issue>4</issue> (<month>Feb</month>. <year>2018</year>), <fpage>953</fpage>–<lpage>966.e8</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.004</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ta-Chu</given-names> <surname>Kao</surname></string-name>, <string-name><given-names>Mahdieh S.</given-names> <surname>Sadabadi</surname></string-name>, and <string-name><given-names>Guillaume</given-names> <surname>Hennequin</surname></string-name></person-group>. <article-title>Optimal Anticipatory Control as a Theory of Motor Preparation: A Thalamo-Cortical Circuit Model</article-title>. <source>Neuron</source> <volume>109</volume>.<issue>9</issue> (<month>May</month> <year>2021</year>), <fpage>1567</fpage>–<lpage>1581.e12</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.009</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mijail D.</given-names> <surname>Serruya</surname></string-name> <etal>et al.</etal></person-group> <article-title>Instant Neural Control of a Movement Signal</article-title>. <source>Nature</source> <volume>416</volume>.<issue>6877</issue> (<month>Mar</month>. <year>2002</year>), pp. <fpage>141</fpage>–<lpage>142</lpage>. issn: <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/416141a</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Leigh R.</given-names> <surname>Hochberg</surname></string-name> <etal>et al.</etal></person-group> <article-title>Neuronal Ensemble Control of Prosthetic Devices by a Human with Tetraplegia</article-title>. <source>Nature</source> <volume>442</volume>.<issue>7099</issue> (<month>July</month> <year>2006</year>), pp. <fpage>164</fpage>–<lpage>171</lpage>. issn: <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature04970</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Meel</given-names> <surname>Velliste</surname></string-name> <etal>et al.</etal></person-group> <article-title>Cortical Control of a Prosthetic Arm for Self-Feeding</article-title>. <source>Nature</source> <volume>453</volume>.<issue>7198</issue> (<month>June</month> <year>2008</year>), pp. <fpage>1098</fpage>–<lpage>1101</lpage>. issn: <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature06996</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Matthew D</given-names> <surname>Golub</surname></string-name>, <string-name><given-names>Byron M</given-names> <surname>Yu</surname></string-name>, and <string-name><given-names>Steven M</given-names> <surname>Chase</surname></string-name></person-group>. <article-title>Internal Models for Interpreting Neural Population Activity during Sensorimotor Control</article-title>. <source>eLife</source> <volume>4</volume> (<month>Dec</month>. <year>2015</year>), <fpage>e10015</fpage>. doi: <pub-id pub-id-type="doi">10.7554/eLife.10015</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sergey D.</given-names> <surname>Stavisky</surname></string-name> <etal>et al.</etal></person-group> <article-title>Motor Cortical Visuomotor Feedback Activity Is Initially Isolated from Downstream Targets in Output-Null Neural State Space Dimensions</article-title>. <source>Neuron</source> <volume>95</volume>.<issue>1</issue> (<month>July</month> <year>2017</year>), <fpage>195</fpage>–<lpage>208.e9</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.023</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Maryam M.</given-names> <surname>Shanechi</surname></string-name> <etal>et al.</etal></person-group> <article-title>Rapid Control and Feedback Rates Enhance Neuroprosthetic Control</article-title>. <source>Nature Communications</source> <volume>8</volume>.<issue>1</issue> (<month>Jan</month>. <year>2017</year>), p. <fpage>13825</fpage>. issn: <issn>2041-1723</issn>. doi: <pub-id pub-id-type="doi">10.1038/ncomms13825</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>W. T.</given-names> <surname>Thach</surname></string-name></person-group>. <article-title>Correlation of Neural Discharge with Pattern and Force of Muscular Activity, Joint Position, and Direction of Intended next Movement in Motor Cortex and Cerebellum</article-title>. <source>Journal of Neurophysiology</source> <volume>41</volume>.<issue>3</issue> (<month>May</month> <year>1978</year>), pp. <fpage>654</fpage>–<lpage>676</lpage>. issn: <issn>0022-3077</issn>. doi: <pub-id pub-id-type="doi">10.1152/jn.1978.41.3.654</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Eberhard E.</given-names> <surname>Fetz</surname></string-name></person-group>. <article-title>Are Movement Parameters Recognizably Coded in the Activity of Single Neurons?</article-title><source> Behavioral and Brain Sciences</source> <volume>15</volume>.<issue>4</issue> (<year>1992</year>), pp. <fpage>679</fpage>–<lpage>690</lpage>. doi: <pub-id pub-id-type="doi">10.1017/S0140525X00072599</pub-id>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Eberhard E.</given-names> <surname>Fetz</surname></string-name></person-group>. <article-title>Volitional Control of Neural Activity: Implications for Brain–Computer Interfaces</article-title>. <source>The Journal of Physiology</source> <volume>579</volume>.<issue>3</issue> (<year>2007</year>), pp. <fpage>571</fpage>–<lpage>579</lpage>. issn: <issn>1469-7793</issn>. doi: <pub-id pub-id-type="doi">10.1113/jphysiol.2006.127142</pub-id>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Stephen H.</given-names> <surname>Scott</surname></string-name></person-group>. <article-title>Inconvenient Truths about Neural Processing in Primary Motor Cortex</article-title>. <source>The Journal of Physiology</source> <volume>586</volume>.<issue>5</issue> (<year>2008</year>), pp. <fpage>1217</fpage>–<lpage>1224</lpage>. issn: <issn>1469-7793</issn>. doi: <pub-id pub-id-type="doi">10.1113/jphysiol.2007.146068</pub-id>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mohsen</given-names> <surname>Omrani</surname></string-name> <etal>et al.</etal></person-group> <article-title>Perspectives on Classical Controversies about the Motor Cortex</article-title>. <source>Journal of Neurophysiology</source> <volume>118</volume>.<issue>3</issue> (<month>June</month> <year>2017</year>), pp. <fpage>1828</fpage>–<lpage>1848</lpage>. issn: <issn>0022-3077</issn>. doi: <pub-id pub-id-type="doi">10.1152/jn.00795.2016</pub-id>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Juan A.</given-names> <surname>Gallego</surname></string-name> <etal>et al.</etal></person-group> <article-title>Neural Manifolds for the Control of Movement</article-title>. <source>Neuron</source> <volume>94</volume>.<issue>5</issue> (<month>June</month> <year>2017</year>), pp. <fpage>978</fpage>–<lpage>984</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.025</pub-id>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Francis R.</given-names> <surname>Willett</surname></string-name> <etal>et al.</etal></person-group> <article-title>Hand Knob Area of Premotor Cortex Represents the Whole Body in a Compositional Way</article-title>. <source>Cell</source> <volume>181</volume>.<issue>2</issue> (<month>Apr</month>. <year>2020</year>), <fpage>396</fpage>–<lpage>409.e26</lpage>. issn: <issn>0092-8674</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cell.2020.02.043</pub-id>.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Peiran</given-names> <surname>Gao</surname></string-name> <etal>et al.</etal></person-group> <article-title>A Theory of Multineuronal Dimensionality, Dynamics and Measurement</article-title>. <source>bioRxiv</source> (<month>Nov</month>. <year>2017</year>), p. <fpage>214262</fpage>. doi: <pub-id pub-id-type="doi">10.1101/214262</pub-id>.</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Xiao</given-names> <surname>Zhou</surname></string-name> <etal>et al.</etal></person-group> <article-title>Distinct Types of Neural Reorganization during Long-Term Learning</article-title>. <source>Journal of Neurophysiology</source> <volume>121</volume>.<issue>4</issue> (<month>Feb</month>. <year>2019</year>), pp. <fpage>1329</fpage>–<lpage>1341</lpage>. issn: <issn>0022-3077</issn>. doi: <pub-id pub-id-type="doi">10.1152/jn.00466.2018</pub-id>.</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Marvin</given-names> <surname>Minsky</surname></string-name></person-group>. <article-title>Steps toward Artificial Intelligence</article-title>. <source>Proceedings of the IRE</source> <volume>49</volume>.<issue>1</issue> (<month>Jan</month>. <year>1961</year>), pp. <fpage>8</fpage>–<lpage>30</lpage>. issn: <issn>2162-6634</issn>. doi: <pub-id pub-id-type="doi">10.1109/JRPROC.1961.287775</pub-id>.</mixed-citation></ref>
<ref id="c63"><label>[63]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Eberhard E.</given-names> <surname>Fetz</surname></string-name></person-group>. <article-title>Operant Conditioning of Cortical Unit Activity</article-title>. <source>Science</source> <volume>163</volume>.<issue>3870</issue> (<month>Feb</month>. <year>1969</year>), pp. <fpage>955</fpage>–<lpage>958</lpage>. issn: <issn>0036-8075</issn>, <issn>1095-9203</issn>. doi: <pub-id pub-id-type="doi">10.1126/science.163.3870.955</pub-id>.</mixed-citation></ref>
<ref id="c64"><label>[64]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Aaron C.</given-names> <surname>Koralek</surname></string-name> <etal>et al.</etal></person-group> <article-title>Corticostriatal Plasticity Is Necessary for Learning Intentional Neuroprosthetic Skills</article-title>. <source>Nature</source> <volume>483</volume>.<issue>7389</issue> (<month>Mar</month>. <year>2012</year>), pp. <fpage>331</fpage>–<lpage>335</lpage>. issn: <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature10845</pub-id>.</mixed-citation></ref>
<ref id="c65"><label>[65]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kelly B.</given-names> <surname>Clancy</surname></string-name> <etal>et al.</etal></person-group> <article-title>Volitional Modulation of Optically Recorded Calcium Signals during Neuroprosthetic Learning</article-title>. <source>Nature Neuroscience</source> <volume>17</volume>.<issue>6</issue> (<month>June</month> <year>2014</year>), pp. <fpage>807</fpage>–<lpage>809</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn.3712</pub-id>.</mixed-citation></ref>
<ref id="c66"><label>[66]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Vivek R.</given-names> <surname>Athalye</surname></string-name> <etal>et al.</etal></person-group> <article-title>Evidence for a Neural Law of Effect</article-title>. <source>Science</source> <volume>359</volume>.<issue>6379</issue> (<year>2018</year>), pp. <fpage>1024</fpage>–<lpage>1029</lpage>.</mixed-citation></ref>
<ref id="c67"><label>[67]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Robert</given-names> <surname>Legenstein</surname></string-name>, <string-name><given-names>Dejan</given-names> <surname>Pecevski</surname></string-name>, and <string-name><given-names>Wolfgang</given-names> <surname>Maass</surname></string-name></person-group>. <article-title>A Learning Theory for Reward-Modulated Spike-Timing-Dependent Plasticity with Application to Biofeedback</article-title>. <source>PLOS Computational Biology</source> <volume>4</volume>.<issue>10</issue> (<month>Oct</month>. <year>2008</year>), <fpage>e1000180</fpage>. issn: <issn>1553-7358</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000180</pub-id>.</mixed-citation></ref>
<ref id="c68"><label>[68]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Peter C.</given-names> <surname>Humphreys</surname></string-name> <etal>et al.</etal></person-group> <article-title>BCI Learning Phenomena Can Be Explained by Gradient-Based Optimization</article-title>. <source>bioRxiv</source> <month>Dec</month>. <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1101/2022.12.08.519453</pub-id>.</mixed-citation></ref>
<ref id="c69"><label>[69]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jordan A.</given-names> <surname>Taylor</surname></string-name>, <string-name><given-names>John W.</given-names> <surname>Krakauer</surname></string-name>, and <string-name><given-names>Richard B.</given-names> <surname>Ivry</surname></string-name></person-group>. <article-title>Explicit and Implicit Contributions to Learning in a Sensorimotor Adaptation Task</article-title>. <source>J. Neurosci</source>. <volume>34</volume>.<issue>8</issue> (<month>Feb</month>. <year>2014</year>), pp. <fpage>3023</fpage>–<lpage>3032</lpage>. issn: <issn>0270-6474</issn>, <issn>1529-2401</issn>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3619-13.2014</pub-id>.</mixed-citation></ref>
<ref id="c70"><label>[70]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Aaron L.</given-names> <surname>Wong</surname></string-name> <etal>et al.</etal></person-group> <article-title>Explicit Knowledge Enhances Motor Vigor and Performance: Motivation versus Practice in Sequence Tasks</article-title>. <source>Journal of Neurophysiology</source> <volume>114</volume>.<issue>1</issue> (<month>July</month> <year>2015</year>), pp. <fpage>219</fpage>– <lpage>232</lpage>. issn: <issn>0022-3077</issn>. doi: <pub-id pub-id-type="doi">10.1152/jn.00218.2015</pub-id>.</mixed-citation></ref>
<ref id="c71"><label>[71]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Alexandre</given-names> <surname>Payeur</surname></string-name>, <string-name><given-names>Amy L.</given-names> <surname>Orsborn</surname></string-name>, and <string-name><given-names>Guillaume</given-names> <surname>Lajoie</surname></string-name></person-group>. <article-title>Neural Manifolds and Gradient-Based Adaptation in Neural-Interface Tasks</article-title>. <source>bioRxiv</source> <month>Mar</month>. <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1101/2023.03.11.532146</pub-id>.</mixed-citation></ref>
<ref id="c72"><label>[72]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jeffrey A.</given-names> <surname>Kleim</surname></string-name> <etal>et al.</etal></person-group> <article-title>Cortical Synaptogenesis and Motor Map Reorganization Occur during Late, But Not Early, Phase of Motor Skill Learning</article-title>. <source>J. Neurosci</source>. <volume>24</volume>.<issue>3</issue> (<month>Jan</month>. <year>2004</year>), pp. <fpage>628</fpage>–<lpage>633</lpage>. issn: <issn>0270-6474</issn>, <issn>1529-2401</issn>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3440-03.2004</pub-id>.</mixed-citation></ref>
<ref id="c73"><label>[73]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Xulu</given-names> <surname>Sun</surname></string-name> <etal>et al.</etal></person-group> <article-title>Cortical Preparatory Activity Indexes Learned Motor Memories</article-title>. <source>Nature</source> <volume>602</volume>.<issue>7896</issue> (<month>Feb</month>. <year>2022</year>), pp. <fpage>274</fpage>–<lpage>279</lpage>. issn: <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41586-021-04329-x</pub-id>.</mixed-citation></ref>
<ref id="c74"><label>[74]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Darby M.</given-names> <surname>Losey</surname></string-name> <etal>et al.</etal></person-group> <article-title>Learning leaves a memory trace in motor cortex</article-title>. <source>Current Biology</source> (<year>2024</year>). <volume>34</volume>: <fpage>1519</fpage>–<lpage>1531.E4</lpage> doi: <pub-id pub-id-type="doi">10.1016/j.cub.2024.03.003</pub-id>.</mixed-citation></ref>
<ref id="c75"><label>[75]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>James B.</given-names> <surname>Heald</surname></string-name>, <string-name><surname>Máté</surname> <given-names>Lengyel</given-names></string-name> and <string-name><given-names>Daniel M.</given-names> <surname>Wolpert</surname></string-name></person-group>. <article-title>Contextual Inference Underlies the Learning of Sensorimotor Repertoires</article-title>. <source>Nature</source> <volume>600</volume>.<issue>7889</issue> (<month>Dec</month>. <year>2021</year>), pp. <fpage>489</fpage>–<lpage>493</lpage>. issn: <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41586-021-04129-3</pub-id>.</mixed-citation></ref>
<ref id="c76"><label>[76]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ta-Chu</given-names> <surname>Kao</surname></string-name> and <string-name><given-names>Guillaume</given-names> <surname>Hennequin</surname></string-name></person-group>. <article-title>Neuroscience out of Control: Control-Theoretic Perspectives on Neural Circuit Dynamics</article-title>. <source>Current Opinion in Neurobiology. Computational Neuroscience</source> <volume>58</volume> (<month>Oct</month>. <year>2019</year>), pp. <fpage>122</fpage>–<lpage>129</lpage>. issn: <issn>0959-4388</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.conb.2019.09.001</pub-id>.</mixed-citation></ref>
<ref id="c77"><label>[77]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Peiran</given-names> <surname>Gao</surname></string-name> and <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name></person-group>. <article-title>On Simplicity and Complexity in the Brave New World of Large-Scale Neuroscience</article-title>. <source>Current Opinion in Neurobiology. Large-Scale Recording Technology</source> (32) <volume>32</volume> (<month>June</month> <year>2015</year>), pp. <fpage>148</fpage>–<lpage>155</lpage>. issn: <issn>0959-4388</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.conb.2015.04.003</pub-id>.</mixed-citation></ref>
<ref id="c78"><label>[78]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Krishna V.</given-names> <surname>Shenoy</surname></string-name> and <string-name><given-names>Jose M.</given-names> <surname>Carmena</surname></string-name></person-group>. <article-title>Combining Decoder Design and Neural Adaptation in Brain-Machine Interfaces</article-title>. <source>Neuron</source> <volume>84</volume>.<issue>4</issue> (<month>Nov</month>. <year>2014</year>), pp. <fpage>665</fpage>–<lpage>680</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.038</pub-id>.</mixed-citation></ref>
<ref id="c79"><label>[79]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Serafeim</given-names> <surname>Perdikis</surname></string-name> and <string-name><given-names>Jose del R.</given-names> <surname>Millan</surname></string-name></person-group>. <article-title>Brain-Machine Interfaces: A Tale of Two Learners</article-title>. <source>IEEE Systems, Man, and Cybernetics Magazine</source> <volume>6</volume>.<issue>3</issue> (<month>July</month> <year>2020</year>), pp. <fpage>12</fpage>–<lpage>19</lpage>. issn: <issn>2333-942X</issn>. doi: <pub-id pub-id-type="doi">10.1109/MSMC.2019.2958200</pub-id>.</mixed-citation></ref>
<ref id="c80"><label>[80]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jane X.</given-names> <surname>Wang</surname></string-name> <etal>et al.</etal></person-group> <article-title>Prefrontal Cortex as a Meta-Reinforcement Learning System</article-title>. <source>Nature Neuroscience</source> <volume>21</volume>.<issue>6</issue> (<month>June</month> <year>2018</year>), pp. <fpage>860</fpage>–<lpage>868</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41593-018-0147-8</pub-id>.</mixed-citation></ref>
<ref id="c81"><label>[81]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ryan M.</given-names> <surname>Neely</surname></string-name> <etal>et al.</etal></person-group> <article-title>Volitional Modulation of Primary Visual Cortex Activity Requires the Basal Ganglia</article-title>. <source>Neuron</source> <volume>97</volume>.<issue>6</issue> (<month>Mar</month>. <year>2018</year>), <fpage>1356</fpage>–<lpage>1368.e4</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.051</pub-id>.</mixed-citation></ref>
<ref id="c82"><label>[82]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Vendrell-Llopis</surname></string-name> <etal>et al.</etal></person-group> <article-title>Ventral Striatum Uses a Temporal Difference Rule for Prediction during Neuroprosthetic Control</article-title>. <conf-name>2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)</conf-name>. <month>Mar</month>. <year>2019</year>, pp. <fpage>562</fpage>–<lpage>565</lpage>. doi: <pub-id pub-id-type="doi">10.1109/NER.2019.8716982</pub-id>.</mixed-citation></ref>
<ref id="c83"><label>[83]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Vivek R</given-names> <surname>Athalye</surname></string-name>, <string-name><given-names>Jose M</given-names> <surname>Carmena</surname></string-name>, and <string-name><given-names>Rui M</given-names> <surname>Costa</surname></string-name></person-group>. <article-title>Neural Reinforcement: Re-Entering and Refining Neural Dynamics Leading to Desirable Outcomes</article-title>. <source>Current Opinion in Neurobiology. Neurobiology of Behavior</source> <volume>60</volume> (<month>Feb</month>. <year>2020</year>), pp. <fpage>145</fpage>–<lpage>154</lpage>. issn: <issn>0959-4388</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.conb.2019.11.023</pub-id>.</mixed-citation></ref>
<ref id="c84"><label>[84]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Barbara</given-names> <surname>Feulner</surname></string-name> <etal>et al.</etal></person-group> <article-title>Small, Correlated Changes in Synaptic Connectivity May Facilitate Rapid Motor Learning</article-title>. <source>Nat Commun</source> <volume>13</volume>.<issue>1</issue> (<month>Sept</month>. <year>2022</year>), p. <fpage>5163</fpage>. issn: <issn>2041-1723</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41467-022-32646-w</pub-id>.</mixed-citation></ref>
<ref id="c85"><label>[85]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Ricky T. Q.</given-names> <surname>Chen</surname></string-name> <etal>et al.</etal></person-group> <article-title>Neural Ordinary Differential Equations</article-title>. <conf-name>Advances in Neural Information Processing Systems 31</conf-name>., <year>2018</year>, pp. <fpage>6571</fpage>–<lpage>6583</lpage>.</mixed-citation></ref>
<ref id="c86"><label>[86]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Richard H.</given-names> <surname>Byrd</surname></string-name> <etal>et al.</etal></person-group> <article-title>A Limited Memory Algorithm for Bound Constrained Optimization</article-title>. <source>SIAM Journal on Scientific Computing</source> <volume>16</volume>.<issue>5</issue> (<month>Sept</month>. <year>1995</year>), pp. <fpage>1190</fpage>–<lpage>1208</lpage>. issn: <issn>1064-8275</issn>. doi: <pub-id pub-id-type="doi">10.1137/0916069</pub-id>.</mixed-citation></ref>
<ref id="c87"><label>[87]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. P.</given-names> <surname>Georgopoulos</surname></string-name>, <string-name><given-names>A. B.</given-names> <surname>Schwartz</surname></string-name>, and <string-name><given-names>R. E.</given-names> <surname>Kettner</surname></string-name></person-group>. <article-title>Neuronal Population Coding of Movement Direction</article-title>. <source>Science</source> <volume>233</volume>.<issue>4771</issue> (<month>Sept</month>. <year>1986</year>), pp. <fpage>1416</fpage>–<lpage>1419</lpage>. issn: <issn>0036-8075</issn>, <issn>1095-9203</issn>. doi: <pub-id pub-id-type="doi">10.1126/science.3749885</pub-id>.</mixed-citation></ref>
<ref id="c88"><label>[88]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Guillaume</given-names> <surname>Hennequin</surname></string-name>, <string-name><given-names>Tim P.</given-names> <surname>Vogels</surname></string-name>, and <string-name><given-names>Wulfram</given-names> <surname>Gerstner</surname></string-name></person-group>. <article-title>Non-Normal Amplification in Random Balanced Neuronal Networks</article-title>. <source>Phys. Rev. E</source> <volume>86</volume>.<issue>1</issue> (<month>July</month> <year>2012</year>), p. <fpage>011909</fpage>. doi: <pub-id pub-id-type="doi">10.1103/PhysRevE.86.011909</pub-id>.</mixed-citation></ref>
<ref id="c89"><label>[89]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Guillaume</given-names> <surname>Hennequin</surname></string-name>, <string-name><given-names>Tim P.</given-names> <surname>Vogels</surname></string-name>, and <string-name><given-names>Wulfram</given-names> <surname>Gerstner</surname></string-name></person-group>. <article-title>Optimal Control of Transient Dynamics in Balanced Networks Supports Generation of Complex Movements</article-title>. <source>Neuron</source> <volume>82</volume>.<issue>6</issue> (<month>June</month> <year>2014</year>), pp. <fpage>1394</fpage>–<lpage>1406</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id>.</mixed-citation></ref>
<ref id="c90"><label>[90]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>David</given-names> <surname>Sussillo</surname></string-name> <etal>et al.</etal></person-group> <article-title>A Neural Network That Finds a Naturalistic Solution for the Production of Muscle Activity</article-title>. <source>Nature Neuroscience</source> <volume>18</volume>.<issue>7</issue> (<month>July</month> <year>2015</year>), pp. <fpage>1025</fpage>–<lpage>1033</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn.4042</pub-id>.</mixed-citation></ref>
<ref id="c91"><label>[91]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Todorov</surname></string-name> and <string-name><given-names>W.</given-names> <surname>Li</surname></string-name></person-group>. <article-title>Optimal Control Methods Suitable for Biomechanical Systems</article-title>. <conf-name>Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat. No.03CH37439)</conf-name>. Vol. <volume>2</volume>. <month>Sept</month>. <year>2003</year>, <fpage>1758</fpage>–<lpage>1761</lpage> Vol.2. doi: <pub-id pub-id-type="doi">10.1109/IEMBS.2003.1279748</pub-id>.</mixed-citation></ref>
<ref id="c92"><label>[92]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ta-Chu</given-names> <surname>Kao</surname></string-name>, <string-name><given-names>Mahdieh S.</given-names> <surname>Sadabadi</surname></string-name>, and <string-name><given-names>Guillaume</given-names> <surname>Hennequin</surname></string-name></person-group>. <article-title>Optimal Anticipatory Control as a Theory of Motor Preparation: A Thalamo-Cortical Circuit Model</article-title>. <source>Neuron</source> <volume>109</volume>.<issue>9</issue> (<month>May</month> <year>2021</year>), <fpage>1567</fpage>–<lpage>1581.e12</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.009</pub-id>.</mixed-citation></ref>
<ref id="c93"><label>[93]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Diederik P.</given-names> <surname>Kingma</surname></string-name> and <string-name><given-names>Jimmy</given-names> <surname>Ba</surname></string-name></person-group>. <article-title>Adam: A Method for Stochastic Optimization</article-title>. <source>arXiv</source> (<month>Jan</month>. <year>2017</year>). <pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id>.</mixed-citation></ref>
<ref id="c94"><label>[94]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Friedrich</given-names> <surname>Schuessler</surname></string-name> <etal>et al.</etal></person-group> <article-title>The Interplay between Randomness and Structure during Learning in RNNs</article-title>. <conf-name>Advances in Neural Information Processing Systems 33</conf-name>, <year>2020</year>, pp. <fpage>13352</fpage>–<lpage>13362</lpage>.</mixed-citation></ref>
<ref id="c95"><label>[95]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Francesca</given-names> <surname>Mastrogiuseppe</surname></string-name> and <string-name><given-names>Srdjan</given-names> <surname>Ostojic</surname></string-name></person-group>. <article-title>Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks</article-title>. <source>Neuron</source> <volume>99</volume>.<issue>3</issue> (<month>Aug</month>. <year>2018</year>), <fpage>609</fpage>–<lpage>623.e29</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.003</pub-id>.</mixed-citation></ref>
<ref id="c96"><label>[96]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Laureline</given-names> <surname>Logiaco</surname></string-name>, <string-name><given-names>L. F.</given-names> <surname>Abbott</surname></string-name>, and <string-name><given-names>Sean</given-names> <surname>Escola</surname></string-name></person-group>. <article-title>Thalamic Control of Cortical Dynamics in a Model of Flexible Motor Sequencing</article-title>. <source>Cell Reports</source> <volume>35</volume>.<issue>9</issue> (<month>June</month> <year>2021</year>), p. <fpage>109090</fpage>. issn: <issn>2211-1247</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.celrep.2021.109090</pub-id>.</mixed-citation></ref>
<ref id="c97"><label>[97]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Beata</given-names> <surname>Jarosiewicz</surname></string-name> <etal>et al.</etal></person-group> <article-title>Functional Network Reorganization during Learning in a Brain-Computer Interface Paradigm</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>105</volume>.<issue>49</issue> (<month>Dec</month>. <year>2008</year>), pp. <fpage>19486</fpage>–<lpage>19491</lpage>. issn: <issn>0027-8424</issn>, <issn>1091-6490</issn>. doi: <pub-id pub-id-type="doi">10.1073/pnas.0808113105</pub-id>.</mixed-citation></ref>
<ref id="c98"><label>[98]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Karunesh</given-names> <surname>Ganguly</surname></string-name> <etal>et al.</etal></person-group> <article-title>Reversible Large-Scale Modification of Cortical Networks during Neuroprosthetic Control</article-title>. <source>Nature Neuroscience</source> <volume>14</volume>.<issue>5</issue> (<month>May</month> <year>2011</year>), pp. <fpage>662</fpage>–<lpage>667</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn.2797</pub-id>.</mixed-citation></ref>
<ref id="c99"><label>[99]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Huibert</given-names> <surname>Kwakernaak</surname></string-name> and <string-name><given-names>Raphael</given-names> <surname>Sivan</surname></string-name></person-group>. <source>Linear Optimal Control Systems</source>. Vol. <volume>1</volume>. <publisher-name>Wiley-interscience</publisher-name> <publisher-loc>New York</publisher-loc>, <year>1972</year>.</mixed-citation></ref>
<ref id="c100"><label>[100]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Emanuel</given-names> <surname>Todorov</surname></string-name> and <string-name><given-names>Michael I.</given-names> <surname>Jordan</surname></string-name></person-group>. <article-title>Optimal Feedback Control as a Theory of Motor Coordination</article-title>. <source>Nature Neuroscience</source> <volume>5</volume>.<issue>11</issue> (<month>Nov</month>. <year>2002</year>), pp. <fpage>1226</fpage>–<lpage>1235</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn963</pub-id>.</mixed-citation></ref>
<ref id="c101"><label>[101]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Matthew D</given-names> <surname>Golub</surname></string-name>, <string-name><given-names>Byron M</given-names> <surname>Yu</surname></string-name>, and <string-name><given-names>Steven M</given-names> <surname>Chase</surname></string-name></person-group>. <article-title>Internal Models for Interpreting Neural Population Activity during Sensorimotor Control</article-title>. <source>eLife</source> <volume>4</volume> (<month>Dec</month>. <year>2015</year>), <fpage>e10015</fpage>. issn: <issn>2050-084X</issn>. doi: <pub-id pub-id-type="doi">10.7554/eLife.10015</pub-id>.</mixed-citation></ref>
<ref id="c102"><label>[102]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sergey D.</given-names> <surname>Stavisky</surname></string-name> <etal>et al.</etal></person-group> <article-title>Motor Cortical Visuomotor Feedback Activity Is Initially Isolated from Downstream Targets in Output-Null Neural State Space Dimensions</article-title>. <source>Neuron</source> <volume>95</volume>.<issue>1</issue> (<month>July</month> <year>2017</year>), <fpage>195</fpage>–<lpage>208.e9</lpage>. issn: <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.023</pub-id>.</mixed-citation></ref>
<ref id="c103"><label>[103]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Maryam M.</given-names> <surname>Shanechi</surname></string-name> <etal>et al.</etal></person-group> <article-title>Rapid Control and Feedback Rates Enhance Neuroprosthetic Control</article-title>. <source>Nature Communications</source> <volume>8</volume>.<issue>1</issue> (<month>Jan</month>. <year>2017</year>), p. <fpage>13825</fpage>. issn: <issn>2041-1723</issn>. doi: <pub-id pub-id-type="doi">10.1038/ncomms13825</pub-id>.</mixed-citation></ref>
<ref id="c104"><label>[104]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michael E.</given-names> <surname>Tipping</surname></string-name> and <string-name><given-names>Christopher M.</given-names> <surname>Bishop</surname></string-name></person-group>. <article-title>Probabilistic Principal Component Analysis</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source> <volume>61</volume>.<issue>3</issue> (<year>1999</year>), pp. <fpage>611</fpage>–<lpage>622</lpage>. issn: <issn>1467-9868</issn>. doi: <pub-id pub-id-type="doi">10.1111/1467-9868.00196</pub-id>.</mixed-citation></ref>
<ref id="c105"><label>[105]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Patrick T.</given-names> <surname>Sadtler</surname></string-name> <etal>et al.</etal></person-group> <article-title>Neural Constraints on Learning</article-title>. <source>Nature</source> <volume>512</volume>.<issue>7515</issue> (<month>Aug</month>. <year>2014</year>), pp. <fpage>423</fpage>–<lpage>426</lpage>. issn: <issn>1476-4687</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature13665</pub-id>.</mixed-citation></ref>
<ref id="c106"><label>[106]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Matthew D.</given-names> <surname>Golub</surname></string-name> <etal>et al.</etal></person-group> <article-title>Learning by Neural Reassociation</article-title>. <source>Nature Neuroscience</source> <volume>21</volume>.<issue>4</issue> (<month>Apr</month>. <year>2018</year>), pp. <fpage>607</fpage>–<lpage>616</lpage>. issn: <issn>1546-1726</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41593-018-0095-3</pub-id>.</mixed-citation></ref>
<ref id="c107"><label>[107]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ake</given-names> <surname>Björck</surname></string-name> and <string-name><given-names>Gene H.</given-names> <surname>Golub</surname></string-name></person-group>. <article-title>Numerical Methods for Computing Angles between Linear Subspaces</article-title>. <source>Mathematics of Computation</source> <volume>27</volume>.<issue>123</issue> (<year>1973</year>), pp. <fpage>579</fpage>–<lpage>594</lpage>. issn: <issn>0025-5718</issn>, <issn>1088-6842</issn>. doi: <pub-id pub-id-type="doi">10.1090/S0025-5718-1973-0348991-3</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106309.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study proposes a network implementation of the &quot;re-aiming&quot; learning strategy, which has been hypothesized to underlie brain-computer interface learning. Combining theoretical arguments, numerical simulations, and analysis of experimental data, the authors provide <bold>convincing</bold> evidence for their hypothesis. This paper will likely be of broad interest to the systems neuroscience community.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106309.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study considers learning with brain-computer interfaces (BCIs) in nonhuman primates, and in particular, the high speed and flexibility with which subjects learn to control these BCIs.</p>
<p>The authors raise the hypothesis that such learning is based on controlling a small number of input or control variables, rather than directly adapting neural connectivity within the network of neurons that drive the BCI. Adapting a small number of input variables would circumvent the issue of credit assignment in high dimensions and allow for quick learning, potentially using cognitive strategies (&quot;re-aiming&quot;). Based on a computational model, the authors show that such a strategy is viable in a number of experimental settings and reproduces previous experimental observations:</p>
<p>(1) Differences in learning with decoders either within or outside of the neural manifold (the space spanned by the dominant modes of neural activity).</p>
<p>(2) A novel, theory-based prediction on biases in BCI learning due to the positivity of neural firing rates, which is then confirmed in data from previous experiments.</p>
<p>(3) An example of &quot;illusory credit assignment&quot;: Changes in neurons' tuning curves depending on whether these neurons are affected by changes in the BCI decoder, even though learning only happens on the level of low-dimensional control variables.</p>
<p>(4) A reproduction of results from operant conditioning of individual neurons, in particular, the observation that it is difficult to change the firing rates of neurons strongly correlated before learning in different directions (up vs down).</p>
<p>Taken together, these observations yield strong evidence for the plausibility that subjects use such a learning strategy, at least during short-term learning.</p>
<p>Strengths:</p>
<p>Text and figures are clearly structured and allow readers to understand the main concepts well. The study presents a very clear and simple model that explains a number of seemingly disparate or even contradictory observations (neuron-specific credit assignment vs. low-dimensional, cognitive control). The predicted and tested bias due to positivity of firing rates provides a neat example of how such a theory can help understand experimental results. The idea that subjects first use a small number of command variables (those sufficient in the calibration task) and later, during learning, add more variables provides a nice illustration of the idea that learning takes place on multiple time scales, potentially with different mechanisms at play. On a more detailed level, the study is a nice example of closely matching the theory to the experiment, in particular regarding the modeling of BCI perturbations.</p>
<p>Weaknesses:</p>
<p>Overall, I find only two minor weaknesses. First, the insights of this study are, first and foremost, of feed-forward nature, and a feed-forward network would have been enough (and the more parsimonious model) to illustrate the results. While using a recurrent neural network (RNN) shows that the results are, in general, compatible with recurrent dynamics, the specific limitations imposed by RNNs (e.g., dynamical stability, low-dimensional internal dynamics) are not the focus of this study. Indeed, the additional RNN models in the supplementary material show that under more constrained conditions for the RNN (low-dimensional dynamics), using the input control alone runs into difficulties.</p>
<p>Second, explaining the quantitative differences between the model and data for shifts in tuning curves seems to take the model a bit too literally. The model serves greatly for qualitative observations. I assume, however, that many of the unconstrained aspects of the model would yield quantitatively different results.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106309.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary :</p>
<p>The paper proposes a model to explain the learning that occurs in brain-computer interface (BCI) tasks when animals need to adapt to novel BCI decoders. The model consists of a network formulation of the &quot;re-aiming&quot; learning strategy, which assumes that BCI learning does not modify the underlying neural circuitry, but instead occurs through a reorganization of existing neural activity patterns.</p>
<p>The authors formalize this in a recurrent neural network (RNN) model, driven by upstream inputs that live in a low-dimensional space.</p>
<p>They show that modelling BCI learning as reorganization of these upstream inputs can explain several experimental findings, such as the difference in the ability of animals to adapt to within vs outside-manifold perturbations, biases in the decoded behaviour after within-manifold perturbations, or qualitative changes in the neural responses observed during credit assignment rotation perturbations or operant conditioning of individual neurons.</p>
<p>Overall, while the idea of re-aiming as a learning strategy has previously been proposed in the literature, the authors show how it can be formalized in a network model, which allows for more direct comparisons to experimental data.</p>
<p>Strengths:</p>
<p>The paper is very well written. The presentation of the model is clear, and the use of vanilla RNN dynamics driven by upstream inputs that are constant in time is consistent with the broader RNN modeling literature.</p>
<p>The main value of the paper lies in the fact that it proposes a network implementation for a learning strategy that had been proposed previously. The network model has a simple form, but the optimization problem is performed in the space of inputs, which requires the authors to solve a nonlinear optimization problem in that space.</p>
<p>While some of the results (eg the fact that the model can adapt to within but not outside-manifold perturbations) are to be expected based on the model assumptions, having a network model allows to make more direct and quantitative comparisons to experiments, to investigate analytically how much the dimension of the output is constrained by the input, and to make predictions that can be tested in data.</p>
<p>The authors perform such comparisons across three different experiments. The results are clearly presented, and the authors show that they hold for various RNN connectivities.</p>
<p>Weaknesses :</p>
<p>The authors mention alternative models (eg, based on synaptic plasticity in the RNN and/or input weights) that can explain the same experimental data that they do, they do not provide any direct comparisons to those models.</p>
<p>Thus, the main argument that the authors have in favor of their model is the fact that it is more plausible because it relies on performing the optimization in a low-dimensional space. It would be nice to see more quantitative arguments for why the re-aiming strategy may be more plausible than synaptic plasticity (either by showing that it explains data better, or explaining why it may be more optimal in the context of fast learning).</p>
<p>In particular, the authors model the adaptation to outside-manifold perturbations (OMPs) through a &quot;generalized re-aiming strategy&quot;. This assumes the existence of additional command variables, which are not used in the original decoding task, but can then be exploited to adapt to these OMPs. While this model is meant to capture the fact that optimization is occurring in a low-dimensional subspace, the fact that animals take longer to adapt to OMPs suggests that WMPs and OMPs may rely on different learning mechanisms, and that synaptic plasticity may actually be a better model of adaptation to OMPs. It would be important to discuss how exactly generalized re-aiming would differ from allowing plasticity in the input weights, or in all weights in the network. Do those models make different predictions, and could they be differentiated in future experiments?</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106309.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Menéndez</surname>
<given-names>Jorge A</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5005-8791</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Hennig</surname>
<given-names>Jay A</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Golub</surname>
<given-names>Matthew D</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Oby</surname>
<given-names>Emily R</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sadtler</surname>
<given-names>Patrick T</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Batista</surname>
<given-names>Aaron P</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chase</surname>
<given-names>Steven M</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yu</surname>
<given-names>Byron M</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Latham</surname>
<given-names>Peter E</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Overall, I find only two minor weaknesses. First, the insights of this study are, first and foremost, of feed-forward nature, and a feed-forward network would have been enough (and the more parsimonious model) to illustrate the results. While using a recurrent neural network (RNN) shows that the results are, in general, compatible with recurrent dynamics, the specific limitations imposed by RNNs (e.g., dynamical stability, low-dimensional internal dynamics) are not the focus of this study. Indeed, the additional RNN models in the supplementary material show that under more constrained conditions for the RNN (low-dimensional dynamics), using the input control alone runs into difficulties.</p>
</disp-quote>
<p>We thank the reviewer for raising this important point. While we agree that recurrent dynamics were not the focus of this study, we would like to point out that 1) dynamics, of some kind, are necessary to simulate the decoder fitting process and 2) recurrent neural networks (RNNs) are valuable for obtaining general insights on how biological constraints shape the reachable manifold:</p>
<p>(1) To simulate the decoder fitting process, we had to simulate neural activity during the so-called “calibration task”. Some dynamics to these responses are necessary to produce a population response with dimensionality resembling what was found in experiments (10 dimensions). Moreover, dynamics are necessary to create a common direction of high variance across population responses to the calibration task stimuli (see Supplementary Figure 2a and surrounding discussion), which is necessary to reproduce the biases in readouts demonstrated in Figure 4 (as many within-manifold decoder perturbations are aligned with it; Supplementary Figure 2b).</p>
<p>Because feed-forward networks lack dynamics, reproducing our results with a feed-forward network would require using an input with dynamics. Rather than making an arbitrary choice for these input dynamics, we chose to keep the input static and instead generate the dynamics with a RNN, which is in line with recent models of motor cortex.</p>
<p>We agree, however, that this is an important point worth clarifying in the manuscript. In our revision we will aim to add a demonstration of how to reproduce a subset of our results with a feed-forward network and a dynamic input.</p>
<p>(2) While we agree that RNNs impose certain limitations over feed-forward networks, we see these limitations as an advantage because they provide a framework for understanding the structure of the reachable manifold in terms of biological constraints. For example, our simulations in Supplementary Figure 1 show that the dimensionality of the reachable manifold is highly dependent on recurrent connectivity: inhibition-stabilized connectivity makes it higher-dimensional whereas task-specific optimized connectivity makes it lower-dimensional. Such insights are valuable to understand the broader implications and experimental predictions of the re-aiming strategy.</p>
<p>Because feed-forward networks are untied from the reality of recurrent cortical circuitry, they cannot be characterized in terms of such biological constraints. For instance, as the reviewer points out, dynamical stability is not a well-defined property of feed-forward networks. Such models therefore cannot provide any insight into how the biological constraint of dynamical stability could influence the reachable manifold (which we show it does in Figure 5b). Relatedly, feed-forward networks cannot be optimized to solve complex spatiotemporal tasks like the ballistic reaching task we used for our task-optimized RNN (Supplementary Figure 1, right column), so cannot be used to understand how such behavioral constraints would influence the reachable manifold.</p>
<p>We agree that these reasons for using RNNs are subtle and left implicit in how they are currently exposed in the text. We will add a discussion point clarifying these in our revision.</p>
<disp-quote content-type="editor-comment">
<p>Second, explaining the quantitative differences between the model and data for shifts in tuning curves seems to take the model a bit too literally. The model serves greatly for qualitative observations. I assume, however, that many of the unconstrained aspects of the model would yield quantitatively different results.</p>
</disp-quote>
<p>We completely agree: our model is best used to provide a qualitative description of the capabilities of the re-aiming strategy. We will be sure to revise our manuscript to keep such quantitative comparisons at a minimum.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>The authors mention alternative models (eg, based on synaptic plasticity in the RNN and/or input weights) that can explain the same experimental data that they do, they do not provide any direct comparisons to those models. Thus, the main argument that the authors have in favor of their model is the fact that it is more plausible because it relies on performing the optimization in a low-dimensional space. It would be nice to see more quantitative arguments for why the re-aiming strategy may be more plausible than synaptic plasticity (either by showing that it explains data better, or explaining why it may be more optimal in the context of fast learning).</p>
</disp-quote>
<p>We agree this remains a limitation of our study. To contrast our re-aiming model with models of synaptic plasticity (in the input and/or recurrent weights), we have included substantial discussion of these alternative models in two sections of the manuscript:</p>
<list list-type="bullet">
<list-item><p>Introduction, where we elaborate on the argument that synaptic plasticity requires solving an exceptionally difficult optimization problem in high dimensions</p>
</list-item>
<list-item><p>Discussion section “The role of synaptic plasticity in BCI learning”, where we review a number of synaptic plasticity models and experimental results they can account for</p>
</list-item>
</list>
<p>We fully agree that more quantitative comparisons remain an important follow-up to this line of research. However, it is worth noting that there are many such models out there. Moreover, as is the case with many computational models, the results one can achieve with any given model can be highly sensitive to a number of different hyperparameters (e.g. learning rates). We therefore feel that a more rigorous comparison requires deeper study and is out of scope of this manuscript.</p>
<disp-quote content-type="editor-comment">
<p>In particular, the authors model the adaptation to outside-manifold perturbations (OMPs) through a &quot;generalized re-aiming strategy&quot;. This assumes the existence of additional command variables, which are not used in the original decoding task, but can then be exploited to adapt to these OMPs. While this model is meant to capture the fact that optimization is occurring in a low-dimensional subspace, the fact that animals take longer to adapt to OMPs suggests that WMPs and OMPs may rely on different learning mechanisms, and that synaptic plasticity may actually be a better model of adaptation to OMPs.</p>
</disp-quote>
<p>We thank the reviewer for raising this question. We agree that the fact that animals take longer to adapt to OMPs suggests that the underlying learning strategy is somehow different. But the argument we try to make in this section of the paper is that it in fact does not require an entirely different mechanism. Our simulations show that the same mechanism of re-aiming can suffice to learn OMPs, but it simply requires re-aiming in the larger space of all command variables available to the motor system (rather than just the two command variables evoked by the calibration task). Because this is a much higher-dimensional search space (10-20 vs. 2 dimensions, which is a substantial difference due to the curse of dimensionality), we argue that learning should be slower, even though the mechanism (i.e. re-aiming) is the same.</p>
<p>This is an important and somewhat surprising takeaway from these simulations, which we will try to bring up more explicitly and clearly in the revision.</p>
<disp-quote content-type="editor-comment">
<p>It would be important to discuss how exactly generalized re-aiming would differ from allowing plasticity in the input weights, or in all weights in the network. Do those models make different predictions, and could they be differentiated in future experiments?</p>
</disp-quote>
<p>They do in fact make different predictions, and we thank the reviewer for asking and pointing out the lack of discussion of this point. The key difference between these two learning mechanisms is demonstrated in Figure 5b: under generalized re-aiming, there is a fundamental limit to the set of activity patterns one can learn to produce in the brain-computer interface (BCI) learning task. This is quantified in that analysis by the asymptotic participation ratio of the reachable manifold as K increases, which indicates that there is a limited ~12-dimensional subspace that the reachable manifold can occupy. The specific orientation of this subspace is determined by the (recurrent and input) connectivity of the recurrent neural network. With synaptic plasticity in any of the weight matrices (Wrec,Win,U), this subspace could be re-oriented in any arbitrary direction. Our theory of “generalized re-aiming” therefore predicts that the reachable manifold is 1) constrained to a low-d subspace and 2) is not modified when learning BCIs with outside-manifold perturbations.</p>
<p>Experimentally testing this would require a within-/outside- manifold perturbation BCI learning task akin to that of Sadtler et al, but where the “intrinsic manifold” is measured from population responses evoked by every possible motor command so as to entirely contain the full reachable manifold at max K. This would require measuring motor cortical activity during naturalistic behavior under a wide range of conditions, rather than just in response to the 2D cursor movements on the screen used in the calibration task of the original study. In this case, learning outside-manifold perturbations would require re-orienting the reachable manifold, so a pure generalized re-aiming strategy would fail to learn them. Synaptic plasticity, on the other hand, would not.</p>
<p>We will be sure to elaborate further on this claim in the revised manuscript.</p>
</body>
</sub-article>
</article>