<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99416</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99416</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99416.3</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.5</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Progressive neural engagement within the IFG-pMTG circuit as gesture and speech entropy and MI advances</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6979-940X</contrib-id>
<name>
<surname>Zhao</surname>
<given-names>Wanying</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>zhaowy@psych.ac.cn</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Zhouyi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Xiang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4512-5221</contrib-id>
<name>
<surname>Du</surname>
<given-names>Yi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>duyi@psych.ac.cn</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/034t30j35</institution-id><institution>State Key Laboratory of Cognitive Science and Mental Health, Institute of Psychology, Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>Department of Psychology, University of Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/029819q61</institution-id><institution>Chinese Institute for Brain Research</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03x1jna21</institution-id><institution>School of Psychology, Central China Normal University</institution></institution-wrap>, <city>Wuhan</city>, <country country="CN">China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country country="NL">Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-12-13">
<day>13</day>
<month>12</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-07-16">
<day>16</day>
<month>07</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99416</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-28">
<day>28</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-06-04">
<day>04</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.11.23.517759"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-12-13">
<day>13</day>
<month>12</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99416.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.99416.1.sa4">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99416.1.sa3">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99416.1.sa2">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99416.1.sa1">Reviewer #3 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.99416.1.sa0">Author response:</self-uri>
</event>
<event>
<event-desc>Reviewed preprint v2</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-05-19">
<day>19</day>
<month>05</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99416.2"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.99416.2.sa3">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99416.2.sa2">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99416.2.sa1">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.99416.2.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Zhao et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Zhao et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99416-v3.pdf"/>
<abstract>
<title>Abstract</title><p>Semantic representation emerges from distributed multisensory modalities, yet a comprehensive understanding of the functional changing pattern within convergence zones or hubs integrating multisensory semantic information remains elusive. In this study, employing information-theoretic metrics, we quantified gesture and speech information, alongside their interaction, utilizing entropy and mutual information (MI). Neural activities were assessed via interruption effects induced by High-Definition transcranial direct current stimulation (HD-tDCS). Additionally, chronometric double-pulse transcranial magnetic stimulation (TMS) and high-temporal event-related potentials were utilized to decipher dynamic neural changes resulting from various information contributors. Results showed gradual inhibition of both inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG) as degree of gesture-speech integration, indexed by MI, increased. Moreover, a time-sensitive and staged progression of neural engagement was observed, evidenced by distinct correlations between neural activity patterns and entropy measures of speech and gesture, as well as MI, across early sensory and lexico-semantic processing stages. These findings illuminate the gradual nature of neural activity during multisensory gesture-speech semantic processing, shaped by dynamic gesture constraints and speech encoding, thereby offering insights into the neural mechanisms underlying multisensory language processing.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>gesture-speech integration</kwd>
<kwd>pMTG-IFG circuit</kwd>
<kwd>information theory</kwd>
<kwd>multisensory</kwd>
<kwd>semantic</kwd>
<kwd>dual-stage modal</kwd>
</kwd-group>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution>The STI 2030—Major Projects</institution>
</institution-wrap>
</funding-source>
<award-id>2021ZD0201500</award-id>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>31822024</award-id>
</award-group>
<award-group id="funding-2a">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>31800964</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Add explanation of chronometric double-pulse TMS in the Introduction.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Semantic representation, distinguished by its cohesive conceptual nature, emerges from distributed modality-specific regions. Consensus acknowledges the presence of ’convergence zones’ within the temporal and inferior parietal areas<sup><xref ref-type="bibr" rid="c1">1</xref></sup>, or the ’semantic hub’ located in the anterior temporal lobe<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, pivotal for integrating, converging, or distilling multimodal inputs. Contemporary theories frame the semantic processing as a dynamic sequence of neural states<sup><xref ref-type="bibr" rid="c3">3</xref></sup>, shaped by systems that are finely tuned to the statistical regularities inherent in sensory inputs<sup><xref ref-type="bibr" rid="c4">4</xref></sup>. These regularities enable the brain to evaluate, weight, and integrate multisensory information, optimizing the reliability of individual sensory signals<sup><xref ref-type="bibr" rid="c5">5</xref></sup>. However, sensory inputs available to the brain are often incomplete and uncertain, necessitating adaptive neural adjustments to resolve these ambiguities<sup><xref ref-type="bibr" rid="c6">6</xref></sup>. In this context, neuronal activity is thought to be linked to the probability density of sensory information, with higher levels of uncertainty resulting in the engagement of a broader population of neurons, thereby reflecting the brain’s adaptive capacity to handle diverse possible interpretations<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>. Although the role of ’convergence zones’ and ’semantic hubs’ in integrating multimodal inputs is well established, the precise functional patterns of neural activity in response to the distribution of unified multisensory information—along with the influence of unisensory signals —remain poorly understood.</p>
<p>To this end, we developed an analytic approach to directly probe the cortical engagement during multisensory gesture-speech semantic integration. Even though gestures convey information in a global-synthetic way, while speech conveys information in a linear segmented way, there exists a bidirectional semantic influence between the two modalities<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>. Gesture is regarded as ‘part of language’<sup><xref ref-type="bibr" rid="c11">11</xref></sup> or functional equivalents of lexical units that alternate and integrate with speech into a ‘single unification space’ to convey a coherent meaning<sup><xref ref-type="bibr" rid="c12">12</xref>–<xref ref-type="bibr" rid="c14">14</xref></sup>. Empirical studies have investigated the semantic integration between gesture and speech by manipulating their semantic relationship<sup><xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c18">18</xref></sup> and revealed a mutual interaction between them<sup><xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c21">21</xref></sup> as reflected by the N400 latency and amplitude<sup><xref ref-type="bibr" rid="c14">14</xref></sup> as well as common neural underpinnings in the left inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG)<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>.</p>
<p>Building on these insights, the present study quantified the amount of information from both sources and their interaction adopting the information-theoretic complexity metrics of <italic>entropy</italic> and <italic>mutual information</italic> (MI). Unisensory Entropy measures the disorder or randomness of information and serves as an index of the uncertainty in modality-specific representations of gesture or speech activated by an event<sup><xref ref-type="bibr" rid="c24">24</xref></sup>. MI assesses share information between modalities<sup><xref ref-type="bibr" rid="c25">25</xref></sup>, indicating multisensory convergence and acting as an index of gesture-speech integration.</p>
<p>To investigate the neural mechanisms underlying gesture-speech integration, we conducted three experiments to assess how neural activity correlates with distributed multisensory integration, quantified using information-theoretic measures of MI. Additionally, we examined the contributions of unisensory signals in this process, quantified through unisensory entropy. <bold>Experiment 1</bold> employed high-definition transcranial direct current stimulation (HD-tDCS) to administer Anodal, Cathodal and Sham stimulation to either the IFG or the pMTG. HD-tDCS induces membrane depolarization with anodal stimulation and membrane hyperpolarization with cathodal stimulation<sup><xref ref-type="bibr" rid="c26">26</xref></sup>, thereby increasing or decreasing cortical excitability in the targeted brain area, respectively. This experiment aimed to determine whether the overall facilitation (Anodal-tDCS minus Sham-tDCS) and/or inhibitory (Cathodal-tDCS minus Sham-tDCS) of these integration hubs is modulated by the degree of gesture-speech integration, as measure by MI.</p>
<p>Given the differential involvement of the IFG and pMTG in gesture-speech integration, shaped by top-down gesture predictions and bottom-up speech processing<sup><xref ref-type="bibr" rid="c23">23</xref></sup>, <bold>Experiment 2</bold> was designed to further assess whether the activity of these regions was associated with relevant informational matrices. To this end, we employed chronometric double-pulse transcranial magnetic stimulation, which is known to transiently reduce cortical excitability at the inter-pulse interval<sup><xref ref-type="bibr" rid="c27">27</xref></sup>. Within a temporal period broad enough to capture the full duration of gesture–speech integration<sup><xref ref-type="bibr" rid="c28">28</xref></sup>, we targeted specific timepoints previously implicated in integrative processing within IFG and pMTG<sup><xref ref-type="bibr" rid="c23">23</xref></sup>. This allowed us to test whether the inhibitory effects of TMS were correlated with unisensory entropy or the multisensory convergence index (MI).</p>
<p><bold>Experiment 3</bold> complemented these investigations by focusing on the temporal dynamics of neural responses during semantic processing, leveraging high-temporal event-related potentials (ERPs). This experiment investigated how distinct information contributors modulated specific ERP components associated with semantic processing. These components included the early sensory effects as P1 and N1–P2<sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup>, the N400 semantic conflict effect<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>, and the late positive component (LPC) reconstruction effect<sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup>. By integrating these ERP findings with results from Experiments 1 and 2, Experiment 3 aimed to provide a more comprehensive understanding of how gesture-speech integration is modulated by neural dynamics.</p>
</sec>
<sec id="s2">
<title>Material and methods</title>
<sec id="s2a">
<title>Participants</title>
<p>Ninety-eight young Chinese participants signed written informed consent forms and took part in the present study (Experiment 1: 29 females, 23 males, age = 20 ± 3.40 years; Experiment 2: 11 females, 13 males, age = 23 ± 4.88 years; Experiment 3: 12 females, 10 males, age = 21 ± 3.53 years). All of the participants were right-handed (Experiment 1: laterality quotient (LQ)<sup><xref ref-type="bibr" rid="c34">34</xref></sup> = 88.71 ± 13.14; Experiment 2: LQ = 89.02 ± 13.25; Experiment 3: LQ = 88.49 ± 12.65), had normal or corrected-to-normal vision and were paid ¥100 per hour for their participation. All experiments were approved by the Ethics Committee of the Institute of Psychology, Chinese Academy of Sciences.</p>
</sec>
<sec id="s3">
<title>Stimuli</title>
<p>Twenty gestures (<xref rid="tbla1" ref-type="table">Appendix Table 1</xref>) with 20 semantically congruent speech signals taken from previous study<sup><xref ref-type="bibr" rid="c23">23</xref></sup> were used. The stimuli set were recorded from two native Chinese speakers (1 male, 1 female). To validate the stimuli, 30 participants were recruited to replicate the multisensory index of semantic congruency effect, hypothesizing that reaction times for semantically incongruent gesture-speech pairs would be significantly longer than those for congruent pairs. The results confirmed this hypothesis, with a significantly (<italic>t</italic>(29) = 7.16, <italic>p</italic> &lt; 0.001) larger reaction time when participants were asked to judge the gender of the speaker if gesture contained incongruent semantic information with speech (a ‘cut’ gesture paired with speech word ‘喷 pen1 (spray)’: mean = 554.51 ms, SE = 11.65) relative to when they were semantically congruent (a ‘cut’ gesture paired with ‘剪 jian3 (cut)’ word: mean = 533.90 ms, SE = 12.02)<sup><xref ref-type="bibr" rid="c23">23</xref></sup>.</p>
<p>Additionally, two separate pre-tests with 30 subjects in each (pre-test 1: 16 females, 14 males, age = 24 ± 4.37 years; pre-test 2: 15 females, 15 males, age = 22 ± 3.26 years) were conducted to determine the comprehensive values of gesture and speech. Participants were presented with segments of increasing duration, beginning at 40 ms, and were prompted to provide a single verb to describe either the isolated gesture they observed (pre-test 1) or the isolated speech they heard (pre-test 2). For each gesture or speech, the action verb consistently provided by participants across four to six consecutive repetitions—with the number of repetitions varied to mitigate learning effects—was considered the comprehensive response for the gesture or speech. The initial instance duration was marked as the discrimination point (DP) for gesture (mean = 183.78 ± 84.82ms) or the identification point (IP) for speech (mean = 176.40 ± 66.21ms) (<xref rid="fig1" ref-type="fig">Figure 1A top</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Experimental design, and stimulus characteristics.</title><p><bold>(A) Experimental stimuli.</bold> Twenty gestures were paired with 20 relevant speech stimuli. Two separate Pre-tests were executed to define the minimal length of each gesture and speech required for semantic identification, namely, the discrimination point (DP) of gesture and the identification point (IP) of speech. Overall, a mean of 183.78 ms (SD = 84.82) was found for the DP of gestures and the IP of speech was 176.40 ms (SD = 66.21). The onset of speech was set at the gesture DP. Responses for each item were assessed utilizing information- theoretic complexity metrics to quantify the information content of both gesture and speech during integration, employing entropy and MI.</p>
<p><bold>Procedure(B) of Experiment 1.</bold> HD-tDCS, including Anodal, Cathodal, or Sham conditions, was administered to the IFG or pMTG) using a 4 * 1 ring-based electrode montage. Electrode F7 targeted the IFG, with return electrodes placed on AF7, FC5, F9, and FT9. For pMTG stimulation, TP7 was targeted, with return electrodes positioned on C5, P5, T9, and P9. Sessions lasted 20 minutes, with a 5-second fade-in and fade-out, while the Sham condition involved only 30 seconds of stimulation.</p>
<p><bold>(C) Procedure of Experiment 2.</bold> Eight time windows (TWs, duration = 40 ms) were segmented in relative to the speech IP. Among the eight TWs, five (TW1, TW2, TW3, TW6, and TW7) were chosen based on the significant results in our prior study<sup><xref ref-type="bibr" rid="c23">23</xref></sup>. Double-pulse TMS was delivered over each of the TW of either the pMTG or the IFG.</p>
<p><bold>(D) Procedure of Experiment 3.</bold> Semantically congruent gesture-speech pairs were presented randomly with Electroencephalogram (EEG) recorded simultaneously. Epochs were time locked to the onset of speech and lasted for 1000 ms. A 200 ms pre-stimulus baseline correction was applied before the onset of gesture stoke. Various elicited components were hypothesized.</p></caption>
<graphic xlink:href="517759v5_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To quantify information content, comprehensive responses for each item were converted into Shannon’s entropy (H) as a measure of information richness (<xref rid="fig1" ref-type="fig">Figure 1A bottom</xref>). With no significant gender differences observed in both gesture (<italic>t</italic>(20) = 0.21, <italic>p</italic> = 0.84) and speech (<italic>t</italic>(20) = 0.52, <italic>p</italic> = 0.61), responses were aggregated across genders, resulting in 60 answers per item (<xref ref-type="table" rid="tbl2b">Appendix Table 2</xref>). Here, p(xi) and p(yi) represent the distribution of 60 answers for a given gesture (<xref rid="tbl2b" ref-type="table">Appendix Table 2B</xref>) and speech (<xref rid="tbl2a" ref-type="table">Appendix Table 2A</xref>), respectively. High entropy indicates diverse answers, reflecting broad representation, while low entropy suggests focused lexical recognition for a specific item (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). MI was used to measure the overlap between gesture and speech information, calculated by subtracting the entropy of the combined gesture-speech dataset (Entropy(gesture + speech)) from the sum of their individual entropies (Entropy(gesture) + Entropy(speech)) (see <xref rid="tbl2c" ref-type="table">Appendix Table 2C</xref>). For specific gesture-speech combinations, equivalence between the combined entropy and the sum of individual entropies (gesture or speech) indicates absence of overlap in response sets. Conversely, significant overlap, denoted by a considerable number of shared responses between gesture and speech datasets, leads to a noticeable discrepancy between combined entropy and the sum of gesture and speech entropies. Elevated MI values thus signify substantial overlap, indicative of a robust mutual interaction between gesture and speech.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Quantification formulas (A) and distributions of each stimulus in Shannon’s entropy (B).</title><p>Two separate pre-tests (N = 30) were conducted to assign a single verb for describing each of the isolated 20 gestures and 20 speech items. Responses provided for each item were transformed into Shannon’s entropy using a relative quantification formula. Gesture (<bold>B left</bold>) and speech (<bold>B right</bold>) entropy quantify the randomness of gestural or speech information, representing the uncertainty of probabilistic representation activated when a specific stimulus occurs. Joint entropy (<bold>B middle</bold>) captures the widespread nature of the two sources of information combined. Mutual information (MI) was calculated as the difference between joint entropy with gesture entropy and speech entropy combined (<bold>A</bold>), thereby capturing the overlap of gesture and speech and representing semantic integration.</p></caption>
<graphic xlink:href="517759v5_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Additionally, the number of responses provided for each gesture and speech, as well as the total number of combined responses, were also recorded. The quantitative data for each stimulus, including gesture entropy, speech entropy, joint entropy, MI, and the respective counts, are presented in <xref rid="tbla3" ref-type="table">Appendix Table 3</xref>.</p>
<p>To determine whether entropy or MI values corresponds to distinct neural changes, the current study first aggregated neural responses (including inhibition effects of tDCS and TMS or ERP amplitudes) that shared identical entropy or MI values, prior to conducting correlational analyses.</p>
<sec id="s3a">
<title>Experimental procedure</title>
<p>Given that gestures induce a semantic priming effect on concurrent speech<sup><xref ref-type="bibr" rid="c35">35</xref></sup>, this study utilized a semantic priming paradigm in which speech onset was aligned with the DP of each gesture<sup><xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>, the point at which the gesture transitions into a lexical form<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. The gesture itself began at the stroke phase, a critical moment when the gesture conveys its primary semantic content<sup><xref ref-type="bibr" rid="c36">36</xref></sup>.</p>
<p>An irrelevant factor of gender congruency (e.g., a man making a gesture combined with a female voice) was created<sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. This involved aligning the gender of the voice with the corresponding gender of the gesture in either a congruent (e.g., male voice paired with a male gesture) or incongruent (e.g., male voice paired with a female gesture) manner. This approach served as a direct control mechanism, facilitating the investigation of the automatic and implicit semantic interplay between gesture and speech<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. In light of previous findings indicating a distinct TMS-disruption effect on the semantic congruency of gesture-speech interactions<sup><xref ref-type="bibr" rid="c23">23</xref></sup>, both semantically congruent and incongruent pairs were included in Experiment 1 and Experiment 2. Experiment 3, conversely, exclusively utilized semantically congruent pairs to elucidate ERP metrics indicative of nuanced semantic progression.</p>
<p>Gesture–speech pairs were presented randomly using Presentation software (<ext-link ext-link-type="uri" xlink:href="https://www.neurobs.com">www.neurobs.com</ext-link>). Participants were asked to look at the screen but respond with both hands as quickly and accurately as possible merely to the gender of the voice they heard. The RT and the button being pressed were recorded. The experiment started with a fixation cross presented on the center of the screen, which lasted for 0.5-1.5 sec.</p>
</sec>
<sec id="s3b">
<title>Experiment 1: HD-tDCS protocol and data analysis</title>
<p>Participants were divided into two groups, with each group undergoing HD-tDCS stimulation at different target sites (IFG or pMTG). Each participant completed three experimental sessions, spaced one week apart, during which 480 gesture-speech pairs were presented across various conditions. In each session, participants received one of three types of HD-tDCS stimulation: Anodal, Cathodal, or Sham. The order of stimulation site and type was counterbalanced using a Latin square design to control for potential order effects.</p>
<p>HD-tDCS protocol employed a constant current stimulator (The Starstim 8 system) delivering stimulation at an intensity of 2000mA. A 4 * 1 ring-based electrode montage was utilized, comprising a central electrode (stimulation) positioned directly over the target cortical area and four return electrodes encircling it to provide focused stimulation. Building on a meta-analysis of prior fMRI studies examining gesture-speech integration<sup><xref ref-type="bibr" rid="c22">22</xref></sup>, we targeted Montreal Neurological Institute (MNI) coordinates for the left IFG at (-62, 16, 22) and the pMTG at (-50, -56, 10). In the stimulation protocol for HD-tDCS, the IFG was targeted using electrode F7 as the optimal cortical projection site<sup><xref ref-type="bibr" rid="c38">38</xref></sup>, with four return electrodes placed at AF7, FC5, F9, and FT9. For the pMTG, TP7 was selected as the cortical projection site<sup><xref ref-type="bibr" rid="c38">38</xref></sup>, with return electrodes positioned at C5, P5, T9, and P9. The stimulation parameters included a 20-minute duration with a 5-second fade-in and fade-out for both Anodal and Cathodal conditions. The Sham condition involved a 5-second fade-in followed by only 30 seconds of stimulation, then 19’20 minutes of no stimulation, and finally a 5-second fade-out (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). Stimulation was controlled using NIC software, with participants blinded to the stimulation conditions.</p>
<p>All incorrect responses (702 out of the total number of 24960, 2.81% of trials) were excluded. To eliminate the influence of outliers, a 2SD trimmed mean for every participant in each session was also calculated. To examine the relationship between the degree of information and neural responses, we conducted Pearson correlation analyses using a sample of 20 sets. Neural responses were quantified based on the effects of HD-tDCS (active tDCS minus sham tDCS) on the semantic congruency effect, defined as the difference in reaction times between semantic incongruent and congruent conditions (Rt(incongruent) - Rt(congruent)). This effect served as an index of multisensory integration<sup><xref ref-type="bibr" rid="c37">37</xref></sup> within the left IFG and pMTG. The variation in information was assessed using three information-theoretic metrics. To account for potential confounds related to multiple candidate representations, we conducted partial correlation analyses between the tDCS effects and gesture entropy, speech entropy, and MI, controlling for the number of responses provided for each gesture and speech, as well as the total number of combined responses. Given that HD-tDCS induces overall disruption at the targeted brain regions, we hypothesized that the neural activity within the left IFG and pMTG would be progressively affected by varying levels of multisensory convergence, as indexed by MI. Moreover, we hypothesized that the modulation of neural activity by MI would differ between the left IFG and pMTG, as reflected in the differential modulation of response numbers in the partial correlations, highlighting their distinct roles in semantic processing<sup><xref ref-type="bibr" rid="c39">39</xref></sup>. False discovery rate (FDR) correction was applied for multiple comparisons.</p>
</sec>
<sec id="s3c">
<title>Experiment 2: TMS protocol and data analysis</title>
<p>Experiment 2 involved 800 gesture-speech pairs, presented across 15 blocks over three days, with one week between sessions. Stimulation was administered at three different sites (IFG, pMTG, or Vertex). Within the time windows (TWs) spanning the gesture-speech integration period, five TWs that exhibited selective disruption of integration were selected: TW1 (-120 to -80 ms relative to the speech identification point), TW2 (-80 to -40 ms), TW3 (-40 to 0 ms), TW6 (80 to 120 ms), and TW7 (120 to 160 ms)<sup><xref ref-type="bibr" rid="c23">23</xref></sup> (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). The order of stimulation site and TW was counterbalanced using a Latin square design.</p>
<p>At an intensity of 50% of the maximum stimulator output, double-pulse TMS was delivered via a 70 mm figure-eight coil using a Magstim Rapid² stimulator (Magstim, UK). High-resolution (1 × 1 × 0.6 mm) T1-weighted MRI scans were obtained using a Siemens 3T Trio/Tim Scanner for image-guided TMS navigation. Frameless stereotaxic procedures (BrainSight 2; Rogue Research) allowed real-time stimulation monitoring. To ensure precision, individual anatomical images were manually registered by identifying the anterior and posterior commissures. Subject-specific target regions were defined using trajectory markers in the MNI coordinate system. Vertex was used as control.</p>
<p>All incorrect responses (922 out of the total number of 19200, 4.8% of trials) were excluded. We focused our analysis on Pearson correlations of the TMS interruption effects (active TMS minus vertex TMS) of the semantic congruency effect with the gesture entropy, speech entropy or MI. To control for potential confounds, partial correlations were also performed between the TMS effects and gesture entropy, speech entropy, and MI, controlling for the number of responses for each gesture and speech, as well as the total number of combined responses. By doing this, we can determine how the time-sensitive contribution of the left IFG and pMTG to gesture–speech integration was affected by gesture and speech information distribution. FDR correction was applied for multiple comparisons.</p>
</sec>
<sec id="s3d">
<title>Experiment 3: Electroencephalogram (EEG) recording and data analysis</title>
<p>Experiment 3, comprising a total of 1760 gesture-speech pairs, was completed in a single-day session. EEG were recorded from 48 Ag/AgCl electrodes mounted in a cap according to the 10-20 system<sup><xref ref-type="bibr" rid="c40">40</xref></sup>, amplified with a PORTI-32/MREFA amplifier (TMS International B.V., Enschede, NL) and digitized online at 500 Hz (bandpass, 0.01-70 Hz). EEGLAB, a MATLAB toolbox, was used to analyze the EEG data<sup><xref ref-type="bibr" rid="c41">41</xref></sup>. Vertical and horizontal eye movements were measured with 4 electrodes placed above the left eyebrow, below the left orbital ridge and at bilateral external canthus. All electrodes were referenced online to the left mastoid. Electrode impedance was maintained below 5 KΩ. The average of the left and right mastoids was used for re-referencing. A high-pass filter with a cutoff of 0.05 Hz and a low-pass filter with a cutoff of 30 Hz were applied. Semi-automated artifact removal, including independent component analysis (ICA) for identifying components of eye blinks and muscle activity, was performed (<xref rid="fig1" ref-type="fig">Figure 1D</xref>). Participants with rejected trials exceeding 30% of their total were excluded from further analysis.</p>
<p>All incorrect responses were excluded (147 out of 1760, 8.35% of trials). To eliminate the influence of outliers, a 2 SD trimmed mean was calculated for every participant in each condition. Data were epoched from the onset of speech and lasted for 1000 ms. To ensure a clean baseline with no stimulus presented, a 200 ms pre-stimulus baseline correction was applied before gesture onset.</p>
<p>To consolidate the data, we conducted both a traditional region-of-interest (ROI) analysis, with ROIs defined based on a well-established work<sup><xref ref-type="bibr" rid="c42">42</xref></sup>, and a cluster-based permutation approach, which utilizes data-driven permutations to enhance robustness and address multiple comparisons.</p>
<p>For the traditional ROI analysis, grand-average ERPs at electrode Cz were compared between the higher (≥50%) and lower (&lt;50%) halves for gesture entropy (<xref rid="fig5" ref-type="fig">Figure 5A1</xref>), speech entropy (<xref rid="fig5" ref-type="fig">Figure 5B1</xref>), and MI (<xref rid="fig5" ref-type="fig">Figure 5C1</xref>). Consequently, four ERP components were determined: the P1 effect observed within the time window of 0-100 ms<sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup>, the N1-P2 effect observed between 150-250ms<sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup>, the N400 within the interval of 250-450ms<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>, and the LPC spanning from 550-1000ms<sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup>. Additionally, seven regions-of-interest (ROIs) were defined in order to locate the modulation effect on each ERP component: left anterior (LA): F1, F3, F5, FC1, FC3, and FC5; left central (LC): C1, C3, C5, CP1, CP3, and CP5; left posterior (LP): P1, P3, P5, PO3, PO5, and O1; right anterior (RA): F2, F4, F6, FC2, FC4, and FC6; right central (RC): C2, C4, C6, CP2, CP4, and CP6; right posterior (RP): P2, P4, P6, PO4, PO6, and O2; and midline electrodes (ML): Fz, FCz, Cz, Pz, Oz, and CPz<sup><xref ref-type="bibr" rid="c42">42</xref></sup>.</p>
<p>Subsequently, cluster-based permutation tests<sup><xref ref-type="bibr" rid="c43">43</xref></sup> in Fieldtrip was further used to determine the significant clusters of adjacent time points and electrodes of ERP amplitude between the higher and lower halves of gesture entropy, speech entropy and MI, respectively. The electrode-level type I error threshold was set to 0.025. Cluster-level statistic was estimated through 5000 Monte Carlo simulations, where the cluster-level statistic is the sum of T-values for each stimulus within a cluster. The cluster-level type I error threshold was set to 0.05. Clusters with a p-value less than the critical alpha-level are considered to be conditionally different.</p>
<p>Paired t-tests were conducted to compare the lower and upper halves of each information model for the averaged amplitude within each ROI or cluster across the four ERP time windows, separately. Pearson correlations were computed between each model value and the averaged ERP amplitudes in each ROI or cluster. Additionally, partial correlations were conducted, accounting for the number of responses for each respective metric. FDR correction was applied for multiple comparisons.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<title>Results</title>
<sec id="s4a">
<title>Experiment 1: Modulation of left pMTG and IFG engagement by gradual changes in gesture-speech semantic information</title>
<p>In the IFG, one-way ANOVA examining the effects of three tDCS conditions (Anodal, Cathodal, or Sham) on semantic congruency (RT (semantic incongruent) – RT (semantic congruent)) demonstrated a significant main effect of stimulation condition (<italic>F</italic>(2, 75) = 3.673, <italic>p</italic> = 0.030, ηp2 = 0.089). Post hoc paired t-tests indicated a significantly reduced semantic congruency effect between the Cathodal condition and the Sham condition (<italic>t</italic>(26) = -3.296, <italic>p</italic> = 0.003, 95% CI = [-11.488, 4.896]) (<xref rid="fig3" ref-type="fig">Figure 3A left</xref>). Subsequent Pearson correlation analysis revealed that the reduced semantic congruency effect was progressively associated with the MI, evidenced by a significant correlation between the Cathodal-tDCS effect (Cathodal-tDCS minus Sham- tDCS) and MI (<italic>r</italic> = -0.595, <italic>p</italic> = 0.007, 95% CI = [-0.995, -0.195]) (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). Additionally, a similar correlation was observed between the Cathodal-tDCS effect and the total response number (<italic>r</italic> = -0.543, <italic>p</italic> = 0.016, 95% CI = [-0.961, -0.125]).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>tDCS effect over semantic congruency.</title>
<p><bold>(A)</bold> tDCS effect was defined as active-tDCS minus sham-tDCS. The semantic congruency effect was calculated as the reaction time (RT) difference between semantically incongruent and semantically congruent pairs (Rt(incongruent) - Rt(congruent)).</p><p><bold>(B)</bold> Correlations of the tDCS effect over the semantic congruency effect with three information models (gesture entropy, speech entropy and MI) are displayed with best-fitting regression lines. Significant correlations are marked in red. * <italic>p</italic> &lt; 0.05, ** <italic>p</italic> &lt; 0.01 after FDR correction.</p></caption>
<graphic xlink:href="517759v5_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>However, partial correlation analysis, controlling for the total response number, revealed that the initially significant correlation between the Cathodal-tDCS effect and MI was no longer significant (<italic>r</italic> = -0.303, <italic>p</italic> = 0.222, 95% CI = [-0.770, 0.164]). This suggests that the observed relationship between Cathodal-tDCS and MI may be confounded by semantic control difficulty, as reflected by the total number of responses. Specifically, the reduced activity in the IFG under Cathodal-tDCS may be driven by variations in the difficulty of semantic control rather than a direct modulation of MI.</p>
<p>In the pMTG, a one-way ANOVA assessing the effects of three tDCS conditions on semantic congruency also revealed a significant main effect of stimulation condition (<italic>F</italic>(2, 75) = 3.250, <italic>p</italic> = 0.044, ηp2 = 0.080). Subsequent paired t-tests identified a significantly reduced semantic congruency effect between the Cathodal condition and the Sham condition (<italic>t</italic>(25) = - 2.740, <italic>p</italic> = 0.011, 95% CI = [-11.915, 6.435]) (<xref rid="fig3" ref-type="fig">Figure 3A right</xref>). Moreover, a significant correlation was observed between the Cathodal-tDCS effect and MI (<italic>r</italic> = -0.457, <italic>p</italic> = 0.049, 95% CI = [-0.900, -0.014]) (<xref rid="fig3" ref-type="fig">Figure 3B</xref>).</p>
<p>Importantly, the reduced activity in the pMTG under Cathodal-tDCS was not influenced by the total response number, as indicated by the non-significant correlation (<italic>r</italic> = -0.253, <italic>p</italic> = 0.295, 95% CI = [-0.735, 0.229]). This finding was further corroborated by the unchanged significance in the partial correlation between Cathodal-tDCS and MI, when controlling for the total response number (<italic>r</italic> = -0.472, <italic>p</italic> = 0.048, 95% CI = [-0.903, -0.041]).</p>
<p>RTs of congruent and incongruent trials of IFG and pMTG in each of the stimulation conditions were shown in <xref rid="tbl4a" ref-type="table">Appendix Table 4A</xref>.</p>
</sec>
<sec id="s4b">
<title>Experiment 2: Time-sensitive modulation of left pMTG and IFG engagements by gradual changes in gesture-speech semantic information</title>
<p>A 2 (TMS effect: active - Vertex) × 5 (TW) ANOVA on semantic congruency revealed a significant interaction between TMS effect and TW (<italic>F</italic>(3.589, 82.538) = 3.273, <italic>p</italic> = 0.019, ηp2 = 0.125). Further t-tests identified a significant TMS effect over the pMTG in TW1 (<italic>t</italic>(23) = - 3.068, <italic>p</italic> = 0.005, 95% CI = [-6.838, 0.702]), TW2 (<italic>t</italic>(23) = -2.923, <italic>p</italic> = 0.008, 95% CI = [-6.490, 0.644]), and TW7 (<italic>t</italic>(23) = -2.005, <italic>p</italic> = 0.047, 95% CI = [-5.628, 1.618]). In contrast, a significant TMS effect over the IFG was found in TW3 (<italic>t</italic>(23) = -2.335, <italic>p</italic> = 0.029, 95% CI = [- 5.928, 1.258]), and TW6 (<italic>t</italic>(23) = -4.839, <italic>p</italic> &lt; 0.001, 95% CI = [-7.617, -2.061]) (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). Raw RTs of congruent and incongruent trials were shown in <xref rid="tbl4b" ref-type="table">Appendix Table 4B</xref>.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>TMS impacts on semantic congruency effect across various time windows (TW).</title>
<p><bold>(A)</bold> Five time windows (TWs) showing selective disruption of gesture-speech integration were chosen: TW1 (-120 to -80 ms relative to speech identification point), TW2 (-80 to -40 ms), TW3 (-40 to 0 ms), TW6 (80 to 120 ms), and TW7 (120 to 160 ms). TMS effect was defined as active-TMS minus vertex-TMS. The semantic congruency effect was calculated as the reaction time (RT) difference between semantically incongruent and semantically congruent pairs.</p><p><bold>(B)</bold> Correlations of the TMS effect over the semantic congruency effect with three information models (gesture entropy, speech entropy and MI) are displayed with best-fitting regression lines. Significant correlations are marked in red. * <italic>p</italic> &lt; 0.05, ** <italic>p</italic> &lt; 0.01, *** <italic>p</italic> &lt; 0.001 after FDR correction.</p></caption>
<graphic xlink:href="517759v5_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Additionally, a significant negative correlation was found between the TMS effect (a larger negative TMS effect signifies a greater disruption of the integration process) and speech entropy when the pMTG was inhibited in TW2 (<italic>r</italic> = -0.792, <italic>p</italic> = 0.004, 95% CI = [-1.252, - 0.331]). Meanwhile, when the IFG activity was interrupted in TW6, a significant negative correlation was found between the TMS effect and gesture entropy (<italic>r</italic> = -0.539, <italic>p</italic> = 0.014, 95% CI = [-0.956, -0.122]), speech entropy (<italic>r</italic> = -0.664, <italic>p</italic> = 0.026, 95% CI = [-1.255, -0.073]), and MI (<italic>r</italic> = -0.677, <italic>p</italic> = 0.001, 95% CI = [-1.054, -0.300]) (<xref rid="fig4" ref-type="fig">Figure 4B</xref>).</p>
<p>Notably, inhibition of pMTG activity in TW2 was not influenced by the number of speech responses (<italic>r</italic> = -0.539, <italic>p</italic> = 0.087, 95% CI = [-1.145, 0.067]). However, the number of speech responses did affect the modulation of speech entropy on the pMTG inhibition effect in TW2. This was evidenced by the non-significant partial correlation between pMTG inhibition and speech entropy when controlling for speech response number (<italic>r</italic> = -0.218, <italic>p</italic> = 0.545, 95% CI = [-0.563, 0.127]).</p>
<p>In contrast, the interrupted IFG activity in TW6 appeared to be consistently influenced by the confound of semantic control difficulty. This was reflected in the significant correlation with both gesture response number (<italic>r</italic> = -0.480, <italic>p</italic> = 0.032, 95% CI = [-904, -0.056]), speech response number (<italic>r</italic> = -0.729, <italic>p</italic> = 0.011, 95% CI = [-1.221, -0.237]), and total response number (<italic>r</italic> = -0.591, <italic>p</italic> = 0.008, 95% CI = [-0.993, -0.189]). Additionally, partial correlation analyses revealed non-significant relationship between interrupted IFG activity in TW6 and gesture entropy (<italic>r</italic> = -0.369, <italic>p</italic> = 0.120, 95% CI = [-0.810, -0.072]), speech entropy (<italic>r</italic> = -0.455, <italic>p</italic> = 0.187, 95% CI = [-1.072, 0.162]), and MI (<italic>r</italic> = -0.410, <italic>p</italic> = 0.091, 95% CI = [-0.856, -0.036]) when controlling for response numbers.</p>
</sec>
<sec id="s4c">
<title>Experiment 3: Temporal modulation of P1, N1-P2, N400 and LPC components by gradual changes in gesture-speech semantic information</title>
<p>Topographical maps illustrating amplitude differences between the lower and higher halves of speech entropy demonstrate a central-posterior P1 amplitude (0-100 ms, <xref rid="fig5" ref-type="fig">Figure 5B</xref>). Aligning with prior findings<sup><xref ref-type="bibr" rid="c29">29</xref></sup>, the paired t-tests demonstrated a significantly larger P1 amplitude within the ML ROI (<italic>t</italic>(22) = 2.510, <italic>p</italic> = 0.020, 95% confidence interval (CI) = [1.66, 3.36]) when contrasting stimuli with higher 50% speech entropy against those with lower 50% speech entropy (<xref rid="fig5" ref-type="fig">Figure 5D1 left</xref>). Subsequent correlation analyses unveiled a significant increase in the P1 amplitude with the rise in speech entropy within the ML ROI (<italic>r</italic> = 0.609, <italic>p</italic> = 0.047, 95% CI = [0.039, 1.179], <xref rid="fig5" ref-type="fig">Figure 5D1 right</xref>). Furthermore, a cluster of neighboring time-electrode samples exhibited a significant contrast between the lower 50% and higher 50% of speech entropy, revealing a P1 effect spanning 16 to 78 ms at specific electrodes (FC2, FCz, C1, C2, Cz, and CPz, <xref rid="fig5" ref-type="fig">Figure 5D2 middle</xref>) (<italic>t</italic>(22) = 2.754, <italic>p</italic> = 0.004, 95% confidence interval (CI) = [1.65, 3.86], <xref rid="fig5" ref-type="fig">Figure 5D2 left</xref>), with a significant correlation with speech entropy (<italic>r</italic> = 0.636, <italic>p</italic> = 0.035, 95% CI = [0.081, 1.191], <xref rid="fig5" ref-type="fig">Figure 5D2 right</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>ERP results of gesture entropy (A), speech entropy (B) or MI (C).</title>
<p>Four ERP components were identified from grand-average ERPs at the Cz electrode, contrasting trials with the lower 50% (red lines) and the higher 50% (blue lines) of gesture entropy, speech entropy or MI. Clusters of adjacent time points and electrodes were subsequently identified within each component using a cluster-based permutation test. Topographical maps depict amplitude differences between the lower and higher halves of each information model, with significant ROIs (<bold>D1-H1 middle</bold>) or electrode clusters (<bold>D2-H2 middle</bold>) highlighted in black. Solid rectangles delineating the ROIs that exhibited the maximal correlation and paired t-values (<bold>D1-H1 middle</bold>). T-test comparisons with normal distribution lines (<bold>left</bold>) and correlations with best-fitting regression lines (<bold>right</bold>) are calculated and illustrated between the average ERP amplitude within the rectangular ROI (<bold>D1-H1</bold>) or the elicited clusters (<bold>D2-H2</bold>) and the three information models individually. * p &lt; 0.05, ** p &lt; 0.01 after FDR correction.</p></caption>
<graphic xlink:href="517759v5_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Additionally, topographical maps comparing the lower 50% and higher 50% gesture entropy revealed a frontal N1-P2 amplitude (150-250 ms, <xref rid="fig5" ref-type="fig">Figure 5A</xref>). In accordance with previous findings on bilateral frontal N1-P2 amplitude<sup><xref ref-type="bibr" rid="c29">29</xref></sup>, paired t-tests displayed a significantly larger amplitude for stimuli with lower 50% gesture entropy than with higher 50% entropy in both ROIs of LA (<italic>t</italic>(22) = 2.820, <italic>p</italic> = 0.011, 95% CI = [2.21, 3.43]) and RA (<italic>t</italic>(22) = 2.223, <italic>p</italic> = 0.038, 95% CI = [1.56, 2.89]) (<xref rid="fig5" ref-type="fig">Figure 5E1 left</xref>). Moreover, a negative correlation was found between N1-P2 amplitude and gesture entropy in both ROIs of LA (<italic>r</italic> = -0.465, <italic>p</italic> = 0.039, 95% CI = [-0.87, -0.06]) and RA (<italic>r</italic> = -0.465, <italic>p</italic> = 0.039, 95% CI = [-0.88, -0.05]) (<xref rid="fig5" ref-type="fig">Figure 5E1 right</xref>). Additionally, through a cluster-permutation test, the N1-P2 effect was identified between 184 to 202 ms at electrodes FC4, FC6, C2, C4, C6, and CP4 (<xref rid="fig5" ref-type="fig">Figure 5E2 middle</xref>) (<italic>t</italic>(22) = 2.638, <italic>p</italic> = 0.015, 95% CI = [1.79, 3.48], (<xref rid="fig5" ref-type="fig">Figure 5E2 left</xref>)), exhibiting a significant correlation with gesture entropy (<italic>r</italic> = -0.485, <italic>p</italic> = 0.030, 95% CI = [-0.91, -0.06], <xref rid="fig5" ref-type="fig">Figure 5E2 right</xref>).</p>
<p>Furthermore, in line with prior research<sup><xref ref-type="bibr" rid="c44">44</xref></sup>, a left-frontal N400 amplitude (250-450 ms) was discerned from topographical maps of gesture entropy (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). Specifically, stimuli with lower 50% values of gesture entropy elicited a larger N400 amplitude in the LA ROI compared to those with higher 50% values (<italic>t</italic>(22) = 2.455, <italic>p</italic> = 0.023, 95% CI = [1.95, 2.96], <xref rid="fig5" ref-type="fig">Figure 5F1 left</xref>). Concurrently, a negative correlation was noted between the N400 amplitude and gesture entropy (<italic>r</italic> = -0.480, <italic>p</italic> = 0.032, 95% CI = [-0.94, -0.03], <xref rid="fig5" ref-type="fig">Figure 5F1 right</xref>) within the LA ROI. The identified clusters showing the N400 effect for gesture entropy (282 – 318 ms at electrodes FC1, FCz, C1, and Cz, <xref rid="fig5" ref-type="fig">Figure 5F2 middle</xref>) (<italic>t</italic>(22) = 2.828, <italic>p</italic> = 0.010, 95% CI = [2.02, 3.64], <xref rid="fig5" ref-type="fig">Figure 5F2 left</xref>) also exhibited significant correlation between the N400 amplitude and gesture entropy (<italic>r</italic> = -0.445, <italic>p</italic> = 0.049, 95% CI = [-0.88, -0.01], <xref rid="fig5" ref-type="fig">Figure 5F2 right</xref>).</p>
<p>Similarly, a left-frontal N400 amplitude (250-450 ms)<sup><xref ref-type="bibr" rid="c44">44</xref></sup> was discerned from topographical maps for MI (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). A larger N400 amplitude in the LA ROI was observed for stimuli with lower 50% values of MI compared to those with higher 50% values (<italic>t</italic>(22) = 3.00, <italic>p</italic> = 0.007, 95% CI = [2.54, 3.46], <xref rid="fig5" ref-type="fig">Figure 5G1 left</xref>). This was accompanied by a significant negative correlation between N400 amplitude and MI (<italic>r</italic> = -0.504, <italic>p</italic> = 0.028, 95% CI = [-0.97, -0.04], <xref rid="fig5" ref-type="fig">Figure 5G1 right</xref>) within the LA ROI. The N400 effect for MI, observed in the 294–306 ms window at electrodes F1, F3, Fz, FC1, FC3, FCz, and C1 (<xref rid="fig5" ref-type="fig">Figure 5G2 middle</xref>) (<italic>t</italic>(22) = 2.461, <italic>p</italic> = 0.023, 95% CI = [1.62, 3.30], <xref rid="fig5" ref-type="fig">Figure 5G2 left</xref>), also showed a significant negative correlation with MI (<italic>r</italic> = -0.569, <italic>p</italic> = 0.011, 95% CI = [-0.98, -0.16], <xref rid="fig5" ref-type="fig">Figure 5G2 right</xref>).</p>
<p>Finally, consistent with previous findings<sup><xref ref-type="bibr" rid="c32">32</xref></sup>, an anterior LPC effect (550-1000 ms) was observed in topographical maps comparing stimuli with lower and higher 50% speech entropy (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). The reduced LPC amplitude was evident in the paired t-tests conducted in ROIs of LA (<italic>t</italic>(22) = 2.614, <italic>p</italic> = 0.016, 95% CI = [1.88, 3.35]); LC (<italic>t</italic>(22) = 2.592, <italic>p</italic> = 0.017, 95% CI = [1.83, 3.35]); RA (<italic>t</italic>(22) = 2.520, <italic>p</italic> = 0.020, 95% CI = [1.84, 3.24]); and ML (<italic>t</italic>(22) = 2.267, <italic>p</italic> = 0.034, 95% CI = [1.44, 3.10]) (<xref rid="fig5" ref-type="fig">Figure 5H1 left</xref>). Simultaneously, a marked negative correlation with speech entropy was evidenced in ROIs of LA (<italic>r</italic> = -0.836, <italic>p</italic> = 0.001, 95% CI = [-1.26, -0.42]); LC (<italic>r</italic> = -0.762, <italic>p</italic> = 0.006, 95% CI = [-1.23, -0.30]); RA (<italic>r</italic> = -0.774, <italic>p</italic> = 0.005, 95% CI = [-1.23, -0.32]) and ML (<italic>r</italic> = -0.730, <italic>p</italic> = 0.011, 95% CI = [-1.22, -0.24]) (<xref rid="fig5" ref-type="fig">Figure 5H1 right</xref>). Additionally, a cluster with the LPC effect (644 - 688 ms at electrodes Cz, CPz, P1, and Pz, <xref rid="fig5" ref-type="fig">Figure 5H2 middle</xref>) (<italic>t</italic>(22) = 2.754, <italic>p</italic> = 0.012, 95% CI = [1.50, 4.01], <xref rid="fig5" ref-type="fig">Figure 5H2 left</xref>) displayed a significant correlation with speech entropy (<italic>r</italic> = -0.699, <italic>p</italic> = 0.017, 95% CI = [-1.24, -0.16], <xref rid="fig5" ref-type="fig">Figure 5H2 right</xref>).</p>
<p>To clarify potential confounds of semantic control difficulty, partial correlation analyses were conducted to examine the relationship between the elicited ERP components and the relevant information matrices, controlling for response numbers. Results consistently indicated modulation by response numbers in the relationship of ERP components with the information matrix, as evidenced by the non-significant partial correlations between the P1 amplitude (P1 component over ML: <italic>r</italic> = -0.574, <italic>p</italic> = 0.082, 95% CI = [-1.141, -0.007]) and the P1 cluster (<italic>r</italic> = -0.503, <italic>p</italic> = 0.138, 95% CI = [-1.102, 0.096]) with speech entropy; the N1-P2 amplitude (N1-P2 component over LA: <italic>r</italic> = -0.080, <italic>p</italic> = 0.746, 95% CI = [-0.554, 0.394]) and N1-P2 cluster (<italic>r</italic> = -0.179, <italic>p</italic> = 0.464, 95% CI = [-0.647, 0.289]) with gesture entropy; the N400 amplitude (N400 component over LA: <italic>r</italic> = 0.264, <italic>p</italic> = 0.247, 95% CI = [-0.195,0.723]) and N400 cluster (<italic>r</italic> = 0.394, <italic>p</italic> = 0.095, 95% CI = [-0.043, 0.831]) with gesture entropy; the N400 amplitude (N400 component over LA: <italic>r</italic> = -0.134, <italic>p</italic> = 0.595, 95% CI = [-0.620, 0.352]) and N400 cluster (<italic>r</italic> = -0.034, <italic>p</italic> = 0.894, 95% CI = [-0.524,0.456]) with MI; and the LPC amplitude (LPC component over LA: <italic>r</italic> = -0.428, <italic>p</italic> = 0.217, 95% CI = [-1.054, 0.198]) and LPC cluster (<italic>r</italic> = -0.202, <italic>p</italic> = 0.575, 95% CI = [-0.881, 0.477]) with speech entropy.</p>
</sec>
</sec>
<sec id="s5">
<title>Discussion</title>
<p>Through mathematical quantification of gesture and speech information using entropy and mutual information (MI), we examined the functional pattern and dynamic neural structure underlying multisensory semantic integration. Our results, for the first time, revealed that the inhibition effect of cathodal-tDCS on the pMTG and IFG correlated with the degree of gesture- speech multisensory convergence, as indexed by MI (<bold>Experiment 1</bold>). Moreover, the gradual neural engagement was found to be time-sensitive and staged, as evidenced by the selectively interrupted time windows (<bold>Experiment 2</bold>) and the distinct correlated ERP components (<bold>Experiment 3</bold>), which were modulated by different information contributors, including unisensory entropy or multisensory MI. These findings significantly expand our understanding of the cortical foundations of statistically regularized multisensory semantic information.</p>
<p>It is widely acknowledged that a single, amodal system mediates the interactions among perceptual representations of different modalities<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c45">45</xref>,<xref ref-type="bibr" rid="c46">46</xref></sup>. Moreover, observations have suggested that semantic dementia patients experience increasing overregularization of their conceptual knowledge due to the progressive deterioration of this amodal system<sup><xref ref-type="bibr" rid="c47">47</xref></sup>. Consistent with this, the present study provides robust evidence, through the application of HD-tDCS and TMS, that the integration hubs for gesture and speech—the pMTG and IFG— operate in an incremental manner. This is supported by the progressive inhibition effect observed in these brain areas as the entropy and mutual information of gesture and speech advances.</p>
<p>Moreover, by dividing the potential integration period into eight time windows (TW) relative to the speech identification point (IP) and administering inhibitory double-pulse TMS across each TW, the current study attributed the gradual TMS-selective regional inhibition to distinct information sources. In TW2 of gesture-speech integration, which precedes the speech identification point<sup><xref ref-type="bibr" rid="c23">23</xref></sup> and represents a pre-lexical stage, the suppression effect observed in the pMTG was correlated with speech entropy. Conversely, during TW6, which follows the speech identification point<sup><xref ref-type="bibr" rid="c23">23</xref></sup> and represents a post-lexical stage, the IFG interruption effect was influenced by both gesture entropy, speech entropy, and their MI. A dual-stage pMTG-IFG-pMTG neurocircuit loop during gesture-speech integration has been proposed previous<sup><xref ref-type="bibr" rid="c23">23</xref></sup>. As an extension, the present study unveils a staged accumulation of engagement within the neurocircuit linking the transmodal regions of pMTG and IFG, arising from distinct contributors of information.</p>
<p>Furthermore, we disentangled the sub-processes of integration with high-temporal ERPs, when representations of gesture and speech were variously presented. Early P1-N1 and P2 sensory effects linked to perception and attentional processes<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup> was comprehended as a reflection of the early audiovisual gesture-speech integration in the sensory-perceptual processing chain<sup><xref ref-type="bibr" rid="c49">49</xref></sup>. Note that a semantic priming paradigm was adopted here to create a top- down prediction of gesture over speech. The observed positive correlation of the P1 effect with speech entropy and the negative correlation of the N1-P2 effect with gesture entropy suggest that the early interaction of gesture-speech information was modulated by both top- down gesture prediction and bottom-up speech processing. Additionally, the lexico-semantic effect of the N400 and the LPC were differentially mediated by top-down gesture prediction, bottom-up speech encoding and their interaction: the N400 was negatively correlated with both the gesture entropy and MI, but the LPC was negatively correlated only with the speech entropy.</p>
<p>The varying contributions of unisensory gesture-speech information and the convergence of multisensory inputs, as reflected in the correlation between distinct ERP components and TMS time windows (TMS TWs), are consistent with recent models suggesting that multisensory processing involves parallel detection of modality-specific information and hierarchical integration across multiple neural levels<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c50">50</xref></sup>. These processes are further characterized by coordination across multiple temporal scales<sup><xref ref-type="bibr" rid="c51">51</xref></sup>. Building on this, the present study offers additional evidence that the multi-level nature of gesture-speech processing is statistically structured, as measured by information matrix of unisensory entropy and multisensory convergence index of MI, the input of either source would activate a distributed representation, resulting in progressively functioning neural responses.</p>
<p>Given that control processes are intrinsically integrated with semantic processing<sup><xref ref-type="bibr" rid="c52">52</xref></sup>, a distributed semantic representation enables dynamic modulation of access to and manipulation of meaningful information, thereby facilitating flexible control over the diverse possibilities inherent in a concept. Accordingly, an increased number of candidate responses amplifies the control demands necessary to resolve competing semantic representations. This effect was observed in the present study, where the association of the information matrix with the tDCS effect in IFG, the inhibition of pMTG activity in TW2, disruption of IFG activity in TW6, and modulation of four distinct ERP components collectively demonstrated that response quantity modulates neural activity. These results underscore the intricate interplay between the difficulty of semantic representation and the control pressures that shape the resulting neural responses.</p>
<p>The IFG and pMTG, central components of the semantic control network, have been extensively implicated in previous research<sup><xref ref-type="bibr" rid="c52">52</xref>–<xref ref-type="bibr" rid="c54">54</xref></sup>. While the role of the IFG in managing both unisensory information and multisensory convergence remains consistent, as evidenced by the confounding difficulty results across Experiments 1 and 2, the current study highlights a more context-dependent function for the pMTG. Specifically, although the pMTG is well- established in the processing of distributed speech information, the multisensory convergence, indexed by MI, did not evoke the same control-related modulation in pMTG activity. These findings suggest that, while the pMTG is critical to semantic processing, its engagement in control processes is likely modulated by the specific nature of the sensory inputs involved.</p>
<p>Considering the close alignment of the ERP components with the TWs of TMS effect, it is reasonable to speculate the ERP components with the cortical involvements (<xref rid="fig6" ref-type="fig">Figure 6</xref>). Consequently, referencing the recurrent neurocircuit connecting the left IFG and pMTG for semantic unification<sup><xref ref-type="bibr" rid="c55">55</xref></sup>, we extended the previously proposed two-stage gesture-speech integration circuit<sup><xref ref-type="bibr" rid="c23">23</xref></sup> into sequential steps. First, bottom-up speech processing mapping acoustic signal to its lexical representation was performed to the pMTG. The larger speech entropy was, the greater effort was made during the matching of the acoustic input with its stored lexical representation, thus leading to a larger involvement of the pMTG at pre-lexical stage (TW2) and a larger P1 effect (<xref rid="fig6" ref-type="fig">Figure 6①</xref>). Second, the gesture representation was activated in the pMTG and further exerted a top-down modulation over the phonological processing of speech<sup><xref ref-type="bibr" rid="c56">56</xref></sup>. The higher the certainty of gesture is, a larger modulation of gesture would be made upon speech, as indexed by a smaller gesture entropy with an enhanced N1-P2 amplitude (<xref rid="fig6" ref-type="fig">Figure 6②</xref>). Third, information was relayed from the pMTG to the IFG for sustained activation, during which a semantic constraint from gesture has been made on the semantic retrieval of speech. Greater TMS inhibitory effect over the IFG at post-lexical stage (TW6) accompanying with a reduced N400 amplitude were found with the increase of gesture entropy, when the representation of gesture was wildly distributed and the constrain over the following speech was weak (<xref rid="fig6" ref-type="fig">Figure 6③</xref>). Fourth, the activated speech representation was compared with that of the gesture in the IFG. At this stage, the larger, overlapped neural populations activated by gesture and speech as indexed by a larger MI is, a greater TMS disruption effect of the IFG and a reduced N400 amplitude indexing easier integration and less semantic conflict were observed (<xref rid="fig6" ref-type="fig">Figure 6④</xref>). Last, the activated speech representation would disambiguate and reanalyze the semantic information and further unify into a coherent comprehension in the pMTG<sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c39">39</xref></sup>. As speech entropy increases, indicating greater uncertainty in the information provided by speech, more cognitive effort is directed towards selecting the targeted semantic representation. This leads to enhanced involvement of the IFG and a corresponding reduction in LPC amplitude (<xref rid="fig6" ref-type="fig">Figure 6⑤</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Progressive processing stages of gesture–speech information within the pMTG-IFG loop.</title><p>Correlations between the TMS disruption effect of pMTG and IFG with three information models are represented by the orange line and the green lines, respectively. Black lines denote the strongest correlations of ROI averaged ERP components with three information models. * p &lt; 0.05, ** p &lt; 0.01 after FDR correction.</p></caption>
<graphic xlink:href="517759v5_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Note that the sequential cortical involvement and ERP components discussed above are derived from a deliberate alignment of speech onset with gesture DP, creating an artificial priming effect with gesture semantically preceding speech. Caution is advised when generalizing these findings to the spontaneous gesture-speech relationships, although gestures naturally precede speech<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. Furthermore, MI quantifies overlap in gesture-speech integration, primarily when gestures convey redundant meaning. Consequently, the conclusions drawn in this study are constrained to contexts in which gestures serve to reinforce the meaning of the speech. Future research should aim to explore the neural responses in cases where gestures convey supplementary, rather than redundant, semantic information.</p>
<p>Limitations exist. ERP components and cortical engagements were linked through intermediary variables of entropy and MI. Dissociations were observed between ERP components and cortical engagement. Importantly, there is no direct evidence of the brain structures underpinning the corresponding ERPs, necessitating clarification in future studies. Additionally, not all influenced TWs exhibited significant associations with entropy and MI. While HD-tDCS and TMS may impact functionally and anatomically connected brain regions<sup><xref ref-type="bibr" rid="c57">57</xref>,<xref ref-type="bibr" rid="c58">58</xref></sup>, whether the absence of influence in certain TWs can be attributed to compensation by other connected brain areas, such as angular gyrus<sup><xref ref-type="bibr" rid="c59">59</xref></sup> or anterior temporal lobe<sup><xref ref-type="bibr" rid="c60">60</xref></sup>, warrants further investigation. Therefore, caution is needed when interpreting the causal relationship between inhibition effects of brain stimulation and information-theoretic metrics (entropy and MI). Finally, the current study incorporated a restricted set of entropy and MI measures. The generalizability of the findings should be assessed in future studies using a more extensive range of matrices.</p>
<p>In summary, utilizing information-theoretic complexity metrics such as entropy and mutual information (MI), our study demonstrates that multisensory semantic processing, involving gesture and speech, gives rise to dynamically evolving representations through the interplay between gesture-primed prediction and speech presentation. This process correlates with the progressive engagement of the pMTG-IFG-pMTG circuit and various ERP components. These findings significantly advancing our understanding of the neural mechanisms underlying multisensory semantic integration.</p>
</sec>

</body>
<back>
<sec>
<table-wrap id="tbla1" orientation="portrait" position="float">
<label>Appendix Table 1.</label>
<caption><title>Gesture description and paring with incongruent and congruent speech.</title></caption>
<graphic xlink:href="517759v5_tbla1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p><bold>Appendix Table 2. Examples of ‘an4 (press)’ for the calculation of speech entropy, gesture entropy and mutual information (MI)</bold></p>
<table-wrap id="tbl2a" orientation="portrait" position="float">
<label>Table 2A:</label>
<caption><title>Calculation of speech entropy for ‘an4.wav (press)’</title></caption>
<graphic xlink:href="517759v5_tbl2a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2b" orientation="portrait" position="float">
<label>Table 2B:</label>
<caption><title>Calculation of gesture entropy for ‘an4.avi (press)’</title></caption>
<graphic xlink:href="517759v5_tbl2b.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2c" orientation="portrait" position="float">
<label>Table 2C:</label>
<caption><title>Calculation of MI for ‘an4.avi (press) + an4.wav (press)</title></caption>
<graphic xlink:href="517759v5_tbl2c.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbla3" orientation="portrait" position="float">
<label>Appendix Table 3.</label>
<caption><title>Quantitative information for each stimulus.</title></caption>
<graphic xlink:href="517759v5_tbla3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p><bold>Appendix Table 4. Raw RT of semantic congruent (Sc) and semantic incongruent (Si) in Experiment 1 and Experiment 2.</bold></p>
<table-wrap id="tbl4a" orientation="portrait" position="float">
<label>Table 4A:</label>
<caption><title>RT of Sc and Si in three HD-tDCS stimulation conditions for IFG and pMTG</title></caption>
<graphic xlink:href="517759v5_tbl4a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl4b" orientation="portrait" position="float">
<label>Table 4B:</label>
<caption><title>RT of Sc and Si in each time window (TW) for IFG, pMTG, and Vertex</title></caption>
<graphic xlink:href="517759v5_tbl4b.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<ack>
<title>Acknowledgments</title>
<p>This research was supported by grants from the STI 2030—Major Projects 2021ZD0201500, the National Natural Science Foundation of China (31822024, 31800964), the Scientific Foundation of Institute of Psychology, Chinese Academy of Sciences (E2CX3625CX), and the Strategic Priority Research Program of Chinese Academy of Sciences (XDB32010300).</p>
</ack>
<sec id="d1e1833" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Author contributions</title>
<p>Conceptualization, W.Y.Z. and Y.D.; Investigation, W.Y.Z. and Z.Y.L.; Formal Analysis, W.Y.Z. and Z.Y.L.; Methodology, W.Y.Z. and Z.Y.L.; Validation, Z.Y.L. and X.L.; Visualization, W.Y.Z. and Z.Y.L. and X.L.; Funding Acquisition, W.Y.Z. and Y.D.; Supervision, Y.D.; Project administration, Y.D.; Writing – Original Draft, W.Y.Z.; Writing – Review &amp; Editing, W.Y.Z., Z.Y.L., X.L., and Y.D.</p>
</sec>
</sec>
<sec id="suppd1e1833" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1824">
<label>Appendix Table 1-4</label>
<media xlink:href="supplements/517759_file02.docx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Damasio</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Grabowski</surname>, <given-names>T.J.</given-names></string-name>, <string-name><surname>Tranel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hichwa</surname>, <given-names>R.D.</given-names></string-name>, and <string-name><surname>Damasio</surname>, <given-names>A.R</given-names></string-name></person-group>. (<year>1996</year>). <article-title>A neural basis for lexical retrieval</article-title>. <source>Nature</source> <volume>380</volume>, <fpage>499</fpage>–<lpage>505</lpage>. DOI <pub-id pub-id-type="doi">10.1038/380499a0</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patterson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Nestor</surname>, <given-names>P.J.</given-names></string-name>, and <string-name><surname>Rogers</surname>, <given-names>T.T</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Where do you know what you know? The representation of semantic knowledge in the human brain</article-title>. <source>Nature Reviews Neuroscience</source> <volume>8</volume>, <fpage>976</fpage>–<lpage>987</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2277</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brennan</surname>, <given-names>J.R.</given-names></string-name>, <string-name><surname>Stabler</surname>, <given-names>E.P.</given-names></string-name>, <string-name><surname>Van Wagenen</surname>, <given-names>S.E.</given-names></string-name>, <string-name><surname>Luh</surname>, <given-names>W.M.</given-names></string-name>, and <string-name><surname>Hale</surname>, <given-names>J.T.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Abstract linguistic structure correlates with temporal activity during naturalistic comprehension</article-title>. <source>Brain and Language</source> <volume>157</volume>, <fpage>81</fpage>–<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2016.04.008</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benetti</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ferrari</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Pavani</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Multimodal processing in face-to-face interactions: A bridging link between psycholinguistics and sensory neuroscience</article-title>. <source>Front Hum Neurosci</source> <volume>17</volume>, <issue>1108354</issue>. <pub-id pub-id-type="doi">10.3389/fnhum.2023.1108354</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noppeney</surname>, <given-names>U</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Perceptual Inference, Learning, and Attention in a Multisensory World</article-title>. <source>Annual Review of Neuroscience</source>, Vol <volume>44</volume>, 2021 44, <fpage>449</fpage>–<lpage>473</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-100120-085519</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname>, <given-names>W.J.</given-names></string-name>, and <string-name><surname>Jazayeri</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Neural coding of uncertainty and probability</article-title>. <source>Annu Rev Neurosci</source> <volume>37</volume>, <fpage>205</fpage>–<lpage>220</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-071013-014017</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischer</surname>, <given-names>B.J.</given-names></string-name>, and <string-name><surname>Pena</surname>, <given-names>J.L</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Owl’s behavior and neural representation predicted by Bayesian inference</article-title>. <source>Nat Neurosci</source> <volume>14</volume>, <fpage>1061</fpage>–<lpage>1066</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2872</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ganguli</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Simoncelli</surname>, <given-names>E.P</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Efficient sensory encoding and Bayesian inference with heterogeneous neural populations</article-title>. <source>Neural Comput</source> <volume>26</volume>, <fpage>2103</fpage>–<lpage>2134</lpage>. <pub-id pub-id-type="doi">10.1162/NECO_a_00638</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hostetter</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Mainela-Arnold</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Gestures occur with spatial and Motoric knowledge: It’s more than just coincidence</article-title>. <source>Perspectives on Language Learning and Education</source> <volume>22</volume>, <fpage>42</fpage>–<lpage>49</lpage>. doi:<pub-id pub-id-type="doi">10.1044/lle22.2.42</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McNeill</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2005</year>). <source>Gesture and though</source> (<publisher-name>University of Chicago Press</publisher-name>). <pub-id pub-id-type="doi">10.7208/chicago/9780226514642.001.0001</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kendon</surname>, <given-names>A</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Gesture</article-title>. <source>Annu Rev Anthropol</source> <volume>26</volume>, <fpage>109</fpage>–<lpage>128</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.anthro.26.1.109</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2005</year>). <article-title>On broca, brain, and binding: a new framework</article-title>. <source>Trends in Cognitive Sciences</source> <volume>9</volume>, <fpage>416</fpage>–<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2005.07.004</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hald</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bastiaansen</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Petersson</surname>, <given-names>K.M</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Integration of word meaning and world knowledge in language comprehension</article-title>. <source>Science</source> <volume>304</volume>, <fpage>438</fpage>–<lpage>441</lpage>. <pub-id pub-id-type="doi">10.1126/science.1095455</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Willems</surname>, <given-names>R.M.</given-names></string-name>, <string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2007</year>). <article-title>On-line integration of semantic information from speech and gesture: Insights from event-related brain potentials</article-title>. <source>J Cognitive Neurosci</source> <volume>19</volume>, <fpage>605</fpage>–<lpage>616</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2007.19.4.605</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Willems</surname>, <given-names>R.M.</given-names></string-name>, <string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Differential roles for left inferior frontal and superior temporal cortex in multimodal integration of action and language</article-title>. <source>Neuroimage</source> <volume>47</volume>, <fpage>1992</fpage>–<lpage>2004</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.05.066</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Drijvers</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Jensen</surname>, <given-names>O.</given-names></string-name>, and <string-name><surname>Spaak</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Rapid invisible frequency tagging reveals nonlinear integration of auditory and visual information</article-title>. <source>Human Brain Mapping</source> <volume>42</volume>, <fpage>1138</fpage>–<lpage>1152</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.25282</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Drijvers</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Ozyurek</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Native language status of the listener modulates the neural integration of speech and iconic gestures in clear and adverse listening conditions</article-title>. <source>Brain and Language</source> <volume>177</volume>, <fpage>7</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2018.01.003</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Drijvers</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>van der Plas</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Jensen</surname>, <given-names>O</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Native and non- native listeners show similar yet distinct oscillatory dynamics when using gestures to access speech in noise</article-title>. <source>Neuroimage</source> <volume>194</volume>, <fpage>55</fpage>–<lpage>67</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.032</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holle</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Gunter</surname>, <given-names>T.C</given-names></string-name></person-group>. (<year>2007</year>). <article-title>The role of iconic gestures in speech disambiguation: ERP evidence</article-title>. <source>J Cognitive Neurosci</source> <volume>19</volume>, <fpage>1175</fpage>–<lpage>1192</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2007.19.7.1175</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Ozyurek</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2003</year>). <article-title>What does cross-linguistic variation in semantic coordination of speech and gesture reveal?: Evidence for an interface representation of spatial thinking and speaking</article-title>. <source>J Mem Lang</source> <volume>48</volume>, <fpage>16</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1016/S0749-596x(02)00505-3</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bernardis</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Gentilucci</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Speech and gesture share the same communication system</article-title>. <source>Neuropsychologia</source> <volume>44</volume>, <fpage>178</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.05.007</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname>, <given-names>W.Y.</given-names></string-name>, <string-name><surname>Riggs</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schindler</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Holle</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Transcranial magnetic stimulation over left inferior frontal and posterior temporal cortex disrupts gesture- speech integration</article-title>. <source>Journal of Neuroscience</source> <volume>38</volume>, <fpage>1891</fpage>–<lpage>1900</lpage>. <pub-id pub-id-type="doi">10.1523/Jneurosci.1748-17.2017</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Du</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2021</year>). <article-title>TMS reveals dynamic interaction between inferior frontal gyrus and posterior middle temporal gyrus in gesture-speech semantic integration</article-title>. <source>The Journal of Neuroscience</source>, <fpage>10356</fpage>–<lpage>10364</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.1355-21.2021</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shannon</surname>, <given-names>C.E</given-names></string-name></person-group>. (<year>1948</year>). <article-title>A mathematical theory of communication</article-title>. <source>Bell Syst Tech J</source> <volume>27</volume>, <fpage>379</fpage>–<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tremblay</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Deschamps</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Baroni</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Hasson</surname>, <given-names>U</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Neural sensitivity to syllable frequency and mutual information in speech perception and production</article-title>. <source>Neuroimage</source> <volume>136</volume>, <fpage>106</fpage>–<lpage>121</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.05.018</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bikson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Inoue</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Akiyama</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Deans</surname>, <given-names>J.K.</given-names></string-name>, <string-name><surname>Fox</surname>, <given-names>J.E.</given-names></string-name>, <string-name><surname>Miyakawa</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Jefferys</surname>, <given-names>J.G.R</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Effects of uniform extracellular DC electric fields on excitability in rat hippocampal slices</article-title>. <source>J Physiol-London</source> <volume>557</volume>, <fpage>175</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.2003.055772</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paulus</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Rothwell</surname>, <given-names>J.C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Membrane resistance and shunting inhibition: where biophysics meets state-dependent human neurophysiology</article-title>. <source>J Physiol-London</source> <volume>594</volume>, <fpage>2719</fpage>–<lpage>2728</lpage>. <pub-id pub-id-type="doi">10.1113/Jp271452</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Obermeier</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Gunter</surname>, <given-names>T.C</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Multisensory integration: The case of a time window of gesture-speech integration</article-title>. <source>J Cognitive Neurosci</source> <volume>27</volume>, <fpage>292</fpage>–<lpage>307</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00688</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Federmeier</surname>, <given-names>K.D.</given-names></string-name>, <string-name><surname>Mai</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Kutas</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Both sides get the point: hemispheric sensitivities to sentential constraint</article-title>. <source>Memory &amp; Cognition</source> <volume>33</volume>, <fpage>871</fpage>–<lpage>886</lpage>. <pub-id pub-id-type="doi">10.3758/bf03193082</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kelly</surname>, <given-names>S.D.</given-names></string-name>, <string-name><surname>Kravitz</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Hopkins</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Neural correlates of bimodal speech and gesture comprehension</article-title>. <source>Brain and Language</source> <volume>89</volume>, <fpage>253</fpage>–<lpage>260</lpage>. <pub-id pub-id-type="doi">10.1016/s0093-934x(03)00335-3</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname>, <given-names>Y.C.</given-names></string-name>, and <string-name><surname>Coulson</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Meaningful gestures: Electrophysiological indices of iconic gesture comprehension</article-title>. <source>Psychophysiology</source> <volume>42</volume>, <fpage>654</fpage>–<lpage>667</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.2005.00356.x</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fritz</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Littlemore</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Krott</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Multimodal language processing: How preceding discourse constrains gesture interpretation and affects gesture integration when gestures do not synchronise with semantic affiliates</article-title>. <source>J Mem Lang</source> <volume>117</volume>, <issue>104191</issue>. <pub-id pub-id-type="doi">10.1016/j.jml.2020.104191</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gunter</surname>, <given-names>T.C.</given-names></string-name>, and <string-name><surname>Weinbrenner</surname>, <given-names>J.E.D</given-names></string-name></person-group>. (<year>2017</year>). <article-title>When to take a gesture seriously: On how we use and prioritize communicative cues</article-title>. <source>J Cognitive Neurosci</source> <volume>29</volume>, <fpage>1355</fpage>–<lpage>1367</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01125</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oldfield</surname>, <given-names>R.C</given-names></string-name></person-group>. (<year>1971</year>). <article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title>. <source>Neuropsychologia</source> <volume>9</volume>, <fpage>97</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname>, <given-names>W</given-names></string-name></person-group>. (<year>2023</year>). <article-title>TMS reveals a two-stage priming circuit of gesture-speech integration</article-title>. <source>Front Psychol</source> <volume>14</volume>, <issue>1156087</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2023.1156087</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McNeilli</surname>, <given-names>D</given-names></string-name></person-group>. (<year>1992</year>). <source>Hand and mind : what gestures reveal about thought</source> (<publisher-name>University of Chicago Press</publisher-name>)..</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kelly</surname>, <given-names>S.D.</given-names></string-name>, <string-name><surname>Creigh</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Bartolotti</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Integrating speech and iconic gestures in a Stroop-like task: Evidence for automatic processing</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>22</volume>, <fpage>683</fpage>–<lpage>694</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2009.21254</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koessler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Maillard</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Benhadid</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Vignal</surname>, <given-names>J.P.</given-names></string-name>, <string-name><surname>Felblinger</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Vespignani</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Braun</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Automated cortical projection of EEG sensors: Anatomical correlation via the international 10-10 system</article-title>. <source>Neuroimage</source> <volume>46</volume>, <fpage>64</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.02.006</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tesink</surname>, <given-names>C.M.J.Y.</given-names></string-name>, <string-name><surname>Petersson</surname>, <given-names>K.M.</given-names></string-name>, <string-name><surname>van Berkum</surname>, <given-names>J.J.A.</given-names></string-name>, <string-name><surname>van den Brink</surname>, <given-names>D</given-names></string-name>., <string-name><surname>Buitelaar</surname>, <given-names>J.K.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Unification of speaker and meaning in language comprehension: An fMRI study</article-title>. <source>J Cognitive Neurosci</source> <volume>21</volume>, <fpage>2085</fpage>–<lpage>2099</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2008.21161</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nuwer</surname>, <given-names>M.R.</given-names></string-name>, <string-name><surname>Comi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Emerson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fuglsang-Frederiksen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Guerit</surname>, <given-names>J.M.</given-names></string-name>, <string-name><surname>Hinrichs</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ikeda</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Luccas</surname>, <given-names>F.J.</given-names></string-name>, and <string-name><surname>Rappelsberger</surname>, <given-names>P</given-names></string-name></person-group>. (<year>1999</year>). <article-title>IFCN standards for digital recording of clinical EEG. The International Federation of Clinical Neurophysiology</article-title>. <source>Electroencephalogr Clin Neurophysiol Suppl</source> <volume>52</volume>, <fpage>11</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1016/S00134694(97)00106-5</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delorme</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Makeig</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2004</year>). <article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source>J Neurosci Methods</source> <volume>134</volume>, <fpage>9</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Habets</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Shao</surname>, <given-names>Z.S.</given-names></string-name>, <string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2011</year>). <article-title>The Role of Synchrony and Ambiguity in Speech-Gesture Integration during Comprehension</article-title>. <source>J Cognitive Neurosci</source> <volume>23</volume>, <fpage>1845</fpage>–<lpage>1854</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2010.21462</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Schoffelen</surname>, <given-names>J.-M</given-names></string-name></person-group>. (<year>2011</year>). <article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source>Computational Intelligence and Neuroscience</source> <volume>2011</volume>, <issue>156869</issue>. <pub-id pub-id-type="doi">10.1155/2011/156869</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kutas</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Federmeier</surname>, <given-names>K.D</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Thirty Years and Counting: Finding Meaning in the N400 Component of the Event-Related Brain Potential (ERP)</article-title>. <source>Annual Review of Psychology</source>, Vol <volume>62</volume> <issue>62</issue>, <fpage>621</fpage>–<lpage>647</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rogers</surname>, <given-names>T.T.</given-names></string-name>, <string-name><surname>Ralph</surname>, <given-names>M.A.L.</given-names></string-name>, <string-name><surname>Garrard</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bozeat</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>McClelland</surname>, <given-names>J.L.</given-names></string-name>, <string-name><surname>Hodges</surname>, <given-names>J.R.</given-names></string-name>, and <string-name><surname>Patterson</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Structure and deterioration of semantic memory: A neuropsychological and computational investigation</article-title>. <source>Psychological Review</source> <volume>111</volume>, <fpage>205</fpage>–<lpage>235</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295x.111.1.205</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ralph</surname>, <given-names>M.A.L.</given-names></string-name>, <string-name><surname>Jefferies</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Patterson</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Rogers</surname>, <given-names>T.T</given-names></string-name></person-group>. (<year>2017</year>). <article-title>The neural and computational bases of semantic cognition</article-title>. <source>Nature Reviews Neuroscience</source> <volume>18</volume>, <fpage>42</fpage>–<lpage>55</lpage>. <pub-id pub-id-type="doi">10.1038/nrn.2016.150</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rogers</surname>, <given-names>T.T.</given-names></string-name>, <string-name><surname>Hodges</surname>, <given-names>J.R.</given-names></string-name>, <string-name><surname>Ralph</surname>, <given-names>M.A.L.</given-names></string-name>, and <string-name><surname>Patterson</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Object recognition under semantic impairment: The effects of conceptual regularities on perceptual decisions</article-title>. <source>Lang Cognitive Proc</source> <volume>18</volume>, <fpage>625</fpage>–<lpage>662</lpage>. <pub-id pub-id-type="doi">10.1080/01690960344000053</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fadiga</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Craighero</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Olivier</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Human motor cortex excitability during the perception of others’ action</article-title>. <source>Current Opinion in Neurobiology</source> <volume>15</volume>, <fpage>213</fpage>–<lpage>218</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2005.03.013</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giard</surname>, <given-names>M.H.</given-names></string-name>, and <string-name><surname>Peronnet</surname>, <given-names>F</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Auditory-visual integration during multimodal object recognition in humans: A behavioral and electrophysiological study</article-title>. <source>J Cognitive Neurosci</source> <volume>11</volume>, <fpage>473</fpage>–<lpage>490</lpage>. <pub-id pub-id-type="doi">10.1162/089892999563544</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meijer</surname>, <given-names>G.T.</given-names></string-name>, <string-name><surname>Mertens</surname>, <given-names>P.E.C.</given-names></string-name>, <string-name><surname>Pennartz</surname>, <given-names>C.M.A.</given-names></string-name>, <string-name><surname>Olcese</surname>, <given-names>U.</given-names></string-name>, and <string-name><surname>Lansink</surname>, <given-names>C.S</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The circuit architecture of cortical multisensory processing: Distinct functions jointly operating within a common anatomical network</article-title>. <source>Prog Neurobiol</source> <volume>174</volume>, <fpage>1</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1016/j.pneurobio.2019.01.004</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Senkowski</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Engel</surname>, <given-names>A.K</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Multi-timescale neural dynamics for multisensory integration</article-title>. <source>Nat Rev Neurosci</source> <volume>25</volume>, <fpage>625</fpage>–<lpage>642</lpage>. <pub-id pub-id-type="doi">10.1038/s41583-024-00845-7</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jackson</surname>, <given-names>R.L</given-names></string-name></person-group>. (<year>2021</year>). <article-title>The neural correlates of semantic control revisited</article-title>. <source>Neuroimage</source> <volume>224</volume>, <issue>117444</issue>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117444</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jefferies</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2013</year>). <article-title>The neural basis of semantic cognition: converging evidence from neuropsychology, neuroimaging and TMS</article-title>. <source>Cortex</source> <volume>49</volume>, <fpage>611</fpage>–<lpage>625</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2012.10.008</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noonan</surname>, <given-names>K.A.</given-names></string-name>, <string-name><surname>Jefferies</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Visser</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Lambon Ralph</surname>, <given-names>M.A</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Going beyond inferior prefrontal involvement in semantic control: evidence for the additional contribution of dorsal angular gyrus and posterior middle temporal cortex</article-title>. <source>J Cogn Neurosci</source> <volume>25</volume>, <fpage>1824</fpage>–<lpage>1850</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00442</pub-id>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2013</year>). <article-title>MUC (Memory, Unification, Control) and beyond</article-title>. <source>Frontiers in Psychology</source> <volume>4</volume>, <issue>416</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00416</pub-id>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bizley</surname>, <given-names>J.K.</given-names></string-name>, <string-name><surname>Maddox</surname>, <given-names>R.K.</given-names></string-name>, and <string-name><surname>Lee</surname>, <given-names>A.K.C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Defining auditory-visual objects: Behavioral tests and physiological mechanisms</article-title>. <source>Trends in Neurosciences</source> <volume>39</volume>, <fpage>74</fpage>–<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2015.12.007</pub-id>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hartwigsen</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Bzdok</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Klein</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wawrzyniak</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Stockert</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wrede</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Classen</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Saur</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Rapid short-term reorganization in the language network</article-title>. <source>eLife</source> <volume>6</volume>. <pub-id pub-id-type="doi">10.7554/eLife.25964</pub-id>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jackson</surname>, <given-names>R.L.</given-names></string-name>, <string-name><surname>Hoffman</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Pobric</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Ralph</surname>, <given-names>M.A.L</given-names></string-name></person-group>. (<year>2016</year>). <article-title>The semantic network at work and rest: Differential connectivity of anterior temporal lobe subregions</article-title>. <source>Journal of Neuroscience</source> <volume>36</volume>, <fpage>1490</fpage>–<lpage>1501</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2999-15.2016</pub-id>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Humphreys</surname>, <given-names>G.F.</given-names></string-name>, <string-name><surname>Ralph</surname>, <given-names>M.A.L.</given-names></string-name>, and <string-name><surname>Simons</surname>, <given-names>J.S</given-names></string-name></person-group>. (<year>2021</year>). <article-title>A Unifying Account of Angular Gyrus Contributions to Episodic and Semantic Cognition</article-title>. <source>Trends in Neurosciences</source> <volume>44</volume>, <fpage>452</fpage>–<lpage>463</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2021.01.006</pub-id>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonner</surname>, <given-names>M.F.</given-names></string-name>, and <string-name><surname>Price</surname>, <given-names>A.R</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Where Is the Anterior Temporal Lobe and What Does It Do?</article-title> <source>Journal of Neuroscience</source> <volume>33</volume>, <fpage>4213</fpage>–<lpage>4215</lpage>. <pub-id pub-id-type="doi">10.1523/Jneurosci.0041-13.2013</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.3.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>useful</bold> study uses brain stimulation and electroencephalography to study speech-gesture integration. It investigates the role of frontotemporal regions in integrating linguistic and extra-linguistic information during communication, focusing on the inferior frontal gyrus and posterior middle temporal gyrus. Reliance on activation patterns of tightly-coupled brain regions over short timescales leads to <bold>incomplete</bold> support for the study's conclusions due to conceptual and methodological limitations.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.3.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors quantified information in gesture and speech, and investigated the neural processing of speech and gestures in pMTG and LIFG, depending on their informational content, in 8 different time-windows, and using three different methods (EEG, HD-tDCS and TMS). They found that there is a time-sensitive and staged progression of neural engagement that is correlated with the informational content of the signal (speech/gesture).</p>
<p>Strengths:</p>
<p>A strength of the paper is that the authors attempted to combine three different methods to investigate speech-gesture processing.</p>
<p>Comments on revisions:</p>
<p>I thank the authors for their careful responses to my comments. However, I remain not convinced by their argumentation regarding the specificity of their spatial targeting and the time-windows that they used.</p>
<p>I do not believe the authors have adequately demonstrated the spatial and temporal specificity required to disentangle the contributions of the IFG and pMTG during the gesture-speech integration process. While the authors have made a sincere effort to address the concerns raised by the reviewers, and have done so with a lot of new analyses, I remain doubtful that the current methodological approach is sufficient to draw conclusions about the causal roles of the IFG and pMTG in gesture-speech integration.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.3.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>The study is an innovative and fundamental study that clarified important aspects of brain processes for integration of information from speech and iconic gesture (i.e., gesture that depicts action, movement, and shape), based on tDCS, TMS and EEG experiments. They evaluated their speech and gesture stimuli in information-theoretic ways and calculated how informative speech is (i.e., entropy), how informative gesture is, and how much shared information speech and gesture encode. The tDCS and TMS studies found that the left IFG and pMTG, the two areas that were activated in fMRI studies on speech-gesture integration in the previous literature, are causally implicated in speech-gesture integration. The size of tDC and TMS effects are correlated with entropy of the stimuli or mutual information, which indicates that the effects stems from the modulation of information decoding/integration processes. The EEG study showed that various ERP (event-related potential, e.g., N1-P2, N400, LPC) effects that have been observed in speech-gesture integration experiments in the previous literature are modulated by the entropy of speech/gesture and mutual information. This makes it clear that these effects are related to information decoding processes. The authors propose a model of how speech-gesture integration process unfolds in time, and how IFG and pMTG interact with each other in that process.</p>
<p>Strengths:</p>
<p>The key strength of this study is that the authors used information-theoretic measures of their stimuli (i.e., entropy and mutual information between speech and gesture) in all of their analyses. This made it clear that the neuro-modulation (tDCS, TMS) affected information decoding/integration and ERP effects reflect information decoding/integration. This study used tDCS and TMS methods to demonstrate that left IFG and pMTG are causally involved in speech-gesture integration. The size of tDCS and TMS effects are correlated with information-theoretic measures of the stimuli, which indicate that the effects indeed stem from disruption/facilitation of information decoding/integration process (rather than generic excitation/inhibition). The authors' results also showed correlation between information-theoretic measures of stimuli with various ERP effects. This indicates that these ERP effects reflect the information decoding/integration process.</p>
<p>Weaknesses:</p>
<p>The &quot;mutual information&quot; cannot capture all types of interplay of the meaning of speech and gesture. The mutual information is calculated based on what information can be decoded from speech alone and what information can be decoded from gesture alone. However, when speech and gesture are combined, a novel meaning can emerge, which cannot be decoded from a single modality alone. When example, a person produce a gesture of writing something with a pen, while saying &quot;He paid&quot;. The speech-gesture combination can be interpreted as &quot;paying by signing a cheque&quot;. It is highly unlikely that this meaning is decoded when people hear speech only or see gestures only. The current study cannot address how such speech-gesture integration occur in the brain, and what ERP effects may reflect such a process. The future studies can classify different types of speech-gesture integration and investigate neural processes that underlie each type. Another important topic for future studies is to investigate how the neural processes of speech-gesture integration change when the relative timing between the speech stimulus and the gesture stimulus changes.</p>
<p>Comments on the previous round of revisions: The authors addressed my concerns well.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.3.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhao</surname>
<given-names>Wanying</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6979-940X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Zhouyi</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Xiang</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Du</surname>
<given-names>Yi</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4512-5221</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the previous reviews</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1:</bold></p>
<p>Comment:</p>
<p>The authors quantified information in gesture and speech, and investigated the neural processing of speech and gestures in pMTG and LIFG, depending on their informational content, in 8 different time-windows, and using three different methods (EEG, HD-tDCS and TMS). They found that there is a time-sensitive and staged progression of neural engagement that is correlated with the informational content of the signal (speech/gesture).</p>
<p>Strengths:</p>
<p>A strength of the paper is that the authors attempted to combine three different methods to investigate speech-gesture processing.</p>
</disp-quote>
<p>We sincerely appreciate the reviewer’s recognition of our efforts in employing a multi-method approach, which integrates three complementary experimental paradigms, each leveraging distinct neurophysiological techniques to provide converging evidence.</p>
<p>In Experiment 1, we found that the degree of inhibition in the pMTG and LIFG was strongly associated with the overlap in gesture-speech representations, as quantified by mutual information. Experiment 2 revealed the time-sensitive dynamics of the pMTG-LIFG circuit in processing both unisensory (gesture or speech) and multisensory information. Experiment 3, utilizing high-temporal-resolution EEG, independently replicated the temporal dynamics of gesture-speech integration observed in Experiment 2, further validating our findings.</p>
<p>The striking convergence across these methodologically independent approaches significantly bolsters the robustness and generalizability of our conclusions regarding the neural mechanisms underlying multisensory integration.</p>
<disp-quote content-type="editor-comment">
<p>Comment 1: I thank the authors for their careful responses to my comments. However, I remain not convinced by their argumentation regarding the specificity of their spatial targeting and the time-windows that they used.</p>
<p>The authors write that since they included a sham TMS condition, that the TMS selectively disrupted the IFG-pMTG interaction during specific time windows of the task related to gesture-speech semantic congruency. This to me does not show anything about the specificity of the time-windows itself, nor the selectivity of targeting in the TMS condition.</p>
</disp-quote>
<p>(1) Selection of brain regions (IFG/pMTG)</p>
<p>We thank the reviewer for their thoughtful consideration. The choice of the left IFG and pMTG as regions of interest (ROIs) was informed by a meta-analysis of fMRI studies on gesture-speech integration, which consistently identified these regions as critical hubs (see Author response table 1 for detailed studies and coordinates).</p>
<table-wrap id="sa3table1">
<label>Author response table 1.</label>
<caption>
<title>Meta-analysis of previous studies on gesture-speech integration.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-table1.jpg" mimetype="image"/>
</table-wrap>
<p>Based on the meta-analysis of previous studies, we selected the IFG and pMTG as ROIs for gesture-speech integration. The rationale for selecting these brain regions is outlined in the introduction in Lines 63-66: “Empirical studies have investigated the semantic integration between gesture and speech by manipulating their semantic relationship[15-18] and revealed a mutual interaction between them19-21 as reflected by the N400 latency and amplitude14 as well as common neural underpinnings in the left inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG)[15,22,23].”</p>
<p>And further described in Lines 77-78: “Experiment 1 employed high-definition transcranial direct current stimulation (HD-tDCS) to administer Anodal, Cathodal and Sham stimulation to either the IFG or the pMTG”. And Lines 85-88: ‘Given the differential involvement of the IFG and pMTG in gesture-speech integration, shaped by top-down gesture predictions and bottom-up speech processing [23], Experiment 2 was designed to assess whether the activity of these regions was associated with relevant informational matrices”.</p>
<p>In the Methods section, we clarified the selection of coordinates in Lines 194-200: “Building on a meta-analysis of prior fMRI studies examining gesture-speech integration[22], we targeted Montreal Neurological Institute (MNI) coordinates for the left IFG at (-62, 16, 22) and the pMTG at (-50, -56, 10). In the stimulation protocol for HD-tDCS, the IFG was targeted using electrode F7 as the optimal cortical projection site[36], with four return electrodes placed at AF7, FC5, F9, and FT9. For the pMTG, TP7 was selected as the cortical projection site[36], with return electrodes positioned at C5, P5, T9, and P9.”</p>
<p>The selection of IFG or pMTG as integration hubs for gesture and speech has also been validated in our previous studies. Specifically, Zhao et al. (2018, J. Neurosci) applied TMS to both areas. Results demonstrated that disrupting neural activity in the IFG or pMTG via TMS selectively impaired the semantic congruency effect (reaction time costs due to semantic incongruence), while leaving the gender congruency effect unaffected.</p>
<p>These findings identified the IFG and pMTG as crucial hubs for gesture-speech integration, guiding the selection of brain regions for our subsequent studies.</p>
<p>(2) Selection of time windows</p>
<p>The five key time windows (TWs) analyzed in this study were derived from our previous TMS work (Zhao et al., 2021, J. Neurosci), where we segmented the gesture-speech integration period (0–320 ms post-speech onset) into eight 40-ms windows. This interval aligns with established literature on gesture-speech integration, particularly the 200–300 ms window noted by the reviewer. As detailed in Lines (776-779): “Procedure of Experiment 2. Eight time windows (TWs, duration = 40 ms) were segmented in relative to the speech IP. Among the eight TWs, five (TW1, TW2, TW3, TW6, and TW7) were chosen based on the significant results in our prior study[23]. Double-pulse TMS was delivered over each of the TW of either the pMTG or the IFG”.</p>
<p>In our prior work (Zhao et al., 2021, J. Neurosci), we employed a carefully controlled experimental design incorporating two key factors: (1) gesture-speech semantic congruency (serving as our primary measure of integration) and (2) gesture-speech gender congruency (implemented as a matched control factor). Using a time-locked, double-pulse TMS protocol, we systematically targeted each of the eight predefined time windows (TWs) within the left IFG, left pMTG, or vertex (serving as a sham control condition). Our results demonstrated that a TW-selective disruption of gesture-speech integration, indexed by the semantic congruency effect (i.e., a cost of reaction time because of semantic conflict), when stimulating the left pMTG in TW1, TW2, and TW7 but when stimulating the left IFG in TW3 and TW6. Crucially, no significant effects were observed during either sham stimulation or the controlled gender congruency factor (Figure 3 from Zhao et al., 2021, J. Neurosci).</p>
<p>This triple dissociation - showing effects only for semantic integration, only in active stimulation, and only at specific time points - provides compelling causal evidence that IFG-pMTG connectivity plays a temporally precise role in gesture-speech integration.</p>
<p>Noted that this work has undergone rigorous peer review by two independent experts who both endorsed our methodological approach. Their original evaluations, provided below:</p>
<p>Reviewer 1: “significance: Using chronometric TMS-stimulation the data of this experiment suggests a feedforward information flow from left pMTG to left IFG followed by an information flow from left IFG back to the left pMTG.  The study is the first to provide causal evidence for the temporal dynamics of the left pMTG and left IFG found during gesture-speech integration.”</p>
<p>Reviewer 2: “Beyond the new results the manuscript provides regarding the chronometrical interaction of the left inferior frontal gyrus and middle temporal gyrus in gesture-speech interaction, the study more basically shows the possibility of unfolding temporal stages of cognitive processing within domain-specific cortical networks using short-time interval double-pulse TMS. Although this method also has its limitations, a careful study planning as shown here and an appropiate discussion of the results can provide unique insights into cognitive processing.”</p>
<p>References:</p>
<p>Willems, R.M., Ozyurek, A., and Hagoort, P. (2009). Differential roles for left inferior frontal and superior temporal cortex in multimodal integration of action and language. Neuroimage <italic>47</italic>, 1992-2004. 10.1016/j.neuroimage.2009.05.066.</p>
<p>Drijvers, L., Jensen, O., and Spaak, E. (2021). Rapid invisible frequency tagging reveals nonlinear integration of auditory and visual information. Human Brain Mapping <italic>42</italic>, 1138-1152. 10.1002/hbm.25282.</p>
<p>Drijvers, L., and Ozyurek, A. (2018). Native language status of the listener modulates the neural integration of speech and iconic gestures in clear and adverse listening conditions. Brain and Language <italic>177</italic>, 7-17. 10.1016/j.bandl.2018.01.003.</p>
<p>Drijvers, L., van der Plas, M., Ozyurek, A., and Jensen, O. (2019). Native and non-native listeners show similar yet distinct oscillatory dynamics when using gestures to access speech in noise. Neuroimage <italic>194</italic>, 55-67. 10.1016/j.neuroimage.2019.03.032.</p>
<p>Holle, H., and Gunter, T.C. (2007). The role of iconic gestures in speech disambiguation: ERP evidence. J Cognitive Neurosci <italic>19</italic>, 1175-1192. 10.1162/jocn.2007.19.7.1175.</p>
<p>Kita, S., and Ozyurek, A. (2003). What does cross-linguistic variation in semantic coordination of speech and gesture reveal?: Evidence for an interface representation of spatial thinking and speaking. J Mem Lang <italic>48</italic>, 16-32. 10.1016/S0749-596x(02)00505-3.</p>
<p>Bernardis, P., and Gentilucci, M. (2006). Speech and gesture share the same communication system. Neuropsychologia <italic>44</italic>, 178-190. 10.1016/j.neuropsychologia.2005.05.007.</p>
<p>Zhao, W.Y., Riggs, K., Schindler, I., and Holle, H. (2018). Transcranial magnetic stimulation over left inferior frontal and posterior temporal cortex disrupts gesture-speech integration. Journal of Neuroscience <italic>38</italic>, 1891-1900. 10.1523/Jneurosci.1748-17.2017.</p>
<p>Zhao, W., Li, Y., and Du, Y. (2021). TMS reveals dynamic interaction between inferior frontal gyrus and posterior middle temporal gyrus in gesture-speech semantic integration. The Journal of Neuroscience, 10356-10364. 10.1523/jneurosci.1355-21.2021.</p>
<p>Hartwigsen, G., Bzdok, D., Klein, M., Wawrzyniak, M., Stockert, A., Wrede, K., Classen, J., and Saur, D. (2017). Rapid short-term reorganization in the language network. Elife <italic>6</italic>. 10.7554/eLife.25964.</p>
<p>Jackson, R.L., Hoffman, P., Pobric, G., and Ralph, M.A.L. (2016). The semantic network at work and rest: Differential connectivity of anterior temporal lobe subregions. Journal of Neuroscience <italic>36</italic>, 1490-1501. 10.1523/JNEUROSCI.2999-15.2016.</p>
<p>Humphreys, G. F., Lambon Ralph, M. A., &amp; Simons, J. S. (2021). A Unifying Account of Angular Gyrus Contributions to Episodic and Semantic Cognition. Trends in neurosciences, 44(6), 452–463. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2021.01.006">https://doi.org/10.1016/j.tins.2021.01.006</ext-link></p>
<p>Bonner, M. F., &amp; Price, A. R. (2013). Where is the anterior temporal lobe and what does it do?. The Journal of neuroscience : the official journal of the Society for Neuroscience, 33(10), 4213–4215. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0041-13.2013">https://doi.org/10.1523/JNEUROSCI.0041-13.2013</ext-link></p>
<disp-quote content-type="editor-comment">
<p>Comment 2: It could still equally well be the case that other regions or networks relevant for gesture-speech integration are targeted, and it can still be the case that these timewindows are not specific, and effects bleed into other time periods. There seems to be no experimental evidence here that this is not the case.</p>
</disp-quote>
<p>The selection of IFG and pMTG as regions of interest was rigorously justified through multiple lines of evidence. First, a comprehensive meta-analysis of fMRI studies on gesture-speech integration consistently identified these regions as central nodes (see response to comment 1). Second, our own previous work (Zhao et al., 2018, JN; 2021, JN) provided direct empirical validation of their involvement. Third, by employing the same experimental paradigm, we minimized the likelihood of engaging alternative networks. Fourth, even if other regions connected to IFG or pMTG might be affected by TMS, the distinct engagement of specific time windows of IFG and pMTG minimizes the likelihood of consistent influence from other regions.</p>
<p>Regarding temporal specificity, our 2021 study (Zhao et al., 2021, JN, see details in response to comment 1) systematically examined the entire 0-320ms integration window and found that only select time windows showed significant effects for gesture-speech semantic congruency, while remaining unaffected during gender congruency processing. This double dissociation (significant effects for semantic integration but not gender processing in specific windows) rules out broad temporal spillover.</p>
<disp-quote content-type="editor-comment">
<p>Comment 3: To be more specific, the authors write that double-pulse TMS has been widely used in previous studies (as found in their table). However, the studies cited in the table do not necessarily demonstrate the level of spatial and temporal specificity required to disentangle the contributions of tightly-coupled brain regions like the IFG and pMTG during the speech-gesture integration process. pMTG and IFG are located in very close proximity, and are known to be functionally and structurally interconnected, something that is not necessarily the case for the relatively large and/or anatomically distinct areas that the authors mention in their table.</p>
</disp-quote>
<p>Our methodological approach is strongly supported by an established body of research employing double-pulse TMS (dpTMS) to investigate neural dynamics across both primary motor and higher-order cognitive regions. As documented in Author response table 1, multiple studies have successfully applied this technique to: (1) primary motor areas (tongue and lip representations in M1), and (2) semantic processing regions (including pMTG, PFC, and ATL). Particularly relevant precedents include:</p>
<p>(1) Teige et al. (2018, Cortex): Demonstrated precise spatial and temporal specificity by applying 40ms-interval dpTMS to ATL, pMTG, and mid-MTG across multiple time windows (0-40ms, 125-165ms, 250-290ms, 450-490ms), revealing distinct functional contributions from ATL versus pMTG.</p>
<p>(2) Vernet et al. (2015, Cortex): Successfully dissociated functional contributions of right IPS and DLPFC using 40ms-interval dpTMS, despite their anatomical proximity and functional connectivity.</p>
<p>These studies confirm double-pulse TMS can discriminate interconnected nodes at short timescales. Our 2021 study further validated this for IFG-pMTG.</p>
<table-wrap id="sa3table2">
<label>Author response table 2.</label>
<caption>
<title>Double-pulse TMS studies on brain regions over 3-60 ms time interval</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-table2.jpg" mimetype="image"/>
</table-wrap>
<p>References:</p>
<p>Teige, C., Mollo, G., Millman, R., Savill, N., Smallwood, J., Cornelissen, P. L., &amp; Jefferies, E. (2018). Dynamic semantic cognition: Characterising coherent and controlled conceptual retrieval through time using magnetoencephalography and chronometric transcranial magnetic stimulation. Cortex, 103, 329-349.</p>
<p>Vernet, M., Brem, A. K., Farzan, F., &amp; Pascual-Leone, A. (2015). Synchronous and opposite roles of the parietal and prefrontal cortices in bistable perception: a double-coil TMS–EEG study. Cortex, 64, 78-88.</p>
<disp-quote content-type="editor-comment">
<p>Comment 4: But also more in general: The mere fact that these methods have been used in other contexts does not necessarily mean they are appropriate or sufficient for investigating the current research question. Likewise, the cognitive processes involved in these studies are quite different from the complex, multimodal integration of gesture and speech. The authors have not provided a strong theoretical justification for why the temporal dynamics observed in these previous studies should generalize to the specific mechanisms of gesture-speech integration..</p>
</disp-quote>
<p>The neurophysiological mechanisms underlying double-pulse TMS (dpTMS) are well-characterized. While it is established that single-pulse TMS can produce brief artifacts (typically within 0–10 ms) due to transient cortical depolarization (Romero et al., 2019, NC), the dynamics of double-pulse TMS (dpTMS) involve more intricate inhibitory interactions. Specifically, the first pulse increases membrane conductance via GABAergic shunting inhibition, effectively lowering membrane resistance and attenuating the excitatory impact of the second pulse. This results in a measurable reduction in cortical excitability at the paired-pulse interval, as evidenced by suppressed motor evoked potentials (MEPs) (Paulus &amp; Rothwell, 2016, J Physiol). Importantly, this neurophysiological mechanism is independent of cognitive domain and has been robustly demonstrated across multiple functional paradigms.</p>
<p>In our study, we did not rely on previously reported timing parameters but instead employed a dpTMS protocol using a 40-ms inter-pulse interval. Based on the inhibitory dynamics of this protocol, we designed a sliding temporal window sufficiently broad to encompass the integration period of interest. This approach enabled us to capture and localize the critical temporal window associated with ongoing integrative processing in the targeted brain region.</p>
<p>We acknowledge that the previous phrasing may have been ambiguous, a clearer and more detailed description of the dpTMS protocol has now been provided in Lines 88-92: “To this end, we employed chronometric double-pulse transcranial magnetic stimulation, which is known to transiently reduce cortical excitability at the inter-pulse interval]27]. Within a temporal period broad enough to capture the full duration of gesture–speech integration[28], we targeted specific timepoints previously implicated in integrative processing within IFG and pMTG [23].”</p>
<p>References:</p>
<p>Romero, M.C., Davare, M., Armendariz, M. et al. Neural effects of transcranial magnetic stimulation at the single-cell level. Nat Commun 10, 2642 (2019). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-019-10638-7">https://doi.org/10.1038/s41467-019-10638-7</ext-link></p>
<p>Paulus W, Rothwell JC. Membrane resistance and shunting inhibition: where biophysics meets state-dependent human neurophysiology. J Physiol. 2016 May 15;594(10):2719-28. doi: 10.1113/JP271452. PMID: 26940751; PMCID: PMC4865581.</p>
<p>Obermeier, C., &amp; Gunter, T. C. (2015). Multisensory Integration: The Case of a Time Window of Gesture-Speech Integration. Journal of Cognitive Neuroscience, 27(2), 292-307. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00688">https://doi.org/10.1162/jocn_a_00688</ext-link></p>
<disp-quote content-type="editor-comment">
<p>Comment 5: Moreover, the studies cited in the table provided by the authors have used a wide range of interpulse intervals, from 20 ms to 100 ms, suggesting that the temporal precision required to capture the dynamics of gesture-speech integration (which is believed to occur within 200-300 ms; Obermeier &amp; Gunter, 2015) may not even be achievable with their 40 ms time windows.</p>
</disp-quote>
<p>Double-pulse TMS has been empirically validated across neurocognitive studies as an effective method for establishing causal temporal relationships in cortical networks, with demonstrated sensitivity at timescales spanning 3-60 m. Our selection of a 40-ms interpulse interval represents an optimal compromise between temporal precision and physiological feasibility, as evidenced by its successful application in dissociating functional contributions of interconnected regions including ATL/pMTG (Teige et al., 2018) and IPS/DLPFC (Vernet et al., 2015). This methodological approach combines established experimental rigor with demonstrated empirical validity for investigating the precisely timed IFG-pMTG dynamics underlying gesture-speech integration, as shown in our current findings and prior work (Zhao et al., 2021).</p>
<p>Our experimental design comprehensively sampled the 0-320 ms post-stimulus period, fully encompassing the critical 200-300 ms window associated with gesture-speech integration, as raised by the reviewer. Notably, our results revealed temporally distinct causal dynamics within this period: the significantly reduced semantic congruency effect emerged at IFG at 200-240ms, followed by feedback projections from IFG to pMTG at 240-280ms. This precisely timed interaction provides direct neurophysiological evidence for the proposed architecture of gesture-speech integration, demonstrating how these interconnected regions sequentially contribute to multisensory semantic integration.</p>
<disp-quote content-type="editor-comment">
<p>Comment 6: I do appreciate the extra analyses that the authors mention. However, my 5th comment is still unanswered: why not use entropy scores as a continous measure?</p>
</disp-quote>
<p>Analysis with MI and entropy as continuous variables were conducted employing Representational Similarity Analysis (RSA) (Popal et.al, 2019). This analysis aimed to build a model to predict neural responses based on these feature metrics.</p>
<p>To capture dynamic temporal features indicative of different stages of multisensory integration, we segmented the EEG data into overlapping time windows (40 ms in duration with a 10 ms step size). The 40 ms window was chosen based on the TMS protocol used in Experiment 2, which also employed a 40 ms time window. The 10 ms step size (equivalent to 5 time points) was used to detect subtle shifts in neural responses that might not be captured by larger time windows, allowing for a more granular analysis of the temporal dynamics of neural activity.</p>
<p>Following segmentation, the EEG data were reshaped into a four-dimensional matrix (42 channels × 20 time points × 97 time windows × 20 features). To construct a neural similarity matrix, we averaged the EEG data across time points within each channel and each time window. The resulting matrix was then processed using the pdist function to compute pairwise distances between adjacent data points. This allowed us to calculate correlations between the neural matrix and three feature similarity matrices, which were constructed in a similar manner. These three matrices corresponded to (1) gesture entropy, (2) speech entropy, and (3) mutual information (MI). This approach enabled us to quantify how well the neural responses corresponded to the semantic dimensions of gesture and speech stimuli at each time window.</p>
<p>To determine the significance of the correlations between neural activity and feature matrices, we conducted 1000 permutation tests. In this procedure, we randomized the data or feature matrices and recalculated the correlations repeatedly, generating a null distribution against which the observed correlation values were compared. Statistical significance was determined if the observed correlation exceeded the null distribution threshold (p &lt; 0.05). This permutation approach helps mitigate the risk of spurious correlations, ensuring that the relationships between the neural data and feature matrices are both robust and meaningful.</p>
<p>Finally, significant correlations were subjected to clustering analysis, which grouped similar neural response patterns across time windows and channels. This clustering allowed us to identify temporal and spatial patterns in the neural data that consistently aligned with the semantic features of gesture and speech stimuli, thus revealing the dynamic integration of these multisensory modalities across time. Results are as follows:</p>
<p>(1)  Two significant clusters were identified for gesture entropy (Figure 1 left). The first cluster was observed between 60-110 ms (channels F1 and F3), with correlation coefficients (r) ranging from 0.207 to 0.236 (<italic>p</italic> &lt; 0.001). The second cluster was found between 210-280 ms (channel O1), with r-values ranging from 0.244 to 0.313 (<italic>p</italic> &lt; 0.001).</p>
<p>(2)  For speech entropy (Figure 1 middle), significant clusters were detected in both early and late time windows. In the early time windows, the largest significant cluster was found between 10-170 ms (channels F2, F4, F6, FC2, FC4, FC6, C4, C6, CP4, and CP6), with r-values ranging from 0.151 to 0.340 (p = 0.013), corresponding to the P1 component (0-100 ms). In the late time windows, the largest significant cluster was observed between 560-920 ms (across the whole brain, all channels), with r-values ranging from 0.152 to 0.619 (p = 0.013).</p>
<p>(3)  For mutual information (MI) (Figure 1 right), a significant cluster was found between 270-380 ms (channels FC1, FC2, FC3, FC5, C1, C2, C3, C5, CP1, CP2, CP3, CP5, FCz, Cz, and CPz), with r-values ranging from 0.198 to 0.372 (p = 0.001).</p>
<fig id="sa3fig1">
<label>Author response image 1.</label>
<caption>
<title>Results of RSA analysis.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-fig1.jpg" mimetype="image"/>
</fig>
<p>These additional findings suggest that even using a different modeling approach, neural responses, as indexed by feature metrics of entropy and mutual information, are temporally aligned with distinct ERP components and ERP clusters, as reported in the current manuscript. This alignment serves to further consolidate the results, reinforcing the conclusion we draw. Considering the length of the manuscript, we did not include these results in the current manuscript.</p>
<p>Reference:</p>
<p>Popal, H., Wang, Y., &amp; Olson, I. R. (2019). A guide to representational similarity analysis for social neuroscience. Social cognitive and affective neuroscience, 14(11), 1243-1253.</p>
<disp-quote content-type="editor-comment">
<p>Comment 7: In light of these concerns, I do not believe the authors have adequately demonstrated the spatial and temporal specificity required to disentangle the contributions of the IFG and pMTG during the gesture-speech integration process. While the authors have made a sincere effort to address the concerns raised by the reviewers, and have done so with a lot of new analyses, I remain doubtful that the current methodological approach is sufficient to draw conclusions about the causal roles of the IFG and pMTG in gesture-speech integration.</p>
</disp-quote>
<p>To sum up:</p>
<p>(1) Empirical validation from our prior work (Zhao et al., 2018,2021,JN): The selection of IFG and pMTG as target regions was informed by both: (1) a comprehensive meta-analysis of fMRI studies on gesture-speech integration, and (2) our own prior causal evidence from Zhao et al. (2018, J Neurosci), with detailed stereotactic coordinates provided in the attached Response to Editors and Reviewers letter. The temporal parameters were similarly grounded in empirical data from Zhao et al. (2021, J Neurosci), where we systematically examined eight consecutive 40-ms windows spanning the full integration period (0-320 ms). This study revealed a triple dissociation of effects - occurring exclusively during: (i)semantic integration (but not control tasks), (ii) active stimulation (but not sham), and (iii) specific time windows (but not all time windows)- providing robust causal evidence for the spatiotemporal specificity of IFG-pMTG interactions in gesture-speech processing. Notably, all reviewers recognized the methodological strength of this dpTMS approach in their evaluations (see attached JN assessment for details).</p>
<p>(2) Convergent evidence from Experiment 3: Our study employed a multi-method approach incorporating three complementary experimental paradigms, each utilizing distinct neurophysiological techniques to provide converging evidence. Specifically, Experiment 3 implemented high-temporal-resolution EEG, which independently replicated the time-sensitive dynamics of gesture-speech integration observed in our double-pulse TMS experiments. The remarkable convergence between these methodologically independent approaches -demonstrating consistent temporal staging of IFG-pMTG interactions across both causal (TMS) and correlational (EEG) measures - significantly strengthens the validity and generalizability of our conclusions regarding the neural mechanisms underlying multisensory integration.</p>
<p>(3) Established precedents in double-pulse TMS literature: The double-pulse TMS methodology employed in our study is firmly grounded in established neuroscience research. As documented in our detailed Response to Editors and Reviewers letter (citing 11 representative studies), dpTMS has been extensively validated for investigating causal temporal dynamics in cortical networks, with demonstrated sensitivity at timescales ranging from 3-60 ms. Particularly relevant precedents include: 1. Teige et al. (2018, Cortex) successfully dissociated functional contributions of anatomically proximal regions (ATL vs. pMTG vs.mid-MTG) using 40-ms-interval double-pulse TMS; 2. Vernet et al. (2015, Cortex) effectively distinguished neural processing in interconnected frontoparietal regions (right IPS vs. DLPFC) using 40-ms double-pulse TMS parameters. Both parameters are identical to those employed in our current study.</p>
<p>(4) Neurophysiological Plausibility: The neurophysiological basis for the transient double-pulse TMS effects is well-established through mechanistic studies of TMS-induced cortical inhibition (Romero et al.,2019; Paulus &amp; Rothwell, 2016).</p>
<p>Taking together, we respectfully submit that our methodology provides robust support for our conclusions.</p>
</body>
</sub-article>
</article>