<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">87197</article-id>
<article-id pub-id-type="doi">10.7554/eLife.87197</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.87197.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Human Attention during Goal-directed Reading Comprehension Relies on Task Optimization</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zou</surname>
<given-names>Jiajie</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2744-1490</contrib-id>
<name>
<surname>Zhang</surname>
<given-names>Yuran</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Jialu</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1629-6304</contrib-id>
<name>
<surname>Tian</surname>
<given-names>Xing</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Ding</surname>
<given-names>Nai</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Sciences, Zhejiang University</institution>, Hangzhou 310027, <country>China</country></aff>
<aff id="a2"><label>2</label><institution>Research Center for Applied Mathematics and Machine Intelligence, Research Institute of Basic Theories</institution>, Zhejiang lab, Hangzhou 311121, <institution>China</institution></aff>
<aff id="a3"><label>3</label><institution>Division of Arts and Sciences, New York University Shanghai</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Zhang</surname>
<given-names>Hang</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>de Lange</surname>
<given-names>Floris P</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Donders Institute for Brain, Cognition and Behaviour</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>Corresponding author:</bold> Nai Ding, Email: <email>ding_nai@zju.edu.cn</email>, College of Biomedical Engineering and Instrument Sciences, Zhejiang University, Hangzhou 310027, China</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-06-23">
<day>23</day>
<month>06</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2023-10-31">
<day>31</day>
<month>10</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP87197</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-04-16">
<day>16</day>
<month>04</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-04-25">
<day>25</day>
<month>04</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.04.25.538252"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-06-23">
<day>23</day>
<month>06</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87197.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.87197.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.87197.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.87197.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.87197.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Zou et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Zou et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-87197-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>The computational principles underlying attention allocation in complex goal-directed tasks remain elusive. Goal-directed reading, i.e., reading a passage to answer a question in mind, is a common real-world task that strongly engages attention. Here, we investigate what computational models can explain attention distribution in this complex task. We show that the reading time on each word is predicted by the attention weights in transformer-based deep neural networks (DNNs) optimized to perform the same reading task. Eye-tracking further reveals that readers separately attend to basic text features and question-relevant information during first-pass reading and rereading, respectively. Similarly, text features and question relevance separately modulate attention weights in shallow and deep DNN layers. Furthermore, when readers scan a passage without a question in mind, their reading time is predicted by DNNs optimized for a word prediction task. Therefore, we offer a computational account of how task optimization modulates attention distribution during real-world reading.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>computational neuroscience</kwd>
<kwd>reading comprehension</kwd>
<kwd>eye movements</kwd>
<kwd>deep neural network</kwd>
<kwd>attention</kwd>
</kwd-group>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>1. To dissociate the effect of model architecture and the effect of training data, we have now compared the attention weights across three transformer-based models that have the same architecture but different training data/task: randomized (with all model parameters being randomized), pre-trained, and fine-tuned models.
2. We have now analyzed eye movements of each participant and used the linear mixed effects model to test how different factors affected human word reading time to account for participants-level and item-level variances.
3. We have now expanded our discussion to encompass a more comprehensive comparison between the current study and a recent similar work (Hahn and Keller, Cognition, 2023). Additionally, we have delved deeper into the potential reasons underlying the similarity between human attention and attention weights of DNN models.
4. We have now characterized how different reading measures, e.g., gaze duration and counts or rereading, were affected by text and task-related features in Experiments 2-4.
5. Supplemental files updated.
</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/jiajiezou/TOA">https://github.com/jiajiezou/TOA</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Attention profoundly influences information processing in the brain [<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref>], and a large number of studies have been devoted to studying the neural mechanisms of attention. From the perspective of David Marr, the attention mechanism can be studied from three levels, i.e., the computational, algorithmic, and implementational levels [<xref ref-type="bibr" rid="c4">4</xref>]. At the computational level, attention is traditionally viewed as a mechanism to allocate limited central processing resources [<xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c9">9</xref>]. More recent studies, however, propose that attention is a mechanism to optimize task performance, even in conditions where the processing resource is not clearly constrained [<xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c14">14</xref>]. The optimization hypothesis can explain the attention distribution in a range of well controlled learning and decision-making tasks [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>], but is rarely tested in complex processing tasks for which the optimal strategy is not obvious. Therefore, the computational principles that underlie the allocation of human attention during complex tasks remain elusive. Nevertheless, complex tasks are critical conditions to test whether the attention mechanisms abstracted from simpler tasks can truly explain real-world attention behaviors.</p>
<p>Reading is one of the most common and most sophisticated human behaviors [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c17">17</xref>], and it is strongly regulated by attention: Since readers can only recognize a couple of words within one fixation, they have to overtly shift their fixation to read a line of text [<xref ref-type="bibr" rid="c3">3</xref>]. Thus, eye movements serve as an overt expression of attention allocation during reading [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c18">18</xref>]. Computational modeling of the eye movements has mostly focused on normal reading of single sentences. At the computational level, it has been proposed that the eye movements are programed to, e.g., minimize the number of eye movements [<xref ref-type="bibr" rid="c12">12</xref>]. At the algorithmic and implementational level, models such as the E-Z reader [<xref ref-type="bibr" rid="c19">19</xref>] can accurately predict the eye movement trajectory with high temporal and spatial resolution. Everyday reading behavior, however, often engages reading of a multi-line passage and generally has a clear goal, e.g., information retrieval or inference generation [<xref ref-type="bibr" rid="c20">20</xref>]. Few models, however, have considered how the reading goal modulates reading behaviors. Here, we address this question by analyzing how readers allocate attention when reading a passage to answer a specific question in mind. The question may require, e.g., information retrieval, inference generation, or text summarization (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). We investigate whether the task optimization hypothesis can explain the attention distribution in such goal-directed reading tasks.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>Experiment and performance.</title>
<p>(<bold>A</bold>) Experimental procedure for Experiments 1-3. In each trial, participants saw a question before reading a passage. After reading the passage, they chose the answer to the question from 4 options. (<bold>B</bold>) Accuracy of question answering for humans and computational models. The question type is color coded and an example question is shown for each type. trans_pre: pre-trained transformer-based models; trans_fine: transformer-based models fine-tuned on the goal-directed reading task. (<bold>C</bold>) Time spent on reading each passage. The box plot shows the mean (horizontal lines inside the box), 25<sup>th</sup> and 75<sup>th</sup> percentiles (box boundaries), and 25<sup>th</sup>/75<sup>th</sup> percentiles ±1.5 ×interquartile range (whiskers). (<bold>D</bold>) Illustration of the training process for transformer-based models. The pre-training process aims to learn general statistical regularities in a language based on large corpora, while the fine-tuning process trains models to perform the reading comprehension task.</p></caption>
<graphic xlink:href="538252v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Finding an optimal solution for the goal-directed reading task, however, is computationally challenging since the information related to question answering is sparsely located in a passage and their orthographic forms may not be predictable. Recent advances in DNN models, however, provide a potential tool to solve this computational problem, since DNN models equipped with attention mechanisms have approached and even surpassed mean human performance on goal-directed reading tasks [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>]. Attention in DNN also functions as a mechanism to selectively extract useful information, and therefore attention may potentially serve a conceptually similar role in DNN. Furthermore, recent studies have provided strong evidence that task-optimized DNN can indeed explain the neural response properties in a range of visual and language processing tasks [<xref ref-type="bibr" rid="c23">23</xref>–<xref ref-type="bibr" rid="c30">30</xref>]. Therefore, although the DNN attention mechanism certainly deviates from the human attention mechanism in terms of its algorithms and implementation, we employ it to probe the computational-level principle underlying human attention distribution during real-world goal-directed reading.</p>
<p>Here, we investigated what computational principles could generate human-like attention distribution during a goal-directed reading task. We employed DNNs to derive a set of attention weights that are optimized for the goal-directed reading task, and tested whether such optimal weights could explain human attention measured by eye tracking. Furthermore, since both human and DNN processing is hierarchical, we also investigated whether the human attention distribution during different processing stages, which are characterized through different eye tracking measures, and the DNN attention weights in different layers may be differentially influenced by visual features, text properties, and the top-down task. Additionally, we recruited both native and non-native readers to probe how language proficiency contributed to the computational optimality of attention distribution.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Experiment 1: Task and Performance</title>
<p>In Experiment 1, the participants (<italic>N</italic> = 25 for each question) first read a question and then read a passage based on which the question should be answered (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). After reading the passage, the participants chose from 4 options which option was the most suitable answer to the question. In total, 800 question/passage pairs were adapted from the RACE dataset [<xref ref-type="bibr" rid="c31">31</xref>], a collection of reading comprehension questions designed for Chinese high school students who learn English as a second language. The questions fell into 6 types (<xref rid="fig1" ref-type="fig">Fig. 1BC</xref>): Three types of questions required attention to details, e.g., retrieving a fact or generate inference based on a fact, which were referred to as local questions. The other 3 types of questions concerned the general understanding of a passage, e.g., summarizing the main idea or identifying the purpose of writing, which were referred to as global questions. None of the question directly appeared in the passage, and the longest string that overlapped in the passage and question was 1.8 ±1.5 words on average.</p>
<p>Participants in Experiment 1 were Chinese college or graduate students who had relatively high English proficiency. The participants correctly answered 77.94% questions on average and the accuracy was comparable across the 6 types of questions (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). We employed computational models to analyze what kinds of computations were required to answer the questions. The simplest heuristic model chose the option that best matched the passage orthographically (<xref rid="figs1" ref-type="fig">Fig. S1A</xref>). This orthographic model achieved 25.6% accuracy (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). Another simple heuristic model only considered word-level semantic matching between the passage and option, and achieved 27.3% accuracy (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). The low accuracy of the two models indicated that the reading comprehension questions could not be answered by word-level orthographic or semantic matching.</p>
<p>Next, we evaluated the performance of 4 context-dependent DNN models, i.e., Stanford Attentive Reader (SAR) [<xref ref-type="bibr" rid="c32">32</xref>], BERT [<xref ref-type="bibr" rid="c33">33</xref>], ALBERT [<xref ref-type="bibr" rid="c21">21</xref>], and RoBERTa [<xref ref-type="bibr" rid="c22">22</xref>], which could integrate information across words to build passage-level semantic representations. The SAR used the bi-directional recurrent neural network (RNN) to integrate contextual information (<xref rid="figs1" ref-type="fig">Fig. S1B</xref>) and achieved 47.6% accuracy. The other 3 models, i.e., BERT, ALBERT, and RoBERTa, were transformer-based models that were trained in 2 steps, i.e., pre-training and fine-tuning (<xref rid="fig1" ref-type="fig">Fig. 1D</xref>). Since the 3 models had similar structures, we averaged the performance over the 3 models (see <xref rid="figs2" ref-type="fig">Fig. S2</xref> for the results of individual models). The model performance on the reading task was 37.08% and 73%, respectively, after pre-training and fine-tuning (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>).</p>
</sec>
<sec id="s2b">
<title>Computational Models of Human Attention Distribution</title>
<p>In Experiment 1, participants were allowed to read each passage for 2 minutes. Nevertheless, to encourage the participants to develop an effective reading strategy, the monetary reward the participant received decreased as they spent more time reading the passage (see <italic>Materials and Methods</italic> for details). The results showed that the participants spent, on average, 0.7 ±0.2 minutes reading each passage (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>), corresponding to a reading speed of 457 ±142 words/minute when divided by the number of words per passage. The speed was almost twice the normal reading speed for native readers [<xref ref-type="bibr" rid="c3">3</xref>], indicating a specialized reading strategy for the task.</p>
<p>Next, we employed eye tracking to quantify how the readers allocated their attention to achieve effective reading and analyze which computational models could explain the reading time on each word, i.e., the total fixation duration on each word during passage reading. In other words, we probed into what kind of computational principles could generate human-like attention distribution during goal-directed reading. A simple heuristic strategy was to attend to words that were orthographically or semantically similar to the words in the question (<xref rid="figs1" ref-type="fig">Fig. S1A</xref>). The predictions of the heuristic models were not highly correlated with the human word reading time, and the predictive power, i.e., the Pearson correlation coefficient between the predicted and real word reading time, was around 0.2 (<xref rid="figs3" ref-type="fig">Fig. S3A</xref>).</p>
<p>The DNN models analyzed here, i.e., SAR, BERT, ALBERT, and RoBERTa, all employed the attention mechanism to integrate over context to find optimal question answering strategies. Roughly speaking, the attention mechanism applied a weighted integration across all input words to generate a passage-level representation and decide whether an option was correct or not, and the weight on each word was referred to as the attention weight (see <xref rid="figs1" ref-type="fig">Fig. S1B</xref> and <xref rid="fig2" ref-type="fig">Fig. 2B</xref> for illustrations about the attention mechanisms in the SAR and transformer-based models, respectively). When the attention weights of the SAR were used to predict the human word reading time, the predictive power was about 0.1 (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, <xref rid="tbls1" ref-type="table">Table S1</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2.</label>
<caption><title>Human attention distribution and computational models.</title>
<p>(<bold>A</bold>) Examples of human attention distribution, quantified by the word reading time. The histograms on the right showed the mean reading time on each line, for both human data and model predictions. trans_pre: pre-trained transformer-based models; trans_fine: transformer-based models fine-tuned on the goal-directed reading task. (B) The general architecture of the 12-layer transformer-based models. The model input consists of all words in the passage and an integrated option. Output of the model relies on the node CLS<sup>12</sup>, which is used to calculate a score reflecting how likely an option is the correct answer. The CLS node is a weighted sum of the vectorial representations of all words and tokens, and the attention weight for each word in the passage, i.e., <italic>α</italic>, is the DNN attention analyzed in this study.</p></caption>
<graphic xlink:href="538252v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>Model word reading time in Experiment 1.</title>
<p>(<bold>AB</bold>) Predict the word reading time based on the attention weights of DNN models, text features, or question relevance. The predictive power is the correlation coefficient between the predicted word reading time and the actual word reading time. Predictive power significantly higher than chance is denoted by stars on the top of each bar. **P &lt; 0.01. trans_rand: transformer-base models with randomized parameters; trans_pre: pre-trained transformer-based models; trans_fine: transformer-based models fine-tuned on the goal-directed reading task. (<bold>C</bold>) Relationship between the word reading time and line index. The word reading time is longer near the beginning of a passage and the effect is stronger for global questions than local questions. (<bold>D</bold>) Relationship between the word reading time and question relevance. Line 0 refers to the line with the highest question relevance. The word reading time is higher for the question-relevant line. Color indicates the question type. The shade area indicates one standard error of the mean (SEM) across participants.</p></caption>
<graphic xlink:href="538252v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In contrast to assigning a single weight on a word, the transformer-based model employed a multi-head attention mechanism: Each of the 12 layers had 12 parallel attention modules, i.e., heads. Consequently, each word had 144 attention weights (12 layers ×12 heads), which were used to model the word reading time of humans based on linear regression. Since the attention weights of 3 transformer-based models showed comparable power to predict human word reading time, we reported the predictive power averaged over models (see <xref rid="figs3" ref-type="fig">Fig. S3A</xref> for the results of individual models). The attention weights of randomly initialized transformer-based models could predict the human word reading time and the predictive power, which was around 0.3, was significantly higher than the chance level and the SAR (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, <xref rid="tbls1" ref-type="table">Table S1</xref>). The attention weights of pre-trained transformer-based models could also predict the human word reading time, the predictive power was around 0.5, significantly higher than the predictive power of heuristic models, the SAR, and randomly initialized transformer-based models (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, <xref rid="tbls1" ref-type="table">Table S1</xref>). The predictive power was further boosted for local but not global questions when the models were fine-tuned to perform the goal-directed reading task (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, <xref rid="tbls1" ref-type="table">Table S1</xref>). The weights assigned to attention heads in the linear regression were shown in Fig. S4. For the fine-tuned models, we also predict the human word reading time using an unweighted averaged of the 144 attention heads and the predictive power was 0.3, significantly higher than that achieved by the attention weights of SAR (P = 4 ×10<sup>-5</sup>, bootstrap). These results suggested that the human attention distribution was consistent with the attention weights in transformer-based models that were optimized to perform the same goal-directed reading task.</p>
</sec>
<sec id="s2c">
<title>Factors Influencing Human Word Reading Time</title>
<p>The attention weights in transformer-based DNN models could predict the human word reading time. Nevertheless, it remained unclear whether such predictions were purely driven by basic text features that were known to modulate word reading time. Therefore, in the following, we first analyzed how basic text features modulated the word reading time during the goal-directed reading task, and then checked whether transformer-based DNNs could capture additional properties of the word reading time that could not be explained by basic text features.</p>
<p>Here, we further decomposed text features into visual layout features, i.e., position of a word on the screen, and word features, e.g., word length, frequency, and surprisal. Layout features were features that were mostly induced by line changes, which could be extracted without recognizing the words, while word features were finer-grained features that could only be extracted when the word or neighboring words were fixated. Linear regression analyses revealed layout features could significantly predict the word reading time (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>, <xref rid="tbls2" ref-type="table">Table S2</xref>). Furthermore, the predictive power was higher for global than local questions (P = 4 ×10<sup>-5</sup>, bootstrap, FDR corrected for comparisons across 3 features, i.e., layout features, word features, and question relevance), suggesting a question-type-specific reading strategy. Word features could also significantly predict human reading time, even when the influence of layout features was regressed out. The predictive power of the layout and word features, however, was lower than the predictive power of attention weights of transformer-based models (P = 4 ×10<sup>-5</sup>, bootstrap, FDR corrected for comparisons across 2 features, i.e., layout and word features).</p>
<p>When the layout and word features were regressed out, the residual word reading time was still significantly predicted by the attention weights in transformer-based models (<xref rid="figs3" ref-type="fig">Fig. S3B</xref>, predictive power about 0.3). This result indicated that what the transformer-based models extracted were more than basic text features. Next, we analyzed whether the transformer-based models, as well as the human word reading time, were sensitive to task-related features. To characterize the relevance of each word to the question answering task, we asked another group of participants to annotate which words contributed most to question answering. The annotated question relevance could significantly predict word reading time, even when the influences of layout and word features were regressed out (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>, <xref rid="tbls2" ref-type="table">Table S2</xref>). When the question relevance was also regressed out, the residual word reading time was still significantly predicted by the attention weights in transformer-based models (<xref rid="figs3" ref-type="fig">Fig. S3C</xref>, P = 0.003, bootstrap, FDR corrected for comparisons across 12 models × 6 question types), but the predictive power dropped to about 0.2. Furthermore, a linear mixed effect model also revealed that more than 85% of the DNN attention heads contribute to the prediction of human reading time when considering text features and question relevance as covariates (<italic>Supplementary Results</italic>). These results demonstrated that the DNN attention weights provided additional information about the human word reading time than the text-related and task-related features analyzed here.</p>
<p>Further analyses revealed two properties of the distribution of question-relevant words. First, for local questions, the question-relevant words were roughly uniformly distributed in the passage, while for global questions, the question-relevant words tended to be near the passage beginning (<xref rid="figs5" ref-type="fig">Fig. S5A</xref>). The eye tracking data showed that readers also spent more time reading the passage beginning for global than local questions (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>), explaining why layout features more strongly influenced the answering of global than local questions. Second, few lines in the passage were question relevant (<xref rid="figs5" ref-type="fig">Fig. S5B</xref>), and the eye tracking data showed that readers spent more time reading the line with the highest question relevance (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>), confirming the influence of question relevance on word reading time.</p>
</sec>
<sec id="s2d">
<title>Attention in Different Processing Stages for Humans and DNNs</title>
<p>Next, we investigated whether humans and DNNs attended to different features in different processing stages. The early stage of human reading was indexed by the gaze duration, i.e., duration of first-pass reading of a word, and the later stage was indexed by the counts of rereading. Results showed the influence of layout features increased from early to late reading stages for global but not local questions (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>, <xref rid="tbls3" ref-type="table">Table S3</xref>). Consequently, the passage-beginning-effect differed between global and local questions only for the late reading stage (<xref rid="figs6" ref-type="fig">Fig. S6A</xref>). The influence of word features did not strongly change between reading stages, while the influence of question relevance significantly increased from early to late reading stages (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>, <xref rid="figs6" ref-type="fig">Fig. S6B</xref>). These results suggested that attention to basic text features developed early, while the influence of task mainly influenced late reading processes.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 4.</label>
<caption><title>Factors influencing attention distribution in different processing stages for humans and DNNs.</title>
<p>(<bold>A</bold>) Human attention in early and late reading stages is differentially modulated by text features and question relevance. The early and late stages are separately characterized by gaze duration, i.e., duration for the first reading of a word, and counts of rereading, respectively. **P &lt; 0.01; ***P &lt; 0.001. (<bold>B</bold>) DNN attention weights in different layers are also differentially modulated by text features and question relevance. Each attention head is separately modeled and averaged within each layer, and the results are further averaged across the 3 transformer-based models. Shallow layers of both fine-tuned and pre-trained models are more sensitive to text features. Deep layers of fine-tuned models are sensitive to question relevance. trans_rand: transformer-base models with randomized parameters; trans_pre: pre-trained transformer-based models; trans_fine: transformer-based models fine-tuned on the goal-directed reading task.</p></caption>
<graphic xlink:href="538252v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the following, we further investigated whether transformer-based DNN attended to different features in different layers, which represented different processing stages. This analysis did not include layout features that were not available to the models. The attention weights in shallow layers were sensitive to word features in randomized, pre-trained, and fine-tuned models (<xref rid="fig4" ref-type="fig">Fig. 4BC</xref>). Only in the fine-tuned models, however, the attention weights in deep layers were sensitive to question relevance (see <xref rid="figs7" ref-type="fig">Figs. S7</xref> &amp; <xref rid="figs8" ref-type="fig">S8</xref> for results of individual models). Therefore, the shallow and deep layers separately evolved text-based and goal-directed attention, and goal-directed attention was induced by fine-tuning on the task.</p>
</sec>
<sec id="s2e">
<title>Experiment 2: Question-Type-Specificity of the Reading Strategy</title>
<p>In Experiment 1, different types of questions were presented in blocks which encouraged the participants to develop question-type-specific reading strategies. Next, we ran Experiment 2, in which questions from different types were mixed and presented in a randomized order, to test whether the participants developed question-type-specific strategies in Experiment 1. Since it was time consuming to measure the response to all 800 questions, we randomly selected 96 questions for Experiment 2 (16 questions per type). In Experiment 2, the reading speed was on average 298 ±123 words/minute, lower than the speed in Experiment 1 (P = 6 ×10<sup>-4</sup>, bootstrap, FDR corrected for the comparisons across 4 experiments), but still much faster than normal reading speed [<xref ref-type="bibr" rid="c3">3</xref>].</p>
<p>The word reading time was better predicted by fine-tuned than pre-trained transformer-based models (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>, <xref rid="tbls4" ref-type="table">Table S4</xref>). For the influence of text and task-related features, compared to Experiment 1, the predictive power in Experiment 2 was higher for layout and word features, but lower for question relevance (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>, <xref rid="tbls5" ref-type="table">Table S5</xref>). For local questions, consistent with Experiment 1, the effects of question relevance significantly increased from early to late processing stages that are separately indexed by gaze duration and counts of rereading (<xref rid="figs9" ref-type="fig">Fig. S9A</xref>, <xref rid="tbls3" ref-type="table">Table S3</xref>). The passage beginning effect was higher for global than local questions (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>, 2<sup>nd</sup> column, P = 2 ×10<sup>-4</sup>, bootstrap, FDR corrected for the comparisons across 4 experiments), but the difference was smaller than in Experiment 1 (<xref rid="fig5" ref-type="fig">Fig. 5C</xref> &amp; <xref rid="figs10" ref-type="fig">Fig. S10A</xref>, P = 2 ×10<sup>-4</sup>, bootstrap, FDR corrected for the comparisons across 4 experiments). The question relevance effect was also smaller in Experiment 2 than Experiment 1 (<xref rid="fig5" ref-type="fig">Fig. 5D</xref> &amp; <xref rid="figs10" ref-type="fig">Fig. S10B</xref>, P = 2 ×10<sup>-4</sup>, bootstrap, FDR corrected for the comparisons across 4 experiments). All these results indicated that the readers developed question-type-specific strategies in Experiment 1, which led to faster reading speed and stronger task modulation of word reading time.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 5.</label>
<caption><title>Influence of task and language proficiency on word reading time.</title>
<p>(<bold>AB</bold>) Predict the word reading time using attention weights of DNN models, text features, and question relevance for all 4 experiments. Predictive powersignificantly higher than chance is marked by stars of the same color as the bar. Significant differences between experiments are denoted by black stars. trans_pre: pre-trained transformer-based models; trans_fine: transformer-based models fine-tuned on the goal-directed reading task. *P &lt; 0.05; **P &lt; 0.01; ***P &lt; 0.001. (<bold>CD</bold>) Passage beginning and question relevance effects for all 4 experiments. The shade area indicates one SEM across participants.</p></caption>
<graphic xlink:href="538252v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2f">
<title>Experiment 3: Effect of Language Proficiency</title>
<p>Experiments 1 and 2 recruited L2 readers. To investigate how language proficiency influenced task modulation of attention and the optimality of attention distribution, we ran Experiment 3, which was the same as Experiment 2 except that the participants were native English readers. In Experiment 3, the reading speed was on average 506 ± 155 words/minute, higher than that in Experiment 2 (P = 6 ×10<sup>-4</sup>, bootstrap, FDR corrected for the comparisons across 4 experiments). The question answering accuracy was comparable to L2 readers (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>).</p>
<p>The word reading time for native readers was slightly better predicted by fine-tuned than pre-trained transformer-based models (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>, <xref rid="tbls4" ref-type="table">Table S4</xref>). For the influence of text and task-related features, compared to Experiment 2, the predictive power in Experiment 3 was higher for word features, but lower for layout features and question relevance (<xref rid="tbls5" ref-type="table">Table S5</xref>). For local questions, the layout effect was more salient for gaze duration than for counts of rereading. In contrast, the effect of word-related features and task relevance was more salient for counts of rereading than gaze duration (<xref rid="figs9" ref-type="fig">Fig. S9B</xref>, <xref rid="tbls3" ref-type="table">Table S3</xref>). The passage beginning effect was higher for global than local questions, but the difference was smaller than in Experiment 2 (<xref rid="fig5" ref-type="fig">Fig. 5C</xref> &amp; <xref rid="figs10" ref-type="fig">Fig. S10A</xref>, P = 2 ×10<sup>-4</sup>, bootstrap, FDR corrected for the comparisons across 4 experiments). The question relevance effect was also smaller for Experiment 3 than Experiment 2 (<xref rid="fig5" ref-type="fig">Fig. 5D</xref> &amp; <xref rid="figs10" ref-type="fig">Fig. S10B</xref>, P = 2 ×10<sup>-4</sup>, bootstrap, FDR corrected for the comparisons across 4 experiments). These results showed that the word reading time of native readers was significantly modulated by the task, but the effect was weaker than that on L2 readers.</p>
</sec>
<sec id="s2g">
<title>Experiment 4: General-Purpose Reading</title>
<p>In the goal-directed reading task, participants read a passage to answer a question that they knew in advance, and the eye tracking results revealed that participants spent more time reading question-relevant words. Question-relevant words, however, were generally longer content words (<xref rid="figs5" ref-type="fig">Fig. S5CD</xref>) that were often associated with longer reading time even without a task [<xref ref-type="bibr" rid="c3">3</xref>]. Therefore, to validate the question relevance effect, we ran Experiment 4 in which the participants read the passages without knowing the question to answer. The experiment used the same 96 questions as in Experiments 2 and 3, but adopted a different experimental procedure: Participants previewed a passage before reading the question, and were allowed to read the passage again to answer the question. We analyzed the reading pattern during passage preview, which was referred to as general-purpose reading.</p>
<p>The participants were given 1.5 minutes to preview the passage, and the reading speed was on average 225 ±40 words/minute, lower than that in Experiments 1-3 (P = 6 × 10<sup>-4</sup>, bootstrap, FDR corrected for comparisons across 4 experiments). Before question answering, they were given another 0.5 minutes to reread the passage, but on average they spent only 0.04 minute on rereading it. During passage preview, the word reading time was similarly predicted by the pre-trained and fine-tuned transformer-based models (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>, <xref rid="tbls4" ref-type="table">Table S4</xref>). Furthermore, the word reading time was significantly predicted by layout and word features, but not question relevance (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>, <xref rid="tbls4" ref-type="table">Table S4</xref>). Both the early and late processing stages of human reading were significantly affected by layout and word features, and the effects were larger for the late processing stage indexed by counts of rereading (<xref rid="figs9" ref-type="fig">Fig. S9C</xref>, <xref rid="tbls3" ref-type="table">Table S3</xref>). The passage beginning effect was not significantly different between local and global questions (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>, 4<sup>th</sup> column, P = 0.994, bootstrap, FDR corrected for comparisons across 4 experiments), and the question relevance effect was significantly smaller than the question relevance effect in Experiments 1-3 (<xref rid="fig5" ref-type="fig">Fig. 5D</xref> &amp; <xref rid="figs10" ref-type="fig">Fig. S10B</xref>, P = 2 ×10<sup>-4</sup>, bootstrap, FDR corrected for comparisons across 4 experiments). These results confirmed that the question relevance effects observed during goal-directed reading were indeed task dependent.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Attention is a crucial mechanism to regulate information processing in the brain and it has been hypothesized that a common computational role of attention is to optimize task performance. Previous support for the hypothesis mostly comes from tasks for which the optimal strategy can be easily derived. The current study, however, considers a real-world reading task in which the participants have to actively sample a passage to answer a question that cannot be answered by simple word-level orthographic or semantic matching. In this challenging task, it is demonstrated that human attention distribution can be explained by the attention weights in transformer-based DNN models that are optimized to perform the same reading task but blind to the human eye tracking data. Furthermore, when participants scan a passage without knowing the question to answer, their attention distribution can also be explained by transformer-based DNN models that are optimized to predict a word based on the context.</p>
<p>Furthermore, we demonstrate that both humans and transformer-based DNN models achieve task-optimal attention distribution in multiple steps: For humans, basic text features strongly modulate the duration of the first reading of a word, while the question relevance of a word only modulates how many times the word is reread, especially for high-proficiency L2 readers compared to native readers. Similarly, the DNN models do not yield a single attention distribution, and instead it generates multiple attention distributions, i.e., heads, for each layer. Here, we demonstrate that basic text features mainly modulate the attention weights in shallow layers, while the question relevance of a word modulates the attention weights in deep layers, reflecting hierarchical control of attention to optimize task performance. The attention weights in both the shallow and deep layers of DNN contribute to the explanation of human word reading time (<xref rid="figs4" ref-type="fig">Fig. S4</xref>).</p>
<sec id="s3a">
<title>Computational models of attention</title>
<p>A large number of computational models of attention have been proposed. According to Marr’s 3 levels of analysis [<xref ref-type="bibr" rid="c4">4</xref>], some models investigate the computational goal of attention [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c12">12</xref>] and some models provide an algorithmic implementation of how different factors modulate attention [<xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c34">34</xref>]. Computationally, it has been hypothesized that attention can be interpreted as a mechanism to optimize learning and decision making, and empirical evidence has been provided that the brain allocates attention among different information sources to optimally reduce the uncertainty of a decision [<xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c12">12</xref>]. The current study provides critical support to this hypothesis in a real-world task that engages multiple forms of attention, e.g., attention to visual layout features, attention to word features, and attention to question-relevant information. These different forms of attention, which separately modulate different eye tracking measures (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>), jointly achieve an attention distribution that is optimal for question answering.</p>
<p>The transformer-based DNN models analyzed here are optimized in two steps, i.e., pre-training and fine-tuning. The results show that pre-training leads to text-based attention that can well explain general-purpose reading in Experiment 4, while the fine-tuning process leads to goal-directed attention in Experiments 1-3 (<xref rid="fig4" ref-type="fig">Fig. 4B</xref> &amp; <xref rid="fig5" ref-type="fig">Fig. 5A</xref>). Pre-training is also achieved through task optimization, and the pre-training task used in all the three models analyzed here is to predict a word based on the context. The purpose of the word prediction task is to let models learn the general statistical regularity in a language based on large corpora, which is crucial for model performance on downstream tasks [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c33">33</xref>], and this process can naturally introduce the sensitivity to word surprisal, i.e., how unpredictable a word is given the context. Previous eye-tracking studies have suggested that the predictability of words, i.e., surprisal, can modulate reading time [<xref ref-type="bibr" rid="c35">35</xref>], and neuroscientific studies have also indicated that the cortical responses to language converge with the representations in pre-trained DNN models [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>]. The results here further demonstrate that the DNN optimized for the word prediction task can evolve attention properties consistent with the human reading process. Additionally, the tokenization process in DNN can also contribute to the similarity between human and DNN attention distributions: DNN first separates words into tokens (e.g., “tokenization” is separated into “token” and “ization”). Tokens are units that are learned based on co-occurrence of letters, and is not strictly linked to any linguistically defined units. Since longer words tend to be separated into more tokens, i.e., fragments of frequently co-occurred letters, longer words receive more attention even if the model pay uniform attention to each of its input, i.e., a token.</p>
<p>A separate class of models investigates which factors shape human attention distribution. A large number of models are proposed to predict bottom-up visual saliency [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>], and recently DNN models are also employed to model top-down visual attention. It is shown that, through either implicit [<xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c39">39</xref>] or explicit training [<xref ref-type="bibr" rid="c40">40</xref>], DNNs can predict which parts of a picture relate to a verbal phrase, a task similar to goal-directed visual search [<xref ref-type="bibr" rid="c41">41</xref>]. The current study distinguishes from these studies in that the DNN model is not trained to predict human attention. Instead, the DNN models naturally generate human-like attention distribution when trained to perform the same task that humans perform, suggesting that task optimization is a potential cause for human attention distribution during reading.</p>
</sec>
<sec id="s3b">
<title>Models for human reading and human attention to question-relevant information</title>
<p>How human readers allocate attention during reading is an extensively studied topic, mostly based on studies that instruct readers to read a sentence in a normal manner, not aimed to extract a specific kind of information [<xref ref-type="bibr" rid="c18">18</xref>]. Previous eye tracking studies have shown that the readers fixate longer upon, e.g., longer words, words of lower-frequency, words that are less predictable based on the context, and words at the beginning of a line [<xref ref-type="bibr" rid="c3">3</xref>]. A number of models, e.g., the E-Z reader [<xref ref-type="bibr" rid="c19">19</xref>] and SWIFT [<xref ref-type="bibr" rid="c42">42</xref>], have been proposed to predict the eye movements during reading based on basic oculomotor properties or lexical processing [<xref ref-type="bibr" rid="c19">19</xref>]. Some models also view reading as an optimization process that minimizes the time or the number of saccades required to read a sentence [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. These models can generate fine-grained predictions, e.g., which letter in a word will be fixated first, for the reading of simple sentences, but have only been occasionally tested for complex sentences or multi-line texts [<xref ref-type="bibr" rid="c43">43</xref>] or to characterize different reading tasks, e.g., z-string reading and visual searching [<xref ref-type="bibr" rid="c44">44</xref>].</p>
<p>When readers read a passage to answer a question that can be answered using a word-matching strategy [<xref ref-type="bibr" rid="c45">45</xref>], a recent study has demonstrated that the specific reading goal modulates the word reading time and the effect can be modeled using a RNN model [<xref ref-type="bibr" rid="c46">46</xref>]. Here, we focus on questions that cannot be answered using a word-matching strategy (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>) and demonstrate that, for these challenging questions, attention is still modulated by the reading goal but the attention modulation cannot be explained by a word-matching model (<xref rid="figs3" ref-type="fig">Fig. S3</xref>). Instead, the attention effect is better captured by transformer models than an advanced RNN model, i.e., the SAR (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>).</p>
<p>Combining the current study and the study by Hahn et al. [<xref ref-type="bibr" rid="c46">46</xref>], it is possible that the word reading time during a general-purpose reading task can be explained by a word prediction task, the word reading time during a simple goal-directed reading task that can be solved by word matching can be modeled by a RNN model, while the word reading time during a more complex goal-directed reading task involving inference is better modeled using a transformer model. The current study also further demonstrates that elongated reading time on task-relevant words is caused by counts of rereading and further studies are required to establish whether earlier eye movement measures can be modulated by, e.g., a word matching task. In addition, future studies can potentially integrate classic eye movement models with DNNs to explain the dynamic eye movement trajectory, possibly with a letter-based spatial resolution.</p>
<p>When human readers read a passage with a particular goal or perspective, previous studies have revealed inconsistent results about whether the readers spent more time reading task-relevant sentences [<xref ref-type="bibr" rid="c47">47</xref>–<xref ref-type="bibr" rid="c49">49</xref>]. To explain the inconsistent results, it has been proposed that the question relevance effect weakens for readers with a higher working memory and when readers read a familiar topic [<xref ref-type="bibr" rid="c50">50</xref>]. Similarly, here, we demonstrate that non-native readers indeed spend more time reading question-relevant information than native readers do (<xref rid="fig5" ref-type="fig">Fig. 5D</xref> &amp; <xref rid="figs10" ref-type="fig">Fig. S10B</xref>). Therefore, it is possible that when readers are more skilled and when the passage is relatively easy to read, their processing is so efficient so that they do not need extra time to encode task-relevant information and may rely on covert attention to prioritize the processing of task-relevant information.</p>
</sec>
<sec id="s3c">
<title>DNN attention to question-relevant information</title>
<p>A number of studies have investigated whether the DNN attention weights are interpretable, but the conclusions are mixed: Some studies find that the DNN attention weights are positively correlated with the importance of each word [<xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c52">52</xref>], while other studies fail to find such correlation [<xref ref-type="bibr" rid="c53">53</xref>, <xref ref-type="bibr" rid="c54">54</xref>]. The inconsistent results are potentially caused by the lack of gold standard to evaluate the contribution of each word to a task. A few recent studies have used the human word reading time as the criterion to quantify word importance, but these studies do not reach consistent conclusions either. Some studies find that the attention weights in the last layer of transformer-based DNN models better correlates with human word reading time than basic word frequency measures [<xref ref-type="bibr" rid="c55">55</xref>], and integrating human word reading time into DNN can slightly improve task performance [<xref ref-type="bibr" rid="c56">56</xref>]. Other studies, however, find no meaningful correlation between the attention weights in transformer-based DNNs and human word reading time [<xref ref-type="bibr" rid="c57">57</xref>].</p>
<p>The current results provide a potential explanation for the discrepancy in the literature: The last layer of transformer-based DNNs is tuned to task relevant information (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>), but the influence of task relevance on word reading time is rather weak for native readers (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>). Consequently, the correlation between the last-layer DNN attention weights and human reading time may not be robust. The current results demonstrate that the reading time of both native and non-native readers are reliably modulated by basic text features, which can be modeled by the attention weights in shallower DNN layers.</p>
<p>Finally, the current study demonstrates that transformer-based DNN models can automatically generate human-like attention, in the absence of any prior knowledge about the properties of the human reading process. Simpler models that fail to explain human performance also fail to predict human attention distribution. It remains possible, however, different models can solve the same computational problem using distinct algorithms, and only some algorithms generate human-like attention distribution. In other words, human-like attention distribution may not be a unique solution to optimize the goal-directed reading task. Sharing similar attention distribution with humans, however, provides a way to interpret the attention weights in computational models. From this perspective, the dataset and methods developed here provides an effective probe to test the biological plausibility of NLP models that can be easily applied to test whether a model evolves human-like attention distribution.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Totally, 162 participants took part in this study (19-30 years old, mean age, 22.5 years; 84 female). All participants had normal or corrected-to-normal vision. Experiment 1 had 102 participants. Experiments 2-4 had 20 participants. No participant took part in more than one experiment. Additional 17 participants were recruited but failed to pass the calibration process for eye tracking and therefore did not participant in the reading experiments.</p>
<p>In Experiments 1, 2 and 4, participants were native Chinese readers. They were college students or graduate students from Zhejiang University, and were thus above the level required to answer high-school-level reading comprehension questions. English proficiency levels were further guaranteed by the following criterion for screening participants: a minimum score of 6 on IELTS, 80 on TOEFL, or 425 on CET6<sup><xref ref-type="fn" rid="fn1">1</xref></sup>. In Experiment 3, participants were native English readers. The experimental procedures were approved by the Research Ethics Committee of the College of Medicine, Zhejiang University (2019–047). The participants provided written consent and were paid.</p>
</sec>
<sec id="s4b">
<title>Experimental materials</title>
<p>The reading materials were selected and adapted from the large-scale RACE dataset, a collection of reading comprehension questions in English exams for middle and high schools in China [<xref ref-type="bibr" rid="c31">31</xref>]. We selected 800 high-school level questions from the test set of RACE and each question was associated with a distinct passage (117 to 456 words per passage). All questions were multiple-choice questions with 4 alternatives including only one correct option among them. The questions fell into 6 types, i.e., Cause (<italic>N</italic> = 200), Fact (<italic>N</italic> = 200), Inference (<italic>N</italic> = 120), Theme (<italic>N</italic> = 100), Title (<italic>N</italic> = 100), and Purpose (<italic>N</italic> = 80). The Cause, Fact, and Inference questions concerned the location, extraction, and comprehension of specific information from a passage, and were referred to as local questions. Questions of Theme, Title, and Purpose tested the understanding of a passage as a whole, and were referred to as global questions.</p>
<p>In a separate online experiment, we acquired annotations about the relevance of each word to the question answering task. For each passage, a participant was allowed to annotate up to 5 key words that were considered relevant to answering the corresponding question. Each passage was annotated by <italic>N</italic> participants (<italic>N</italic> ≥ 26), producing <italic>N</italic> versions of annotated key words. Each version of annotation was then validated by a separate participant. In the validation procedure, the participant was required to answer the question solely based on the key words of a specific annotation version; if the person could not derive the correct answer, this version of annotation was discarded. The percentage of questions correctly answered in the validation procedure was 75.9% and 67.6%, for local and global questions respectively. If <italic>M</italic> versions of annotation passed the validation procedure and a word was annotated in <italic>K</italic> versions, the question relevance of the word was <italic>K/M</italic>. More details about the question types and the annotation procedures could be found in the reference [<xref ref-type="bibr" rid="c58">58</xref>].</p>
</sec>
<sec id="s4c">
<title>Experimental procedures</title>
<sec id="s4c1">
<title>Experiment 1</title>
<p>Experiment 1 included all 800 passages, and different question types were separately tested in different sessions, hence 6 sessions in total. Each session included 25 participants and one participant could participate in multiple sessions. Before each session, participants were familiarized with 5 questions that were not used in the formal session. During the formal session, questions were presented in a randomized order. Considering the quantities of questions, for Cause and Fact questions, the session was carried out in 3 separate days (one third questions on each day), and for other question types, the session was carried out in 2 separate days (fifty percent of questions on each day).</p>
<p>The experiment procedure in Experiment 1 was illustrated in <xref rid="fig1" ref-type="fig">Fig. 1A</xref>. In each trial, participants first read a question, pressed the space bar to read the corresponding passage, pressed the space bar again to read the question coupled with 4 options, and chose the correct answer. The time limit for passage reading was 120 s. To encourage the participants to read as quickly as possible, the bonus they received for a specific question would decrease linearly from 1.5 to 0.5 RMB over time. They did not receive any bonus for the question, however, if they gave a wrong answer.</p>
<p>Furthermore, before answering the comprehension question, the participants reported whether they were confident about that they could correctly answer the question (yes or no). Participants selected yes for 90.47% of questions (89.62% and 92.04% for local and global questions, respectively). After answering the question, they also rated their confidence about their answer on the scale of 1-4 (low to high). The mean confidence rating was 3.25 (3.28 and 3.18 for local and global question, respectively), suggesting that the participants were confident about their answers.</p>
</sec>
<sec id="s4c2">
<title>Experiments 2 and 3</title>
<p>Experiments 2 and 3 included 96 reading passages and questions that were randomly selected from the questions used in Experiment 1 and included 16 questions for each question type. The 6 types of questions were mixed and presented in a randomized order. The trial structure, as well as the familiarization procedure, in Experiments 2 and 3 was identical to that in Experiment 1. Experiments 2 and 3 were identical except that Experiment 2 recruited high-proficiency L2 readers while Experiment 3 recruited native English readers.</p>
</sec>
<sec id="s4c3">
<title>Experiment 4</title>
<p>Experiment 4 included the 96 questions presented in Experiments 2 and 3, which were presented in a randomized order. The trial structure in Experiment 4 is similar to that in Experiments 1-3, except that a 90-s passage preview stage was introduced at the beginning of each trial. During passage preview, participants had no prior information of the relevant question. The participants could press the space bar to terminate the preview and to read a question. Then, participants read the passage again with a time limit of 30 s, before proceeding to answer the question. The payment method was similar to Experiment 2, and the bonus was calculated based on the duration of second-pass passage reading.</p>
</sec>
</sec>
<sec id="s4d">
<title>Stimulus presentation and eye tracking</title>
<p>The text was presented using the bold Courier New font, and each letter occupied 14 ×27 pixels. We set the maximum number of letters on each line to 120 and used double space. We separated paragraphs by indenting the first line of each new paragraph. Participants sat about 880 mm from a monitor, at which each letter horizontally subtended approximately 0.25 degrees of visual angle.</p>
<p>Eye tracking data were recorded from the left eye with 500-Hz sampling rate (Eyelink Portable Duo, SR Research). The experiment stimuli were presented on a 24-inch monitor (1920 ×1080 resolution; 60 Hz refresh rate) and administered using MATLAB Psychtoolbox [<xref ref-type="bibr" rid="c59">59</xref>]. Each experiment started with a 13-point calibration and validation of eye tracker, and the validation error was required to be below 0.5 degrees of visual angle. Furthermore, before each trial, a 1-point validation was applied, and if the calibration error was higher than 0.5 degrees of visual angle, a recalibration was carried out. Head movements were minimized using a chin and forehead rest.</p>
</sec>
<sec id="s4e">
<title>Word-level reading comprehension models</title>
<p>The orthographic and semantic models probed whether the reading comprehension questions could be answered based on word-level orthographic or semantic information. Both models calculated the similarity between each content word in the passage and each content word in an option, and averaged the word-by-word similarity across all words in the passage and all words in the option (<xref rid="figs1" ref-type="fig">Fig. S1A</xref>). The option with the highest mean similarity value was chosen as the answer. For the orthographic model, similarity was quantified using the edit distance [<xref ref-type="bibr" rid="c60">60</xref>]. For the semantic model, similarity was quantified by the correlation between vectorial representations of word meaning, i.e., the glove model [<xref ref-type="bibr" rid="c61">61</xref>]. Performance of the models remained similar if the answer was chosen based on the maximal word-by-word similarity, instead of the mean similarity.</p>
</sec>
<sec id="s4f">
<title>RNN-based reading comprehension models</title>
<p>The SAR was a classical RNN-based model for the reading comprehension task [<xref ref-type="bibr" rid="c32">32</xref>]. In contrast to the word-level models, the SAR was context sensitive and employed bi-directional RNNs to integrate information across words (<xref rid="figs1" ref-type="fig">Fig. S1B</xref>). Independent bi-directional RNNs were employed to build a vectorial representation for the question and each option. An additional bi-directional RNN was applied to construct a vectorial representation for each word in the passage, and a passage representation was built by a weighted sum of the representations of individual words in the passage. The weight on each word, i.e., the attention weight, captured the similarity between the representation of the word and the question representation using a bilinear function. Finally, based on the passage representation and each option representation, a bilinear dot layer calculated the possibility that the option was the correct answer.</p>
</sec>
<sec id="s4g">
<title>Transformer-based reading comprehension models</title>
<p>We tested 3 popular transformer-based DNN models, i.e., BERT [<xref ref-type="bibr" rid="c33">33</xref>], ALBERT [<xref ref-type="bibr" rid="c21">21</xref>], and RoBERTa [<xref ref-type="bibr" rid="c22">22</xref>], which were all reported to reach high performance on the reading comprehension task. ALBERT and RoBERTa were both adapted from BERT, and had the same basic structure. RoBERTa differed from BERT in its pre-training procedure [<xref ref-type="bibr" rid="c22">22</xref>] while ALBERT applied factorized embedding parameterization and cross-layer parameter sharing to reduce memory consumption [<xref ref-type="bibr" rid="c21">21</xref>]. Following previous studies [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>], each option was independently processed. For the <italic>i</italic><sup>th</sup> option (<italic>i</italic> = 1, 2, 3, or 4), the question and the option were concatenated to form an integrated option. As shown in the left panel of <xref rid="fig2" ref-type="fig">Fig. 2B</xref>, for the <italic>i</italic><sup>th</sup> option, the input to models was the following sequence:
<disp-formula>
<graphic xlink:href="538252v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>CLS<sub>i</sub></italic>, <italic>S<sub>i,</sub></italic><sub>1</sub>, and <italic>S<sub>i,</sub></italic><sub>2</sub> denoted special tokens separating different components of the input. <italic>P</italic><sub>1</sub>, <italic>P</italic><sub>2</sub>, …, <italic>P<sub>N</sub></italic> denoted all the <italic>N</italic> words of a passage, and <italic>O<sub>i,</sub></italic><sub>1</sub>, <italic>O<sub>i,</sub></italic><sub>2</sub>, …, <italic>O<sub>i,M</sub></italic> denoted all the <italic>M</italic> words in the <italic>i</italic><sup>th</sup> integrated option. Each of the token was represented by a vector. The vectorial representation was updated in each layer, and in the following the output of the <italic>l</italic><sup>th</sup> layer was denoted as a superscript, e.g., <italic>CLS <sup>l</sup></italic>. Following previous studies [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>], we calculated a score for each option, which indicated the possibility that the option was the correct answer. The score was calculated by first applying a linear transform to the final representation of the CLS token, i.e.,
<disp-formula>
<graphic xlink:href="538252v2_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>CLS<sub>i</sub></italic><sup>12</sup> was the final output representation of CLS and <italic>Φ</italic> was a vector learned from data. The score was independently calculated for each option and then normalized using the following equation:
<disp-formula>
<graphic xlink:href="538252v2_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The answer to a question was determined as the option with the highest score, and all the models were trained to maximize the logarithmic score of the correct option. The transformer-based models were trained in two steps (<xref rid="fig1" ref-type="fig">Fig. 1D</xref>). The pre-training process aimed to learn general statistical regularities in a language based on large corpora, i.e., BooksCorpus [<xref ref-type="bibr" rid="c62">62</xref>] and English Wikipedia, while the fine-tuning process trained models to perform the reading comprehension task based on RACE dataset. All models were implemented based on HuggingFace [<xref ref-type="bibr" rid="c63">63</xref>] and all hyperparameters for fine-tuning were adopted from previous studies [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c64">64</xref>, <xref ref-type="bibr" rid="c65">65</xref>] (see <xref rid="tbls6" ref-type="table">Table S6</xref>).</p>
</sec>
<sec id="s4h">
<title>Attention in transformer-based models</title>
<p>The transformer-based models we applied had 12 layers, and each layer had 12 parallel attention heads. Each attention head calculated an attention weight between any pair of inputs, including words and special tokens. The vectorial representation of each input was then updated by the weighted sum of the vectorial representations of all inputs [<xref ref-type="bibr" rid="c66">66</xref>]. Since only the CLS token was directly related to question answering, here we restrained the analysis to the attention weights that were used to calculate the vectorial representation of CLS (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, right panel). In the <italic>h</italic><sup>th</sup> head, the vectorial representation of CLS was computed using the following equations. For the sake of clarity, we did not distinguish the input words and special tokens and simply denoted them as <italic>X<sub>i</sub></italic>.
<disp-formula>
<graphic xlink:href="538252v2_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>W<sup>V</sup></italic>, <italic>W<sup>Q</sup></italic>, <italic>W<sup>K</sup></italic>, <italic>b<sup>V</sup></italic>, <italic>b<sup>Q</sup></italic>, and <italic>b<sup>K</sup></italic> were parameters to learn from the data, and <italic>α<sub>i</sub></italic> was the attention weight between CLS and <italic>X<sub>i</sub></italic>. The attention weight between CLS and the <italic>n</italic><sup>th</sup> word in the passage, i.e., <italic>α<sub>Pn</sub></italic>, was compared to human attention. Here, we only considered the attention weights associated with the correct option. Additionally, DNNs used byte-pair tokenization which split some words into multiple tokens. We converted the token-level attention weights to word-level attention weights by summing the attention weights over tokens within a word [<xref ref-type="bibr" rid="c55">55</xref>, <xref ref-type="bibr" rid="c67">67</xref>].</p>
</sec>
<sec id="s4i">
<title>Eye tracking measures</title>
<p>We analyzed eye movements during passage reading in Experiments 1-3, and the passage preview in Experiment 4. For each word, the total fixation time, gaze duration, and run counts was extracted using the SR Research Data Viewer software. The total fixation time of a word was referred to as the word reading time. The gaze duration was the how long a word was fixated before the gaze moved to other words, reflected first-pass processing of a word. To characterize late processing of a word, we further calculated the counts of rereading, which were defined as the run counts minus 1. Words that were not reread were excluded from the analysis of counts of rereading. Each of the eye tracking measure was averaged across all participants who correctly answered the question.</p>
</sec>
<sec id="s4j">
<title>Regression models</title>
<p>We employed linear regression to analyze how well each model, as well as each set of text/task-related features, could explain human attention measured by eye tracking. In all regression analyses, each regressor and the eye-tracking measure were normalized within each passage by taking the z-score. The predictive power, i.e., the Pearson correlation coefficient between the predicted eye-tracking measure and the actual eye-tracking measure, was calculated based on five-fold cross-validation.</p>
<p><italic>Regressors:</italic> For the SAR, each word had one attention weight, which was used as the regressor. For transformer-based models, since each model contained 12 layers and each layer contained 12 attention heads, all together there were 144 regressors. Text features included layout features and word features. The layout features concerned the visual position of text, including the coordinate of the left most pixel of a word, ordinal paragraph number of a word in a passage, ordinal line number of a word in a paragraph, and ordinal line number of a word in a passage. The word features included word length, logarithmic word frequency estimated based on the BookCorpus [<xref ref-type="bibr" rid="c62">62</xref>] and English Wikipedia using SRILM [<xref ref-type="bibr" rid="c68">68</xref>], and word surprisal estimated from GPT-2 Medium [<xref ref-type="bibr" rid="c69">69</xref>]. The task-related feature referred to the question relevance annotated by another group of participants (see <italic>Experimental materials</italic> for details).</p>
<p>Additionally, we also applied linear regression to probe how DNN attention was affected by text features and question relevance. Since information of lines and paragraphs were not available to DNNs, the layout features only included the ordinal position of a word in a sentence, ordinal position of a word in a passage, and ordinal sentence number of a word in this analysis</p>
</sec>
<sec id="s4k">
<title>Statistical tests</title>
<p>In the regression analysis, we employed a one-sided permutation test to test whether a set of features could statistically significantly predict an eye tracking measure. Five hundred chance-level predictive power was calculated by predicting the eye tracking measure shuffled across all words within a passage: The eye tracking measure to predict was shuffled but the features were not. The procedure was repeated 500 times, creating 500 chance-level predictive power. If the actual correlation was smaller than <italic>N</italic> out of the 500 chance-level correlation, the significance level was (<italic>N</italic> + 1)/501.</p>
<p>When comparing the responses to local and global questions, the 3 types of local/global questions were pooled. The comparison between local and global questions, as well as the comparison between experiments, was based on bias-corrected and accelerated bootstrap [<xref ref-type="bibr" rid="c70">70</xref>]. For example, to test whether the predictive power differed between the 2 types of questions, all global questions were resampled with replacement 50000 times and each time the predictive power was calculated based on the resampled questions, resulting in 50000 resampled predictive power. If the predictive power for local questions was greater (or smaller) than <italic>N</italic> out of the 50000 resampled predictive power for global questions, the significance level of their difference was 2(<italic>N</italic> + 1)/50001. When multiple comparisons were performed, the p-value was further adjusted using the false discovery rate (FDR) correction.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank David Poeppel, Yunyi Pan, and Erik D. Reichle for valuable comments on earlier versions of this manuscript; Jonathan Simon, Bingjiang Lyu, and members of the Ding lab for thoughtful discussions and feedback; Qian Chu, Yuhan Lu, Anqi Dai, Zhonghua Tang, and Yan Chen for assistance with experiments. Work supported by STI2030-Major Project 2021ZD0204105, National Natural Science Foundation of China 32222035, Major Scientific Research Project of Zhejiang Lab 2019KB0AC02, and Fundamental Research Funds for the Central Universities 226-2023-00091.</p>
</ack>
<sec id="s5">
<title>Author Contributions</title>
<p>Jiajie Zou implemented the experiments and models, analyzed data, and wrote the manuscript. Nai Ding acquired the funding, conceived and coordinated the project, analyzed data, and wrote the manuscript. Xing Tian coordinated the project and revised the manuscript. Yuran Zhang and Jialu Li implemented the experiments.</p>
</sec>
<sec id="s6">
<title>Competing Interest Statement</title>
<p>The authors declare no competing interests.</p>
</sec>
<sec id="s7">
<title>Data Availability</title>
<p>All eye tracking data is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jiajiezou/TOA">https://github.com/jiajiezou/TOA</ext-link>.</p>
</sec>
<sec id="s8">
<title>Code availability</title>
<p>The code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jiajiezou/TOA">https://github.com/jiajiezou/TOA</ext-link>.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Posner</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>SE</given-names></string-name>. <article-title>The attention system of the human brain</article-title>. <source>Annu Rev Neurosci</source>. <year>1990</year>;<volume>13</volume>(<issue>1</issue>):<fpage>25</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Treisman</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Gelade</surname> <given-names>G</given-names></string-name>. <article-title>A feature-integration theory of attention</article-title>. <source>Cogn Psychol</source>. <year>1980</year>;<volume>12</volume>(<issue>1</issue>):<fpage>97</fpage>–<lpage>136</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>. <article-title>Eye movements in reading and information processing: 20 years of research</article-title>. <source>Psychol Bull</source>. <year>1998</year>;<volume>124</volume>(<issue>3</issue>):<fpage>372</fpage>–<lpage>422</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Marr</surname> <given-names>D</given-names></string-name>. <article-title>Vision: A computational investigation into the human representation and processing of visual information, henry holt and co. Inc, New York</article-title>, <source>NY</source>. <year>1982</year>;<fpage>2</fpage>(<lpage>4</lpage>.2).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="other"><string-name><surname>Kahneman</surname> <given-names>D.</given-names></string-name> <article-title>Attention and effort: Citeseer</article-title>; <year>1973</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Franconeri</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Alvarez</surname> <given-names>GA</given-names></string-name>, <string-name><surname>Cavanagh</surname> <given-names>P</given-names></string-name>. <article-title>Flexible cognitive resources: competitive content maps for attention and memory</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2013</year>;<volume>17</volume>(<issue>3</issue>):<fpage>134</fpage>–<lpage>41</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2013.01.010</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Lennie</surname> <given-names>P</given-names></string-name>. <article-title>The Cost of Cortical Computation</article-title>. <source>Current Biology</source>. <year>2003</year>;<volume>13</volume>(<issue>6</issue>):<fpage>493</fpage>–<lpage>7</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0960-9822(03)00135-0</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Carrasco</surname> <given-names>M</given-names></string-name>. <article-title>Visual attention: The past 25 years</article-title>. <source>Vision research</source>. <year>2011</year>;<volume>51</volume>(<issue>13</issue>):<fpage>1484</fpage>–<lpage>525</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.visres.2011.04.012</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Borji</surname> <given-names>A</given-names></string-name>, <string-name><surname>Itti</surname> <given-names>L</given-names></string-name>. <article-title>State-of-the-art in visual attention modeling</article-title>. <source>IEEE transactions on pattern analysis and machine intelligence</source>. <year>2012</year>;<volume>35</volume>(<issue>1</issue>):<fpage>185</fpage>–<lpage>207</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Kakade</surname> <given-names>S</given-names></string-name>, <string-name><surname>Montague</surname> <given-names>PR</given-names></string-name>. <article-title>Learning and selective attention</article-title>. <source>Nat Neurosci</source>. <year>2000</year>;<volume>3</volume>(<issue>11</issue>):<fpage>1218</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Gottlieb</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hayhoe</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hikosaka</surname> <given-names>O</given-names></string-name>, <string-name><surname>Rangel</surname> <given-names>A.</given-names></string-name> <article-title>Attention, reward, and information seeking</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>(<issue>46</issue>):<fpage>15497</fpage>–<lpage>504</lpage>. Epub 2014/11/14. PubMed PMID: <pub-id pub-id-type="pmid">25392517</pub-id>; PubMed Central PMCID: <pub-id pub-id-type="pmcid">PMCPMC4228145</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Legge</surname> <given-names>GE</given-names></string-name>, <string-name><surname>Hooven</surname> <given-names>TA</given-names></string-name>, <string-name><surname>Klitz</surname> <given-names>TS</given-names></string-name>, <string-name><surname>Mansfield</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Tjan</surname> <given-names>BS</given-names></string-name>. <article-title>Mr. Chips 2002: New insights from an ideal-observer model of reading</article-title>. <source>Vision Res</source>. <year>2002</year>;<volume>42</volume>(<issue>18</issue>):<fpage>2219</fpage>–<lpage>34</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Liu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Reichle</surname> <given-names>E</given-names></string-name>. <article-title>The emergence of adaptive eye movements in reading</article-title>. <source>Cogsci</source>. <year>2010</year>;<volume>32</volume>(<issue>32</issue>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Najemnik</surname> <given-names>J</given-names></string-name>, <string-name><surname>Geisler</surname> <given-names>WS</given-names></string-name>. <article-title>Optimal eye movement strategies in visual search</article-title>. <source>Nature</source>. <year>2005</year>;<volume>434</volume>(7031):<fpage>387</fpage>–<lpage>91</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Navalpakkam</surname> <given-names>V</given-names></string-name>, <string-name><surname>Koch</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rangel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Perona</surname> <given-names>P</given-names></string-name>. <article-title>Optimal reward harvesting in complex perceptual environments</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2010</year>;<volume>107</volume>(<issue>11</issue>):<fpage>5232</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Li</surname> <given-names>X</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Yao</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hyönä</surname> <given-names>J.</given-names></string-name> <article-title>Universal and specific reading mechanisms across different writing systems</article-title>. <source>Nat Rev Psychol</source>. <year>2022</year>;<volume>1</volume>:<fpage>133</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Gagl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Gregorova</surname> <given-names>K</given-names></string-name>, <string-name><surname>Golch</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hawelka</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sassenhagen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Tavano</surname> <given-names>A</given-names></string-name>, <etal>et al.</etal> <article-title>Eye movements during text reading align with the rate of speech production</article-title>. <source>Nat Hum Behav</source>. <year>2021</year>;<volume>6</volume>:<fpage>429</fpage>–<lpage>42</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41562-021-01215-4</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Clifton</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ferreira</surname> <given-names>F</given-names></string-name>, <string-name><surname>Henderson</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Inhoff</surname> <given-names>AW</given-names></string-name>, <string-name><surname>Liversedge</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Reichle</surname> <given-names>ED</given-names></string-name>, <etal>et al.</etal> <article-title>Eye movements in reading and information processing: Keith Rayner’s 40 year legacy</article-title>. <source>J Mem Lang</source>. <year>2016</year>;<volume>86</volume>:<fpage>1</fpage>–<lpage>19</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jml.2015.07.004</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Reichle</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Pollatsek</surname> <given-names>A</given-names></string-name>. <article-title>The EZ Reader model of eye-movement control in reading: Comparisons to other models</article-title>. <source>Behav Brain Sci</source>. <year>2003</year>;<volume>26</volume>(<issue>4</issue>):<fpage>445</fpage>–<lpage>76</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>White</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Forsyth</surname> <given-names>B</given-names></string-name>. <article-title>Reading-related literacy activities of American adults: Time spent, task types, and cognitive skills used</article-title>. <source>J Lit Res</source>. <year>2010</year>;<volume>42</volume>(<issue>3</issue>):<fpage>276</fpage>–<lpage>307</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="confproc"><string-name><surname>Lan</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Goodman</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gimpel</surname> <given-names>K</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>P</given-names></string-name>, <string-name><surname>Soricut</surname> <given-names>R</given-names></string-name>. <conf-name>Albert: A lite bert for self-supervised learning of language representations. International Conference on Learning Representations; 2019: ICLR</conf-name>; <year>2020</year>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="other"><string-name><surname>Liu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Ott</surname> <given-names>M</given-names></string-name>, <string-name><surname>Goyal</surname> <given-names>N</given-names></string-name>, <string-name><surname>Du</surname> <given-names>J</given-names></string-name>, <string-name><surname>Joshi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal> <article-title>Roberta: A robustly optimized bert pretraining approach</article-title>. <source>arXiv</source>. <year>2019</year>. doi: arXiv:1907.11692.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Yamins</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>H</given-names></string-name>, <string-name><surname>Cadieu</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Solomon</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Seibert</surname> <given-names>D</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2014</year>;<volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Kell</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Yamins</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Shook</surname> <given-names>EN</given-names></string-name>, <string-name><surname>Norman-Haignere</surname> <given-names>SV</given-names></string-name>, <string-name><surname>McDermott</surname> <given-names>JH</given-names></string-name>. <article-title>A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title>. <source>Neuron</source>. <year>2018</year>;<volume>98</volume>(<issue>3</issue>):<fpage>630</fpage>–<lpage>44</lpage>. e16.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Goldstein</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zada</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Buchnik</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schain</surname> <given-names>M</given-names></string-name>, <string-name><surname>Price</surname> <given-names>A</given-names></string-name>, <string-name><surname>Aubrey</surname> <given-names>B</given-names></string-name>, <etal>et al.</etal> <article-title>Shared computational principles for language processing in humans and deep language models</article-title>. <source>Nat Neurosci</source>. <year>2022</year>;<volume>25</volume>(<issue>3</issue>):<fpage>369</fpage>–<lpage>80</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Schrimpf</surname> <given-names>M</given-names></string-name>, <string-name><surname>Blank</surname> <given-names>IA</given-names></string-name>, <string-name><surname>Tuckute</surname> <given-names>G</given-names></string-name>, <string-name><surname>Kauf</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hosseini</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name>, <etal>et al.</etal> <article-title>The neural architecture of language: Integrative modeling converges on predictive processing</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2021</year>;<volume>118</volume>(<issue>45</issue>):<fpage>e2105646118</fpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.2105646118</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Hasson</surname> <given-names>U</given-names></string-name>, <string-name><surname>Nastase</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Goldstein</surname> <given-names>A</given-names></string-name>. <article-title>Direct Fit to Nature: An Evolutionary Perspective on Biological and Artificial Neural Networks</article-title>. <source>Neuron</source>. <year>2020</year>;<volume>105</volume>(<issue>3</issue>):<fpage>416</fpage>–<lpage>34</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2019.12.002</pub-id>. PubMed PMID: <pub-id pub-id-type="pmid">32027833</pub-id>; PubMed Central PMCID: <pub-id pub-id-type="pmcid">PMC7096172</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Donhauser</surname> <given-names>PW</given-names></string-name>, <string-name><surname>Baillet</surname> <given-names>S</given-names></string-name>. <article-title>Two distinct neural timescales for predictive speech processing</article-title>. <source>Neuron</source>. <year>2020</year>;<volume>105</volume>(<issue>2</issue>):<fpage>385</fpage>–<lpage>93</lpage>. e9.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Rabovsky</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hansen</surname> <given-names>SS</given-names></string-name>, <string-name><surname>McClelland</surname> <given-names>JL</given-names></string-name>. <article-title>Modelling the N400 brain potential as change in a probabilistic representation of meaning</article-title>. <source>Nature Human Behaviour</source>. <year>2018</year>;<volume>2</volume>(<issue>9</issue>):<fpage>693</fpage>–<lpage>705</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Heilbron</surname> <given-names>M</given-names></string-name>, <string-name><surname>Armeni</surname> <given-names>K</given-names></string-name>, <string-name><surname>Schoffelen</surname> <given-names>J-M</given-names></string-name>, <string-name><surname>Hagoort</surname> <given-names>P</given-names></string-name>, <string-name><surname>de Lange</surname> <given-names>FP</given-names></string-name>. <article-title>A hierarchy of linguistic predictions during natural language comprehension</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2022</year>;<volume>119</volume>(<issue>32</issue>):<fpage>e2201968119</fpage>. doi: doi:<pub-id pub-id-type="doi">10.1073/pnas.2201968119</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="confproc"><string-name><surname>Lai</surname> <given-names>G</given-names></string-name>, <string-name><surname>Xie</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hovy</surname> <given-names>E</given-names></string-name>. <conf-name>Race: Large-scale reading comprehension dataset from examinations. 2017 Conference on Empirical Methods in Natural Language Processing; 2017: ACL</conf-name>; <year>2017</year>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Chen</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bolton</surname> <given-names>J</given-names></string-name>, <string-name><surname>Manning</surname> <given-names>CD</given-names></string-name>. <article-title>A thorough examination of the cnn/daily mail reading comprehension task</article-title>. <source>54th annual meeting of the association for computational linguistics; 2016: ACL</source>; <year>2016</year>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="confproc"><string-name><surname>Devlin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>M-W</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toutanova</surname> <given-names>K.</given-names></string-name> <conf-name>Bert: Pre-training of deep bidirectional transformers for language understanding. 2019 Conference of the North American Chapter of the Association for Computational Linguistics; 2019: ACL</conf-name>; <year>2019</year>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Itti</surname> <given-names>L</given-names></string-name>, <string-name><surname>Koch</surname> <given-names>C</given-names></string-name>, <string-name><surname>Niebur</surname> <given-names>E</given-names></string-name>. <article-title>A model of saliency-based visual attention for rapid scene analysis</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>1998</year>;<volume>20</volume>(<issue>11</issue>):<fpage>1254</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Hale</surname> <given-names>J</given-names></string-name>. <article-title>Information-theoretical complexity metrics</article-title>. <source>Lang Linguist Compass</source>. <year>2016</year>;<volume>10</volume>(<issue>9</issue>):<fpage>397</fpage>–<lpage>412</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Tatler</surname> <given-names>BW</given-names></string-name>, <string-name><surname>Hayhoe</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Land</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Ballard</surname> <given-names>DH</given-names></string-name>. <article-title>Eye guidance in natural vision: reinterpreting salience</article-title>. <source>J Vis</source>. <year>2011</year>;<volume>11</volume>(<issue>5</issue>):<fpage>5</fpage>–<lpage>25</lpage>. Epub 2011/05/31. doi: <pub-id pub-id-type="doi">10.1167/11.5.5</pub-id>. PubMed PMID: <pub-id pub-id-type="pmid">21622729</pub-id>; PubMed Central PMCID: <pub-id pub-id-type="pmcid">PMCPMC3134223</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Borji</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sihite</surname> <given-names>DN</given-names></string-name>, <string-name><surname>Itti</surname> <given-names>L</given-names></string-name>. <article-title>Quantitative analysis of human-model agreement in visual saliency modeling: a comparative study</article-title>. <source>IEEE Trans Image Process</source>. <year>2013</year>;<volume>22</volume>(<issue>1</issue>):<fpage>55</fpage>–<lpage>69</lpage>. Epub 2012/08/08. PubMed PMID: <pub-id pub-id-type="pmid">22868572</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Anderson</surname> <given-names>P</given-names></string-name>, <string-name><surname>He</surname> <given-names>X</given-names></string-name>, <string-name><surname>Buehler</surname> <given-names>C</given-names></string-name>, <string-name><surname>Teney</surname> <given-names>D</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gould</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal> <article-title>Bottom-up and top-down attention for image captioning and visual question answering</article-title>. <source>The IEEE Conference on Computer Vision and Pattern Recognition; 2018: CVPR</source>; <year>2018</year>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="other"><string-name><surname>Xu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Ba</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kiros</surname> <given-names>R</given-names></string-name>, <string-name><surname>Cho</surname> <given-names>K</given-names></string-name>, <string-name><surname>Courville</surname> <given-names>A</given-names></string-name>, <string-name><surname>Salakhudinov</surname> <given-names>R</given-names></string-name>, <etal>et al.</etal> <article-title>Show, attend and tell: Neural image caption generation with visual attention</article-title>. <source>32nd International Conference on Machine Learning; 2015: PMLR</source>; <year>2015</year>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Das</surname> <given-names>A</given-names></string-name>, <string-name><surname>Agrawal</surname> <given-names>H</given-names></string-name>, <string-name><surname>Zitnick</surname> <given-names>L</given-names></string-name>, <string-name><surname>Parikh</surname> <given-names>D</given-names></string-name>, <string-name><surname>Batra</surname> <given-names>D</given-names></string-name>. <article-title>Human attention in visual question answering: Do humans and deep networks look at the same regions?</article-title> <source>Comput Vis Image Underst</source>. <year>2017</year>;<volume>163</volume>:<fpage>90</fpage>–<lpage>100</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Wolfe</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Horowitz</surname> <given-names>TS</given-names></string-name>. <article-title>Five factors that guide attention in visual search</article-title>. <source>Nat Hum Behav</source>. <year>2017</year>;<volume>1</volume>(<issue>3</issue>):<fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Engbert</surname> <given-names>R</given-names></string-name>, <string-name><surname>Nuthmann</surname> <given-names>A</given-names></string-name>, <string-name><surname>Richter</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Kliegl</surname> <given-names>R</given-names></string-name>. <article-title>SWIFT: A dynamical model of saccade generation during reading</article-title>. <source>Psychol Rev</source>. <year>2005</year>;<volume>112</volume>(<issue>4</issue>):<fpage>777</fpage>–<lpage>813</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Mancheva</surname> <given-names>L</given-names></string-name>, <string-name><surname>Reichle</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Lemaire</surname> <given-names>B</given-names></string-name>, <string-name><surname>Valdois</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ecalle</surname> <given-names>J</given-names></string-name>, <string-name><surname>Guérin-Dugué</surname> <given-names>A</given-names></string-name>. <article-title>An analysis of reading skill development using EZ Reader</article-title>. <source>J Cogn Psychol</source>. <year>2015</year>;<volume>27</volume>(<issue>5</issue>):<fpage>657</fpage>–<lpage>76</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Reichle</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Pollatsek</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>. <article-title>Using EZ Reader to simulate eye movements in nonreading tasks: A unified framework for understanding the eye–mind link</article-title>. <source>Psychol Rev</source>. <year>2012</year>;<volume>119</volume>(<issue>1</issue>):<fpage>155</fpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Hermann</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Kocisky</surname> <given-names>T</given-names></string-name>, <string-name><surname>Grefenstette</surname> <given-names>E</given-names></string-name>, <string-name><surname>Espeholt</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kay</surname> <given-names>W</given-names></string-name>, <string-name><surname>Suleyman</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal> <article-title>Teaching machines to read and comprehend</article-title>. <source>Advances in Neural Information Processing Systems</source>; <year>2015</year>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Hahn</surname> <given-names>M</given-names></string-name>, <string-name><surname>Keller</surname> <given-names>F</given-names></string-name>. <article-title>Modeling task effects in human reading with neural network-based attention</article-title>. <source>Cognition</source>. <year>2023</year>;<volume>230</volume>:<fpage>105289</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cognition.2022.105289</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Yeari</surname> <given-names>M</given-names></string-name>, <string-name><surname>van den Broek</surname> <given-names>P</given-names></string-name>, <string-name><surname>Oudega</surname> <given-names>M</given-names></string-name>. <article-title>Processing and memory of central versus peripheral information as a function of reading goals: Evidence from eye-movements</article-title>. <source>Read Writ</source>. <year>2015</year>;<volume>28</volume>(<issue>8</issue>):<fpage>1071</fpage>–<lpage>97</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><stsring-name><surname>Grabe</surname> <given-names>M</given-names></stsring-name>. <article-title>Reader imposed structure and prose retention</article-title>. <source>Contemporary Educational Psychology</source>. <year>1979</year>;<volume>4</volume>(<issue>2</issue>):<fpage>162</fpage>–<lpage>71</lpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Kaakinen JK</surname>, <given-names>HyönäJ</given-names></string-name>, <string-name><surname>Keenan</surname> <given-names>JM</given-names></string-name>. <article-title>Perspective effects on online text processing</article-title>. <source>Discourse processes</source>. <year>2002</year>;<volume>33</volume>(<issue>2</issue>):<fpage>159</fpage>–<lpage>73</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Kaakinen JK</surname>, <given-names>HyönäJ</given-names></string-name>, <string-name><surname>Keenan</surname> <given-names>JM</given-names></string-name>. <article-title>How prior knowledge, WMC, and relevance of information affect eye fixations in expository text</article-title>. <source>J Exp Psychol</source>. <year>2003</year>;<volume>29</volume>(<issue>3</issue>):<fpage>447</fpage>–<lpage>57</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="confproc"><string-name><surname>Yang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Dyer</surname> <given-names>C</given-names></string-name>, <string-name><surname>He</surname> <given-names>X</given-names></string-name>, <string-name><surname>Smola</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hovy</surname> <given-names>E</given-names></string-name>. <conf-name>Hierarchical attention networks for document classification. 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies; 2016: ACL</conf-name>; <year>2016</year>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="other"><string-name><surname>Lin</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Feng</surname> <given-names>M</given-names></string-name>, <string-name><surname>Santos</surname> <given-names>CNd</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Xiang</surname> <given-names>B</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>B</given-names></string-name>, <etal>et al.</etal> <article-title>A structured self-attentive sentence embedding. International Conference on Learning Representations; 2017: ICLR</article-title>; <year>2017</year>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="confproc"><string-name><surname>Serrano</surname> <given-names>S</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>NA</given-names></string-name>. <conf-name>Is Attention Interpretable? 57th Annual Meeting of the Association for Computational Linguistics; 2019: Association for Computational Linguistics</conf-name>; <year>2019</year>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="book"><string-name><surname>Jain</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wallace</surname> <given-names>BC</given-names></string-name>. <collab>Attention is not Explanation</collab>. <chapter-title>2019 Conference of the North American Chapter of the Association for Computational Linguistics; 2019 jun; Minneapolis</chapter-title>, <publisher-loc>Minnesota</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Bolotova</surname> <given-names>V</given-names></string-name>, <string-name><surname>Blinov</surname> <given-names>V</given-names></string-name>, <string-name><surname>Zheng</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Croft</surname> <given-names>WB</given-names></string-name>, <string-name><surname>Scholer</surname> <given-names>F</given-names></string-name>, <string-name><surname>Sanderson</surname> <given-names>M</given-names></string-name>. <article-title>Do People and Neural Nets Pay Attention to the Same Words: Studying Eye-tracking Data for Non-factoid QA Evaluation</article-title>. <source>29th ACM International Conference on Information &amp; Knowledge Management; 2020: ACM</source>; <year>2020</year>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Malmaud</surname> <given-names>J</given-names></string-name>, <string-name><surname>Levy</surname> <given-names>R</given-names></string-name>, <string-name><surname>Berzak</surname> <given-names>Y</given-names></string-name>. <article-title>Bridging Information-Seeking Human Gaze and Machine Reading Comprehension</article-title>. <source>arXiv</source>. <year>2020</year>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="confproc"><string-name><surname>Sood</surname> <given-names>E</given-names></string-name>, <string-name><surname>Tannert</surname> <given-names>S</given-names></string-name>, <string-name><surname>Frassinelli</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bulling</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vu</surname> <given-names>NT</given-names></string-name>. <conf-name>Interpreting attention models with human visual attention in machine reading comprehension. 24th Conference on Computational Natural Language Learning</conf-name>; <year>2020</year>: <collab>Association for Computational Linguistics</collab>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="web"><string-name><surname>Zou</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Jin</surname> <given-names>P</given-names></string-name>, <string-name><surname>Luo</surname> <given-names>C</given-names></string-name>, <string-name><surname>Pan</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ding</surname> <given-names>N</given-names></string-name>. <article-title>PALRACE: Reading Comprehension Dataset with Human Data and Labeled Rationales2021</article-title>. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2106.12373">https://arxiv.org/abs/2106.12373</ext-link>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name>. <article-title>The psychophysics toolbox</article-title>. <source>Spat Vis</source>. <year>1997</year>;<volume>10</volume>(<issue>4</issue>):<fpage>433</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="other"><string-name><surname>Levenshtein</surname> <given-names>VI</given-names></string-name>. <article-title>Binary codes capable of correcting deletions, insertions, and reversals</article-title>. <source>Soviet physics doklady</source>; <year>1966</year>: <publisher-loc>Soviet Union</publisher-loc>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="confproc"><string-name><surname>Pennington</surname> <given-names>J</given-names></string-name>, <string-name><surname>Socher</surname> <given-names>R</given-names></string-name>, <string-name><surname>Manning</surname> <given-names>C</given-names></string-name>. <conf-name>Glove: Global vectors for word representation. 2014 Conference on Empirical Methods in Natural Language Processing</conf-name>; <year>2014</year>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><string-name><surname>Zhu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Kiros</surname> <given-names>R</given-names></string-name>, <string-name><surname>Zemel</surname> <given-names>R</given-names></string-name>, <string-name><surname>Salakhutdinov</surname> <given-names>R</given-names></string-name>, <string-name><surname>Urtasun</surname> <given-names>R</given-names></string-name>, <string-name><surname>Torralba</surname> <given-names>A</given-names></string-name>, <etal>et al.</etal> <article-title>Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</article-title>. <source>Proceedings of the IEEE international conference on computer vision</source>; <year>2015</year>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="book"><string-name><surname>Wolf</surname> <given-names>T</given-names></string-name>, <string-name><surname>Debut</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sanh</surname> <given-names>V</given-names></string-name>, <string-name><surname>Chaumond</surname> <given-names>J</given-names></string-name>, <string-name><surname>Delangue</surname> <given-names>C</given-names></string-name>, <string-name><surname>Moi</surname> <given-names>A</given-names></string-name>, <etal>et al.</etal> <chapter-title>HuggingFace’s Transformers: State-of-the-art natural language processing</chapter-title>. <source>2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations; 2019</source>: <publisher-name>Association for Computational Linguistics</publisher-name>; <year>2020</year>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>X</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>X</given-names></string-name>. <article-title>DCMN+: Dual co-matching network for multi-choice reading comprehension</article-title>. <source>AAAI conference on artificial intelligence; 2019: AAAI</source> <year>2020</year>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="other"><string-name><surname>Ran</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Li</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>W</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>J</given-names></string-name>. <article-title>Option comparison network for multiple-choice reading comprehension</article-title>. <source>arXiv</source>. <year>2019</year>. doi: arXiv:1903.03033.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><string-name><surname>Vaswani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname> <given-names>N</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gomez</surname> <given-names>AN</given-names></string-name>, <etal>et al.</etal> <article-title>Attention is all you need</article-title>. <source>Advances in Neural Information Processing Systems</source>; <year>2017</year>: Curran Associates; 2017.</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><surname>Clark</surname> <given-names>K</given-names></string-name>, <string-name><surname>Khandelwal</surname> <given-names>U</given-names></string-name>, <string-name><surname>Levy</surname> <given-names>O</given-names></string-name>, <string-name><surname>Manning</surname> <given-names>CD</given-names></string-name>. <article-title>What does BERT look at? An analysis of BERT’s attention</article-title>. <source>2019 ACL workshop blackboxNLP: Analyzing and interpreting neural networks for NLP; 2019: ACL</source>; <year>2019</year>.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="confproc"><string-name><surname>Stolcke</surname> <given-names>A</given-names></string-name>. <conf-name>SRILM-an extensible language modeling toolkit. Seventh International Conference on Spoken Language Processing</conf-name>; <year>2002</year>.</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="other"><string-name><surname>Radford</surname> <given-names>A</given-names></string-name>, <string-name><surname>Narasimhan</surname> <given-names>K</given-names></string-name>, <string-name><surname>Salimans</surname> <given-names>T</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name>. <article-title>Improving language understanding by generative pre-training</article-title>. <source>arXiv</source>. <year>2018</year>.</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="book"><string-name><surname>Efron</surname> <given-names>B</given-names></string-name>, <string-name><surname>Tibshirani</surname><given-names>RJ</given-names></string-name>. <source>An introduction to the bootstrap</source>: <publisher-name>CRC press</publisher-name>; <year>1994</year>.</mixed-citation></ref>
</ref-list>
<sec id="s11">
<title>Supplementary Material</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S1.</label>
<caption><title>Illustration of the word-level heuristic models and the RNN-based SAR model.</title>
<p>(<bold>A</bold>) The orthographic and semantic models calculate the word-wise similarities between all words in the integrated option and all words in the passage, forming a similarity matrix. The similarity measures used in the orthographic and semantic models are the edit distance and correlation between word embeddings, respectively. For each option, the similarity matrix is averaged across all rows and all columns to form a scalar decision score. The option with the largest decision score is chosen as the answer. (<bold>B</bold>) The SAR model uses bi-directional RNNs to encode contextual information. A vectorial representation for the passage is created using the weighted sum of the vectorial representation of each word, and the weight on each word, i.e., the attention weight, is calculated based on its similarity to the vectorial representation of the question. The summarized passage representation and the option representation is used to form the decision score with a bilinear dot layer.</p></caption>
<graphic xlink:href="538252v2_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S2.</label>
<caption><title>Question answering accuracy for individual transformer-based models.</title>
<p>Human results and other computational models are also plotted for comparison. *_pre: pre-trained transformer-based models; *_fine: transformer-based models fine-tuned on the goal-directed reading task.</p></caption>
<graphic xlink:href="538252v2_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S3.</label>
<caption><title>Transformer-based models can explain word reading time even when the influences of text features and question relevance are regressed out.</title>
<p>(<bold>A</bold>) Predict the raw word reading time using the attention weights of individual transformer-based models. Results from other computational models are also plotted for comparison. (<bold>B</bold>) Predict the residual word reading time when basic text features, i.e., layout and word features, are regressed out. (C) Predict the residual word reading time when both basic text features and question relevance are regressed out. Prediction accuracy significantly higher than chance is denoted by stars of the same color as the bar. *_rand: transformer-base models with randomized parameters; *_pre: pre-trained transformer-based models; *_fine: transformer-based models fine-tuned on the goal-directed reading task. *P &lt; 0.05; **P &lt; 0.01.</p></caption>
<graphic xlink:href="538252v2_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S4.</label>
<caption><title>Weights on individual attention heads in the linear regression when predicting human word reading time.</title>
<p>The weights of the linear regression are normalized by their maximum value. The light-colored dots denote the weights on each head, and the dark-colored dots represent the mean weight within a layer. trans_rand: transformer-base models with randomized parameters; trans_pre: pre-trained transformer-based models; trans_fine: transformer-based models fine-tuned on the goal-directed reading task.</p></caption>
<graphic xlink:href="538252v2_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S5.</label>
<caption><title>Properties of the question relevance of words.</title>
<p>(<bold>A</bold>) Question relevance as a function of the word position within a passage. For global but not local questions, question-relevant words concentrate near the beginning of a passage. (<bold>B</bold>) Decay of mean question relevance across lines. The question relevance is averaged within each line, and all lines in a passage are sorted based on the mean question relevance in descending order. Therefore, line 1 is the line with the highest question relevance, and line 2 is the line with the 2<sup>nd</sup> highest question relevance. For both global and local questions, the mean question relevance sharply decreases over lines. (<bold>C</bold>) The mean word length, in terms of the number of letters, for words with the question relevance greater or smaller than 0.1. Words of higher relevance are generally longer. (<bold>D</bold>) Percentage of content words for words with higher or lower question relevance. Question-relevant words are more often content words.</p></caption>
<graphic xlink:href="538252v2_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S6.</label>
<caption><p>Passage beginning effects (A) and question relevance effects (B) in early and late reading stages. The passage beginning effect differ between global and local questions mainly in the late reading stage reflected by the counts of rereading. The question-relevance effect is also only reliably observed in the late reading stage.</p></caption>
<graphic xlink:href="538252v2_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S7.</label>
<caption><title>Factors influencing attention weights in each layer of DNNs for local questions.</title>
<p>Similar results are observed for all 3 models: The sensitivity to text features decreases from shallow to deep layers, while the sensitivity to question relevance increases across layers. trans_rand: transformer-base models with randomized parameters; trans_pre: pre-trained transformer-based models; trans_fine: transformer-based models fine-tuned on the goal-directed reading task.</p></caption>
<graphic xlink:href="538252v2_figs7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S8.</label>
<caption><title>Factors influencing attention weights in each layer of DNNs for global questions.</title>
<p>Similar results are observed for all 3 models: The sensitivity to text features decreases from shallow to deep layers, while the sensitivity to question relevance increases across layers. trans_rand: transformer-base models with randomized parameters; trans_pre: pre-trained transformer-based models; trans_fine: transformer-based models fine-tuned on the goal-directed reading task.</p></caption>
<graphic xlink:href="538252v2_figs8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S9.</label>
<caption><title>Factors influencing human reading in different processing stages in Experiments 2-4.</title>
<p>The early and late stages are separately characterized by gaze duration, i.e., duration for the first reading of a word, and counts of rereading, respectively. *P &lt; 0.05; **P &lt; 0.01; ***P &lt; 0.001.</p></caption>
<graphic xlink:href="538252v2_figs9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs10" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S10.</label>
<caption><title>Passage-beginning and question-relevance effects in 4 experiments.</title>
<p>The passage beginning effect was quantified by the ratio between the mean word reading time on the first 3 lines of a passage and the mean word reading time on other lines. The question relevance effect was quantified by the ratio between mean word reading time on the line that was most relevant to the question and lines that were more than 5 lines away. See <xref rid="fig1" ref-type="fig">Fig. 1</xref> for the explanation for the box plots. **P &lt; 0.01; ***P &lt; 0.001.</p></caption>
<graphic xlink:href="538252v2_figs10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>P-values for the model prediction of word reading time.</title></caption>
<graphic xlink:href="538252v2_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Table S2.</label>
<caption><title>P-values for the prediction of word reading time using text or task-related features.</title></caption>
<graphic xlink:href="538252v2_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls3" orientation="portrait" position="float">
<label>Table S3.</label>
<caption><title>P-values for the prediction of early and late eye tracking measures using text or task-related features.</title></caption>
<graphic xlink:href="538252v2_tbls3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls4" orientation="portrait" position="float">
<label>Table S4.</label>
<caption><title>P-values for the prediction of word reading time for all 4 experiments.</title></caption>
<graphic xlink:href="538252v2_tbls4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls5" orientation="portrait" position="float">
<label>Table S5.</label>
<caption><title>P-values for the comparisons between experiments.</title></caption>
<graphic xlink:href="538252v2_tbls5.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls6" orientation="portrait" position="float">
<label>Table S6.</label>
<caption><title>Hyperparameters for DNN fine-tuning.</title>
<p>We adapted these hyperparamemers from references [<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c4">4</xref>].</p></caption>
<graphic xlink:href="538252v2_tbls6.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<sec id="s9">
<title>Supplementary Methods</title>
<p>To characterize the influences of different factors on human word reading time, we employed linear mixed effects models [<xref ref-type="bibr" rid="c5">5</xref>] implemented in the lmerTest package [<xref ref-type="bibr" rid="c6">6</xref>] of R. For the baseline model, we treated the type of questions (local vs. global; local = baseline) and all text/task-related features as fixed factors, and considered the interaction between the type of questions and these text/task-related features. We included participants and items (i.e., questions) as random factors, each with associated random intercepts. The formulation of the baseline model was: <italic>reading-time</italic> ∼ <italic>ParagraphNumber</italic> * <italic>QuestionType</italic> + <italic>LineNumberInPassage</italic> * <italic>QuestionType</italic> + <italic>LeftMostPixel</italic> * <italic>QuestionType</italic> + <italic>LineNumberInParagraph</italic> * <italic>QuestionType</italic> + <italic>LogWordFreq</italic> * <italic>QuestionType</italic> + <italic>WordLength</italic> * <italic>QuestionType</italic> + <italic>Surprisal</italic> * <italic>QuestionType</italic> + <italic>QuestionRelevance</italic> * <italic>QuestionType</italic> + (1 | <italic>Participant</italic>) + (1 | <italic>question</italic>). Additionally, starting from the baseline model, we augmented the baseline model by adding DNN attention as additional fixed factors. This augmentation facilitated an examination of whether DNN attention demonstrated a statistically significant contribution to the prediction of human word reading time. Notably, the DNN attention was derived from diverse sources, including SAR, randomized BERT, pre-trained BERT, and fine-tuned BERT.</p>
</sec>
<sec id="s10">
<title>Supplementary Results</title>
<p>The baseline mixed model revealed significant fixed effects for question type and all text/task-related features, as well as significant interactions between question type and these text/task-related features (<xref rid="tbls7" ref-type="table">Table S7</xref>). Upon involving SAR attention, we observed a statistically significant fixed effect associated with SAR attention. When involving attention weights of randomly initialized BERT, the mixed model revealed that most attention heads exhibited significant fixed effects, suggesting their contributions to the prediction of human word reading time. A broader range of attention heads showed significant fixed effects for both pre-trained and fine-tuned BERT.</p>
<table-wrap id="tbls7" orientation="portrait" position="float">
<label>Table S7.</label>
<caption><title>Linear mixed effects modeling of human word reading time.</title>
<p>The question type is coded as 0 (local question) or 1 (global question), and other factors are continuous regressors. Given the substantial number of attention weights in BERT (i.e., 144), we present the 1<sup>st</sup> quartile and 3<sup>rd</sup> quartile values for b, SE, and t and report the ratio of attention weights that reach significant level.</p></caption>
<graphic xlink:href="538252v2_tbls7.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="538252v2_tbls7a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="sc1"><label>1.</label><mixed-citation publication-type="confproc"><string-name><surname>Lan</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Goodman</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gimpel</surname> <given-names>K</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>P</given-names></string-name>, <string-name><surname>Soricut</surname> <given-names>R</given-names></string-name>. <conf-name>Albert: A lite bert for self-supervised learning of language representations. International Conference on Learning Representations; 2019: ICLR</conf-name>; <year>2020</year>.</mixed-citation></ref>
<ref id="sc2"><label>2.</label><mixed-citation publication-type="other"><string-name><surname>Liu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Ott</surname> <given-names>M</given-names></string-name>, <string-name><surname>Goyal</surname> <given-names>N</given-names></string-name>, <string-name><surname>Du</surname> <given-names>J</given-names></string-name>, <string-name><surname>Joshi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal> <article-title>Roberta: A robustly optimized bert pretraining approach</article-title>. arXiv. <year>2019</year>. doi: arXiv:1907.11692.</mixed-citation></ref>
<ref id="sc3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>X</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>X</given-names></string-name>. <article-title>DCMN+: Dual co-matching network for multi-choice reading comprehension</article-title>. <source>AAAI conference on artificial intelligence; 2019: AAAI</source> <year>2020</year>.</mixed-citation></ref>
<ref id="sc4"><label>4.</label><mixed-citation publication-type="other"><string-name><surname>Ran</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Li</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>W</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>J</given-names></string-name>. <article-title>Option comparison network for multiple-choice reading comprehension</article-title>. <source>arXiv</source>. <year>2019</year>. doi: arXiv:1903.03033.</mixed-citation></ref>
<ref id="sc5"><label>5.</label><mixed-citation publication-type="other"><string-name><surname>Pinheiro</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bates</surname> <given-names>D</given-names></string-name>. <article-title>Mixed-effects models in S and S-PLUS: Springer science &amp; business media</article-title>; <year>2006</year>.</mixed-citation></ref>
<ref id="sc6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Kuznetsova</surname> <given-names>A</given-names></string-name>, <string-name><surname>Brockhoff</surname> <given-names>PB</given-names></string-name>, <string-name><surname>Christensen</surname> <given-names>RHB</given-names></string-name>. <article-title>lmerTest Package: Tests in Linear Mixed Effects Models</article-title>. <source>Journal of Statistical Software</source>. <year>2017</year>;<volume>82</volume>(<issue>13</issue>):<fpage>1</fpage>–<lpage>26</lpage>. doi: <pub-id pub-id-type="doi">10.18637/jss.v082.i13</pub-id>.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1"><label>1</label><p>The National College English Test (CET) is a national English test system developed to examine the English proficiency of college students in China. The CET includes tests of two levels: a lower level test CET4 and a higher level test CET6.</p></fn>
</fn-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87197.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Hang</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study provides a <bold>valuable</bold> contribution to the study of eye-movements in reading, revealing that attention-weights from a deep neural network show a statistically reliable fit to the word-level reading patterns of humans. Its evidence is <bold>convincing</bold> and strengthens a line of research arguing that attention in reading reflects task optimization. The work would be of interest to psychologists, neuroscientists, and machine learning researchers.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87197.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This manuscript describes a set of four passage-reading experiments which are paired with computational modeling to evaluate how task-optimization might modulate attention during reading. Broadly, participants show faster reading and modulated eye-movement patterns of short passages when given a preview of a question they will be asked. The attention weights of a Transformer-based neural network (BERT and variants) show a statistically reliable fit to these reading patterns above-and-beyond text- and semantic-similarity baseline metrics, as well as a recurrent-network-based baseline. Reading strategies are modulated when questions are not previewed, and when participants are L1 versus L2 readers, and these patterns are also statistically tracked by the same transformer-based network.</p>
<p>Strengths:</p>
<p>- Task-optimization is a key notion in current models of reading and the current effort provides a computationally rigorous account of how such task effects might be modeled</p>
<p>
- Multiple experiments provide reasonable effort towards generalization across readers and different reading scenarios</p>
<p>
- Use of RNN-based baseline, text-based features, and semantic features provides a useful baseline for comparing Transformer-based models like BERT</p>
<p>Weaknesses:</p>
<p>- Generalization across neural network models may be limited (models differ in size, training data etc.); it is thus not always clear which specific model characteristics support their fit to human reading patterns.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87197.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this study, researchers aim to understand the computational principles behind attention allocation in goal-directed reading tasks. They explore how deep neural networks (DNNs) optimized for reading tasks can predict reading time and attention distribution. The findings show that attention weights in transformer-based DNNs predict reading time for each word. Eye tracking reveals that readers focus on basic text features and question-relevant information during initial reading and rereading, respectively. Attention weights in shallow and deep DNN layers are separately influenced by text features and question relevance. Additionally, when readers read without a specific question in mind, DNNs optimized for word prediction tasks can predict their reading time. Based on these findings, the authors suggests that attention in real-world reading can be understood as a result of task optimization.</p>
<p>Strengths of the Methods and Results:</p>
<p>
The present study employed stimuli consisting of paragraphs read by middle and high school students, covering a wide range of diverse topics. This choice ensured that the reading experience for participants remained natural, ultimately enhancing the ecological validity of the findings and conclusions.</p>
<p>In Experiments 1-3, participants were instructed to read questions before the text, while in Experiment 4 participants were instructed to read questions after the text. This deliberate manipulation allowed the paper to assess how different reading task conditions influence reading and eye movements.</p>
<p>Weaknesses of the Methods and Results:</p>
<p>While the study benefits from several strengths, it is important to acknowledge its limitations. Notably, recent months have seen significant advancements in Deep Neural Network (DNN) models, including the development of models such as GPT-3.5 and GPT-4, which have demonstrated remarkable capabilities in tasks resembling human cognition, like Theory of Mind. However, as the code for these cutting-edge models was not publicly accessible, they were unable to evaluate whether the attention mechanisms in the most up-to-date DNN models could provide improved predictions for human eye-movement data. This constraint represents a limitation in the investigation.</p>
<p>The methods and data presented in this study are valuable for gaining insights into the psychological mechanisms of reading. Moreover, the data provided in this paper may prove instrumental in enhancing the performance of future DNN models.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87197.2.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper presents several eyetracking experiments measuring task-directed reading behavior where subjects read texts and answered questions. It then models the measured reading times using attention patterns derived from deep-neural network models from the natural language processing literature. Results are taken to support the theoretical claim that human reading reflects task-optimized attention allocation.</p>
<p>Strengths:</p>
<p>(1) The paper leverages modern machine learning to model a high-level behavioral task (reading comprehension). While the claim that human attention reflects optimal behavior is not new, the paper considers a substantially more high-level task in comparison to prior work. The paper leverages recent models from the NLP literature which are known to provide strong performance on such question-answering tasks, and is methodologically well grounded in the NLP literature.</p>
<p>(2) The modeling uses text- and question-based features in addition to DNNs, specifically evaluates relevant effects, and compares vanilla pretrained and task-finetuned models. This makes the results more transparent and helps assess the contributions of task optimization. In particular, besides fine-tuned DNNs, the role of the task is further established by directly modeling the question relevance of each word. Specifically, the claim that human reading is predicted better by task-optimized attention distributions rests on (i) a role of question relevance in influencing reading in Expts 1-2 but not 4, and (ii) the fact that fine-tuned DNNs improve prediction of gaze in Expts 1-2 but not 4.</p>
<p>(3) The paper conducts experiments on both L2 and L1 speakers.</p>
<p>Weaknesses:</p>
<p>(1) Under the hypothesis advanced, human reading should adapt rationally to task demands. Indeed, Experiment 1 tests questions from different types in blocks (local and global), and the paper provides evidence that this encourages the development of question-type-specific reading strategies -- indeed, this specifically motivates Experiment 2, and is confirmed indirectly in the comparison of the effects found in the two experiments (&quot;all these results indicated that the readers developed question-type-specific strategies in Experiment 1&quot;). On the other hand, finetuning the model on one of the two types does not seem to reproduce this differential behavior, in the sense that fit to reading data is not improved. In this sense, the model seems to have limited abilities in reproducing the observed task dependence of human reading.</p>
<p>The results support the conclusions well, with the weakness described above a limitation of the modeling approach chosen.</p>
<p>The data are likely to be useful as a benchmark in further modeling of eye-movements, an area of interest to computational research on psycholinguistics.</p>
<p>
The modeling results contribute to theoretical understanding of human reading behavior, and strengthens a line of research arguing that it reflects task-adaptive behavior.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87197.2.sa4</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zou</surname>
<given-names>Jiajie</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Yuran</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2744-1490</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Jialu</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tian</surname>
<given-names>Xing</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1629-6304</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Ding</surname>
<given-names>Nai</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1:</bold></p>
<p>This manuscript describes a set of four passage-reading experiments which are paired with computational modeling to evaluate how task-optimization might modulate attention during reading. Broadly, participants show faster reading and modulated eye-movement patterns of short passages when given a preview of a question they will be asked. The attention weights of a Transformerbased neural network (BERT and variants) show a statistically reliable fit to these reading patterns above-and-beyond text- and semantic-similarity baseline metrics, as well as a recurrent-networkbased baseline. Reading strategies are modulated when questions are not previewed, and when participants are L1 versus L2 readers, and these patterns are also statistically tracked by the same transformer-based network.</p>
<p>I should note that I served as a reviewer on an earlier version of this manuscript at a different venue. I had an overall positive view of the paper at that point, and the same opinion holds here as well.</p>
<p>Strengths:</p>
<list list-type="bullet">
<list-item><p>Task-optimization is a key notion in current models of reading and the current effort provides a computationally rigorous account of how such task effects might be modeled</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Multiple experiments provide reasonable effort towards generalization across readers and different reading scenarios</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Use of RNN-based baseline, text-based features, and semantic features provides a useful baseline for comparing Transformer-based models like BERT</p>
</list-item></list>
</disp-quote>
<p>Thank you for the accurate summary and positive evaluation.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>1. Generalization across neural network models seems, to me, somewhat limited: The transformerbased models differ from baseline models in numerous ways (model size, training data, scoring algorithm); it is thus not clear what properties of these models necessarily supports their fit to human reading patterns.</p>
</disp-quote>
<p>Thank you for the insightful comment. To dissociate the effect of model architecture and the effect of training data, we have now compared the attention weights across three transformer-based models that have the same architecture but different training data/task: randomized (with all model parameters being randomized), pretrained, and fine-tuned models. Remarkably, even without training on any data, the attention weights in randomly initialized models exhibited significant similarity to human attention patterns (Figure. 3A). The predictive power of randomly initialized transformer-based models outperformed that of the SAR model. Through subsequent pre-training and fine-tuning, the predictive capacity of the models was further elevated. Therefore, both model architecture and the training data/task contribute to human-like attention distribution in the transformer models. We have now reported this result:</p>
<p>“The attention weights of randomly initialized transformer-based models could predict the human word reading time and the predictive power, which was around 0.3, was significantly higher than the chance level and the SAR (Fig. 3A, Table S1). The attention weights of pre-trained transformerbased models could also predict the human word reading time, and the predictive power was around 0.5, significantly higher than the predictive power of heuristic models, the SAR, and randomly initialized transformer-based models (Fig. 3A, Table S1). The predictive power was further boosted for local but not global questions when the models were fine-tuned to perform the goal-directed reading task (Fig. 3A, Table S1).”</p>
<p>In addition, we reported how training influenced the sensitivity of attention weights to text features and question relevance. As shown in Figure 4AB, attention in the randomized models were sensitive to text features across all layers. After pretraining, the models exhibited increased sensitivity to text features in the shallow layers, and decreased sensitivity to text features in deep layers. Subsequent finetuning on the reading comprehension task further attenuates the encoding of text features in deep layers but strengthens the sensitivity to task-relevant information.</p>
<disp-quote content-type="editor-comment">
<p>1. Inferential statistics are based on a series of linear regressions, but these differ markedly in model size (BERT models involve 144 attention-based regressor, while the RNN-based model uses just 1 attention-based regressor). How are improvements in model fit balanced against changes in model size?</p>
</disp-quote>
<p>Thank you for pointing out this issue. The performance of linear regressions was evaluated based on 5-fold cross-validation, and the performance we reported was the performance on the test set. To match the number of parameters, we have now predicted human attention using the average of all heads. The predictive power of the average head was still significantly higher than the predictive power of the SAR model. We have now reported this result in our revised manuscript:</p>
<p>“For the fine-tuned models, we also predict the human word reading time using an unweighted averaged of the 144 attention heads and the predictive power was 0.3, significantly higher than that achieved by the attention weights of SAR (P = 4 × 10-5, bootstrap).”</p>
<p>Also, it was not clear to me how participant-level variance was accounted for in the modeling effort (mixed-effects regression?) These questions may well be easily remedied by more complete reporting.</p>
<p>In the previous manuscript, the word reading time was averaged across participants, and we did not consider the variance between participants. We have now analyzed eye movements of each participant and used the linear mixed effects model to test how different factors affected human word reading time to account for participantslevel and item-level variances.</p>
<p>“Furthermore, a linear mixed effect model also revealed that more than 85% of the DNN attention heads contribute to the prediction of human reading time when considering text features and question relevance as covariates (Supplementary Results).”</p>
<p>“Supplementary Methods
To characterize the influences of different factors on human word reading time, we employed linear mixed effects models [5] implemented in the lmerTest package [6] of R. For the baseline model, we treated the type of questions (local vs. global; local = baseline) and all text/task-related features as fixed factors, and considered the interaction between the type of questions and these text/taskrelated features. We included participants and items (i.e., questions) as random factors, each with associated random intercepts…”</p>
<p>Supplementary Results
The baseline mixed model revealed significant fixed effects for question type and all text/task-related features, as well as significant interactions between question type and these text/task-related features (Table S7). Upon involving SAR attention, we observed a statistically significant fixed effect associated with SAR attention. When involving attention weights of randomly initialized BERT, the mixed model revealed that most attention heads exhibited significant fixed effects, suggesting their contributions to the prediction of human word reading time. A broader range of attention heads showed significant fixed effects for both pre-trained and fine-tuned BERT.</p>
<disp-quote content-type="editor-comment">
<p>1. Experiment 1 was paired with a relatively comprehensive discussion of how attention weights mapped to reading times, but the same sort of analysis was not reported for Exps 2-4; this seems like a missed opportunity given the broader interest in testing how reading strategies might change across the different parameters of the four experiments.</p>
</disp-quote>
<p>Thank you for the valuable suggestion. We have now also characterized how different reading measures, e.g., gaze duration and counts or rereading, were affected by text and task-related features in Experiments 2-4.</p>
<p>For Experiment 2:
“For local questions, consistent with Experiment 1, the effects of question relevance significantly increased from early to late processing stages that are separately indexed by gaze duration and counts of rereading (Fig. S9A, Table S3).”</p>
<p>For Experiment 3:
“For local questions, the layout effect was more salient for gaze duration than for counts of rereading. In contrast, the effect of word-related features and task relevance was more salient for counts of rereading than gaze duration (Fig. S9B, Table S3).”</p>
<p>For Experiment 4:
“Both the early and late processing stages of human reading were significantly affected by layout and word features, and the effects were larger for the late processing stage indexed by counts of rereading (Fig. S9C, Table S3).”</p>
<disp-quote content-type="editor-comment">
<p>1. Comparison of predictive power of BERT weights to human annotations of text relevance is limited: The annotation task asked participants to chose the 5 &quot;most relevant&quot; words for a given question; if &gt;5 words carried utility in answering a question, this would not be captured by the annotation. It seems to me that the improvement of BERT over human annotations discussed around page 10-11 could well be due to this arbitrary limitation of the annotations.</p>
</disp-quote>
<p>Thank you for the insightful comment. We only allowed a participant to label 5 words since we wanted the participant to only label the most important information. As the reviewer pointed out, five words may not be enough. However, this problem is alleviated by having &gt;26 annotators per question. Although each participant can label up to 5 words, pooling the results across &gt;26 annotators results in nonzero relevance rating for an average 21.1 words for local questions and 26.1 words for global question. More important, as was outlined in Experimental Materials, we asked additional participants to answer questions based on only 5 annotated keywords. The accuracy for question answering were 75.9% for global questions and 67.6% for local questions, which was close to the accuracy achieved when the complete passage was present (Fig. 1B), suggesting that even 5 keywords could support question answering.</p>
<disp-quote content-type="editor-comment">
<p>1. Abstract ln 35: This concluding sentence didn't really capture the key contribution of the paper which, at least from my perspective, was something closer to &quot;we offer a computational account of how task optimization modulates attention during reading&quot;</p>
<p>p 4 ln 66: I think this sentence does a good job capturing the main contributions of this paper</p>
</disp-quote>
<p>Thanks for your suggestion. We have modified our conclusion in Abstract accordingly.</p>
<disp-quote content-type="editor-comment">
<p>1. p 4 ln 81: &quot;therefore is conceptually similar&quot; maybe &quot;may serve a conceptually similar role&quot;</p>
</disp-quote>
<p>We have rewritten the sentence.</p>
<p>“Attention in DNN also functions as a mechanism to selectively extract useful information, and therefore attention may potentially serve a conceptually similar role in DNN.”</p>
<disp-quote content-type="editor-comment">
<p>1. p. 7 ln 140: &quot;disproportional to the reading time&quot; I didn't understand this sentence</p>
</disp-quote>
<p>Sorry for the confusion and we have rewritten the sentence.</p>
<p>“In Experiment 1, participants were allowed to read each passage for 2 minutes. Nevertheless, to encourage the participants to develop an effective reading strategy, the monetary reward the participant received decreased as they spent more time reading the passage (see Materials and Methods for details).”</p>
<disp-quote content-type="editor-comment">
<p>1. p 8 ln 151: This was another sentence that helped solidify the main research contributions for me; I wonder if this framing could be promoted earlier?</p>
</disp-quote>
<p>Thank you for the suggestion and we have moved the sentence to Introduction.</p>
<disp-quote content-type="editor-comment">
<p>1. p. 33: I may be missing something here, but I didn't follow the reasoning behind quantifying model fit against eye-tracking measures using accuracy in a permutation test. Models are assessed in terms of the proportion of random shuffles that show a greater statistical correlation. Does that mean that an accuracy value like 0.3 (p. 10 ln 208) means that 0.7 random permutations of word order led to higher correlations between attention weights and RT? Given that RT is continuous, I wonder if a measure of model fit such as RMSE or even R^2 could be more interpretable.</p>
</disp-quote>
<p>We have now realized that the term “prediction accuracy” was not clearly defined and have caused confusion. Therefore, in the revised manuscript, we have replaced this term with “predictive power”. Additionally, we have now introduced a clear definition of “prediction power” at its first mention in Result:</p>
<p>“…the predictive power, i.e., the Pearson correlation coefficient between the predicted and real word reading time, was around 0.2”</p>
<p>The permutation test was used to test if the predictive power is above chance. Specifically, if the predictive power is higher than the 95 percentile of the chancelevel predictive power estimated using permutations, the significant level (i.e., the p value) is 0.05. We have explained this in Statistical tests.</p>
<disp-quote content-type="editor-comment">
<p>1. p. 33: FDR-based multiple comparisons are noted several times, but wasn't clear to me what the comparison set is for any given test; more details would be helpful (e.g. X comparisons were conducted across passages/model-variants/whatever)</p>
</disp-quote>
<p>Sorry for missing this important information. We have now mentioned which comparisons are corrected,</p>
<p>“…Furthermore, the predictive power was higher for global than local questions (P = 4 × 10-5, bootstrap, FDR corrected for comparisons across 3 features, i.e., layout features, word features, and question relevance)…”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2:</bold></p>
<p>In this study, researchers aim to understand the computational principles behind attention allocation in goal-directed reading tasks. They explore how deep neural networks (DNNs) optimized for reading tasks can predict reading time and attention distribution. The findings show that attention weights in transformer-based DNNs predict reading time for each word. Eye tracking reveals that readers focus on basic text features and question-relevant information during initial reading and rereading, respectively. Attention weights in shallow and deep DNN layers are separately influenced by text features and question relevance. Additionally, when readers read without a specific question in mind, DNNs optimized for word prediction tasks can predict their reading time. Based on these findings, the authors suggest that attention in real-world reading can be understood as a result of task optimization.</p>
<p>The research question pursued by the study is interesting and important. The manuscript was well written and enjoyable to read. However, I do have some concerns.</p>
</disp-quote>
<p>We thank the reviewer for the accurate summary and positive evaluation.</p>
<disp-quote content-type="editor-comment">
<p>1. In the first paragraph of the manuscript, it appears that the purpose of the study was to test the optimization hypothesis in natural tasks. However, the cited papers mainly focus on covert visual attention, while the present study primarily focuses on overt attention (eye movements). It is crucial to clearly distinguish between these two types of attention and state that the study mainly focuses on overt attention at the beginning of the manuscript.</p>
</disp-quote>
<p>Thank you for pointing out this issue. We have explicitly mentioned that we focus on overt attention in the current study. Furthermore, we have also discussed that native readers may rely more on covert attention so that they do not need to spend more time overtly fixating at the task relevant words.</p>
<p>In Introduction:</p>
<p>“Reading is one of the most common and most sophisticated human behaviors [16, 17], and it is strongly regulated by attention: Since readers can only recognize a couple of words within one fixation, they have to overtly shift their fixation to read a line of text [3]. Thus, eye movements serve as an overt expression of attention allocation during reading [3, 18].”</p>
<p>In Discussion:</p>
<p>“Therefore, it is possible that when readers are more skilled and when the passage is relatively easy to read, their processing is so efficient so that they do not need extra time to encode task-relevant information and may rely on covert attention to prioritize the processing of task-relevant information.”</p>
<disp-quote content-type="editor-comment">
<p>1. The manuscript correctly describes attention in DNN as a mechanism to selectively extract useful information. However, eye-movement measures such as gaze duration and total reading time are primarily influenced by the time needed to process words. Therefore, there is a doubt whether the argument stating that attention in DNN is conceptually similar to the human attention mechanism at the computational level is correct. It is strongly suggested that the authors thoroughly discuss whether these concepts describe the same or different things.</p>
</disp-quote>
<p>Thank you for bringing up this very important issue and we have added discussions about why human and DNN may generate similar attention distributions. For example, we found that both DNN and human attention distributions are modulated by task relevance and word properties, which include word length, word frequency, and word surprisal. The influence of task relevance is relatively straightforward since both human readers and DNN should rely more on task relevant words to answer questions. The influence of word properties is less apparent for models than for human readers and we have added discussions:</p>
<p>For DNN’s sensitivity to word surprisal:</p>
<p>“The transformer-based DNN models analyzed here are optimized in two steps, i.e., pre-training and fine-tuning. The results show that pre-training leads to text-based attention that can well explain general-purpose reading in Experiment 4, while the fine-tuning process leads to goal-directed attention in Experiments 1-3 (Fig. 4B &amp; Fig. 5A). Pre-training is also achieved through task optimization, and the pre-training task used in all the three models analyzed here is to predict a word based on the context. The purpose of the word prediction task is to let models learn the general statistical regularity in a language based on large corpora, which is crucial for model performance on downstream tasks [21, 22, 33], and this process can naturally introduce the sensitivity to word surprisal, i.e., how unpredictable a word is given the context.”</p>
<p>For DNN’s sensitivity to word length:</p>
<p>“Additionally, the tokenization process in DNN can also contribute to the similarity between human and DNN attention distributions: DNN first separates words into tokens (e.g., “tokenization” is separated into “token” and “ization”). Tokens are units that are learned based on co-occurrence of letters, and is not strictly linked to any linguistically defined units. Since longer words tend to be separated into more tokens, i.e., fragments of frequently co-occurred letters, longer words receive more attention even if the model pay uniform attention to each of its input, i.e., a token.”</p>
<disp-quote content-type="editor-comment">
<p>1. When reporting how reading time was predicted by attention weights, the authors used &quot;prediction accuracy.&quot; While this measure is useful for comparing different models, it is less informative for readers to understand the quality of the prediction. It would be more helpful if the results of regression models were also reported.</p>
</disp-quote>
<p>Sorry for the confusion. The prediction accuracy was defined as the correlation coefficient between the predicted and actual eye-tracking measures. We have now realized that the term “prediction accuracy” might have caused confusion. Therefore, in the revised manuscript, we have replaced this term with “predictive power”. Additionally, we have now introduced a clear definition of “prediction power” at its first mention in Result:</p>
<p>“…the predictive power, i.e., the Pearson correlation coefficient between the predicted and real word reading time, was around 0.2”</p>
<disp-quote content-type="editor-comment">
<p>1. The motivations of Experiments 2 and 3 could be better described. In their current form, it is challenging to understand how these experiments contribute to understanding the major research question of the study.</p>
</disp-quote>
<p>Thank you for pointing out this issue. In Experiments 1, different types of questions were presented in separate blocks, and all the participants were L2 reader. Therefore, we conducted Experiments 2 and 3 to examine how reading behaviors were modulated when different types of questions were presented in a mixed manner, or when participants were L1 readers. We have now clarified the motivations:</p>
<p>“In Experiment 1, different types of questions were presented in blocks which encouraged the participants to develop question-type-specific reading strategies. Next, we ran Experiment 2, in which questions from different types were mixed and presented in a randomized order, to test whether the participants developed question-type-specific strategies in Experiment 1.”</p>
<p>“Experiments 1 and 2 recruited L2 readers. To investigate how language proficiency influenced task modulation of attention and the optimality of attention distribution, we ran Experiment 3, which was the same as Experiment 2 except that the participants were native English readers.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3:</bold></p>
<p>This paper presents several eyetracking experiments measuring task-directed reading behavior where subjects read texts and answered questions.</p>
<p>It then models the measured reading times using attention patterns derived from deep-neural network models from the natural language processing literature.</p>
<p>Results are taken to support the theoretical claim that human reading reflects task-optimized attention allocation.</p>
<p>STRENGTHS:</p>
<p>1. The paper leverages modern machine learning to model a high-level behavioral task (reading comprehension). While the claim that human attention reflects optimal behavior is not new, the paper considers a substantially more high-level task in comparison to prior work. The paper leverages recent models from the NLP literature which are known to provide strong performance on such question-answering tasks, and is methodologically well grounded in the NLP literature.</p>
<p>1. The modeling uses text- and question-based features in addition to DNNs, specifically evaluates relevant effects, and compares vanilla pretrained and task-finetuned models. This makes the results more transparent and helps assess the contributions of task optimization. In particular, besides finetuned DNNs, the role of the task is further established by directly modeling the question relevance of each word. Specifically, the claim that human reading is predicted better by task-optimized attention distributions rests on (i) a role of question relevance in influencing reading in Expts 1-2 but not 4, and (ii) the fact that fine-tuned DNNs improve prediction of gaze in Expts 1-2 but not 4.</p>
<p>1. The paper conducts experiments on both L2 and L1 speakers.</p>
</disp-quote>
<p>We thank the reviewer for the accurate summary and positive evaluation.</p>
<disp-quote content-type="editor-comment">
<p>WEAKNESSES:</p>
<p>1. The paper aims to show that human gaze is predicted the the DNN-derived task-optimal attention distribution, but the paper does not actually derive a task-optimal attention distribution. Rather, the DNNs are used to extract 144 different attention distributions, which are then put into a regression with coefficients fitted to predict human attention. As a consequence, the model has 144 free parameters without apparent a-priori constraint or theoretical interpretation. In this sense, there is a slight mismatch between what the modeling aims to establish and what it actually does.</p>
<p>Regarding Weakness (1): This weakness should be made explicit, at least by rephrasing line 90. The authors could also evaluate whether there is either a specific attention head, or one specific linear combination (e.g. a simple average of all heads) that predicts the human data well.</p>
</disp-quote>
<p>Thank you for pointing out this issue. One the one hand, we have now also predicted human attention using the average of all heads, i.e., the simple average suggested by the reviewer. The predictive power of the average head was still significantly higher than the predictive power of the SAR model. We have now reported this result in our revised manuscript.</p>
<p>“For the fine-tuned models, we also predict the human word reading time using an unweighted averaged of the 144 attention heads and the predictive power was 0.3, significantly higher than that achieved by the attention weights of SAR (P = 4 × 10-5, bootstrap).”</p>
<p>On the other hand, since different attention weights may contribute differently to the prediction of human reading time, we have now also reported the weights assigned to individual attention head during the original regression analysis (Fig. S4). It was observed that the weight was highly distributed across attention head and was not dominated by a single head.</p>
<p>Even more importantly, we have now rephrased the statement in line 90 of the previous manuscript:</p>
<p>“We employed DNNs to derive a set of attention weights that are optimized for the goal-directed reading task, and tested whether such optimal weights could explain human attention measured by eye tracking.”</p>
<p>Furthermore, in Discussion, we mentioned that:</p>
<p>“Furthermore, we demonstrate that both humans and transformer-based DNN models achieve taskoptimal attention distribution in multiple steps… Similarly, the DNN models do not yield a single attention distribution, and instead it generates multiple attention distributions, i.e., heads, for each layer. Here, we demonstrate that basic text features mainly modulate the attention weights in shallow layers, while the question relevance of a word modulates the attention weights in deep layers, reflecting hierarchical control of attention to optimize task performance. The attention weights in both the shallow and deep layers of DNN contribute to the explanation of human word reading time (Fig. S4).”</p>
<disp-quote content-type="editor-comment">
<p>1. While Experiment 1 tests questions from different types in blocks, and the paper mentions that this might encourage the development of question-type-specific reading strategies -- indeed, this specifically motivates Experiment 2, and is confirmed indirectly in the comparison of the effects found in the two experiments (&quot;all these results indicated that the readers developed question-typespecific strategies in Experiment 1&quot;) -- the paper seems to miss the opportunity to also test whether DNNs fine-tuned for each of the question-types predict specifically the reading times on the respective question types in Experiment 1. Testing not only whether DNN-derived features can differentially predict normal reading vs targeted reading, but also different targeted reading tasks, would be a strong test of the approach.</p>
<p>Regarding Weakness (2): results after finetuning for each question type could be reported.</p>
</disp-quote>
<p>Thank you for the valuable suggestion. We have now fine-tuned the models separately based on global and local questions. The detailed fine-tuning parameters employed in the fine-tuning process were presented in Author response table 1.</p>
<table-wrap id="sa4table1">
<label>Author response table 1.</label>
<caption>
<title>The hyperparameter for fine-tuning DNN models with specific question type.</title>
</caption>
<graphic mime-subtype="png" xlink:href="elife-87197-sa4-table1.png" mimetype="image"/>
</table-wrap>
<p>The fine-tuning process yielded a slight reduction in loss (i.e., the negative logarithmic score of the correct option) on the validation set. Specifically, for BERT, the loss decreased from 1.08 to 0.96; for ALBERT, it decreased from 1.16 to 0.76; for RoBERTa, it went down from 0.68 to 0.54. Nevertheless, the fine-tuning process did not improve the prediction of reading time (Author response image 1). A likely reason is that the number of global and local questions for training is limited (local questions: 520; global questions: 280), and similar questions also exist in RACE dataset that is used for the original fine tuning (sample size: 87,866). Therefore, a small number of questions can significantly change the reading strategy of human readers but using these questions to effectively fine-tune a model seems to be a more challenging task.</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<caption>
<title>Fine-tuning based on local and global questions does not significantly modulate the prediction of human reading time.</title>
<p>Lighter-color symbols show the results for the 3 BERT-family models (i.e., BERT, ALBERT, and RoBERTa) and the darker-color symbols show the average over the 3 BERT-family models. trans_fine: model fine-tuned based on the RACE dataset; trans_local: models additionally fine-tuned using local questions; trans_global: models additionally fine-tuned using global questions.</p>
</caption>
<graphic mime-subtype="png" xlink:href="elife-87197-sa4-fig1.png" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>1. The paper compares the DNN-derived features to word-related features such as frequency and surprisal and reports that the DNN features are predictive even when the others are regressed out (Figure S3). However, these features are operationalized in a way that puts them at an unfair disadvantage when compared to the DNNs: word frequency is estimated from the BNC corpus; surprisal is derived from the same corpus and derived using a trigram model. The BNC corpus contains 100 Million words, whereas BERT was trained on several Billions of words. Relatedly, trigram models are now far surpassed by DNN-based language models. Specifically, it is known that such models do not fit human eyetracking reading times as well as modern DNN-based models (e.g., Figure 2 Dundee in: Wilcox et al, On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior, CogSci 2020). This means that the predictive power of the word-related features is likely to be underestimated and that some residual predictive power is contained in the DNNs, which may implicitly compute quantities related to frequency and surprisal, but were trained on more data. In order to establish that the DNN models are predictive over and above word-related features, and to reliably quantify the predictive power gained by this, the authors could draw on (1) frequency estimated from the corpora used for BERT (BookCorpus + Wikipedia), (2) either train a strong DNN language model, or simply estimate surprisal from a strong off-the-shelf model such as GPT-2.</p>
<p>This concern does not fundamentally cast doubt on the conclusions, since the authors found a clear effect of the task relevance of individual words, which by definition is not contained in those baseline models. However, Figure S3 -- specifically Figure S3C -- is likely to inflate the contribution of the DNN model over and above the text-based features.</p>
</disp-quote>
<p>Thank you for pointing out these issues. Following the valuable suggestion of the reviewer, we have now 1) computed word frequencies based on BookCorpus and Wikipedia and 2) calculated word surprisal using GPT-2.</p>
<p>“The word features included word length, logarithmic word frequency estimated based on the BookCorpus [62] and English Wikipedia using SRILM [68], and word surprisal estimated from GPT-2 Medium [69].”</p>
<p>These recalculated word frequency and surprisal are correlated with the original measures (word frequency: 0.98; surprisal: 0.59), and the updated results are also closely aligned with those reported in the previous manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Others:</p>
<p>1. How does the statistical modeling take into account that measures are repeated both within the items (same texts read by different subjects) and within the subjects (some subject read multiple texts)? I only see the items-level repetition be addressed in line 715-721 in comparing between local and global questions, but not elsewhere. The standard approach in the literature on human reading times (e.g. the Wilcox et al paper mentioned above, or ref. 44) is to use mixed-effects regression with appropriate random effects for items and subjects. The same question applies to the calculation of chance accuracy (line 702-709), which is done by shuffling words within a passage. Relatedly, how exactly was cross-validation (line 681) calculated? On the level of subjects, individual words, trials, texts, ...?</p>
</disp-quote>
<p>Thank you for raising up this issue. In the previous manuscript, the word reading time was averaged across participants. The cross-validation was conducted on the level of texts (i.e., passages). Following the valuable suggestion, we have now separately analyzed each participant and applied the linear mixed effects models.</p>
<p>“Furthermore, a linear mixed effect model also revealed that more than 85% of the DNN attention heads contribute to the prediction of human reading time when considering text features and question relevance as covariates (Supplementary Results).”</p>
<p>“Supplementary Methods
To characterize the influences of different factors on human word reading time, we employed linear mixed effects models [5] implemented in the lmerTest package [6] of R. For the baseline model, we treated the type of questions (local vs. global; local = baseline) and all text/task-related features as fixed factors, and considered the interaction between the type of questions and these text/taskrelated features. We included participants and items (i.e., questions) as random factors, each with associated random intercepts…”</p>
<p>Supplementary Results
The baseline mixed model revealed significant fixed effects for question type and all text/task-related features, as well as significant interactions between question type and these text/task-related features (Table S7). Upon involving SAR attention, we observed a statistically significant fixed effect associated with SAR attention. When involving attention weights of randomly initialized BERT, the mixed model revealed that most attention heads exhibited significant fixed effects, suggesting their contributions to the prediction of human word reading time. A broader range of attention heads showed significant fixed effects for both pre-trained and fine-tuned BERT.</p>
<disp-quote content-type="editor-comment">
<p>1. I could not find any statement about code availability (only about data availability). Will the source code and statistical analysis code also be made available?</p>
</disp-quote>
<p>We have added the code availability statement.</p>
<p>“The code is now available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jiajiezou/TOA">https://github.com/jiajiezou/TOA</ext-link>.”</p>
<disp-quote content-type="editor-comment">
<p>1. The theoretical claim, and some basic features of the research, are quite similar to other recent work (Hahn and Keller, Modeling task effects in human reading with neural network-based attention, Cognition, 2023; cited with very little discussion as ref 44), which also considered task-directed reading in a question-answering task and derived task-optimized attention distributions. There are various differences, and the paper under consideration has both weaknesses and strengths when compared to that existing work -- e.g., that paper derived a single attention distribution from task optimization, but the paper under consideration provides more detailed qualitative analysis of the task effects, uses questions requiring more high-level reasoning, and uses more state-of-the-art DNNs.</p>
</disp-quote>
<p>The paper would benefit from being more explicit about how the work under review provides a novel angle over Ref 44 (Hahn and Keller, Cognition, 2023).</p>
<p>Thanks for bringing up this issue. We have now incorporated a more comprehensive discussion that compare the current study with the recent work conducted by Hahn and Keller:</p>
<p>“When readers read a passage to answer a question that can be answered using a word-matching strategy [45], a recent study has demonstrated that the specific reading goal modulates the word reading time and the effect can be modeled using a RNN model [46]. Here, we focus on questions that cannot be answered using a word-matching strategy (Fig. 1B) and demonstrate that, for these challenging questions, attention is still modulated by the reading goal but the attention modulation cannot be explained by a word-matching model (Fig. S3). Instead, the attention effect is better captured by transformer models than an advanced RNN model, i.e., the SAR (Fig. 3A). Combining the current study and the study by Hahn et al. [46], it is possible that the word reading time during a general-purpose reading task can be explained by a word prediction task, the word reading time during a simple goal-directed reading task that can be solved by word matching can be modeled by a RNN model, while the word reading time during a more complex goal-directed reading task involving inference is better modeled using a transformer model. The current study also further demonstrates that elongated reading time on task-relevant words is caused by counts of rereading and further studies are required to establish whether earlier eye movement measures can be modulated by, e.g., a word matching task.”</p>
<disp-quote content-type="editor-comment">
<p>1. In Materials&amp;Methods, line 599-636, specifically when &quot;pretraining&quot; is mentioned (line 632), it should be mentioned what datasets these DNNs were pretrained on.</p>
</disp-quote>
<p>We have now mentioned this in the revised manuscript:</p>
<p>“The pre-training process aimed to learn general statistical regularities in a language based on large corpora, i.e., BooksCorpus [62] and English Wikipedia…”</p>
</body>
</sub-article>
</article>