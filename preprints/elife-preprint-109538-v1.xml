<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">109538</article-id>
<article-id pub-id-type="doi">10.7554/eLife.109538</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.109538.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Synaptic Theory of Chunking in Working Memory</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6474-4880</contrib-id>
<name>
<surname>Zhong</surname>
<given-names>Weishun</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>wszhong@ias.edu</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9532-8484</contrib-id>
<name>
<surname>Katkov</surname>
<given-names>Mikhail</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>mikhail.katkov@gmail.com</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5661-4349</contrib-id>
<name>
<surname>Tsodyks</surname>
<given-names>Misha</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>misha@weizmann.ac.il</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f809463</institution-id><institution>School of Natural Sciences, Institute for Advanced Study</institution></institution-wrap>, <city>Princeton</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0316ej306</institution-id><institution>Department of Brain Sciences, Weizmann Institute of Science</institution></institution-wrap>, <city>Rehovot</city>, <country country="IL">Israel</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Lim</surname>
<given-names>Sukbin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02vpsdb40</institution-id><institution>New York University Shanghai</institution>
</institution-wrap>
<city>Shanghai</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country country="GR">Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2026-01-09">
<day>09</day>
<month>01</month>
<year>2026</year>
</pub-date>
<volume>15</volume>
<elocation-id>RP109538</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-10-21">
<day>21</day>
<month>10</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-09-21">
<day>21</day>
<month>09</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.08.14.607952"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2026, Zhong et al</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>Zhong et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-109538-v1.pdf"/>
<abstract>
<p>Working memory often appears to exceed its basic span by organizing items into compact representations called chunks. Chunking can be learned over time for familiar inputs; however, it can also arise spontaneously for novel stimuli. Such on-the-fly structuring is crucial for cognition, yet the underlying neural mechanism remains unclear. Here we introduce a synaptic theory of chunking, in which short-term synaptic plasticity enables the formation of chunk representations in working memory. We show that a specialized population of “chunking neurons” selectively controls groups of stimulus-responsive neurons, akin to gating. As a result, the network maintains and retrieves the stimuli in chunks, thereby exceeding the basic capacity. Moreover, we show that our model can dynamically construct hierarchical representations within working memory through hierarchical chunking. A consequence of this proposed mechanism is a new limit on the number of items that can be stored and subsequently retrieved from working memory, depending only on the basic working memory capacity when chunking is not invoked. Predictions from our model were confirmed by analyzing single-unit responses in epileptic patients and memory experiments with verbal material. Our work provides a novel conceptual and analytical framework for understanding how the brain organizes information in real time.</p>
</abstract>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
<institution>Eric and Wendy Schmidt Member in Biology Fund and the Simons Foundation</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Zhong</surname>
<given-names>Weishun</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-2">
<funding-source>
<institution-wrap>
<institution>Fran Morris Rosman and Richard Rosman</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Katkov</surname>
<given-names>Mikhail</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-3">
<funding-source>
<institution-wrap>
<institution>Simons Foundation, MBZUAI-WIS Joint Program for Artificial Intelligence Research and Foundation Adelis</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Tsodyks</surname>
<given-names>Misha Misha</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>New title and abstract, added control analysis in Fig.3</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Working memory (WM) is hypothesized to be a distinct capacity for holding and manipulating multiple pieces of information, which is crucial for human cognitive abilities such as verbal communication, reading comprehension, and abstract reasoning [<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref>]. Paradoxically, however, people typically cannot simultaneously hold more than four items in WM [<xref ref-type="bibr" rid="c4">4</xref>]. For example, repeating several words or digits is practically effortless and mistake-free, but for lists of five random words, people begin making mistakes [<xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c7">7</xref>]. How, then, are people able to process much larger streams of inputs, such as long passages of text or movies? One attractive idea is chunking, i.e., organizing several items into higher-level units [<xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c13">13</xref>]. Sometimes chunks are stored in long-term memory due to previous experience [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>], e.g., familiar expression like “Oh my God” or “Easier said than done” can be processed as a coherent unit rather than individual words. These pre-existing chunks could be thought of as having stable memory representations learned and consolidated over time, and could therefore be encoded and processed as a single item. However, conceptually more challenging is the phenomenon of <italic>spontaneous</italic> chunking, where novel combinations of items are grouped into separate units “on the fly”, as when a phone number is divided into chunks of 2-3 digits each, or words in a sentence are combined into units based on their syntactic role, such as “–a little boy–was dressed – in a green shirt”. Indeed, this sentence is much easier to remember than a random sequence of nine words. Surprisingly, a minor manipulation like introducing slight pauses between presentations of consecutive groups of items is enough to trigger chunking and the corresponding increase in capacity [<xref ref-type="bibr" rid="c16">16</xref>–<xref ref-type="bibr" rid="c19">19</xref>]. In this study, we addressed two interrelated questions inspired by the above considerations: how spontaneous chunking might emerge in the brain and what is (if there is one) the limit for the number of items that can be held in WM when spontaneous chunking is activated.</p>
<p>Neuronal mechanisms of WM and the origin of WM capacity are still under debate. While the most accepted theory assumes that WM is carried by persistent activity of item-specific neurons [<xref ref-type="bibr" rid="c20">20</xref>–<xref ref-type="bibr" rid="c23">23</xref>], we propose that a more economic and robust mechanism is to rely on short-term plasticity (STP) in item-specific <italic>synapses</italic> [<xref ref-type="bibr" rid="c24">24</xref>] (see [<xref ref-type="bibr" rid="c25">25</xref>] for a recent review of activity-silent WM). When several items are loaded into WM, rather than having all of the neurons persistently active, information could be maintained by periodic reactivations of the corresponding clusters in the form of population spikes [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c26">26</xref>]. After each reactivation of a certain cluster, the recurrent self-connections in this cluster remain facilitated, allowing it to bounce back after a period of silence when other clusters activate. The largest possible number of co-active clusters, i.e., the WM capacity, is determined in this theory by the longest possible time between consecutive reactivations for each cluster, which in turn depends on STP time constants [<xref ref-type="bibr" rid="c26">26</xref>]. In the current contribution, we extend the STP theory of WM by including longer-lasting forms of facilitation, such as synaptic augmentation (SA) [<xref ref-type="bibr" rid="c27">27</xref>]. In [<xref ref-type="bibr" rid="c28">28</xref>], it was shown that due to its slow build-up, SA level in recurrent self-connections encodes the order of presentation of stimuli in WM. While SA does not significantly change the maximal possible number of coactivating clusters, i.e., the basic WM capacity, it allows the network to selectively switch some of the clusters off for a longer period of time, without fully erasing information about their prior activity from the recurrent selfconnections [<xref ref-type="bibr" rid="c28">28</xref>]. Here, we will show that SA enables consecutive chunks to be activated one after another by switching on and off specialized chunking clusters that serve as controls, and in this way enhance the <italic>effective</italic> WM capacity. In the next section, we demonstrate this mechanism in a simplified neural network model of WM and show how much WM capacity can be increased by chunking compared to the basic regime.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Network model of working memory and chunking</title>
<p>Following our previous work on the synaptic theory of working memory [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c28">28</xref>], we consider a recurrent neural network model (RNN) where memory items are represented by specific clusters of excitatory neurons coupled to a global inhibitory neural pool, see <xref rid="fig1" ref-type="fig">Fig. 1(a)</xref> and Methods Sec. A. The feedback inhibition is assumed to be strong enough such that at any given moment, only one excitatory cluster can be active. To simplify the model, we neglect the overlaps between the stimulus-specific clusters, such that each cluster <italic>µ</italic> can be described by a single activity variable corresponding to the average firing rate of the corresponding neurons at a given moment <italic>R</italic><sub><italic>µ</italic></sub>(<italic>t</italic>). Furthermore, we assume that all the recurrent self-connections are <italic>dynamic</italic> [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>], i.e., instantaneous synaptic efficacy depends on the pre-synaptic activity within a certain time window due to a combination of short-term synaptic depression and facilitation: <italic>J</italic> <sup>Self</sup>(<italic>t</italic>) = <italic>u</italic>(<italic>t</italic>)<italic>x</italic>(<italic>t</italic>)<italic>A</italic> [<xref ref-type="bibr" rid="c29">29</xref>], where <italic>A</italic> is the amplitude of the recurrent strength, <italic>u</italic>(<italic>t</italic>) is the current value of release probability, and <italic>x</italic>(<italic>t</italic>) is the current fraction of the maximal amount of neurotransmitter that is available for release.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption>
<title>Illustration of the hierarchical working memory model.</title>
<p><bold>(a)</bold> Network architecture. Stimulus clusters and chunking clusters both have recurrent self-excitations (thick sharp arrows) and reciprocal connections to the global inhibitory pool (not shown). Chunking clusters have dense but weak connections to the stimulus clusters (thin blunt arrows in the background). <bold>(b)</bold> Effective network architecture after presentation. Activities in the network selectively augment connections between stimuli within chunks and the corresponding chunking clusters, effectively forming a hierarchical structure. <bold>(c)</bold> Dynamics of the recurrent self-connections. Upon the arrival of pre-synaptic inputs (top panel), the release probability <italic>u</italic> increases, and the fraction of available neurotransmitters <italic>x</italic> decreases (left axis of the middle panel). The amplitude of the recurrent strength <italic>A</italic> gradually increases with each reactivation of the cluster (right axis of the middle panel). As a result, the total synaptic efficacy of the recurrent self-connection <italic>J</italic> <sup>Self</sup> = <italic>uxA</italic> oscillates (bottom panel). Activity traces are taken from the first stimulus cluster from the top panel of (d) below. <bold>(d)</bold> Network simulation. The first three memories are colored in blue, and the other three memories are colored in green. Shades represent external input to the cluster. Top: Memories are loaded at a uniform speed; chunking clusters are not activated. Only four out of six memories remain active in the WM. Bottom: Slight pauses after chunks activate the chunking clusters, which inhibit the stimulus clusters presented before the pause. All memories are retrieved chunk-by-chunk in the retrieval stage. The full activity trace of the synaptic variables is presented in <xref ref-type="fig" rid="figS1">Fig. S1</xref>.</p>
</caption>
<graphic xlink:href="607952v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>When the cluster’s activity is high, the release probability in the corresponding recurrent connections (<italic>u</italic>) increases above its baseline level <italic>U</italic>, constituting shortterm facilitation, and the fraction of available neurotransmitters (<italic>x</italic>) decreases, representing short-term depression (<xref rid="fig1" ref-type="fig">Fig. 1(c)</xref>). When the cluster activity is low, both <italic>u</italic> and <italic>x</italic> variables relax towards their baseline values with time constants <italic>τ</italic><sub><italic>f</italic></sub> and <italic>τ</italic><sub><italic>d</italic></sub>, respectively (Methods Sec. A). Such transient changes in the synapses are well observed in experiments and are reported to last on the order of hundreds of milliseconds to seconds [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>].</p>
<p>The RNN detailed in Methods Sec. A exhibits different dynamical regimes, depending on the STP parameters and external background input. In particular, as shown in [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c26">26</xref>], at high background input level, there exists a persistent activity regime where clusters have sustained elevated firing rates corresponding to loaded memory items. As the background input is lowered, there exists a low-activity regime with cyclic behavior where items that were loaded into the network via external stimuli are maintained in WM in the form of sequential brief reactivations called population spikes [<xref ref-type="bibr" rid="c33">33</xref>]. As the number of loaded memories increases, the network eventually fails to maintain some of them, i.e., there is a maximal number of items that can be maintained in the WM, <italic>C</italic>, which depends on the synaptic-level parameters of the RNN [<xref ref-type="bibr" rid="c26">26</xref>].</p>
<p>In addition to short-term facilitation and depression, experiments observed longer-scale forms of synaptic facilitation in cortical synapses, called synaptic augmentation (SA), characterized by slow, compared to STP, build-up with activity and decay of tens of seconds [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c34">34</xref>–<xref ref-type="bibr" rid="c36">36</xref>]. We introduce SA as a small transient change in synaptic strength <italic>A</italic> that is strengthened from its baseline value due to cluster activity, similar to <italic>u</italic>, but with a much longer time constant <italic>τ</italic><sub><italic>A</italic></sub> ≫ <italic>τ</italic><sub><italic>f</italic></sub> [<xref ref-type="bibr" rid="c28">28</xref>] (see <xref rid="fig1" ref-type="fig">Fig. 1(c)</xref>).</p>
<p>The main modification of the current model compared to our earlier work is the introduction of distinct excitatory/inhibitory “chunking” clusters which serve to control the stimulus clusters. Both stimulus clusters and chunking clusters have recurrent excitatory self-connections. Each time the system receives a chunking cue (e.g., when there is a temporal pause in stimulus presentations), one of the chunking clusters is activated and quickly suppresses the currently active stimulus clusters, effectively grouping them into a chunk (<xref rid="fig1" ref-type="fig">Fig. 1(b)</xref>). Sub-sequent stimulus clusters are then free to be loaded into the network until the next chunking cue is received and another chunking cluster is activated. At the end of the presentation, only chunking clusters reactivate cyclically while all the stimulus clusters are inhibited (<xref rid="fig1" ref-type="fig">Fig. 1(d)</xref>).</p>
</sec>
<sec id="s2b">
<title>Chunking increases working memory capacity</title>
<p>The main idea of the proposed chunking mechanism is that the chunking clusters can selectively activate and suppress the stimulus clusters, so that at no point in time do more than a small number of stimulus clusters reactivate as population spikes, thus not exceeding the basic WM capacity. Due to synaptic augmentation, stimulus clusters that are currently suppressed by the chunking clusters still have stronger recurrent self-connections than the ones that were not active at a given trial as long as augmentation has not disappeared. Therefore, the network can <italic>retrieve</italic> temporarily suppressed items by sequentially switching off the chunking clusters, releasing the suppressed stimulus clusters within the corresponding chunk from inhibition.</p>
<p>To demonstrate the chunking mechanism, we simulate a network of 16 clusters (both stimulus and chunking), 6 of which are activated consecutively with transient external input (presentation stage, the shades in <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref>). We first consider continuous presentation of 6 inputs to the stimulus clusters with no chunking activated. At the end of the presentation, 4 of the corresponding clusters remain active in the form of periodic population spikes while two other clusters drop out of WM, corresponding to a WM capacity of 4 for the chosen values of parameters, similar to [<xref ref-type="bibr" rid="c26">26</xref>] (the top panel of <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref>). Now consider presenting the same six memory items, but with a slightly longer interval between the presentation of the 3<sup><italic>rd</italic></sup> and 4<sup><italic>th</italic></sup> items, during which a chunking cluster is activated (shown in red in the bottom panel of <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref>). We assume that the chunking cluster quickly inhibits the three stimulus clusters that were presented before it (the three blue colors) and remains the only cluster active until the items of the next chunk are presented to thenetwork (shown in green). A second chunking cluster is then activated, shown in purple. This way, the network effectively binds the stimulus clusters in each chunk to their corresponding chunking cluster (<xref rid="fig1" ref-type="fig">Fig. 1(b)</xref>). Such group-specific binding is akin to gating [<xref ref-type="bibr" rid="c37">37</xref>], where the activity of each chunking cluster gates the entire chunk of stimulus clusters via inhibition.</p>
<p>We assume that the fast inhibition between chunking clusters and corresponding stimulus clusters happens through strengthening the existing dense but weak inhibitory synapses between them (<xref rid="fig1" ref-type="fig">Fig. 1(a)</xref>). After all stimuli are presented, the network maintains reactivations of two chunking clusters while the synaptic variables of the stimulus clusters slowly decay to their baseline values. However, if the chunking cluster is suppressed within the augmentation time-window <italic>τ</italic><sub><italic>A</italic></sub>, the items that were inhibited by it will bounce back (<xref rid="fig1" ref-type="fig">Fig. 1(d)</xref> bottom panel, the blue colors in the retrieval stage). At this point in time, four clusters are active: the second chunking cluster and three stimulus clusters from the first chunk, with all items from the first chunk being successfully retrieved. When the second chunk is to be retrieved, the first chunking cluster is again activated by control input while the second chunking cluster is suppressed, allowing the stimulus clusters from the second chunk to activate. This chunking scenario allows the retrieval of all six memory items while at any given moment in time, the network maintains no more than four active clusters, not exceeding basic WM capacity. In this way, chunking increases <italic>effective</italic> working memory capacity by reducing the concurrent load on working memory, at the expense of activating higher-level representations (chunking clusters).</p>
<p>Above, we chose to illustrate the chunking mechanism in the periodic activity regime because the mechanistic effects of chunking clusters are most apparent with regular firing traces. Nevertheless, our proposed chunking mechanism applies to both the persistent-activity and periodic-activity regimes, with chunking clusters serving the same function in each. Note that, although we model the chunking cues here as slight pauses between presentations, in general chunking can be triggered by other cues, such as tonic variations and semantic meanings. The idea that chunking reduces the load on working memory was first introduced in the psychology literature [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c38">38</xref>]. Subsequently, neuroimaging studies observed that chunking reduces neural activity in upstream brain regions that process raw stimuli but increases activity in downstream regions associated with higher-level representations [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>], which is consistent with our proposed mechanism.</p>
</sec>
<sec id="s2c">
<title>Hierarchical chunking predicts a new capacity</title>
<p>Our model assumes that several stimulus clusters are grouped into chunks by chunking clusters. A natural question then arises: Can chunks also form meta-chunks? If so, is there a limit to how many levels of such hierarchical representations in working memory can be formed? Here we argue that the answers to both questions are affirmative and moreover, one can derive a surprisingly simple formula for the largest possible number of items in WM (Methods Sec. B):
<disp-formula id="eqn1">
<graphic xlink:href="607952v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>C</italic> is the basic WM capacity in the absence of chunking. As mentioned above, <italic>C</italic> corresponds to the number of active clusters that can be maintained in the RNN model (the top panel of <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref> illustrates the case of <italic>C</italic> = 4), and it depends on all the synaptic-level parameters (Methods Sec. A)[<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c28">28</xref>].</p>
<p><xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> is a direct consequence of the limited amount of activity that the working memory network can sustain (Methods Sec. B) and does not depend on specific STP mechanisms. Therefore, we expect <italic>M</italic> <sup><italic>∗</italic></sup> to hold in working memory models with similar architecture but possibly different microscopic implementation from Methods Sec. A. <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> defines a new capacity for working memory that accounts for hierarchical chunking. Thus, we refer to <italic>M</italic> <sup><italic>∗</italic></sup> as the new magic number, in the original spirit of Miller [<xref ref-type="bibr" rid="c14">14</xref>].</p>
<p>Below we illustrate how the limited number of <italic>C</italic> clusters in the network constrains the total number of memory items that can be maintained and retrieved in WM. Let us consider the example corresponding to <italic>C</italic> = 4, with a capacity of <italic>M</italic> <sup><italic>∗</italic></sup> = 2<sup>4<italic>−</italic>1</sup> = 8. In this case, the optimal chunking structure is a binary tree with three levels (<xref rid="fig2" ref-type="fig">Fig. 2(a)</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption>
<title>Memory retrieval from a hierarchical structure.</title>
<p><bold>(a)</bold> Top: Schematic of an emergent hierarchy of three levels. The top node (black) denotes the global inhibitory neural pool. The first two levels represent chunking clusters, and the lowest level represents stimulus clusters. Grey stripes denote the clusters that need to be suppressed to retrieve the 1<sup><italic>st</italic></sup> chunk. Blue dashed circles represent clusters that are active during the retrieval of the 1<sup><italic>st</italic></sup> chunk during the retrieval stage. Bottom: Architecture of the underlying recurrent neural network. <bold>(b)</bold> Simulation of the network in (a). <italic>R</italic><sup>(<italic>k</italic>)</sup>: activity trace of firing rates, color-coded to match the corresponding clusters in (a). The time-course of the traces is labeled as chunks (stimulus clusters), pauses (chunking clusters), and long pauses (meta-chunking clusters). <italic>I</italic><sup><italic>b</italic>(<italic>k</italic>)</sup>: activity traces of background input currents. Decreasing the background input to a cluster at level <italic>k</italic> suppresses its reactivation and removes the inhibition on its children clusters at level <italic>k −</italic> 1.</p>
</caption>
<graphic xlink:href="607952v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Eight memories are loaded as four chunks of two into the working memory network (the third panel in <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>): a slight pause in-between items of different colors (such as the 2<sup><italic>nd</italic></sup> and 3<sup><italic>rd</italic></sup> items) serves as the chunking cue to activate the chunking clusters (the second panel in <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>), which binds item clusters in pairs of two, similar to the chunks in the bottom panel of <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref>. However, here we introduce a slightly longer pause in-between the 4<sup><italic>th</italic></sup> and 5<sup><italic>th</italic></sup> items, during which a chunking cluster binding items 3 and 4 into a chunk is first activated, which is quickly followed by the activation of another chunking cluster to group the first two chunks into a meta-chunk (the first panel in <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>). In this way, after the presentation of the eight items, we have two meta-chunks, giving rise to a tree-like hierarchical structure of three levels (<xref rid="fig2" ref-type="fig">Fig. 2(a)</xref>).</p>
<p>To differentiate clusters at different levels of the hierarchy, we denote the <italic>i</italic><sup><italic>th</italic></sup> stimulus clusters at the <italic>k</italic> = 3 level as <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="607952v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and its activity as <inline-formula id="inline-eqn-2"><inline-graphic xlink:href="607952v2_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>i</italic> is the order within the level during presentation. Similarly, the <italic>m</italic><sup><italic>th</italic></sup> chunking clusters at the <italic>k</italic> = 1, 2 levels is denoted as <inline-formula id="inline-eqn-3"><inline-graphic xlink:href="607952v2_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and its activity as <inline-formula id="inline-eqn-4"><inline-graphic xlink:href="607952v2_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The process of meta-chunking introduced in the previous paragraph can be described more precisely as follows. Immediately after presenting<inline-formula id="inline-eqn-5"><inline-graphic xlink:href="607952v2_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (item 4), chunking cluster <inline-formula id="inline-eqn-6"><inline-graphic xlink:href="607952v2_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is active and sup-Presses <inline-formula id="inline-eqn-7"><inline-graphic xlink:href="607952v2_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula id="inline-eqn-8"><inline-graphic xlink:href="607952v2_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula>(items 1 and 2, first two blue colors). Clusters <inline-formula id="inline-eqn-9"><inline-graphic xlink:href="607952v2_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula id="inline-eqn-10"><inline-graphic xlink:href="607952v2_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (items 3 and 4, first two green colors) are also active. Once the chunking signal is received, cluster <inline-formula id="inline-eqn-11"><inline-graphic xlink:href="607952v2_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is activated and suppresses clusters <inline-formula id="inline-eqn-12"><inline-graphic xlink:href="607952v2_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula id="inline-eqn-13"><inline-graphic xlink:href="607952v2_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Furthermore, when this suppression is established, another meta-chunking cluster <inline-formula id="inline-eqn-14"><inline-graphic xlink:href="607952v2_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is activated and suppresses all the <italic>k</italic> = 2, 3 clusters that were active before it: <inline-formula id="inline-eqn-15"><inline-graphic xlink:href="607952v2_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <inline-formula id="inline-eqn-16"><inline-graphic xlink:href="607952v2_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. When presentation is finished (around <italic>t</italic> ~ 7.5 s in <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>), only the <italic>k</italic> = 1 level clusters <inline-formula id="inline-eqn-17"><inline-graphic xlink:href="607952v2_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in orange, and <inline-formula id="inline-eqn-18"><inline-graphic xlink:href="607952v2_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in yellow) remain active in the working memory.</p>
<p>As the retrieval begins (at <italic>t</italic> ≳ 8 s in <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>), <inline-formula id="inline-eqn-19"><inline-graphic xlink:href="607952v2_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is suppressed by a drop in its background input <inline-formula id="inline-eqn-20"><inline-graphic xlink:href="607952v2_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, indicated by grey stripes in <xref rid="fig2" ref-type="fig">Fig. 2(a)</xref>. Subsequently, the <italic>k</italic> = 2 clusters become active, and the network now maintains three clusters: <inline-formula id="inline-eqn-21"><inline-graphic xlink:href="607952v2_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. In the next step, <inline-formula id="inline-eqn-22"><inline-graphic xlink:href="607952v2_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula>(darker red) is suppressed, which reactivates the <italic>k</italic> = 3 stimulus clusters that were inhibited by <inline-formula id="inline-eqn-23"><inline-graphic xlink:href="607952v2_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Now, the first chunk has been retrieved, and the working memory maintains four clusters: <inline-formula id="inline-eqn-24"><inline-graphic xlink:href="607952v2_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (illustrated by blue dashed circles in <xref rid="fig2" ref-type="fig">Fig. 2(a)-(b)</xref>). Other chunks can be retrieved in a similar manner by suppressing the corresponding higher-level chunking clusters. Note that even though hierarchical chunking reduces the load on working memory from eight to two, upon unpacking of the chunk, one still needs to maintain all the intermediate chunks that were not unpacked, in this case, <inline-formula id="inline-eqn-25"><inline-graphic xlink:href="607952v2_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula id="inline-eqn-26"><inline-graphic xlink:href="607952v2_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (lighter red and yellow, respectively). Similar considerations result in the above expression for <italic>M</italic> <sup><italic>∗</italic></sup> in <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> (Methods Sec. B). In the following sections, we first examine the neuroscience evidence of chunking clusters, then test the prediction of <italic>M</italic> <sup><italic>∗</italic></sup> on memory experiments.</p>
</sec>
<sec id="s2d">
<title>Experimental evidence for the existence of chunking clusters</title>
<p>Segmentation of sensory stimuli in human memory has been extensively studied in behavioral experiments from the early days of cognitive neuroscience and psychology [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>], but its neural correlates have not been explored until recently [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c43">43</xref>–<xref ref-type="bibr" rid="c45">45</xref>]. The key assumption in the hierarchical working memory model is the existence of chunking clusters that segment stimuli into chunks. Our model predicts that chunking reduces the load on working memory through inhibition. Upon the firing of the chunking clusters, we expect to see a decrease in the average firing rate of the stimulus clusters. Furthermore, as stimuli continue to be presented after chunking, the average firing rate should gradually increase after the drop. Overall, the hierarchical working memory model predicts two qualitative features in the firing rates of the cluster of neurons that encode stimuli (such as in the bottom panel of <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref>): (1) there should be a “dip” in the activities of stimulus clusters upon the firing of the chunking clusters; (2) there should be a continuous “ramping-up” of activities following the dip.</p>
<p>Thanks to advances in single-neuron recording technologies, we can now test our hypothesis using data collected from drug-resistant epilepsy patients [<xref ref-type="bibr" rid="c41">41</xref>]. Consider the experiment reported in [<xref ref-type="bibr" rid="c41">41</xref>], where subjects are asked to watch a series of movie clips, each consisting of two episodes separated by a “cut” in the middle of the movie. Such movie cuts serve to induce cognitive boundaries for event segmentation in episodic memory. The authors in [<xref ref-type="bibr" rid="c41">41</xref>] identified a group of neurons in the medial temporal lobe that fire selectively at these boundaries and termed them “cognitive boundary” neurons. If these neurons segment episodic memories in a manner similar to how chunking clusters segment working memory in our model, then we should also observe a decrease in the firing rates of the stimulus neurons upon the firing of the cognitive boundary neurons. In [<xref ref-type="bibr" rid="c41">41</xref>], although the boundary neurons can be unambiguously identified by aligning their responses to movie cuts, it is difficult to pinpoint stimulus neurons due to the continuous nature of the visual stimulus. Therefore, we study the putative effect of boundary neurons on the rest of the system by aggregating neurons that are detected but not classified as boundary neurons. We align all the neurons to the movie cuts. Upon averaging over subjects and trials, we find that after the peak in the firing rate of boundary neurons (top panel of <xref rid="fig3" ref-type="fig">Fig. 3 (a)</xref>), about ~ 130 ms later, there is a dip in the average activities of the rest of the recorded neurons (bottom panel of <xref rid="fig3" ref-type="fig">Fig. 3 (a)</xref>). Further-more, there is a continuous ramp-up of activities following the dip. This trend is also evident at the level of individual subjects (<xref rid="fig3" ref-type="fig">Fig. 3 (b)</xref>), and qualitatively agrees with the prediction of our hierarchical working memory model. As a control, within the same recorded population we label the subset of neurons that respond to the onset of the movie clip as “onset” neurons. Aligning firing rates to the movie onset, we observe a peak in the on-set neurons as reported in [<xref ref-type="bibr" rid="c41">41</xref>]; however, unlike <xref rid="fig3" ref-type="fig">Fig. 3 (a)</xref>, the remaining neurons (including boundary neurons) do not exhibit the dip-then-ramp pattern (<xref rid="fig3" ref-type="fig">Fig. 3 (c)</xref>). This indicates that the dip and ramp-up are specific to boundary neurons and suggests an internal network mechanism rather than simple inhibitory feedback or statistical artifacts.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption>
<title>Cognitive boundary neurons in the medial temporal lobe.</title>
<p><bold>(a)</bold> Average firing rate from single-neuron recording data in [<xref ref-type="bibr" rid="c41">41</xref>]. The mean z-score firing rates are plotted in solid lines, with one standard deviation included as the shades. Firing rates are averaged over all subjects and trials, and the relative time zero is chosen to be the location of the movie cut. Two qualitative features in the firing rates of the non-boundary neurons: a dip followed by a ramp, are predicted by the hierarchical working memory model. Top: Boundary neurons. Bottom: Non-boundary neurons. <bold>(b)</bold> Average firing rates of non-boundary neurons over all trials for individual subjects. Subjects are sorted based on the location of the dip. A trend similar to panel (a) is observed for each subject. For individual 2D plots, see <xref ref-type="fig" rid="figS3">Fig. S3</xref>. <bold>(c)</bold> Average firing rates of neurons aligned to the onset of the movie (relative time zero). After the peak in onset-specific neurons, the non-onset-specific neurons do not exhibit the dip-then-ramp pattern seen in panel (a). Top: onset-specific neurons. Bottom: Non-onset specific neurons.</p>
</caption>
<graphic xlink:href="607952v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2e">
<title>Experimental tests of the new magic number</title>
<p>An important prediction of the hierarchical working memory model is the existence of an absolute limit <italic>M</italic> <sup><italic>∗</italic></sup>, beyond which perfect retrieval is impossible (<xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref>). One of the earliest studies to quantify this transition is the experiment performed by Miller and Selfridge [<xref ref-type="bibr" rid="c42">42</xref>] on the statistical approximation of language. In this experiment, the authors constructed <italic>n</italic>-gram approximations to English, where <italic>n</italic> refers to coherent occurrences with the previous <italic>n−</italic> 1 words. For example, a 1-gram approximation would consist of words randomly chosen from a corpus. In a 2-gram approximation, each word would appear coherently with the previous word, but coherence for any sliding window of three words is not required. As <italic>n</italic> increases, the constructed text gradually approaches natural text. In [<xref ref-type="bibr" rid="c42">42</xref>], subjects were presented with verbal materials constructed from such <italic>n</italic>-gram approximations and asked to recall the words. The fraction of recalled words <italic>f</italic> decreases with the length <italic>L</italic> of the material and increases with the degree of approximation <italic>n</italic> (<xref rid="fig4" ref-type="fig">Fig. 4(a)</xref> inset). Here, we are interested in the critical length <italic>L</italic><sub><italic>c</italic></sub> beyond which retrieval begins to be imperfect, i.e., <italic>f</italic> (<italic>L</italic><sub><italic>c</italic></sub>) = 1. Since the defining feature of working memory is the ability to perfectly retrieve items that are sustained in the memory, <italic>L</italic><sub><italic>c</italic></sub> is a measure of working memory capacity.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>FIG. 4.</label>
<caption>
<title>The new magic number bounds perfect-recall performance on verbal memory.</title>
<p><bold>(a)</bold> Fraction of recalled words as a function of the length of the presented text. Different shades of blue correspond to different <italic>n</italic>-gram approximations. Black color represents natural text. Inset: Original data as presented in [<xref ref-type="bibr" rid="c42">42</xref>]. Main: Different <italic>n</italic>-gram approximation curves become straight lines in a semi-log plot and can be collapsed into a single universal curve (red dashed line) by adjusting the offsets on the individual intercepts. <bold>(b</bold> Critical length of perfect recall as a function of <italic>n</italic>-gram approximations. The location of the critical length <italic>L</italic><sub><italic>c</italic></sub> is determined by extrapolating the individual <italic>n</italic>-gram approximation curves to where <italic>f</italic> (<italic>L</italic><sub><italic>c</italic></sub>) = 1 using the universal slope. Different colored lines represent experiments in different languages. The grey dashed line corresponds to <italic>M</italic> <sup><italic>∗</italic></sup> = 2<sup><italic>C−</italic>1</sup> for <italic>C</italic> = 4.</p>
</caption>
<graphic xlink:href="607952v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In [<xref ref-type="bibr" rid="c42">42</xref>], the fraction of reported words for smallest stimulus length was less than one. To estimate <italic>L</italic><sub><italic>c</italic></sub>, we replotted the data from [<xref ref-type="bibr" rid="c42">42</xref>] in a semi-log plot (with <italic>f</italic> as a function of log<sub>2</sub> <italic>L</italic>) and observed that all the different <italic>n</italic>-gram curves are well approximated by straight lines. We hence collapsed all the curves into a common line by adjusting the individual intercepts (red dashed line in <xref rid="fig4" ref-type="fig">Fig. 4(a)</xref>). We then used the slope of this line to extrapolate each <italic>n</italic>-gram approximation curve to its critical length <italic>L</italic><sub><italic>c</italic></sub>. We plot <italic>L</italic><sub><italic>c</italic></sub> as a function of <italic>n</italic> in <xref rid="fig4" ref-type="fig">Fig. 4(b)</xref>. <italic>L</italic><sub><italic>c</italic></sub> increases with <italic>n</italic> as expected but starts to plateau around <italic>n</italic> = 4, saturating at roughly the predicted value of 8. Note that <italic>n</italic> = 0 corresponds to words randomly chosen from a dictionary, and is dominated by rare words many of which may not be familiar to the subjects. Therefore, the capacity for <italic>n</italic> = 0 is expected to be lower than that of common words as in the case for <italic>n</italic> = 1. The same analysis of two replicates of the Miller-Selfridge experiment in Danish and Hindi [<xref ref-type="bibr" rid="c46">46</xref>, <xref ref-type="bibr" rid="c47">47</xref>] reveals similar trends. As <italic>n</italic> increases, the verbal material becomes more structured, which allows for the construction of hierarchical representations. Naively, one might expect that the number of perfectly recalled items <italic>L</italic><sub><italic>c</italic></sub> would continue to increase with <italic>n</italic>, as more structured materials are generally easier to remember. However, we observe that the performance plateaus around <italic>n</italic> ~ 4. This may be due to the fact that longer sentences need to be broken into smaller chunks to be stored in working memory, and there exists an optimal chunk size beyond which storage becomes inefficient and no longer improves memory. This observation qualitatively agrees with our theory in <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref>, and the value <italic>n</italic> ~ 4 at which capacity saturates could correspond to the size of a meta-chunk in the optimal hierarchical scheme illustrated above. Furthermore, our prediction that natural texts are chunked into pairs of two meaningful words resembles the empirical observation of collocations in language, such as adjective-noun, verb-noun, and subject-verb pairs, etc [<xref ref-type="bibr" rid="c48">48</xref>–<xref ref-type="bibr" rid="c51">51</xref>].</p>
<p>Notably, in <xref rid="fig4" ref-type="fig">Fig. 4(b)</xref> for all three languages, <italic>L</italic><sub><italic>c</italic></sub> saturates within the region predicted by <italic>M</italic> <sup><italic>∗</italic></sup> = 2<sup><italic>C−</italic>1</sup>, when substituting for <italic>C</italic> = 4 [<xref ref-type="bibr" rid="c4">4</xref>]. Therefore, we conclude that the recall performance of verbal materials from working memory agrees with the prediction of our new magic number.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Chunking is classically believed to be a crucial process for overcoming extremely limited working memory capacity. In the current contribution, we suggest a simple mechanism of chunking in the context of the synaptic theory of working memory. The proposed mechanism relies on the ability of the system to temporarily suppress groups of items without permanently erasing them from WM, which is enabled by the longer-term form of synaptic facilitation, called synaptic augmentation. For chunking to work properly in the model, the system has to utilize separate neuronal clusters, which we call “chunking clusters” that effectively combine groups of several items each into distinct chunks. Moreover, the activity of chunking clusters has to be controlled in order to allow the suppression and reactivation of subsequent chunks at the right times to avoid saturating working memory capacity at any given moment. In particular, each chunking cluster has to be activated right after all of the corresponding stimuli are presented and later suppressed for them to be retrieved. Our model has no explicit mechanisms for this hypothesized control of chunking clusters; we speculate that it could be triggered by corresponding cues, e.g., chunking clusters could be activated by extra temporal pausing or intonation accentuation, and suppressed by internally generated retrieval signals. While further experimental and theoretical studies are needed to elucidate these suggestions, the existence of specialized chunking neurons has some recent neurophysiological support in electrical recordings in epileptic patients, where neurons responding to cuts in video clips were identified. We analyzed the data collected in these experiments and found that the activity of these and other neurons during clip watching is broadly consistent with our model predictions.</p>
<p>Apart from proposing the biological mechanism of chunking in working memory, we considered the question of whether the hierarchical organization of items in working memory could emerge from the subsequent chunking of chunks. Indeed, we demonstrated that the model allows for such a hierarchical scheme; however, due to working memory capacity, the overall number of items that can be retrieved is still constrained even for the optimal chunking scheme. We derived the universal relation between capacity and the maximal number of retrievable items, which we call a magic number following the classical Miller paper [<xref ref-type="bibr" rid="c14">14</xref>]. In particular, this relation predicts the new magic number of 8 for a working memory capacity of 4, which is currently accepted as the best estimate of capacity. The chunking scheme achieving this limit corresponds to dividing the inputs into 4 chunks of 2, with two “meta-chunks”, each consisting of two chunks. We reanalyzed the results of a memory study where subjects were presented with progressively higher-order approximations of meaningful passages for recall, and found that indeed the average maximal number of words that could be fully recalled was close to the predicted value of 8, and that this number saturated for a 4<sup><italic>th</italic></sup> order approximation of meaningful passages, corresponding to the size of a “meta-chunk” in the optimal chunking scheme predicted by the model. While encouraging, more studies should be performed to elaborate on this issue, in particular to more directly demonstrate the ability of subjects to form chunks of chunks during working memory tasks.</p>
<p>Our theory and the proposed neural network mechanism attempt to bridge the microscopic level of neural activities and the macroscopic level of behaviors in the context of hierarchically-structured memories. Our analytical results and data analysis methods offer new perspectives on classical results in cognitive neuroscience and psychology. The proposal of a hierarchical structure in working memory can open many new directions. For instance, long-term memory is usually organized in a hierarchical manner, as reflected in our ability to gradually zoom into increasingly fine details of an event during recall [<xref ref-type="bibr" rid="c52">52</xref>]. While working memory underlies our ability to construct such hierarchical representations, little is known about how the transient tree-like structure in working memory is related to the hierarchy in long-term memory. Furthermore, one of the hallmarks of fluid intelligence — the ability to compress and summarize information — is also related to re-coding information in a hierarchical manner [<xref ref-type="bibr" rid="c53">53</xref>]. Understanding how our mind is capable of making use of hierarchical structures for complex cognitive functions such as summarization and comprehension remains an important open question.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<label>A.</label>
<title>RNN model for hierarchical working memory</title>
<p>As illustrated in <xref rid="fig1" ref-type="fig">Fig. 1(a)</xref>, the recurrent network that implements WM has 3 functionally distinct types of neuronal populations: stimulus clusters that encode different items (indexed by <italic>i</italic> below), chunking clusters (indexed by <italic>m</italic>), and a single inhibitory neural pool indexed by <italic>I</italic>. WM implementation is based on the previously introduced synaptic theory of working memory [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c28">28</xref>]. All stimulus and chunking clusters exhibit short-term synaptic plasticity in the recurrent self-connections, such that the instantaneous strength of connections for cluster <italic>µ</italic> (<italic>µ</italic> = (<italic>i, m</italic>)) is given by
<disp-formula id="eqn2">
<graphic xlink:href="607952v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>A</italic> is the amplitude of the recurrent strength, <italic>u</italic> is the probability of release, and <italic>x</italic> is the fraction of available neurotransmitters; all three factors depend on time via the following dynamical equations reflecting different STP processes:
<disp-formula id="eqn3">
<graphic xlink:href="607952v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn4">
<graphic xlink:href="607952v2_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="607952v2_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>R</italic><sub><italic>µ</italic></sub> is the activity of cluster <italic>µ</italic>; <italic>U</italic> is the baseline value of release probability; <italic>τ</italic><sub><italic>f</italic></sub>, <italic>τ</italic><sub><italic>d</italic></sub> and <italic>τ</italic><sub><italic>A</italic></sub> are time constants of synaptic facilitation, depression and augmentation, correspondingly; <italic>A</italic><sub>min</sub>, <italic>A</italic><sub>max</sub> and <italic>κ</italic><sub><italic>A</italic></sub> are parameters of synaptic augmentation that distinguish this model from earlier versions. Apart from self-connections, each stimulus and chunking cluster is reciprocally connected to the inhibitory pool, and some of the chunking clusters develop quick inhibition on groups of stimulus clusters as explained below. The activity of each cluster is determined as a non-linear gain function of its input, and all inputs satisfy the following standard dynamics:
<disp-formula id="eqn6">
<graphic xlink:href="607952v2_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7">
<graphic xlink:href="607952v2_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>R</italic>(<italic>h</italic>) = <italic>α</italic> ln(1+exp(<italic>h/α</italic>)) is a soft threshold-linear gain function mentioned above. <italic>I</italic><sup><italic>b</italic></sup> stands for external background inputs from other regions of the brain that reflect the general level of activity in the network, and <italic>I</italic><sup><italic>e</italic></sup> is the external input used to load memory stimuli. <italic>w</italic><sup><italic>EI</italic></sup> and <italic>w</italic><sup><italic>IE</italic></sup> define the strength of feedback inhibition between stimulus and chunk clusters and the global inhibitory cluster. Furthermore, we assume that when a chunking cluster <italic>m</italic> gets activated by a chunking cue at <italic>t</italic><sub><italic>c</italic></sub> during the presentation, the weak inhibitory synapses are selectively strengthened between the chunking cluster and the stimulus clusters <italic>i</italic> in the same chunk presented before it:
<disp-formula id="eqn8">
<graphic xlink:href="607952v2_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
See <xref rid="figS1" ref-type="fig">Fig. S1</xref> for an illustration of the synaptic matrix before and after chunking. For the hierarchical structure in <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>, we generalize <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> to higher-level chunking clusters, such that the <italic>k</italic><sup><italic>th</italic></sup> level chunking clusters inhibit all the lower-level clusters presented before them (both chunking and stimulus).</p>
<p>The detailed synaptic mechanism for behavioral time scale plasticity such as <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> is subject to much active research [<xref ref-type="bibr" rid="c54">54</xref>–<xref ref-type="bibr" rid="c58">58</xref>]. Here in the RNN model, we do not attempt to explain its mechanism but rather assume that it takes place via external control. The microscopic implementation of <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> is not crucial to the proposed chunking mechanism, and in Methods Sec. D, we present additional RNN simulations that adopt a possible implementation of <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> and achieve similar activity traces as in <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref> and <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>.</p>
</sec>
<sec id="s4b">
<label>B.</label>
<title>The new magic number</title>
<p>At any given moment, the network cannot maintain more than <italic>C</italic> active clusters (<xref rid="fig1" ref-type="fig">Fig. 1(d)</xref> top panel illustrates the case of <italic>C</italic> = 4), and we refer to <italic>C</italic> as the basic working memory capacity. Even though we can potentially encode an arbitrarily deep hierarchical representation, <italic>C</italic> nevertheless constrains how many stimulus clusters can be retrieved. To understand the consequence of this constraint, we abstract away from the recurrent neural network and consider the effective hierarchical representation entailed by its activity (<xref rid="fig2" ref-type="fig">Fig. 2(a)</xref>).</p>
<p>Let us denote the size of the <italic>m</italic><sup><italic>th</italic></sup> chunk at the <italic>k</italic><sup><italic>th</italic></sup> level (1 <italic>≤k≤ K</italic>) as <italic>c</italic><sub><italic>km</italic></sub>, which is the same as the branching ratio of its parent level. For example, the effective treelike hierarchical structure in <xref rid="fig2" ref-type="fig">Fig. 2(a)</xref> has four chunks of two stimulus clusters at the <italic>k</italic> = 3 level. It proves to be instructive to first consider a slightly simplified setting, where at a given level <italic>k</italic> all the chunk sizes are the same horizontally, <italic>c</italic><sub><italic>km</italic></sub> = <italic>c</italic><sub><italic>k</italic></sub> for all chunks <italic>m</italic> (e.g., <italic>c</italic><sub>2<italic>m</italic></sub> = 2 for all four of the <italic>k</italic> = 3 level chunks in <xref rid="fig2" ref-type="fig">Fig. 2(a)</xref>). Later, we will relax this assumption and show that the result we derive below still holds.</p>
<p>To retrieve a chunk from the bottom of the hierarchy, i.e., the stimulus clusters that encode actual memories, we need to suppress nodes upstream of the desired chunk. As a result, children of the suppressed node will become reactivated. A series of suppressions from the top to the bottom of the hierarchy requires the working memory to simultaneously maintain <italic>c</italic><sub><italic>K</italic></sub> stimulus chunks from the bottom level, as well as <italic>c</italic><sub><italic>k</italic></sub> <italic>−</italic> 1 chunking clusters from each of the <italic>k</italic><sup><italic>th</italic></sup> level above (1<italic>≤ k &lt; K</italic>) that were not suppressed but become active due to the suppression of their parent. However, the total number of clusters that can be maintained must not exceed <italic>C</italic> (e.g., the total number of clusters enclosed by the blue dashed circles in <xref rid="fig2" ref-type="fig">Fig. 2(a)</xref> should not exceed 4),
<disp-formula id="eqn9">
<graphic xlink:href="607952v2_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Meanwhile, the total number of stimulus clusters encoded in the hierarchical structure is
<disp-formula id="eqn10">
<graphic xlink:href="607952v2_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To achieve maximum capacity, we maximize <xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref> subject to the constraint in <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>. Using the arithmetic and geometric mean inequality, we arrive at
<disp-formula id="eqn11">
<graphic xlink:href="607952v2_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the equality is saturated when the branching ratio (chunk size) <italic>c</italic><sub><italic>k</italic></sub> at all levels are equal,
<disp-formula id="eqn12">
<graphic xlink:href="607952v2_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We notice that <italic>M</italic><sub><italic>c</italic></sub>(<italic>K</italic>) monotonically increases with <italic>K</italic>. Since the chunk size considered here <italic>c</italic><sub><italic>k</italic></sub> needs to be an integer, we have the optimal level <italic>K</italic><sup><italic>∗</italic></sup> and optimal branching ratio <italic>c</italic><sup><italic>∗</italic></sup>
<disp-formula id="eqn13">
<graphic xlink:href="607952v2_eqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Substituting <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref> into <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>, we arrive at the ca-pacity
<disp-formula id="eqn14">
<graphic xlink:href="607952v2_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Next, let us consider relaxing the simplifying assumption of <italic>c</italic><sub><italic>km</italic></sub> = <italic>c</italic><sub><italic>k</italic></sub>. Without loss of generality, suppose that at the <italic>k</italic><sup><italic>th</italic></sup> level, <italic>c</italic><sub><italic>k m</italic></sub><italic>&gt; c</italic><sub><italic>km</italic>+1</sub>. In order to retrieve the <italic>m</italic><sup><italic>th</italic></sup>chunk at this level, the WM needs to at least maintain<italic>c</italic><sub><italic>km</italic></sub> clusters, which implies that when trying to retrieve the (<italic>m</italic> + 1)<sup><italic>th</italic></sup> chunk the WM is not saturated because all the levels above the <italic>k</italic><sup><italic>th</italic></sup> are identical for the <italic>m</italic><sup><italic>th</italic></sup> and (<italic>m</italic> + 1)<sup><italic>th</italic></sup> chunk. This is sub-optimal since our goal is to maximize <italic>M</italic>. Therefore, <italic>c</italic><sub><italic>km</italic>+1</sub> can be increased to at least as large as <italic>c</italic><sub><italic>km</italic></sub>. The same logic can be applied recursively to all levels of the hierarchy, which demands that the optimal hierarchical structure for maximum <italic>M</italic> has <italic>c</italic><sub><italic>km</italic></sub> = <italic>c</italic><sub><italic>k</italic></sub>, so we again arrive at <italic>M</italic> <sup><italic>∗</italic></sup> in <xref ref-type="disp-formula" rid="eqn14">Eq. (14)</xref>.</p>
</sec>
<sec id="s4c">
<label>C.</label>
<title>RNN simulations</title>
<p>Activity traces of all the dynamical variables in <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>-(8) are shown in the <xref rid="figS1" ref-type="fig">Fig. S1</xref>. In particular, the synaptic matrix <italic>J</italic><sub><italic>µv</italic></sub> before and after chunking in <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref> is shown for comparison. All simulation parameters are reported in <xref ref-type="table" rid="tbl1">Table I</xref>. All the external inputs <italic>I</italic><sup><italic>e</italic></sup> used for loading the memories are rectangular functions with support only at the presentation time, and have an amplitude of 750 Hz, and all the background input <italic>I</italic><sup><italic>b</italic></sup> has amplitude of |<italic>I</italic><sup><italic>b</italic></sup>| = 10 Hz. Additionally, the timing of the external control signals are summarized in below.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Simulation parameters in <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref> and <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>.</title></caption>
<graphic xlink:href="607952v2_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p><xref rid="fig1" ref-type="fig">Fig. 1(d)</xref> top panel: Stimulus starts to load at <italic>t</italic> = 1 s for a duration of 0.025 s with an interval of 0.45 s. Background input <italic>I</italic><sup><italic>b</italic></sup> has a constant value of 10 Hz.</p>
<p><xref rid="fig1" ref-type="fig">Fig. 1(d)</xref> bottom panel: Stimulus starts to load at <italic>t</italic> = 1 s for a duration of 0.025 s with an interval of 0.45 s. Chunking clusters are loaded for a duration of 0.025 s with an interval of 0.3 s. Background input <italic>I</italic><sup><italic>b</italic></sup> has a constant value of 10 Hz during the presentation stage and switches between 10 Hz and<italic>−</italic> 10 Hz for a duration of 1.35 s during the retrieval stage.</p>
<p><xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>: The <italic>k</italic> = 3 level stimulus clusters start to load at <italic>t</italic> = 1 s for a duration of 0.15 s with an interval of 0.45 s. <italic>k</italic> = 2, 3 level chunking clusters load for a duration of 0.01 s with an interval of 0.2 s. Background input <italic>I</italic><sup><italic>b</italic></sup> has a constant value of 10 Hz during the presentation stage and switches between 10 Hz and<italic>−</italic> 10 Hz for a duration of 0.8 s during the retrieval stage.</p>
</sec>
<sec id="s4d">
<label>D.</label>
<title>Additional simulations</title>
<p><xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> assumes that chunking clusters can quickly bind with the stimulus clusters from the same chunk. For such binding to be selective, the synapse of the stimulus clusters need to be able to maintain a memory trace of its past activities. In this section, we attempt to provide a possible mechanism. We assume that there is a time-delayed Hebbian-like strengthening on the inhibitory synapses from the chunking clusters to the stimulus clusters. Such strengthening integrates back in time over a window <italic>τ</italic><sub><italic>s</italic></sub> (<italic>τ</italic><sub><italic>f</italic></sub>≪ <italic>τ</italic><sub><italic>s</italic></sub> ≪ <italic>τ</italic><sub><italic>A</italic></sub>) for stimulus clusters that were presented before the activation of the chunking cluster, and strengthens the originally present but weak synapses between them. Given a stimulus cluster <italic>i</italic> presented within <italic>τ</italic><sub><italic>s</italic></sub> before the chunking cluster <italic>m</italic>, the strength of the inhibitory synapses <italic>J</italic><sub><italic>im</italic></sub> between them gets strengthened according to
<disp-formula id="eqn15">
<graphic xlink:href="607952v2_eqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula id="inline-eqn-27"><inline-graphic xlink:href="607952v2_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula> represents a time-delayed synapse that maintains memory traces over a window <italic>τ</italic><sub><italic>s</italic></sub>, <italic>σ</italic>(·) is a non-linear function chosen to be the same as the gain function for the firing rates, and <italic>θ</italic><sub>0</sub> is a threshold that filters out reactivations.</p>
<p>We expect <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref> to work in the regime where the external input to the network during presentation is much stronger than the subsequent reactivations, which is typically the case. Here, the reactivations are filtered out so that they do not contribute to the binding process and form cross-linking between different chunks. <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref> only strengthens the binding between the chunking cluster <italic>m</italic> and the stimulus cluster <italic>i</italic> that were presented within the <italic>τ</italic><sub><italic>s</italic></sub> time window, but not the stimuli that were presented outside of <italic>τ</italic><sub><italic>s</italic></sub> but reactivate during <italic>τ</italic><sub><italic>s</italic></sub>, which have much weaker amplitudes. As a result, the time-delayed augmentation effectively binds the chunking cluster with the stimulus clusters presented before it within <italic>τ</italic><sub><italic>s</italic></sub>. Time-delayed synapses were first introduced in the context of memory sequences [<xref ref-type="bibr" rid="c59">59</xref>–<xref ref-type="bibr" rid="c61">61</xref>], and are found to be related to behavioral time scale synaptic plasticity through dendritic computation [<xref ref-type="bibr" rid="c56">56</xref>, <xref ref-type="bibr" rid="c57">57</xref>, <xref ref-type="bibr" rid="c62">62</xref>].</p>
<p>As a potential detailed mechanism of <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>, we perform additional RNN simulations with <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref>. We find that <xref ref-type="disp-formula" rid="eqn6">Eq. (6)</xref>-(3) with <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref>, instead of <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>, is able to approximate the activity traces as in <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref> and <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref> (see <xref rid="figS2" ref-type="fig">Fig. S2</xref>). However, it requires fine-tuning between the presentation time and the integration window <italic>τ</italic><sub><italic>s</italic></sub>, as well as the threshold <italic>θ</italic><sub>0</sub>. We report the additional parameters used in <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref> below.</p>
<p>Parameters that are independent of the presentation times: <italic>τ</italic><sub><italic>J</italic></sub> = 75 s, <italic>J</italic><sub>min</sub> = 0, <italic>J</italic><sub>max</sub> = 10, <italic>κ</italic><sub><italic>J</italic></sub> = 1 Hz. Parameters that depend on the presentation times: <xref rid="figS2" ref-type="fig">Fig. S2) (a)-(b)</xref>: <italic>τ</italic><sub><italic>s</italic></sub> = 1.8 s and <italic>θ</italic><sub>0</sub> = 7000 Hz. <xref rid="figS2" ref-type="fig">Fig. S2) (c)</xref>: threshold <italic>θ</italic><sub>0</sub> is chosen to be proportional to the duration of the loading time with the external input: <italic>θ</italic><sub>0</sub> = 25600 Hz for <italic>J</italic> <sup>(2)<italic>⊣</italic>(3)</sup> and <italic>J</italic> <sup>(1)<italic>⊣</italic>(3)</sup>, but for <italic>J</italic> <sup>(1)<italic>⊣</italic>(2)</sup> is reduced by a factor of five, where we use <italic>J</italic> <sup>(<italic>k</italic>)<italic>⊣</italic>(<italic>l</italic>)</sup> to denote synaptic matrix components that correspond to the inhibition from level <italic>k</italic> to <italic>l</italic>; Integration window <italic>τ</italic><sub><italic>s</italic></sub> is chosen such that adjacent levels are shorter than skip levels: <italic>τ</italic><sub><italic>s</italic></sub> = 1.9 s for adjacent levels (<italic>k</italic> = 1 to <italic>k</italic> = 2 and <italic>k</italic> = 2 to <italic>k</italic> = 3) and <italic>τ</italic><sub><italic>s</italic></sub> = 3.1 s for skip level (<italic>k</italic> = 1 to <italic>k</italic> = 3).</p>
</sec>
<sec id="s4e">
<label>E.</label>
<title>Cognitive boundary neurons</title>
<p>Two types of boundary neurons are reported in [<xref ref-type="bibr" rid="c41">41</xref>]: neurons that code for soft boundaries (change of camera position after the cut) and neurons that code for hard boundaries (change of movie content after the cut). In the present study, we do not distinguish between the two types of neurons and classify both as boundary neurons. In <xref rid="fig3" ref-type="fig">Fig. 3</xref>, we pool together the raw firing rates of all the boundary (or non-boundary) neurons from a subject, then perform the z-score averaging across different subjects. We have excluded four subjects out of eighteen in [<xref ref-type="bibr" rid="c41">41</xref>] from our analysis, because in those subjects either no neurons responding to the onset of the movie were detected, or no neurons responding to the onset of the cut were detected. The z-score firing rates of non-boundary neurons from individual subjects are shown in the <xref rid="figS3" ref-type="fig">Fig. S3</xref>. Data analyzed in <xref rid="fig3" ref-type="fig">Fig. 3</xref> is downloaded from the DANDI Archive at <ext-link ext-link-type="uri" xlink:href="https://dandiarchive.org/dandiset/000207/0.220216.0323">https://dandiarchive.org/dandiset/000207/0.220216.0323</ext-link>.</p>
</sec>
</sec>
</body>
<back>
<sec id="s7" sec-type="supplementary">
<title>Supplementary figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption>
<title>Full activity trace of the bottom panel in <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref>.</title>
<p><bold>(a)</bold> Activity traces of all variables. From top to bottom: firing rates <italic>R</italic><sub><italic>µ</italic></sub>, background input currents<inline-formula id="inline-eqn-28"><inline-graphic xlink:href="607952v2_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, release probability <italic>u</italic><sub><italic>µ</italic></sub>, fraction of available neurotransmitters <italic>x</italic><sub><italic>µ</italic></sub>, amplitude of the recurrent strength <italic>A</italic><sub><italic>µ</italic></sub>, and effective recurrent self-connection strength<inline-formula id="inline-eqn-29"><inline-graphic xlink:href="607952v2_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula> <bold>(b)</bold> Snapshots of the synaptic matrix <italic>J</italic><sub><italic>µv</italic></sub> before and after chunking. Clusters 1-14 are stimulus clusters and 15-16 are chunking clusters. At <italic>t</italic> = 2 s, only the first chunk (the blue colors) is presented, chunking clusters are not activated, and only the recurrent self-connections are nonzero in the synaptic matrix. At <italic>t</italic> = 4.5 s, both chunks are formed by the chunking connections (dark blue colors in the top right corner).</p>
</caption>
<graphic xlink:href="607952v2_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption>
<title>Additional RNN simulations with delayed Hebbian plasticity.</title>
<p><bold>(a)</bold> Approximating the chunking dynamics in <xref rid="fig1" ref-type="fig">Fig. 1(d)</xref> using <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref> instead of <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>. Top: activity traces of the firing rates. Bottom: activity traces of the inhibitory connections from chunking clusters to stimulus clusters <italic>J</italic><sup><italic>SC</italic></sup>. <bold>(b)</bold> Snapshot of the synaptic matrix after chunking, resulting from the dynamics described in <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref>. <bold>(c)</bold> Approximating the chunking dynamics in <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref> using <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref> instead of <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>. Synaptic matrix components that correspond to the inhibition from level <italic>k</italic> to <italic>l</italic> are collectively denoted as <italic>J</italic> <sup>(<italic>k</italic>)<italic>⊣</italic>(<italic>l</italic>)</sup>. First three panels: firing rate activity traces of the clusters in <xref rid="fig2" ref-type="fig">Fig. 2(a)</xref>. Fourth and fifth panels: inhibitory connections between adjacent levels <italic>J</italic> <sup>(1)<italic>⊣</italic>(2)</sup> and <italic>J</italic> <sup>(2)<italic>⊣</italic>(3)</sup>, inhibitory connections between skip levels <italic>J</italic> <sup>(1)<italic>⊣</italic>(3)</sup>, resulting from the dynamics described in <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref>.</p>
</caption>
<graphic xlink:href="607952v2_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Individual 2D plots of <xref rid="fig3" ref-type="fig">Fig. 3(b)</xref>.</title>
<p>Individual subjects’ z-score firing rates of the non-boundary neurons are shown in blue, with one standard deviation included as shades. Black dashed lines denote <italic>t</italic> = 0 s where the movie cut occurs. Red dashed lines denote the location of the maximum firing rate of the boundary neurons. Results are pooled from the raw firing rates of all non-boundary neurons from that subject. Subject IDs are presented according to the data in [<xref ref-type="bibr" rid="c41">41</xref>]. While some subjects do not exhibit the qualitative trend as predicted (e.g., the firing rate of subject P64CS does not have a ramp, and TWH120 does not have a dip), most of the subjects’ firing rates follow the same qualitative trend as observed in the average plot in <xref rid="fig3" ref-type="fig">Fig. 3(a)</xref>.</p></caption>
<graphic xlink:href="607952v2_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>Fig.1-2 data are from numerical simulations. The code for simulation will be made available in a Github repository at a later stage. Fig.3 analyzed single-neuron recording data from [1] Fig.4 analyzed behavioral data extracted from [2]. [1] Zheng, J., Schjetnan, A.G.P., Yebra, M. et al. Neurons detect cognitive boundaries to structure episodic memories in humans. Nat Neurosci 25, 358-368 (2022). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41593-022-01020-w">https://doi.org/10.1038/s41593-022-01020-w</ext-link> [2] G. A. Miller and J. A. Selfridge, Verbal context and the recall of meaningful material, The American journal of psychology 63, 176 (1950).</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>All authors were involved in the conceptualization of the study. WZ conducted the numerical simulations and data analysis. All authors contributed to writing the manuscript. The authors declare no competing interests. All data analyzed in the manuscript are publicly available online. The simulation code will be made available in a GitHub repository upon publication.</p>
<p>We want to thank Tankut Can, Antonis Georgiou, MátéLengyel, Gianluigi Mongillo, Sandro Romani, Sebastian Seung, and Jie Zheng for helpful discussions. W.Z. is supported by Eric and Wendy Schmidt Member in Biology Fund and the Simons Foundation at the Institute for Advanced Study. MK is supported in part by a grant from Fran Morris Rosman and Richard Rosman. M.T. is supported by the Simons Foundation, MBZUAI-WIS Joint Program for Artificial Intelligence Research and Foundation Adelis.</p>
</ack>
<ref-list>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Baddeley</surname></string-name></person-group>, <article-title>Working memory</article-title>, <source>Science</source> <volume>255</volume>, <fpage>556</fpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>O.</given-names> <surname>Barak</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name></person-group>, <article-title>Working models of working memory</article-title>, <source>Current opinion in neurobiology</source> <volume>25</volume>, <fpage>20</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E. K.</given-names> <surname>Miller</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Lundqvist</surname></string-name>, and <string-name><given-names>A. M.</given-names> <surname>Bastos</surname></string-name></person-group>, <article-title>Working memory 2.0</article-title>, <source>Neuron</source> <volume>100</volume>, <fpage>463</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Cowan</surname></string-name></person-group>, <article-title>The magical mystery four: How is working memory capacity limited, and why?</article-title>, <source>Current directions in psychological science</source> <volume>19</volume>, <fpage>51</fpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. A.</given-names> <surname>Klein</surname></string-name>, <string-name><given-names>K. M.</given-names> <surname>Addis</surname></string-name>, and <string-name><given-names>M. J.</given-names> <surname>Kahana</surname></string-name></person-group>, <article-title>A comparative analysis of serial and free recall</article-title>, <source>Memory &amp; Cognition</source> <volume>33</volume>, <fpage>833</fpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Ward</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Tan</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Grenfell-Essam</surname></string-name></person-group>, <article-title>Examining the relationship between free recall and immediate serial recall: the effects of list length and output order</article-title>., <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>36</volume>, <fpage>1207</fpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Grenfell-Essam</surname></string-name> and <string-name><given-names>G.</given-names> <surname>Ward</surname></string-name></person-group>, <article-title>Examining the relationship between free recall and immediate serial recall: The role of list length, strategy use, and test expectancy</article-title>, <source>Journal of Memory and Language</source> <volume>67</volume>, <fpage>106</fpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F.</given-names> <surname>Gobet</surname></string-name>, <string-name><given-names>P. C.</given-names> <surname>Lane</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Croker</surname></string-name>, <string-name><given-names>P. C.</given-names> <surname>Cheng</surname></string-name>, <string-name><given-names>G. Jones</given-names>, <surname>Oliver</surname></string-name>, and <string-name><given-names>J. M.</given-names> <surname>Pine</surname></string-name></person-group>, <article-title>Chunking mechanisms in human learning</article-title>, <source>Trends in cognitive sciences</source> <volume>5</volume>, <fpage>236</fpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Thalmann</surname></string-name>, <string-name><given-names>A. S.</given-names> <surname>Souza</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Oberauer</surname></string-name></person-group>, <article-title>How does chunking help working memory?</article-title>, <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>45</volume>, <fpage>37</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F.</given-names> <surname>Mathy</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Feldman</surname></string-name></person-group>, <article-title>What’s magic about magic numbers? chunking and data compression in short-term memory</article-title>, <source>Cognition</source> <volume>122</volume>, <fpage>346</fpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H. A.</given-names> <surname>Simon</surname></string-name></person-group>, <article-title>How big is a chunk? by combining data from several experiments, a basic human memory unit can be identified and measured</article-title>., <source>Science</source> <volume>183</volume>, <fpage>482</fpage> (<year>1974</year>).</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Zhang</surname></string-name> and <string-name><given-names>H. A.</given-names> <surname>Simon</surname></string-name></person-group>, <article-title>Stm capacity for chinese words and idioms: Chunking and acoustical loop hypotheses</article-title>, <source>Memory &amp; Cognition</source> <volume>13</volume>, <fpage>193</fpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Saint-Aubin</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Poirier</surname></string-name></person-group>, <article-title>Immediate serial recall of words and nonwords: Tests of the retrieval-based hypothesis</article-title>, <source>Psychonomic Bulletin &amp; Review</source> <volume>7</volume>, <fpage>332</fpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G. A.</given-names> <surname>Miller</surname></string-name></person-group>, <article-title>The magical number seven, plus or minus two: Some limits on our capacity for processing information</article-title>., <source>Psychological review</source> <volume>63</volume>, <fpage>81</fpage> (<year>1956</year>).</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Tulving</surname></string-name> and <string-name><given-names>J. E.</given-names> <surname>Patkau</surname></string-name></person-group>, <article-title>Concurrent effects of contextual constraint and word frequency on immediate recall and learning of verbal material</article-title>., <source>Canadian Journal of Psychology/Revue canadienne de psychologie</source> <volume>16</volume>, <fpage>83</fpage> (<year>1962</year>).</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. T.</given-names> <surname>Maybery</surname></string-name>, <string-name><given-names>F. B.</given-names> <surname>Parmentier</surname></string-name>, and <string-name><given-names>D. M.</given-names> <surname>Jones</surname></string-name></person-group>, <article-title>Grouping of list items reflected in the timing of recall: Implications for models of serial verbal memory</article-title>, <source>Journal of Memory and Language</source> <volume>47</volume>, <fpage>360</fpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. N.</given-names> <surname>Henson</surname></string-name></person-group>, <article-title>Short-term memory for serial order: The start-end model</article-title>, <source>Cognitive psychology</source> <volume>36</volume>, <fpage>73</fpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Kalm</surname></string-name>, <string-name><given-names>M. H.</given-names> <surname>Davis</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Norris</surname></string-name></person-group>, <article-title>Neural mechanisms underlying the grouping effect in short-term memory</article-title>, <source>Human brain mapping</source> <volume>33</volume>, <fpage>1634</fpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Oberauer</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Lewandowsky</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Awh</surname></string-name>, <string-name><given-names>G. D.</given-names> <surname>Brown</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Conway</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Cowan</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Donkin</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Farrell</surname></string-name>, <string-name><given-names>G. J.</given-names> <surname>Hitch</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Hurlstone</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>Benchmarks for models of shortterm and working memory</article-title>., <source>Psychological bulletin</source> <volume>144</volume>, <fpage>885</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. M.</given-names> <surname>Fuster</surname></string-name></person-group>, <article-title>Unit activity in prefrontal cortex during delayed-response performance: neuronal correlates of transient memory</article-title>., <source>Journal of neurophysiology</source> <volume>36</volume>, <fpage>61</fpage> (<year>1973</year>).</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y.</given-names> <surname>Miyashita</surname></string-name> and <string-name><given-names>H. S.</given-names> <surname>Chang</surname></string-name></person-group>, <article-title>Neuronal correlate of pictorial short-term memory in the primate temporal cortexyasushi miyashita</article-title>, <source>Nature</source> <volume>331</volume>, <fpage>68</fpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P. S.</given-names> <surname>Goldman-Rakic</surname></string-name></person-group>, <article-title>Cellular basis of working memory</article-title>, <source>Neuron</source> <volume>14</volume>, <fpage>477</fpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. J.</given-names> <surname>Amit</surname></string-name></person-group>, <article-title>The hebbian paradigm reintegrated: Local reverberations as internal representations</article-title>, <source>Behavioral and Brain Sciences</source> <volume>18</volume>, <fpage>617</fpage>–<lpage>626</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Mongillo</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Barak</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name></person-group>, <article-title>Synaptic theory of working memory</article-title>, <source>Science</source> <volume>319</volume>, <fpage>1543</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. G.</given-names> <surname>Stokes</surname></string-name></person-group>, <article-title>‘activity-silent’working memory in prefrontal cortex: a dynamic coding framework</article-title>, <source>Trends in cognitive sciences</source> <volume>19</volume>, <fpage>394</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y.</given-names> <surname>Mi</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Katkov</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name></person-group>, <article-title>Synaptic correlates of working memory capacity</article-title>, <source>Neuron</source> <volume>93</volume>, <fpage>323</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. S.</given-names> <surname>Zucker</surname></string-name> and <string-name><given-names>W. G.</given-names> <surname>Regehr</surname></string-name></person-group>, <article-title>Short-term synaptic plasticity</article-title>, <source>Annual review of physiology</source> <volume>64</volume>, <fpage>355</fpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Mongillo</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name></person-group>, <article-title>Synaptic encoding of time in working memory</article-title>, <source>eLife</source> <pub-id pub-id-type="doi">10.7554/eLife.107005.1</pub-id> (<year>2025</year>).</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H.</given-names> <surname>Markram</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Wang</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name></person-group>, <article-title>Differential signaling via the same axon of neocortical pyramidal neurons</article-title>, <source>Proceedings of the National Academy of Sciences</source> <volume>95</volume>, <fpage>5323</fpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Pawelzik</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Markram</surname></string-name></person-group>, <article-title>Neural networks with dynamic synapses</article-title>, <source>Neural computation</source> <volume>10</volume>, <fpage>821</fpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. M.</given-names> <surname>Hempel</surname></string-name>, <string-name><given-names>K. H.</given-names> <surname>Hartman</surname></string-name>, <string-name><given-names>X.-J.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>G. G.</given-names> <surname>Turrigiano</surname></string-name>, and <string-name><given-names>S. B.</given-names> <surname>Nelson</surname></string-name></person-group>, <article-title>Multiple forms of short-term plasticity at excitatory synapses in rat medial prefrontal cortex</article-title>, <source>Journal of neurophysiology</source> <volume>83</volume>, <fpage>3031</fpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Markram</surname></string-name>, <string-name><given-names>P. H.</given-names> <surname>Goodman</surname></string-name>, <string-name><given-names>T. K.</given-names> <surname>Berger</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Ma</surname></string-name>, and <string-name><given-names>P. S.</given-names> <surname>Goldman-Rakic</surname></string-name></person-group>, <article-title>Heterogeneity in the pyramidal network of the medial prefrontal cortex</article-title>, <source>Nature neuroscience</source> <volume>9</volume>, <fpage>534</fpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Uziel</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Markram</surname></string-name></person-group>, <article-title>Synchrony generation in recurrent networks with frequency-dependent synapses</article-title>, <source>The Journal of neuroscience</source> <volume>20</volume>, <fpage>RC50</fpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S. A.</given-names> <surname>Fisher</surname></string-name>, <string-name><given-names>T. M.</given-names> <surname>Fischer</surname></string-name>, and <string-name><given-names>T. J.</given-names> <surname>Carew</surname></string-name></person-group>, <article-title>Multiple overlapping processes underlying short-term synaptic enhancement</article-title>, <source>Trends in neurosciences</source> <volume>20</volume>, <fpage>170</fpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. M.</given-names> <surname>Thomson</surname></string-name></person-group>, <article-title>Facilitation, augmentation and potentiation at central synapses</article-title>, <source>Trends in neurosciences</source> <volume>23</volume>, <fpage>305</fpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Fioravante</surname></string-name> and <string-name><given-names>W. G.</given-names> <surname>Regehr</surname></string-name></person-group>, <article-title>Short-term forms of presynaptic plasticity</article-title>, <source>Current opinion in neurobiology</source> <volume>21</volume>, <fpage>269</fpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Soni</surname></string-name> and <string-name><given-names>M. J.</given-names> <surname>Frank</surname></string-name></person-group>, <article-title>Adaptive chunking improves effective working memory capacity in a prefrontal cortex and basal ganglia circuit</article-title>, <source>eLife</source> <volume>13</volume>, <elocation-id>RP97894</elocation-id> (<year>2025</year>). <pub-id pub-id-type="doi">10.7554/eLife.97894</pub-id></mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Cowan</surname></string-name></person-group>, <article-title>The magical number 4 in short-term memory: A reconsideration of mental storage capacity</article-title>, <source>Behavioral and brain sciences</source> <volume>24</volume>, <fpage>87</fpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Pesenti</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Zago</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Crivello</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Mellet</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Samson</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Duroux</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Seron</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Mazoyer</surname></string-name>, and <string-name><given-names>N.</given-names> <surname>Tzourio-Mazoyer</surname></string-name></person-group>, <article-title>Mental calculation in a prodigy is sustained by right prefrontal and medial temporal areas</article-title>, <source>Nature neuroscience</source> <volume>4</volume>, <fpage>103</fpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Guida</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Gobet</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Tardieu</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Nicolas</surname></string-name>, <string-name><given-names>How</given-names> <surname>chunks</surname></string-name></person-group>, <article-title>long-term working memory and templates offer a cognitive explanation for neuroimaging data on expertise acquisition: a two-stage framework</article-title>, <source>Brain and cognition</source> <volume>79</volume>, <fpage>221</fpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>A. G.</given-names> <surname>Schjetnan</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Yebra</surname></string-name>, <string-name><given-names>B. A.</given-names> <surname>Gomes</surname></string-name>, <string-name><given-names>C. P.</given-names> <surname>Mosher</surname></string-name>, <string-name><given-names>S. K.</given-names> <surname>Kalia</surname></string-name>, <string-name><given-names>T. A.</given-names> <surname>Valiante</surname></string-name>, <string-name><given-names>A. N.</given-names> <surname>Mamelak</surname></string-name>,<string-name><given-names>G.</given-names> <surname>Kreiman</surname></string-name>, and <string-name><given-names>U.</given-names> <surname>Rutishauser</surname></string-name></person-group>, <article-title>Neurons detect cognitive boundaries to structure episodic memories in humans</article-title>, <source>Nature neuroscience</source> <volume>25</volume>, <fpage>358</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G. A.</given-names> <surname>Miller</surname></string-name> and <string-name><given-names>J. A.</given-names> <surname>Selfridge</surname></string-name></person-group>, <article-title>Verbal context and the recall of meaningful material</article-title>, <source>The American journal of psychology</source> <volume>63</volume>, <fpage>176</fpage> (<year>1950</year>).</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. M.</given-names> <surname>Zacks</surname></string-name>, <string-name><given-names>T. S.</given-names> <surname>Braver</surname></string-name>, <string-name><given-names>M. A.</given-names> <surname>Sheridan</surname></string-name>, <string-name><given-names>D. I.</given-names> <surname>Donaldson</surname></string-name>, <string-name><given-names>A. Z.</given-names> <surname>Snyder</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Ollinger</surname></string-name>, <string-name><given-names>R. L.</given-names> <surname>Buckner</surname></string-name>, and <string-name><given-names>M. E.</given-names> <surname>Raichle</surname></string-name></person-group>, <article-title>Human brain activity time-locked to perceptual event boundaries</article-title>, <source>Nature neuroscience</source> <volume>4</volume>, <fpage>651</fpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Ben-Yakov</surname></string-name> and <string-name><given-names>R. N.</given-names> <surname>Henson</surname></string-name></person-group>, <article-title>The hippocampal film editor: sensitivity and specificity to event boundaries in continuous experience</article-title>, <source>Journal of Neuroscience</source> <volume>38</volume>, <fpage>10057</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Baldassano</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Zadbood</surname></string-name>, <string-name><given-names>J. W.</given-names> <surname>Pillow</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Hasson</surname></string-name>, and <string-name><given-names>K. A.</given-names> <surname>Norman</surname></string-name></person-group>, <article-title>Discovering event structure in continuous narrative perception and memory</article-title>, <source>Neuron</source> <volume>95</volume>, <fpage>709</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B. A.</given-names> <surname>Maher</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Skovengaard</surname></string-name></person-group>, <article-title>Contextual constraint: a danish language replication of miller-selfridge with methodological improvements</article-title>, <source>Scandinavian journal of psychology</source> <volume>29</volume>, <fpage>194</fpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Sharma</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Sen</surname></string-name></person-group>, <article-title>Effect of verbal context on communication in hindi</article-title>., <source>Indian Journal of Psychology</source> (<year>1977</year>).</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Bartsch</surname></string-name></person-group>, <source>Structural and functional properties of collocations in English: A corpus study of lexical and pragmatic constraints on lexical co-occurrence</source> (<publisher-name>Gunter Narr Verlag</publisher-name>, <year>2004</year>).</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>I.</given-names> <surname>Mel’čuk</surname></string-name></person-group>, <chapter-title>Collocations and lexical functions</chapter-title>, <person-group person-group-type="editor"><string-name><given-names>A.P.</given-names> <surname>Cowie</surname></string-name></person-group> <source>Phraseology. Theory, analysis, and applications</source>, <publisher-name>Oxford University Press</publisher-name> (<year>1998</year>).</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Evert</surname></string-name> <etal>et al.</etal></person-group>, <article-title>Corpora and collocations, Corpus linguistics</article-title>. <source>An international handbook</source> <volume>2</volume>, <fpage>1212</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>K. R.</given-names> <surname>McKeown</surname></string-name> and <string-name><given-names>D. R.</given-names> <surname>Radev</surname></string-name></person-group>, <chapter-title>Collocations</chapter-title>. <person-group person-group-type="editor"><string-name><given-names>R.</given-names> <surname>Dale</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Moisl</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Somers</surname></string-name></person-group> <source>A Handbook of Natural Language Processing</source>, <publisher-name>Marcel Dekker</publisher-name> (<year>2000</year>).</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. A.</given-names> <surname>Kurby</surname></string-name> and <string-name><given-names>J. M.</given-names> <surname>Zacks</surname></string-name></person-group>, <article-title>Segmentation in the perception and memory of events</article-title>, <source>Trends in cognitive sciences</source> <volume>12</volume>, <fpage>72</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Chekaf</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Gauvrit</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Guida</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>Mathy</surname></string-name></person-group>, <article-title>Compression in working memory and its relationship with fluid intelligence</article-title>, <source>Cognitive Science</source> <volume>42</volume>, <fpage>904</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. A.</given-names> <surname>Nicoll</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Schmitz</surname></string-name></person-group>, <article-title>Synaptic plasticity at hippocampal mossy fibre synapses</article-title>, <source>Nature Reviews Neuroscience</source> <volume>6</volume>, <fpage>863</fpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Vandael</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Borges-Merjane</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Zhang</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Jonas</surname></string-name></person-group>, <article-title>Short-term plasticity at hippocampal mossy fiber synapses is induced by natural activity patterns and associated with vesicle pool engram formation</article-title>, <source>Neuron</source> <volume>107</volume>, <fpage>509</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. C.</given-names> <surname>Bittner</surname></string-name>, <string-name><given-names>A. D.</given-names> <surname>Milstein</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Grienberger</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Romani</surname></string-name>, and <string-name><given-names>J. C.</given-names> <surname>Magee</surname></string-name></person-group>, <article-title>Behavioral time scale synaptic plasticity underlies ca1 place fields</article-title>, <source>Science</source> <volume>357</volume>, <fpage>1033</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. D.</given-names> <surname>Milstein</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>K. C.</given-names> <surname>Bittner</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Grienberger Soltesz</surname></string-name>, <string-name><given-names>J. C.</given-names> <surname>Magee</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Romani</surname></string-name></person-group>, <article-title>Bidirectional synaptic plasticity rapidly modifies hippocampal representations</article-title>, <source>eLife</source> <volume>10</volume>, <elocation-id>e73046</elocation-id> (<year>2021</year>). <pub-id pub-id-type="doi">10.7554/eLife.73046</pub-id></mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Pang</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Recanatesi</surname></string-name></person-group>, <article-title>A non-hebbian code for episodic memory</article-title>, <source>bioRxiv</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H.</given-names> <surname>Sompolinsky</surname></string-name> and <string-name><given-names>I.</given-names> <surname>Kanter</surname></string-name></person-group>, <article-title>Temporal association in asymmetric neural networks</article-title>, <source>Physical review letters</source> <volume>57</volume>, <fpage>2861</fpage> (<year>1986</year>).</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Kleinfeld</surname></string-name></person-group>, <article-title>Sequential state generation by model neural networks</article-title>., <source>Proceedings of the National Academy of Sciences</source> <volume>83</volume>, <fpage>9469</fpage> (<year>1986</year>).</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>J. A.</given-names> <surname>Hertz</surname></string-name></person-group>, <source>Introduction to the theory of neural computation</source> (<publisher-name>Crc Press</publisher-name>, <year>2018</year>).</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>London</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Häusser</surname></string-name></person-group>, <article-title>Dendritic computation</article-title>, <source>Annu. Rev. Neurosci</source>. <volume>28</volume>, <fpage>503</fpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="dataref1"><mixed-citation publication-type="data" specific-use="analyzed"><person-group person-group-type="author"><string-name><surname>Zheng</surname> <given-names>J.</given-names></string-name>, <string-name><surname>Schjetnan</surname> <given-names>A.G.</given-names></string-name>, <string-name><surname>Yebra</surname> <given-names>M.</given-names></string-name>, <string-name><surname>Gomes</surname> <given-names>B.A.</given-names></string-name>, <string-name><surname>Mosher</surname> <given-names>C.P.</given-names></string-name>, <string-name><surname>Kalia</surname> <given-names>S.K.</given-names></string-name>, <string-name><surname>Valiante</surname> <given-names>T.A.</given-names></string-name>, <string-name><surname>Mamelak</surname> <given-names>A.N.</given-names></string-name>, <string-name><surname>Kreiman G. and Rutishauser</surname></string-name>, <string-name><surname>U</surname></string-name>, </person-group> (<year iso-8601-date="2022">2022</year>) <article-title>Data for: Neurons detect cognitive boundaries to structure episodic memories in humans (Zheng et al., 2022, Nat Neuro in press)</article-title>. <source>DANDI</source>. <pub-id pub-id-type="accession" xlink:href="https://dandiarchive.org/dandiset/000207/0.220216.0323">000207/0.220216.0323</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109538.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Lim</surname>
<given-names>Sukbin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02vpsdb40</institution-id><institution>New York University Shanghai</institution>
</institution-wrap>
<city>Shanghai</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study links psychological theories of chunking with a physiological implementation based on short-term synaptic plasticity and synaptic augmentation. The theoretical derivation for increased memory capacity via hierarchical chunking is <bold>solid</bold>. However, the model robustness and biological grounding of the mechanism - including many aspects that were hard-wired, chunking cues, and parameter ranges - as well as its evaluation in the task settings that motivated the study, are <bold>incomplete</bold>. Additional simulations to test robustness in more cognitively and biologically realistic settings, a systematic parameter analysis, and stronger links to prior work would substantially strengthen the manuscript and increase its impact across disciplines.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109538.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study extends the short-term synaptic plasticity (STP)-based theory of activity-silent working memory (WM) by introducing a physiological mechanism for chunking that relies on synaptic augmentation (SA) and specialized chunking clusters. The model consists of a recurrent neural network comprising excitatory clusters representing individual items and a global inhibitory pool. The self-connections within each cluster dynamically evolve through the combined effects of STP and SA. When a chunking cue, such as a brief pause in a stimulus sequence, is presented, the chunking cluster transiently suppresses the activity of the item clusters, enabling the grouped items to be maintained as a coherent unit and subsequently reactivated in sequence. This mechanism allows the network to enhance its effective memory capacity without exceeding the number of simultaneously active clusters, which defines the basic capacity. They further derive a new upper limit of WM capacity, the new magic number. When the basic capacity is four, the upper bound for complete recall becomes eight, and the optimal hierarchical structure corresponds to a binary tree of two-item pairs forming four chunks that combine into two meta-chunks. Reanalysis of linguistic data and single-neuron recordings from human epilepsy patients (identifying boundary neurons) provides qualitative support for the model's predictions.</p>
<p>Strengths:</p>
<p>This study makes an important contribution to theoretical and computational neuroscience by proposing a physiologically grounded mechanism for chunking based on STP and SA. By embedding these processes in a recurrent neural network, the authors provide a unified account of how chunks can be formed, maintained, and sequentially retrieved through local circuit dynamics, rather than through top-down cognitive strategies. The work is conceptually original, analytically rigorous, and clearly presented, deriving a simple yet powerful capacity law that extends the classical magic number framework from four to eight items under hierarchical chunking. The modeling results are further supported by preliminary empirical evidence from linguistic data and single-neuron recordings in the human medial temporal lobe, lending credibility to the proposed mechanism. Overall, this is a well-designed and well-written study that offers novel insights into the neural basis of working-memory capacity and establishes a solid bridge between theoretical modeling and experimental findings.</p>
<p>Weaknesses:</p>
<p>This study is conceptually strong and provides an elegant theoretical framework, but several aspects limit its biological and empirical grounding.</p>
<p>First, the control mechanism that triggers and suppresses chunking clusters remains only schematically defined. The model assumes that chunking events are initiated by pauses, prosodic cues, or internal control signals, but does not specify the underlying neural circuits (e.g., prefrontal-basal ganglia loops) that could mediate this gating in the brain. Clarifying where, when, and how the chunking clusters are turned on and off will be critical for establishing biological plausibility.</p>
<p>Second, the network representation is simplified: item clusters are treated as non-overlapping and homogeneous, whereas real cortical circuits exhibit overlapping representations, distinct excitatory/inhibitory populations, and multiscale local and long-range connectivity. It remains unclear how robust the proposed dynamics and derived capacity limit would be under such biologically realistic conditions.</p>
<p>Third, the model heavily relies on SA operating over a timescale of several seconds, yet in vivo, the time constants and prevalence of SA can vary widely across cortical regions and neuromodulatory states. The stability of the predicted &quot;new magic number&quot; under realistic noise levels and modulatory influences, therefore, needs to be systematically evaluated.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109538.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work extends a previous recurrent neural network model of activity-silent working memory to account for well-established findings from psychology and neuroscience suggesting that working memory capacity constraints can be partially overcome when stimuli can be organized into chunks. This is accomplished via the introduction of specialized chunking clusters of neurons to the original model. When these chunking clusters are activated by a cue (such as a longer delay between stimuli), they rapidly suppress recently active stimulus clusters. This makes these stimulus clusters available for later retrieval via a synaptic augmentation mechanism, thereby expanding the network's overall effective capacity. Furthermore, these chunking clusters can be arranged in a hierarchical fashion, where chunking clusters are themselves chunked by higher-level chunking clusters, further expanding the network's overall effective capacity to a new &quot;magic number&quot;, 2^{C-1} (where C is the basic capacity without chunking). In addition to illustrating the basic dynamics of the model with detailed simulations (Figures 1 and 2), the paper also utilizes qualitative predictions from the model to (re-)analyze data collected in previous experiments, including single-unit recordings from human medial temporal lobe as well as behavioral findings from a classic study of human memory.</p>
<p>Strengths:</p>
<p>The writing and figures are very clear, and the general topic is relevant to a broad interdisciplinary audience. The work is strongly theory-driven, but also makes some effort to engage with existing data from two empirical studies. The basic results showcasing how chunking can be achieved in an activity-silent working memory model via suppression and synaptic augmentation dynamics are interesting. Furthermore, we agree with the authors that the derivation of their new &quot;magic number&quot; is relatively general and could apply to other models, so those findings in particular may be of interest even to researchers using different modeling frameworks.</p>
<p>Weaknesses:</p>
<p>(1) Very important aspects of the model are assumed / hard-coded, raising the concern that it relies too much on an external controller, and that it would therefore be difficult to implement the same principles in a fully behaving model responsible for producing its own outputs from a sequence of stimuli (i.e., without a priori knowledge of the structure of incoming sequences).</p>
<p>(i) One such aspect is the use of external chunking cues provided to the model at critical times to activate the chunking clusters. The simulations reported in the paper were conducted in a setting where signals to chunk are conveniently indicated by longer delays between stimuli. In this case, it is not difficult to imagine how an external component could detect the presence of such a delay and activate a chunking cluster in response. However, in order for the model to be more broadly applicable to different memory tasks that elicit chunking-related phenomena, a more general-purpose detector would be required (see further comments below and alternative models).</p>
<p>(ii) Relatedly, and as the authors acknowledge in the discussion, the network relies on a pretty sophisticated external controller that decides when the individual chunking clusters are activated or deactivated during readout/retrieval. This seems especially complex in the hierarchical case. How might a network decide which chunking/meta-chunking clusters are activated/deactivated in which order? This was hard-coded in their simulations, but we imagine that it would be difficult to implement a general solution to this problem, especially in cases where there is ambiguity about which stimuli should be chunked, or where the structure of the incoming sequence is not known in advance.</p>
<p>(iii) One of the central mechanisms of the model is the rapid synaptic plasticity in the inhibitory connections responsible for binding chunking clusters to their corresponding stimulus clusters. This mechanism again appears to have been hard-coded in the main simulations. Although we appreciate that the authors worked on one possible way that this could be implemented (Methods section D, Supplementary Figure S2), in the end, their solution seems to rely on precisely fine-tuning the timing with which stimuli are presented - a factor that seems unlikely to matter very much in humans/animals. This stands in contrast with models of working memory that rely on persistent activity, which are more robust to changes in timing. Note that we do not discount the possibility of activity-silent WM, and indeed it should be studied in its own right, but it is then even more important to highlight which of its features are dependent on the time constants, etc.</p>
<p>(2) Another key shortcoming of this work is its limited direct engagement with empirical evidence and alternative computational accounts of chunking in WM. Although the efforts to re-analyze existing empirical results in light of the new predictions made by the model are commendable, in the end, we think they fall short of being convincing. As noted above, the model doesn't actually perform the same two tasks used in the human experiments, so direct quantitative comparisons between the model and human behavior or neural data are not possible. Instead, the authors rely on isolating two qualitative predictions of the model - the &quot;dip&quot; and &quot;ramp&quot; phenomena observed after a chunking cluster is activated (Figure 3), and the new magic number for effective capacity derived from the model in the case where stimuli are chunkable, which approximately converges with human recall performance in a memory study (Figure 4). Below, we highlight some specific issues related to these two sets of analyses, but the larger point is that if the model is making a commitment about how these neural mechanisms relate to behavioral phenomena, it would be important to test if the model can produce the behavioral patterns of data in experimental paradigms that have been extensively used to characterize those phenomena. For example, modern paradigms characterizing capacity limits have been more careful to isolate the contributions of WM per se (whereas the original magic number 7 is now thought to reflect a combination of episodic and working memory; see Cowan 2010). There are several existing models that more directly engage with this literature (e.g., Edin et al., 2009; Matthey et al., 2015; Nassar et al., 2018; Soni &amp; Frank, 2025; Swan &amp; Wyble, 2014; van den Berg et al., 2014; Wei et al., 2012), some of which also account for chunking-related phenomena (e.g., Wei et al, 2012; Nassar et al., 2018; Panichello et al., 2019; Soni &amp; Frank, 2025). A number of related proposals suggest that WM capacity limits emerge from fundamentally different mechanisms than the one considered here - for example, content-related interference (Bays, 2014; Ma et al., 2014; Schurgin et al., 2020), or limitations in the number of content-independent pointers that can be deployed at a given time (Awh &amp; Vogel, 2025), and/or the inherent difficulty of learning this binding problem (Soni &amp; Frank, 2025). We think it would be worth discussing how these ideas could be considered complementary or alternatives to the ones presented here.</p>
<p>(i) Single unit recordings. We found it odd that the authors chose to focus on evidence from single-unit recordings in the medial temporal lobe from a study focused on episodic memory. It was unclear how exactly these data are supposed to relate to their proposal. Is the suggestion that a mechanism similar to the boundary neurons might be operative in the case of working memory over shorter timescales in WM-related areas such as the prefrontal cortex, or that their chunking mechanism may relate not only to working memory but also to episodic memory in the medial temporal lobe?</p>
<p>(ii) N-gram memory experiment. Our main complaint about the analysis of the behavioral data from the human memory study (Figure 4) is that the model clearly does not account for the main effect observed in that study - namely, the better recall observed for higher-order n-gram approximations to English. We acknowledge that this was perhaps not the main point of the analysis (which related more to the prediction about the absolute capacity limit M*), but it relates to a more general criticism that the model cannot account for chunking behavior associated with statistical learning or semantic similarity. Most of the examples used in the introduction and discussion are of this kind (e.g., expressions such as &quot;Oh my God&quot; or &quot;Easier said than done&quot;, etc.). However, the chunking mechanism of the model should not have any preference for segmenting based on statistical regularities or semantic similarity - it should work just as well if statistical anomalies or semantic dissimilarity were used as external chunking cues. In our view, these kinds of effects are likely to relate to the brain's use of distributed representations that can capture semantic similarity and learn statistical regularities in the environment. Although these kinds of effects may be beyond the scope of this model, some effort could be made to highlight this in the discussion. But again, more generally, the paper would be more compelling if the model were challenged to simulate more modern experimental paradigms aimed at testing the nature of capacity limits in WM, or chunking, etc.</p>
<p>(iii) There are a number of other empirical phenomena that we're not sure the model can explain. In particular, one of the hallmarks of WM capacity limits is that it suffers from a recency bias, where people are more likely to remember the most recent items at the expense of items presented prior to that (Oberauer et al 2012). [There are also studies showing primacy effects in addition to recency effects, but the primacy effects are generally attributed to episodic rather than working memory - for example, introducing a distractor task abolishes the recency but not primacy effect]. But the current model seems to make the opposite prediction: when the stimuli exceed its base capacity, it appears to forget the most recent stimuli rather than the earliest ones (Figure 1d). This seems to result from the number of representations that can be reactivated within a cycle and thus seems inherent to the dynamics of the model, but the authors can clarify if, instead, it depends on the particular values of certain parameters. (In contrast, this recency effect is captured in other models with chunking capabilities based on attractive dynamics and/or gating mechanisms - eg Boboeva et al 2023; Soni &amp; Frank (2025)). Relatedly, we're not sure if the model could account for the more recent finding that recall is specifically enhanced when chunks occur in early serial positions compared to later ones (Thalmann, Souza, Oberauer, 2019).</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109538.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The paper presents a synaptic mechanism for chunking in working memory, extending previous work of the last author by introducing specialized &quot;chunking clusters&quot;, neural populations that can dynamically segment incoming items into chunks. The idea is that this enables hierarchical representations that increase the effective capacity of working memory. They also derive a theoretical bound for working memory capacity based on this idea, suggesting that hierarchical chunking expands the number of retrievable items beyond the basic WM capacity. Finally, they present neural and behavioral data related to their hypothesis.</p>
<p>Strengths</p>
<p>A major strength of the paper is its clear theoretical ambition of developing a mechanistic model of working memory chunking.</p>
<p>Weaknesses</p>
<p>Despite the inspiration in biophysical mechanisms (short-term synaptic plasticity with different time constants), the model is &quot;cartoonish&quot;. It is unclear whether the proposed mechanism would work reliably in the presence of noise and non-zero background activity or in a more realistic implementation (e.g., a spiking network).</p>
<p>As far as I know, there is no evidence for cyclic neural activation patterns, which are supposed to limit WM capacity (such as in Figure 1d). In fact, I believe there is no evidence for population bursts in WM, which are a crucial ingredient of the model. For example, Panicello et al. 2024 have found evidence for periods during which working memory decoding accuracy decreases, but no population bursts were observed in their data. In brief, my critique is that including some biophysical mechanism in an abstract model does not make the model plausible per se.</p>
<p>It is claimed that &quot;our proposed chunking mechanism applies to both the persistent-activity and periodic-activity regimes, with chunking clusters serving the same function in each&quot;, but this is not shown. If the results and model predictions are the same, irrespective of whether WM is activity-silent or persistent, I suggest highlighting this more and including the corresponding simulations.</p>
<p>The empirical validations of the model are weak. The single-unit analysis is purely descriptive, without any statistical quantification of the apparent dip-ramp pattern. I agree that the dip-ramp pattern may be consistent with the proposed model, but I don't believe that this pattern is a specific prediction of the proposed model. It seems just to be an interesting observation that may be compatible with several network mechanisms involving some inhibition and a rebound.</p>
<p>Moreover, the reanalyses of n-gram behavioral data do not constitute a mechanistic test of the model. The &quot;new magic number&quot; depends strongly on structural assumptions about how chunking operates, and it is unclear whether human working memory uses the specific hierarchical scheme required to achieve the predicted limit.</p>
<p>The presentation of the modeling results is highly compressed in two figures and is rather hard to follow. Plotting the activity of different neural clusters in separate subplots or as heatmaps (x-axis time, y-axis neural population, color = firing rate) would help to clarify (Figure 1d). Also, control signals that activate the chunking clusters should be shown.</p>
<p>Overall, the theoretical proposal is interesting, but its empirical grounding and biological plausibility need to be substantially reinforced.</p>
</body>
</sub-article>
</article>