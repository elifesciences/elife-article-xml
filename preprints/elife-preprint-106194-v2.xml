<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106194</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106194</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106194.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Virtual Brain Inference (VBI): A flexible and integrative toolkit for efficient probabilistic inference on virtual brain models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4696-9947</contrib-id>
<name>
<surname>Ziaeemehr</surname>
<given-names>Abolfazl</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
<email>abolfazl.ziaee-mehr@univ-amu.fr</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8410-4581</contrib-id>
<name>
<surname>Woodman</surname>
<given-names>Marmaduke</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4822-2046</contrib-id>
<name>
<surname>Domide</surname>
<given-names>Lia</given-names>
</name>
<xref ref-type="aff" rid="a2">b</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4540-6293</contrib-id>
<name>
<surname>Petkoski</surname>
<given-names>Spase</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8251-8860</contrib-id>
<name>
<surname>Jirsa</surname>
<given-names>Viktor</given-names>
</name>
<xref ref-type="author-notes" rid="n1">‚Ä†</xref>
<xref ref-type="aff" rid="a1">a</xref>
<email>viktor.jirsa@univ-amu.fr</email>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5289-9837</contrib-id>
<name>
<surname>Hashemi</surname>
<given-names>Meysam</given-names>
</name>
<xref ref-type="author-notes" rid="n1">‚Ä†</xref>
<xref ref-type="aff" rid="a1">a</xref>
<email>meysam.hashemi@univ-amu.fr</email>
</contrib>
<aff id="a1"><label>a</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Aix Marseille Univ, INSERM, INS, Inst Neurosci Syst</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff>
<aff id="a2"><label>b</label><institution>Codemart</institution>, <city>Cluj-Napoca</city>, <country country="RO">Romania</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Fornito</surname>
<given-names>Alex</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Monash University</institution>
</institution-wrap>
<city>Clayton</city>
<country country="AU">Australia</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country country="NL">Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>‚Ä†</label><p>These authors contributed equally.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-05-21">
<day>21</day>
<month>05</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-08-28">
<day>28</day>
<month>08</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106194</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-02-11">
<day>11</day>
<month>02</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-01-22">
<day>22</day>
<month>01</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.01.21.633922"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-05-21">
<day>21</day>
<month>05</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106194.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.106194.1.sa3">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.106194.1.sa2">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.106194.1.sa1">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.106194.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>¬© 2025, Ziaeemehr et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Ziaeemehr et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106194-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Network neuroscience has proven essential for understanding the principles and mechanisms underlying complex brain (dys)function and cognition. In this context, wholebrain network modeling‚Äìalso known as virtual brain modeling‚Äìcombines computational models of brain dynamics (placed at each network node) with individual brain imaging data (to coordinate and connect the nodes), advancing our understanding of the complex dynamics of the brain and its neurobiological underpinnings. However, there remains a critical need for automated model inversion tools to estimate control (bifurcation) parameters at large scales associated with neuroimaging modalities, given their varying spatio-temporal resolutions. This study aims to address this gap by introducing a flexible and integrative toolkit for efficient Bayesian inference on virtual brain models, called Virtual Brain Inference (VBI). This open-source toolkit provides fast simulations, taxonomy of feature extraction, efficient data storage and loading, and probabilistic machine learning algorithms, enabling biophysically interpretable inference from non-invasive and invasive recordings. Through in-silico testing, we demonstrate the accuracy and reliability of inference for commonly used whole-brain network models and their associated neuroimaging data. VBI shows potential to improve hypothesis evaluation in network neuroscience through uncertainty quantification, and contribute to advances in precision medicine by enhancing the predictive power of virtual brain models.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Virtual brain models</kwd>
<kwd>Simulation-based inference</kwd>
<kwd>Structural and functional neuroimaging modalities</kwd>
<kwd>Low-dimensional data features</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We have adapted the Wilson-Cowan model to follow the same brain network modeling notation as the other models. Additionally, we have included multiple figures in the supplementary material presenting extensive in-silico testing to demonstrate the accuracy and reliability of the estimations across different configurations, as well as the sensitivity to both additive and dynamical noise.
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1.</label>
<title>Introduction</title>
<p>Understanding the complex dynamics of the brain and their neurobiological underpinnings, with the potential to advance precision medicine [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c4">4</xref>], is a central goal in neuroscience. Modeling these dynamics provides crucial insights into causality and mechanisms underlying both normal brain function and various neurological disorders [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref>]. By integrating the average activity of large populations of neurons (e.g., neural mass models; [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>]) with information provided by structural imaging modalities (i.e., connectome; [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c17">17</xref>]), whole-brain network modeling has proven to be a powerful tractable approach for simulating brain activities and emergent dynamics as recorded by functional imaging modalities (such as (s)EEG/MEG/fMRI; [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref>].</p>
<p>The whole-brain models have been well-established in network neuroscience [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>] for understanding the brain structure and function [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>] and investigating the mechanisms underlying brain dynamics at rest [<xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>], normal aging [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c38">38</xref>], and also altered states such as anaesthesia and loss of consciousness [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>, <xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c42">42</xref>]. This class of computational models, also known as virtual brain models [<xref ref-type="bibr" rid="c43">43</xref>, <xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c45">45</xref>, <xref ref-type="bibr" rid="c46">46</xref>], have shown remarkable capability in delineating the pathophysiological causes of a wide range of brain diseases, such as epilepsy [<xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c6">6</xref>], multiple sclerosis [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>], Alzheimer‚Äôs disease [<xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c52">52</xref>], Parkinson‚Äôs disease [<xref ref-type="bibr" rid="c53">53</xref>, <xref ref-type="bibr" rid="c54">54</xref>], neuropsychiatric disorders [<xref ref-type="bibr" rid="c55">55</xref>, <xref ref-type="bibr" rid="c56">56</xref>], stroke [<xref ref-type="bibr" rid="c57">57</xref>, <xref ref-type="bibr" rid="c58">58</xref>] and focal lesions [<xref ref-type="bibr" rid="c59">59</xref>]. In particular, they enable the personalized simulation of both normal and abnormal brain activities, along with their associated imaging recordings, thereby stratifying between healthy and diseased states [<xref ref-type="bibr" rid="c60">60</xref>, <xref ref-type="bibr" rid="c61">61</xref>, <xref ref-type="bibr" rid="c52">52</xref>] and potentially informing targeted interventions and treatment strategies [<xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c62">62</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c45">45</xref>, <xref ref-type="bibr" rid="c23">23</xref>]. Although there are only a few tools available for forward simulations at the whole-brain level, e.g., the brain network simulator The Virtual Brain (TVB; [<xref ref-type="bibr" rid="c44">44</xref>]), there is a lack of tools for addressing the inverse problem, i.e., finding the set of control (generative) parameters that best explains the observed data. This study aims to bridge this gap by addressing the inverse problem in large-scale brain networks, a crucial step toward making these models operable for clinical applications.</p>
<p>Accurately and reliably estimating the parameters of whole-brain models remains a formidable challenge, mainly due to the high-dimensionality and nonlinearity inherent in brain activity data, as well as the non-trivial effects of noise and network inputs. A large number of previous studies in whole-brain modeling have relied on optimization techniques to identify a single optimal value from an objective function, scoring the model‚Äôs performance against observed data [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c63">63</xref>, <xref ref-type="bibr" rid="c64">64</xref>]. This approach often involves minimizing metrics such as the Kolmogorov-Smirnov distance or maximizing the Pearson correlation between observed and generated data features such as functional connectivity (FC), functional connectivity dynamics (FCD), and/or power spectral density (PSD). Although fast, such a parametric approach results in only point estimates and fails to capture the relationship between parameters and their associated uncertainty. This limits the generalizability of findings, and hinders identifiability analysis, which explores the uniqueness of solutions. Furthermore, optimization algorithms can easily get stuck in local extrema, requiring multi-start strategies to address potential parameter degeneracies. These additional steps, while necessary, ultimately increase the computational cost. Critically, the estimation heavily depends on the form of the objective function defined for optimization [<xref ref-type="bibr" rid="c65">65</xref>, <xref ref-type="bibr" rid="c66">66</xref>]. These limitations can be overcome by employing Bayesian inference, which naturally quantifies the uncertainty in the estimation and statistical dependencies between parameters, leading to more robust and generalizable models. Bayesian inference is a principal method for updating prior beliefs with information provided by data through the likelihood function, resulting in a posterior probability distribution that encodes all the information necessary for inferences and predictions. This approach has proven essential for understanding the intricate relationships between brain structure and function [<xref ref-type="bibr" rid="c67">67</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c59">59</xref>], as well as for revealing the pathophysiological causes underlying brain disorders [<xref ref-type="bibr" rid="c68">68</xref>, <xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c46">46</xref>, <xref ref-type="bibr" rid="c69">69</xref>].</p>
<p>In this context, simulation-based inference (SBI; [<xref ref-type="bibr" rid="c70">70</xref>, <xref ref-type="bibr" rid="c71">71</xref>, <xref ref-type="bibr" rid="c68">68</xref>, <xref ref-type="bibr" rid="c69">69</xref>]) has gained prominence as an efficient methodology for conducting Bayesian inference in complex models where traditional inference techniques become inapplicable. SBI leverages computational simulations to generate synthetic data and employs advanced probabilistic machine learning methods to infer the joint distribution over parameters that best explain the observed data, along with associated uncertainty. This approach is particularly well-suited for Bayesian inference on whole-brain models, which often exhibit complex dynamics that are difficult to retrieve from neuroimaging data with conventional estimation techniques. Crucially, SBI circumvents the need for explicit likelihood evaluation and the Markovian (sequential) property in sampling. Markov chain Monte Carlo (MCMC) is a standard non-parametric technique and asymptotically exact for sampling from a probability distribution [<xref ref-type="bibr" rid="c72">72</xref>]. However, for Bayesian inference on whole-brain models given high-dimensional data, the likelihood function becomes intractable, rendering MCMC sampling computationally prohibitive. SBI offers significant advantages, such as parallel simulation while leveraging amortized learning, making it effective for personalized inference from large datasets [<xref ref-type="bibr" rid="c69">69</xref>]. Amortization in artificial neural networks refers to the idea of reusing learned computations across multiple tasks or inputs [<xref ref-type="bibr" rid="c73">73</xref>]. Amortization in Bayesian inference refers to the process of training a shared inference network (e.g., a neural network) with with an intensive upfront computational cost, to perform fast inference across many different observations. Instead of re-running inference for each new observation, the trained model can rapidly return posterior estimates, significantly reducing computational cost at test time [<xref ref-type="bibr" rid="c68">68</xref>]. Following an initial computational cost during simulation and training to learn all posterior distributions, subsequent evaluation of new hypotheses can be conducted efficiently, without additional computational overhead for further simulations [<xref ref-type="bibr" rid="c68">68</xref>]. Importantly, SBI sidesteps the convergence issues caused by complex geometries that are often encountered when using gradient-based MCMC methods [<xref ref-type="bibr" rid="c74">74</xref>, <xref ref-type="bibr" rid="c75">75</xref>, <xref ref-type="bibr" rid="c76">76</xref>]. It also substantially outperforms approximate Bayesian computation (ABC) methods, which rely on a threshold to accept or reject samples [<xref ref-type="bibr" rid="c77">77</xref>, <xref ref-type="bibr" rid="c78">78</xref>, <xref ref-type="bibr" rid="c71">71</xref>]. Such a likelihood-free approach provides us with generic inference on complex systems as long as we can provide three modules:</p>
<list list-type="order">
<list-item><p>A prior distribution, describing the possible range of parameters from which random samples can be easily drawn, i.e.,<inline-formula><inline-graphic xlink:href="633922v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p></list-item>
<list-item><p>A simulator in computer code, that takes parameters as input and generates data as output, i.e.,<inline-formula><inline-graphic xlink:href="633922v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p></list-item>
<list-item><p>A set of low-dimensional data features, which are informative of the parameters that we aim to infer.</p></list-item>
</list>
<p>These elements prepare us with a training data set <inline-formula><inline-graphic xlink:href="633922v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> with a budget of <italic>N</italic><sub><italic>sim</italic></sub> simulations. Then, using a class of <italic>deep neural density estimators</italic> (such as masked autoregressive flows [<xref ref-type="bibr" rid="c79">79</xref>] or neural spline flows [<xref ref-type="bibr" rid="c80">80</xref>]), we can approximate the posterior distribution of parameters given a set of observed data, i.e.,<inline-formula><inline-graphic xlink:href="633922v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Therefore, a versatile toolkit should be flexible and integrative, adeptly incorporating these modules to enable efficient Bayesian inference over complex models.</p>
<p>To address the need for widely applicable, reliable, and efficient parameter estimation from different (source-localized) neuroimaging modalities, we introduce Virtual Brain Inference (<monospace>VBI</monospace>), a flexible and integrative toolkit for probabilistic inference at whole-brain level. This open-source toolkit offers fast simulation through just-intime (JIT) compilation of various brain models in different programming languages (Python/C++) and devices (CPUs/GPUs). It supports space-efficient storage of simulated data (HDF5/NPZ/PT), provides a memory-efficient loader for batched data, and facilitates the extraction of low-dimensional data features (FC/FCD/PSD). Additionally, it enables the training of deep neural density estimators (MAFs/NSFs), making it a versatile tool for inference on neural sources corresponding to (s)EEG, MEG, fMRI recordings.<monospace>VBI</monospace> leverages high-performance computing, significantly enhancing computational efficiency through parallel processing of large-scale datasets, which would be impractical with current alternative methods. Although SBI has been used for low-dimensional parameter spaces [<xref ref-type="bibr" rid="c71">71</xref>, <xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c81">81</xref>], we demonstrate that it can scale to whole-brain models with high-dimensional unknown parameters, as long as informative data features are provided.<monospace>VBI</monospace> is now accessible on the cloud platform EBRAINS (<ext-link ext-link-type="uri" xlink:href="https://ebrains.eu">https://ebrains.eu</ext-link>), enabling users to explore more realistic brain dynamics underlying brain (dys)functioning using Bayesian inference.</p>
<p>In the following sections, we will provide an overview of the theoretical foundations of whole-brain models and simulation-based inference, describe the architecture and features of the VBI toolkit, and demonstrate the validation through a series of case studies using in-silico data. We explore various whole-brain models corresponding to different types of brain recordings: a whole-brain network model of WilsonCowan [<xref ref-type="bibr" rid="c8">8</xref>], Jansen-Rit [<xref ref-type="bibr" rid="c82">82</xref>, <xref ref-type="bibr" rid="c83">83</xref>], and Stuart-Landau [<xref ref-type="bibr" rid="c84">84</xref>] for simulating neural activity associated with EEG/MEG signals, the Epileptor [<xref ref-type="bibr" rid="c11">11</xref>] related to stereoelectro-EEG (sEEG) recordings, and Montbri√≥ [<xref ref-type="bibr" rid="c12">12</xref>], and Wong-Wang [<xref ref-type="bibr" rid="c85">85</xref>, <xref ref-type="bibr" rid="c86">86</xref>] mapped to fMRI BOLD signals. Although these models represent source signals and could be applied to other modalities (e.g., Stuart-Landau representing generic oscillatory dynamics), we focused on their capabilities to perform optimally in specific contexts. For instance, some are better suited for encephalographic signals (e.g., EEG/MEG) due to their ability to preserve spectral properties, while others have been used for fMRI data, emphasizing their ability to capture dynamic features such as bistability and time-varying functional connectivity.</p>
</sec>
<sec id="s2">
<label>2.</label>
<title>Materials and methods</title>
<sec id="s2a">
<label>2.1.</label>
<title>The virtual brain models</title>
<p>To build a virtual brain model (see <xref rid="fig1" ref-type="fig">Figure 1</xref>), the process begins with parcellating the brain into regions using anatomical data, typically derived from T1-MRI scans. Each region, represented as nodes in the network, is then equipped with a neural mass model to simulate the collective behavior of neurons within that area. These nodes are interconnected using a structural connectivity (SC) matrix, typically obtained from diffusion-weighted magnetic resonance imaging (DW-MRI). The connectome was built with TVB-specific reconstruction pipeline using generally available neuroimaging software [<xref ref-type="bibr" rid="c16">16</xref>]. The entire network of interconnected nodes is then simulated using neuroinformatic tools, such as The Virtual Brain (TVB; [<xref ref-type="bibr" rid="c18">18</xref>]), generating neural activities at the source level. However, neural sources are not directly observable in real-world experiments, and a projection needs to be established to transform the simulated neural activity into empirically measurable quantities, such as (s)EEG, MEG, and fMRI. This approach offers insights into both normal brain function and neurological disorders [<xref ref-type="bibr" rid="c23">23</xref>]. In the following, we describe commonly used whole-brain network models at the source level, which can be mapped to different types of neuroimaging recordings. Note that each model represents one of many possible variants used in the literature, and the choice of model often depends on the specific research question, the spatial and temporal resolution of the available data, and the desired level of biological or mathematical detail.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>The workflow of Virtual Brain Inference (VBI).</title>
<p>This probabilistic approach is designed to estimate the posterior distribution of control parameters in virtual brain models from whole-brain recordings. (<bold>A</bold>) The process begins with constructing a personalized connectome using diffusion tensor imaging and a brain parcellation atlas. (<bold>B</bold>) The personalized virtual brain model is then assembled. Neural mass models describing the averaged activity of neural populations, in the generic form of <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline28.gif"/></inline-formula>, are placed to each brain region, and interconnected via the structural connectivity matrix. Initially, the control parameters are randomly drawn from a simple prior distribution. (<bold>C</bold>) Next, the VBI operates as a simulator that uses these samples to generate time series data associated with neuroimaging recordings. (<bold>D</bold>) We extract a set summary statistics from the low-dimensional features of the simulations (FC, FCD, PSD) for training. (<bold>E</bold>) Subsequently, a class of deep neural density estimators is trained on pairs of random parameters and their corresponding data features to learn the joint posterior distribution of the model parameters. (<bold>F</bold>) Finally, the amortized network allows us to quickly approximate the posterior distribution for new (empirical) data features, enabling us to make probabilistic predictions that are consistent with the observed data.</p></caption>
<graphic xlink:href="633922v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2a1">
<label>2.1.1.</label>
<title>Wilson-Cowan model</title>
<p>The Wilson-Cowan model [<xref ref-type="bibr" rid="c8">8</xref>] is a seminal neural mass model that describes the dynamics of connected excitatory and inhibitory neural populations, at cortical microcolumn level. It has been widely used to understand the collective behavior of neurons and simulate neural activities recorded by methods such as local field potentials (LFPs) and EEG. The model effectively captures phenomena such as oscillations, wave propagation, pattern formation in neural tissue, and responses to external stimuli, offering insights into various brain (dys)functions, particularly in Parkinson‚Äôs disease [<xref ref-type="bibr" rid="c87">87</xref>, <xref ref-type="bibr" rid="c88">88</xref>].</p>
<p>The Wilson-Cowan model describes the temporal evolution of the mean firing rates of excitatory (<italic>E</italic>) and inhibitory (<italic>I</italic>) populations using nonlinear differential equations. Each population‚Äôs activity is governed by a balance of self-excitation, cross-inhibition, external inputs, and network interactions through long-range coupling. The nonlinearity arises from a sigmoidal transfer function <italic>S</italic><sub><italic>i,e</italic></sub>(<italic>x</italic>), which maps the total synaptic input to the firing rate, capturing saturation effects and thresholds in neural response. In the whole-brain network extension, each neural population at node <italic>i</italic> receives input from other nodes via a weighted connectivity matrix, allowing the study of large-scale brain dynamics and spatial pattern formation [<xref ref-type="bibr" rid="c89">89</xref>, <xref ref-type="bibr" rid="c90">90</xref>, <xref ref-type="bibr" rid="c91">91</xref>].
<disp-formula id="eqn1">
<graphic xlink:href="633922v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
which incorporates both local dynamics and global interactions, modulated by coupling strengths and synaptic weights. Here, SC<sub><italic>kl</italic></sub> is an element of the (non)symmetric structural connectivity matrix and is nonzero if there is a connection between regions <italic>k</italic> and <italic>l</italic>. The nominal parameter values and the prior range for the target parameters are summarized in <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>Parameter descriptions for capturing whole-brain dynamics using the Wilson-Cowan neural mass model.</p></caption>
<graphic xlink:href="633922v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2a2">
<label>2.1.2.</label>
<title>Jansen-Rit model</title>
<p>The Jansen-Rit neural mass model [<xref ref-type="bibr" rid="c92">92</xref>] has been widely used to simulate physiological signals from various recording methods like intracranial LFPs, and scalp MEG/EEG recordings. For example, it has been shown to recreate responses similar to evoked-related potentials after a series of impulse stimulations [<xref ref-type="bibr" rid="c83">83</xref>, <xref ref-type="bibr" rid="c93">93</xref>], generating high-alpha and low-beta oscillations (with added recurrent inhibitory connections and spike-rate modulation) [<xref ref-type="bibr" rid="c94">94</xref>], and also seizure patterns similar to those seen in temporal lobe epilepsy [<xref ref-type="bibr" rid="c95">95</xref>]. This biologically motivated model comprises three main populations of neurons: excitatory pyramidal neurons, inhibitory interneurons and excitatory interneurons. These populations interact with each other through synaptic connections, forming a feedback loop that produces oscillatory activity governed by a set of nonlinear ordinary differential equations [<xref ref-type="bibr" rid="c82">82</xref>, <xref ref-type="bibr" rid="c83">83</xref>, <xref ref-type="bibr" rid="c96">96</xref>]:
<disp-formula id="eqn2">
<graphic xlink:href="633922v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>y</italic><sub>0<italic>i</italic></sub>, <italic>y</italic><sub>1<italic>i</italic></sub>, and <italic>y</italic><sub>2<italic>i</italic></sub> denote the average membrane potentials of pyramidal cells, excitatory interneurons, and inhibitory interneurons, respectively. Their corresponding time derivatives, <italic>y</italic><sub>3<italic>i</italic></sub>(<italic>t</italic>), <italic>y</italic><sub>4<italic>i</italic></sub>(<italic>t</italic>), and <italic>y</italic><sub>5<italic>i</italic></sub>(<italic>t</italic>), represent the rates of change of these membrane potentials. <italic>P</italic> (<italic>t</italic>) also represents an external input current. The sigmoid function <italic>S</italic>(<italic>x</italic>) maps the average membrane potential of neurons to their mean action potential firing rate. SC is a normalized structural connectivity matrix. The model‚Äôs output at <italic>i</italic><sub><italic>th</italic></sub> region, corresponds to the membrane potential of pyramidal cells, and is given by <italic>y</italic><sub>1<italic>i</italic></sub> ‚àí <italic>y</italic><sub>2<italic>i</italic></sub>. The nominal parameter values and the prior range for the target parameters are summarized in <xref rid="tbl2" ref-type="table">Table 2</xref>.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Parameter descriptions for capturing whole-brain dynamics using Jansen-Rit neural mass model.</title>
<p>EP: excitatory populations, IP: inhibitory populations, PSP: post synaptic potential, PSPA: post synaptic potential amplitude.</p></caption>
<graphic xlink:href="633922v2_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2a3">
<label>2.1.3.</label>
<title>Stuart-Landau oscillator</title>
<p>The Stuart-Landau oscillator [<xref ref-type="bibr" rid="c84">84</xref>] is a generic mathematical model used to describe oscillatory phenomena, particularly those near a Hopf bifurcation, which is often employed to study the nonlinear dynamics of neural activity [<xref ref-type="bibr" rid="c97">97</xref>, <xref ref-type="bibr" rid="c98">98</xref>, <xref ref-type="bibr" rid="c63">63</xref>, <xref ref-type="bibr" rid="c49">49</xref>]. One approach uses this model to capture slow hemodynamic changes in BOLD signal [<xref ref-type="bibr" rid="c97">97</xref>], while others apply it to model fast neuronal dynamics, which can be linked directly to EEG/MEG data [<xref ref-type="bibr" rid="c98">98</xref>, <xref ref-type="bibr" rid="c63">63</xref>, <xref ref-type="bibr" rid="c49">49</xref>]. Note that this is a phenomenological framework, and both applications operate on completely different time scales.</p>
<p>In the network, each brain region, characterized by an autonomous Stuart-Landau oscillator, can exhibit either damped or limit-cycle oscillations depending on the bifurcation parameter <italic>a</italic>. If <italic>a &lt;</italic> 0, the system shows damped oscillations, similar to a pendulum under friction. In this regime, the system, when subjected to perturbation, relaxes back to its stable fixed point through damped oscillations with an angular frequency <italic>œâ</italic><sub>0</sub>. The rate of amplitude damping is determined by |<italic>a</italic>|. Conversely, if <italic>a &gt;</italic> 0, the system supports limit cycle solutions, allowing for self-sustained oscillations even in the absence of external noise. At a critical value of <italic>a</italic> = 0, the system undergoes a Hopf bifurcation, i.e., small changes in parameters can lead to large variations in the system‚Äôs behavior.</p>
<p>Using whole-brain network modeling of EEG/MEG data [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c63">63</xref>], the oscillators are interconnected via white-matter pathways, with coupling strengths specified by subject-specific DTI fiber counts, i.e., elements of SC matrix. This adjacency matrix is then scaled by a global coupling parameter <italic>G</italic>. Note that coupling between regions accounts for finite conduction times, which are often estimated by dividing the Euclidean distances between nodes by an average conduction velocity <italic>T</italic><sub><italic>jk</italic></sub> = <italic>d</italic><sub><italic>jk</italic></sub><italic>/v</italic>.</p>
<p>Knowing the personalized time-delays [<xref ref-type="bibr" rid="c99">99</xref>, <xref ref-type="bibr" rid="c100">100</xref>], we can use the distance as a proxy, assuming a constant propagation velocity. The distance itself can be defined as either the length of the tracts or the Euclidean distance. Taking this into account, the activity of each regions is given by a set of complex differential equations:
<disp-formula id="eqn3">
<graphic xlink:href="633922v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>Z</italic> is a complex variable, and Re[<italic>Z</italic>(<italic>t</italic>)] is the corresponding time series. In this particular realization, each region has a natural frequency of 40 Hz (<italic>œâ</italic><sub><italic>j</italic></sub> = <italic>œâ</italic><sub>0</sub> = 2<italic>œÄ</italic> ¬∑ 40 <italic>rad/s</italic>), motivated by empirical studies demonstrating the emergence of gamma oscillations from the balance of excitation and inhibition, playing a role in local circuit computations [<xref ref-type="bibr" rid="c101">101</xref>].</p>
<p>In this study, for the sake of simplicity, a common cortico-cortical conduction velocity is estimated, i.e., the distance-dependent average velocities <italic>v</italic><sub><italic>jk</italic></sub> = <italic>v</italic>. We also consider <italic>a</italic> = ‚àí5, capturing the highly variable amplitude envelope of gamma oscillations as reported in experimental recordings [<xref ref-type="bibr" rid="c102">102</xref>, <xref ref-type="bibr" rid="c63">63</xref>]. This choice also best reflects the slowest decay time constants of GABA<sub>B</sub> inhibitory receptors‚Äìapproximately 1 second [<xref ref-type="bibr" rid="c103">103</xref>]. A Gaussian noise (here denoted by <italic>Œæ</italic><sub><italic>i</italic></sub>) with an intensity of <italic>œÉ</italic> = 10<sup><italic>‚àí</italic>4</sup> is added to each oscillator to mimic stochastic fluctuations. The nominal parameter values and the prior range for the target parameters are summarized in <xref rid="tbl3" ref-type="table">Table 3</xref>.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><p>Parameter descriptions for capturing whole-brain dynamics using Stuart-Landau oscillator.</p></caption>
<graphic xlink:href="633922v2_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2a4">
<label>2.1.4.</label>
<title>Epileptor model</title>
<p>In personalized whole-brain network modeling of epilepsy spread [<xref ref-type="bibr" rid="c47">47</xref>], the dynamics of each brain region are governed by the Epileptor model [<xref ref-type="bibr" rid="c11">11</xref>]. The Epileptor model provides a comprehensive description of epileptic seizures, encompassing the complete taxonomy of system bifurcations to simultaneously reproduce the dynamics of seizure onset, progression, and termination [<xref ref-type="bibr" rid="c104">104</xref>]. The full Epileptor model comprises five state variables that couple two oscillatory dynamical systems operating on three different time scales [<xref ref-type="bibr" rid="c11">11</xref>]. Then motivated by Synergetic theory [<xref ref-type="bibr" rid="c105">105</xref>, <xref ref-type="bibr" rid="c106">106</xref>] and under time-scale separation [<xref ref-type="bibr" rid="c107">107</xref>], the fast variables rapidly collapse on the slow manifold [<xref ref-type="bibr" rid="c108">108</xref>], whose dynamics is governed by the slow variable. This adiabatic approximation yields the 2D reduction of whole-brain model of epilepsy spread, also known as the Virtual Epileptic Patient (VEP) as follows:
<disp-formula id="eqn4">
<graphic xlink:href="633922v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>x</italic><sub><italic>i</italic></sub> and <italic>z</italic><sub><italic>i</italic></sub> indicate the fast and slow variables corresponding to <italic>i</italic><sub><italic>th</italic></sub> brain region, respectively, and the set of unknown <italic>Œ∑</italic><sub><italic>i</italic></sub> is the spatial map of epileptogenicity to be estimated. SC is a normalized structure connectivity matrix. In real-world epilepsy applications [<xref ref-type="bibr" rid="c67">67</xref>, <xref ref-type="bibr" rid="c68">68</xref>, <xref ref-type="bibr" rid="c6">6</xref>], we compute the envelope function from sEEG data to perform inference. The nominal parameter values and the prior range for the target parameters are summarized in <xref rid="tbl4" ref-type="table">Table 4</xref>.</p>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4.</label>
<caption><p>Parameter descriptions for capturing whole-brain dynamics using 2D Epileptor neural mass model.</p></caption>
<graphic xlink:href="633922v2_tbl4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2a5">
<label>2.1.5.</label>
<title>Montbri√≥ model</title>
<p>The exact macroscopic dynamics of a specific brain region (represented as a node in the network) can be analytically derived in thermodynamic limit of infinitely allto-all coupled spiking neurons [<xref ref-type="bibr" rid="c12">12</xref>] or Œò neuron representation [<xref ref-type="bibr" rid="c109">109</xref>]. By assuming a Lorentzian distribution on excitabilities in large ensembles of quadratic integrateandfire neurons with synaptic weights <italic>J</italic> and a half-width Œî centered at <italic>Œ∑</italic>, the macroscopic dynamics has been derived in terms of the collective firing activity and mean membrane potential [<xref ref-type="bibr" rid="c12">12</xref>]. Then, by coupling the brain regions via an additive current (e.g., in the average membrane potential equations), the dynamics of the whole-brain network can be described as follow [<xref ref-type="bibr" rid="c59">59</xref>]:
<disp-formula id="eqn5">
<graphic xlink:href="633922v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>v</italic><sub><italic>i</italic></sub> and <italic>r</italic><sub><italic>i</italic></sub> are the average membrane potential and firing rate, respectively, at the <italic>i</italic><sub><italic>th</italic></sub> brain region, and parameter <italic>G</italic> is the network scaling parameter that modulates the overall impact of brain connectivity on the state dynamics. The SC<sub><italic>ij</italic></sub> denotes the connection weight between <italic>i</italic><sub><italic>th</italic></sub> and <italic>j</italic><sub><italic>th</italic></sub> regions, and the dynamical noise <italic>Œæ</italic>(<italic>t</italic>) ‚àº ùí© (0, <italic>œÉ</italic><sup>2</sup>) follows a Gaussian distribution with mean zero and variance <italic>œÉ</italic><sup>2</sup>.</p>
<p>The model parameters are tuned so that each decoupled node is in a bistable regime, exhibiting a down-state stable fixed point (low-firing rate) and an up-state stable focus (high-firing rate) in the phase-space [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c81">81</xref>]. The bistability is a fundamental property of regional brain dynamics to ensure a switching behavior in the data (e.g., to generate FCD), that has been recognized as representative of realistic dynamics observed empirically [<xref ref-type="bibr" rid="c110">110</xref>, <xref ref-type="bibr" rid="c111">111</xref>, <xref ref-type="bibr" rid="c112">112</xref>, <xref ref-type="bibr" rid="c59">59</xref>].</p>
<p>The solution of the coupled system yields a neuroelectric dataset that describes the evolution of the variables (<italic>r</italic><sub><italic>i</italic></sub>(<italic>t</italic>), <italic>v</italic><sub><italic>i</italic></sub>(<italic>t</italic>)) in each brain region <italic>i</italic>, providing measures of macroscopic activity. The surrogate BOLD activity for each region is then derived by filtering this activity through the Balloon-Windkessel model [<xref ref-type="bibr" rid="c113">113</xref>]. The input current <italic>I</italic><sub><italic>stim</italic></sub> represents the stimulation to selected brain regions, which increase the basin of attraction of the up-state in comparison to the down-state, while the fixed points move farther apart [<xref ref-type="bibr" rid="c110">110</xref>, <xref ref-type="bibr" rid="c111">111</xref>, <xref ref-type="bibr" rid="c112">112</xref>, <xref ref-type="bibr" rid="c59">59</xref>].</p>
<p>The nominal parameter values and the prior range for the target parameters are summarized in <xref rid="tbl5" ref-type="table">Table 5</xref>.</p>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5.</label>
<caption><p>Parameter descriptions for capturing whole-brain dynamics using Montbri√≥ model.</p></caption>
<graphic xlink:href="633922v2_tbl5.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2a6">
<label>2.1.6.</label>
<title>Wong-Wang model</title>
<p>Another commonly used whole-brain model for simulation of neural activity is the so-called parameterized dynamics mean-field (pDMF) model [<xref ref-type="bibr" rid="c114">114</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c86">86</xref>]. At each region, it comprises a simplified system of two nonlinear coupled differential equations, motivated by the attractor network model, which integrates sensory information over time to make perceptual decisions, known as Wong-Wang model [<xref ref-type="bibr" rid="c85">85</xref>]. This biophysically realistic cortical network model of decision making then has been simplified further into a single-population model [<xref ref-type="bibr" rid="c86">86</xref>], which has been widely used to understand the mechanisms underpinning brain resting state dynamics [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c115">115</xref>, <xref ref-type="bibr" rid="c38">38</xref>]. The pDMF model has been also used to study whole-brain dynamics in various brain disorders, including Alzheimer‚Äôs disease [<xref ref-type="bibr" rid="c116">116</xref>], schizophrenia [<xref ref-type="bibr" rid="c117">117</xref>], and stroke [<xref ref-type="bibr" rid="c118">118</xref>]. The pDMF model equations are given as:
<disp-formula id="eqn6">
<graphic xlink:href="633922v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>H</italic>(<italic>x</italic><sub><italic>i</italic></sub>) and <italic>S</italic><sub><italic>i</italic></sub>, and <italic>x</italic><sub><italic>i</italic></sub> denote the population firing rate, the average synaptic gating variable, and the total input current at the <italic>i</italic><sub><italic>th</italic></sub> brain region, respectively. <italic>Œæ</italic><sub><italic>i</italic></sub>(<italic>t</italic>) is uncorrelated standard Gaussian noise and the noise amplitude is controlled by <italic>œÉ</italic>. The nominal parameter values and the prior range for the target parameters are summarized in <xref rid="tbl6" ref-type="table">Table 6</xref>.</p>
<table-wrap id="tbl6" orientation="portrait" position="float">
<label>Table 6.</label>
<caption><p>Parameter descriptions for capturing whole-brain dynamics using the Wong-Wang model.</p></caption>
<graphic xlink:href="633922v2_tbl6.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>According to recent studies [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c38">38</xref>], we can parameterize the set of <italic>w, I</italic> and <italic>œÉ</italic> as linear combinations of group-level T1w/T2w myelin maps [<xref ref-type="bibr" rid="c119">119</xref>] and the first principal gradient of functional connectivity:
<disp-formula id="eqn7">
<graphic xlink:href="633922v2_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold>Mye</bold><sub><italic>i</italic></sub> and <bold>Grad</bold><sub><italic>i</italic></sub> are the average values of the T1w/T2w myelin map and the first FC principal gradient, respectively, within the <italic>i</italic><sub><italic>th</italic></sub> brain region. Therefore, the set of unknown parameters to estimate includes <italic>G</italic> and linear coefficients (<italic>a</italic><sub><italic>w</italic></sub>, <italic>b</italic><sub><italic>w</italic></sub>, <italic>c</italic><sub><italic>w</italic></sub>, <italic>a</italic><sub><italic>I</italic></sub>, <italic>b</italic><sub><italic>I</italic></sub>, <italic>c</italic><sub><italic>I</italic></sub>, <italic>a</italic><sub><italic>œÉ</italic></sub>, <italic>b</italic><sub><italic>œÉ</italic></sub>, <italic>c</italic><sub><italic>œÉ</italic></sub>) <italic>‚àà</italic> ‚Ñù<sup>9</sup>, hence 10 parameters in total.</p>
</sec>
<sec id="s2a7">
<label>2.1.7.</label>
<title>The Balloon-Windkessel model</title>
<p>The Balloon-Windkessel model is a biophysical framework that links neural activity to the BOLD signals detected in fMRI. This is not a neuronal model but rather a representation of neurovascular coupling, describing how neural activity influences hemodynamic responses. The model is characterized by two state variables: venous blood volume (<italic>v</italic>) and deoxyhemoglobin content (<italic>q</italic>). The system‚Äôs input is blood flow (<italic>f</italic><sub><italic>in</italic></sub>), and the output is the BOLD signal (<italic>y</italic>):
<disp-formula id="eqn8">
<graphic xlink:href="633922v2_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>V</italic><sub>0</sub> represents the resting blood volume fraction, <italic>E</italic><sub>0</sub> is the oxygen extraction fraction at rest, <italic>E</italic> is the ratio of intrato extravascular signals, <italic>r</italic><sub>0</sub> is the slope of the relationship between the intravascular relaxation rate and oxygen saturation, <italic>œë</italic><sub>0</sub> is the frequency offset at the surface of a fully deoxygenated vessel at 1.5 T, and TE is the echo time. The dynamics of venous blood volume <italic>v</italic> and deoxyhemoglobin content <italic>q</italic> are governed by the Balloon model‚Äôs hemodynamic state equations:
<disp-formula id="eqn9">
<graphic xlink:href="633922v2_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>œÑ</italic><sub>0</sub> is the transit time of blood flow, <italic>Œ±</italic> reflects the resistance of the venous vessel (stiffness), and <italic>f</italic> (<italic>t</italic>) denotes blood inflow at time <italic>t</italic>, given by
<disp-formula id="ueqn1">
<graphic xlink:href="633922v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>s</italic> is an exponentially decaying vasodilatory signal defined by
<disp-formula id="eqn10">
<graphic xlink:href="633922v2_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where, <italic>E</italic> represents the efficacy of neuronal activity <italic>x</italic>(<italic>t</italic>) (i.e., integrated synaptic activity) in generating a signal increase, <italic>œÑ</italic><sub><italic>s</italic></sub> is the time constant for signal decay, and <italic>œÑ</italic><sub><italic>f</italic></sub> is the time constant for autoregulatory blood flow feedback [<xref ref-type="bibr" rid="c113">113</xref>]. For parameter values, see <xref rid="tbl7" ref-type="table">Table 7</xref>, taken from [<xref ref-type="bibr" rid="c113">113</xref>, <xref ref-type="bibr" rid="c120">120</xref>, <xref ref-type="bibr" rid="c121">121</xref>]. The resulting time series is downsampled to match the TR value in seconds.</p>
<table-wrap id="tbl7" orientation="portrait" position="float">
<label>Table 7.</label>
<caption><p>Parameter descriptions for the Balloon-Windkessel model to map neural activity to the BOLD signals detected in fMRI.</p></caption>
<graphic xlink:href="633922v2_tbl7.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</sec>
<sec id="s2b">
<label>2.2.</label>
<title>Simulation-based inference</title>
<p>In the Bayesian framework [<xref ref-type="bibr" rid="c122">122</xref>], parameter estimation involves quantifying and propagating uncertainty through probability distributions placed on the parameters (prior information before seeing data), which are updated with information provided by the data (likelihood function). The formidable challenge to conducting efficient Bayesian inference is evaluating the likelihood function <italic>p</italic>(<italic>x</italic> | <italic>Œ∏</italic>). This typically involves intractable integrating over all possible trajectories in the latent space: <italic>p</italic>(<italic>x</italic> | <italic>Œ∏</italic>) = ‚é∞ <italic>p</italic>(<italic>x, z</italic> | <italic>Œ∏</italic>)<italic>dz</italic>, where <italic>p</italic>(<italic>x, z</italic> | <italic>Œ∏</italic>) is the joint probability density of the data <italic>x</italic> and latent variables <italic>z</italic>, given parameters <italic>Œ∏</italic>. For whole-brain network models with highdimensional and nonlinear latent spaces, the computational cost can be prohibitive, making likelihood-based inference with MCMC sampling challenging to converge [<xref ref-type="bibr" rid="c76">76</xref>, <xref ref-type="bibr" rid="c67">67</xref>, <xref ref-type="bibr" rid="c123">123</xref>].</p>
<p>Simulation-based inference (SBI; [<xref ref-type="bibr" rid="c70">70</xref>, <xref ref-type="bibr" rid="c71">71</xref>, <xref ref-type="bibr" rid="c68">68</xref>]), or likelihood-free inference [<xref ref-type="bibr" rid="c124">124</xref>, <xref ref-type="bibr" rid="c125">125</xref>, <xref ref-type="bibr" rid="c126">126</xref>], addresses issues with explicit likelihood evaluation in complex systems, where the it often becomes intractable. The task of density estimation, one of the most fundamental problems in statistics, is to infer an underlying probability distribution based on a set of independently and identically distributed data points drawn from that distribution. Traditional density estimators, such as histograms and kernel density estimators, typically perform well only in low-dimensional settings. Recently, neural network-based approaches have been proposed for conditional density estimation, showing promising results in Bayesian inference problems involving highdimensional data [<xref ref-type="bibr" rid="c127">127</xref>, <xref ref-type="bibr" rid="c79">79</xref>, <xref ref-type="bibr" rid="c125">125</xref>, <xref ref-type="bibr" rid="c71">71</xref>, <xref ref-type="bibr" rid="c128">128</xref>, <xref ref-type="bibr" rid="c129">129</xref>, <xref ref-type="bibr" rid="c68">68</xref>].</p>
<p>Given a prior distribution <inline-formula><inline-graphic xlink:href="633922v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> placed on the parameters <inline-formula><inline-graphic xlink:href="633922v2_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> random simulations are generated (with samples from prior), resulting in pairs <inline-formula><inline-graphic xlink:href="633922v2_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="633922v2_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="633922v2_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the simulated data given <inline-formula><inline-graphic xlink:href="633922v2_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. By training a deep neural density estimator <italic>F</italic> (such as normalizing flows [<xref ref-type="bibr" rid="c79">79</xref>, <xref ref-type="bibr" rid="c80">80</xref>]), we can approximate the posterior <inline-formula><inline-graphic xlink:href="633922v2_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> with <inline-formula><inline-graphic xlink:href="633922v2_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula> by minimizing the loss function
<disp-formula id="eqn11">
<graphic xlink:href="633922v2_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
over network parameters <italic>œï</italic>. Once the parameters of the neural network <italic>F</italic> are optimized, for observed data <inline-formula><inline-graphic xlink:href="633922v2_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> we can readily estimate the target posterior <inline-formula><inline-graphic xlink:href="633922v2_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This allows for rapid approximation and sampling from the posterior distribution for any new observed data through a forward pass in the trained network [<xref ref-type="bibr" rid="c68">68</xref>, <xref ref-type="bibr" rid="c69">69</xref>].</p>
<p>This approach uses a class of generative machine learning models known as normalizing flows ([<xref ref-type="bibr" rid="c130">130</xref>, <xref ref-type="bibr" rid="c131">131</xref>]) to transform a simple based distribution into any complex target through a sequence of invertible mappings. Here, generative modeling is an unsupervised machine learning method for modeling a probability distribution given samples drawn from that distribution. The state-of-the-art normalizing flows, such as masked autoregressive flows (MAF; [<xref ref-type="bibr" rid="c79">79</xref>]) and neural spline flows (NSF; [<xref ref-type="bibr" rid="c80">80</xref>]), enable fast and exact density estimation and sampling from high-dimensional distributions. These models learn mappings between input data and probability densities, capturing complex dependencies and multi-modal distributions [<xref ref-type="bibr" rid="c131">131</xref>, <xref ref-type="bibr" rid="c132">132</xref>].</p>
<p>In our work, we integrate the implementation of these models from the opensource SBI tool, leveraging both MAF and NSF architectures. The MAF model comprises 5 flow transforms, each with two blocks and 50 hidden units, tanh nonlinearity and batch normalization after each layer. The NSF model consists of 5 flow transforms, two residual blocks of 50 hidden units each, ReLU nonlinearity, and 10 spline bins. We apply these generative models to virtual brain simulations conducted with random parameters, to approximate the full posterior distribution of parameters from low-dimensional data features. Note that we employ a single round of SBI to benefit from amortization strategy rather than using a sequential approach that is designed to achieve a better fit but only for a specific dataset [<xref ref-type="bibr" rid="c68">68</xref>, <xref ref-type="bibr" rid="c69">69</xref>].</p>
</sec>
<sec id="s2c">
<label>2.3.</label>
<title>Sensitivity analysis</title>
<p>Sensitivity analysis is a crucial step for identifying which model parameters influence the model‚Äôs behavior in response to changes in input [<xref ref-type="bibr" rid="c66">66</xref>, <xref ref-type="bibr" rid="c68">68</xref>]. A local sensitivity can be quantified by computing the curvature of objective function through the Hessian matrix [<xref ref-type="bibr" rid="c133">133</xref>, <xref ref-type="bibr" rid="c66">66</xref>]. Using SBI, after estimating the posterior for a specific observation, we can perform sensitivity analysis by computing the eigenvalues and corresponding eigenvectors of the following matrix [<xref ref-type="bibr" rid="c134">134</xref>, <xref ref-type="bibr" rid="c135">135</xref>]:
<disp-formula id="eqn12">
<graphic xlink:href="633922v2_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
which then does an eigendecomposition <italic>M</italic> = <italic>Q</italic>Œõ<italic>Q</italic><sup><italic>‚àí</italic>1</sup>. A large eigenvalue in the so-called active subspaces [<xref ref-type="bibr" rid="c136">136</xref>] indicates that the gradient of the posterior is large in the corresponding direction, suggesting that the system output is sensitive to changes along that eigenvector.</p>
</sec>
<sec id="s2d">
<label>2.4.</label>
<title>Evaluation of posterior fit</title>
<p>To assess the reliability of Bayesian inference using synthetic data, we evaluate the posterior z-scores (denoted by <italic>z</italic>) against the posterior shrinkage (denoted by <italic>s</italic>), as defined by [<xref ref-type="bibr" rid="c75">75</xref>]:
<disp-formula id="eqn13">
<graphic xlink:href="633922v2_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn14">
<graphic xlink:href="633922v2_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="633922v2_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <italic>Œ∏</italic><sup><italic>‚àó</italic></sup> are the posterior mean and the true values, respectively, <italic>œÉ</italic><sub><italic>prior</italic></sub> is the standard deviation of the prior, and <italic>œÉ</italic><sub><italic>post</italic></sub> is the standard deviation of the posterior.</p>
<p>The z-score quantifies how far the posterior mean of a parameter lies from a reference value (e.g., the true value), scaled by the posterior standard deviation. The shrinkage quantifies how much the posterior distribution has contracted relative to the initial prior distribution after learning from data. A small z-score indicates that the posterior estimate is close to the true value, reflecting accurate inference. A large shrinkage value suggests that the posterior is sharply concentrated, indicating that the parameter is well-identified. According to these definitions, an ideal Bayesian inference is characterized by z-scores close to zero and posterior shrinkage values close to one, reflecting both accuracy and reliability in the inferred parameters.</p>
</sec>
<sec id="s2e">
<label>2.5.</label>
<title>Flexible simulator and model building</title>
<p>A key feature of the<monospace>VBI</monospace> pipeline is its modularity and flexibility in integrating various simulators (see <xref rid="fig2" ref-type="fig">Figure 2</xref>). The <italic>Simulation</italic> module of the VBI pipeline is designed to be easily interchangeable, allowing researchers to replace it with other simulators, such as<monospace>TVB</monospace> [<xref ref-type="bibr" rid="c18">18</xref>],<monospace>Neurolib</monospace> [<xref ref-type="bibr" rid="c137">137</xref>],<monospace>Brian</monospace> [<xref ref-type="bibr" rid="c138">138</xref>],<monospace>Brainpy</monospace> [<xref ref-type="bibr" rid="c139">139</xref>]. This adaptability supports a wide range of simulation needs and computational environments, making the<monospace>VBI</monospace> a versatile tool for inference in system neuroscience. In particular, <italic>Simulation</italic> module offers a comprehensive implementation of commonly used wholebrain models. This is a customized version of implementation from open-source<monospace>TVB</monospace> simulator. While VBI does not encompass all the features of the original TVB, it is mainly designed to leverage the computational power of GPU devices and significantly reduce RAM requirements (see <xref rid="figS1" ref-type="fig">Figure S1<bold>A</bold></xref>). This optimization ensures that high-performance clusters can be fully utilized, enabling parallel and scalable simulations, as often is required to perform scalable SBI.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Flowchart of the VBI Structure.</title>
<p>This toolkit consists of three main modules: (1) The <italic>Simulation</italic> module, implementing various whole-brain models, such as Wilson-Cowan (WCo), Jansen-Rit (JR), Stuart-Landau (SL), Epileptor (EPi), Montbri√≥ (MPR), and Wong-Wang (WW), across different numerical computing libraries (C++, Cupy, PyTorch, Numba). (2) The <italic>Features</italic> module, offering an extensive toolbox for extracting low-dimensional data features, such as spectral, temporal, connectivity, statistical, and information theory features. (3) The <italic>Inference</italic> module, providing neural density estimators (such as MAF and NSF) to approximate the posterior of parameters.</p></caption>
<graphic xlink:href="633922v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2f">
<label>2.6.</label>
<title>Comprehensive feature extraction</title>
<p>VBI offers a comprehensive toolbox for feature extraction across various datasets. The <italic>Features</italic> module includes but not limited to: (1) <italic>Statistical features</italic>, including mean (average of elements), variance (spread of the elements around mean), kurtosis (tailedness of the distribution of elements), and skewness (the asymmetry of the distribution of elements), that can be applied to any matrix. (2) <italic>Spectral features</italic>, such as low-dimensional summary statistics of power spectrum density (PSD). (3) <italic>Temporal features</italic>, including zero crossings, area under the curve, average power, and envelope. (4) <italic>Connectivity features</italic>, including functional connectivity (FC), which represents the statistical dependencies or correlations between activity patterns of different brain regions, and functional connectivity dynamics (FCD), which captures the temporal variations and transitions in these connectivity patterns over time. These calculations are performed for the whole-brain and/or subnetwork (e.g., limbic system, resting state networks). However, since these matrices are still highdimensional, we use standard dimensional reduction techniques, such as principal component analysis (PCA) on FC/FCD matrices, to extract their associated lowdimensional summary statistics. (5) <italic>Information theory features</italic>, such as mutual information and transfer entropy. Following [<xref ref-type="bibr" rid="c69">69</xref>], we use the term <italic>spatio-temporal data features</italic> to refer to both <italic>statistical features</italic> and <italic>temporal features</italic> derived from time series. In contrast, we refer to the <italic>connectivity features</italic> extracted from FC/FCD matrices as <italic>functional data features</italic>.</p>
<p>The <italic>Features</italic> module uses parallel multiprocessing to speed up feature calculation. Additionally, it provides flexibility for users to add their own custom feature calculations with minimal effort and expertise, or to adjust the parameters of existing features based on the type of input time series. The feature extraction module is designed to be interchangeable with existing feature extraction libraries such as <monospace>tsfel</monospace> [<xref ref-type="bibr" rid="c140">140</xref>], <monospace>pyspi</monospace> [<xref ref-type="bibr" rid="c141">141</xref>], <monospace>hctsa</monospace>, [<xref ref-type="bibr" rid="c142">142</xref>], and <monospace>scikit-learn</monospace> [<xref ref-type="bibr" rid="c143">143</xref>]. Note that some lightweight libraries such as <monospace>catch22</monospace> [<xref ref-type="bibr" rid="c144">144</xref>] are directly accessible from the VBI feature extraction module.</p>
</sec>
<sec id="s2g">
<label>2.7.</label>
<title>VBI workflow</title>
<p><xref rid="fig1" ref-type="fig">Figure 1</xref> illustrates an overview of our approach in VBI, which combines virtual brain models and simulation-based inference to make probabilistic predictions on brain dynamics from (source-localized) neuroimaging recordings. The inputs to the pipeline include the structural imaging data (for building the connectome), functional imaging data such as (s)EEG/MEG, and fMRI as the target for fitting, and prior information as a plausible range over control parameters for generating random simulations. The main computational costs involve model simulations and data feature extraction. The output of the pipeline is the joint posterior distribution of control parameters (such as excitability, synaptic weights, or effective external input) that best explains the observed data. Since the approach is amortized (i.e., it learns across all combinations in the parameter space), it can be readily applied to any new data from a specific subject.</p>
<p>In the first step, non-invasive brain imaging data, such as T1-weighted MRI and Diffusion-weighted MRI (DW-MRI), are collected for a specific subject (<xref rid="fig1" ref-type="fig">Figure 1<bold>A</bold></xref>). T1-weighted MRI images are processed to obtain brain parcellation, while DW-MRI images are used for tractography. Using the estimated fiber tracts and the defined brain regions from the parcellation, the connectome (i.e., the complete set of links between brain regions) is constructed by counting the fibers connecting all regions. The SC matrix, with entries representing the connection strength between brain regions, forms the structural component of the virtual brain which constrains the generation of brain dynamics and functional data at arbitrary brain locations (e.g., cortical and subcortical structures).</p>
<p>Subsequently, each brain network node is equipped with a computational model of average neuronal activity, known as neural mass models (see <xref rid="fig1" ref-type="fig">Figure 1<bold>B</bold></xref> and subsection 2.1). They can be represented in the generic form of a dynamical model as <inline-formula><inline-graphic xlink:href="633922v2_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with the system variables <inline-formula><inline-graphic xlink:href="633922v2_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (such as membrane potential and firing rate), the control parameters <inline-formula><inline-graphic xlink:href="633922v2_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (such as excitability), and the input current <italic>I</italic><sub><italic>input</italic></sub> (such as stimulation). This integration of mathematical mean-field modeling (neural mass models) with anatomical information (connectome) allows us to efficiently analyze functional neuroimaging modalities at the whole-brain level.</p>
<p>To quantify the posterior distribution of control parameters given a set of observations, <inline-formula><inline-graphic xlink:href="633922v2_inline19.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, we first need to define a plausible range for the control parameters based on background knowledge <inline-formula><inline-graphic xlink:href="633922v2_inline20.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, i.e., a simple base distribution known as a prior. We draw random samples from the prior and provide them as input to the VBI simulator (implemented by <italic>Simulation</italic> module) to generate simulated time series associated with neuroimaging recordings, as shown in <xref rid="fig1" ref-type="fig">Figure 1<bold>C</bold></xref>. Subsequently, we extract low-dimensional data features (implemented by <italic>Features</italic> module), as shown in <xref rid="fig1" ref-type="fig">Figure 1<bold>D</bold></xref> for FC/FCD/PSD, to prepare the training dataset <inline-formula><inline-graphic xlink:href="633922v2_inline21.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with a budget of <italic>N</italic><sub><italic>sim</italic></sub> simulations. Then, we use a class of deep neural density estimators, such as MAF or NSF models, as schematically shown in <xref rid="fig1" ref-type="fig">Figure 1<bold>E</bold></xref>, to learn all the posterior <inline-formula><inline-graphic xlink:href="633922v2_inline22.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Finally, we can readily sample from <inline-formula><inline-graphic xlink:href="633922v2_inline23.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which determines the probability distribution in parameter space that best explains the observed data. <xref rid="fig2" ref-type="fig">Figure 2</xref> depicts the structure of the VBI toolkit, which consists of three main modules. The first module, referred to as <italic>Simulation</italic> module, is designed for fast simulation of whole-brain models, such as Wilson-Cowan (subsubsection 2.1.1), Jansen-Rit (subsubsection 2.1.2), Stuart-Landau (subsubsection 2.1.3), Epileptor (subsubsection 2.1.4), Montbri√≥ (subsubsection 2.1.5), and Wong-Wang (subsubsection 2.1.6). These whole-brain models are implemented across various numerical computing libraries such as Cupy (GPU-accelerated computing with Python), C++ (a highperformance systems programming language), Numba (a just-in-time compiler for accelerating Python code), and PyTorch (an open-source machine learning library for creating deep neural network).</p>
<p>The second module, <italic>Features</italic>, provides a versatile tool for extracting low-dimensional features from simulated time series (see subsection 2.6). The features include, but are not limited to, <italic>spectral, temporal, connectivity, statistical</italic>, and <italic>information theory</italic> related features, and the associated summary statistics. The third module focuses on <italic>Inference</italic>, i.e., training the deep neural density estimators, such as MAF and NSF (see subsection 2.2), to learn the joint posterior distribution of control parameters.</p>
</sec>
</sec>
<sec id="s3">
<label>3.</label>
<title>Results</title>
<p>In the following, we demonstrate the capability of VBI for inference on the stateof-the-art whole-brain network models using in-silico testing, where the ground truth is known. We apply this approach to simulate neural activity and associated measurements, including (s)EEG/MEG, and fMRI, while also providing diagnostics for the accuracy and reliability of the estimation. Note that for (s)EEG/MEG neuroimaging, we perform inference at the regional level rather than at the sensor level, whereas for fMRI, it is mapped using the Balloon-Windkessel model (see subsubsection 2.1.7). The results presented are based on synthetic data generated using a set of predefined parameters, referred to as the ground truth, randomly selected within biologically plausible ranges and incorporating a certain level of heterogeneity.</p>
<p>We first demonstrate inference on the whole-brain network model of the WilsonCowan (<xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>), which is capable of generating a wide range of oscillatory dynamics depending on the control parameters. Specifically, we estimate the bifurcation parameters <italic>P</italic><sub><italic>i</italic></sub> ‚àà ‚Ñù <sup>88</sup>, representing the external input to each excitatory population, and the global excitatory coupling parameter <italic>g</italic><sub><italic>e</italic></sub>. <xref rid="fig3" ref-type="fig">Figure 3<bold>A</bold></xref> and <xref ref-type="fig" rid="fig3"><bold>B</bold></xref> present the observed and predicted EEG-like signals, represented by the activity of excitatory populations <italic>E</italic> across regions. Panels <bold>C</bold> and <bold>D</bold> show the corresponding power spectral density (PSD), as data features. Panels <bold>E</bold> and <bold>F</bold> illustrate the inferred posterior distributions for parameters <italic>P</italic><sub><italic>i</italic></sub> and <italic>g</italic><sub><italic>e</italic></sub>, respectively, given <italic>Œ∏</italic> = {<italic>g</italic><sub><italic>e</italic></sub>, <italic>P</italic><sub><italic>i</italic></sub>} ‚àà ‚Ñù <sup>89</sup>. For training, we conducted 250k random simulations from uniform priors <italic>g</italic><sub><italic>e</italic></sub> ‚àº ùí∞ (0, 3) and <italic>P</italic><sub><italic>i</italic></sub> ‚àº ùí∞ (0, 3) (shown in blue; see <xref rid="tbl1" ref-type="table">Table 1</xref>). After approximately 2 hours of training using MAF density estimators, posterior sampling was completed within a few seconds. Due to the large number of simulations and informativeness of data features, we achieved accurate estimations of the high-dimensional and heterogeneous control parameters. Ground-truth values (shown in green) are well recovered, leading to close agreement between observed and predicted PSDs of the signals. Finally, <xref rid="fig3" ref-type="fig">Figure 3<bold>G</bold></xref> reports the posterior shrinkage and z-score metrics used to evaluate the quality of the parameter estimation. The results indicate that the inferred posteriors are both precise and well-centered around the ground-truth values, as reflected by high shrinkage and low z-scores. See <xref rid="figS2" ref-type="fig">Figure S2</xref> for estimation over other configurations. Moreover, <xref rid="figS3" ref-type="fig">Figure S3</xref>, and <xref rid="figS4" ref-type="fig">Figure S4</xref>, indicate the higher accuracy of NSF, though with substantially more computational cost for training compared to MAF.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Bayesian inference on heterogeneous control parameters in the whole-brain network of Wilson-Cowan model.</title>
<p>The set of inferred parameters is <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline29.gif"/></inline-formula>, with the global scaling parameter <italic>G</italic> and average external input current to excitatory populations per <italic>P</italic><sub><italic>i</italic></sub> region, given <italic>i</italic> ‚àà{1, 2, ‚Ä¶, <italic>N</italic><sub><italic>n</italic></sub> = 88} parcelled regions. Summary statistics of power spectrum density (PSD) were used for training of MAF density estimator, with a budget of 260k simulations. (<bold>A</bold>) and (<bold>B</bold>) illustrate the observed and predicted neural activities, respectively. (<bold>C</bold>) and (<bold>D</bold>) show the observed and predicted PSDs as the data. (<bold>E</bold>) and (<bold>F</bold>) display the posterior distribution of <italic>P</italic><sub><italic>i</italic></sub> per region, and global coupling <italic>G</italic>, respectively. The ground truth and prior are represented by a vertical green line and a blue distribution, respectively. (<bold>G</bold>) shows the inference evaluation using posterior shrinkage and z-score.</p></caption>
<graphic xlink:href="633922v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Then, we demonstrate the inference on heterogeneous control parameters in the whole-brain network of Jansen-Rit (see subsubsection 2.1.2), commonly used for modeling EEG/MEG data, e.g., in dementia and Alzheimer‚Äôs disease [<xref ref-type="bibr" rid="c145">145</xref>, <xref ref-type="bibr" rid="c146">146</xref>]. <xref rid="fig4" ref-type="fig">Figure 4<bold>A</bold></xref> and <xref ref-type="fig" rid="fig4">B</xref> show the observed and predicted EEG signals, given by (<italic>y</italic><sub>1<italic>i</italic></sub> ‚àí <italic>y</italic><sub>2<italic>i</italic></sub>) at each region, while <xref rid="fig4" ref-type="fig">Figure 4<bold>C</bold></xref> and <xref ref-type="fig" rid="fig4">D</xref> illustrate the observed and predicted features such as PSD, respectively. <xref rid="fig4" ref-type="fig">Figure 4<bold>E</bold></xref> and <xref ref-type="fig" rid="fig4">F</xref> show the estimated posterior distributions of synaptic connections <italic>C</italic><sub><italic>i</italic></sub>, and the global coupling parameter <italic>G</italic>, respectively, given the set of unknown parameters <inline-formula><inline-graphic xlink:href="633922v2_inline24.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Here we conducted 50k random simulations with samples drawn from uniform priors <italic>G</italic> ‚àà ùí∞ (0, 5) and <italic>C</italic><sub><italic>i</italic></sub> ‚àà ùí∞ (100, 650) (shown in blue, see <xref rid="tbl2" ref-type="table">Table 2</xref>). After approximately 45 min of training (MAF density estimator), the posterior sampling took only a few seconds. With such a sufficient number of simulations and informative data features, VBI shows accurate estimation of high-dimensional heterogeneous parameters (given the ground truth, shown in green), leading to a strong correspondence between the observed and predicted PSD of EEG/MEG data. <xref rid="fig4" ref-type="fig">Figure 4<bold>G</bold></xref> displays the shrinkage and z-score as the evaluation metrics, indicating an ideal Bayesian estimation for <italic>C</italic><sub><italic>i</italic></sub> parameters, but not for the coupling parameter <italic>G</italic>. This occurred because the network input did not induce a significant change in the intrinsic frequency of activities at regional level, resulting in diffuse uncertainty in its estimation for this model.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Bayesian inference on heterogeneous control parameters in the whole-brain network of Jansen-Rit model.</title>
<p>The set of inferred parameters is <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline30.gif"/></inline-formula>, with the global scaling parameter <italic>G</italic> and average numbers of synapse between neural populations per <italic>C</italic><sub><italic>i</italic></sub> region, given <italic>i</italic> ‚àà {1, 2, ‚Ä¶, <italic>N</italic><sub><italic>n</italic></sub> = 88} parcelled regions. Summary statistics of power spectrum density (PSD) were used for training, with a budget of 50k simulations. (<bold>A</bold>) and (<bold>B</bold>) illustrate the observed and predicted neural activities, respectively. (<bold>C</bold>) and (<bold>D</bold>) show the observed and predicted data features, such as PSD. (<bold>E</bold>) and (<bold>F</bold>) display the posterior distribution of <italic>C</italic><sub><italic>i</italic></sub> per region, and global coupling <italic>G</italic>, respectively. The ground truth and prior are represented by vertical green lines and a blue distribution, respectively. (<bold>G</bold>) shows the inference evaluation using the shrinkage and z-score of the estimated posterior distributions.</p></caption>
<graphic xlink:href="633922v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Note that relying on only alpha-peak while excluding other summary statistics, such as total power (i.e., area under the curve), leads to poor estimation of synaptic connections across brain regions (see <xref rid="figS5" ref-type="fig">Figure S5</xref>). This results in less accurate predictions of the PSD, with more dispersion in their amplitudes. This example demonstrates that<monospace>VBI</monospace> provides a valuable tool for hypothesis evaluation and improved insight into data features by uncertainty quantification, and their impact on predictions.</p>
<p>To demonstrate efficient inference on the whole-brain time delay from EEG/MEG data, we used a whole-brain network model of coupled generic oscillators (StuartLandau model (see subsubsection 2.1.3). This model could establish a causal link between empirical spectral changes and the slower conduction velocities observed in multiple sclerosis patients, resulting from immune system attacks on the myelin sheath [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>]. The parameter set to estimate is <inline-formula><inline-graphic xlink:href="633922v2_inline25.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, consisting of the global scaling parameter <italic>G</italic> and the averaged velocity of signal transmission <italic>V</italic>. The training was performed using a budget of only 2k simulations, which was sufficient due to the low dimensionality of the parameter space. <xref rid="fig5" ref-type="fig">Figure 5<bold>A</bold></xref> illustrates the comparison between observed (in green) and predicted neural activities (in red). <xref rid="fig5" ref-type="fig">Figure 5<bold>B</bold></xref> shows a close agreement between observed and predicted PSD signals, as the data feature used for training. <xref rid="fig5" ref-type="fig">Figure 5<bold>C</bold></xref> and <xref ref-type="fig" rid="fig5">D</xref> provide visualizations of the posterior distributions for the averaged velocity <italic>V</italic> and the global coupling <italic>G</italic>. In these panels, we can see a large shrinkage in the posterior (in red) the uniform prior (in blue) centered around the true values (vertical green lines). Importantly, <xref rid="fig5" ref-type="fig">Figure 5<bold>E</bold></xref> presenting the joint posterior distribution, indicates a high correlation of <italic>œÅ</italic> = 0.7 between parameters <italic>G</italic> and <italic>V</italic>. This illustrates the advantage of Bayesian estimation in identifying statistical relationships between parameters, which helps to detect degeneracy among them. This is crucial for causal hypothesis evaluation and guiding conclusions in clinical settings. Finally, <xref rid="fig5" ref-type="fig">Figure 5<bold>F</bold></xref> illustrates the sensitivity analysis (based on the eigenvalues of the posterior distribution), revealing that the posterior is more sensitive to changes in <italic>V</italic> compared to <italic>G</italic>. This highlights the relative impact of these parameters on the model‚Äôs posterior estimates.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Bayesian inference on global scaling parameter <italic>G</italic> and the averaged velocity <italic>V</italic> of signal transmission using the whole-brain network model of Stuart-Landau oscillators.</title>
<p>The set of estimated parameters is <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline31.gif"/></inline-formula>, and the summary statistics of PSD signals with a budget of 2k simulations were used for training. (<bold>A</bold>) illustrates exemplary observed and predicted neural activities (in green and red, respectively). (<bold>B</bold>) shows the observed and predicted PSD signals (in green and red, respectively). (<bold>C</bold>) and (<bold>D</bold>) display the posterior distribution of averaged velocity <italic>V</italic> and global coupling <italic>G</italic>, respectively. The true values and prior are shown as vertical green lines and a blue distribution, respectively. (<bold>E</bold>) shows the joint posterior distribution indicating a high correlation between posterior samples. (<bold>F</bold>) illustrates the sensitivity analysis based on the eigenvalues of the posterior distribution.</p></caption>
<graphic xlink:href="633922v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, we demonstrate the inference on a whole-brain model of epilepsy spread, known as the Virtual Epileptic Patient (VEP; [<xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c76">76</xref>]), used to delineate the epileptogenic and propagation zone networks from the invasive sEEG recordings (subsubsection 2.1.4). Here, we used a large value for system time constant <italic>œÑ</italic> = 90 <italic>ms</italic> (see <xref rid="tbl4" ref-type="table">Table 4</xref>) to generate slow-fast dynamics in pathological areas, corresponding to seizure envelope at each brain region. <xref rid="fig6" ref-type="fig">Figure 6</xref> demonstrates the inference the set of inferred parameters <inline-formula><inline-graphic xlink:href="633922v2_inline26.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with the global scaling parameter <italic>G</italic> and spatial map of epileptogenicity <italic>Œ∑</italic><sub><italic>i</italic></sub>, given <italic>i</italic> ‚àà {1, 2, ‚Ä¶, <italic>N</italic><sub><italic>n</italic></sub> = 88} parcelled regions. <xref rid="fig6" ref-type="fig">Figure 6<bold>A</bold></xref> and <xref rid="fig6" ref-type="fig">B</xref> show the observed and predicted envelope, respectively, at each brain region. Here, the whole brain regions are classified into two epileptogenic zones (in red, corresponding to high excitability), three propagation zones (in yellow, corresponding to excitability close to bifurcation), and the rest as healthy regions (in green, corresponding to low excitability). <xref rid="fig6" ref-type="fig">Figure 6<bold>C</bold></xref> and <xref rid="fig6" ref-type="fig">D</xref> illustrate the observed and predicted data features as the total power energy per region, calculated as the area under the curve. Additionally the seizure onset at each region was used as a data feature for training the MAF density estimator. From these panels, we observe accurate recovery of seizure envelopes in pathological regions. <xref rid="fig6" ref-type="fig">Figure 6<bold>E</bold></xref> and <xref rid="fig6" ref-type="fig">F</xref> show that the posterior distribution of heterogeneous <italic>Œ∑</italic><sub><italic>i</italic></sub>, and global coupling parameter <italic>G</italic>, respectively, indicating 100% accurate recovery of the true values (in green). <xref rid="fig6" ref-type="fig">Figure 6<bold>G</bold></xref> confirms the reliability and accuracy of the estimates through shrinkage and z-score diagnostics. With our efficient implementation, generating 10k whole-brain simulations took less than a minute (using 10 CPU cores). The training took approximately 13 minutes to converge, while posterior sampling required only a few seconds. See <xref rid="figS6" ref-type="fig">Figure S6</xref> for a similar analysis with a faster time separation (<italic>œÑ</italic> = 10 <italic>ms</italic>). These results demonstrate an ideal and fast Bayesian estimation, despite the stiffness of equations in each region and the high dimensionality of the parameters. See <xref rid="figS7" ref-type="fig">Figure S7</xref> and <xref rid="figS8" ref-type="fig">Figure S8</xref> showing the accuracy and reliability of estimation under different levels of additive and dynamical noise. Note that for the VEP model, the total integration time is less than 100 ms, and due to the model‚Äôs stable behavior and a large time step integration, the simulation cost is significantly lower compared to other whole-brain models.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Bayesian inference on the spatial map of epileptogenicity across brain regions in the VEP model.</title>
<p>The set of inferred parameters is <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline32.gif"/></inline-formula>, as the global scaling parameter and spatial map of epileptogenicity with <italic>i</italic> ‚àà {1, 2, ‚Ä¶, <italic>N</italic><sub><italic>n</italic></sub> = 88} parcelled regions. (<bold>A</bold>) The observed seizure envelope generated by the Epileptor model, given two regions as epileptogenic zones (in red) and three regions as propagation zones (in yellow), while the rest are healthy (in green). (<bold>B</bold>) The predicted seizure envelope, by training MAF model on a dataset containing 10k simulations, using only the total power and seizure onset per region as the data features. (<bold>C</bold>) and (<bold>D</bold>) show the observed and predicted data features, respectively. (<bold>E</bold>) and (<bold>F</bold>) show the posterior distributions of heterogeneous control parameters <italic>Œ∑</italic><sub><italic>i</italic></sub>, and global coupling parameter <italic>G</italic>, respectively. (<bold>G</bold>) The posterior z-scores versus posterior shrinkages for estimated parameters.</p></caption>
<graphic xlink:href="633922v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Targeting the fMRI data, we demonstrate the inference on the whole-brain dynamics using Montbri√≥ model (see subsubsection 2.1.5). <xref rid="fig7" ref-type="fig">Figure 7</xref> demonstrates the inference on heterogeneous control parameters of the Montbri√≥ model, operating in a bistable regime (<xref rid="tbl5" ref-type="table">Table 5</xref>). <xref rid="fig7" ref-type="fig">Figure 7<bold>A</bold></xref> and <xref ref-type="fig" rid="fig7">B</xref> show the observed and predicted BOLD time series, respectively, while <xref rid="fig7" ref-type="fig">Figure 7<bold>C</bold></xref> and <xref ref-type="fig" rid="fig7">D</xref> illustrate the observed and predicted data features, such as the static and dynamical functional connectivity matrices (FC and FCD, respectively). <xref rid="fig7" ref-type="fig">Figure 7<bold>E</bold></xref> and <xref ref-type="fig" rid="fig7">F</xref> show the estimated posterior distributions of excitability <italic>Œ∑</italic><sub><italic>i</italic></sub> per brain region, and the global coupling parameter <italic>G</italic>. <xref rid="fig7" ref-type="fig">Figure 7<bold>G</bold></xref> displays the reliability and accuracy of estimation through the evaluation of posterior shrinkage and z-score (see <xref ref-type="disp-formula" rid="eqn13">Equation 13</xref> and <xref ref-type="disp-formula" rid="eqn14">14</xref>). See <xref rid="figS9" ref-type="fig">Figure S9</xref> for estimation over different configurations of the ground-truth values in this model.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Bayesian inference on heterogeneous control parameters in the whole-brain dynamics using Montbri√≥ model.</title>
<p>The set of inferred parameters is <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline33.gif"/></inline-formula>, as the global scaling parameter and excitability per region, with <italic>i</italic> ‚àà{1, 2, ‚Ä¶, <italic>N</italic><sub><italic>n</italic></sub> = 88} parcelled regions. VBI provides accurate and reliable posterior estimation using both spatio-temporal and functional data features for training, with a budget of 500k simulations. (<bold>A</bold>) and (<bold>B</bold>) illustrate the observed and predicted BOLD signals, respectively. (<bold>C</bold>) and (<bold>D</bold>) show the observed (upper triangular) and predicted (lower triangular) data features (FC and FCD), respectively. (<bold>E</bold>) and (<bold>F</bold>) display the posterior distribution of excitability parameters <italic>Œ∑</italic><sub><italic>i</italic></sub> per region, and global coupling <italic>G</italic>, respectively. The true values and prior are shown as vertical green lines and a blue distribution, respectively. (<bold>G</bold>) shows the inference evaluation by the shrinkage and z-score of the posterior distributions.</p></caption>
<graphic xlink:href="633922v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Due to the large number of simulations for training and the informativeness of the data features (both spatio-temporal and functional data features), the results indicate that we achieved accurate parameter estimation, consequently, a close agreement between the observed and predicted features of BOLD data. This required 500k simulations for training (see <xref rid="figS10" ref-type="fig">Figure S10</xref>), given the uniform priors <italic>G</italic> ‚àà ùí∞ (0, 1) and <italic>Œ∑</italic><sub><italic>i</italic></sub> ‚àà ùí∞ (‚àí 6, ‚àí3.5). After approximately 10 <italic>h</italic> of training (of MAF density estimator), posterior sampling took only 1 min. Our results indicate that training the MAF model was 2 to 4 times faster than the NSF model (see <xref rid="figS1" ref-type="fig">Figure S1<bold>B</bold></xref>). Note that removing the spatio-temporal features and considering only FC/FCD as the data features (see <xref rid="figS11" ref-type="fig">Figure S11</xref>) leads to poor estimation of the excitability parameter across brain regions (see <xref rid="figS12" ref-type="fig">Figure S12</xref>). Interestingly, accurate estimation of the only global coupling parameter, <inline-formula><inline-graphic xlink:href="633922v2_inline27.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, from only FC/FCD requires around 100 simulations (see <xref rid="figS13" ref-type="fig">Figure S13</xref> and <xref rid="figS14" ref-type="fig">Figure S14</xref>). This showcase demonstrates the capability of<monospace>VBI</monospace> in inferring heterogeneous excitability, given the bistable brain dynamics, for fMRI studies. See <xref rid="figS15" ref-type="fig">Figure S15</xref> and <xref rid="figS16" ref-type="fig">Figure S16</xref> showing the accuracy and reliability of estimation under different levels of additive and dynamical noise.</p>
<p>Finally, in <xref rid="fig8" ref-type="fig">Figure 8</xref>, we show the inference on the so-called pDMF model, i.e., a whole-brain network model of the reduced Wong-Wang equation (see subsubsection 2.1.6), comprising 10 control parameters: the global scaling of connections <italic>G</italic> and the linear coefficients (<italic>a</italic><sub><italic>w</italic></sub>, <italic>b</italic><sub><italic>w</italic></sub>, <italic>c</italic><sub><italic>w</italic></sub>, <italic>a</italic><sub><italic>I</italic></sub>, <italic>b</italic><sub><italic>I</italic></sub>, <italic>c</italic><sub><italic>I</italic></sub>, <italic>a</italic><sub><italic>œÉ</italic></sub>, <italic>b</italic><sub><italic>œÉ</italic></sub>, <italic>c</italic><sub><italic>œÉ</italic></sub>) ‚àà ‚Ñù<sup>9</sup>. These parameters are introduced to reduce the dimension of whole-brain parameters as recurrent connection strength <italic>w</italic><sub><italic>i</italic></sub>, external input current <italic>I</italic><sub><italic>i</italic></sub>, and noise amplitude <italic>œÉ</italic><sub><italic>i</italic></sub> for each region (in total, 264 parameters were reduced to 9 dimensions; see <xref ref-type="disp-formula" rid="eqn7">Equation 7</xref>). Here, we used summary statistics of both spatio-temporal and functional data features extracted from simulated BOLD data to train the MAF density estimator, with a budget of 50k simulations. The training took around 160 min to converge, whereas posterior sampling took only a few seconds.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Bayesian inference on the parametric mean-field model of Wong-Wang (also known as pDMF model), with linear coefficients (<italic>a</italic><sub><italic>w</italic></sub>, <italic>b</italic><sub><italic>w</italic></sub>, <italic>c</italic><sub><italic>w</italic></sub>, <italic>a</italic><sub><italic>I</italic></sub>, <italic>b</italic><sub><italic>I</italic></sub>, <italic>c</italic><sub><italic>I</italic></sub>, <italic>a</italic><sub><italic>œÉ</italic></sub>, <italic>b</italic><sub><italic>œÉ</italic></sub>, <italic>c</italic><sub><italic>œÉ</italic></sub>) ‚àà ‚Ñù<sup>9</sup>, reparameterizing the recurrent connection strength <italic>w</italic><sub><italic>i</italic></sub>, external input current <italic>I</italic><sub><italic>i</italic></sub>, and noise amplitude <italic>œÉ</italic><sub><italic>i</italic></sub> for each region.</title>
<p>Summary statistics of spatio-temporal and functional data features were used for training, with a budget of 50k simulations. (<bold>A</bold>) The diagonal panels display the ground-true values (in green), the uniform prior (in blue), and the estimated posterior distributions (in red). The upper diagonal panels illustrate the joint posterior distributions between parameters, along with their correlation (<italic>œÅ</italic>, in the upper left corners), and ground-truth values (green stars). High-probability areas are colorcoded in yellow, while low-probability areas are shown in black. (<bold>B</bold>) The observed and predicted BOLD time series (in green and red, respectively). (<bold>C</bold>) The observed and predicted data features, such as FC/FCD matrices. (<bold>D</bold>) The inference evaluation by calculating the shrinkage and z-score of the estimated posterior distributions.</p></caption>
<graphic xlink:href="633922v2_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The diagonal panels in <xref rid="fig8" ref-type="fig">Figure 8<bold>A</bold></xref> show estimated posterior distributions (in red), along with the prior (in blue) and true values (in green). The upper diagonal panels illustrate the joint posterior distributions between parameters (i.e., statistical dependency between parameters). <xref rid="fig8" ref-type="fig">Figure 8<bold>B</bold></xref> illustrates the observed and predicted BOLD time series, generated by true and estimated parameters (in blue and red, respectively). From <xref rid="fig8" ref-type="fig">Figure 8<bold>C</bold></xref>, we can see a close agreement between the observed and predicted data features (FC/FCD matrices). Note that due to the stochastic nature of the generative process, we do not expect an exact element-wise correspondence between these features, but rather a match in their summary statistics, such as the mean, variance, and higher-order moments (see <xref rid="figS17" ref-type="fig">Figure S17</xref>). <xref rid="fig8" ref-type="fig">Figure 8<bold>D</bold></xref> shows the posterior z-score versus shrinkage, indicating less accurate estimation for the coefficients <italic>c</italic><sub><italic>w</italic></sub>, <italic>c</italic><sub><italic>I</italic></sub>, and <italic>c</italic><sub><italic>œÉ</italic></sub> compared to others, as they are not informed by anatomical data such as the T1w/T2w myelin map and the first FC principal gradient (see <xref ref-type="disp-formula" rid="eqn7">Equation 7</xref>). This showcase demonstrates the advantage of Bayesian inference over optimization in assessing the accuracy and reliability of parameter estimation, whether informed by anatomical data.</p>
<p>Note that in the whole-brain network of Wong-Wang model (6), the global scaling parameter <italic>G</italic> and synaptic coupling <italic>J</italic> exhibit structural non-identifiability, meaning their combined effects on the system cannot be uniquely disentangled (see <xref rid="figS18" ref-type="fig">Figure S18</xref>, and <xref rid="figS19" ref-type="fig">Figure S19</xref>). This is evident in the parameter estimations corresponding to selected observations, where the posterior distributions appear diffuse. The joint posterior plots reveal a nonlinear dependency (banana shape) between <italic>G</italic> and <italic>J</italic>, arising from their product in the neural mass equation (see <xref ref-type="disp-formula" rid="eqn6">Equation 6</xref>). Such a nonlinear relationship between parameters poses challenges for deriving causal conclusions, as often occurs in other neural mass models. This is a demonstration of how Bayesian inference facilitates causal hypothesis testing without requiring additional non-identifiability analysis.</p>
</sec>
<sec id="s4">
<label>4.</label>
<title>Discussion</title>
<p>This study introduces the Virtual Brain Inference (<monospace>VBI</monospace>), a flexible and integrative toolkit designed to facilitate probabilistic inference on complex whole-brain dynamics using connectome-based models (forward problem) and simulation-based inference (inverse problem). The toolkit leverages high-performance programming languages (C++) and dynamic compilers (such as Python‚Äôs JIT compiler), alongside the computational power of parallel processors (GPUs), to significantly enhance the speed and efficiency of simulations. Additionally,<monospace>VBI</monospace> integrates popular feature extraction libraries with parallel multiprocessing to efficiently convert simulated time series into low-dimensional summary statistics. Moreover,<monospace>VBI</monospace> incorporates state-of-the-art deep neural density estimators (such as MAF and NSF generative models) to estimate the posterior density of control parameters within whole-brain models given low-dimensional data features. Our results demonstrated the versatility and efficacy of the<monospace>VBI</monospace> toolkit across commonly used whole-brain network models, such as WilsonCowan, Jansen-Rit, Stuart-Landau, Epileptor, Montbri√≥, and Wong-Wang equations placed at each region. The ability to perform parallel and rapid simulations, coupled with a taxonomy of feature extraction, allows for detailed and accurate parameter estimation from associated neuroimaging modalities such as (s)EEG/MEG/fMRI. This is crucial for advancing our understanding of brain dynamics and the underlying mechanisms of various brain disorders. Overall, VBI represents a substantial improvement over alternative methods, offering a robust framework for both simulation and parameter estimation, and contributing to the advancement of network neuroscience, potentially, to precision medicine.</p>
<p>The alternatives for parameter estimation include optimization techniques [<xref ref-type="bibr" rid="c66">66</xref>, <xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c63">63</xref>, <xref ref-type="bibr" rid="c64">64</xref>], approximate Bayesian computation (ABC) method, and MCMC sampling. Optimization techniques are sensitive to the choice of the objective function (e.g., minimizing distance error or maximizing correlation) and do not provide estimates of uncertainty. Although multiple runs and thresholding can be used to address these issues, such methods often fall short in revealing relationships between parameters, such as identifying degeneracy, which is crucial for reliable causal inference. Alternatively, a technique known as ABC compares observed and simulated data using a distance measure based on summary statistics [<xref ref-type="bibr" rid="c147">147</xref>, <xref ref-type="bibr" rid="c148">148</xref>, <xref ref-type="bibr" rid="c149">149</xref>]. It is known that ABC methods suffer from the curse of dimensionality, and their performance also depends critically on the tolerance level in the accepted/rejected setting [<xref ref-type="bibr" rid="c71">71</xref>, <xref ref-type="bibr" rid="c70">70</xref>]. The self-tuning variants of MCMC sampling have also been used for model inversion at the whole-brain level [<xref ref-type="bibr" rid="c76">76</xref>, <xref ref-type="bibr" rid="c67">67</xref>]. Although MCMC is unbiased and exact with infinite runs, it can be computationally prohibitive, and sophisticated reparameterization methods are often required to facilitate convergence at whole-brain level [<xref ref-type="bibr" rid="c123">123</xref>, <xref ref-type="bibr" rid="c150">150</xref>]. This becomes more challenging for gradient-based MCMC algorithms, due to the bistability and stiffness of neural mass models. Tailored to Bayes‚Äô rule, SBI sidesteps these issues by relying on expressive deep neural density estimators (such as MAF and NSF) on low-dimensional data features to efficiently approximate the posterior distribution of model parameters. Taking spiking neurons as generative models, this approach has demonstrated superior performance compared to alternative methods, as it does not require model or data features to be differentiable [<xref ref-type="bibr" rid="c71">71</xref>, <xref ref-type="bibr" rid="c81">81</xref>].</p>
<p>In previous studies, we demonstrated the effectiveness of SBI on virtual brain models of neurological [<xref ref-type="bibr" rid="c68">68</xref>, <xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c54">54</xref>], and neurodegenerative diseases [<xref ref-type="bibr" rid="c69">69</xref>, <xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c23">23</xref>] as well as focal intervention [<xref ref-type="bibr" rid="c59">59</xref>] and healthy aging [<xref ref-type="bibr" rid="c37">37</xref>]. In this work, we extended this probabilistic methodology to encompass a broader range of whole-brain network models, highlighting its flexibility and scalability in leveraging diverse computational resources, from CPUs/GPUs to high-performance computing facilities.</p>
<p>Our results indicated that the VBI toolkit effectively estimates posterior distributions of control parameters in whole-brain network modeling, offering a deeper understanding of the mechanisms underlying brain activity. For example, using the Montbrio and Wong-Wang models, we achieved a close match between observed and predicted FC/FCD matrices derived from BOLD time series (<xref rid="fig7" ref-type="fig">Figure 7</xref> and <xref rid="fig8" ref-type="fig">Figure 8</xref>). Additionally, the Jansen-Rit and Stuart-Landau models provided accurate inferences of PSD from neural activity (<xref rid="fig4" ref-type="fig">Figure 4</xref>, and <xref rid="fig5" ref-type="fig">Figure 5</xref>), while the Epileptor model precisely captured the spread of of seizure envelopes (<xref rid="fig6" ref-type="fig">Figure 6</xref>). These results underscore the toolkit‚Äôs capability to manage complex, high-dimensional data with precision. Uncertainty quantification using<monospace>VBI</monospace> can illuminate and combine the informativeness of data features (e.g., FC/FCD) and reveal the causal drivers behind interventions [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c69">69</xref>, <xref ref-type="bibr" rid="c59">59</xref>]. This adaptability ensures that<monospace>VBI</monospace> can be applied across various (source-localized) neuroimaging modalities, accommodating different computational capabilities and research needs.</p>
<p>Note that there is no specific rule for determining the optimal number of simulations required for training. In general, a larger number of simulations, depending on the available computational resources, tends to improve the quality of posterior estimation. However, when using synthetic data, we can monitor the z-score and posterior shrinkage to assess the accuracy and reliability of the inferred parameters (see <xref rid="figS10" ref-type="fig">Figure S10</xref>). This also critically depends on the parameter dimensionality. For instance, in estimating only global coupling parameter, a maximum of 300 simulations was used, demonstrating accurate estimation across models and different configurations (see <xref rid="figS20" ref-type="fig">Figure S20</xref>), except for the Jansen-Rit model, where coupling did not induce a significant change in the intrinsic frequency of regional activity. Importantly, the choice of data features is critical, and some factors (e.g., that lead to inaccurate feature calculation) can lead to the collapse of this method. For instance, high noise levels in observations or dynamical noise can compromise the accurate calculation of data features, undermining the inference process (see <xref rid="figS7" ref-type="fig">Figure S7</xref>, <xref rid="figS8" ref-type="fig">Figure S8</xref>, <xref rid="figS15" ref-type="fig">Figure S15</xref>, <xref rid="figS16" ref-type="fig">Figure S16</xref>). Identifying the set of low-dimensional data features that are relevant to the control parameters for each case study is another challenge in effectively applying SBI. Nevertheless, the uncertainty of the posterior informs us about the predictive power of these features. Statistical moments of time series could be effective candidates for most models. However, this poses a formidable challenge for inference from empirical data, as certain moments, such as the mean and variance, may be lost during preprocessing steps. The hyperparameter and noise estimation can also be challenging for SBI. Moreover, there is no established rule for determining the number of simulations for training, aside from relying on z-score values during in-silico testing, as it depends on available computational resources.</p>
<p>Various sequential methods, such as SNPE [<xref ref-type="bibr" rid="c125">125</xref>], SNLE [<xref ref-type="bibr" rid="c151">151</xref>], and SNRE [<xref ref-type="bibr" rid="c152">152</xref>], have been proposed to reduce computational costs of SBI by iteratively refining the fit to specific targets. These approaches aim for more precise parameter estimation by progressively adjusting the model based on each new data set or subset, potentially enhancing the accuracy of the fit at the reduced computational effort. The choice of method depends on the specific characteristics and requirements of the problem being addressed [<xref ref-type="bibr" rid="c128">128</xref>]. Our previous study indicates that for inferring whole-brain dynamics of epilepsy spread, the SNPE method outperforms alternative approaches [<xref ref-type="bibr" rid="c68">68</xref>]. Nevertheless, sequential methods can become unstable, with simulators potentially diverging and causing probability mass to leak into regions that lack prior support [<xref ref-type="bibr" rid="c68">68</xref>]. In this study, we used single-round training to benefit from an amortization strategy. This approach brings the costs of simulation and network training upfront, enabling inference on new data to be performed rapidly (within seconds). This strategy facilitates personalized inference at the subject level, as the generative model is tailored by the SC matrix, thereby allowing for rapid hypothesis evaluation specific to each subject (e.g., in delineating the epileptogenic and propagation zones). Note that model comparison across different configurations or model structures, as well-established in dynamic causal modeling [<xref ref-type="bibr" rid="c153">153</xref>, <xref ref-type="bibr" rid="c154">154</xref>, <xref ref-type="bibr" rid="c155">155</xref>], has yet to be explored in this context.</p>
<p>Deep learning algorithms are increasingly gaining traction in the context of wholebrain modeling. The VBI toolkit leverages a class of deep generative models, called Normalizing Flows (NFs; [<xref ref-type="bibr" rid="c130">130</xref>, <xref ref-type="bibr" rid="c131">131</xref>]), to model probability distributions given samples drawn from those distributions. Using NFs, a base probability distribution (e.g., a standard normal) is transformed into any complex distribution (potentially multimodal) through a sequence of invertible transformations. Variational autoencoders (VAEs; [<xref ref-type="bibr" rid="c156">156</xref>, <xref ref-type="bibr" rid="c157">157</xref>]) is a class of deep generative models to encode data into a latent space and then decode it back to reconstruct the original data. Recently, Sip et al.</p>
<p>[<xref ref-type="bibr" rid="c158">158</xref>] introduced a method using VAEs for nonlinear dynamical system identification, enabling the inference of neural mass models and regionand subject-specific parameters from functional data. VAEs have also been employed for dimensionality reduction of whole-brain functional connectivity [<xref ref-type="bibr" rid="c42">42</xref>], and to investigate various pathologies and their severity by analyzing the evolution of trajectories within a lowdimensional latent space [<xref ref-type="bibr" rid="c52">52</xref>]. Additionally, Generative Adversarial Networks (GANs; [<xref ref-type="bibr" rid="c159">159</xref>, <xref ref-type="bibr" rid="c160">160</xref>]) have demonstrated remarkable success in mapping latent space to data space by learning a manifold induced from a base density [<xref ref-type="bibr" rid="c129">129</xref>]. This method merits further exploration within the context of whole-brain dynamics. To fully harness the potential of deep generative models in large-scale brain network modeling, integrating VAEs and GANs into the <monospace>VBI</monospace> framework would be beneficial. This will elucidate their strengths and limitations within this context and guide future advancements in the field.</p>
<p>In summary, VBI offers fast simulations, taxonomy of feature extraction, and deep generative models, making it a versatile tool for model-based inference from different neuroimaging modalities, helping researchers to explore brain (dys)functioning in greater depth. This advancement not only enhances our theoretical understanding but also holds promise for practical applications in diagnosing and treating neurological conditions.</p>
</sec>
</body>
<back>
<glossary id="s5">
<label>5.</label>
<title>Glossary of technical terms</title>
<def-list>
<def-item>
<term>Bayesian Rule</term>
<def>
<p>A fundamental belief updating principle that calculates the probability of a hypothesis given new evidence.</p>
</def>
</def-item>
<def-item>
<term>Deep neural density estimators</term>
<def>
<p>A class of artificial neural network-based approaches that are used to learn and approximate the underlying probability distribution from a given dataset.</p>
</def>
</def-item>
<def-item>
<term>Control (generative) parameters</term>
<def>
<p>The bifurcation parameters, setting, or configuration within a generative model that controls the synthesis of data and potentially represents causal relationships.</p>
</def>
</def-item>
<def-item>
<term>Generative model</term>
<def>
<p>A statistical, machine learning, or mechanistic model that represents the underlying data distribution to generate new data resembling the original dataset.</p>
</def>
</def-item>
<def-item>
<term>Likelihood</term>
<def>
<p>The conditional probability of observing the evidence given a particular hypothesis.</p>
</def>
</def-item>
<def-item>
<term>Markov chain Monte Carlo</term>
<def>
<p>A family of stochastic algorithms used for uncertainty quantification by drawing random samples from probability distributions, in which the sampling process does not require knowledge of the entire distribution.</p>
</def>
</def-item>
<def-item>
<term>Prior</term>
<def>
<p>The initial probability assigned to a hypothesis before considering new evidence.</p>
</def>
</def-item>
<def-item>
<term>Probability distribution</term>
<def>
<p>The statistical description of potential outcomes of random events, where a numerical measure is assigned to the possibility of each specific outcome.</p>
</def>
</def-item>
<def-item>
<term>Probability density estimation</term>
<def>
<p>The process of inferring the underlying probability distribution of a random event based on observed data.</p>
</def>
</def-item>
<def-item>
<term>Posterior</term>
<def>
<p>The updated probability of a hypothesis after taking into account both prior beliefs and observed evidence.</p>
</def>
</def-item>
<def-item>
<term>Simulation-based inference</term>
<def>
<p>A machine learning approach that involves generating synthetic data through forward simulations to make inferences about complex systems, often when analytic or computational solutions are unavailable.</p>
</def>
</def-item>
<def-item>
<term>Whole-brain or virtual brain models</term>
<def>
<p>Computational models informed by personalized anatomical data, i.e., a set of equations describing regional brain dynamics placed at each node, which are then connected through structural connectivity matrix.</p>
</def>
</def-item>
</def-list>
</glossary>
<sec id="s5m">
<title>Information Sharing Statement</title>
<p>All code is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ins-amu/vbi">https://github.com/ins-amu/vbi</ext-link>).</p>
</sec>
<sec id="s7">
<title>Supplementary figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><p>(<bold>A</bold>) B enchmarking the simulation cost of the whole-brain network of Montbri√≥ model, with 2.5M time step on Intel(R) Core(TM) i9-10900 CPU 2.80 GHz (in red) and NVIDIA RTX A5000 GPU (in blue). GPUs deliver substantial speedups up to 100x over multi-core CPUs. (<bold>B</bold>) Comparing MAF and NSF density estimators, MAF is typically 2-4 times faster than NSF during the training process.</p></caption>
<graphic xlink:href="633922v2_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Inference across different configurations of heterogeneous generative parameters in the whole-brain network model based on the Wilson-Cowan framework.</title>
<p>The set of unknown control parameters is <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline34.gif"/></inline-formula>. We observe accurate posterior estimation across different scenarios by varying the external input current map across brain regions. Features derived from the power spectrum density were used for training, with a simulation budget of 260k. The regions are color-coded based on their current values: blue indicates low, green medium, and red high external input current and the size of the regions reflects the shrinkage of the posterior distribution for the corresponding inferred parameters. Top: axial view. Bottom: sagittal view. The three configurations (from left to right) correspond to: randomly assigned heterogeneous <italic>P</italic><sub><italic>i</italic></sub> values, increased <italic>P</italic><sub><italic>i</italic></sub> for the five strongest nodes, and increased <italic>P</italic><sub><italic>i</italic></sub> for the five weakest nodes.</p></caption>
<graphic xlink:href="633922v2_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Bayesian inference on heterogeneous control parameters in the whole-brain network of Wilson-Cowan model, as <xref ref-type="fig" rid="fig3">Figure 3</xref>, but ignoring the spatial information of the data features.</title>
<p>Here, we observe that the MAF density estimator leads to a multimodal posterior distribution for low values of <italic>P</italic><sub><italic>i</italic></sub>, when trained with a reduced set of data features, suggesting increased uncertainty or potential non-identifiability under limited information.</p></caption>
<graphic xlink:href="633922v2_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4.</label>
<caption><title>Same as <xref ref-type="fig" rid="figS3">Figure S3</xref>, but using the NSF density estimator.</title>
<p>Here, we observe more accurate estimation, however, NSF required approximately 6 hours of training compared to 20 minutes for MAF.</p></caption>
<graphic xlink:href="633922v2_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5.</label>
<caption><title>Inference on heterogeneous generative parameters in the whole-brain network of the Jansen-Rit model (<xref ref-type="disp-formula" rid="eqn2">Equation 2</xref>).</title>
<p>The set of inferred parameters is <italic>Œ∏-</italic> = <italic>G, C</italic><sub><italic>i</italic></sub> R<sup>89</sup>, consisting of the global scaling parameter <italic>G</italic> and the average number of synapses between neural populations <italic>C</italic><sub><italic>i</italic></sub> per region, with <italic>i</italic> ‚àà {1, 2, ‚Ä¶, <italic>N</italic><sub><italic>n</italic></sub> = 88} parcelled regions. For panels <bold>A</bold> to <bold>C</bold>, only the area under the curve of the power spectral density (PSD) in the [0,<xref ref-type="bibr" rid="c30">30</xref>] Hz band was used, while for panels <bold>D</bold> to <bold>F</bold>, additional statistical features of the PSD were included in the training, with a total budget of 50k simulations. (<bold>A</bold>) and (<bold>D</bold>) illustrate the observed and predicted EEG signals. (<bold>B</bold>) and (<bold>E</bold>) show the observed and predicted PSD signals. (<bold>C</bold>) and (<bold>F</bold>) display the posterior distribution of <italic>C</italic><sub><italic>i</italic></sub> for each region. The true values are indicated by vertical green lines, and the prior distribution is shown in blue.</p></caption>
<graphic xlink:href="633922v2_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6.</label>
<caption><title>Bayesian inference on the spatial map of epileptogenicity across brain regions in the VEP model (<xref ref-type="disp-formula" rid="eqn4">Equation 4</xref>), with slow time scale separation, <italic>œÑ</italic> = 10 <italic>ms</italic>.</title>
<p>(<bold>A</bold>) The observed seizure envelope generated by the 2D Epileptor model given two regions as epileptogenic zones (in red) and three regions as propagation zones (in yellow), while the rest are healthy (in green). (<bold>B</bold>) The predicted seizure envelope, by training MAF density estimator on a dataset containing 10k simulations, using only the total power per region as the data features. (<bold>C</bold>) and (<bold>D</bold>) show the observed and predicted data features, respectively. (<bold>E</bold>) and (<bold>F</bold>) show the posterior distribution of heterogeneous <italic>Œ∑</italic><sub><italic>i</italic></sub>, and global coupling parameter <italic>G</italic>, respectively. (<bold>F</bold>) The posterior z-scores versus posterior shrinkages for estimated parameters.</p></caption>
<graphic xlink:href="633922v2_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Figure S7.</label>
<caption><title>Evaluating the performance of estimation over the global coupling parameter, under different levels of additive noise in the VEP model.</title>
<p>(<bold>A</bold>) Simulated time series under different levels of additive noise. (<bold>B</bold>) and (<bold>C</bold>) Posterior shrinkage and z-score, respectively, as function of noise level <italic>œÉ</italic>. (<bold>D</bold>) Ground-true (in green), prior (in blue), and posterior distributions (in red) for global coupling parameter. We observe that at higher noise levels (e.g., <italic>œÉ</italic> = 0.3 and <italic>œÉ</italic> = 0.6), the ground truth is not captured within the posterior, leading to increased z-scores compared to the noise-free case <italic>œÉ</italic> = 0.</p></caption>
<graphic xlink:href="633922v2_figS7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS8" position="float" fig-type="figure">
<label>Figure S8.</label>
<caption><title>Evaluating the accuracy of dynamical noise estimation in the VEP model.</title>
<p>(<bold>A</bold>) Simulated time series generated under different levels of dynamical noise. (<bold>B</bold>) and (<bold>C</bold>) Posterior shrinkage and z-score, respectively, across different scenarios, demonstrating the accuracy and reliability of estimating the dynamical noise parameter. (<bold>D</bold>) Ground-true (in green), prior (in blue) and posterior distributions (in red), for different realizations of the dynamical noise parameter <italic>œÉ</italic> in the system. We observe robust estimation even at high levels of dynamical noise, though for <italic>œÉ</italic> ‚â• 1.6 the posterior sampling fails (shaded area).</p></caption>
<graphic xlink:href="633922v2_figS8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS9" position="float" fig-type="figure">
<label>Figure S9.</label>
<caption><title>Inference over different configurations of heterogeneous generative parameters in the whole-brain network model of the Montbri√≥, (see <xref ref-type="fig" rid="fig7">Figure 7</xref>).</title>
<p>The set of inferred parameters is <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline35.gif"/></inline-formula>, as the global scaling parameter and excitability per region, with <italic>i</italic> ‚àà {1, 2, ‚Ä¶, <italic>N</italic><sub><italic>n</italic></sub> = 88} parcelled regions.</p></caption>
<graphic xlink:href="633922v2_figS9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS10" position="float" fig-type="figure">
<label>Figure S10.</label>
<caption><title>Evaluating the reliability and accuracy of estimation (parameter <italic>G</italic>) with respect to the number of simulation used for training the MAF/NSF density estimator by (<bold>A</bold>) posterior shrinkage, and (<bold>B</bold>) posterior z-score, versus the number of simulations.</title></caption>
<graphic xlink:href="633922v2_figS10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS11" position="float" fig-type="figure">
<label>Figure S11.</label>
<caption><title>Scatter plot of summary statistics from the FC/FCD matrices in the whole-brain network of Montbri√≥ model (<xref ref-type="disp-formula" rid="eqn5">Equation 5</xref>).</title></caption>
<graphic xlink:href="633922v2_figS11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS12" position="float" fig-type="figure">
<label>Figure S12.</label>
<caption><title>Inference on heterogeneous generative parameters in the whole-brain network of Montbri√≥ model (<xref ref-type="disp-formula" rid="eqn5">Equation 5</xref>) involves estimating a parameter set denoted as <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline36.gif"/></inline-formula> from FC/FCD matrices.</title>
<p>Here, <italic>G</italic> is the global scaling parameter, and <italic>Œ∑</italic><sub><italic>i</italic></sub> represents the excitability of each brain region, where <italic>i</italic> ‚àà {1, 2, ‚Ä¶, <italic>N</italic><sub><italic>n</italic></sub> = 88} corresponds to the parcelled regions. Relying on only functional data features (FC/FCD) for training proves insufficient to accurately estimate the posterior distribution of heterogeneous excitabilities, even with a budget of 500k simulations. In (<bold>A</bold>), the posterior estimates are shown in red, the true values of <italic>Œ∑</italic><sub><italic>i</italic></sub> in green, and the prior distribution in blue. Panels (<bold>B</bold>) and (<bold>C</bold>) show the observed and predicted FC/FCD matrices, respectively, as the data features. The simulation to generate training data took approximately 24 hours, utilizing 10 GPUs concurrently. The training process lasted around 8 hours, while sampling required just a few seconds.</p></caption>
<graphic xlink:href="633922v2_figS12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS13" position="float" fig-type="figure">
<label>Figure S13.</label>
<caption><title>Inference of the global coupling parameter <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline37.gif"/></inline-formula> in the whole-brain network of Montbri√≥ model (<xref ref-type="disp-formula" rid="eqn5">Equation 5</xref>) using functional data features, using a budget of only 100 simulations.</title>
<p>(<bold>A</bold>) The uniform prior distribution of <italic>G</italic> in blue, the estimated posterior in red and the true value is shown in green. (<bold>B</bold>) and (<bold>C</bold>) The observed and predicted FC/FCD matrices, respectively.</p></caption>
<graphic xlink:href="633922v2_figS13.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS14" position="float" fig-type="figure">
<label>Figure S14.</label>
<caption><title>Inference of the global coupling parameter <inline-formula><inline-graphic mime-subtype="gif" mimetype="image" xlink:href="633922v2_inline38.gif"/></inline-formula> in the whole-brain model of Montbri√≥ (<xref ref-type="disp-formula" rid="eqn5">Equation 5</xref>) using TVB simulator, using summation of FC and FCD matrix elements as low dimensional data features, constrained by a budget of only 100 simulations.</title>
<p>(<bold>A</bold>) The uniform prior distribution of <italic>G</italic> in blue, the estimated posterior in red and the true value is shown in green. (<bold>B</bold>) and (<bold>C</bold>) show the observed and predicted FC/FCD matrices, respectively.</p></caption>
<graphic xlink:href="633922v2_figS14.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS15" position="float" fig-type="figure">
<label>Figure S15.</label>
<caption><title>Evaluating the performance of estimation over the global coupling parameter under different levels of noise added to the BOLD signal in the Montbri√≥ model.</title>
<p>(<bold>A</bold>) and (<bold>B</bold>) Simulated time series and corresponding FCD matrices, respectively, under different levels of additive noise. (<bold>C</bold>) and (<bold>D</bold>) Posterior shrinkages and z-scores, respectively, for different noise values. (<bold>E</bold>) Ground-true (in green), prior (in blue) and posterior distribution (in red), for estimation of coupling parameter <italic>G</italic>, given different values of additive noise to the BOLD signals, indicating robust inference. However, for very high values of noise (<italic>œÉ</italic> ‚â• 0.45) the posterior sampling fails (shaded area).</p></caption>
<graphic xlink:href="633922v2_figS15.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS16" position="float" fig-type="figure">
<label>Figure S16.</label>
<caption><title>Evaluating the accuracy of dynamical noise estimation in the Montbri√≥ model.</title>
<p>(<bold>A</bold>) and (<bold>B</bold>) Simulated time series and corresponding FCD matrices, respectively, under different levels of dynamical noise. (<bold>C</bold>) and (<bold>D</bold>) Posterior shrinkages and z-scores, respectively, for different noise values. (<bold>E</bold>) Ground-true (in green), prior (in blue) and posterior distributions (in red), for different realizations of the dynamical noise parameter <italic>œÉ</italic> in the system. We observe robust estimation even at high levels of dynamical noise.</p></caption>
<graphic xlink:href="633922v2_figS16.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS17" position="float" fig-type="figure">
<label>Figure S17.</label>
<caption><title>The distribution of observed and predicted functional connectivity (FC) in panel and the functional connectivity dynamics in panel (<bold>B</bold>), which are demonstrated in <xref rid="fig8" ref-type="fig">Figure 8</xref>.</title></caption>
<graphic xlink:href="633922v2_figS17.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS18" position="float" fig-type="figure">
<label>Figure S18.</label>
<caption><title>The non-identifiability in estimating <italic>G</italic> and <italic>J</italic> in the Wong-Wang model (<xref ref-type="disp-formula" rid="eqn6">Equation 6</xref>).</title>
<p>A total of 16k simulations and functional data features (see <xref ref-type="fig" rid="figS18">Figure S18</xref>) were used for training. The upper row illustrates the FCD variance and the sum of FC matrices plotted against the parameters <italic>G</italic> and <italic>J</italic>, respectively, for three selected observations marked by red circles. The lower panels display the parameter estimations corresponding to the selected observations. Due to structural non-identifiability in the product of <italic>G</italic> and <italic>J</italic>, the posterior distributions appear diffuse, showing a nonlinear dependency (banana shape) in the joint posterior plots.</p></caption>
<graphic xlink:href="633922v2_figS18.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS19" position="float" fig-type="figure">
<label>Figure S19.</label>
<caption><title>Scatter plot of summary statistics from the FC/FCD matrices in the whole-brain network of Wong-Wang model (<xref ref-type="disp-formula" rid="eqn6">Equation 6</xref>).</title></caption>
<graphic xlink:href="633922v2_figS19.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS20" position="float" fig-type="figure">
<label>Figure S20.</label>
<caption><title>Estimation of the global coupling parameter across whole-brain models, evaluated for multiple configurations (each represented by a violin plot).</title>
<p>The MAF density estimator was trained using the same features as described for the heterogeneous case in the main text. The simulation budget was limited to a maximum of 300. The green dashed line indicates an ideal estimation, while the gray dashed line represents a linear regression based on the mean values of the posterior sampling distributions. We can observe an accurate posterior estimation for coupling parameter across different models and configurations, except for the Jansen-Rit model, where coupling did not induce a significant change in the intrinsic frequency of regional activity. Note that the Montbri√≥ model shows more diffuse estimations compared to the others, due to switching dynamics driven by noise.</p></caption>
<graphic xlink:href="633922v2_figS20.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This project/research has received funding from the European Union‚Äôs Horizon Europe Programme under the Specific Grant Agreement No. 101147319 (EBRAINS 2.0 Project), No. 101137289 (Virtual Brain Twin Project), No. 101057429 (project environMENTAL), and government grant managed by the Agence Nationale de la Recherche reference ANR-22-PESN-0012 (France 2030 program). We acknowledge the use of Fenix Infrastructure resources, which are partially funded from the European Union‚Äôs Horizon 2020 research and innovation programme through the ICEI project under the grant agreement No. 800858. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
</ack>
<sec id="d1e3422" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Author contributions</title>
<p>Conceptualization: V.J., and M.H. Methodology: A.Z., M.W., L.D., and M.H. Software: A.Z, M.W., L.D., and M.H. Investigation: A.Z. Visualization: A.Z., Supervision: V.J., S.P., and M.H. Funding acquisition: V.J., M.H. Writing - original draft: A.Z., and M.H. Writing - review &amp; editing: A.Z, M.W., L.D., S.P., V.J, and M.H.</p>
</sec>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item>
<term>VBI</term>
<def><p>virtual brain inference</p></def>
</def-item>
<def-item>
<term>BOLD</term>
<def><p>blood-oxygen-level-dependent</p></def>
</def-item>
<def-item>
<term>fMRI</term>
<def><p>functional magnetic resonance imaging</p></def>
</def-item>
<def-item>
<term>EEG</term>
<def><p>Electroencephalography</p></def>
</def-item>
<def-item>
<term>MEG</term>
<def><p>Magnetoencephalography</p></def>
</def-item>
<def-item>
<term>sEEG</term>
<def><p>Stereoelectroencephalography</p></def>
</def-item>
<def-item>
<term>SC</term>
<def><p>structural connectivity</p></def>
</def-item>
<def-item>
<term>FC</term>
<def><p>functional connectivity</p></def>
</def-item>
<def-item>
<term>FCD</term>
<def><p>functional connectivity dynamic</p></def>
</def-item>
<def-item>
<term>PSD</term>
<def><p>power spectral density</p></def>
</def-item>
<def-item>
<term>SBI</term>
<def><p>simulation-based inference</p></def>
</def-item>
<def-item>
<term>MAF</term>
<def><p>masked autoregressive flow</p></def>
</def-item>
<def-item>
<term>NSF</term>
<def><p>neural spline flow</p></def>
</def-item>
<def-item>
<term>MCMC</term>
<def><p>Markov chain Monte Carlo</p></def>
</def-item>
</def-list>
</glossary>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Maria I</given-names> <surname>Falcon</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>Ana</given-names> <surname>Solodkin</surname></string-name></person-group>. <article-title>A new neuroinformatics approach to personalized medicine in neurology: The virtual brain</article-title>. <source>Current opinion in neurology</source>, <volume>29</volume>(<issue>4</issue>):<fpage>429</fpage>‚Äì<lpage>436</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Lin</given-names> <surname>Tan</surname></string-name>, <string-name><given-names>Teng</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>Lan</given-names> <surname>Tan</surname></string-name>, and <string-name><given-names>Jin-Tai</given-names> <surname>Yu</surname></string-name></person-group>. <article-title>Toward precision medicine in neurological diseases</article-title>. <source>Annals of translational medicine</source>, <volume>4</volume>(<issue>6</issue>), <year>2016</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jacob W</given-names> <surname>Vogel</surname></string-name>, <string-name><given-names>Nick</given-names> <surname>Corriveau-Lecavalier</surname></string-name>, <string-name><given-names>Nicolai</given-names> <surname>Franzmeier</surname></string-name>, <string-name><given-names>Joana B</given-names> <surname>Pereira</surname></string-name>, <string-name><given-names>Jesse A</given-names> <surname>Brown</surname></string-name>, <string-name><given-names>Anne</given-names> <surname>Maass</surname></string-name>, <string-name><given-names>Hugo</given-names> <surname>Botha</surname></string-name>, <string-name><given-names>William W</given-names> <surname>Seeley</surname></string-name>, <string-name><given-names>Dani S</given-names> <surname>Bassett</surname></string-name>, <string-name><given-names>David T</given-names> <surname>Jones</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Connectome-based modelling of neurodegen-erative diseases: towards precision medicine and mechanistic insight</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>24</volume>(<issue>10</issue>):<fpage>620</fpage>‚Äì<lpage>639</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Leanne M</given-names> <surname>Williams</surname></string-name> and <string-name><given-names>Susan Whitfield</given-names> <surname>Gabrieli</surname></string-name></person-group>. <article-title>Neuroimaging for precision medicine in psychiatry</article-title>. <source>Neuropsychopharmacology</source>, pages <fpage>1</fpage>‚Äì<lpage>12</lpage>, <year>2024</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michael</given-names> <surname>Breakspear</surname></string-name></person-group>. <article-title>Dynamic models of large-scale brain activity</article-title>. <source>Nature neuroscience</source>, <volume>20</volume>(<issue>3</issue>):<fpage>340</fpage>‚Äì<lpage>352</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Huifang E</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Marmaduke</given-names> <surname>Woodman</surname></string-name>, <string-name><given-names>Paul</given-names> <surname>Triebkorn</surname></string-name>, <string-name><given-names>Jean-Didier</given-names> <surname>Lemarechal</surname></string-name>, <string-name><given-names>Jayant</given-names> <surname>Jha</surname></string-name>, <string-name><given-names>Borana</given-names> <surname>Dollomaja</surname></string-name>, <string-name><given-names>Anirudh Nihalani</given-names> <surname>Vattikonda</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Sip</surname></string-name>, <string-name><given-names>Samuel Medina</given-names> <surname>Villalon</surname></string-name>, <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Delineating epileptogenic networks using brain imaging data and personalized modelling in drug-resistant epilepsy</article-title>. <source>Science Translational Medicine</source>, <volume>15</volume>(<issue>680</issue>):<fpage>eabp8982</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Lauren N</given-names> <surname>Ross</surname></string-name> and <string-name><given-names>Dani S</given-names> <surname>Bassett</surname></string-name></person-group>. <article-title>Causation in neuroscience: Keeping mechanism meaningful</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>25</volume>(<issue>2</issue>):<fpage>81</fpage>‚Äì<lpage>90</lpage>, <year>2024</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H.R.</given-names> <surname>Wilson</surname></string-name> and <string-name><given-names>J.D.</given-names> <surname>Cowan</surname></string-name></person-group>. <article-title>Excitatory and inhibitory interactions in localized populations of model neurons</article-title>. <source>Biophys. J.</source>, <volume>12</volume>:<fpage>1</fpage>‚Äì<lpage>24</lpage>, <year>1972</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.K.</given-names> <surname>Jirsa</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Haken</surname></string-name></person-group>. <article-title>Field theory of electromagnetic brain activity</article-title>. <source>Phys. Rev. Lett.</source>, <volume>77</volume>(<issue>5</issue>):<fpage>960</fpage>‚Äì<lpage>963</lpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Deco</surname></string-name>, <string-name><given-names>V.K.</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>P.A.</given-names> <surname>Robinson</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Breakspear</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Friston</surname></string-name></person-group>. <article-title>The dynamic brain: from spiking neurons to neural masses and cortical fields</article-title>. <source>PloS Comp. Biol.</source>, <volume>4</volume>(<issue>8</issue>), <year>2008</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Viktor K.</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>William C.</given-names> <surname>Stacey</surname></string-name>, <string-name><given-names>Pascale P.</given-names> <surname>Quilichini</surname></string-name>, <string-name><given-names>Anton I.</given-names> <surname>Ivanov</surname></string-name>, and <string-name><given-names>Christophe</given-names> <surname>Bernard</surname></string-name></person-group>. <article-title>On the nature of seizure dynamics</article-title>. <source>Brain</source>, <volume>137</volume>(<issue>8</issue>):<fpage>2210</fpage>‚Äì <lpage>2230</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ernest</given-names> <surname>Montbri√≥</surname></string-name>, <string-name><given-names>Diego</given-names> <surname>Paz√≥</surname></string-name>, and <string-name><given-names>Alex</given-names> <surname>Roxin</surname></string-name></person-group>. <article-title>Macroscopic description for networks of spiking neurons</article-title>. <source>Physical Review X</source>, <volume>5</volume>(<issue>2</issue>):<fpage>021028</fpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Blake J</given-names> <surname>Cook</surname></string-name>, <string-name><given-names>Andre DH</given-names> <surname>Peterson</surname></string-name>, <string-name><given-names>Wessel</given-names> <surname>Woldman</surname></string-name>, and <string-name><given-names>John R</given-names> <surname>Terry</surname></string-name></person-group>. <article-title>Neural field models: A mathematical overview and unifying framework</article-title>. <source>Mathematical Neuroscience and Applications</source>, <volume>2</volume>, <year>2022</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Christopher J</given-names> <surname>Honey</surname></string-name>, <string-name><given-names>Olaf</given-names> <surname>Sporns</surname></string-name>, <string-name><given-names>Leila</given-names> <surname>Cammoun</surname></string-name>, <string-name><given-names>Xavier</given-names> <surname>Gigandet</surname></string-name>, <string-name><given-names>Jean-Philippe</given-names> <surname>Thiran</surname></string-name>, <string-name><given-names>Reto</given-names> <surname>Meuli</surname></string-name>, and <string-name><given-names>Patric</given-names> <surname>Hagmann</surname></string-name></person-group>. <article-title>Predicting human resting-state functional connectivity from structural connectivity</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>106</volume>(<issue>6</issue>):<fpage>2035</fpage>‚Äì<lpage>2040</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Olaf</given-names> <surname>Sporns</surname></string-name>, <string-name><given-names>Giulio</given-names> <surname>Tononi</surname></string-name>, and <string-name><given-names>Rolf</given-names> <surname>K√∂tter</surname></string-name></person-group>. <article-title>The human connectome: a structural description of the human brain</article-title>. <source>PLoS computational biology</source>, <volume>1</volume>(<issue>4</issue>):<fpage>e42</fpage>, <year>2005</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michael</given-names> <surname>Schirner</surname></string-name>, <string-name><given-names>Simon</given-names> <surname>Rothmeier</surname></string-name>, <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>Anthony Randal</given-names> <surname>McIntosh</surname></string-name>, and <string-name><given-names>Petra</given-names> <surname>Ritter</surname></string-name></person-group>. <article-title>An automated pipeline for constructing personalized virtual brains from multimodal neuroimaging data</article-title>. <source>NeuroImage</source>, <volume>117</volume>:<fpage>343</fpage>‚Äì<lpage>357</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Vincent</given-names> <surname>Bazinet</surname></string-name>, <string-name><given-names>Justine Y</given-names> <surname>Hansen</surname></string-name>, and <string-name><given-names>Bratislav</given-names> <surname>Misic</surname></string-name></person-group>. <article-title>Towards a biologically annotated brain connectome</article-title>. <source>Nature reviews neuroscience</source>, <volume>24</volume>(<issue>12</issue>):<fpage>747</fpage>‚Äì<lpage>760</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Paula</given-names> <surname>Sanz-Leon</surname></string-name>, <string-name><given-names>Stuart A.</given-names> <surname>Knock</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Spiegler</surname></string-name>, and <string-name><given-names>Viktor K.</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Mathematical framework for large-scale brain network modeling in the virtual brain</article-title>. <source>NeuroImage</source>, <volume>111</volume>:<fpage>385</fpage> ‚Äì <lpage>430</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michael</given-names> <surname>Schirner</surname></string-name>, <string-name><given-names>Lia</given-names> <surname>Domide</surname></string-name>, <string-name><given-names>Dionysios</given-names> <surname>Perdikis</surname></string-name>, <string-name><given-names>Paul</given-names> <surname>Triebkorn</surname></string-name>, <string-name><given-names>Leon</given-names> <surname>Stefanovski</surname></string-name>, <string-name><given-names>Roopa</given-names> <surname>Pai</surname></string-name>, <string-name><given-names>Paula</given-names> <surname>Prodan</surname></string-name>, <string-name><given-names>Bogdan</given-names> <surname>Valean</surname></string-name>, <string-name><given-names>Jessica</given-names> <surname>Palmer</surname></string-name>, <string-name><given-names>Chlo√™</given-names> <surname>Langford</surname></string-name>, <string-name><given-names>Andr√©</given-names> <surname>Blickensd√∂rfer</surname></string-name>, <string-name><given-names>Michiel</given-names> <surname>van der Vlag</surname></string-name>, <string-name><given-names>Sandra</given-names> <surname>Diaz-Pier</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Peyser</surname></string-name>, <string-name><given-names>Wouter</given-names> <surname>Klijn</surname></string-name>, <string-name><given-names>Dirk</given-names> <surname>Pleiter</surname></string-name>, <string-name><given-names>Anne</given-names> <surname>Nahm</surname></string-name>, <string-name><given-names>Oliver</given-names> <surname>Schmid</surname></string-name>, <string-name><given-names>Marmaduke</given-names> <surname>Woodman</surname></string-name>, <string-name><given-names>Lyuba</given-names> <surname>Zehl</surname></string-name>, <string-name><given-names>Jan</given-names> <surname>Fousek</surname></string-name>, <string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name>, <string-name><given-names>Lionel</given-names> <surname>Kusch</surname></string-name>, <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Daniele</given-names> <surname>Marinazzo</surname></string-name>, Jean-<string-name><given-names>Fran√ßois</given-names> <surname>Mangin</surname></string-name>, Agnes Fl√∂el, <string-name><given-names>Simisola</given-names> <surname>Akintoye</surname></string-name>, <string-name><given-names>Bernd Carsten</given-names> <surname>Stahl</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Cepic</surname></string-name>, <string-name><given-names>Emily</given-names> <surname>Johnson</surname></string-name>, <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, <string-name><given-names>Anthony R.</given-names> <surname>McIntosh</surname></string-name>, <string-name><given-names>Claus C.</given-names> <surname>Hilgetag</surname></string-name>, <string-name><given-names>Marc</given-names> <surname>Morgan</surname></string-name>, <string-name><given-names>Bernd</given-names> <surname>Schuller</surname></string-name>, <string-name><given-names>Alex</given-names> <surname>Upton</surname></string-name>, <string-name><given-names>Colin</given-names> <surname>McMurtrie</surname></string-name>, <string-name><given-names>Timo</given-names> <surname>Dickscheid</surname></string-name>, <string-name><given-names>Jan G.</given-names> <surname>Bjaalie</surname></string-name>, <string-name><given-names>Katrin</given-names> <surname>Amunts</surname></string-name>, <string-name><given-names>Jochen</given-names> <surname>Mersmann</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>Petra</given-names> <surname>Ritter</surname></string-name></person-group>. <article-title>Brain simulation as a cloud service: The virtual brain on ebrains</article-title>. <source>NeuroImage</source>, <volume>251</volume>:<fpage>118973</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Katrin</given-names> <surname>Amunts</surname></string-name>, <string-name><given-names>Javier</given-names> <surname>DeFelipe</surname></string-name>, <string-name><given-names>Cyriel</given-names> <surname>Pennartz</surname></string-name>, <string-name><given-names>Alain</given-names> <surname>Destexhe</surname></string-name>, <string-name><given-names>Michele</given-names> <surname>Migliore</surname></string-name>, <string-name><given-names>Philippe</given-names> <surname>Ryvlin</surname></string-name>, <string-name><given-names>Steve</given-names> <surname>Furber</surname></string-name>, <string-name><given-names>Alois</given-names> <surname>Knoll</surname></string-name>, <string-name><given-names>Lise</given-names> <surname>Bitsch</surname></string-name>, <string-name><given-names>Jan G</given-names> <surname>Bjaalie</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Linking brain structure, activity, and cognitive function through computation</article-title>. <source>Eneuro</source>, <volume>9</volume>(<issue>2</issue>), <year>2022</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Egidio</given-names> <surname>D‚ÄôAngelo</surname></string-name> and <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>The quest for multiscale brain modeling</article-title>. <source>Trends in neurosciences</source>, <volume>45</volume>(<issue>10</issue>):<fpage>777</fpage>‚Äì<lpage>790</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gustavo</given-names> <surname>Patow</surname></string-name>, <string-name><given-names>Ignacio</given-names> <surname>Martin</surname></string-name>, <string-name><given-names>Yonatan Sanz</given-names> <surname>Perl</surname></string-name>, <string-name><given-names>Morten L</given-names> <surname>Kringelbach</surname></string-name>, and <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name></person-group>. <article-title>Whole-brain modelling: an essential tool for understanding brain dynamics</article-title>. <source>Nature Reviews Methods Primers</source>, <volume>4</volume>(<issue>1</issue>):<fpage>53</fpage>, <year>2024</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Damien</given-names> <surname>Depannemaecker</surname></string-name>, <string-name><given-names>Marisa</given-names> <surname>Saggio</surname></string-name>, <string-name><given-names>Paul</given-names> <surname>Triebkorn</surname></string-name>, <string-name><given-names>Giovanni</given-names> <surname>Rabuffo</surname></string-name>, <string-name><given-names>Jan</given-names> <surname>Fousek</surname></string-name>, <string-name><given-names>Abolfazl</given-names> <surname>Ziaeemehr</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Sip</surname></string-name>, <string-name><given-names>Anastasios</given-names> <surname>Athanasiadis</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Breyton</surname></string-name>, <string-name><given-names>Marmaduke</given-names> <surname>Woodman</surname></string-name>, <string-name><given-names>Huifang</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name>, <string-name><given-names>Pierpaolo</given-names> <surname>Sorrentino</surname></string-name>, and <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Principles and operation of virtual brain twins</article-title>. <source>IEEE Reviews in Biomedical Engineering</source>, <volume>14</volume>:<fpage>1</fpage>‚Äì<lpage>29</lpage>, <year>2025</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Olaf</given-names> <surname>Sporns</surname></string-name></person-group>. <source>Networks of the Brain</source>. <publisher-name>MIT press</publisher-name>, <year>2016</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Danielle S</given-names> <surname>Bassett</surname></string-name> and <string-name><given-names>Olaf</given-names> <surname>Sporns</surname></string-name></person-group>. <article-title>Network neuroscience</article-title>. <source>Nature neuroscience</source>, <volume>20</volume>(<issue>3</issue>):<fpage>353</fpage>‚Äì<lpage>364</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Anandamohan</given-names> <surname>Ghosh</surname></string-name>, <string-name><given-names>Y</given-names> <surname>Rho</surname></string-name>, <string-name><given-names>Anthony Randal</given-names> <surname>McIntosh</surname></string-name>, <string-name><given-names>Rolf</given-names> <surname>K√∂tter</surname></string-name>, and <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Noise during rest enables the exploration of the brain‚Äôs dynamic repertoire</article-title>. <source>PLoS computational biology</source>, <volume>4</volume>(<issue>10</issue>):<fpage>e1000196</fpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Christopher J</given-names> <surname>Honey</surname></string-name>, <string-name><given-names>Jean-Philippe</given-names> <surname>Thivierge</surname></string-name>, and <string-name><given-names>Olaf</given-names> <surname>Sporns</surname></string-name></person-group>. <article-title>Can structure predict function in the human brain?</article-title> <source>Neuroimage</source>, <volume>52</volume>(<issue>3</issue>):<fpage>766</fpage>‚Äì<lpage>776</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Hae-Jeong</given-names> <surname>Park</surname></string-name> and <string-name><given-names>Karl</given-names> <surname>Friston</surname></string-name></person-group>. <article-title>Structural and functional brain networks: from connections to cognition</article-title>. <source>Science</source>, <volume>342</volume>(<issue>6158</issue>):<fpage>1238411</fpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Francesca</given-names> <surname>Melozzi</surname></string-name>, <string-name><given-names>Eyal</given-names> <surname>Bergmann</surname></string-name>, <string-name><given-names>Julie A.</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>Itamar</given-names> <surname>Kahn</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>Christophe</given-names> <surname>Bernard</surname></string-name></person-group>. <article-title>Individual structural features constrain the mouse functional connectome</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>116</volume>(<issue>52</issue>):<fpage>26961</fpage>‚Äì<lpage>26969</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Laura E</given-names> <surname>Su√°rez</surname></string-name>, <string-name><given-names>Ross D</given-names> <surname>Markello</surname></string-name>, <string-name><given-names>Richard F</given-names> <surname>Betzel</surname></string-name>, and <string-name><given-names>Bratislav</given-names> <surname>Misic</surname></string-name></person-group>. <article-title>Linking structure and function in macroscale brain networks</article-title>. <source>Trends in cognitive sciences</source>, <volume>24</volume>(<issue>4</issue>):<fpage>302</fpage>‚Äì<lpage>315</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Guozheng</given-names> <surname>Feng</surname></string-name>, <string-name><given-names>Yiwen</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Weijie</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Haojie</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Jian</given-names> <surname>Cheng</surname></string-name>, and <string-name><given-names>Ni</given-names> <surname>Shu</surname></string-name></person-group>. <article-title>Spatial and temporal pattern of structure‚Äìfunction coupling of human brain connectome with development</article-title>. <source>eLife</source>, <volume>13</volume>:<elocation-id>RP93325</elocation-id>, <year>2024</year>. <pub-id pub-id-type="doi">10.7554/eLife.93325</pub-id></mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jacob</given-names> <surname>Tanner</surname></string-name>, <string-name><given-names>Joshua</given-names> <surname>Faskowitz</surname></string-name>, <string-name><given-names>Andreia Sofia</given-names> <surname>Teixeira</surname></string-name>, <string-name><given-names>Caio</given-names> <surname>Seguin</surname></string-name>, <string-name><given-names>Ludovico</given-names> <surname>Coletta</surname></string-name>, <string-name><given-names>Alessandro</given-names> <surname>Gozzi</surname></string-name>, <string-name><given-names>Bratislav</given-names> <surname>Mi≈°ic</surname></string-name>, and <string-name><given-names>Richard F</given-names> <surname>Betzel</surname></string-name></person-group>. <article-title>A multi-modal, asymmetric, weighted, and signed description of anatomical connectivity</article-title>. <source>Nature Communications</source>, <volume>15</volume>(<issue>1</issue>):<fpage>5865</fpage>, <year>2024</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Deco</surname></string-name>, <string-name><given-names>V.K.</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>A.R.</given-names> <surname>McIntosh</surname></string-name></person-group>. <article-title>Emerging concepts for the dynamical organization of resting-state activity in the brain</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>12</volume>(<issue>1</issue>):<fpage>43</fpage>‚Äì<lpage>56</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Peng</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Ru</given-names> <surname>Kong</surname></string-name>, <string-name><given-names>Xiaolu</given-names> <surname>Kong</surname></string-name>, Rapha√´l Li√©geois, <string-name><given-names>Csaba</given-names> <surname>Orban</surname></string-name>, <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, <string-name><given-names>Martijn P</given-names> <surname>Van Den Heuvel</surname></string-name>, and <string-name><given-names>BT Thomas</given-names> <surname>Yeo</surname></string-name></person-group>. <article-title>Inversion of a large-scale circuit model reveals a cortical hierarchy in the dynamic resting human brain</article-title>. <source>Science advances</source>, <volume>5</volume>(<issue>1</issue>):<fpage>eaat7854</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Abolfazl</given-names> <surname>Ziaeemehr</surname></string-name>, <string-name><given-names>Mina</given-names> <surname>Zarei</surname></string-name>, <string-name><given-names>Alireza</given-names> <surname>Valizadeh</surname></string-name>, and <string-name><given-names>Claudio R</given-names> <surname>Mirasso</surname></string-name></person-group>. <article-title>Frequency-dependent organization of the brain‚Äôs functional network through delayed-interactions</article-title>. <source>Neural Networks</source>, <volume>132</volume>:<fpage>155</fpage>‚Äì<lpage>165</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Xiaolu</given-names> <surname>Kong</surname></string-name>, <string-name><given-names>Ru</given-names> <surname>Kong</surname></string-name>, <string-name><given-names>Csaba</given-names> <surname>Orban</surname></string-name>, <string-name><given-names>Peng</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Shaoshi</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Kevin</given-names> <surname>Anderson</surname></string-name>, <string-name><given-names>Avram</given-names> <surname>Holmes</surname></string-name>, <string-name><given-names>John D</given-names> <surname>Murray</surname></string-name>, <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, <string-name><given-names>Martijn</given-names> <surname>van den Heuvel</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Sensory-motor cortices shape functional connectivity dynamics in the human brain</article-title>. <source>Nature communications</source>, <volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>15</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mario</given-names> <surname>Lavanga</surname></string-name>, <string-name><given-names>Johanna</given-names> <surname>Stumme</surname></string-name>, <string-name><given-names>Bahar Hazal</given-names> <surname>Yalcinkaya</surname></string-name>, <string-name><given-names>Jan</given-names> <surname>Fousek</surname></string-name>, <string-name><given-names>Christiane</given-names> <surname>Jockwitz</surname></string-name>, <string-name><given-names>Hiba</given-names> <surname>Sheheitli</surname></string-name>, <string-name><given-names>Nora</given-names> <surname>Bittner</surname></string-name>, <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name>, <string-name><given-names>Svenja</given-names> <surname>Caspers</surname></string-name>, <etal>et al.</etal></person-group> <article-title>The virtual aging brain: Causal inference supports interhemispheric dedifferentiation in healthy aging</article-title>. <source>NeuroImage</source>, <volume>283</volume>:<fpage>120403</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Shaoshi</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Bart</given-names> <surname>Larsen</surname></string-name>, <string-name><given-names>Valerie J</given-names> <surname>Sydnor</surname></string-name>, <string-name><given-names>Tianchu</given-names> <surname>Zeng</surname></string-name>, <string-name><given-names>Lijun</given-names> <surname>An</surname></string-name>, <string-name><given-names>Xiaoxuan</given-names> <surname>Yan</surname></string-name>, <string-name><given-names>Ru</given-names> <surname>Kong</surname></string-name>, <string-name><given-names>Xiaolu</given-names> <surname>Kong</surname></string-name>, <string-name><given-names>Ruben C</given-names> <surname>Gur</surname></string-name>, <string-name><given-names>Raquel E</given-names> <surname>Gur</surname></string-name>, <etal>et al.</etal></person-group> <article-title>In vivo whole-cortex marker of excitation-inhibition ratio indexes cortical maturation and cognitive ability in youth</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>121</volume>(<issue>23</issue>):<fpage>e2318641121</fpage>, <year>2024</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Pablo</given-names> <surname>Barttfeld</surname></string-name>, <string-name><given-names>Lynn</given-names> <surname>Uhrig</surname></string-name>, <string-name><given-names>Jacobo D</given-names> <surname>Sitt</surname></string-name>, <string-name><given-names>Mariano</given-names> <surname>Sigman</surname></string-name>, <string-name><given-names>B√©chir</given-names> <surname>Jarraya</surname></string-name>, and <string-name><given-names>Stanislas</given-names> <surname>Dehaene</surname></string-name></person-group>. <article-title>Signature of consciousness in the dynamics of resting-state brain activity</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>112</volume>(<issue>3</issue>):<fpage>887</fpage>‚Äì<lpage>892</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Hutt</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Darren</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Sleigh</surname></string-name></person-group>. <article-title>Anesthetic action on the transmission delay between cortex and thalamus explains the beta-buzz observed under propofol anesthesia</article-title>. <source>PLOS One</source>, <volume>12</volume>(<issue>6</issue>):<fpage>1</fpage>‚Äì<lpage>29</lpage>, 06 <year>2017</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andrea I</given-names> <surname>Luppi</surname></string-name>, <string-name><given-names>Joana</given-names> <surname>Cabral</surname></string-name>, <string-name><given-names>Rodrigo</given-names> <surname>Cofre</surname></string-name>, <string-name><given-names>Pedro AM</given-names> <surname>Mediano</surname></string-name>, <string-name><given-names>Fernando E</given-names> <surname>Rosas</surname></string-name>, <string-name><given-names>Abid Y</given-names> <surname>Qureshi</surname></string-name>, <string-name><given-names>Amy</given-names> <surname>Kuceyeski</surname></string-name>, <string-name><given-names>Enzo</given-names> <surname>Tagliazucchi</surname></string-name>, <string-name><given-names>Federico</given-names> <surname>Raimondo</surname></string-name>, <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Computational modelling in disorders of consciousness: Closing the gap towards personalised models for restoring consciousness</article-title>. <source>NeuroImage</source>, <volume>275</volume>:<fpage>120162</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yonatan Sanz</given-names> <surname>Perl</surname></string-name>, <string-name><given-names>Carla</given-names> <surname>Pallavicini</surname></string-name>, <string-name><given-names>Juan</given-names> <surname>Piccinini</surname></string-name>, <string-name><given-names>Athena</given-names> <surname>Demertzi</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Bonhomme</surname></string-name>, <string-name><given-names>Charlotte</given-names> <surname>Martial</surname></string-name>, <string-name><given-names>Rajanikant</given-names> <surname>Panda</surname></string-name>, <string-name><given-names>Naji</given-names> <surname>Alnagger</surname></string-name>, <string-name><given-names>Jitka</given-names> <surname>Annen</surname></string-name>, <string-name><given-names>Olivia</given-names> <surname>Gosseries</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Low-dimensional organization of global brain states of reduced consciousness</article-title>. <source>Cell Reports</source>, <volume>42</volume>(<issue>5</issue>), <year>2023</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>Olaf</given-names> <surname>Sporns</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Breakspear</surname></string-name>, <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, and <string-name><given-names>Anthony Randal</given-names> <surname>McIntosh</surname></string-name></person-group>. <article-title>Towards the virtual brain: network modeling of the intact and the damaged brain</article-title>. <source>Archives italiennes de biologie</source>, <volume>148</volume>(<issue>3</issue>):<fpage>189</fpage>‚Äì<lpage>205</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Paula Sanz</given-names> <surname>Leon</surname></string-name>, <string-name><given-names>Stuart</given-names> <surname>Knock</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Woodman</surname></string-name>, <string-name><given-names>Lia</given-names> <surname>Domide</surname></string-name>, <string-name><given-names>Jochen</given-names> <surname>Mersmann</surname></string-name>, <string-name><given-names>Anthony</given-names> <surname>McIntosh</surname></string-name>, and <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>The virtual brain: a simulator of primate brain network dynamics</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>7</volume>:<fpage>10</fpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>Huifang</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Paul</given-names> <surname>Triebkorn</surname></string-name>, <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Jayant</given-names> <surname>Jha</surname></string-name>, <string-name><given-names>Jorge</given-names> <surname>Gonzalez-Martinez</surname></string-name>, <string-name><given-names>Maxime</given-names> <surname>Guye</surname></string-name>, <string-name><given-names>Julia</given-names> <surname>Makhalova</surname></string-name>, and <string-name><given-names>Fabrice</given-names> <surname>Bartolomei</surname></string-name></person-group>. <article-title>Personalised virtual brain models in epilepsy</article-title>. <source>The Lancet Neurology</source>, <volume>22</volume>(<issue>5</issue>):<fpage>443</fpage>‚Äì<lpage>454</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Huifang E</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Paul</given-names> <surname>Triebkorn</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Breyton</surname></string-name>, <string-name><given-names>Borana</given-names> <surname>Dollomaja</surname></string-name>, <string-name><given-names>JeanDidier</given-names> <surname>Lemarechal</surname></string-name>, <string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name>, <string-name><given-names>Pierpaolo</given-names> <surname>Sorrentino</surname></string-name>, <string-name><given-names>Damien</given-names> <surname>Depannemaecker</surname></string-name>, <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, and <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Virtual brain twins: from basic neuroscience to clinical use</article-title>. <source>National Science Review</source>, <volume>11</volume>(<issue>5</issue>):<elocation-id>nwae079</elocation-id>, <year>2024</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.K.</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Proix</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Perdikis</surname></string-name>, <string-name><given-names>M.M.</given-names> <surname>Woodman</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Gonzalez-Martinez</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Bernard</surname></string-name>, <string-name><given-names>C.</given-names> <surname>B√©nar</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Guye</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Chauvel</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>Bartolomei</surname></string-name></person-group>. <article-title>The virtual epileptic patient: Individualized whole-brain models of epilepsy spread</article-title>. <source>NeuroImage</source>, <volume>145</volume>:<fpage>377</fpage> ‚Äì <lpage>388</lpage>, <year>2017</year>. Individual Subject Prediction.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Timoth√©e</given-names> <surname>Proix</surname></string-name>, <string-name><given-names>Fabrice</given-names> <surname>Bartolomei</surname></string-name>, <string-name><given-names>Maxime</given-names> <surname>Guye</surname></string-name>, and <string-name><given-names>Viktor K.</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Individual brain structure and modelling predict seizure propagation</article-title>. <source>Brain</source>, <volume>140</volume>(<issue>3</issue>):<fpage>641</fpage>‚Äì<lpage>654</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P</given-names> <surname>Sorrentino</surname></string-name>, <string-name><given-names>A</given-names> <surname>Pathak</surname></string-name>, <string-name><given-names>A</given-names> <surname>Ziaeemehr</surname></string-name>, <string-name><given-names>E</given-names> <surname>Troisi Lopez</surname></string-name>, <string-name><given-names>L</given-names> <surname>Cipriano</surname></string-name>, <string-name><given-names>A</given-names> <surname>Romano</surname></string-name>, <string-name><given-names>M</given-names> <surname>Sparaco</surname></string-name>, <string-name><given-names>M</given-names> <surname>Quarantelli</surname></string-name>, <string-name><given-names>A</given-names> <surname>Banerjee</surname></string-name>, <string-name><given-names>G</given-names> <surname>Sorrentino</surname></string-name>, <etal>et al.</etal></person-group> <article-title>The virtual multiple sclerosis patient</article-title>. <source>iScience</source>, <volume>27</volume>(<issue>7</issue>), <year>2024</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Mazzara</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Ziaeemehr</surname></string-name>, <string-name><given-names>E. Troisi</given-names> <surname>Lopez</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Cipriano</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Angiolelli</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sparaco</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Quarantelli</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Granata</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Sorrentino</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Sorrentino</surname></string-name></person-group>. <article-title>Mapping brain lesions to conduction delays: The next step for personalized brain models in multiple sclerosis</article-title>. <source>Human Brain Mapping</source>, <volume>46</volume>(<issue>7</issue>):<fpage>e70219</fpage>, <year>2025</year>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Bahar Hazal</given-names> <surname>Yalcinkaya</surname></string-name>, <string-name><given-names>Abolfazl</given-names> <surname>Ziaeemehr</surname></string-name>, <string-name><given-names>Jan</given-names> <surname>Fousek</surname></string-name>, <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Mario</given-names> <surname>Lavanga</surname></string-name>, <string-name><given-names>Ana</given-names> <surname>Solodkin</surname></string-name>, <string-name><given-names>Randy</given-names> <surname>McIntosh</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name></person-group>. <article-title>Personalized virtual brains of alzheimer‚Äôs disease link dynamical biomarkers of fmri with increased local excitability</article-title>. <source>medRxiv</source>, <year>2023</year>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yonatan Sanz</given-names> <surname>Perl</surname></string-name>, <string-name><given-names>Sol</given-names> <surname>Fittipaldi</surname></string-name>, <string-name><given-names>Cecilia Gonzalez</given-names> <surname>Campo</surname></string-name>, <string-name><given-names>Sebasti√°n</given-names> <surname>Moguilner</surname></string-name>, <string-name><given-names>Josephine</given-names> <surname>Cruzat</surname></string-name>, <string-name><given-names>Matias E</given-names> <surname>Fraile-Vazquez</surname></string-name>, <string-name><given-names>Rub√©n</given-names> <surname>Herzog</surname></string-name>, <string-name><given-names>Morten L</given-names> <surname>Kringelbach</surname></string-name>, <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, <string-name><given-names>Pavel</given-names> <surname>Prado</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Model-based whole-brain perturbational landscape of neurodegenerative diseases</article-title>. <source>eLife</source>, <volume>12</volume>:<elocation-id>e83970</elocation-id>, <year>2023</year>. <pub-id pub-id-type="doi">10.7554/eLife.83970</pub-id></mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kyesam</given-names> <surname>Jung</surname></string-name>, <string-name><given-names>Esther</given-names> <surname>Florin</surname></string-name>, <string-name><given-names>Kaustubh R</given-names> <surname>Patil</surname></string-name>, <string-name><given-names>Julian</given-names> <surname>Caspers</surname></string-name>, <string-name><given-names>Christian</given-names> <surname>Rubbert</surname></string-name>, <string-name><given-names>Simon B</given-names> <surname>Eickhoff</surname></string-name>, and <string-name><given-names>Oleksandr V</given-names> <surname>Popovych</surname></string-name></person-group>. <article-title>Whole-brain dynamical modelling for classification of parkinson‚Äôs disease</article-title>. <source>Brain communications</source>, <volume>5</volume>(<issue>1</issue>):<fpage>fcac331</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Marianna</given-names> <surname>Angiolelli</surname></string-name>, <string-name><given-names>Damien</given-names> <surname>Depannemaecker</surname></string-name>, <string-name><given-names>Hasnae</given-names> <surname>Agouram</surname></string-name>, <string-name><given-names>Jean</given-names> <surname>Regis</surname></string-name>, <string-name><given-names>Romain</given-names> <surname>Carron</surname></string-name>, <string-name><given-names>Marmaduke</given-names> <surname>Woodman</surname></string-name>, <string-name><given-names>Letizia</given-names> <surname>Chiodo</surname></string-name>, <string-name><given-names>Paul</given-names> <surname>Triebkorn</surname></string-name>, <string-name><given-names>Abolfazl</given-names> <surname>Ziaeemehr</surname></string-name>, <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <etal>et al.</etal></person-group> <article-title>The virtual parkinsonian patient</article-title>. <source>npj Systems Biology and Applications</source>, <volume>11</volume>(<issue>1</issue>):<fpage>40</fpage>, <year>2025</year>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name> and <string-name><given-names>Morten L</given-names> <surname>Kringelbach</surname></string-name></person-group>. <article-title>Great expectations: using wholebrain computational connectomics for understanding neuropsychiatric disorders</article-title>. <source>Neuron</source>, <volume>84</volume>(<issue>5</issue>):<fpage>892</fpage>‚Äì<lpage>905</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Behzad</given-names> <surname>Iravani</surname></string-name>, <string-name><given-names>Artin</given-names> <surname>Arshamian</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Fransson</surname></string-name>, and <string-name><given-names>Neda</given-names> <surname>Kaboodvand</surname></string-name></person-group>. <article-title>Whole-brain modelling of resting state fmri differentiates adhd subtypes and facilitates stratified neuro-stimulation therapy</article-title>. <source>Neuroimage</source>, <volume>231</volume>:<fpage>117844</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Anna</given-names> <surname>Letizia</surname></string-name> <string-name><given-names>Allegra</given-names> <surname>Mascaro</surname></string-name>, <string-name><given-names>Egidio</given-names> <surname>Falotico</surname></string-name>, <string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name>, <string-name><given-names>Maria</given-names> <surname>Pasquini</surname></string-name>, <string-name><given-names>Lorenzo</given-names> <surname>Vannucci</surname></string-name>, <string-name><given-names>N√∫ria</given-names> <surname>Tort-Colet</surname></string-name>, <string-name><given-names>Emilia</given-names> <surname>Conti</surname></string-name>, <string-name><given-names>Francesco</given-names> <surname>Resta</surname></string-name>, <string-name><given-names>Cristina</given-names> <surname>Spalletti</surname></string-name>, <string-name><given-names>Shravan Tata</given-names> <surname>Ramalingasetty</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Experimental and computational study on motor control and recovery after stroke: toward a constructive loop between experimental and virtual embodied neuroscience</article-title>. <source>Frontiers in systems neuroscience</source>, <volume>14</volume>:<fpage>31</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sebastian</given-names> <surname>Idesis</surname></string-name>, <string-name><given-names>Chiara</given-names> <surname>Favaretto</surname></string-name>, <string-name><given-names>Nicholas V</given-names> <surname>Metcalf</surname></string-name>, <string-name><given-names>Joseph C</given-names> <surname>Griffis</surname></string-name>, <string-name><given-names>Gordon L</given-names> <surname>Shulman</surname></string-name>, <string-name><given-names>Maurizio</given-names> <surname>Corbetta</surname></string-name>, and <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name></person-group>. <article-title>Inferring the dynamical effects of stroke lesions through whole-brain modeling</article-title>. <source>NeuroImage: Clinical</source>, <volume>36</volume>:<fpage>103233</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Giovanni</given-names> <surname>Rabuffo</surname></string-name>, <string-name><given-names>Houefa-Armelle</given-names> <surname>Lokossou</surname></string-name>, <string-name><given-names>Zengmin</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Abolfazl</given-names> <surname>Ziaee-Mehr</surname></string-name>, <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Pascale P.</given-names> <surname>Quilichini</surname></string-name>, <string-name><given-names>Antoine</given-names> <surname>Ghestem</surname></string-name>, <string-name><given-names>Ouafae</given-names> <surname>Arab</surname></string-name>, <string-name><given-names>Monique</given-names> <surname>Esclapez</surname></string-name>, <string-name><given-names>Parul</given-names> <surname>Verma</surname></string-name>, <string-name><given-names>Ashish</given-names> <surname>Raj</surname></string-name>, <string-name><given-names>Alessandro</given-names> <surname>Gozzi</surname></string-name>, <string-name><given-names>Pierpaolo</given-names> <surname>Sorrentino</surname></string-name>, <string-name><given-names>Kai-Hsiang</given-names> <surname>Chuang</surname></string-name>, <string-name><given-names>Teodora-Adriana</given-names> <surname>Perles-Barbacaru</surname></string-name>, <string-name><given-names>Ang√É√Øe</given-names> <surname>Viola</surname></string-name>, <string-name><given-names>Viktor K.</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>Christophe</given-names> <surname>Bernard</surname></string-name></person-group>. <article-title>Mapping global brain reconfigurations following local targeted manipulations</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>122</volume>(<issue>16</issue>):<fpage>e2405706122</fpage>, <year>2025</year>.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jin</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Min</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Wei</given-names> <surname>Lan</surname></string-name>, <string-name><given-names>Fang-Xiang</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Pan</surname></string-name>, and <string-name><given-names>Jianxin</given-names> <surname>Wang</surname></string-name></person-group>. <article-title>Classification of alzheimer‚Äôs disease using whole brain hierarchical network</article-title>. <source>IEEE/ACM transactions on computational biology and bioinformatics</source>, <volume>15</volume>(<issue>2</issue>):<fpage>624</fpage>‚Äì<lpage>632</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gustavo</given-names> <surname>Patow</surname></string-name>, <string-name><given-names>Leon</given-names> <surname>Stefanovski</surname></string-name>, <string-name><given-names>Petra</given-names> <surname>Ritter</surname></string-name>, <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, <string-name><given-names>Xenia</given-names> <surname>Kobeleva</surname></string-name>, and <collab>Alzheimer‚Äôs Disease Neuroimaging Initiative</collab></person-group>. <article-title>Whole-brain modeling of the differential influences of amyloid-beta and tau in alzheimer‚Äôs disease</article-title>. <source>Alzheimer‚Äôs Research &amp; Therapy</source>, <volume>15</volume>(<issue>1</issue>):<fpage>210</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Timoth√©e</given-names> <surname>Proix</surname></string-name>, <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>Fabrice</given-names> <surname>Bartolomei</surname></string-name>, <string-name><given-names>Maxime</given-names> <surname>Guye</surname></string-name>, and <string-name><given-names>Wilson</given-names> <surname>Truccolo</surname></string-name></person-group>. <article-title>Predicting the spatiotemporal diversity of seizure propagation and termination in human focal epilepsy</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>):<fpage>1088</fpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c63"><label>[63]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Joana</given-names> <surname>Cabral</surname></string-name>, <string-name><given-names>Francesca</given-names> <surname>Castaldo</surname></string-name>, <string-name><given-names>Jakub</given-names> <surname>Vohryzek</surname></string-name>, <string-name><given-names>Vladimir</given-names> <surname>Litvak</surname></string-name>, <string-name><given-names>Christian</given-names> <surname>Bick</surname></string-name>, <string-name><given-names>Renaud</given-names> <surname>Lambiotte</surname></string-name>, <string-name><given-names>Karl</given-names> <surname>Friston</surname></string-name>, <string-name><given-names>Morten L</given-names> <surname>Kringelbach</surname></string-name>, and <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name></person-group>. <article-title>Metastable oscillatory modes emerge from synchronization in the brain spacetime connectome</article-title>. <source>Communications Physics</source>, <volume>5</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>13</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c64"><label>[64]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yuanzhe</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Caio</given-names> <surname>Seguin</surname></string-name>, <string-name><given-names>Sina</given-names> <surname>Mansour</surname></string-name>, <string-name><given-names>Stuart</given-names> <surname>Oldham</surname></string-name>, <string-name><given-names>Richard</given-names> <surname>Betzel</surname></string-name>, <string-name><given-names>Maria A</given-names> <surname>Di Biase</surname></string-name>, and <string-name><given-names>Andrew</given-names> <surname>Zalesky</surname></string-name></person-group>. <article-title>Parameter estimation for connectome generative models: Accuracy, reliability, and a fast parameter fitting method</article-title>. <source>Neuroimage</source>, <volume>270</volume>:<fpage>119962</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c65"><label>[65]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Carl-Magnus</given-names> <surname>Svensson</surname></string-name>, <string-name><given-names>Stephen</given-names> <surname>Coombes</surname></string-name>, and <string-name><given-names>Jonathan Westley</given-names> <surname>Peirce</surname></string-name></person-group>. <article-title>Using Evolutionary Algorithms for Fitting High-Dimensional Models to Neuronal Data</article-title>. <source>Neuroinformatics</source>, <volume>10</volume>(<issue>2</issue>):<fpage>199</fpage>‚Äì<lpage>218</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c66"><label>[66]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Axel</given-names> <surname>Hutt</surname></string-name>, <string-name><given-names>Laure</given-names> <surname>Buhry</surname></string-name>, and <string-name><given-names>Jamie</given-names> <surname>Sleigh</surname></string-name></person-group>. <article-title>Optimal model parameter estimation from eeg power spectrum features observed during general anesthesia</article-title>. <source>Neuroinformatics</source>, <volume>16</volume>(<issue>2</issue>):<fpage>231</fpage>‚Äì<lpage>251</lpage>, <month>Apr</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c67"><label>[67]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Anirudh N</given-names> <surname>Vattikonda</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Sip</surname></string-name>, <string-name><given-names>Sandra</given-names> <surname>Diaz-Pier</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Peyser</surname></string-name>, <string-name><given-names>Huifang</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Maxime</given-names> <surname>Guye</surname></string-name>, <string-name><given-names>Fabrice</given-names> <surname>Bartolomei</surname></string-name>, <string-name><given-names>Marmaduke M</given-names> <surname>Woodman</surname></string-name>, and <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>On the influence of prior informa tion evaluated by fully bayesian criteria in a personalized whole-brain model of epilepsy spread</article-title>. <source>PLoS computational biology</source>, <volume>17</volume>(<issue>7</issue>):<fpage>e1009129</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c68"><label>[68]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Anirudh N</given-names> <surname>Vattikonda</surname></string-name>, <string-name><given-names>Jayant</given-names> <surname>Jha</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Sip</surname></string-name>, <string-name><given-names>Marmaduke M</given-names> <surname>Woodman</surname></string-name>, <string-name><given-names>Fabrice</given-names> <surname>Bartolomei</surname></string-name>, and <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Amortized bayesian inference on generative dynamical network models of epilepsy using deep neural density estimators</article-title>. <source>Neural Networks</source>, <volume>163</volume>:<fpage>178</fpage>‚Äì<lpage>194</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c69"><label>[69]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Abolfazl</given-names> <surname>Ziaeemehr</surname></string-name>, <string-name><given-names>Marmaduke M</given-names> <surname>Woodman</surname></string-name>, <string-name><given-names>Jan</given-names> <surname>Fousek</surname></string-name>, <string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name>, and <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Simulation-based inference on virtual brain models of disorders</article-title>. <source>Machine Learning: Science and Technology</source>, <volume>5</volume>(<issue>3</issue>):<fpage>035019</fpage>, <month>jul</month> <year>2024</year>.</mixed-citation></ref>
<ref id="c70"><label>[70]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kyle</given-names> <surname>Cranmer</surname></string-name>, <string-name><given-names>Johann</given-names> <surname>Brehmer</surname></string-name>, and <string-name><given-names>Gilles</given-names> <surname>Louppe</surname></string-name></person-group>. <article-title>The frontier of simulation-based inference</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>48</issue>):<fpage>30055</fpage>‚Äì<lpage>30062</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c71"><label>[71]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Pedro J</given-names> <surname>Gon√ßalves</surname></string-name>, <string-name><given-names>Jan-Matthis</given-names> <surname>Lueckmann</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Deistler</surname></string-name>, <string-name><given-names>Marcel</given-names> <surname>Nonnenmacher</surname></string-name>, Kaan √ñcal, <string-name><given-names>Giacomo</given-names> <surname>Bassetto</surname></string-name>, <string-name><given-names>Chaitanya</given-names> <surname>Chintaluri</surname></string-name>, <string-name><given-names>William F</given-names> <surname>Podlaski</surname></string-name>, <string-name><given-names>Sara A</given-names> <surname>Haddad</surname></string-name>, <string-name><given-names>Tim P</given-names> <surname>Vogels</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title>. <source>eLife</source>, <volume>9</volume>:<elocation-id>e56261</elocation-id>, <year>2020</year>. <pub-id pub-id-type="doi">10.7554/eLife.56261</pub-id></mixed-citation></ref>
<ref id="c72"><label>[72]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Andrew</given-names> <surname>Gelman</surname></string-name>, <string-name><given-names>Christian</given-names> <surname>Robert</surname></string-name>, <string-name><given-names>Nicolas</given-names> <surname>Chopin</surname></string-name>, and <string-name><given-names>Judith</given-names> <surname>Rousseau</surname></string-name></person-group>. <source>Bayesian Data Analysis</source>. <publisher-name>CRC Press</publisher-name>, <year>1995</year>.</mixed-citation></ref>
<ref id="c73"><label>[73]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Samuel</given-names> <surname>Gershman</surname></string-name> and <string-name><given-names>Noah</given-names> <surname>Goodman</surname></string-name></person-group>. <article-title>Amortized inference in probabilistic reasoning</article-title>. In <source>Proceedings of the annual meeting of the cognitive science society</source>, volume <volume>36</volume>, <year>2014</year>.</mixed-citation></ref>
<ref id="c74"><label>[74]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Betancourt</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Girolami</surname></string-name></person-group>. <article-title>Hamiltonian monte carlo for hierarchical models</article-title>. <source>arxiv</source> <pub-id pub-id-type="arxiv">1312.0906</pub-id>, <year>2013</year>.</mixed-citation></ref>
<ref id="c75"><label>[75]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Betancourt</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Byrne</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Livingstone</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Girolami</surname></string-name></person-group>. <article-title>The geometric foundations of hamiltonian monte carlo</article-title>. <source>arxiv</source> <pub-id pub-id-type="arxiv">1410.5110</pub-id>, <year>2014</year>.</mixed-citation></ref>
<ref id="c76"><label>[76]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>AN</given-names> <surname>Vattikonda</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Sip</surname></string-name>, <string-name><given-names>Maxime</given-names> <surname>Guye</surname></string-name>, <string-name><given-names>Fabrice</given-names> <surname>Bartolomei</surname></string-name>, <string-name><given-names>Michael Marmaduke</given-names> <surname>Woodman</surname></string-name>, and <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>The Bayesian virtual epileptic patient: A probabilistic framework designed to infer the spatial map of epileptogenicity in a personalized large-scale brain model of epilepsy spread</article-title>. <source>NeuroImage</source>, <volume>217</volume>:<fpage>116839</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c77"><label>[77]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Scott A</given-names> <surname>Sisson</surname></string-name>, <string-name><given-names>Yanan</given-names> <surname>Fan</surname></string-name>, and <string-name><given-names>Mark M</given-names> <surname>Tanaka</surname></string-name></person-group>. <article-title>Sequential monte carlo without likelihoods</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>104</volume>(<issue>6</issue>):<fpage>1760</fpage>‚Äì <lpage>1765</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c78"><label>[78]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mark A</given-names> <surname>Beaumont</surname></string-name>, <string-name><given-names>Jean-Marie</given-names> <surname>Cornuet</surname></string-name>, <string-name><given-names>Jean-Michel</given-names> <surname>Marin</surname></string-name>, and <string-name><given-names>Christian P</given-names> <surname>Robert</surname></string-name></person-group>. <article-title>Adaptive approximate bayesian computation</article-title>. <source>Biometrika</source>, <volume>96</volume>(<issue>4</issue>):<fpage>983</fpage>‚Äì <lpage>990</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c79"><label>[79]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>George</given-names> <surname>Papamakarios</surname></string-name>, <string-name><given-names>Theo</given-names> <surname>Pavlakou</surname></string-name>, and <string-name><given-names>Iain</given-names> <surname>Murray</surname></string-name></person-group>. <article-title>Masked autoregressive flow for density estimation</article-title>. <conf-name>Advances in Neural Information Processing Systems</conf-name>, <year>2017</year>.</mixed-citation></ref>
<ref id="c80"><label>[80]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Conor</given-names> <surname>Durkan</surname></string-name>, <string-name><given-names>Artur</given-names> <surname>Bekasov</surname></string-name>, <string-name><given-names>Iain</given-names> <surname>Murray</surname></string-name>, and <string-name><given-names>George</given-names> <surname>Papamakarios</surname></string-name></person-group>. <article-title>Neural spline flows</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>32</volume>:<fpage>7511</fpage>‚Äì <lpage>7522</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c81"><label>[81]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nina</given-names> <surname>Baldy</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Breyton</surname></string-name>, <string-name><given-names>Marmaduke M</given-names> <surname>Woodman</surname></string-name>, <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name></person-group>. <article-title>Inference on the macroscopic dynamics of spiking neurons</article-title>. <source>Neural Computation</source>, pages <fpage>1</fpage>‚Äì<lpage>43</lpage>, 08 <year>2024</year>.</mixed-citation></ref>
<ref id="c82"><label>[82]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.H.</given-names> <surname>Jansen</surname></string-name> and <string-name><given-names>V.G.</given-names> <surname>Rit</surname></string-name></person-group>. <article-title>Electroencephalogram and visual evoked potential generation in a mathematical model of coupled cortical columns</article-title>. <source>Biol. Cybern</source>, <volume>73</volume>:<fpage>357</fpage>‚Äì<lpage>366</lpage>, <year>1995</year>.</mixed-citation></ref>
<ref id="c83"><label>[83]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Olivier</given-names> <surname>David</surname></string-name> and <string-name><given-names>Karl J.</given-names> <surname>Friston</surname></string-name></person-group>. <article-title>A neural mass model for meg/eeg:: coupling and neuronal dynamics</article-title>. <source>NeuroImage</source>, <volume>20</volume>(<issue>3</issue>):<fpage>1743</fpage> ‚Äì <lpage>1755</lpage>, <year>2003</year>.</mixed-citation></ref>
<ref id="c84"><label>[84]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Anton A</given-names> <surname>Selivanov</surname></string-name>, <string-name><given-names>Judith</given-names> <surname>Lehnert</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Dahms</surname></string-name>, <string-name><given-names>Philipp</given-names> <surname>H√∂vel</surname></string-name>, <string-name><given-names>Alexander L</given-names> <surname>Fradkov</surname></string-name>, and <string-name><given-names>Eckehard</given-names> <surname>Sch√∂ll</surname></string-name></person-group>. <article-title>Adaptive synchronization in delay-coupled networks of stuart-landau oscillators</article-title>. <source>Physical Review E‚ÄìStatistical, Nonlinear, and Soft Matter Physics</source>, <volume>85</volume>(<issue>1</issue>):<fpage>016201</fpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c85"><label>[85]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kong-Fatt</given-names> <surname>Wong</surname></string-name> and <string-name><given-names>Xiao-Jing</given-names> <surname>Wang</surname></string-name></person-group>. <article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title>. <source>Journal of Neuroscience</source>, <volume>26</volume>(<issue>4</issue>):<fpage>1314</fpage>‚Äì<lpage>1328</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c86"><label>[86]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, <string-name><given-names>Adri√°n</given-names> <surname>Ponce-Alvarez</surname></string-name>, <string-name><given-names>Dante</given-names> <surname>Mantini</surname></string-name>, <string-name><given-names>Gian Luca</given-names> <surname>Romani</surname></string-name>, <string-name><given-names>Patric</given-names> <surname>Hagmann</surname></string-name>, and <string-name><given-names>Maurizio</given-names> <surname>Corbetta</surname></string-name></person-group>. <article-title>Resting-state functional connectivity emerges from structurally and dynamically shaped slow linear fluctuations</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>27</issue>):<fpage>11239</fpage>‚Äì<lpage>11252</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c87"><label>[87]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Benoit</given-names> <surname>Duchet</surname></string-name>, <string-name><given-names>Filippo</given-names> <surname>Ghezzi</surname></string-name>, <string-name><given-names>Gihan</given-names> <surname>Weerasinghe</surname></string-name>, <string-name><given-names>Gerd</given-names> <surname>Tinkhauser</surname></string-name>, <string-name><given-names>Andrea A</given-names> <surname>K√ºhn</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Brown</surname></string-name>, <string-name><given-names>Christian</given-names> <surname>Bick</surname></string-name>, and <string-name><given-names>Rafal</given-names> <surname>Bogacz</surname></string-name></person-group>. <article-title>Average beta burst duration profiles provide a signature of dynamical changes between the on and off medication states in parkinson‚Äôs disease</article-title>. <source>PLoS computational biology</source>, <volume>17</volume>(<issue>7</issue>):<fpage>e1009116</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c88"><label>[88]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>James J</given-names> <surname>Sermon</surname></string-name>, <string-name><given-names>Maria</given-names> <surname>Olaru</surname></string-name>, <string-name><given-names>Juan</given-names> <surname>Anso</surname></string-name>, <string-name><given-names>Stephanie</given-names> <surname>Cernera</surname></string-name>, <string-name><given-names>Simon</given-names> <surname>Little</surname></string-name>, <string-name><given-names>Maria</given-names> <surname>Shcherbakova</surname></string-name>, <string-name><given-names>Rafal</given-names> <surname>Bogacz</surname></string-name>, <string-name><given-names>Philip A</given-names> <surname>Starr</surname></string-name>, <string-name><given-names>Timothy</given-names> <surname>Denison</surname></string-name>, and <string-name><given-names>Benoit</given-names> <surname>Duchet</surname></string-name></person-group>. <article-title>Sub-harmonic entrainment of cortical gamma oscillations to deep brain stimulation in parkinson‚Äôs disease: model based predictions and validation in three human subjects</article-title>. <source>Brain Stimulation</source>, <volume>16</volume>(<issue>5</issue>):<fpage>1412</fpage>‚Äì<lpage>1424</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c89"><label>[89]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Hugh R</given-names> <surname>Wilson</surname></string-name> and <string-name><given-names>Jack D</given-names> <surname>Cowan</surname></string-name></person-group>. <article-title>Excitatory and inhibitory interactions in localized populations of model neurons</article-title>. <source>Biophysical journal</source>, <volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>24</lpage>, <year>1972</year>.</mixed-citation></ref>
<ref id="c90"><label>[90]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Hugh R</given-names> <surname>Wilson</surname></string-name> and <string-name><given-names>Jack D</given-names> <surname>Cowan</surname></string-name></person-group>. <article-title>A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue</article-title>. <source>Kybernetik</source>, <volume>13</volume>(<issue>2</issue>):<fpage>55</fpage>‚Äì<lpage>80</lpage>, <year>1973</year>.</mixed-citation></ref>
<ref id="c91"><label>[91]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andreas</given-names> <surname>Daffertshofer</surname></string-name> and <string-name><given-names>Bernadette CM</given-names> <surname>van Wijk</surname></string-name></person-group>. <article-title>On the influence of amplitude on the connectivity between phases</article-title>. <source>Frontiers in neuroinformatics</source>, <volume>5</volume>:<fpage>6</fpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c92"><label>[92]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ben H</given-names> <surname>Jansen</surname></string-name> and <string-name><given-names>Vincent G</given-names> <surname>Rit</surname></string-name></person-group>. <article-title>Electroencephalogram and visual evoked potential generation in a mathematical model of coupled cortical columns</article-title>. <source>Biological cybernetics</source>, <volume>73</volume>(<issue>4</issue>):<fpage>357</fpage>‚Äì<lpage>366</lpage>, <year>1995</year>.</mixed-citation></ref>
<ref id="c93"><label>[93]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Olivier</given-names> <surname>David</surname></string-name>, <string-name><given-names>Stefan J.</given-names> <surname>Kiebel</surname></string-name>, <string-name><given-names>Lee M.</given-names> <surname>Harrison</surname></string-name>, <string-name><given-names>J√©r√©mie</given-names> <surname>Mattout</surname></string-name>, <string-name><given-names>James M.</given-names> <surname>Kilner</surname></string-name>, and <string-name><given-names>Karl J.</given-names> <surname>Friston</surname></string-name></person-group>. <article-title>Dynamic causal modeling of evoked responses in eeg and meg</article-title>. <source>NeuroImage</source>, <volume>30</volume>(<issue>4</issue>):<fpage>1255</fpage> ‚Äì <lpage>1272</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c94"><label>[94]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.J.</given-names> <surname>Moran</surname></string-name>, <string-name><given-names>S.J.</given-names> <surname>Kiebel</surname></string-name>, <string-name><given-names>K.E.</given-names> <surname>Stephan</surname></string-name>, <string-name><given-names>R.B.</given-names> <surname>Reilly</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Daunizeau</surname></string-name>, and <string-name><given-names>K.J.</given-names> <surname>Friston</surname></string-name></person-group>. <article-title>A neural mass model of spectral responses in electrophysiology</article-title>. <source>NeuroImage</source>, <volume>37</volume>(<issue>3</issue>):<fpage>706</fpage> ‚Äì <lpage>720</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c95"><label>[95]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Fabrice</given-names> <surname>Wendling</surname></string-name>, <string-name><given-names>Fabrice</given-names> <surname>Bartolomei</surname></string-name>, <string-name><given-names>Jean-Jcques</given-names> <surname>Bellanger</surname></string-name>, and <string-name><given-names>Patrick</given-names> <surname>Chauvel</surname></string-name></person-group>. <article-title>Interpretation of interdependencies in epileptic signals using a macroscopic physiological model of the eeg</article-title>. <source>Clinical neurophysiology</source>, <volume>112</volume>(<issue>7</issue>):<fpage>1201</fpage>‚Äì <lpage>1218</lpage>, <year>2001</year>.</mixed-citation></ref>
<ref id="c96"><label>[96]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Sheida</given-names> <surname>Kazemi</surname></string-name> and <string-name><given-names>Yousef</given-names> <surname>Jamali</surname></string-name></person-group>. <article-title>On the influence of input triggering on the dynamics of the jansen-rit oscillators network</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">2202.06634</pub-id>, <year>2022</year>.</mixed-citation></ref>
<ref id="c97"><label>[97]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, <string-name><given-names>Morten L</given-names> <surname>Kringelbach</surname></string-name>, <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>Petra</given-names> <surname>Ritter</surname></string-name></person-group>. <article-title>The dynamics of resting fluctuations in the brain: metastability and its dynamical cortical core</article-title>. <source>Scientific reports</source>, <volume>7</volume>(<issue>1</issue>):<fpage>3095</fpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c98"><label>[98]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name> and <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Transmission time delays organize the brain network synchronization</article-title>. <source>Philosophical Transactions of the Royal Society A</source>, <volume>377</volume>(<issue>2153</issue>):<fpage>20180132</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c99"><label>[99]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jean-Didier</given-names> <surname>Lemar√©chal</surname></string-name>, <string-name><given-names>Maciej</given-names> <surname>Jedynak</surname></string-name>, <string-name><given-names>Lena</given-names> <surname>Trebaul</surname></string-name>, <string-name><given-names>Anthony</given-names> <surname>Boyer</surname></string-name>, <string-name><given-names>Fran√ßois</given-names> <surname>Tadel</surname></string-name>, <string-name><given-names>Manik</given-names> <surname>Bhattacharjee</surname></string-name>, <string-name><given-names>Pierre</given-names> <surname>Deman</surname></string-name>, <string-name><given-names>Viateur</given-names> <surname>Tuyisenge</surname></string-name>, <string-name><given-names>Leila</given-names> <surname>Ayoubian</surname></string-name>, <string-name><given-names>Etienne</given-names> <surname>Hugues</surname></string-name>, <etal>et al.</etal></person-group> <article-title>A brain atlas of axonal and synaptic delays based on modelling of cortico-cortical evoked potentials</article-title>. <source>Brain</source>, <volume>145</volume>(<issue>5</issue>):<fpage>1653</fpage>‚Äì <lpage>1667</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c100"><label>[100]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Pierpaolo</given-names> <surname>Sorrentino</surname></string-name>, <string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name>, <string-name><given-names>Maddalena</given-names> <surname>Sparaco</surname></string-name>, E <string-name><given-names>Troisi</given-names> <surname>Lopez</surname></string-name>, <string-name><given-names>Elisabetta</given-names> <surname>Signoriello</surname></string-name>, <string-name><given-names>Fabio</given-names> <surname>Baselice</surname></string-name>, <string-name><given-names>Simona</given-names> <surname>Bonavita</surname></string-name>, <string-name><given-names>Maria Agnese</given-names> <surname>Pirozzi</surname></string-name>, <string-name><given-names>Mario</given-names> <surname>Quarantelli</surname></string-name>, <string-name><given-names>Giuseppe</given-names> <surname>Sorrentino</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Whole-brain propagation delays in multiple sclerosis, a combined tractography-magnetoencephalography study</article-title>. <source>Journal of Neuroscience</source>, <volume>42</volume>(<issue>47</issue>):<fpage>8807</fpage>‚Äì<lpage>8816</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c101"><label>[101]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Agnes P</given-names> <surname>Funk</surname></string-name> and <string-name><given-names>Charles M</given-names> <surname>Epstein</surname></string-name></person-group>. <article-title>Natural rhythm: evidence for occult 40 hz gamma oscillation in resting motor cortex</article-title>. <source>Neuroscience letters</source>, <volume>371</volume>(<issue>2-3</issue>):<fpage>181</fpage>‚Äì<lpage>184</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c102"><label>[102]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gy√∂rgy</given-names> <surname>Buzs√°ki</surname></string-name> and <string-name><given-names>Xiao-Jing</given-names> <surname>Wang</surname></string-name></person-group>. <article-title>Mechanisms of gamma oscillations</article-title>. <source>Annual review of neuroscience</source>, <volume>35</volume>(<issue>1</issue>):<fpage>203</fpage>‚Äì<lpage>225</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c103"><label>[103]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alfons</given-names> <surname>Schnitzler</surname></string-name> and <string-name><given-names>Joachim</given-names> <surname>Gross</surname></string-name></person-group>. <article-title>Normal and pathological oscillatory communication in the brain</article-title>. <source>Nature reviews neuroscience</source>, <volume>6</volume>(<issue>4</issue>):<fpage>285</fpage>‚Äì<lpage>296</lpage>, <year>2005</year>.</mixed-citation></ref>
<ref id="c104"><label>[104]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Maria Luisa</given-names> <surname>Saggio</surname></string-name>, <string-name><given-names>Dakota</given-names> <surname>Crisp</surname></string-name>, <string-name><given-names>Jared M</given-names> <surname>Scott</surname></string-name>, <string-name><given-names>Philippa</given-names> <surname>Karoly</surname></string-name>, <string-name><given-names>Levin</given-names> <surname>Kuhlmann</surname></string-name>, <string-name><given-names>Mitsuyoshi</given-names> <surname>Nakatani</surname></string-name>, <string-name><given-names>Tomohiko</given-names> <surname>Murai</surname></string-name>, Matthias D√É1/4mpelmann, <string-name><given-names>Andreas</given-names> <surname>Schulze-Bonhage</surname></string-name>, <string-name><given-names>Akio</given-names> <surname>Ikeda</surname></string-name>, <string-name><given-names>Mark</given-names> <surname>Cook</surname></string-name>, <string-name><given-names>Stephen V</given-names> <surname>Gliske</surname></string-name>, <string-name><given-names>Jack</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>Christophe</given-names> <surname>Bernard</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>William C</given-names> <surname>Stacey</surname></string-name></person-group>. <article-title>A taxonomy of seizure dynamotypes</article-title>. <source>eLife</source>, <volume>9</volume>:<elocation-id>e55632</elocation-id>, <month>jul</month> <year>2020</year>. <pub-id pub-id-type="doi">10.7554/eLife.55632</pub-id></mixed-citation></ref>
<ref id="c105"><label>[105]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Herman</given-names> <surname>Haken</surname></string-name></person-group>. <article-title>Synergetics</article-title>. <source>Physics Bulletin</source>, <volume>28</volume>(<issue>9</issue>):<fpage>412</fpage>, <year>1977</year>.</mixed-citation></ref>
<ref id="c106"><label>[106]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name> and <string-name><given-names>Hermann</given-names> <surname>Haken</surname></string-name></person-group>. <article-title>A derivation of a macroscopic field theory of the brain from the quasi-microscopic neural dynamics</article-title>. <source>Physica D: Nonlinear Phenomena</source>, <volume>99</volume>(<issue>4</issue>):<fpage>503</fpage>‚Äì<lpage>526</lpage>, <year>1997</year>.</mixed-citation></ref>
<ref id="c107"><label>[107]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Timoth√©e</given-names> <surname>Proix</surname></string-name>, <string-name><given-names>Fabrice</given-names> <surname>Bartolomei</surname></string-name>, <string-name><given-names>Patrick</given-names> <surname>Chauvel</surname></string-name>, <string-name><given-names>Christophe</given-names> <surname>Bernard</surname></string-name>, and <string-name><given-names>Viktor K.</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Permittivity coupling across brain regions determines seizure recruitment in partial epilepsy</article-title>. <source>Journal of Neuroscience</source>, <volume>34</volume>(<issue>45</issue>):<fpage>15009</fpage>‚Äì <lpage>15021</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c108"><label>[108]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Anthony R.</given-names> <surname>McIntosh</surname></string-name> and <string-name><given-names>Viktor K.</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>The hidden repertoire of brain dynamics and dysfunction</article-title>. <source>Network Neuroscience</source>, <volume>3</volume>(<issue>4</issue>):<fpage>994</fpage>‚Äì<lpage>1008</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c109"><label>[109]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>√Åine</given-names> <surname>Byrne</surname></string-name>, <string-name><given-names>Reuben D</given-names> <surname>O‚ÄôDea</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Forrester</surname></string-name>, <string-name><given-names>James</given-names> <surname>Ross</surname></string-name>, and <string-name><given-names>Stephen</given-names> <surname>Coombes</surname></string-name></person-group>. <article-title>Next-generation neural mass and field modeling</article-title>. <source>Journal of neurophysiology</source>, <volume>123</volume>(<issue>2</issue>):<fpage>726</fpage>‚Äì<lpage>742</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c110"><label>[110]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Giovanni</given-names> <surname>Rabuffo</surname></string-name>, <string-name><given-names>Jan</given-names> <surname>Fousek</surname></string-name>, <string-name><given-names>Christophe</given-names> <surname>Bernard</surname></string-name>, and <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Neuronal cascades shape whole-brain functional dynamics at rest</article-title>. <source>ENeuro</source>, <volume>8</volume>(<issue>5</issue>), <year>2021</year>.</mixed-citation></ref>
<ref id="c111"><label>[111]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>M</given-names> <surname>Breyton</surname></string-name>, <string-name><given-names>J</given-names> <surname>Fousek</surname></string-name>, <string-name><given-names>G</given-names> <surname>Rabuffo</surname></string-name>, <string-name><given-names>P</given-names> <surname>Sorrentino</surname></string-name>, <string-name><given-names>L</given-names> <surname>Kusch</surname></string-name>, <string-name><given-names>M</given-names> <surname>Massimini</surname></string-name>, <string-name><given-names>S</given-names> <surname>Petkoski</surname></string-name>, and <string-name><given-names>V</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Spatiotemporal brain complexity quantifies consciousness outside of perturbation paradigms</article-title>. <source>bioRxiv</source>, <year>2023</year>.</mixed-citation></ref>
<ref id="c112"><label>[112]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jan</given-names> <surname>Fousek</surname></string-name>, <string-name><given-names>Giovanni</given-names> <surname>Rabuffo</surname></string-name>, <string-name><given-names>Kashyap</given-names> <surname>Gudibanda</surname></string-name>, <string-name><given-names>Hiba</given-names> <surname>Sheheitli</surname></string-name>, <string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name>, and <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Symmetry breaking organizes the brain‚Äôs resting state manifold</article-title>. <source>Scientific Reports</source>, <volume>14</volume>(<issue>1</issue>):<fpage>31970</fpage>, <year>2024</year>.</mixed-citation></ref>
<ref id="c113"><label>[113]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Karl J</given-names> <surname>Friston</surname></string-name>, <string-name><given-names>Andrea</given-names> <surname>Mechelli</surname></string-name>, <string-name><given-names>Robert</given-names> <surname>Turner</surname></string-name>, and <string-name><given-names>Cathy J</given-names> <surname>Price</surname></string-name></person-group>. <article-title>Nonlinear responses in fmri: the balloon model, volterra kernels, and other hemodynamics</article-title>. <source>NeuroImage</source>, <volume>12</volume>(<issue>4</issue>):<fpage>466</fpage>‚Äì<lpage>477</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c114"><label>[114]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Enrique C.A.</given-names> <surname>Hansen</surname></string-name>, <string-name><given-names>Demian</given-names> <surname>Battaglia</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Spiegler</surname></string-name>, <string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, and <string-name><given-names>Viktor K.</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Functional connectivity dynamics: Modeling the switching behavior of the resting state</article-title>. <source>NeuroImage</source>, <volume>105</volume>:<fpage>525</fpage> ‚Äì <lpage>535</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c115"><label>[115]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gustavo</given-names> <surname>Deco</surname></string-name>, <string-name><given-names>Morten L</given-names> <surname>Kringelbach</surname></string-name>, <string-name><given-names>Aurina</given-names> <surname>Arnatkeviciute</surname></string-name>, <string-name><given-names>Stuart</given-names> <surname>Oldham</surname></string-name>, <string-name><given-names>Kristina</given-names> <surname>Sabaroedin</surname></string-name>, <string-name><given-names>Nigel C</given-names> <surname>Rogasch</surname></string-name>, <string-name><given-names>Kevin M</given-names> <surname>Aquino</surname></string-name>, and <string-name><given-names>Alex</given-names> <surname>Fornito</surname></string-name></person-group>. <article-title>Dynamical consequences of regional heterogeneity in the brain‚Äôs transcriptional landscape</article-title>. <source>Science Advances</source>, <volume>7</volume>(<issue>29</issue>):<fpage>eabf4752</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c116"><label>[116]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Anita</given-names> <surname>Monteverdi</surname></string-name>, <string-name><given-names>Fulvia</given-names> <surname>Palesi</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Schirner</surname></string-name>, <string-name><given-names>Francesca</given-names> <surname>Argentino</surname></string-name>, <string-name><given-names>Mariateresa</given-names> <surname>Merante</surname></string-name>, <string-name><given-names>Alberto</given-names> <surname>Redolfi</surname></string-name>, <string-name><given-names>Francesca</given-names> <surname>Conca</surname></string-name>, <string-name><given-names>Laura</given-names> <surname>Mazzocchi</surname></string-name>, <string-name><given-names>Stefano F</given-names> <surname>Cappa</surname></string-name>, <string-name><given-names>Matteo Cotta</given-names> <surname>Ramusino</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Virtual brain simulations reveal network-specific parameters in neurodegenerative dementias</article-title>. <source>Frontiers in Aging Neuroscience</source>, <volume>15</volume>:<fpage>1204134</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c117"><label>[117]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Pedro Costa</given-names> <surname>Klein</surname></string-name>, <string-name><given-names>Ulrich</given-names> <surname>Ettinger</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Schirner</surname></string-name>, <string-name><given-names>Petra</given-names> <surname>Ritter</surname></string-name>, <string-name><given-names>Dan</given-names> <surname>Rujescu</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Falkai</surname></string-name>, <string-name><given-names>Nikolaos</given-names> <surname>Koutsouleris</surname></string-name>, <string-name><given-names>Lana</given-names> <surname>Kambeitz-Ilankovic</surname></string-name>, and <string-name><given-names>Joseph</given-names> <surname>Kambeitz</surname></string-name></person-group>. <article-title>Brain network simulations indicate effects of neuregulin-1 genotype on excitation-inhibition balance in cortical dynamics</article-title>. <source>Cerebral Cor-tex</source>, <volume>31</volume>(<issue>4</issue>):<fpage>2013</fpage>‚Äì<lpage>2025</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c118"><label>[118]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Pedro Costa</given-names> <surname>Klein</surname></string-name>, <string-name><given-names>Ulrich</given-names> <surname>Ettinger</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Schirner</surname></string-name>, <string-name><given-names>Petra</given-names> <surname>Ritter</surname></string-name>, <string-name><given-names>Dan</given-names> <surname>Rujescu</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Falkai</surname></string-name>, <string-name><given-names>Nikolaos</given-names> <surname>Koutsouleris</surname></string-name>, <string-name><given-names>Lana</given-names> <surname>Kambeitz-Ilankovic</surname></string-name>, and <string-name><given-names>Joseph</given-names> <surname>Kambeitz</surname></string-name></person-group>. <article-title>Brain network simulations indicate effects of neuregulin-1 genotype on excitation-inhibition balance in cortical dynamics</article-title>. <source>Cerebral Cortex</source>, <volume>31</volume>(<issue>4</issue>):<fpage>2013</fpage>‚Äì<lpage>2025</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c119"><label>[119]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Matthew F</given-names> <surname>Glasser</surname></string-name> and <string-name><given-names>David C</given-names> <surname>Van Essen</surname></string-name></person-group>. <article-title>Mapping human cortical areas in vivo based on myelin content as revealed by t1-and t2-weighted mri</article-title>. <source>Journal of neuroscience</source>, <volume>31</volume>(<issue>32</issue>):<fpage>11597</fpage>‚Äì<lpage>11616</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c120"><label>[120]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Klaas Enno</given-names> <surname>Stephan</surname></string-name>, <string-name><given-names>Nikolaus</given-names> <surname>Weiskopf</surname></string-name>, <string-name><given-names>Peter M</given-names> <surname>Drysdale</surname></string-name>, <string-name><given-names>Peter A</given-names> <surname>Robinson</surname></string-name>, and <string-name><given-names>Karl J</given-names> <surname>Friston</surname></string-name></person-group>. <article-title>Comparing hemodynamic models with dcm</article-title>. <source>Neuroimage</source>, <volume>38</volume>(<issue>3</issue>):<fpage>387</fpage>‚Äì<lpage>401</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c121"><label>[121]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Klaas Enno</given-names> <surname>Stephan</surname></string-name>, <string-name><given-names>Lars</given-names> <surname>Kasper</surname></string-name>, <string-name><given-names>Lee M</given-names> <surname>Harrison</surname></string-name>, <string-name><given-names>Jean</given-names> <surname>Daunizeau</surname></string-name>, <string-name><given-names>Hanneke EM</given-names> <surname>den Ouden</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Breakspear</surname></string-name>, and <string-name><given-names>Karl J</given-names> <surname>Friston</surname></string-name></person-group>. <article-title>Nonlinear dynamic causal models for fmri</article-title>. <source>Neuroimage</source>, <volume>42</volume>(<issue>2</issue>):<fpage>649</fpage>‚Äì<lpage>662</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c122"><label>[122]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Rens</given-names> <surname>van de Schoot</surname></string-name>, <string-name><given-names>Sarah</given-names> <surname>Depaoli</surname></string-name>, <string-name><given-names>Ruth</given-names> <surname>King</surname></string-name>, <string-name><given-names>Bianca</given-names> <surname>Kramer</surname></string-name>, <string-name><given-names>Kaspar</given-names> <surname>M√§rtens</surname></string-name>, <string-name><given-names>Mahlet G</given-names> <surname>Tadesse</surname></string-name>, <string-name><given-names>Marina</given-names> <surname>Vannucci</surname></string-name>, <string-name><given-names>Andrew</given-names> <surname>Gelman</surname></string-name>, <string-name><given-names>Duco</given-names> <surname>Veen</surname></string-name>, <string-name><given-names>Joukje</given-names> <surname>Willemsen</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Bayesian statistics and modelling</article-title>. <source>Nature Reviews Methods Primers</source>, <volume>1</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>26</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c123"><label>[123]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jayant</given-names> <surname>Jha</surname></string-name>, <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Anirudh Nihalani</given-names> <surname>Vattikonda</surname></string-name>, <string-name><given-names>Huifang</given-names> <surname>Wang</surname></string-name>, and <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Fully bayesian estimation of virtual brain parameters with self-tuning hamiltonian monte carlo</article-title>. <source>Machine Learning: Science and Technology</source>, <volume>3</volume>(<issue>3</issue>):<fpage>035016</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c124"><label>[124]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>George</given-names> <surname>Papamakarios</surname></string-name>, <string-name><given-names>David</given-names> <surname>Sterratt</surname></string-name>, and <string-name><given-names>Iain</given-names> <surname>Murray</surname></string-name></person-group>. <article-title>Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows</article-title>. In <conf-name>The 22nd International Conference on Artificial Intelligence and Statistics</conf-name>, pages <fpage>837</fpage>‚Äì<lpage>848</lpage>. <publisher-name>PMLR</publisher-name>, <year>2019</year>.</mixed-citation></ref>
<ref id="c125"><label>[125]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>David</given-names> <surname>Greenberg</surname></string-name>, <string-name><given-names>Marcel</given-names> <surname>Nonnenmacher</surname></string-name>, and <string-name><given-names>Jakob</given-names> <surname>Macke</surname></string-name></person-group>. <article-title>Automatic posterior transformation for likelihood-free inference</article-title>. In <conf-name>International Conference on Machine Learning</conf-name>, pages <fpage>2404</fpage>‚Äì<lpage>2414</lpage>. <publisher-name>PMLR</publisher-name>, <year>2019</year>.</mixed-citation></ref>
<ref id="c126"><label>[126]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Johann</given-names> <surname>Brehmer</surname></string-name>, <string-name><given-names>Gilles</given-names> <surname>Louppe</surname></string-name>, <string-name><given-names>Juan</given-names> <surname>Pavez</surname></string-name>, and <string-name><given-names>Kyle</given-names> <surname>Cranmer</surname></string-name></person-group>. <article-title>Mining gold from implicit models to improve likelihood-free inference</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>10</issue>):<fpage>5242</fpage>‚Äì<lpage>5249</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c127"><label>[127]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>George</given-names> <surname>Papamakarios</surname></string-name> and <string-name><given-names>Iain</given-names> <surname>Murray</surname></string-name></person-group>. <article-title>Fast Œµ-free inference of simulation models with bayesian conditional density estimation</article-title>. In <conf-name>Advances in Neural Information Processing Systems</conf-name>, pages <fpage>1028</fpage>‚Äì<lpage>1036</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c128"><label>[128]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Jan-Matthis</given-names> <surname>Lueckmann</surname></string-name>, <string-name><given-names>Jan</given-names> <surname>Boelts</surname></string-name>, <string-name><given-names>David</given-names> <surname>Greenberg</surname></string-name>, <string-name><given-names>Pedro</given-names> <surname>Goncalves</surname></string-name>, and <string-name><given-names>Jakob</given-names> <surname>Macke</surname></string-name></person-group>. <article-title>Benchmarking simulation-based inference</article-title>. In <conf-name>International Conference on Artificial Intelligence and Statistics</conf-name>, pages <fpage>343</fpage>‚Äì<lpage>351</lpage>. <publisher-name>PMLR</publisher-name>, <year>2021</year>.</mixed-citation></ref>
<ref id="c129"><label>[129]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Qiao</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Jiaze</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>Rui</given-names> <surname>Jiang</surname></string-name>, and <string-name><given-names>Wing Hung</given-names> <surname>Wong</surname></string-name></person-group>. <article-title>Density estimation using deep generative neural networks</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>15</issue>):<fpage>e2101344118</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c130"><label>[130]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Danilo</given-names> <surname>Rezende</surname></string-name> and <string-name><given-names>Shakir</given-names> <surname>Mohamed</surname></string-name></person-group>. <article-title>Variational inference with normalizing flows</article-title>. In <conf-name>International conference on machine learning (ICML)</conf-name>, pages <fpage>1530</fpage>‚Äì 1538. PMLR, <year>2015</year>.</mixed-citation></ref>
<ref id="c131"><label>[131]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>George</given-names> <surname>Papamakarios</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Nalisnick</surname></string-name>, <string-name><given-names>Danilo Jimenez</given-names> <surname>Rezende</surname></string-name>, <string-name><given-names>Shakir</given-names> <surname>Mohamed</surname></string-name>, and <string-name><given-names>Balaji</given-names> <surname>Lakshminarayanan</surname></string-name></person-group>. <article-title>Normalizing flows for probabilistic modeling and inference</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">1912.02762</pub-id>, <year>2019</year>.</mixed-citation></ref>
<ref id="c132"><label>[132]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Ivan</given-names> <surname>Kobyzev</surname></string-name>, <string-name><given-names>Simon</given-names> <surname>Prince</surname></string-name>, and <string-name><given-names>Marcus</given-names> <surname>Brubaker</surname></string-name></person-group>. <article-title>Normalizing flows: An introduction and review of current methods</article-title>. <conf-name>IEEE Transactions on Pattern Analysis and Machine Intelligence</conf-name>, <year>2020</year>.</mixed-citation></ref>
<ref id="c133"><label>[133]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.M.</given-names> <surname>Bates</surname></string-name> and <string-name><given-names>D.G.</given-names> <surname>Watts</surname></string-name></person-group>. <article-title>Relative curvature measures of nonlinearity</article-title>. <source>J R Stat Soc Ser B (Methodological)</source>, <volume>42</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>25</lpage>, <year>1980</year>.</mixed-citation></ref>
<ref id="c134"><label>[134]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alvaro</given-names> <surname>Tejero-Cantero</surname></string-name>, <string-name><given-names>Jan</given-names> <surname>Boelts</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Deistler</surname></string-name>, <string-name><given-names>Jan-Matthis</given-names> <surname>Lueckmann</surname></string-name>, <string-name><given-names>Conor</given-names> <surname>Durkan</surname></string-name>, Pedro J. Gon√É¬ßalves, <string-name><given-names>David S.</given-names> <surname>Greenberg</surname></string-name>, and <string-name><given-names>Jakob H.</given-names> <surname>Macke</surname></string-name></person-group>. <article-title>sbi: A toolkit for simulation-based inference</article-title>. <source>Journal of Open Source Software</source>, <volume>5</volume>(<issue>52</issue>):<fpage>2505</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c135"><label>[135]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Michael</given-names> <surname>Deistler</surname></string-name>, <string-name><given-names>Jakob H</given-names> <surname>Macke</surname></string-name></person-group>, and Pedro J Gon√ßalves. <article-title>Disparate energy consumption despite similar network activity</article-title>. <source>bioRxiv</source>, <year>2021</year>.</mixed-citation></ref>
<ref id="c136"><label>[136]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Paul G</given-names> <surname>Constantine</surname></string-name></person-group>. <source>Active subspaces: Emerging ideas for dimension reduction in parameter studies</source>. <publisher-name>SIAM</publisher-name>, <year>2015</year>.</mixed-citation></ref>
<ref id="c137"><label>[137]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Caglar</given-names> <surname>Cakan</surname></string-name>, <string-name><given-names>Nikola</given-names> <surname>Jajcay</surname></string-name>, and <string-name><given-names>Klaus</given-names> <surname>Obermayer</surname></string-name></person-group>. <article-title>neurolib: A simulation framework for whole-brain neural mass modeling</article-title>. <source>Cognitive Computation</source>, <month>Oct</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c138"><label>[138]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Marcel</given-names> <surname>Stimberg</surname></string-name>, <string-name><given-names>Romain</given-names> <surname>Brette</surname></string-name>, and <string-name><given-names>Dan FM</given-names> <surname>Goodman</surname></string-name></person-group>. <article-title>Brian 2, an intuitive and efficient neural simulator</article-title>. <source>eLife</source>, <volume>8</volume>:<elocation-id>e47314</elocation-id>, <year>2019</year>. <pub-id pub-id-type="doi">10.7554/eLife.47314</pub-id></mixed-citation></ref>
<ref id="c139"><label>[139]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Chaoming</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Tianqiu</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Xiaoyu</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Sichao</given-names> <surname>He</surname></string-name>, <string-name><given-names>Shangyang</given-names> <surname>Li</surname></string-name>, and <string-name><given-names>Si</given-names> <surname>Wu</surname></string-name></person-group>. <article-title>Brainpy, a flexible, integrative, efficient, and extensible framework for general-purpose brain dynamics programming</article-title>. <source>eLife</source>, <volume>12</volume>:<elocation-id>e86365</elocation-id>, <year>2023</year>. <pub-id pub-id-type="doi">10.7554/eLife.86365</pub-id></mixed-citation></ref>
<ref id="c140"><label>[140]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mar√≠lia</given-names> <surname>Barandas</surname></string-name>, <string-name><given-names>Duarte</given-names> <surname>Folgado</surname></string-name>, <string-name><given-names>Let√≠cia</given-names> <surname>Fernandes</surname></string-name>, <string-name><given-names>Sara</given-names> <surname>Santos</surname></string-name>, <string-name><given-names>Mariana</given-names> <surname>Abreu</surname></string-name>, <string-name><given-names>Patr√≠cia</given-names> <surname>Bota</surname></string-name>, <string-name><given-names>Hui</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Tanja</given-names> <surname>Schultz</surname></string-name>, and <string-name><given-names>Hugo</given-names> <surname>Gamboa</surname></string-name></person-group>. <article-title>Tsfel: Time series feature extraction library</article-title>. <source>SoftwareX</source>, <volume>11</volume>:<fpage>100456</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c141"><label>[141]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Oliver M</given-names> <surname>Cliff</surname></string-name>, <string-name><given-names>Annie G</given-names> <surname>Bryant</surname></string-name>, <string-name><given-names>Joseph T</given-names> <surname>Lizier</surname></string-name>, <string-name><given-names>Naotsugu</given-names> <surname>Tsuchiya</surname></string-name>, and <string-name><given-names>Ben D</given-names> <surname>Fulcher</surname></string-name></person-group>. <article-title>Unifying pairwise interactions in complex dynamics</article-title>. <source>Nature Computational Science</source>, <volume>3</volume>(<issue>10</issue>):<fpage>883</fpage>‚Äì<lpage>893</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c142"><label>[142]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ben D</given-names> <surname>Fulcher</surname></string-name> and <string-name><given-names>Nick S</given-names> <surname>Jones</surname></string-name></person-group>. <article-title>hctsa: A computational framework for automated time-series phenotyping using massive feature extraction</article-title>. <source>Cell systems</source>, <volume>5</volume>(<issue>5</issue>):<fpage>527</fpage>‚Äì<lpage>531</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c143"><label>[143]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F.</given-names> <surname>Pedregosa</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Varoquaux</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Michel</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Thirion</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Grisel</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Blondel</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Prettenhofer</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Weiss</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Dubourg</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Vanderplas</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Passos</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Cournapeau</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Brucher</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Perrot</surname></string-name>, and <string-name><given-names>E.</given-names> <surname>Duchesnay</surname></string-name></person-group>. <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>Journal of Machine Learning Research</source>, <volume>12</volume>:<fpage>2825</fpage>‚Äì<lpage>2830</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c144"><label>[144]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Carl H</given-names> <surname>Lubba</surname></string-name>, <string-name><given-names>Sarab S</given-names> <surname>Sethi</surname></string-name>, <string-name><given-names>Philip</given-names> <surname>Knaute</surname></string-name>, <string-name><given-names>Simon R</given-names> <surname>Schultz</surname></string-name>, <string-name><given-names>Ben D</given-names> <surname>Fulcher</surname></string-name>, and <string-name><given-names>Nick S</given-names> <surname>Jones</surname></string-name></person-group>. <article-title>catch22: Canonical time-series characteristics: Selected through highly comparative time-series analysis</article-title>. <source>Data Mining and Knowledge Discovery</source>, <volume>33</volume>(<issue>6</issue>):<fpage>1821</fpage>‚Äì<lpage>1852</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c145"><label>[145]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Paul</given-names> <surname>Triebkorn</surname></string-name>, <string-name><given-names>Leon</given-names> <surname>Stefanovski</surname></string-name>, <string-name><given-names>Kiret</given-names> <surname>Dhindsa</surname></string-name>, <string-name><given-names>Margarita-Arimatea</given-names> <surname>Diaz-Cortes</surname></string-name>, <string-name><given-names>Patrik</given-names> <surname>Bey</surname></string-name>, <string-name><given-names>Konstantin</given-names> <surname>B√ºlau</surname></string-name>, <string-name><given-names>Roopa</given-names> <surname>Pai</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Spiegler</surname></string-name>, <string-name><given-names>Ana</given-names> <surname>Solodkin</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Brain simulation augments machine-learning‚Äì based classification of dementia</article-title>. <source>Alzheimer‚Äôs &amp; Dementia: Translational Research &amp; Clinical Interventions</source>, <volume>8</volume>(<issue>1</issue>):<fpage>e12303</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c146"><label>[146]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Leon</given-names> <surname>Stefanovski</surname></string-name>, <string-name><given-names>Paul</given-names> <surname>Triebkorn</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Spiegler</surname></string-name>, <string-name><given-names>Margarita-Arimatea</given-names> <surname>Diaz-Cortes</surname></string-name>, <string-name><given-names>Ana</given-names> <surname>Solodkin</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name>, <string-name><given-names>Anthony Randal</given-names> <surname>McIntosh</surname></string-name>, <string-name><given-names>Petra</given-names> <surname>Ritter</surname></string-name>, and <collab>Alzheimer‚Äôs Disease Neuroimaging Initiative</collab></person-group>. <article-title>Linking molecular pathways and large-scale computational modeling to assess candidate disease mechanisms and pharmacodynamics in alzheimer‚Äôs disease</article-title>. <source>Frontiers in computational neuroscience</source>, <volume>13</volume>:<fpage>54</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c147"><label>[147]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mark A.</given-names> <surname>Beaumont</surname></string-name>, <string-name><given-names>Wenyang</given-names> <surname>Zhang</surname></string-name>, and <string-name><given-names>David J.</given-names> <surname>Balding</surname></string-name></person-group>. <article-title>Approximate bayesian computation in population genetics</article-title>. <source>Genetics</source>, <volume>162</volume>(<issue>4</issue>):<fpage>2025</fpage>‚Äì<lpage>2035</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c148"><label>[148]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mark A</given-names> <surname>Beaumont</surname></string-name></person-group>. <article-title>Approximate bayesian computation in evolution and ecology</article-title>. <source>Annual review of ecology, evolution, and systematics</source>, <volume>41</volume>:<fpage>379</fpage>‚Äì<lpage>406</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c149"><label>[149]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>S.A.</given-names> <surname>Sisson</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Fan</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Beaumont</surname></string-name></person-group>. <source>Handbook of Approximate Bayesian Computation. Chapman &amp; Hall/CRC handbooks of modern statistical methods</source>. <publisher-name>CRC Press, Taylor &amp; Francis Group</publisher-name>, <year>2018</year>.</mixed-citation></ref>
<ref id="c150"><label>[150]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nina</given-names> <surname>Baldy</surname></string-name>, <string-name><given-names>Nicolas</given-names> <surname>Simon</surname></string-name>, <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name></person-group>. <article-title>Hierarchical bayesian pharmacometrics analysis of baclofen for alcohol use disorder</article-title>. <source>Machine Learning: Science and Technology</source>, <year>2023</year>.</mixed-citation></ref>
<ref id="c151"><label>[151]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Jan-Matthis</given-names> <surname>Lueckmann</surname></string-name>, <string-name><given-names>Giacomo</given-names> <surname>Bassetto</surname></string-name>, <string-name><given-names>Theofanis</given-names> <surname>Karaletsos</surname></string-name>, and <string-name><given-names>Jakob H</given-names> <surname>Macke</surname></string-name></person-group>. <article-title>Likelihood-free inference with emulator networks</article-title>. In <conf-name>Symposium on Advances in Approximate Bayesian Inference</conf-name>, pages <fpage>32</fpage>‚Äì<lpage>53</lpage>. <publisher-name>PMLR</publisher-name>, <year>2019</year>.</mixed-citation></ref>
<ref id="c152"><label>[152]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Conor</given-names> <surname>Durkan</surname></string-name>, <string-name><given-names>Iain</given-names> <surname>Murray</surname></string-name>, and <string-name><given-names>George</given-names> <surname>Papamakarios</surname></string-name></person-group>. <article-title>On contrastive learning for likelihood-free inference</article-title>. In <conf-name>International Conference on Machine Learning</conf-name>, pages <fpage>2771</fpage>‚Äì<lpage>2781</lpage>. <publisher-name>PMLR</publisher-name>, <year>2020</year>.</mixed-citation></ref>
<ref id="c153"><label>[153]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>W.D.</given-names> <surname>Penny</surname></string-name>, <string-name><given-names>K.E.</given-names> <surname>Stephan</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Mechelli</surname></string-name>, and <string-name><given-names>K.J.</given-names> <surname>Friston</surname></string-name></person-group>. <article-title>Comparing dynamic causal models</article-title>. <source>NeuroImage</source>, <volume>22</volume>(<issue>3</issue>):<fpage>1157</fpage> ‚Äì <lpage>1172</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c154"><label>[154]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>W.D.</given-names> <surname>Penny</surname></string-name></person-group>. <article-title>Comparing dynamic causal models using aic, bic and free energy</article-title>. <source>NeuroImage</source>, <volume>59</volume>(<issue>1</issue>):<fpage>319</fpage> ‚Äì <lpage>330</lpage>, <year>2012</year>. Neuroergonomics: The human brain in action and at work.</mixed-citation></ref>
<ref id="c155"><label>[155]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nina</given-names> <surname>Baldy</surname></string-name>, <string-name><given-names>Marmaduke</given-names> <surname>Woodman</surname></string-name>, <string-name><given-names>Viktor K</given-names> <surname>Jirsa</surname></string-name>, and <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name></person-group>. <article-title>Dynamic causal modelling in probabilistic programming languages</article-title>. <source>Journal of the Royal Society Interface</source>, <volume>22</volume>(<issue>227</issue>):<fpage>20240880</fpage>, <year>2025</year>.</mixed-citation></ref>
<ref id="c156"><label>[156]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Durk P</given-names> <surname>Kingma</surname></string-name>, <string-name><given-names>Shakir</given-names> <surname>Mohamed</surname></string-name>, <string-name><given-names>Danilo Jimenez</given-names> <surname>Rezende</surname></string-name>, and <string-name><given-names>Max</given-names> <surname>Welling</surname></string-name></person-group>. <article-title>Semi-supervised learning with deep generative models</article-title>. <source>Advances in neural information processing systems</source>, <volume>27</volume>, <year>2014</year>.</mixed-citation></ref>
<ref id="c157"><label>[157]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Carl</given-names> <surname>Doersch</surname></string-name></person-group>. <article-title>Tutorial on variational autoencoders</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">1606.05908</pub-id>, <year>2016</year>.</mixed-citation></ref>
<ref id="c158"><label>[158]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Viktor</given-names> <surname>Sip</surname></string-name>, <string-name><given-names>Meysam</given-names> <surname>Hashemi</surname></string-name>, <string-name><given-names>Timo</given-names> <surname>Dickscheid</surname></string-name>, <string-name><given-names>Katrin</given-names> <surname>Amunts</surname></string-name>, <string-name><given-names>Spase</given-names> <surname>Petkoski</surname></string-name>, and <string-name><given-names>Viktor</given-names> <surname>Jirsa</surname></string-name></person-group>. <article-title>Characterization of regional differences in restingstate fmri with a data-driven network model of brain dynamics</article-title>. <source>Science Advances</source>, <volume>9</volume>(<issue>11</issue>):<fpage>eabq7547</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c159"><label>[159]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ian</given-names> <surname>Goodfellow</surname></string-name>, <string-name><given-names>Jean</given-names> <surname>Pouget-Abadie</surname></string-name>, <string-name><given-names>Mehdi</given-names> <surname>Mirza</surname></string-name>, <string-name><given-names>Bing</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>David</given-names> <surname>WardeFarley</surname></string-name>, <string-name><given-names>Sherjil</given-names> <surname>Ozair</surname></string-name>, <string-name><given-names>Aaron</given-names> <surname>Courville</surname></string-name>, and <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name></person-group>. <article-title>Generative adversarial nets</article-title>. <source>Advances in neural information processing systems</source>, <volume>27</volume>, <year>2014</year>.</mixed-citation></ref>
<ref id="c160"><label>[160]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Antonia</given-names> <surname>Creswell</surname></string-name>, <string-name><given-names>Tom</given-names> <surname>White</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Dumoulin</surname></string-name>, <string-name><given-names>Kai</given-names> <surname>Arulkumaran</surname></string-name>, <string-name><given-names>Biswa</given-names> <surname>Sengupta</surname></string-name>, and <string-name><given-names>Anil A</given-names> <surname>Bharath</surname></string-name></person-group>. <article-title>Generative adversarial networks: An overview</article-title>. <source>IEEE signal processing magazine</source>, <volume>35</volume>(<issue>1</issue>):<fpage>53</fpage>‚Äì<lpage>65</lpage>, <year>2018</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106194.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Fornito</surname>
<given-names>Alex</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Monash University</institution>
</institution-wrap>
<city>Clayton</city>
<country>Australia</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This paper presents a <bold>valuable</bold> software package, named &quot;Virtual Brain Inference&quot; (VBI), that enables faster and more efficient inference of parameters in dynamical system models of whole-brain activity, grounded in artificial network networks for Bayesian statistical inference. The authors have provided <bold>convincing</bold> evidence, across several case studies, for the utility and validity of the methods using simulated data from several commonly used models, but more thorough benchmarking could be used to demonstrate the practical utility of the toolkit. This work will be of interest to computational neuroscientists interested in modelling large-scale brain dynamics.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106194.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This work provides a new Python toolkit for combining generative modeling of neural dynamics and inversion methods to infer likely model parameters that explain empirical neuroimaging data. The authors provided tests to show the toolkit's broad applicability, accuracy, and robustness; hence, it will be very useful for people interested in using computational approaches to better understand the brain.</p>
<p>Strengths:</p>
<p>The work's primary strength is the tool's integrative nature, which seamlessly combines forward modelling with backward inference. This is important as available tools in the literature can only do one and not the other, which limits their accessibility to neuroscientists with limited computational expertise. Another strength of the paper is the demonstration of how the tool can be applied to a broad range of computational models popularly used in the field to interrogate diverse neuroimaging data, ensuring that the methodology is not optimal to only one model. Moreover, through extensive in-silico testing, the work provided evidence that the tool can accurately infer ground-truth parameters even in the presence of noise, which is important to ensure results from future hypothesis testing are meaningful.</p>
<p>Weaknesses</p>
<p>The paper still lacks appropriate quantitative benchmarking relative to non-Bayesian-based inference tools, especially with respect to performance accuracy and computational complexity and efficiency. Without this benchmarking, it is difficult to fully comprehend the power of the software or its ability to be extended to contexts beyond large-scale computational brain modelling.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106194.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Whole-brain network modeling is a common type of dynamical systems-based method to create individualized models of brain activity incorporating subject-specific structural connectome inferred from diffusion imaging data. This type of model has often been used to infer biophysical parameters of the individual brain that cannot be directly measured using neuroimaging but may be relevant to specific cognitive functions or diseases. Here, Ziaeemehr et al introduce a new toolkit, named &quot;Virtual Brain Inference&quot; (VBI), offering a new computational approach for estimating these parameters using Bayesian inference powered by artificial neural networks. The basic idea is to use simulated data, given known parameters, to train artificial neural networks to solve the inverse problem, namely, to infer the posterior distribution over the parameter space given data-derived features. The authors have demonstrated the utility of the toolkit using simulated data from several commonly used whole-brain network models in case studies.</p>
<p>Strength:</p>
<p>- Model inversion is an important problem in whole-brain network modeling. The toolkit presents a significant methodological step up from common practices, with the potential to broadly impact how the community infers model parameters.</p>
<p>
- Notably, the method allows the estimation of the posterior distribution of parameters instead of a point estimation, which provides information about the uncertainty of the estimation, which is generally lacking in existing methods.</p>
<p>
- The case studies were able to demonstrate the detection of degeneracy in the parameters, which is important. Degeneracy is quite common in this type of models. If not handled mindfully, they may lead to spurious or stable parameter estimation. Thus, the toolkit can potentially be used to improve feature selection or to simply indicate the uncertainty.</p>
<p>
- In principle, the posterior distribution can be directly computed given new data without doing any additional simulation, which could improve the efficiency of parameter inference on the artificial neural network is well-trained.</p>
<p>Weaknesses:</p>
<p>- The z-scores used to measure prediction error are generally between 1-3, which seems quite large to me. It would give readers a better sense of the utility of the method if comparisons to simpler methods, such as k-nearest neighbor methods, are provided in terms of accuracy.</p>
<p>
- A lot of simulations are required to train the posterior estimator, which is computationally more expensive than existing approaches. Inferring from Figure S1, at the required order of magnitudes of the number of simulations, the simulation time could range from days to years, depending on the hardware. The payoff is that once the estimator is well-trained, the parameter inversion will be very fast given new data. However, it is not clear to me how often such use cases would be encountered. It would be very helpful if the authors could provide a few more concrete examples of using trained models for hypothesis testing, e.g., in various disease conditions.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106194.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ziaeemehr</surname>
<given-names>Abolfazl</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4696-9947</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Woodman</surname>
<given-names>Marmaduke</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8410-4581</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Domide</surname>
<given-names>Lia</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4822-2046</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Petkoski</surname>
<given-names>Spase</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4540-6293</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Jirsa</surname>
<given-names>Viktor</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8251-8860</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Hashemi</surname>
<given-names>Meysam</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5289-9837</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors‚Äô response to the original reviews</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>This work provides a new Python toolkit for combining generative modeling of neural dynamics and inversion methods to infer likely model parameters that explain empirical neuroimaging data. The authors provided tests to show the toolkit's broad applicability and accuracy; hence, it will be very useful for people interested in using computational approaches to better understand the brain.</p>
<p>Strengths:</p>
<p>The work's primary strength is the tool's integrative nature, which seamlessly combines forward modelling with backward inference. This is important as available tools in the literature can only do one and not the other, which limits their accessibility to neuroscientists with limited computational expertise. Another strength of the paper is the demonstration of how the tool can be applied to a broad range of computational models popularly used in the field to interrogate diverse neuroimaging data, ensuring that the methodology is not optimal to only one model. Moreover, through extensive in-silico testing, the work provided evidence that the tool can accurately infer ground-truth parameters, which is important to ensure results from future hypothesis testing are meaningful.</p>
</disp-quote>
<p>We are happy to hear the positive feedback on our effort to provide an open-source and widely accessible tool for both fast forward simulations and flexible model inversion, applicable across popular models of large-scale brain dynamics.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>Although the tool itself is the main strength of the work, the paper lacked a thorough analysis of issues concerning robustness and benchmarking relative to existing tools.</p>
<p>The first issue is the robustness to the choice of features to be included in the objective function. This choice significantly affects the training and changes the results, as the authors even acknowledged themselves multiple times (e.g., Page 17 last sentence of first paragraph or Page 19 first sentence of second paragraph). This brings the question of whether the accurate results found in the various demonstrations are due to the biased selection of features (possibly from priors on what worked in previous works). The robustness of the neural estimator and the inference method to noise was also not demonstrated. This is important as most neuroimaging measurements are inherently noisy to various degrees.</p>
<p>The second issue is on benchmarking. Because the tool developed is, in principle, only a combination of existing tools specific to modeling or Bayesian inference, the work failed to provide a more compelling demonstration of its added value. This could have been demonstrated through appropriate benchmarking relative to existing methodologies, specifically in terms of accuracy and computational efficiency.</p>
</disp-quote>
<p>We fully agree with the reviewer that the VBI estimation heavily depends on the choice of data features, and this is the core of the inference procedure, not its weakness. We have demonstrated different scenarios showing how the informativeness of features (commonly used in the literature) results in varying uncertainty quantification. For instance, using summary statistics of functional connectivity (FC) and functional connectivity dynamics (FCD) matrices to estimate global coupling parameter leads to fast convergence; however, it is not sufficient to accurately estimate the whole-brain heterogeneous excitability parameter, which requires features such as statistical moments of time series. VBI provides a taxonomy of data features that users can employ to test their hypotheses. It is important to note that one major advantage of VBI is its ability to make estimation using a battery of data features, rather than relying on a limited set (such as only FC or FCD) as is often the case in the literature. In the revised version, we will elaborate further by presenting additional scenarios to demonstrate the robustness of the estimation. We will also evaluate the robustness of the neural density estimators to (dynamical/additive) noise.</p>
<p>More importantly, relative to benchmarking, we would like to draw attention to a key point regarding existing tools and methods. The literature often uses optimization for fitting whole-brain network models, and its limitations for reliable causal hypothesis testing have been pointed out in the Introduction/Discussion. As also noted by the reviewer under strengths, and to the best of our knowledge, there are no existing tools other than VBI that can scale and generalize to operate across whole-brain models for <italic>Bayesian model inversion</italic>. Previously, we developed Hamiltonian Monte Carlo (HMC) sampling for Epileptor model in epilepsy (Hashemi et al., 2020, Jha et al., 2022). This phenomenological model is very well-behaved in terms of numerical integration, gradient calculation, and dynamical system properties (Jirsa et al., 2014). However, this does not directly generalize to other models, particularly the Montbri√≥ model for resting-state, which exhibits bistability with noise driving transitions between states. As shown in Baldy et al., 2024, even at the level of a single neural mass model (i.e., one brain region), gradient-based HMC failed to capture such switching behaviour, particularly when only one state variable (membrane potential) was observed while the other (firing rate) was missing. Our attempts to use other methods (e.g., the second-derivative-based Laplace approximation used in Dynamic Causal Modeling) also failed, due to divergence in gradient calculation. Nevertheless, reparameterization techniques (Baldy et al., 2024) and hybrid algorithms (Gabri√© et al., 2022) could offer improvements, although this remains an open problem for these classes of computational models.</p>
<p>In sum, for oscillatory systems, it has been shown previously that SBI approach used in VBI substantially outperforms both gradient-based and gradient-free alternative methods (Gon√ßalves et al., 2020, Hashemi et al., 2023, Baldy et al., 2024). Importantly, for bistable systems with switching dynamics, gradient-based methods fail to converge, while gradient-free methods do not scale to the whole-brain level (Hashemi et al., 2020). Hence, the generalizability of VBI relies on the fact that neither the model nor the data features need to be differentiable. We will clarify this point in the revised version. Moreover, we will provide better explanations for some terms mentioned by the reviewer in Recommendations.</p>
<p>Hashemi, M., Vattikonda, A. N., Sip, V., Guye, M., Bartolomei, F., Woodman, M. M., &amp; Jirsa, V. K. (2020). The Bayesian Virtual Epileptic Patient: A probabilistic framework designed to infer the spatial map of epileptogenicity in a personalized large-scale brain model of epilepsy spread. NeuroImage, 217, 116839.</p>
<p>Jha, J., Hashemi, M., Vattikonda, A. N., Wang, H., &amp; Jirsa, V. (2022). Fully Bayesian estimation of virtual brain parameters with self-tuning Hamiltonian Monte Carlo. Machine Learning: Science and Technology, 3(3), 035016.</p>
<p>Jirsa, V. K., Stacey, W. C., Quilichini, P. P., Ivanov, A. I., &amp; Bernard, C. (2014). On the nature of seizure dynamics. Brain, 137(8), 2210-2230.</p>
<p>Baldy, N., Breyton, M., Woodman, M. M., Jirsa, V. K., &amp; Hashemi, M. (2024). Inference on the macroscopic dynamics of spiking neurons. Neural Computation, 36(10), 2030-2072.</p>
<p>Baldy, N., Woodman, M., Jirsa, V., &amp; Hashemi, M. (2024). Dynamic Causal Modeling in Probabilistic Programming Languages. bioRxiv, 2024-11.</p>
<p>Gabri√©, M., Rotskoff, G. M., &amp; Vanden-Eijnden, E. (2022). Adaptive Monte Carlo augmented with normalizing flows. Proceedings of the National Academy of Sciences, 119(10), e2109420119.</p>
<p>Gon√ßalves, P. J., Lueckmann, J. M., Deistler, M., Nonnenmacher, M., √ñcal, K., Bassetto, G., ... &amp; Macke, J. H. (2020). Training deep neural density estimators to identify mechanistic models of neural dynamics. elife, 9, e56261.</p>
<p>Hashemi, M., Vattikonda, A. N., Jha, J., Sip, V., Woodman, M. M., Bartolomei, F., &amp; Jirsa, V. K. (2023). Amortized Bayesian inference on generative dynamical network models of epilepsy using deep neural density estimators. Neural Networks, 163, 178-194.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>Whole-brain network modeling is a common type of dynamical systems-based method to create individualized models of brain activity incorporating subject-specific structural connectome inferred from diffusion imaging data. This type of model has often been used to infer biophysical parameters of the individual brain that cannot be directly measured using neuroimaging but may be relevant to specific cognitive functions or diseases. Here, Ziaeemehr et al introduce a new toolkit, named &quot;Virtual Brain Inference&quot; (VBI), offering a new computational approach for estimating these parameters using Bayesian inference powered by artificial neural networks. The basic idea is to use simulated data, given known parameters, to train artificial neural networks to solve the inverse problem, namely, to infer the posterior distribution over the parameter space given data-derived features. The authors have demonstrated the utility of the toolkit using simulated data from several commonly used whole-brain network models in case studies.</p>
<p>Strengths:</p>
<p>(1) Model inversion is an important problem in whole-brain network modeling. The toolkit presents a significant methodological step up from common practices, with the potential to broadly impact how the community infers model parameters.</p>
<p>(2) Notably, the method allows the estimation of the posterior distribution of parameters instead of a point estimation, which provides information about the uncertainty of the estimation, which is generally lacking in existing methods.</p>
<p>(3) The case studies were able to demonstrate the detection of degeneracy in the parameters, which is important. Degeneracy is quite common in this type of model. If not handled mindfully, they may lead to spurious or stable parameter estimation. Thus, the toolkit can potentially be used to improve feature selection or to simply indicate the uncertainty.</p>
<p>(4) In principle, the posterior distribution can be directly computed given new data without doing any additional simulation, which could improve the efficiency of parameter inference on the artificial neural network if well-trained.</p>
</disp-quote>
<p>We thank the reviewer for the careful consideration of important aspects of the VBI tool, such as uncertainty quantification, degeneracy detection, parallelization, and amortization strategy.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1) While the posterior estimator was trained with a large quantity of simulated data, the testing/validation is only demonstrated with a single case study (one point in parameter space) per model. This is not sufficient to demonstrate the method's accuracy and reliability, but only its feasibility. Demonstrating the accuracy and reliability of the posterior estimation in large test sets would inspire more confidence.</p>
<p>(2) The authors have only demonstrated validation of the method using simulated data, but not features derived from actual EEG/MEG or fMRI data. So, it is unclear if the posterior estimator, when applied to real data, would produce results as sensible as using simulated data. Human data can often look quite different from the simulated data, which may be considered out of distribution. Thus, the authors should consider using simulated test data with out-of-distribution parameters to validate the method and using real human data to demonstrate, e.g., the reliability of the method across sessions.</p>
<p>(3) The z-scores used to measure prediction error are generally between 1-3, which seems quite large to me. It would give readers a better sense of the utility of the method if comparisons to simpler methods, such as k-nearest neighbor methods, are provided in terms of accuracy.</p>
<p>(4) A lot of simulations are required to train the posterior estimator, which seems much more than existing approaches. Inferring from Figure S1, at the required order of magnitudes of the number of simulations, the simulation time could range from days to years, depending on the hardware. Although once the estimator is well-trained, the parameter inverse given new data will be very fast, it is not clear to me how often such use cases would be encountered. Because the estimator is trained based on an individual connectome, it can only be used to do parameter inversion for the same subject. Typically, we only have one session of resting state data from each participant, while longitudinal resting state data where we can assume the structural connectome remains constant, is rare. Thus, the cost-efficiency and practical utility of training such a posterior estimator remains unclear.</p>
</disp-quote>
<p>We agree with the reviewer that it is necessary to show results on larger synthetic test sets, and we will elaborate further by presenting additional scenarios to demonstrate the robustness of the estimation. However, there are some points raised by the reviewer that we need to clarify.</p>
<p>The validation on empirical data was beyond the scope of this study, as it relates to model validation rather than the inversion algorithms. This is also because we aimed to avoid repetition, given that we have previously demonstrated model validation on empirical data using theses techniques, for invasive sEEG (Hashemi et al., 2023), MEG (Sorrentino et al., 2024), EEG (Angiolelli et al., 2025) and fMRI (Lavanga et al., 2024, Rabuffo et al., 2025). Note that if the features of the observed data are not included during training, VBI ignores them, as it requires an invertible mapping function between parameters and data features.</p>
<p>We have used z-scores and posterior shrinkage to measure prediction performance, as these are Bayesian metrics that take into account the variance of both prior and posterior rather than only the mean value or thresholding for ranking of the prediction used in k-NN or confusion matrix methods. This helps avoid biased accuracy estimation, for instance, if the mean posterior is close to the true value but there is no posterior shrinkage. Although shrinkage is bounded between 0 and 1, we agree that z-scores have no upper bound for such diagnostics.</p>
<p>Finally, the number of required simulations depends on the dimensionality of the parameter space and the informativeness of the data features. For instance, estimating a single global scaling parameter requires around 100 simulations, whereas estimating whole-brain heterogeneous parameters requires substantially more simulations. Nevertheless, we have provided fast simulations, and one key advantage of VBI is that simulations can be run in parallel (unlike MCMC sampling, which is more limited in this regard). Hence, with commonly accessible CPUs/GPUs, the fast simulations and parallelization capabilities of the VBI tool allow us to run on the order of 1 million simulations within 2‚Äì3 days on desktops, or in less than half a day on supercomputers at cohort level, rather than over several years! It has been previously shown that the SBI method used in VBI provides an order-of-magnitude faster inversion than HMC for whole-brain epilepsy spread (Hashemi et al., 2023). Moreover, after training, the amortized strategy is critical for enabling hypothesis testing within seconds to minutes. We agree that longitudinal resting-state data under the assumption of a constant structural connectome is rare; however, this strategy is essential in brain diseases such as epilepsy, where experimental hypothesis testing is prohibitive.</p>
<p>We will clarify these points and better explain some terms mentioned by the reviewer in the revised manuscript.</p>
<p>Hashemi, M., Vattikonda, A. N., Jha, J., Sip, V., Woodman, M. M., Bartolomei, F., &amp; Jirsa, V. K. (2023). Amortized Bayesian inference on generative dynamical network models of epilepsy using deep neural density estimators. Neural Networks, 163, 178-194.</p>
<p>Sorrentino, P., Pathak, A., Ziaeemehr, A., Lopez, E. T., Cipriano, L., Romano, A., ... &amp; Hashemi, M. (2024). The virtual multiple sclerosis patient. IScience, 27(7).</p>
<p>Angiolelli, M., Depannemaecker, D., Agouram, H., Regis, J., Carron, R., Woodman, M., ... &amp; Sorrentino, P. (2025). The virtual parkinsonian patient. npj Systems Biology and Applications, 11(1), 40.</p>
<p>Lavanga, M., Stumme, J., Yalcinkaya, B. H., Fousek, J., Jockwitz, C., Sheheitli, H., ... &amp; Jirsa, V. (2023). The virtual aging brain: Causal inference supports interhemispheric dedifferentiation in healthy aging. NeuroImage, 283, 120403.</p>
<p>Rabuffo, G., Lokossou, H. A., Li, Z., Ziaee-Mehr, A., Hashemi, M., Quilichini, P. P., ... &amp; Bernard, C. (2025). Mapping global brain reconfigurations following local targeted manipulations. Proceedings of the National Academy of Sciences, 122(16), e2405706122.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
</disp-quote>
<p>We appreciate the time and effort of the reviewers, and their insightful and constructive comments to improve the paper. We have now addressed the reviewers‚Äô comments in our revised manuscript and provide here below detailed explanations of the changes.</p>
<p>We have adapted the Wilson-Cowan model to follow the same brain network modeling notation as the other models (Fig. 3 in the main text and Figs. S2‚ÄìS4 in the supplementary materials). Additionally, we have included multiple figures in the supplementary material presenting extensive in-silico testing to demonstrate the accuracy and reliability of the estimations across different configurations, as well as the sensitivity to both additive and dynamical noise.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations for the authors):</bold></p>
<p>(1) There were some inaccurate statements throughout the text that need to be corrected.</p>
<p>a) In section 2.1, paragraph 1, the authors mentioned that they would describe network models corresponding to different types of neuroimaging recordings. This is inaccurate. The models were developed to approximate various aspects of the architecture of neural circuits. They were not developed per se to solely describe a specific neuroimaging modality.</p>
</disp-quote>
<p>Thank you for pointing this out. We agree that our phrasing in Section 2.1, paragraph 1, was not clear that the network models were developed to generate neural activity at the source level, and that a projection needs to be established to transform the simulated neural activity into empirically measurable quantities, such as BOLD fMRI, EEG, or MEG. We have revised the wording in the revised manuscript to clarify this point accordingly.</p>
<disp-quote content-type="editor-comment">
<p>b) The use of the term &quot;spatio-temporal data features&quot; is misleading as there are no true spatial features extracted.</p>
</disp-quote>
<p>We have clarified that:Following Hashemi et al., 2024, we use the term spatio-temporal data features to refer to both statistical and temporal features derived from time series. In contrast, we refer to the connectivity features extracted from FC/FCD matrices as functional data features. We would like to retain this term, as it is used consistently in the code.</p>
<disp-quote content-type="editor-comment">
<p>(2) The authors need to improve the model descriptions in Equations (1)-(10). Several variables/parameters were not explained, limiting the accessibility of the work to those without prior experience in computational modeling.</p>
</disp-quote>
<p>Thank you for pointing this out. In the revised manuscript, we have improved the model descriptions, all variables and parameters used in these equations.</p>
<disp-quote content-type="editor-comment">
<p>(3) Various things need further clarification and/or explanation:</p>
<p>a) There is a need to highlight that the models section only provides examples of one of the many possible variants of the models. For example, the Wilson-Cowan model described is not your typical and more popular cortico-cortical-based Wilson-Cowan model. This is important to ensure that the work reflects an accurate account of the literature, avoiding future references that the models presented are THE models.</p>
</disp-quote>
<p>This is a very important point. We have now highlighted that each model represents one of many possible variants. Moreover, we adapted the Wilson-Cowan model as a whole-brain network modeling approach to harmonize with all other models.</p>
<disp-quote content-type="editor-comment">
<p>b) In Figure 1, it is unclear where the empirical data come into play. The neural density estimator also sounds like a black box and needs further explanation (e.g., its architecture).</p>
</disp-quote>
<p>Thank you for the careful reading. This is correct. We have now clarified where the empirical data enters as input to the neural density estimator and have added further explanation in section 2.2.</p>
<disp-quote content-type="editor-comment">
<p>c) There is also a need to better explain what shrinkage means and what the z-score vs shrinkage implies.</p>
</disp-quote>
<p>We have elaborated on the definition of posterior z-score and shrinkage.</p>
<disp-quote content-type="editor-comment">
<p>d) It is unclear how the authors decided on the number of training samples to use.</p>
</disp-quote>
<p>There is no specific rule for determining the optimal number of simulations required for training. In general, the larger number of simulations, within the available computational budget, the better the posterior estimation is likely to be. In the case of synthetic data, we have monitored the z-score and posterior shrinkage to assess the quality and reliability of the inferred parameters.  This also critically depends on the parameter dimensionality. For instance, in estimating only global coupling parameter, a maximum of 300 simulations was used, demonstrating accurate estimation across models and different realizations (Fig S20), except for the Jansen-Rit model, where coupling did not induce a significant change in the intrinsic frequency of regional activity. We have now pointed this out in the discussion.</p>
<disp-quote content-type="editor-comment">
<p>e) In the Results section, paragraph 1, there is a need to clarify that &quot;ground truth&quot; is available because you simulate data using predefined parameters. In fact, these predefined parameters and how they were chosen to generate the observed data were never described in the text.</p>
</disp-quote>
<p>The &quot;ground truth&quot; is often chosen randomly within biologically plausible ranges, typically with some level of heterogeneity, and this has now been highlighted.</p>
<disp-quote content-type="editor-comment">
<p>f) Can the authors comment on why the median of the posterior distributions (e.g., in Figure 4E) is actually far off from the ground truth parameters? This is probably understandable in the Jansen-Ritt model due to complexity, but not obvious in the very low-dimensional Stuart-Landau oscillator model.</p>
</disp-quote>
<p>This can happen due to non-identifiability in high-dimensional settings. Figure 4E represents the posterior estimation using Jansen-Rit model with high-dimensional parameters. An accurate estimation close to the true values can be observed in the low-dimensional Stuart-Landau model, as shown in Figure 5.</p>
<disp-quote content-type="editor-comment">
<p>g) In Figure 7, the FC and FCD matrices look weird relative to those typically seen in other works.</p>
</disp-quote>
<p>We have updated Figure 7. To do the our best, we have followed the code and the parameters from the following paper Kong et al., Nat Commun 12, 6373 (2021), and the following repo <ext-link ext-link-type="uri" xlink:href="https://github.com/ThomasYeoLab/CBIG/blob/master/stable_projects/fMRI_dynamics/Kong2021_pMFM/examples/scripts/CBIG_pMFM_parameter_estimation_example.py">https://github.com/ThomasYeoLab/CBIG/blob/master/stable_projects/fMRI_dynamics/Kong2021_pMFM/examples/scripts/CBIG_pMFM_parameter_estimation_example.py</ext-link></p>
<p>We considered 300 iterations for optimizing the parameters, using CMA-ES method, and with window length of 60 sec, and TR=0.72 sec, yielding a 1118 √ó 1118 FCD matrix for each run. Nevertheless, some discrepancy can happen with the shown FC/FCD, due to convergence of the optimization process and other model parameters.</p>
<disp-quote content-type="editor-comment">
<p>h) In Figure 8, results for the J parameter are missing. Also, the BOLD signal time series of some regions in Figure 8B looks very weird, with some having very large deflections.</p>
</disp-quote>
<p>We have updated Figure 8. In this figure, the parameter J is not inferred; it is instead presented in the appendix (S18). Please note that the system is in a bistable regime. We have implemented the full Wong-Wang model (Deco, 2014, Journal of Neuroscience), by optimized external current and global coupling (using CMA-ES optimization) to maximize the fluidity of FCD, as those typically seen in other works:</p>
<fig id="sa3fig1">
<label>Author response image 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-106194-sa3-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>i) On page 14, the authors mentioned that they perform a PCA on the FC/FCD matrices. Can the authors explain this step further and what it specifically gives out, as this is something unusual in the generative model fitting literature?</p>
</disp-quote>
<p>Indeed, PCA is a widely used dimension reduction method in machine learning. Please note that in SBI, any dimensionality reduction technique, such as PCA, can be used, as long as it preserves information relevant to the target parameters.</p>
<disp-quote content-type="editor-comment">
<p>j) On page 3, what does ABC in ABC methods stand for?</p>
</disp-quote>
<p>ABC stands for Approximate Bayesian Computation, which is now spelled out in the text.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>Overall, I found the paper well-written. These are basically just minor comments:</p>
</disp-quote>
<p>We appreciate your positive feedback.</p>
<disp-quote content-type="editor-comment">
<p>(1) P3:</p>
<p>- Amortization requires more explanation for the neuroscience audience.</p>
<p>- What does ABC stand for?</p>
</disp-quote>
<p>We have elaborated on Amortization. ABC stands for Approximate Bayesian Computation, which is now spelled out in the text.</p>
<disp-quote content-type="editor-comment">
<p>(2) Section 2.1:</p>
<p>Should clarify the parcellation used</p>
</disp-quote>
<p>In section 2.1, we now mentioned that: ‚ÄúThe structural connectome was built with TVB-specific reconstruction pipeline using generally available neuroimaging software (Schirner et al., Neuroimage 2015)‚Äù.</p>
<disp-quote content-type="editor-comment">
<p>(3) P20: The method for sensitivity analysis (Figure 5F) is not clearly described.</p>
</disp-quote>
<p>We have now added a subsection in the Methods section to explain the sensitivity analysis.</p>
<disp-quote content-type="editor-comment">
<p>(4) P21: statement that 10k simulations took less than 1 min doesn't match info shown in Figure S1. Please clarify.</p>
</disp-quote>
<p>This is correct, as for the Epileptor model, the total integration time is less than 100 ms. Due to the model‚Äôs stable behavior with a large time step and the use of 10 CPU cores, all simulations were completed in less than a minute. Previously (Hashemi et al., 2023) it has been reported that each VEP run to simulate 100sec of whole-brain epileptic patterns takes only 0.003 s using a JIT compiler. The other models require more computational cost due to longer integration durations and smaller time steps. We have clarified this point.</p>
<disp-quote content-type="editor-comment">
<p>(5) P23-24: the distribution of FCDs also doesn't match well even if we don't consider element-wise correspondence. Please clarify.</p>
</disp-quote>
<p>This is correct, as we used summary statistics of the FCD, such as fluidity, and due to noise, each realization of the FCD matrix exhibits different element-wise correspondence. We have already mentioned this point.</p>
</body>
</sub-article>
</article>