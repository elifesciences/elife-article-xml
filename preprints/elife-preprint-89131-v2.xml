<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">89131</article-id>
<article-id pub-id-type="doi">10.7554/eLife.89131</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89131.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>When and why does motor preparation arise in recurrent neural network models of motor control?</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Schimel</surname>
<given-names>Marine</given-names>
</name>
<xref ref-type="corresp" rid="cor1">@</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kao</surname>
<given-names>Ta-Chu</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hennequin</surname>
<given-names>Guillaume</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Computational and Biological Learning Lab, Department of Engineering, University of Cambridge</institution>, Cambridge, <country>U.K.</country></aff>
<aff id="a2"><label>2</label><institution>Meta Reality Labs</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Diedrichsen</surname>
<given-names>Jörn</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Western University</institution>
</institution-wrap>
<city>London</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>de Lange</surname>
<given-names>Floris P</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Donders Institute for Brain, Cognition and Behaviour</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>@</label>Corresponding author (<email>mmcs3@cam.ac.uk</email>)</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-08-10">
<day>10</day>
<month>08</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-05-20">
<day>20</day>
<month>05</month>
<year>2024</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP89131</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-05-23">
<day>23</day>
<month>05</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-06-19">
<day>19</day>
<month>06</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.04.03.535429"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-08-10">
<day>10</day>
<month>08</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89131.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.89131.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.89131.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.89131.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.89131.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Schimel et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Schimel et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-89131-v2.pdf"/>
<abstract>
<title>Summary</title>
<p>During delayed ballistic reaches, motor areas consistently display movement-specific activity patterns prior to movement onset. It is unclear why these patterns arise: while they have been proposed to seed an initial neural state from which the movement unfolds, recent experiments have uncovered the presence and necessity of ongoing inputs during movement, which may lessen the need for careful initialization. Here, we modelled the motor cortex as an input-driven dynamical system, and we asked what the optimal way to control this system to perform fast delayed reaches is. We find that delay-period inputs consistently arise in an optimally controlled model of M1. By studying a variety of network architectures, we could dissect and predict the situations in which it is beneficial for a network to prepare. Finally, we show that optimal input-driven control of neural dynamics gives rise to multiple phases of preparation during reach sequences, providing a novel explanation for experimentally observed features of monkey M1 activity in double reaching.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>The revised manuscript includes a little bit more discussion and 4 additional supplementary figures.</p></fn>
</fn-group>
</notes>
</front>
<body>
<p>During the production of ballistic movements, the motor cortex is thought to operate as a dynamical system whose state trajectories trace out the appropriate motor commands for downstream effectors (<xref ref-type="bibr" rid="c42">Shenoy et al., 2013</xref>; <xref ref-type="bibr" rid="c32">Miri et al., 2017</xref>; <xref ref-type="bibr" rid="c38">Russo et al., 2018</xref>). The extent to which these cortical dynamics are controlled by exogenous inputs before and/or during movement is the subject of ongoing study.</p>
<p>On the one hand, several experimental and modelling studies point to a potential role for exogenous inputs in motor preparation. First, cortical state trajectories are empirically well described by a low-dimensional dynamical system evolving near-autonomously during movement (<xref ref-type="bibr" rid="c4">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="c34">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="c40">Schimel et al., 2022</xref>), such that there is a priori no reason to suspect that inputs are required for motor production. Rather, inputs would be required during preparation to bring the state of the cortical network into a suitable initial condition. This input-driven seeding process is corroborated by observations of movementspecific primary motor cortex (M1) activity arising well before movement initiation (<xref ref-type="bibr" rid="c25">Lara et al., 2018</xref>; <xref ref-type="bibr" rid="c23">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="c4">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="c30">Meirhaeghe et al., 2023</xref>; <xref rid="fig1" ref-type="fig">Figure 1A</xref>), and associated models demonstrate the critical role of preparatory inputs therein (<xref ref-type="bibr" rid="c49">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="c18">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kao et al., 2021</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Control is possible under different strategies.</title>
<p><bold>(A)</bold> Trial-averaged firing rate of two representative monkey M1 neurons, across 8 different movements, separately aligned to target onset (left) and movement onset (right). Neural activity starts separating across movements well before the animal starts moving. <bold>(B)</bold> Top: a RNN model of M1 dynamics receives external inputs <bold><italic>u</italic></bold>(<italic>t</italic>) from a higher-level controller, and outputs control signals for a biophysical two-jointed arm model. Inputs are optimized for the correct production of eight center-out reaches to targets regularly positioned around a circle. Bottom: firing rate of a representative neuron in the RNN model for each reach, under two extreme control strategies. In the first strategy (left, solid lines), the external inputs <bold><italic>u</italic></bold>(<italic>t</italic>) are optimized whilst being temporally confined to the preparatory period. In the second strategy (right, dashed lines), they are optimized whilst confined to the movement period. Although slight differences in hand kinematics can be seen (compare corresponding solid and dashed hand trajectories), both control policies lead to successful reaches. These introductory simulations are shown for illustration purposes; the particular choice of network connectivity and the way the control inputs were found are described in the Results section.</p></caption>
<graphic xlink:href="535429v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>On the other hand, recent studies in mice have shown that the motor cortex receives critical pattern-generating input from the thalamus during movement production (<xref ref-type="bibr" rid="c39">Sauerbrei et al., 2020</xref>), and recurrent neural network (RNN)-based modelling of the motor feed-back loop involved in reaching movements suggests that sensory feedback may also contribute significantly to the observed dynamics of M1 (<xref ref-type="bibr" rid="c19">Kalidindi et al., 2021</xref>). Moreover, most published network models of delayed reaches are able to perform the task just as well without preparatory inputs, i.e. with external inputs forcefully confined to the movement epoch – an illustratory example is shown in <xref rid="fig1" ref-type="fig">Figure 1B</xref>. Thus, the relative contributions of preparatory vs. movement-epoch inputs to the dynamics implemented by M1 (potentially as part of a broader set of areas) remain unclear.</p>
<p>In addition to the specific form that inputs to cortical dynamics might take, one may ask more broadly about the computational role of motor preparation. Motor preparation is known to benefit behaviour (e.g. by shortening reaction times and enabling more accurate execution <xref ref-type="bibr" rid="c37">Riehle and Requin, 1989</xref>; <xref ref-type="bibr" rid="c6">Churchland and Shenoy, 2007</xref>; <xref ref-type="bibr" rid="c31">Michaels et al., 2015</xref>) and may facilitate motor learning (<xref ref-type="bibr" rid="c41">Sheahan et al., 2016</xref>; <xref ref-type="bibr" rid="c48">Sun et al., 2022</xref>). However, from the perspective of cortical dynamics, preparation also introduces additional constraints. Specifically, the high density of M1 neurons projecting directly to the spinal cord (<xref ref-type="bibr" rid="c11">Dum and Strick, 1991</xref>) suggests that motor cortical outputs control lower-level effectors with little intermediate processing. For preparatory processes to avoid triggering premature movement, any pre-movement activity in the motor and dorsal premotor (PMd) cortices must carefully exclude those pyramidal tract neurons. While this can be achieved by constraining neural activity to evolve in a nullspace of the motor output (<xref ref-type="bibr" rid="c23">Kaufman et al., 2014</xref>), the question nevertheless arises: what advantage is there to having neural dynamics begin earlier in a constrained manner, rather than unfold freely just in time for movement production?</p>
<p>Here we sought a normative explanation for motor preparation at the level of motor cortex dynamics: we asked whether preparation arises in recurrent neural networks (RNNs) performing delayed reaching tasks, and what factors lead to more or less preparation. Such an explanation could not be obtained from previous network models of delayed reaches, as they typically assume from the get-go that the cortical network receives preparatory inputs during a fixed time window preceding the go cue (<xref ref-type="bibr" rid="c49">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="c22">Kao et al., 2021</xref>). In this case, pre-movement activity is by design a critical determinant of the subsequent behaviour (<xref ref-type="bibr" rid="c49">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="c22">Kao et al., 2021</xref>; <xref ref-type="bibr" rid="c54">Zimnik and Churchland, 2021</xref>). In this work, we removed this modelling assumption and studied models in which the correct behaviour could in principle be obtained without explicit motor preparation.</p>
<p>To study the role of motor preparation, and that of exogenous inputs in this process, we followed an optimal control approach (<xref ref-type="bibr" rid="c15">Harris and Wolpert, 1998</xref>; <xref ref-type="bibr" rid="c51">Todorov and Jordan, 2002</xref>; <xref ref-type="bibr" rid="c53">Yeo et al., 2016</xref>). We considered the dynamics of an RNN model of M1 coupled to a model arm (<xref ref-type="bibr" rid="c52">Todorov and Li, 2003</xref>), and used a standard control cost functional to quantify and optimize performance in a delayed-reaching task. We used the iLQR algorithm (<xref ref-type="bibr" rid="c27">Li and Todorov, 2004</xref>) to find the spatiotemporal patterns of network inputs that minimize this cost functional, for any given network connectivity. Critically, these inputs could arise both before and during movement; thus, our framework allowed for principled selection amongst a continuum of motor strategies, going from purely autonomous motor generation following preparation, to purely input-driven unprepared dynamics.</p>
<p>We considered an inhibition-stabilized network – which was shown previously to capture prominent aspects of monkey M1 activity (<xref ref-type="bibr" rid="c18">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="c22">Kao et al., 2021</xref>) – and found that optimal control of the model requires preparation, with optimal inputs arising well before movement begins. To understand what features of network connectivity lead to optimal preparatory control strategies, we first turned to low-dimensional models, which could be more easily dissected. We then generalized insights from those systems back to highdimensional networks using tools from control theory, and found that preparation can be largely explained by two quantities summarizing the dynamical response properties of the network.</p>
<p>Finally, we studied the optimal control of movement <italic>sequences</italic>. Consistent with recent experimental findings (<xref ref-type="bibr" rid="c54">Zimnik and Churchland, 2021</xref>), we observed that optimal control of compound reaches leads to input-driven preparatory activity in a dedicated activity subspace prior to each movement.</p>
<p>Overall, our results show that preparatory neural activity patterns arise from optimal control of reaching movements at the level of motor cortical circuits, thus providing a possible explanation for a number of observed experimental findings.</p>
<sec id="s1">
<title>Model</title>
<sec id="s1a">
<title>A model of cortical dynamics for reaching movements</title>
<p>We considered a simple reaching task, in which the hand must move from a resting location to one of eight radially located targets in a 2D plane as fast as possible (<xref rid="fig1" ref-type="fig">Figure 1</xref>). The target had to be reached within 600 ms of a go cue that follows a delay period of varying (but known) duration. We modelled the trajectory of the hand via a two-jointed model arm (<xref ref-type="bibr" rid="c27">Li and Todorov, 2004</xref>; <xref ref-type="bibr" rid="c22">Kao et al., 2021</xref>), driven into motion by a pair of torques <bold><italic>m</italic></bold>(<italic>t</italic>) (Methods). We further assumed that these torques arise as a linear readout of the momentary firing rates <bold><italic>r</italic></bold>(<italic>t</italic>) of a population of M1 neurons,
<disp-formula id="eqn1">
<graphic xlink:href="535429v3_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold><italic>C</italic></bold> was a randomly generated readout matrix, projecting the neural activity into the output space. We modelled the dynamics of <italic>N</italic> = 200 M1 neurons using a standard rate equation,
<disp-formula id="eqn2">
<graphic xlink:href="535429v3_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="535429v3_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the momentary population firing rate vector <bold><italic>r</italic></bold>(<italic>t</italic>) was obtained by passing a vector of internal neuronal activations <bold><italic>x</italic></bold>(<italic>t</italic>) through a rectified linear function <italic>ϕ</italic> [ ], element-wise. In <xref ref-type="disp-formula" rid="eqn2">Equation 2</xref>, <bold><italic>h</italic></bold> is a constant input that establishes a baseline firing rate of 5 Hz on average, with a standard deviation of 5 Hz across neurons, <bold><italic>u</italic></bold>(<italic>t</italic>) is a task-dependent control input (see below), and <bold><italic>W</italic></bold> denotes the matrix of recurrent connection weights. Throughout most of this work, we considered inhibition-stabilized M1 dynamics (<xref ref-type="bibr" rid="c18">Hennequin et al., 2014</xref>; Methods), which have previously been shown to produce activity resembling that of M1 during reaching (<xref ref-type="bibr" rid="c22">Kao et al., 2021</xref>). Thus, our model can be viewed as a two-level controller, with the arm being controlled by M1, and M1 being controlled by external inputs. Note that each instantiation of our model corresponds to a set of <bold><italic>W</italic></bold>, <bold><italic>C</italic></bold>, and <bold><italic>h</italic></bold>, none of which are specifically optimized for the task.</p>
</sec>
<sec id="s1b">
<title>To prepare or not to prepare?</title>
<p>Previous experimental (<xref ref-type="bibr" rid="c4">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="c42">Shenoy et al., 2013</xref>) and modelling (<xref ref-type="bibr" rid="c18">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="c49">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="c34">Pandarinath et al., 2018</xref>) work suggests that fast ballistic movements rely on strong, near-autonomous internal dynamics in M1. Network-level models of ballistic control thus rely critically on a preparation phase during which they are driven into a movement-specific state that seeds their subsequent autonomous dynamics (<xref ref-type="bibr" rid="c22">Kao et al., 2021</xref>; <xref ref-type="bibr" rid="c49">Sussillo et al., 2015</xref>). However, somewhat paradoxically, the same recurent neural network models can also solve the task in a completely different regime, in which task-related inputs arise during movement only, with no preparatory inputs whatsoever. We illustrate this dichotomy in <xref rid="fig1" ref-type="fig">Figure 1</xref>. The same center-out reach can be produced with control inputs to M1 that arise either prior to movement only (full lines), or during movement only (dashed lines). In the latter case, no reach-specific preparatory activity is observed, making the model inconsistent with experimental findings. But what rationale is there in preparing for upcoming movements, then?</p>
<p>To address this question, we formulated delayed reaching as an optimal control problem, and asked what external inputs are required, and at what time, to drive the hand into the desired position with minimum control effort. Specifically, we sought inputs that were as weak as possible yet accurately drove the hand to the target within an alloted time window. We also penalized inputs that caused premature movement before the go cue.</p>
<p>Thus, we solved for spatio-temporal input trajectories that minimized a cost functional capturing the various task requirements. Our cost was composed of three terms: <italic>𝒥</italic><sub>target</sub> penalizes deviations away from the target, with an “urgency” weight that increases quadratically with time, thus capturing the implicit incentive for animals to perform fast reaches in such experiments (which are normally conducted in sessions of fixed duration). 𝒥<sub>null</sub> penalizes premature movement during preparation, as measured by any deviation in position, speed and acceleration of the hand. Finally, 𝒥<sub>effort</sub> penalizes control effort in the form of input magnitude throughout the whole trial, thus promoting energy-efficient control solutions amongst a typically infinite set of possibilities (<xref ref-type="bibr" rid="c22">Kao et al., 2021</xref>; <xref ref-type="bibr" rid="c46">Sterling and Laughlin, 2015</xref>). Note that 𝒥<sub>effort</sub> can be viewed as a standard regularization term, and must be included to ensure the control problem is well defined. The total objective thus had the following form :
<disp-formula id="eqn4">
<graphic xlink:href="535429v3_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold><italic>θ</italic></bold> and <inline-formula><inline-graphic xlink:href="535429v3_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> denote the position and velocity of the hand in angular space, Δ<sub>prep</sub> was the duration of the delay period and <italic>T</italic> that of the movement period. As 𝒥<sub>target</sub> and 𝒥<sub>null</sub> depend on <bold><italic>u</italic></bold>(<italic>t</italic>) implicitly through <xref ref-type="disp-formula" rid="eqn1">Equations 1</xref> and <xref ref-type="disp-formula" rid="eqn2">2</xref>, 𝒥 is a function of <bold><italic>u</italic></bold> only. Importantly, we allowed for inputs within a time window beginning Δ<sub>prep</sub> ms before, and ending <italic>T</italic> ms after the go cue (set at <italic>t</italic> = 0). Therefore, both preparation-only and movement-only input strategies (cf. <xref rid="fig1" ref-type="fig">Figure 1</xref>) could potentially arise, as well as anything in between.</p>
<p>Note that this control objective assumes that the delay duration (Δ<sub>prep</sub>) is known ahead of time, an assumption that does not hold for many delayed-reaching tasks in monkeys where the delay is uncertain. We make this assumption for computational tractability and later discuss extensions to the uncertain case (Discussion).</p>
<p>Here, we solved for the optimal control inputs using the iterative linear quadratic regulator algorithm (iLQR; <xref ref-type="bibr" rid="c27">Li and Todorov, 2004</xref>), an efficient trajectory optimization algorithm that is well-suited for handling the nonlinear nature of both the arm’s and the network’s dynamics. As our primary goal was to assess the role of preparation in a normative way, we did not study the putative circuit dynamics upstream of M1 that might lead to the computation of these optimal inputs.</p>
<p>We balanced the various components of our cost functional by choosing <italic>α</italic><sub>null</sub> and <italic>α</italic><sub>effort</sub> to qualitatively match the behavioural requirements of a typical reach- and-hold task. Specifically, we tuned them jointly so as to ensure (i) stillness during preparation, and (ii) reach duration of approximately <italic>∼</italic> 400 ms, with the hand staying within 0.5cm of the target for <italic>∼</italic> 200 ms after the end of the reach. We ensured that the main qualitative features of the solution, i.e. the results presented below, were robust to the choice of hyperparameter values within the fairly large range in which the above soft-constraints are satisfied (Supplementary Material S1).</p>
</sec>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Preparation arises as an optimal control strategy</title>
<p>Using the above control framework, we assessed whether the optimal way of performing a delayed reach involves preparation. More concretely, does the optimal control strategy of the model described in <xref ref-type="disp-formula" rid="eqn2">Equation 2</xref> involve any preparatory inputs during the delay period? For any single optimally performed reach, we found that network activity began changing well before the go cue (<xref rid="fig2" ref-type="fig">Figure 2B</xref>, bottom), and that this was driven by inputs that arose early (<xref rid="fig2" ref-type="fig">Figure 2B</xref>, middle). Thus, although preparatory network activity cancels in the readout (such that the hand remains still; <xref rid="fig2" ref-type="fig">Figure 2B</xref>, top) and therefore does not contribute directly to movement, it still forms an integral part of the optimal reach strategy. To quantify how much the optimal control strategy relied on inputs prior to movement, we defined the <italic>preparation index</italic> as the ratio of input magnitude during the delay period to that during the remainder of the trial:
<disp-formula id="eqn5">
<graphic xlink:href="535429v3_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<p>We found that the preparation index rose sharply as we increased the delay period, and eventually plateaued at <italic>∼</italic> 1.3 for delay periods longer than 300 ms (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). Similarly, the total cost of the task was highest in the absence of preparation, and decreased until it also reached a plateau at Δ<sub>prep</sub> <italic>∼</italic> 300 ms (<xref rid="fig2" ref-type="fig">Figure 2C</xref>, black). This appears somewhat counterintuitive, as having a larger Δ<sub>prep</sub> means that both 𝒥<sub>effort</sub> and 𝒥<sub>null</sub> are accumulated over a longer period. To resolve this paradox, we examined each component of the cost function. We found that the overall decrease in cost with increasing preparation time was driven by a concurrent decrease in both𝒥 <sub>tgt</sub> and 𝒥<sub>effort</sub>. The former effect was due to the model producing faster reaches (<xref rid="fig2" ref-type="fig">Figure 2C</xref> inset; hand position for a reach with (red) and without (blue) preparation) while the latter arose from smaller control inputs being necessary when preparation was allowed. Together, these results suggest that the presence of a delay period changes the optimal control strategy for reaching, and increases performance in the task.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Optimal control of the ISN network.</title>
<p><bold>(A)</bold> Illustration of the different terms in the control cost function, designed to capture the different requirements of the task. “Tgt” marks the time of target onset, “Go” that of the go cue (known in advance) and “End” the end of the trial. <bold>(B)</bold> Time course of the hand velocity (top), optimal control inputs (middle; 10 example neurons), and firing rates (bottom, same neurons) during a delayed reach to one of the 8 targets shown in <xref rid="fig1" ref-type="fig">Figure 1A</xref>. Here, the delay period was set to Δ<sub>prep</sub> = 300 ms. Note that inputs arise well before the go cue, even though they have no direct effect on behaviour at that stage. <bold>(C)</bold> Dependence of the different terms of the cost function on preparation time. All costs are normalized by the total cost at Δ<sub>prep</sub> = 0 ms. The inset shows the time course of the hand’s average distance to the relevant target when no preparation is allowed (blue) and when preparation is allowed (red). Although the target is eventually reached for all values of Δ<sub>prep</sub>, the hand gets there faster with longer preparation times, causing a decrease in <italic>𝒥</italic><sub>tgt</sub> – and therefore also in <italic>𝒥</italic><sub>tot</sub>. Another part of the decrease in <italic>𝒥</italic><sub>tot</sub> is due to a progressively lower input energy cost <italic>𝒥</italic><sub>effort</sub>. On the other hand, the cost of staying still before the Go cue increases slightly with Δ<sub>prep</sub>. <bold>(D)</bold> We define the preparation index as the ratio of the norms of the external inputs during preparation and during movement (see text). The preparation index measures how much the optimal strategy relies on the preparatory period. As more preparation time is allowed, this is used by the optimal controller and more inputs are given during preparation. For longer preparation times, this ratio increases sub-linearly, and eventually settles.</p></caption>
<graphic xlink:href="535429v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The results above show that delaying the reach beyond <italic>∼</italic> 300 ms brings little benefit; in particular, all components of the cost stabilize past that point (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). We thus wondered what features the optimally controlled dynamics would display as Δ<sub>prep</sub> increased beyond 300 ms. Would the network defer preparation to a last minute surge, or prepare more gently over the entire preparatory window? Would the network produce the same neural activity patterns? We found that the optimal controller made very little use of any preparation time available up to 300 ms before the go cue: with longer preparation times, external input continued to arise just a couple of hundred milliseconds before movement initiation, and single neuron firing rates remained remarkably similar (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). This was also seen in PCA projections of the firing rates, which traced out similar trajectories irrespective of the delay period (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). We hypothesized that this behaviour is due to the network dynamics having a certain maximum characteristic timescale, such that inputs that arrive too early end up being “forgotten” – they increase <italic>𝒥</italic><sub>effort</sub> and possibly <italic>𝒥</italic><sub>null</sub> without having a chance to influence <italic>𝒥</italic><sub>tgt</sub>. We confirmed this by varying the characteristic time constant (<italic>τ</italic> in <xref ref-type="disp-formula" rid="eqn2">Equation 2</xref>). For a fixed Δ<sub>prep</sub>, we found that for larger (resp. lower) values of <italic>τ</italic>, the optimal control inputs started rising earlier (resp. later) and thus occupied more (resp. less) of the alloted preparatory period (<xref rid="figS3" ref-type="fig">Figure S3</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Conservation of the optimal control strategy across delays</title>
<p><bold>(A)</bold> Optimal control inputs to ten randomly chosen neurons in the model RNN (left) and their corresponding firing rates (right) for different preparation times Δ<sub>prep</sub> (ranging from 0 to 800 ms; c.f. labels). <bold>(B)</bold> Projection of the movement-epoch population activity for each of the 8 reaches (panels) and each value of Δ<sub>prep</sub> shown in A (darker to lighter colors). These population trajectories are broadly conserved across delay times, and become more similar for larger delays.</p></caption>
<graphic xlink:href="535429v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Understanding optimal control in simplified models</title>
<p>Having established that the ISN model of M1 relies on preparatory inputs to solve the delayed reaching task, we next tried to understand <italic>why</italic> it does so. To further unravel the interplay between the structure of the network and the optimal control strategy, i.e. what aspects of the dynamics of the network warrant preparation, we turned to simpler, two-dimensional models of cortical dynamics. These 2D models are small enough to enable detailed analysis (Supplementary Material S3), yet rich enough to capture the two dominant dynamical phenomena that arise in ISN dynamics: nonnormal amplification (<xref ref-type="bibr" rid="c33">Murphy and Miller, 2009</xref>; <xref ref-type="bibr" rid="c13">Goldman, 2009</xref>; <xref ref-type="bibr" rid="c17">Hennequin et al., 2012</xref>) and oscillations (<xref ref-type="bibr" rid="c3">Brunel, 2000</xref>; <xref ref-type="bibr" rid="c8">Dayan and Abbott, 2001</xref>). Specifically, networks of E and I neurons have been shown to embed two main motifs of effective connectivity which are revealed by appropriate orthogonal changes of basis: (i) feedforward (“nonnormal”) connectivity whereby a “source mode” of E-I imbalance feeds into a “sink mode” in which balance is restored, and (ii) anti-symmetric connectivity that causes the two populations to oscillate.</p>
<p>To study the impact of each of these prototypical connectivity motifs on movement preparation, we implemented them separately, i.e. as two small networks of two units each, with an overall connectivity scale parameter <italic>w</italic> which we varied (<xref rid="fig4" ref-type="fig">Figure 4A</xref> and <xref ref-type="fig" rid="fig4">D</xref>; Methods). As both nonnormal and oscillatory dynamics arise from linear algebraic properties of the connectivity matrix, we considered linear network dynamics for this analysis (<italic>ϕ</italic>(<italic>x</italic>) = <italic>x</italic> in <xref ref-type="disp-formula" rid="eqn3">Equation 3</xref>). Moreover, to preserve the existence of an output nullspace in which preparation could in principle occur without causing premature movement, we reduced the dimensionality of the motor readout from 2D (where there would be no room left for a nullspace) to 1D (leaving a 1D nullspace), and adapted the motor task so that the network now had to move the hand position along a single dimension (<xref rid="fig4" ref-type="fig">Figure 4B</xref> and <xref ref-type="fig" rid="fig4">E</xref>, top). Analogous to the previous arm model, we assumed that the hand’s acceleration along this axis was directly given by the 1D network readout.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Analysis of the interplay between the optimal control strategy and two canonical motifs of E-I network dynamics:</title>
<p>nonnormal transients driven by feedforward connectivity (A–C), and oscillations driven by anti-symmetric connectivity (D–F). <bold>(A)</bold> Activity flow field (10 example trajectories) of the nonnormal network, in which a “source” unit (orange) drives a “sink” unit (brown). We consider two opposite readout configurations, where it is either the sink (left) or the source (right) that drives the acceleration of the hand. <bold>(B)</bold> Temporal evolution of the hand position (top; the dashed horizontal line indicates the reach target), hand acceleration (middle) and optimal control inputs to the two units (bottom; colours matching panel A), under optimal control given each of the two readout configurations shown in A (left vs. right). The dashed vertical line marks the go cue, and the gray bar indicates the delay period. While the task can be solved successfully in both cases, preparatory inputs are only useful when the sink is read out. <bold>(C)</bold> Network activity trajectories under optimal control. Each trajectory begins at the origin, and the end of the delay period is shown with a black cross. <bold>(D-F)</bold> Same as (A-C), for the oscillatory network. <bold>(G-H)</bold> Preparation index (top) and total amount of preparatory inputs (bottom) as a function of the scale parameter <italic>w</italic> of the network connectivity, for various readout configurations (colour-coded as shown in the top inset). The nonnormal network (top) prepares more when the readout is aligned to the most controllable mode, while the amount of preparation in the oscillatory network (bottom) is independent of the readout direction. The optimal strategy must balance the benefits from preparatory inputs which allow to exploit the intrinsic network dynamics, with the constraint to remain still. This is more difficult when the network dynamics are strong and pushing activity out of the readout-null subspace, explaining the decrease in preparation index for large values of <italic>w</italic> in the oscillatory network.</p></caption>
<graphic xlink:href="535429v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We found that optimal control of both dynamical motifs generally led to preparatory dynamics, with inputs arising before the go cue (<xref rid="fig4" ref-type="fig">Figure 4B</xref> and <xref ref-type="fig" rid="fig4">E</xref>, bottom). In the feedforward motif, the amount of preparatory inputs appeared to depend critically on the orientation of the readout. When the readout was aligned with the sink (brown) mode (<xref rid="fig4" ref-type="fig">Figure 4B</xref>, left), the controller prepared the network by moving its activity along the source (orange) mode (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, left). This placed the network in a position from which it had a natural propensity to generate large activity transients along the readout dimension (c.f. flow field in <xref rid="fig4" ref-type="fig">Figure 4A</xref>); here, these transients were exploited to drive the fast upstroke in hand acceleration and throw the hand towards the target location. Note that this strategy reduces the amount of input the controller needs to deliver during the movement, because the network itself does most of the work. Nevertheless, in this case the network’s own impulse response was not rich enough to accommodate the phase reversal required to subsequently slow the hand down and terminate the movement. Optimal control therefore also involved inputs during the movement epoch, leading to a preparatory index of <italic>∼</italic> 0.54 (<xref rid="fig4" ref-type="fig">Figure 4G</xref>, dark blue).</p>
<p>When it was instead the source mode that was read out (<xref rid="fig4" ref-type="fig">Figure 4B</xref>, right), the only dimension along which the system could prepare without moving was the sink mode. Preparing this way is of no benefit, because the flow field along the sink mode has no component along the source (readout) mode. Thus, here the optimal strategy was to defer control to the movement epoch, during which the transient growth of network activity along the readout rested entirely on adequate control inputs. This led to a preparation index of <italic>∼</italic> 0 (<xref rid="fig4" ref-type="fig">Figure 4G</xref>, pale green). Although the network did react with large activity excursions along the sink mode (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, right), these were inconsequential for the movement. Importantly, of the two extreme readout configurations discussed above, the first one yielded a smaller overall optimal control cost (by a factor of <italic>∼</italic> 1.5). Thus, at a meta-control level, ideal downstream effectors would read out the sink mode, not the source mode. Note that while increasing the connectivity strength initially led to more preparation (<xref rid="fig4" ref-type="fig">Figure 4H</xref>), a plateau was eventually reached for <italic>w ≥</italic> 4. Indeed, while stronger dynamics initially make preparation more beneficial, they also make it more difficult for preparatory activity to remain in the readout nullspace.</p>
<p>We obtained similar insights for oscillatory network dynamics (<xref rid="fig4" ref-type="fig">Figure 4D-F</xref>). A key difference however was that the flow field was rotationally symmetric such that no distinction could be made between “source” and “sink” units – indeed the optimal control strategy yielded the same results (up to a rotation of the state space) irrespective of which of the two units was driving the hand’s acceleration (compare left and right panels in <xref rid="fig4" ref-type="fig">Figure 4D-F</xref>). Nevertheless, the optimal controller consistently moved the network’s activity along the output-null axis during preparation, in such a way as to engage the network’s own rotational flow immediately after the go cue (<xref rid="fig4" ref-type="fig">Figure 4F</xref>). This rotational flow drove a fast rise and decay of activity in the readout unit, thus providing the initial segment of the required hand acceleration. The hand was subsequently slowed down by modest movement-epoch control inputs which eventually receded, leading to a preparation index of <italic>∼</italic> 0.58. Interestingly, the preparation index showed a decrease for very large <italic>w</italic> (<xref rid="fig4" ref-type="fig">Figure 4G</xref>), which did not reflect smaller preparatory inputs (<xref rid="fig4" ref-type="fig">Figure 4H</xref>) but rather reflected the larger inputs that were required during movement to cancel the fast oscillations naturally generated by the network.</p>
<p>The above results highlight how the optimal control strategy is shaped by the dynamical motifs present in the network. Crucially, we found that the optimal way to control the movement depends not only on the strength and flow of the internal network dynamics, but also on their interactions with the readout.</p>
</sec>
<sec id="s2c">
<title>Control-theoretic properties predict the amount of preparation</title>
<p>Our investigation of preparation in a low-dimensional system allowed us to isolate the impact of core dynamical motifs, and highlighted how preparation depends on the geometry of the flow field, and its alignment to the readout. However, these intuitions remain somewhat qualitative, making them difficult to generalize to our high-dimensional ISN model.</p>
<p>To quantify the key criteria that appear important for preparation, we turned to tools from control theory. We reasoned that, for a network to be able to benefit from preparation and thus exhibit a large preparation index, there must be some advantage to using early inputs that do not immediately cause movement, relative to using later inputs that do. We hypothesized that this advantage could be broken down into two criteria. First, there must exist activity patterns that are momentarily output-null (i.e. do not immediately cause movement) yet seed output-potent dynamics that subsequently move the arm. The necessity of this criterion was obvious in the 2D nonnormal network, which did not display any preparation when its nullspace was aligned with its “sink” mode. In the language of control theory, this criterion implies that the nullspace of the readout must be sufficiently “observable” – we captured this in a scalar quantity <italic>α</italic> (Methods; <xref ref-type="bibr" rid="c21">Kao and Hennequin, 2019</xref>; <xref ref-type="bibr" rid="c43">Skogestad and Postlethwaite, 2007</xref>). Second, there must be a sizeable cost to performing the movement in an entirely input-driven manner without relying on preparation. In other words, the network should be hard to steer along the readout direction, i.e. the readout must be of limited “controllability” – we captured this in another scalar quantity <italic>β</italic> (Methods).</p>
<p>We illustrate the meaning of these two metrics in <xref rid="fig5" ref-type="fig">Figure 5A</xref> and <xref ref-type="fig" rid="fig5">B</xref> for a 2-dimensional example network that combines nonnormality and oscillations. We show two extreme choices of readout direction (<xref rid="fig5" ref-type="fig">Figure 5A</xref>, dashed black): the one that maximizes <italic>α</italic> (top) and the one that minimizes it (bottom). In the first case, the readout nullspace (dashed orange) is very observable, i.e. trajectories that begin in the nullspace evolve to produce large transients along the readout (solid orange &amp; inset). In the second case, the opposite is true. For each case, we also assessed the controllability of the readout (<italic>β</italic>). The controllability of a direction corresponds to how much variance activity trajectories exhibit along that direction, when they are randomly and isotropically initialized (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). In other words, a very controllable direction is one along which network trajectories have a natural tendency to evolve.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Predicting the preparation index from the observability of the output nullspace (<italic>α</italic>) and the controllability of the readout (<italic>β</italic>, see details in text).</title>
<p><bold>(A)</bold> Illustration of the observability of the output nullspace in a synthetic 2-dimensional system. The observability of a direction is characterized by how much activity (integrated squared norm) is generated along the readout by a unit-norm initial condition aligned with that direction. The top and bottom panels show the choices of readout directions (dotted black) for which the corresponding nullspace (dotted orange) is most (maximum <italic>α</italic>) and least (minimum <italic>α</italic>) observable, respectively. Trajectories initialized along the null direction are shown in solid orange, and their projections onto the readout are shown in the inset. <bold>(B)</bold> Illustration of the controllability of the readout in the same 2D system as in (A). To compute controllability, the distribution of activity patterns collected along randomly initialized trajectories is estimated (heatmap); the controllability of a given direction then corresponds to how much variance it captures in this distribution. Here, the network has a natural propensity to generate activity patterns aligned with the dashed white line (‘most controllable’ direction). The readout directions are repeated from panel A (dotted black). The largest (resp. smallest) value of <italic>β</italic> would by definition be obtained when the readout is most (resp. least) controllable. Note the tradeoff in this example: the choice of readout that maximizes <italic>α</italic> (top) does not lead to the smallest <italic>β</italic>. <bold>(C)</bold> The values of <italic>α</italic> and <italic>β</italic> accurately predict the preparation index (<italic>R</italic><sup>2</sup> = 0.93) for a range of high-dimensional ISNs (maroon dots) with different connectivity strengths and characteristic timescales (Methods). The best fit (after z-scoring) is given by f(<italic>α, β</italic>) = (16.94 <italic>±</italic> 0.02)<italic>α −</italic> (15.97 <italic>±</italic> 0.02)<italic>β</italic> (mean <italic>±</italic> sem were evaluated by boostrapping). This confirms our hypothesis that optimal control relies more on preparation when <italic>α</italic> is large and <italic>β</italic> is small. Note that <italic>α</italic> and <italic>β</italic> alone only account for 34.8% and 30.4% of the variance in the preparation index, respectively (inset). Thus, <italic>α</italic> and <italic>β</italic> provide largely complementary information about the networks’ ability to use inputs, and can be combined into a very good predictor of the preparation index. Importantly, even though this fit was obtained <italic>using ISNs only</italic>, it still captures 69% of preparation index variance across networks from other families (blue dots; Methods).</p></caption>
<graphic xlink:href="535429v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We then assessed how well <italic>α</italic> and <italic>β</italic> could predict the preparation index of individual networks. In 2D networks, we found that a simple function that grows with <italic>α</italic> and decreases with <italic>β</italic> could accurately predict preparation across thousands of networks Supplementary Material S3.2. To assess whether these insights carried over to high-dimensional networks, we then generated a range of large ISNs with parameterically varied connectivity strengths and decay timescales (Methods). We then regressed the preparation index against the values of <italic>α</italic> and <italic>β</italic> computed for each of these networks (as controllability and observability are only defined for linear networks, we set <italic>ϕ</italic>(<italic>x</italic>) = <italic>x</italic> for this investigation). We found that a simple linear mapping, prep. index = <italic>k</italic><sub>0</sub> + <italic>k</italic><sub><italic>α</italic></sub><italic>α</italic> + <italic>k</italic><sub><italic>β</italic></sub><italic>β</italic>, with parameters fitted to one half of the ISN networks, accurately predicted the preparation indices of the other half (<xref rid="fig5" ref-type="fig">Figure 5C</xref>; <italic>R</italic><sup>2</sup> = 0.93, 5-fold cross-validated). Interestingly, we observed that although <italic>α</italic> and <italic>β</italic> (which are both functions of the network connectivity) were highly correlated across different networks, discarding either variable in our linear regression led to a significant drop in <italic>R</italic><sup>2</sup> (<xref rid="fig5" ref-type="fig">Figure 5C</xref>, inset). Importantly, it was their difference that best predicted the preparation index (<italic>k</italic><sub><italic>α</italic></sub> <italic>&gt;</italic> 0 and <italic>k</italic><sub><italic>β</italic></sub> <italic>&lt;</italic> 0), consistent with our hypothesis that the preparation index is a relative quantity which increases as the nullspace becomes more observable, but decreases as readout dimensions become more controllable.</p>
<p>We were able to confirm the generality of this predictive model by generating networks with other types of connectivity (oscillatory networks, and networks with unstructured random weights), which displayed dynamics very different from the ISNs (see Supplementary Material S4). Interestingly, despite the different distribution of <italic>α</italic> and <italic>β</italic> parameters in those networks, we could still capture a large fraction of the variance in their preparation index (<italic>R</italic><sup>2</sup> = 0.69) using the linear fit obtained from the ISNs alone. This confirms that <italic>α</italic> and <italic>β</italic> can capture information about the networks’ dynamics in a universal manner.</p>
<p>Note that we do not make any claims about the specific functional form of the relationship between <italic>α, β</italic> and the preparation index. Rather, we claim that there should be a broad trend for the preparation index to increase with <italic>α</italic> and decrease with <italic>β</italic>, and we acknowledge that this relationship could in general be nonlinear. Indeed, in 2D networks, we found that the preparation index was in fact better predicted by the ratio of <italic>α</italic> over <italic>β</italic> than by their difference (Supplementary Material S3.2).</p>
<p>Finally, as the above results highlight that the amount of preparation depends on the alignment between internal dynamics and readout, one may wonder how much our conclusions depend on our use of a random unstructured readout matrix. First, we note that the effect of the alignment on preparation index is greatly amplified in the low-dimensional networks (<xref rid="fig4" ref-type="fig">Figure 4G</xref>). In high-dimensional networks, any random readout matrix <bold><italic>C</italic></bold> will pick out activity dimensions in the RNN that are sufficiently aligned with the most controllable directions of the dynamics to encourage preparation. Second, we performed additional simulations where we meta-optimized the readout so as to minimize the average optimal cost per movement. The resulting system is more observable overall (as it allows the network to solve the task at a lower cost) but relies just as much on preparation (<xref rid="figS7" ref-type="fig">Figure S7</xref>).</p>
</sec>
<sec id="s2d">
<title>Modelling movement sequences</title>
<p>Having gained a better understanding of what features lead a network to prepare, we next set out to assess whether optimal control could also explain the neural preparatory processes underlying the generation of movement <italic>sequences</italic>. We revisited the experimental studies of <xref ref-type="bibr" rid="c54">Zimnik and Churchland (2021)</xref>, where monkeys were trained to perform two consecutive reaches. Each trial started with the display of both targets, followed by an explicitly enforced delay period before the onset of the first reach. A distinction was made between “double” reaches in which a pause was enforced between reaches, and “compound” reaches in which no pause was required. This study concluded that, rather than the whole movement sequence unrolling from a single preparatory period, each reach was instead successively seeded by its own preparatory activity.</p>
<p>Here, we asked whether such an independent, successive preparation strategy would arise as an optimal control solution, in the same way that single-reach preparation did. Importantly, we could not answer this question by directly examining network inputs as we did for single reaches. Indeed, any network input observed before the second reach could be contributing either to the end of the first movement, or to the preparation of the next. In fact, the issue of teasing apart preparatory vs. movement-related activity patterns also arose in the analysis of the monkey dat a. To address this, <xref ref-type="bibr" rid="c54">Zimnik and Churchland (2021)</xref> exploited the fact that monkey M1 activity just before and during single reaches is segregated into two distinct subspaces. Thus, momentary activity patterns (during either single or double reaches) can be unambiguously labelled as preparatory or movement-related depending on which of the two subspaces they occupied. We performed a similar analysis (Methods) and verified that preparatory and movement activity patterns in the model were also well segregated in their respective subspaces in the single-reach task (<xref rid="fig6" ref-type="fig">Figure 6A-B</xref>). We then assessed the occupancy of the preparatory subspace during double reaching in the model, and took this measure as a signature of preparation.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>The model executes a sequence of two reaches using a holistic strategy.</title>
<p><bold>(A)</bold> Hand velocity during one of the reaches, with the corresponding hand trajectory shown in the inset. <bold>(B-C)</bold> We identified two 6-dimensional orthogonal sub-spaces, capturing 79% and 85% of total activity variance during single-reach preparation and movement respectively. <bold>(B)</bold> First principal component of the model activity for the 8 different reaches projected into the subspaces identified using preparatory (top) and movement-epoch (bottom) activity. <bold>(C)</bold> Occupancy (total variance captured across movements) of the orthogonalized preparatory and movement subspaces, in the model (top) and in monkey motor cortical activity (bottom; reproduced from <xref ref-type="bibr" rid="c25">Lara et al., 2018</xref> for monkey Ax). We report mean <italic>±</italic> s.e.m., where the error is computed by bootstrapping from the neural population as in <xref ref-type="bibr" rid="c25">Lara et al. (2018)</xref>. We normalize each curve separately to have a maximum mean value of 1. To align the model and monkey temporally, we re-defined the model’s ‘movement onset’ time to be 120 ms after the model’s hand velocity crossed a threshold – this accounts for cortico-spinal delays and muscle inertia in the monkey. Consistent with <xref ref-type="bibr" rid="c25">Lara et al. (2018</xref>)’s monkey M1 recordings, preparatory subspace occupancy in the model peaks shortly before movement onset, rapidly dropping thereafter to give way to pronounced occupancy of the movement subspace. Conversely, there is little movement subspace occupancy during preparation. <bold>(D)</bold> Behavioural (top) and neural (middle) correlates of the delayed reach for one example of a double reach with an enforced pause of 0.6 s. The optimal strategy relies on preparatory inputs preceding each movement. <bold>(E)</bold> Same as (C), for double reaches. The onsets of the monkey’s two reaches are separately aligned to the model’s using the same convention as in (C). The preparatory subspace displays two clear peaks of occupancy. This double occupancy peak is also observed in monkey neural activity (bottom; reproduced from <xref ref-type="bibr" rid="c54">Zimnik and Churchland, 2021</xref>, with the first occupancy peak aligned to that of the model). <bold>(F)</bold> Same as (D), for compound reaches with no enforced pause in between. Even though the sequence could be viewed as a single long movement, the control strategy relies on two periods of preparation. Here, inputs before the second reach are used to reinject energy into the system after slowing down at the end of the first reach. <bold>(G)</bold> Even though no explicit delay period is enforced in-between reaches during the compound movement, the preparatory occupancy rises twice, before the first reach and once again before the second reach. This is similar to observations in neural data (bottom; reproduced from <xref ref-type="bibr" rid="c54">Zimnik and Churchland, 2021</xref>).</p></caption>
<graphic xlink:href="535429v3_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To model optimal control of a double reach, we modified our cost functional to account for the presence of two consecutive targets (see Methods). We considered the same set of eight targets as in our single-reach task, and modelled all possible combinations of two targets (one example shown in <xref rid="fig6" ref-type="fig">Figure 6</xref>). We set the hyperparameters of the cost function such that both targets could be reached by the resulting optimal controller, in a way that matched important qualitative aspects of the monkeys’ behaviour (in particular, such that both reaches were performed at similar velocities, with the second reach lasting slightly longer on average; <xref rid="fig6" ref-type="fig">Figure 6B-C</xref>; top).</p>
<p>We projected the network activity onto preparatory and movement subspaces identified using single and double reaches activity (Methods). For double reaches with a long (600ms) pause, the preparatory subspace was transiently occupied twice, with the two peaks occurring just before the onset of each movement in the sequence (<xref rid="fig6" ref-type="fig">Figure 6B</xref>; bottom).</p>
<p>Notably, the occupancy during the “compound” reach (without pause; <xref rid="fig6" ref-type="fig">Figure 6C</xref>) also started rising prior to the first movement before decaying very slightly and peaking again before the second reach, indicating two independent preparatory events. This is somewhat surprising, given that a movement sequence can also be viewed as a single “compound” movement, for which we have shown previously a unique preparatory phase is sufficient (<xref rid="fig2" ref-type="fig">Figure 2</xref>). In our model, this behaviour can be understood to arise from the requirement that the hand stop briefly at the first target. To produce the second reach, the hand needs to accelerate again, which requires transient re-growth of activity in the network. Given that the network’s dynamical repertoire exhibits limited timescales, this is most easily achieved by reinjecting inputs into the system.</p>
<p>In summary, our results suggest that the “independent” preparation strategy observed in monkeys is consistent with the optimal control of a two-reach sequence. While <xref ref-type="bibr" rid="c54">Zimnik and Churchland (2021)</xref> showed that RNNs trained on this task used this “independent” strategy, this was by design as the network was only cued for the second reach after the first one had started. In addition to replicating this proof of concept that it is possible to prepare whilst moving, our model also shows how and why independent preparation might arise as an optimal control solution.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this work, we proposed a model for the dynamics of motor cortex during a delayed reaching task in non-human primates. Unlike previous work, we treated M1 as an input-driven nonlinear dynamical system, with generic connectivity not specifically optimized for the task, but with external inputs assumed to be optimal for each reach. Motivated by a large body of evidence suggesting that preparation is useful before delayed reaches (<xref ref-type="bibr" rid="c5">Churchland et al., 2010</xref>; <xref ref-type="bibr" rid="c25">Lara et al., 2018</xref>; <xref ref-type="bibr" rid="c1">Afshar et al., 2011</xref>; <xref ref-type="bibr" rid="c42">Shenoy et al., 2013</xref>), but also evidence for thalamic inputs being necessary for accurate movement execution (<xref ref-type="bibr" rid="c39">Sauerbrei et al., 2020</xref>), we used this model to investigate whether and why neural circuits might rely on motor preparation during delayed reaching tasks. Interestingly, preparation arose as an optimal control strategy in our model, with the optimal solution to the task relying strongly on inputs prior to movement onset. Moreover, the benefits of preparation were dependent on the network connectivity, with preparation being more prevalent in networks whose rich internal dynamics can be advantageously seeded by early external inputs. We were able to quantify this intuition with a predictive model relating the dynamical response properties of a network to the amount of preparation it exhibits when controlled optimally. Finally, we found that prominent features of the monkeys’ neural activity during sequential reaches arose naturally from optimal control assumptions. Specifically, optimally controlled networks relied on two phases of preparation when executing sequences of two reaches, corroborating recent experimental observations in monkey M1 (<xref ref-type="bibr" rid="c54">Zimnik and Churchland, 2021</xref>). Together, our results provide a normative explanation for the emergence of preparatory activity in both single and sequential reaching movements.</p>
<p>In recent years, task-optimized recurrent neural networks have become a very popular tool to model neural circuit dynamics. Classically, those models incorporate only those inputs that directly reflect task-related stimuli (e.g. motor target, go cue, etc). This requires assumptions about the form of the inputs, such as modelling them as simple step functions active during specific task epochs. However, as local neural circuits are part of a wider network of brain areas, a large portion of their inputs come from other brain areas at intermediate stages of the computation and may therefore not be directly tied to task stimuli. Thus, it is not always obvious what assumptions can reasonably be made about the inputs that drive the circuit’s dynamics. Our optimization framework, which does not require us to make any specific assumptions about when and how inputs enter the network (although it does allow to incorporate prior information in the form of constraints), allows to bypass this problem and to implicitly model unobserved inputs from other areas. Importantly, our framework allows to ask questions – such as “why prepare” – that are difficult to phrase in standard input-driven RNN models. We note, however, that in the investigation we have presented here, the lack of imposed structure for the inputs also implied that the model could not make use of mechanisms known to contribute certain aspects of preparatory neural activity. For example, our model did not exhibit the usual visually-driven response to the target input, nor did it have to use the delay epoch to keep such a transient sensory input in memory (<xref ref-type="bibr" rid="c14">Guo et al., 2014</xref>; <xref ref-type="bibr" rid="c26">Li et al., 2015</xref>).</p>
<p>The main premise of our approach is that one can somehow delineate the dynamical system which M1 implements, and attribute any activity patterns that it cannot automously generate to external inputs. Just where the anatomical boundary of “M1 dynamics” lie – and therefore where “external inputs” originate – is unclear, and our results must be interpreted with this limitation in mind. Operationally, previous works in reaching monkeys have shown that M1 data can be mathematically well described by a dynamical system that appears largely autonomous during movement. These works have emphasized that those abstract dynamics, while inferred from M1 data alone, may not be anatomically confined to M1 itself. Instead, they may involve interactions between multiple brain areas, and even possibly parts of the body through delayed sensory feedback. Here, we too tend to think of our M1 models in this way, and therefore attribute external input to brain areas that are one-step removed from this potentially broad motor-generating network. Nevertheless, a more detailed multi-area model of the motorgenerating circuitry including e.g. spinal networks (<xref ref-type="bibr" rid="c35">Prut and Fetz, 1999</xref>) could enable more detailed comparisons to multi-region neural data. In a similar vein, our model makes no distinction between external inputs that drive movement-specific planning computations, and other types of movement-unspecific inputs that might drive the transition from planning to execution (e.g. “trigger” inputs, <xref ref-type="bibr" rid="c24">Kaufman et al., 2016</xref>). Incorporating such distinctions (e.g. by temporally modulating the cost in individual input channels depending on specific task events, or by having separate channels for movement-unspecific inputs) might allow to ask more targeted questions about the role and provenance of external inputs.</p>
<p>A major limitation of our study is the specific choice of a quadratic penalty on the external input in our control objective. Whilst there are possible justifications for such a cost (e.g. regularization of the dynamics to promote robustness of the control solution), its use here is mainly motivated by mathematical tractability. Other costs might be conceivably more relevant and might affect our results. For example, studies of motor cortex have long thought of its dynamics as converting relatively simple inputs reflecting high-level, temporally stable plans, into detailed, temporally-varying motor commands. Thus, a potentially relevant form of a penalty for external inputs would be their temporal complexity. Such a penalty would have the advantage of encouraging a clearer separation between the inputs and the RNN activations; indeed, in our current model, we find that the optimal controls themselves have a temporal structure, part of which could be generated by a dynamical system and thus potentially absorbed into our “M1 dynamics”. To address this, we note that our optimization framework can be adjusted to penalize the magnitude of the temporal <italic>derivative</italic> of the external input <inline-formula><inline-graphic xlink:href="535429v3_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, instead of ∥<bold><italic>u</italic></bold>∥<sup>2</sup>. We experimented with this extension and found qualitatively different optimal inputs and M1 firing rates, which evolved more slowly and plateaued for sufficiently long preparation (<xref rid="figS8" ref-type="fig">Figure S8A-D</xref>) – this is in fact more consistent with monkey M1 data (e.g. <xref ref-type="bibr" rid="c12">Elsayed et al., 2016</xref>). Despite these qualitative difference in the specific form of preparation, our main conclusion stands that input-driven preparation continues to arise as an optimal solution (<xref rid="figS8" ref-type="fig">Figure S8E-F</xref>).</p>
<p>Another important assumption we have made is that the optimal controller is aware of the duration of the delay period. While this made solving for the optimal control inputs easier, it made our task more akin to a self-initiated reach (<xref ref-type="bibr" rid="c25">Lara et al., 2018</xref>) than to a typical delayed reach with unpredictable, stochastic delay durations. Future work could revisit this assumption. As a first step towards this, we now briefly outline pilot experiments in this direction. We used an exponential distribution of delays (with mean 300 ms) and devised two modified versions of our model that dealt with the resulting uncertainty in two different ways. In the first strategy, at any time during preparation, the model would estimate the most probable time-to-go-cue given that it hadn’t arrived yet (in this case, this is always 300 ms in the future) and would plan an optimal sequence of inputs accordingly. In the second strategy, the network would prudently assume the earliest possible go cue (i.e. the next time step) and plan accordingly. In both cases, only the first input in the optimal input sequence would be used at each step, and complete replanning would follow in the next step, as the model re-assesses the situation given new information (i.e. whether the actual go cue arrived or not; this is a form of “model predictive control”, <xref ref-type="bibr" rid="c36">Rawlings et al., 2017</xref>). Preparatory inputs arose in both settings, but we found that only the latter strategy led to activity patterns that plateaued early during preparation (see <xref rid="figS9" ref-type="fig">Figure S9</xref>).</p>
<p>Throughout the main text, we have referred to Δ<sub>prep</sub> as the task-enforced delay period. However, a more accurate description may be that it corresponds to a delay period determined by an internally set go signal, which may lag behind the external go cue. While we would not expect a large difference between those two signals, the way in which we define Δ<sub>prep</sub> becomes important as it approaches 0 ms (limit of a quasi-automatic reach, <xref ref-type="bibr" rid="c25">Lara et al., 2018</xref>). Indeed, in this limit, our model exhibits almost no activity in the preparatory subspace (as defined in <xref rid="fig6" ref-type="fig">Figure 6</xref>) (see further analyses in <xref rid="figS10" ref-type="fig">Figure S10</xref>). In contrast, monkey M1 activity was found to transiently occupancy the preparatory subspace even in this case <xref ref-type="bibr" rid="c25">Lara et al. (2018)</xref>, which suggests that a more realistic model should incorporate the additional processing occurring between the external and internal go cues.</p>
<p>Dynamical systems have a longstanding history as models of neural populations (<xref ref-type="bibr" rid="c8">Dayan and Abbott, 2001</xref>). However, understanding how neural circuits can perform various computations remains a challenging question. Recently, there has been increased interest in trying to understand the role of inputs in shaping cortical dynamics. This question has been approached both from a data-driven perspective (<xref ref-type="bibr" rid="c29">Malonis et al., 2021</xref>; <xref ref-type="bibr" rid="c45">Soldado-Magraner et al., 2023</xref>), and in modelling work with e.g <xref ref-type="bibr" rid="c9">Driscoll et al. (2022)</xref> showing how a single network can perform different tasks by reorganizing its dynamics under the effect of external inputs and <xref ref-type="bibr" rid="c10">Dubreuil et al. (2021)</xref> relating network structure to the ability to process contextual inputs. To better understand how our motor system can generate flexible behaviours (<xref ref-type="bibr" rid="c28">Logiaco et al., 2021</xref>; <xref ref-type="bibr" rid="c47">Stroud et al., 2018</xref>), and to characterize learning on short timescales (<xref ref-type="bibr" rid="c44">Sohn et al., 2020</xref>; <xref ref-type="bibr" rid="c16">Heald et al., 2023</xref>), it is important to study how network dynamics can be modulated by external signals that allow rapid adaptation to new contexts without requiring extensive modifications of the network’s connectivity. The optimal control approach we proposed here offers a way to systematically perform such evaluations, in a variety of tasks and under different assumptions regarding how inputs are allowed to impact the dynamics of the local circuit of interest. While our model’s predictions will depend on e.g. the choice of connectivity or the design of the cost function, an exciting direction for future work will be to obtain those parameters in a data-driven manner, for instance using recently developed methods to infer dynamics from data (<xref ref-type="bibr" rid="c34">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="c40">Schimel et al., 2022</xref>), and advances in inverse reinforcement learning and differentiable control (<xref ref-type="bibr" rid="c2">Amos et al., 2018</xref>) to infer the cost function that behaviour optimizes. These could additionally be combined with more biomechanically realistic effectors, such as the differentiable arm models from <xref ref-type="bibr" rid="c7">Codol et al. (2023)</xref>.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Experimental model and subject details</title>
<p>In <xref rid="fig1" ref-type="fig">Figure 1</xref>, we showed data from two primate datasets that were made available to us by Mark Churchland, Matthew Kaufman and Krishna Shenoy. Details of animal care, surgery, electrophysiological recordings, and behavioral task have been reported previously in <xref ref-type="bibr" rid="c4">Churchland et al. (2012)</xref>; <xref ref-type="bibr" rid="c23">Kaufman et al. (2014)</xref> (see in particular the details associated with the J and N “array” datasets). The subjects of this study, J and N, were two adult male macaque monkeys (Macaca mulatta). The animal protocols were approved by the Stanford University Institutional Animal Care and Use Committee. Both monkeys were trained to perform a delayed reaching task on a fronto-parallel screen. At the beginning of each trial, they fixated on the center of the screen for some time, after which a target appeared on the screen. After a variable delay period (0–1000 ms), a go cue appeared instructing the monkeys to reach toward the target. Recordings were made in the dorsal premotor cortex and in the primary motor cortex using a pair of implanted 96-electrode arrays. In <xref rid="fig6" ref-type="fig">Figure 6</xref>, we also reproduced data from <xref ref-type="bibr" rid="c25">Lara et al. (2018)</xref> and <xref ref-type="bibr" rid="c54">Zimnik and Churchland (2021)</xref>. Details of animal care, surgery, electrophysiological recordings, and behavioral task for those data can be found in the Methods section of the respective papers.</p>
</sec>
<sec id="s4b">
<title>Arm model</title>
<p>To simulate reaching movements, we used the planar two-link arm model described in <xref ref-type="bibr" rid="c27">Li and Todorov (2004)</xref>. The two links have lengths <italic>L</italic><sub>1</sub> and <italic>L</italic><sub>2</sub>, masses <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub>, and moments of inertia <italic>I</italic><sub>1</sub> and <italic>I</italic><sub>2</sub> respectively. The lower arm’s center of mass is located a distance <italic>D</italic><sub>2</sub> from the elbow. By considering the geometry of the upper and lower limb, the position of the hand and elbow can be written as vectors <bold><italic>y</italic></bold><sub>h</sub>(<italic>t</italic>) and <bold><italic>y</italic></bold><sub>e</sub> given by
<disp-formula id="eqn6">
<graphic xlink:href="535429v3_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The joint angles <bold><italic>θ</italic></bold> = (<italic>θ</italic><sub>1</sub>; <italic>θ</italic><sub>2</sub>)<sup><italic>T</italic></sup> evolve dynamically according to the differential equation
<disp-formula id="eqn7">
<graphic xlink:href="535429v3_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold><italic>m</italic></bold>(<italic>t</italic>) is the momentary torque vector, <italic>ℳ</italic> is the matrix of inertia, <italic>𝒳</italic> accounts for the centripetal and Coriolis forces, and <italic>ℬ</italic> is a damping matrix representing joint friction. These parameters are given by
<disp-formula id="eqn8">
<graphic xlink:href="535429v3_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn9">
<graphic xlink:href="535429v3_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn10">
<graphic xlink:href="535429v3_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="535429v3_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <italic>a</italic><sub>3</sub> = <italic>I</italic><sub>2</sub>.</p>
</sec>
<sec id="s4c">
<title>iLQR algorithm</title>
<p>Throughout this work, we used the iLQR algorithm (<xref ref-type="bibr" rid="c27">Li and Todorov, 2004</xref>) to find the locally optimal inputs that minimize our cost function. iLQR is a trajectory optimization algorithm that can handle nonlinear dynamics and non-quadratic costs. iLQR works in an iterative manner, by linearizing the dynamics and performing a quadratic approximation of the cost at each iteration, thus turning the control problem into a local linear-quadratic problem whose unique solution is found using LQR (Kalman et al., 1960). The LQR solver uses a highly efficient dynamic programming approach that exploits the sequential structure of the problem. Our implementation of iLQR followed from <xref ref-type="bibr" rid="c27">Li and Todorov (2004)</xref>, with the difference that we performed regularization of the local curvature matrix as recommended by <xref ref-type="bibr" rid="c50">Tassa (2011)</xref>.</p>
</sec>
<sec id="s4d">
<title>Generation of the high-dimensional readouts and networks</title>
<sec id="s4d1">
<title>Generation of inhibitory-stabilized networks</title>
<p>Simulations in <xref rid="fig1" ref-type="fig">Figures 1</xref>, <xref rid="fig3" ref-type="fig">3</xref>, <xref rid="fig5" ref-type="fig">5</xref> and <xref rid="fig6" ref-type="fig">6</xref> were conducted using inhibition-stabilized networks (ISN). Those were generated according to the procedure described in <xref ref-type="bibr" rid="c18">Hennequin et al. (2014)</xref> with minor adjustments. In brief, we initialized strongly connected chaotic networks with sparse and log-normally distributed excitatory weights, and stabilized them through progressive ℋ<sub>2</sub>-optimal adjustments of the inhibitory weights until the spectral abscissa of the connectivity matrix fell below 0.8. This yielded strongly connected but stable networks with a strong degree of non-normality. When considering a larger range of ISNs (<xref rid="fig5" ref-type="fig">Figure 5</xref>), we independently varied both the variance of the distribution of initial excitatory weights and the spectral abscissa below which we stopped optimizing the inhibitory weights.</p>
</sec>
<sec id="s4d2">
<title>Generation of additional networks in <xref rid="fig5" ref-type="fig">Figure 5</xref></title>
<p>To assess the generality of our findings in <xref rid="fig5" ref-type="fig">Figure 5</xref>, we additionally generated randomly connected networks by sampling each weight from a Gaussian distribution with <inline-formula><inline-graphic xlink:href="535429v3_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where the spectral radius <italic>R</italic> was varied between 0 and 0.99. We also sampled skew-symmetric networks by drawing a random network <bold><italic>S</italic></bold> as above, and setting <bold><italic>W</italic></bold> = (<bold><italic>S</italic></bold> <italic>−</italic> <bold><italic>S</italic></bold><sup><italic>T</italic></sup> )<italic>/</italic>2. We varied the radius <italic>R</italic> of the <bold><italic>S</italic></bold> matrices between 0 and 5. Moreover, we considered diagonally shifted skew-symmetric networks <bold><italic>W</italic></bold> = (<bold><italic>S</italic></bold> <italic>−</italic> <bold><italic>S</italic></bold><sup><italic>T</italic></sup> )<italic>/</italic>2 + <italic>λ</italic><bold><italic>I</italic></bold> where <italic>λ</italic> denotes the real part of all the eigenvalues and was varied between 0 and 0.8.</p>
<p>The elements of the readout matrix <bold><italic>C</italic></bold> mapping neural activity onto torques were drawn from a normal distribution with zero mean and standard deviation <inline-formula><inline-graphic xlink:href="535429v3_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This was chosen to ensure that firing rates of standard deviation on the order of 30Hz would be decoded into torques of standard deviation <italic>∼</italic> 2 N/m, which is the natural variation required for the production of the reaches we considered.</p>
</sec>
<sec id="s4d3">
<title>Details of <xref rid="fig4" ref-type="fig">Figure 4</xref></title>
<p>To more easily dissect the phenomena leading to the presence or absence of preparation, we turned to 2D linear networks in <xref rid="fig4" ref-type="fig">Figure 4</xref>. We modelled nonnormal networks with a connectivity <inline-formula><inline-graphic xlink:href="535429v3_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and oscillatory networks with connectivity <inline-formula><inline-graphic xlink:href="535429v3_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The activity of the two units evolved as
<disp-formula id="eqn11">
<graphic xlink:href="535429v3_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and directly influenced the acceleration of a one-dimensional output <italic>y</italic>(<italic>t</italic>) according to
<disp-formula id="eqn12">
<graphic xlink:href="535429v3_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold><italic>C</italic></bold><sub><italic>i</italic></sub> = [cos <italic>θ</italic><sub><italic>C</italic></sub> sin <italic>θ</italic><sub><italic>C</italic></sub>] was a row matrix reading the activity of the network along an angle <italic>θ</italic><sub><italic>C</italic></sub> from the horizontal (first unit). Our setup aimed to mirror the reaching task studied in this work. We thus optimized inputs to minimize the following cost function :
<disp-formula id="eqn13">
<graphic xlink:href="535429v3_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>y</italic><sup><italic>⋆</italic></sup> = 20 was the target position.</p>
</sec>
</sec>
<sec id="s4e">
<title>Computing networks’ controllability and observability to predict preparation in <xref rid="fig5" ref-type="fig">Figure 5</xref></title>
<p>As part of our attempt to predict how much a network will prepare given its intrinsic properties only, we computed the prospective potency of the nullspace <italic>α</italic>, and the controllability of the readout <italic>β</italic>. For a stable linear dynamical system described by
<disp-formula id="eqn14">
<graphic xlink:href="535429v3_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn15">
<graphic xlink:href="535429v3_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
the system’s observability Gramian <bold><italic>Q</italic></bold> can be computed as the unique positive-definite solution of the Lyapunov equation
<disp-formula id="eqn16">
<graphic xlink:href="535429v3_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The prospective potency of the nullspace <bold><italic>C</italic></bold><sup>⊥</sup> is then defined as
<disp-formula id="eqn17">
<graphic xlink:href="535429v3_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Note that this measure <italic>α</italic> is invariant to the specific choice of basis for the nullspace <bold><italic>C</italic></bold><sup>⊥</sup>. Similarly, to assess the controllability of the readout, we first computed the controllability Gramian of the system <bold><italic>P</italic></bold>, which is the solution of
<disp-formula id="eqn18">
<graphic xlink:href="535429v3_eqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <bold><italic>B</italic></bold> = <bold><italic>I</italic></bold> in our system. We then defined the controllability of the readout as
<disp-formula id="eqn19">
<graphic xlink:href="535429v3_eqn19.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4f">
<title>Details of <xref rid="fig6" ref-type="fig">Figure 6</xref></title>
<sec id="s4f1">
<title>Cost function</title>
<p>We modelled sequences of reaches by modifying our cost functional to account for the presence of two targets, as
<disp-formula id="eqn20">
<graphic xlink:href="535429v3_eqn20.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn21">
<graphic xlink:href="535429v3_eqn21.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>τ</italic> describes how long the monkey’s hands had to stay on the intermediate target before performing its second reach. We used <italic>τ</italic> = 600 ms and <italic>α</italic><sub>pause</sub> = 100 for the double reaches in which a pause was explicitely enforced during the experiment. For compound reaches, the experiment did not require monkeys to stop for any specific duration. However, to ensure that the hand stopped on the target in the model (as it does in experiments when monkeys touch the screen) rather than fly through it, we set <italic>τ</italic> = 6 ms and <italic>α</italic><sub>pause</sub> = 100 when modelling compound reaches.</p>
</sec>
<sec id="s4f2">
<title>Preparatory subspace analysis</title>
<p><xref ref-type="bibr" rid="c25">Lara et al. (2018)</xref> proposed an analysis to identify preparatory and movement-related subspaces. This analysis allows to assess when the neural activity enters those subspaces, independently of whether it is delay-period or post-go-cue activity.</p>
<p>The method identifies a set of preparatory dimensions and a set of movement dimensions, constrained to be orthogonal to one another, as in <xref ref-type="bibr" rid="c12">Elsayed et al. (2016)</xref>. These are found in the following manner: the trial-averaged neural activity is split between preparatory and movement-related epochs, yielding two matrices of size <italic>N MT</italic> where <italic>N</italic> is the number of neurons, <italic>T</italic> is the number of time bins and <italic>M</italic> is the number of reaches. One then optimizes the <inline-formula><inline-graphic xlink:href="535429v3_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="535429v3_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (where <italic>d</italic><sub>prep</sub> and <italic>d</italic><sub>mov</sub> are the predefined dimensions of the two subspaces) such that the subspaces respectively capture most variance in the preparatory and movement activities, while being orthogonal to one another. This is achieved by maximizing the following objective :
<disp-formula id="eqn22">
<graphic xlink:href="535429v3_eqn22.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>C</italic><sub>prep<italic>/</italic>mov</sub> are the covariance matrices of the neural activity during the preparatory and movement epochs, respectively. The normalizing constant <italic>Z</italic><sub>prep</sub>(<italic>d</italic><sub>prep</sub>) denotes the maximum amount of variance in preparatory activity that can be captured by any subspace of dimension <italic>d</italic><sub>prep</sub> (this is found via SVD), and similarly for <italic>Z</italic><sub>mov</sub>(<italic>d</italic><sub>mov</sub>). The objective is maximized under the constraints <inline-formula><inline-graphic xlink:href="535429v3_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="535429v3_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We set subspace dimensions <italic>d</italic><sub>prep</sub> = <italic>d</italic><sub>mov</sub> = 6, although our results were robust to this choice.</p>
<p>The occupancy of the preparatory subspace was defined as
<disp-formula id="ueqn1">
<graphic xlink:href="535429v3_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and that of the movement subspace was defined as
<disp-formula id="ueqn2">
<graphic xlink:href="535429v3_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For single reaches, we defined preparatory epoch responses as the activity in a 300ms window before the end of the delay period, and movement epoch responses as the activity in a 300ms window starting 50ms after the go cue. We normalized all neural activity traces using the same procedure as <xref ref-type="bibr" rid="c4">Churchland et al. (2012)</xref>; <xref ref-type="bibr" rid="c12">Elsayed et al. (2016)</xref>. For double reaches, we followed <xref ref-type="bibr" rid="c54">Zimnik and Churchland (2021)</xref> and used neural activity traces from both single reaches and the first reach of double-reach sequences. Note that we did not include any activity from the second reaches in the sequence, or from compound reaches, when defining the subspaces.</p>
</sec>
</sec>
<sec id="s4g">
<title>Parameter table</title>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Parameters used for the various simulations.</title></caption>
<graphic xlink:href="535429v3_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We are grateful to Matthew T. Kaufman and Mark M. Churchland for sharing data for the monkey experiments. We thank Kristopher Jensen, David Liu, Javier Antorán, and Rory Byrne for helpful comments on the manuscript. M.S. was funded by an EPSRC DTP studentship, and part of this work was performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service (<ext-link ext-link-type="uri" xlink:href="http://www.hpc.cam.ac.uk">http://www.hpc.cam.ac.uk</ext-link>) funded by EPSRC Tier-2 capital grant EP/P020259/1. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising from this submission.</p>
</ack>
<sec id="s5">
<title>Competing interests</title>
<p>T-C.K. is currently a research scientist at Meta Reality Labs, but only contributed to this work while studying at the University of Cambridge.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Afshar</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Santhanam</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Byron</surname>, <given-names>M. Y.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name>, <string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name> (<year>2011</year>). <article-title>Single-trial neural correlates of arm movement preparation</article-title>. <source>Neuron</source>, <volume>71</volume>(<issue>3</issue>):<fpage>555</fpage>–<lpage>564</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="other"><string-name><surname>Amos</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Jimenez</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Sacks</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Boots</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Kolter</surname>, <given-names>J. Z.</given-names></string-name> (<year>2018</year>). <article-title>Differentiable mpc for end-to-end planning and control</article-title>. <source>In Advances in Neural Information Processing Systems</source>, pages <fpage>8289</fpage>–<lpage>8300</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name> (<year>2000</year>). <article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>J. Comput. Neurosci</source>., <volume>8</volume>:<fpage>183</fpage>–<lpage>208</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Cunningham</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Foster</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Nuyujukian</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name>, and <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name> (<year>2012</year>). <article-title>Neural population dynamics during reaching</article-title>. <source>Nature</source>, <volume>487</volume>(<issue>7405</issue>):<fpage>51</fpage>–<lpage>56</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Cunningham</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name>, and <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name> (<year>2010</year>). <article-title>Cortical preparatory activity: representation of movement or first cog in a dynamical machine?</article-title> <source>Neuron</source>, <volume>68</volume>(<issue>3</issue>):<fpage>387</fpage>–<lpage>400</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name> and <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name> (<year>2007</year>). <article-title>Delay of movement caused by disruption of cortical preparatory activity</article-title>. <source>Journal of neurophysiology</source>, <volume>97</volume>(<issue>1</issue>):<fpage>348</fpage>–<lpage>359</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="other"><string-name><surname>Codol</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Michaels</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Kashefi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pruszynski</surname>, <given-names>J. A.</given-names></string-name>, and <string-name><surname>Gribble</surname>, <given-names>P. L.</given-names></string-name> (<year>2023</year>). <source>Motornet: a python toolbox for controlling differentiable biomechanical effectors with artificial neural networks</source>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="book"><string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name> and <string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name> (<year>2001</year>). <source>Theoretical neuroscience</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="other"><string-name><surname>Driscoll</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Shenoy</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Sussillo</surname>, <given-names>D.</given-names></string-name> (<year>2022</year>). <article-title>Flexible multitask computation in recurrent networks utilizes shared dynamical motifs</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="other"><string-name><surname>Dubreuil</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Valente</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Beiran</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mastrogiuseppe</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Ostojic</surname>, <given-names>S.</given-names></string-name> (<year>2021</year>). <article-title>The role of population structure in computations through neural dynamics</article-title>. <source>bioRxiv</source>, pages <fpage>2020</fpage>–<lpage>07</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Dum</surname>, <given-names>R. P.</given-names></string-name> and <string-name><surname>Strick</surname>, <given-names>P. L.</given-names></string-name> (<year>1991</year>). <article-title>The origin of corticospinal projections from the premotor areas in the frontal lobe</article-title>. <source>Journal of Neuroscience</source>, <volume>11</volume>(<issue>3</issue>):<fpage>667</fpage>–<lpage>689</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Elsayed</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Lara</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name>, and <string-name><surname>Cunningham</surname>, <given-names>J. P.</given-names></string-name> (<year>2016</year>). <article-title>Reorganization between preparatory and movement population responses in motor cortex</article-title>. <source>Nature communications</source>, <volume>7</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Goldman</surname>, <given-names>M. S.</given-names></string-name> (<year>2009</year>). <article-title>Memory without feedback in a neural network</article-title>. <source>Neuron</source>, <volume>61</volume>:<fpage>621</fpage>–<lpage>634</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Guo</surname>, <given-names>Z. V.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Huber</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ophir</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Gutnisky</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ting</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name> (<year>2014</year>). <article-title>Flow of cortical activity underlying a tactile decision in mice</article-title>. <source>Neuron</source>, <volume>81</volume>:<fpage>179</fpage>–<lpage>194</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Harris</surname>, <given-names>C. M.</given-names></string-name> and <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name> (<year>1998</year>). <article-title>Signal-dependent noise determines motor planning</article-title>. <source>Nature</source>, <volume>394</volume>(<issue>6695</issue>):<fpage>780</fpage>–<lpage>784</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="other"><string-name><surname>Heald</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wolpert</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Lengyel</surname>, <given-names>M.</given-names></string-name> (<year>2023</year>). <article-title>The computational and neural bases of context-dependent learning</article-title>. <source>Ann. Rev. Neurosci</source>., qq:<fpage>1</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Hennequin</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Vogels</surname>, <given-names>T. P.</given-names></string-name>, and <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name> (<year>2012</year>). <article-title>Nonnormal amplification in random balanced neuronal networks</article-title>. <source>Physical Review E</source>, <volume>86</volume>(<issue>1</issue>):<fpage>011909</fpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Hennequin</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Vogels</surname>, <given-names>T. P.</given-names></string-name>, and <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name> (<year>2014</year>). <article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title>. <source>Neuron</source>, <volume>82</volume>(<issue>6</issue>):<fpage>1394</fpage>–<lpage>1406</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Kalidindi</surname>, <given-names>H. T.</given-names></string-name>, <string-name><surname>Cross</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Lillicrap</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Omrani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Falotico</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Sabes</surname>, <given-names>P. N.</given-names></string-name>, and <string-name><surname>Scott</surname>, <given-names>S. H.</given-names></string-name> (<year>2021</year>). <article-title>Rotational dynamics in motor cortex are consistent with a feedback controller</article-title>. <source>Elife</source>, <volume>10</volume>:<fpage>e67256</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Kalman</surname>, <given-names>R. E.</given-names></string-name> <etal>et al.</etal> (<year>1960</year>). <article-title>Contributions to the theory of optimal control</article-title>. <source>Bol. soc. mat. mexicana</source>, <volume>5</volume>(<issue>2</issue>):<fpage>102</fpage>–<lpage>119</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Kao</surname>, <given-names>T.-C.</given-names></string-name> and <string-name><surname>Hennequin</surname>, <given-names>G.</given-names></string-name> (<year>2019</year>). <article-title>Neuroscience out of control: control-theoretic perspectives on neural circuit dynamics</article-title>. <source>Cur. Op. Neurobiol</source>., <volume>58</volume>:<fpage>122</fpage>–<lpage>129</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Kao</surname>, <given-names>T.-C.</given-names></string-name>, <string-name><surname>Sadabadi</surname>, <given-names>M. S.</given-names></string-name>, and <string-name><surname>Hennequin</surname>, <given-names>G.</given-names></string-name> (<year>2021</year>). <article-title>Optimal anticipatory control as a theory of motor preparation: a thalamo-cortical circuit model</article-title>. <source>Neuron</source>, <volume>109</volume>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name>, and <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name> (<year>2014</year>). <article-title>Cortical activity in the null space: permitting preparation without movement</article-title>. <source>Nature neuroscience</source>, <volume>17</volume>(<issue>3</issue>):<fpage>440</fpage>–<lpage>448</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Seely</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Sussillo</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name>, <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name>, and <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name> (<year>2016</year>). <article-title>The largest response component in the motor cortex reflects movement timing but not movement type</article-title>. <source>Eneuro</source>, <volume>3</volume>(<issue>4</issue>).</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Lara</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Elsayed</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Zimnik</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Cunningham</surname>, <given-names>J. P.</given-names></string-name>, and <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name> (<year>2018</year>). <article-title>Conservation of preparatory neural events in monkey motor cortex regardless of how movement is initiated</article-title>. <source>Elife</source>, <volume>7</volume>:<fpage>e31826</fpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>T.-W.</given-names></string-name>, <string-name><surname>Guo</surname>, <given-names>Z. V.</given-names></string-name>, <string-name><surname>Gerfen</surname>, <given-names>C. R.</given-names></string-name>, and <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name> (<year>2015</year>). <article-title>A motor cortex circuit for motor planning and movement</article-title>. <source>Nature</source>, <volume>519</volume>:<fpage>51</fpage>–<lpage>56</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="other"><string-name><surname>Li</surname>, <given-names>W.</given-names></string-name> and <string-name><surname>Todorov</surname>, <given-names>E.</given-names></string-name> (<year>2004</year>). <article-title>Iterative linear quadratic regulator design for nonlinear biological movement systems</article-title>. <source>In ICINCO (1)</source>, pages <fpage>222</fpage>–<lpage>229</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Logiaco</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Abbott</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Escola</surname>, <given-names>S.</given-names></string-name> (<year>2021</year>). <article-title>Thalamic control of cortical dynamics in a model of flexible motor sequencing</article-title>. <source>Cell Reports</source>, <volume>35</volume>(<issue>9</issue>):<fpage>109090</fpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="other"><string-name><surname>Malonis</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Hatsopoulos</surname>, <given-names>N. G.</given-names></string-name>, <string-name><surname>MacLean</surname>, <given-names>J. N.</given-names></string-name>, and <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name> (<year>2021</year>). <article-title>M1 dynamics share similar inputs for initiating and correcting movement</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Meirhaeghe</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Riehle</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Brochier</surname>, <given-names>T.</given-names></string-name> (<year>2023</year>). <article-title>Parallel movement planning is achieved via an optimal preparatory state in motor cortex</article-title>. <source>Cell Reports</source>, <volume>42</volume>(<issue>2</issue>):<fpage>112136</fpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Michaels</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Dann</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Intveld</surname>, <given-names>R. W.</given-names></string-name>, and <string-name><surname>Scherberger</surname>, <given-names>H.</given-names></string-name> (<year>2015</year>). <article-title>Predicting reaction time from the neural state space of the premotor and parietal grasping network</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>32</issue>):<fpage>11415</fpage>–<lpage>11432</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Miri</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Warriner</surname>, <given-names>C. L.</given-names></string-name>, <string-name><surname>Seely</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Elsayed</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Cunningham</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name>, and <string-name><surname>Jessell</surname>, <given-names>T. M.</given-names></string-name> (<year>2017</year>). <article-title>Behaviorally selective engagement of short-latency effector pathways by motor cortex</article-title>. <source>Neuron</source>, <volume>95</volume>:<fpage>683</fpage>–<lpage>696</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Murphy</surname>, <given-names>B. K.</given-names></string-name> and <string-name><surname>Miller</surname>, <given-names>K. D.</given-names></string-name> (<year>2009</year>). <article-title>Balanced amplification: a new mechanism of selective amplification of neural activity patterns</article-title>. <source>Neuron</source>, <volume>61</volume>:<fpage>635</fpage>–<lpage>648</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Pandarinath</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>O’Shea</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Collins</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jozefowicz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Stavisky</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Kao</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Trautmann</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name>, <string-name><surname>Hochberg</surname>, <given-names>L. R.</given-names></string-name>, <etal>et al.</etal> (<year>2018</year>). <article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title>. <source>Nature methods</source>, <volume>15</volume>(<issue>10</issue>):<fpage>805</fpage>–<lpage>815</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Prut</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Fetz</surname>, <given-names>E. E.</given-names></string-name> (<year>1999</year>). <article-title>Primate spinal interneurons show pre-movement instructed delay activity</article-title>. <source>Nature</source>, <volume>401</volume>(<issue>6753</issue>):<fpage>590</fpage>–<lpage>594</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="book"><string-name><surname>Rawlings</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Mayne</surname>, <given-names>D. Q.</given-names></string-name>, and <string-name><surname>Diehl</surname>, <given-names>M.</given-names></string-name> (<year>2017</year>). <source>Model predictive control: theory, computation, and design</source>, volume <volume>2</volume>. <publisher-name>Nob Hill Publishing Madison, WI</publisher-name>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Riehle</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Requin</surname>, <given-names>J.</given-names></string-name> (<year>1989</year>). <article-title>Monkey primary motor and premotor cortex: single-cell activity related to prior information about direction and extent of an intended movement</article-title>. <source>Journal of neurophysiology</source>, <volume>61</volume>(<issue>3</issue>):<fpage>534</fpage>–<lpage>549</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Russo</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Bittner</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Perkins</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Seely</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>London</surname>, <given-names>B. M.</given-names></string-name>, <string-name><surname>Lara</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Miri</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Marshall</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Kohn</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jessell</surname>, <given-names>T. M.</given-names></string-name>, <etal>et al.</etal> (<year>2018</year>). <article-title>Motor cortex embeds muscle-like commands in an untangled population response</article-title>. <source>Neuron</source>, <volume>97</volume>(<issue>4</issue>):<fpage>953</fpage>–<lpage>966</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Sauerbrei</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Guo</surname>, <given-names>J.-Z.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Mischiati</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Guo</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Kabra</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Verma</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mensh</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Hantman</surname>, <given-names>A. W.</given-names></string-name> (<year>2020</year>). <article-title>Cortical pattern generation during dexterous movement is input-driven</article-title>. <source>Nature</source>, <volume>577</volume>(<issue>7790</issue>):<fpage>386</fpage>–<lpage>391</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="other"><string-name><surname>Schimel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kao</surname>, <given-names>T.-C.</given-names></string-name>, <string-name><surname>Jensen</surname>, <given-names>K. T.</given-names></string-name>, and <string-name><surname>Hennequin</surname>, <given-names>G.</given-names></string-name> (<year>2022</year>). <article-title>iLQR-VAE : control-based learning of inputdriven dynamics with applications to neural data</article-title>. <source>In International Conference on Learning Representations</source>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Sheahan</surname>, <given-names>H. R.</given-names></string-name>, <string-name><surname>Franklin</surname>, <given-names>D. W.</given-names></string-name>, and <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name> (<year>2016</year>). <article-title>Motor planning, not execution, separates motor memories</article-title>. <source>Neuron</source>, <volume>92</volume>(<issue>4</issue>):<fpage>773</fpage>–<lpage>779</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name>, <string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name> (<year>2013</year>). <article-title>Cortical control of arm movements: a dynamical systems perspective</article-title>. <source>Annual review of neuroscience</source>, <volume>36</volume>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="book"><string-name><surname>Skogestad</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Postlethwaite</surname>, <given-names>I.</given-names></string-name> (<year>2007</year>). <source>Multivariable feedback control: analysis and design</source>, volume <volume>2</volume>. <publisher-name>Wiley New York</publisher-name>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="other"><string-name><surname>Sohn</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Meirhaeghe</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Rajalingham</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Jazayeri</surname>, <given-names>M.</given-names></string-name> (<year>2020</year>). <article-title>A network perspective on sensorimotor learning</article-title>. <source>Trends in Neurosciences</source>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="other"><string-name><surname>Soldado-Magraner</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mante</surname>, <given-names>V.</given-names></string-name>, and <string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name> (<year>2023</year>). <article-title>Inferring context-dependent computations through linear approximations of prefrontal cortex dynamics</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="book"><string-name><surname>Sterling</surname>, <given-names>P.</given-names></string-name> and <string-name><surname>Laughlin</surname>, <given-names>S.</given-names></string-name> (<year>2015</year>). <source>Principles of neural design</source>. <publisher-name>MIT press</publisher-name>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Stroud</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Porter</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Hennequin</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Vogels</surname>, <given-names>T. P.</given-names></string-name> (<year>2018</year>). <article-title>Motor primitives in space and time via targeted gain modulation in cortical networks</article-title>. <source>Nature neuroscience</source>, <volume>21</volume>(<issue>12</issue>):<fpage>1774</fpage>–<lpage>1783</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="other"><string-name><surname>Sun</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>O’Shea</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Golub</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Trautmann</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Vyas</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name>, and <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name> (<year>2022</year>). <article-title>Cortical preparatory activity indexes learned motor memories</article-title>. <source>Nature</source>, pages <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Sussillo</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, and <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name> (<year>2015</year>). <article-title>A neural network that finds a naturalistic solution for the production of muscle activity</article-title>. <source>Nature neuroscience</source>, <volume>18</volume>(<issue>7</issue>):<fpage>1025</fpage>–<lpage>1033</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="book"><string-name><surname>Tassa</surname>, <given-names>Y.</given-names></string-name> (<year>2011</year>). <source>Theory and Implementation of Biomimetic Motor Controllers</source>. <publisher-name>Hebrew University of Jerusalem</publisher-name>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Todorov</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Jordan</surname>, <given-names>M. I.</given-names></string-name> (<year>2002</year>). <article-title>Optimal feedback control as a theory of motor coordination</article-title>. <source>Nature neuroscience</source>, <volume>5</volume>(<issue>11</issue>):<fpage>1226</fpage>–<lpage>1235</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="book"><string-name><surname>Todorov</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Li</surname>, <given-names>W.</given-names></string-name> (<year>2003</year>). <chapter-title>Optimal control methods suitable for biomechanical systems</chapter-title>. <source>In Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat. No. 03CH37439)</source>, volume <volume>2</volume>, pages <fpage>1758</fpage>–<lpage>1761</lpage>. <publisher-name>IEEE</publisher-name>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Yeo</surname>, <given-names>S.-H.</given-names></string-name>, <string-name><surname>Franklin</surname>, <given-names>D. W.</given-names></string-name>, and <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name> (<year>2016</year>). <article-title>When optimal feedback control is not enough: Feedforward strategies are required for optimal control with active sensing</article-title>. <source>PLoS computational biology</source>, <volume>12</volume>(<issue>12</issue>):<fpage>e1005190</fpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Zimnik</surname>, <given-names>A. J.</given-names></string-name> and <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name> (<year>2021</year>). <article-title>Independent generation of sequence elements by motor cortex</article-title>. <source>Nature neuroscience</source>, <volume>24</volume>(<issue>3</issue>):<fpage>412</fpage>–<lpage>424</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s6">
<title>Supplementary material</title>
<sec id="s6a">
<label>S1</label>
<title>Choice of the hyperparameters of the model</title>
<p>Our cost function for the delayed single-reaching task was composed of 3 components. The relative weighings of the different terms in our cost, which are hyperparameters of the model, affect the way in which the task is solved. To ensure robustness of our results to hyperparameter changes, we scanned the space of <italic>α</italic><sub>null</sub> and <italic>α</italic><sub>effort</sub> (as the solution is invariant to scaling of the cost, only those relative weighings matter), and evaluated the solutions found across this hyperparameter space for a delayed reach of 300 ms.</p>
<p>Our evaluation was based on multiple criteria. We considered the target to have been successfully reached if the mean distance to the target in the last 200 ms of the movement was lower than 5 mm (for a reach radius of 12 cm). We considered that the requirement to stay still during the delay period was satisfied if the mean torques during preparation were smaller than 0.02 N<italic>/</italic>m. We computed the preparation index and total cost as described in <xref ref-type="disp-formula" rid="eqn5">Equation (5)</xref> and <xref ref-type="disp-formula" rid="eqn4">Equation (4)</xref>. We moreover computed the total input energy per neuron <inline-formula><inline-graphic xlink:href="535429v3_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and the maximum velocity as <inline-formula><inline-graphic xlink:href="535429v3_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. These various quantities are shown for a range of hyperparameters in <xref rid="figS1" ref-type="fig">Figure S1</xref>, with the choice of hyperparameters used throughout our simulations marked with a red star. This shows that the behaviour of the model is consistent across a range of hyperparameter settings around the one we used.</p>
<p>In <xref rid="figS2" ref-type="fig">Figure S2</xref>, we illustrate the output of the model for several hyperparameter settings. One can notice that for very small values of <italic>α</italic><sub>effort</sub> the reach is successful, but executed with larger torques and velocity than is necessary – e.g the red and yellow reaches are equally successful but the red one is much faster – which comes at the cost of larger inputs. We chose the set of hyperparameters for our simulations such as to lie in an intermediate regime in which the task is solved successfully, but without requiring more inputs than necessary.</p>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1:</label>
<caption><p>Correlates of the behaviour and control strategy across a wide range of hyperparameters. The “reach success” and “holding success” are set to 1 if the success criterion (see text) is satisfied and 0 otherwise. The task is executed successfully over a wide range of hyperparameters. The red star denotes the set of hyperparameters used in the main text simulations. This configuration was chosen to lie in a region in which the task can be successfully solved, with the performance being robust to small changes in the hyperparameters.</p></caption>
<graphic xlink:href="535429v3_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2:</label>
<caption><p>Illustration of the behaviour for several hyperparameter settings. (Left) Hand position along the horizontal axis, with the dotted line denoting the position of the target. (Middle) Temporal profile of the hand velocity. (Right) Temporal profile of the torques driving the hand.</p></caption>
<graphic xlink:href="535429v3_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s6b">
<label>S2</label>
<title>Investigation of the effect of the network decay timescale</title>
<p><xref rid="fig3" ref-type="fig">Figure 3</xref> highlighted that preparatory inputs tend to consistently arise late during the delay period. We hypothesized that this may be a reflection of the intrinsic tendency of the network dynamics to decay, such that inputs given too early may be “lost”. To test this, we changed the characteristic timescale of the dynamics <italic>during preparation only</italic>, leading to the following dynamics:
<disp-formula id="eqnS1">
<graphic xlink:href="535429v3_eqnS1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>τ</italic><sub>mov</sub> = 150ms. This allowed us to evaluate whether having dynamics decaying more slowly during preparation led to inputs starting earlier. Note that we also rescaled the inputs during preparation by <inline-formula><inline-graphic xlink:href="535429v3_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, to ensure that the effective cost of the inputs was not affected by the timescale change.</p>
<p>As shown in <xref rid="figS3" ref-type="fig">Figure S3</xref>, inputs started rising earlier when the network’s decay timescale was longer. This was consistent with the hypothesis that the length of the window of preparation that the optimal controller uses depends on the network’s intrinsic timescale.</p>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3:</label>
<caption><title>Illustration of the effect of the characteristic neuronal timescale on the temporal distribution of the inputs.</title>
<p>We modified the characteristic neuronal timescale of the ISN during preparation only and assessed how that changed the temporal distribution of inputs for 3 different timescales (<italic>τ</italic><sub>prep</sub> = 50ms, <italic>τ</italic><sub>prep</sub> = 150ms, <italic>τ</italic><sub>prep</sub> = 300ms, top to bottom). As hypothesized, inputs start earlier during the preparation window when the decay timescale of the network was longer.</p></caption>
<graphic xlink:href="535429v3_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s6c">
<label>S3</label>
<title>Additional results in the 2D system</title>
<p>Our visualization of the behaviour of 2D networks in <xref rid="fig4" ref-type="fig">Figure 4</xref> allowed us to identify features of the dynamics that were well-suited to predicting preparation. Below, we compute <italic>α</italic> and <italic>β</italic> numerically and analytically in 2D oscillatory and nonnormal networks, to gain insights into how these quantities vary with the networks’ dynamics. We then show how preparation can be predicted highly accurately across a large number of 2D systems, using only those quantities to summarize the network dynamics.</p>
<sec id="s6c1">
<label>S3.1</label>
<title>Controllability and observability computations</title>
<p>In <xref rid="figS4" ref-type="fig">Figure S4</xref>, we computed <italic>α</italic> and <italic>β</italic> numerically, as a function of the connectivity strength and the choice of readout, for the nonnormal and the oscillatory motifs shown in <xref rid="fig4" ref-type="fig">Figure 4</xref>. This highlights the very different behaviours of the two networks, which are to some extent also reflected in higher-dimensional models. In particular, we find a strong effect of the alignment between the readout and the network dynamics in nonnormal networks, while <italic>α</italic> and <italic>β</italic> are independent of <italic>θ</italic><sub><italic>C</italic></sub> in oscillatory networks. Interestingly, we see that <italic>β</italic> is constant across all oscillatory networks, while <italic>α</italic> increases with <italic>w</italic>.</p>
<p>As the reduced 2D model is more amenable to mathematical analysis than its high-dimensional counterpart, we can gain further insights into the origin of these differences by computing <italic>α</italic>(<italic>w, θ</italic><sub><italic>C</italic></sub>) and <italic>β</italic>(<italic>w, θ</italic><sub><italic>C</italic></sub>) analytically.</p>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4:</label>
<caption><p>Illustration of <italic>α</italic> and <italic>β</italic> as a function of <italic>θ</italic><sub><italic>C</italic></sub> and <italic>w</italic> in the 2D networks.</p></caption>
<graphic xlink:href="535429v3_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Recall that the observability Gramian Q of a linear input-driven dynamical system satisfies
<disp-formula id="eqnS2">
<graphic xlink:href="535429v3_eqnS2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and the controllability Gramian satisfies
<disp-formula id="eqnS3">
<graphic xlink:href="535429v3_eqnS3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and that we defined <inline-formula><inline-graphic xlink:href="535429v3_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <italic>β</italic> = Tr(<bold><italic>CP C</italic></bold><sup><italic>T</italic></sup> ) where <bold><italic>C</italic></bold><sup>⊥</sup> denotes the nullspace of the readout matrix. Below, we compute these quantities for the 2D oscillatory and nonnormal networks, with <bold><italic>B</italic></bold> = <bold><italic>I</italic></bold> and <bold><italic>C</italic></bold> a unit-norm vector whose direction we parametrize via a quantity <italic>θ</italic><sub><italic>C</italic></sub>. Note that we ignore the effect of dt and <italic>τ</italic> in the mathematical analysis, as those quantities can straightforwardly be included in the final result via a rescaling of <italic>w</italic> and <bold><italic>B</italic></bold>.</p>
<sec id="s6c1a">
<title>Oscillatory network</title>
<p>In the case of <bold><italic>A</italic></bold> = <italic>−</italic><bold><italic>I</italic></bold> + <bold><italic>S</italic></bold> where <bold><italic>S</italic></bold> is a skew-symmetric network (i.e <bold><italic>S</italic></bold><sup><italic>T</italic></sup> = <italic>−</italic><bold><italic>S</italic></bold>), <xref ref-type="disp-formula" rid="eqnS3">Equation (S3)</xref> is solved by <bold><italic>P</italic></bold> = <bold><italic>I</italic></bold><italic>/</italic>2 independently of the value of <bold><italic>S</italic></bold>. This explains why <inline-formula><inline-graphic xlink:href="535429v3_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is independent of both the connectivity strength <italic>w</italic> and the orientation of the readout <italic>θ</italic><sub><italic>C</italic></sub> for skew-symmetric networks (see <xref rid="figS4" ref-type="fig">Figure S4</xref>; bottom right). Practically, this means that skew-symmetric networks are equally controllable in all directions: when driven by random inputs, these networks display isotropic activity of equal variance along all directions. Moreover, as <italic>w</italic> controls the oscillation frequency of the network, but does not change the decay timescale of the eigenmodes, the amount of variance generated by a random stimulation is independent of <italic>w</italic>. Interestingly, we can see in <xref rid="figS4" ref-type="fig">Figure S4</xref> (top right) that <italic>α</italic> displays a different behaviour, and increases with <italic>w</italic>. As highlighted above, skew-symmetric systems are rotationally symmetric. Without loss of generality, we can thus define our 1D vector to read out the first unit, i.e <bold><italic>C</italic></bold> = [1 0].</p>
<p>The observability Gramian must satisfy
<disp-formula id="eqnS4">
<graphic xlink:href="535429v3_eqnS4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This can be found in closed-form by solving the 2D system of equations, yielding
<disp-formula id="eqnS5">
<graphic xlink:href="535429v3_eqnS5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
From there, we obtain <inline-formula><inline-graphic xlink:href="535429v3_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. As found empirically, this quantity will initially increase before plateauing towards 1<italic>/</italic>4 as <italic>w</italic> becomes large.</p>
<p>One might wonder why observability displays such a dependence on the oscillatory frequency of the network, even though the network is rotationally symmetric, and <italic>w</italic> does not affect the decay timescale. As highlighted in <xref ref-type="disp-formula" rid="eqnS2">Equation (S2)</xref>, controllability and observability Gramian would be identical for a skew-symmetric system if <bold><italic>C</italic></bold> = <bold><italic>I</italic></bold>. However, a feature of the systems we consider is the existence of a nullspace, i.e the fact that the readout <bold><italic>C</italic></bold> only targets a subset of dimensions across the whole space (implying that <bold><italic>C</italic></bold><sup><italic>T</italic></sup> <bold><italic>C</italic></bold> is a low-rank matrix). Intuitively, the reason why <italic>α</italic> increases with <italic>w</italic> while <italic>β</italic> is constant in skew-symmetric networks can be understood as follows: <italic>α</italic> is computing how much <italic>readout activity</italic> a vector initialized in the nullspace of <bold><italic>C</italic></bold> will generate, while <italic>β</italic> is computing the amount of energy that will be generated <italic>across all directions</italic> by a vector initialized in the readout space. Thus, assuming once again <bold><italic>C</italic></bold> = [1 0] and <bold><italic>C</italic></bold><sup>⊥</sup> = [0 1], the activity of vectors initialized along <bold><italic>C</italic></bold> and <bold><italic>C</italic></bold><sup>⊥</sup> respectively and evolving autonomously from there is given by <bold><italic>v</italic></bold><sub><italic>C</italic></sub>(<italic>t</italic>) = [<italic>e</italic><sup><italic>−t</italic></sup> cos(<italic>wt</italic>) <italic>e</italic><sup><italic>−t</italic></sup> sin(<italic>wt</italic>)] and <inline-formula><inline-graphic xlink:href="535429v3_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
<p>From there, we can compute <inline-formula><inline-graphic xlink:href="535429v3_inline19.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Thus, as found above, only the decay timescale of the envelope (fixed to 1 here) affects the value of <italic>β</italic>.</p>
<p>Importantly, <italic>α</italic> will instead have a dependence on <italic>w</italic> arising from the fact that it depends on the size of the activity <italic>projected into the readout</italic>, as
<disp-formula id="eqnS6">
<graphic xlink:href="535429v3_eqnS6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqnS7">
<graphic xlink:href="535429v3_eqnS7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqnS8">
<graphic xlink:href="535429v3_eqnS8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqnS9">
<graphic xlink:href="535429v3_eqnS9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqnS10">
<graphic xlink:href="535429v3_eqnS10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The dependence of this quantity on <italic>w</italic> can be understood by the fact that activity patterns initialized in the readout nullspace benefit from the existence of rotational dynamics, which allows them to be read-out before the activity decays completely.</p>
</sec>
<sec id="s6c1b">
<title>Nonnormal network</title>
<p>In the nonnormal network, we have <inline-formula><inline-graphic xlink:href="535429v3_inline20.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The nonnormal 2D system, unlike its oscillatory counterpart, does not have rotational symmetry. Thus, to]remain general, we will consider <bold><italic>C</italic></bold>(<italic>θ</italic><sub><italic>C</italic></sub>) = [cos <italic>θ</italic><sub><italic>C</italic></sub> sin <italic>θ</italic><sub><italic>C</italic></sub>], and <bold><italic>C</italic></bold><sup>⊥</sup>(<italic>θ</italic><sub><italic>C</italic></sub>) = [<italic>−</italic> sin <italic>θ</italic><sub><italic>C</italic></sub> cos <italic>θ</italic><sub><italic>C</italic></sub>]. Solving <xref ref-type="disp-formula" rid="eqnS3">Equation (S3)</xref> for <bold><italic>B</italic></bold> = <bold><italic>I</italic></bold> leads to an expression for the controllability Gramian of the nonnormal system as
<disp-formula id="eqnS11">
<graphic xlink:href="535429v3_eqnS11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Similarly, computation of the observability Gramian leads to
<disp-formula id="eqnS12">
<graphic xlink:href="535429v3_eqnS12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We can then compute
<disp-formula id="eqnS13">
<graphic xlink:href="535429v3_eqnS13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and
<disp-formula id="eqnS14">
<graphic xlink:href="535429v3_eqnS14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This highlights the dependence of <italic>α</italic> and <italic>β</italic> on <italic>θ</italic><sub><italic>C</italic></sub>, which can also be seen in <xref rid="figS4" ref-type="fig">Figure S4</xref> (left). Interestingly, these expressions also make evident the supralinear scaling of <italic>α</italic> and <italic>β</italic> with <italic>w</italic> in nonnormal networks. Note however that we never investigate preparation in the very large <italic>w</italic> regime, as the simulation of such networks with discretized dynamics is prone to numerical issues.</p>
</sec>
</sec>
<sec id="s6c2">
<label>S3.2</label>
<title>Predicting preparation in 2D networks</title>
<p>To assess how well preparation could be predicted from the control-theoretic properties <italic>α</italic> and <italic>β</italic> (c.f. main text) of 2D networks, we generated 20000 networks with weight matrix
<disp-formula id="eqnS15">
<graphic xlink:href="535429v3_eqnS15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>a ∼ 𝒰</italic> (0, 0.8), <italic>ω ∼ 𝒰</italic> (0, 4), and <italic>w</italic><sub>ff</sub> <italic>∼ 𝒰</italic> (0, 4). <xref ref-type="disp-formula" rid="eqnS15">Equation (S15)</xref> implies that <bold><italic>W</italic></bold> has a pair of complex-conjugate eigenvalues <italic>a ± iω</italic>, and also embeds a feedforward coupling of strength <italic>w</italic><sub>ff</sub> from the second to the first dimension. For each network configuration, we computed the corresponding values of <italic>α</italic> and <italic>β</italic>. To confirm our intuition that the preparation index should increase with <italic>α</italic> and decrease with <italic>β</italic>, we first attempted to fit prep. index <inline-formula><inline-graphic xlink:href="535429v3_inline21.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Interestingly, we found that while this quantity was positively correlated with the preparation index across networks, a substantial fraction of variance remained unexplained (test <italic>R</italic><sup>2</sup> = 0.16). Labelling the preparation index by the rotational frequency of the network highlighted that a substantial fraction of the variance across networks came from this timescale of oscillations (<xref rid="figS5" ref-type="fig">Figure S5</xref>, left). Indeed, a regression model of the for prep. index = <inline-formula><inline-graphic xlink:href="535429v3_inline22.gif" mimetype="image" mime-subtype="gif"/></inline-formula> captured 80% of the variance in preparation index, yielding an accurate fit across networks with only two free parameters (<xref rid="figS5" ref-type="fig">Figure S5</xref>, right).</p>
<p>We stress that the predictive power of these simple fits is remarkable given that the preparation index comes out of a complex process of optimization over control inputs. Thus, the control-theoretic quantities <italic>α</italic> and <italic>β</italic> appear to appropriately summarize the benefits of preparation for individual networks.</p>
<p>The fact that the preparation index also grows with <italic>ω</italic> can be understood by considering the alignment between the activity trajectories which the network can autonomously generate and those that are required for solving the motor task. Indeed, a network that is intrinsically unable to generate outputs with the right oscillatory timescale would have to rely on movement-related inputs, i.e. would have a low preparation index. As observed here, the network’s characteristic frequency has a big impact in 2D networks, consistent with <italic>ω</italic> determining the <italic>only</italic> oscillatory pattern that the network can generate on its own. For high-dimensional networks, however, we did not have to incorporate such a measure of compatibility between task requirements and network dynamics (c.f. <xref rid="fig5" ref-type="fig">Figure 5</xref>). We speculate that this is due to averaging effects. Indeed, larger networks possess a wide range of intrinsic oscillatory timescales, and the readout matrix – which here was not aligned to the network’s dynamics in any specific way – is expected to read out a little bit of all frequencies, including task-appropriate ones.</p>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5:</label>
<caption><title>Predicting the preparation index from characteristic network quantities.</title>
<p>We evaluated how well the preparation index could be predicted as a linear function of <inline-formula><inline-graphic xlink:href="535429v3_inline23.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (left). A substantial amount of residual variance appeared to arise from variability in the oscillation frequency <italic>ω</italic> (color). Accounting for this frequency by regressing the preparation index against <italic>ω</italic> <inline-formula><inline-graphic xlink:href="535429v3_inline24.gif" mimetype="image" mime-subtype="gif"/></inline-formula> gave a better fit (right).</p></caption>
<graphic xlink:href="535429v3_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6:</label>
<caption><title>Preparation arises across a range of network architectures:</title>
<p>neural correlates of the reach are shown for 5 different networks (A), alongside the loss and prep. index as a function of Δ<sub>prep</sub>. <bold>(A)</bold> Eigenvalue spectrum (top), internal network activations (middle) and inputs (bottom) for different network types. The unconnected network does not rely on preparatory inputs at all. The random network with weights draw fr om <inline-formula><inline-graphic xlink:href="535429v3_inline25.gif" mimetype="image" mime-subtype="gif"/></inline-formula> uses very little delay period inputs while the skew-symmetric network with <inline-formula><inline-graphic xlink:href="535429v3_inline26.gif" mimetype="image" mime-subtype="gif"/></inline-formula> shows a substantial amount of inputs during the delay period. The inhibition-stabilized network can be seen to rely most on preparation, more so than the similarity transformed ISN. <bold>(B)</bold> Loss (top) and preparation index (bottom) as a function of delay period length for the different networks. The unconnected and random networks can be seen to benefit very little from longer preparation times. Indeed, even as Δ<sub>prep</sub> increases, their amount of preparatory inputs remains very close to 0. On the other hand, the skew-symmetric network and the ISN use preparatory inputs (bottom), which allow them to have a lower loss for larger values of Δ<sub>prep</sub>. Interestingly, the surrogate ISN prepares considerably less than the full ISN.</p></caption>
<graphic xlink:href="535429v3_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s6d">
<label>S4</label>
<title>Comparison across networks</title>
<p>Our main investigation was largely focused on on behaviour of inhibition-stabilized networks, which are believed to constitute good models of M1. We however found that the expression we derived to obtain a network’s preparation index from its control-theoretic properties generalized across to other types of networks. Below, we detail the other network families we considered, and show how their dynamics qualitatively differ from the ISN, although their preparation can be predicted using the same quantities.</p>
<p>We modelled three additional classes of networks: randomly connected networks with either (i) unstructured or (ii) skew-symmetric connectivities, (iii) a surrogate network obtained by applying a similarity transformation to the ISN that preserved its eigenvalue spectrum but eliminated any “nonnormality” (i.e, we found <italic>𝒯</italic> such that <bold><italic>Ã</italic></bold> = <italic>𝒯</italic><sup><italic>−</italic>1</sup><bold><italic>A</italic></bold><italic>𝒯</italic> where <bold><italic>Ã</italic></bold> was a diagonal matrix with the same eigenvalues as <bold><italic>A</italic></bold>). Note that we did not apply the transformation to the readout or input matrices, such that the transfer function of the system was changed by our transformation. This was voluntary, as we were interested in the effect that transforming the dynamics would have on the input-output response. These networks were chosen for the diversity of dynamical motifs they exhibit: combinations of rapidly and slowly decaying modes, oscillations, and transient dynamics. Moreover, each of these network families could be sampled from in a straightforward manner, allowing to compute results across many instantiations of each network type. We again used random readout matrices not specifically adjusted to the dynamics of the network nor to the motor task. To get an intuition for how different networks solve the task, we generated one network from each family and qualitatively compared their inputs and internal activations when performing the same delayed reach (<xref rid="figS6" ref-type="fig">Figure S6A</xref>). We first considered an unconnected network, i.e. a network whose recurrent weights were all 0. Unsurprisingly, this network had no use for a preparation phase. Indeed, there is no benefit to giving early inputs as the network is unable to amplify them. More surprisingly, a random network with a much stronger connectivity – as can be seen in its eigenvalue spectrum forming a small ball of radius close to 1 (<xref rid="figS6" ref-type="fig">Figure S6A</xref>(top)) – also displayed very little preparation. The strong, visually apparent similarity between the inputs to the random and unconnected networks suggests that the optimal way of controlling the random network relies largely on ignoring its internal dynamics and solving the task almost entirely in an input-driven regime. The example skew-symmetric network, which had imaginary eigenvalues only (ranging between -5.5 and 5.5), displayed considerably more preparation, but still relied on strong inputs during the movement phase that resembled those of the unconnected and random networks. Finally, the ISN relied much more on preparation; the small inputs it receives are strongly amplified into large activity patterns owing to its strong, nonnormal recurrent connectivity. Interestingly however, the similarity transformed ISN lost much of that ability to amplify inputs, instead displaying dynamics resembling that of the skew-symmetric network. This highlights the effect of the ISN’s nonnormal dynamics in shaping the network’s activity and optimal inputs.</p>
<p>Next, we assessed more directly how beneficial preparation was for the different networks. We evaluated how the total loss and preparation index evolved as a function of the delay period length (<xref rid="figS6" ref-type="fig">Figure S6B</xref>). As expected, the control of networks that relied on preparation (skew-symmetric and ISN) benefitted more from longer delays. The ISN has markedly lower control cost and higher preparation index than other networks, reflecting the fact that even weak (thus energetically cheap) inputs were sufficient to produce internal activity and thus output torques of the desired magnitude (c.f. <xref rid="figS6" ref-type="fig">Figure S6A</xref>, right).</p>
<p>The above results give a sense of the range of possible dynamics that different types of networks display. Interestingly, despite these differences, we showed in <xref rid="fig5" ref-type="fig">Figure 5</xref> that the preparation index could be predicted with a simple formula across all networks.</p>
<fig id="figS7" position="float" fig-type="figure">
<label>Figure S7:</label>
<caption><title>Illustration of the effect of optimizing the readout matrix such as to minimize the cost of the reaches, across all movements.</title>
<p>To evaluate the effect that our choice of random readout directions has on our conclusions, we additionally compare to a model with the same dynamics, but where the readout was optimized such as to minimize the cost across movements (i.e <italic>ℒ</italic> (<bold><italic>C</italic></bold>) = Σ<sub><italic>i ∈</italic>targets</sub> <italic>𝒥</italic> <sup>(<italic>i</italic>)</sup>(<bold><italic>C</italic></bold>)), under the constraint that its norm was fixed. In <bold>(A)</bold>, we see that this leads to an increase in the observability of the system (compare the observability of the modes of the optimized system in black with those of the random readout in red). In <bold>(B)</bold> and <bold>(C)</bold>, we see that this leads to an output of similar amplitude (<bold>B</bold>), but that is generated using smaller inputs (<bold>C</bold>). Importantly, we see that the system still relies on preparatory inputs. Thus, the exact choice of readout does not alter the network strategy, but can help the system perform the same movements in a more efficient manner.</p></caption>
<graphic xlink:href="535429v3_figS7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS8" position="float" fig-type="figure">
<label>Figure S8:</label>
<caption><title>Comparison of the effect of penalizing temporal input smoothness versus input norm.</title>
<p>We compare the effect of using a cost over inputs that penalizes input norm, versus using a cost that penalizes the “temporal complexity” of the inputs – defined here as the temporal derivative of the inputs (i.e ∥<bold><italic>u</italic></bold>(<italic>t</italic> + 1) <italic>−</italic> <bold><italic>u</italic></bold>(<italic>t</italic>) ∥<sup>2</sup> in discrete time). This is achieved by augmenting the dynamical system to include an input integration stage, which then feeds into the original dynamical system; this way, the input to the augmented system – of which we continue to penalize the squared norm to enable the iLQR framework – is the derivative of the input to the original system. We perform this comparison in linear RNNs, across a range of different preparation times. We show the activation of an example neuron in <bold>(A)</bold> and <bold>(B)</bold>, and activity in an example input channel in <bold>(C)</bold> and <bold>(D)</bold>. Each color denotes a different reach. We see that the rates vary more slowly when penalizing the temporal complexity <bold>(A)</bold> versus the input norm <bold>(B)</bold>, exhibiting a plateau for longer preparation times that is more similar to neural recordings. This is a reflection of the fact that the inputs themselves vary more slowly when the temporal complexity is penalized (compare <bold>C</bold> and <bold>D</bold>). As we do not penalize the input norm within our definition of temporal complexity, the optimal strategy is for the network to rely on steady inputs, which is different from the strategy used when the norm is penalized (compare <bold>C</bold> and <bold>D</bold>). We note that, under this different choice of input penalty, preparation nevertheless remains optimal, with the normalized loss (shown in <bold>E</bold>) decreasing as Δ<sub>prep</sub> increases, and the preparation index (shown in <bold>F</bold>) increasing as Δ<sub>prep</sub> increases.</p></caption>
<graphic xlink:href="535429v3_figS8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS9" position="float" fig-type="figure">
<label>Figure S9:</label>
<caption><title>Control strategy adopted by the model when plannning with uncertain delays.</title>
<p>We investigate an extension of the model, that includes uncertainty about the arrival of the Go cue, and involves replanning at regular intervals to update the model with new information (e.g whether the Go cue has arrived). In <bold>(A)</bold>, we model this by assuming that the network is adopting a strategy whereby it plans to be ready to move <italic>as early as possible</italic> following the target onset. We optimize the inputs using this assumption, and replan every 20ms to update the model with the available information (which here corresponds to the actual Go cue only arriving 500ms after Target onset). We plot the activity of two example neurons (left and right panels, respectively), for each of the reaches (each color denotes a different reach). We can see that the neuronal activations start ramping up at the beginning of the task, and plateau before the actual target onset. In <bold>(B)</bold>, we use a similar optimization strategy, but use a different “mental model” for the network, whereby we assume that, until it sees the actual Go cue, the model is always assuming that the delay period will be equal to the most likely a posteriori preparation time. Under the assumption of exponentially distributed delays with a mean of 150ms, this corresponds to always replanning assuming a delay of 150ms. We see that the network then adopts a different strategy, which does not include the ramping/plateauing behaviour seen above.</p></caption>
<graphic xlink:href="535429v3_figS9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS10" position="float" fig-type="figure">
<label>Figure S10:</label>
<caption><title>Comparison of the occupancy of the preparatory and movement subspaces across different delay periods.</title>
<p>Occupancy (normalized by the maximum value across preparatory and movement occupancies) of the preparatory and movement subspaces identified using a delay period of 500ms, for the activity generated using Δ<sub>prep</sub> = 0ms (left), Δ<sub>prep</sub> = 100ms (center), and Δ<sub>prep</sub> = 500ms (right). We see that the network does not rely on preparatory activity when Δ<sub>prep</sub> = 0<italic>ms</italic>.</p></caption>
<graphic xlink:href="535429v3_figS10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89131.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Diedrichsen</surname>
<given-names>Jörn</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Western University</institution>
</institution-wrap>
<city>London</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study provides a new perspective on why preparatory activity occurs before the onset of movement. The authors report that when there is a cost on the inputs, the optimal inputs should start before the desired network output for a wide variety of recurrent networks. The authors present <bold>convincing</bold> evidence by combining mathematically tractable analyses in linear networks and numerical simulation in nonlinear networks.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89131.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this work, the authors investigate an important question - under what circumstances should a recurrent neural network optimised to produce motor control signals receive preparatory input before the initiation of a movement, even though it is possible to use inputs to drive activity just-in-time for movement?</p>
<p>This question is important because many studies across animal models have show that preparatory activity is widespread in neural populations close to motor output (e.g. motor cortex / M1), but it isn't clear under what circumstances this preparation is advantageous for performance, especially since preparation could cause unwanted motor output during a delay.</p>
<p>They show that networks optimised under reasonable constraints (speed, accuracy, lack of pre-movement) will use input to seed the state of the network before movement, and that these inputs reduce the need for ongoing input during the movement. By examining many different parameters in simplified models they identify a strong connection between the structure of the network and the amount of preparation that is optimal for control - namely, that preparation has the most value when nullspaces are highly observable relative to the readout dimension and when the controllability of readout dimensions is low. They conclude by showing that their model predictions are consistent with the observation in monkey motor cortex that even when a sequence of two movements is known in advance, preparatory activity only arises shortly before movement initiation.</p>
<p>Overall, this study provides valuable theoretical insight into the role of preparation in neural populations that generate motor output, and by treating input to motor cortex as a signal that is optimised directly this work is able to sidestep many of the problematic questions relating to estimating the potential inputs to motor cortex.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89131.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This work clarifies neural mechanisms that can lead to a phenomenology consistent with motor preparation in its broader sense. In this context, motor preparation refers to activity that occurs before the corresponding movement. Another property often associated with preparatory activity is a correlation with global movement characteristics such as reach speed (Churchland et al., Neuron 2006), reach angle (Sun et al., Nature 2022), or grasp type (Meirhaeghe et al., Cell Reports 2023). Such activity has notably been observed in premotor and primary motor cortices, and it has been hypothesized to serve as an input to a motor execution circuit. The timing and mechanisms by which such 'preparatory' inputs are made available to motor execution circuits remain however unclear in general, especially in light of the presence of a 'trigger-like' signal that appears to relate to the transition from preparatory dynamics to execution activity (Kaufman et al. eNeuron 2016, Iganaki et al., Cell 2022, Zimnik and Churchland, Nature Neuroscience 2021).</p>
<p>The preparatory inputs have been hypothesized to fulfill one or several (non-mutually-exclusive) possible objectives. Two notable hypotheses are that these inputs could be shaped to maximize output accuracy under regularization of the input magnitude; or that they may help the flexible re-use of the neural machinery involved in the control of movements in different contexts.</p>
<p>Here, the authors investigate in detail how the former hypothesis may be compatible with the presence of early inputs in recurrent network models driving arm movements, and compare models to data.</p>
<p>Strengths:</p>
<p>The authors are able to deploy an in-depth evaluation of inputs that are optimized for producing an accurate output at a pre-defined time while using a regularization term on the input magnitude, in the case of movements that are thought to be controlled in a quasi-open loop fashion such as reaches.</p>
<p>First, the authors have identified that optimal control theory is a great framework to study this question as it provides methods to find and analyze exact solutions to this cost function in the case of models with linear dynamics. The authors not only use this framework to get an exact assessment of how much pre-movement input arises in large recurrent networks, but also give insight into the mechanisms by which it happens by dissecting in detail low-dimensional networks. The authors find that two key network properties - observability of the readout's nullspace and limited controllability - give rise to optimal inputs that are large before the start of the movement (while the corresponding network activity lies in the nullspace of the readout). Further, the authors numerically investigate the timing of optimized inputs in models with nonlinear dynamics, and find that pre-movement inputs can also arise in these more general networks. The authors also explore how some variations on their model's constraints - such as penalizing the input roughness or changing task contingencies about the go cue timing - affect their results. Finally, the authors point out some coarse-grained similarities between the pre-movement activity driven by the optimized inputs in some of the models they studied, and the phenomenology of preparation observed in the brain during single reaches and reach sequences. Overall, the authors deploy an impressive arsenal of tools and a very in-depth analysis of their models.</p>
<p>Limitations:</p>
<p>(1) Though the optimal control theory framework is ideal to determine inputs that minimize output error while regularizing the input norm or other simple input features, it cannot easily account for some other varied types of objectives - especially those that may lead to a complex optimization landscape. For instance, the reusability of parts of the circuit, sparse use of additional neurons when learning many movements, and ease of planning (especially under uncertainty about when to start the movement), may be alternative or additional reasons that could help explain the preparatory activity observed in the brain. It is interesting to note that inputs that optimize the objective chosen by the authors arguably lead to a trade-off in terms of other desirable objectives. Specifically, the inputs the authors derive are time-dependent, so a recurrent network would be needed to produce them and it may not be easy to interpolate between them to drive new movement variants. In addition, these inputs depend on the desired time of output and therefore make it difficult to plan, e.g. in circumstances when timing should be decided depending on sensory signals. Finally, these inputs are specific to the full movement chain that will unfold, so they do not permit reuse of the inputs e.g. in movement sequences of different orders. Of note, the authors have pointed out in the discussion how their framework may be extended in future work to account for some additional objectives, such as inputs' temporal smoothness or some strategies for dealing with go cue timing uncertainty.</p>
<p>(2) Relatedly, if the motor circuits were to balance different types of objectives, the activity and inputs occurring before each movement may be broken down into different categories that may each specialize into their own objective. For instance, previous work (Kaufman et al. eNeuron 2016, Iganaki et al., Cell 2022, Zimnik and Churchland, Nature Neuroscience 2021) has suggested that inputs occurring before the movement could be broken down into preparatory inputs 'stricto sensu' - relating to the planned characteristics of the movement - and a trigger signal, relating to the transition from planning to execution - irrespective of whether the movement is internally timed or triggered by an external event. The current work does not address which type(s) of early input may be labeled as 'preparatory' or may be thought of as a part of 'planning' computations, or whether these inputs may come from several different source circuits.</p>
<p>(3) While the authors rightly point out some similarities between the inputs that they derive and observed preparatory activity in the brain, notably during motor sequences, there are also some differences. For instance, while both the derived inputs and the data show two peaks during sequences, the data reproduced from Zimnik and Churchland show preparatory inputs that have a very asymmetric shape that really plummets before the start of the next movement, whereas the derived inputs have larger amplitude during the movement period - especially for the second movement of the sequence. In addition, the data show trigger-like signals before each of the two reaches. Finally, while the data show a very high correlation between the pattern of preparatory activity of the second reach in the double reach and compound reach conditions, the derived inputs appear to be more different between the two conditions. Note that the data would be consistent with separate planning of the two reaches even in the compound reach condition, as well as the re-use of the preparatory input between the compound and double reach conditions. Therefore, different motor sequence datasets - notably, those that would show even more coarticulation between submovements - may be more promising to find a tight match between the data and the author's inputs. Further analyses in these datasets could help determine whether the coarticulation could be due to simple filtering by the circuits and muscles downstream of M1, planning of movements with adjusted curvature to mitigate the work performed by the muscles while permitting some amount of re-use across different sequences, or - as suggested by the authors - inputs fully tailored to one specific movement sequence that maximize accuracy and minimize the M1 input magnitude.</p>
<p>(4) Though iLQR is a powerful optimization method to find inputs optimizing the author's cost function, it also has some limitations. First, given that it relies on a linearization of the dynamics at each timestep, it has a limited ability to leverage potential advantages of nonlinearities in the dynamics. Second, the iLQR algorithm is not a biologically plausible learning rule and therefore it might be difficult for the brain to learn to produce the inputs that it finds. Therefore, when observing differences between model and data, this can confound the question of whether it comes from a difference of assumed objective or a difference of optimization procedure. It remains unclear whether using alternative algorithms with different limitations - for instance, using variants of BPTT to train a separate RNN to produce the inputs in question - could impact some of the results.</p>
<p>(5) Under the objective considered by the authors, the amount of input occurring before the movement might be impacted by the presence of online sensory signals for closed-loop control. Even if considering that the inputs could include some sensory activity and/or that the RNN activity could represent general variables whose states can be decoded from M1, the model would not include mechanisms that process imperfect (delayed, noisy) sensory feedback to adapt the output in a trial-specific manner. It is therefore an open question whether the objective and network characteristics suggested by the authors could also explain the presence of preparatory activity before e.g. grasping movements that are thought to be more sensory-driven (Meirhaeghe et al., Cell Reports 2023).</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89131.2.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>I remain enthusiastic about this study. The manuscript is well-written, logical, and conceptually clear. To my knowledge, no prior modeling study has tackled the question of 'why prepare before executing, why not just execute?' Prior studies have simply assumed, to emulate empirical findings, that preparatory inputs precede execution. They never asked why. The authors show that, when there are constraints on inputs, preparation becomes a natural strategy. In contrast, with no constraint on inputs, there is no need for preparation as one could get anything one liked just via the inputs during movement. For the sake of tractability, the authors use a simple magnitude constraint: the cost function punishes the integral of the squared inputs. Thus, if small inputs before movement can reduce the size of the inputs needed during movement, preparation is a good strategy. This occurs if (and only if) the network has strong dynamics (otherwise feeding it preparatory activity would not produce anything interesting). All of this is sensible and clarifying.</p>
<p>As discussed in the prior round of reviews, the central constraint that the authors use is a mathematically tractable stand-in for a range of plausible (but often trickier to define and evaluate) constraints, such as simplicity of inputs (or inputs being things that other areas could provide). The manuscript now embraces this fact more explicitly, and also gives some results showing that other constraints (such as on the derivative of activity, which is one component of complexity) can have the same effect. The manuscript also now discusses and addresses a modest weakness of the previous manuscript: the preparatory activity in their simulations is often overly complex temporally, lacking the (rough) plateau typically seen for data. Depending on your point of view, this is simply 'window dressing', but from my perspective it was important to know that their approach could yield more realistic-looking preparatory activity. Both these additions (the new constraint, and the more realistic temporal profile of preparatory activity) are added simply as supplementary figures rather than in the main text, and are brought up only in the Discussion. At first this struck me as slightly odd, but in the end I think this is appropriate. These are really Discussion-type issues, and dealing with them there makes sense. The 'different constraints' issue in particular is deep, tricky to explore for technical reasons, and could thus support a small research program. I think it is fair to talk about it thoughtfully (as the Discussion now does) and then just mention some simple results.</p>
<p>My remaining comments largely pertain to some subtle (but to me important) nuances at a few locations in the text. These should be easy for the authors to address, in whatever way they see fit.</p>
<p>Specific comments:</p>
<p>(1) The authors state the following on line 56: &quot;For preparatory processes to avoid triggering premature movement, any pre-movement activity in the motor and dorsal pre-motor (PMd) cortices must carefully exclude those pyramidal tract neurons.&quot;</p>
<p>
This constraint is overly restrictive. PT neurons absolutely can change their activity during preparation in principle (and appear to do so in practice). The key constraint is looser: those changes should have no net effect on the muscles. E.g., if d is the vector of changes in PT neuron firing rates, and b is the vector of weights, then the constraint is that b'd = 0. d = 0 is one good way of doing this, but only one. Half the d's could go up and half could go down. Or they all go up, but half the b's are negative. Put differently, there is no reason the null space has to be upstream of the PT neurons. It could be partly, or entirely, downstream.</p>
<p>
In the end, this doesn't change the point the authors are making. It is still the case that d has to be structured to avoid causing muscle activity, which raises exactly the point the authors care about: why risk this unless preparation brings benefits? However, this point can be made with a more accurate motivation. This matters, because people often think that a null-space is a tricky thing to engineer, when really it is quite natural. With enough neurons, preparing in the null space is quite simple.</p>
<p>(2) Line 167: 'near-autonomous internal dynamics in M1'.</p>
<p>
It would be good if such statements, early in the paper, could be modified to reflect the fact that the dynamics observed in M1 may depend on recurrence that is NOT purely internal to M1. A better phrase might be 'near-autonomous dynamics that can be observed in M1'. A similar point applies on line 13. This issue is handled very thoughtfully in the Discussion, starting on line 713. Obviously it is not sensible to also add multiple sentences making the same point early on. However, it is still worth phrasing things carefully, otherwise the reader may have the wrong impression up until the Discussion (i.e. they may think that both the authors, and prior studies, believe that all the relevant dynamics are internal to M1). If possible, it might also be worth adding one sentence, somewhere early, to keep readers from falling into this hole (and then being stuck there till the Discussion digs them out).</p>
<p>(3) The authors make the point, starting on line 815, that transient (but strong) preparatory activity empirically occurs without a delay. They note that their model will do this but only if 'no delay' means 'no external delay'. For their model to prepare, there still needs to be an internal delay between when the first inputs arrive and when movement generating inputs arrive.</p>
<p>This is not only a reasonable assumption, but is something that does indeed occur empirically. This can be seen in Figure 8c of Lara et al. Similarly, Kaufman et al. 2016 noted that &quot;the sudden change in the CIS [the movement triggering event] occurred well after (~150 ms) the visual go cue... (~60 ms latency)&quot; Behavioral experiments have also argued that internal movement-triggering events tend to be quite sluggish relative to the earliest they could be, causing RTs to be longer than they should be (Haith et al. Independence of Movement Preparation and Movement Initiation). Given this empirical support, the authors might wish to add a sentence indicating that the data tend to justify their assumption that the internal delay (separating the earliest response to sensory events from the events that actually cause movement to begin) never shrinks to zero.</p>
<p>While on this topic, the Haith and Krakauer paper mentioned above good to cite because it does ponder the question of whether preparation is really necessary. By showing that they could get RTs to shrink considerably before behavior became inaccurate, they showed that people normally (when not pressured) use more preparation time than they really need. Given Lara et al, we know that preparation does always occur, but Haith and Krakauer were quite right that it can be very brief. This helped -- along with neural results -- change our view of preparation from something more cognitive that had to occur, so something more mechanical that was simply a good network strategy, which is indeed the authors current point. Working a discussion of this into the current paper may or may not make sense, but if there is a place where it is easy to cite, it would be appropriate.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89131.2.sa4</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Schimel</surname>
<given-names>Marine</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kao</surname>
<given-names>Ta-Chu</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hennequin</surname>
<given-names>Guillaume</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<p>General response:</p>
<p>We thank all the reviewers for their detailed reviews.</p>
<p>All reviewers made a number of valuable comments, in particular by highlighting several points that would benefit from additional clarifications and discussion. We really appreciate the time and effort that went into the reviews. We have updated the paper to reflect the changes we have made in response to the reviewers' comments (largely by including more discussion regarding the model limitations and the effect of various modeling choices). We have also included several new supplementary figures (S7, S8, S9, S10) that provide further details of the model behavior, and show the effect of changing some of the terms in the cost. Below, we go through the individual comments, and highlight the places in which we have made changes to address the reviewers’ comments.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 1:</bold></p>
</disp-quote>
<p>Thank you for your review and pointing out multiple things to be discussed and clarified! Below, we go through the various limitations you pointed out and refer to the places where we have tried to address them.</p>
<disp-quote content-type="editor-comment">
<p>(1) It's important to keep in mind that this work involves simplified models of the motor system, and often the terminology for 'motor cortex' and 'models of motor cortex' are used interchangeably, which may mislead some readers. Similarly, the introduction fails in many cases to state what model system is being discussed (e.g. line 14, line 29, line 31), even though these span humans, monkeys, mice, and simulations, which all differ in crucial ways that cannot always be lumped together.</p>
</disp-quote>
<p>That is a good point. We have clarified this in the text (Introduction and Discussion), to highlight the fact that our model isn’t necessarily meant to just capture M1. We have also updated the introduction to make it more clear which species the experiments which motivate our investigation were performed in.</p>
<disp-quote content-type="editor-comment">
<p>(2) At multiple points in the manuscript thalamic inputs during movement (in mice) is used as a motivation for examining the role of preparation. However, there are other more salient motivations, such as delayed sensory feedback from the limb and vision arriving in the motor cortex, as well as ongoing control signals from other areas such as the premotor cortex.</p>
</disp-quote>
<p>Yes – the motivation for thalamic inputs came from the fact that those have specifically been shown to be necessary for accurate movement generation in mice. However, it is true that the inputs in our model are meant to capture any signals external to the dynamical system modeled, and as such are likely to represent a mixture of sensory signals, and feedback from other areas. We have clarified this in the Discussion, and have added this additional motivation in the Introduction.</p>
<disp-quote content-type="editor-comment">
<p>(3) Describing the main task in this work as a delayed reaching task is not justified without caveats (by the authors' own admission: line 687), since each network is optimized with a fixed delay period length. Although this is mentioned to the reader, it's not clear enough that the dynamics observed during the delay period will not resemble those in the motor cortex for typical delayed reaching tasks.</p>
</disp-quote>
<p>Yes, we completely agree that the terminology might be confusing. While the task we are modeling is a delayed reaching task, it does differ from the usual setting since the network has knowledge of the delay period, and that is indeed a caveat of the model. We have added a brief paragraph just after the description of the optimal control objective to highlight this limitation.</p>
<p>We have also performed additional simulations using two different variants of a model-predictive control approach that allow us to relax the assumption that the go-cue time is known in advance. We show that these modifications of the optimal controller yield results that remain consistent with our main conclusions, and can in fact in some settings lead to preparatory activity plateaus during the preparation epoch as often found in monkey M1 (e.g in Elsayed et al. 2016). We have modified the Discussion to explain these results and their limitations, which are summarized in a new Supplementary Figure (S9).</p>
<disp-quote content-type="editor-comment">
<p>(4) A number of simplifications in the model may have crucial consequences for interpretation.</p>
<p>a) Even following the toy examples in Figure 4, all the models in Figure 5 are linear, which may limit the generalisability of the findings.</p>
</disp-quote>
<p>While we agree that linear models may be too simplistic, much prior analyses of M1 data suggest that it is often good enough to capture key aspects of M1 dynamics; for example, the generative model underlying jPCA is linear, and Sussillo et al. (2015) showed that the internal activity of nonlinear RNN models trained to reproduce EMG data aligned best with M1 activity when heavily regularized; in this regime, the RNN dynamics were close to linear. Nevertheless, this linearity assumption is indeed convenient from a modeling viewpoint: the optimal control problem is more easily solved for linear network dynamics and the optimal trajectories are more consistent across networks. Indeed, we had originally attempted to perform the analyses of Figure 5 in the nonlinear setting, but found that while the results were overall similar to what we report in the linear regime, iLQR was occasionally trapped into local minimal, resulting in more variable results especially for inhibition-stabilized network in the strongly connected end of the spectrum. Finally, Figure 5 is primarily meant to explore to what extent motor preparation can be predicted from basic linear control-theoretic properties of the Jacobian of the dynamics; in this regard, it made sense to work with linear RNNs (for which the Jacobian is constant).</p>
<disp-quote content-type="editor-comment">
<p>b) Crucially, there is no delayed sensory feedback in the model from the plant. Although this simplification is in some ways a strength, this decision allows networks to avoid having to deal with delayed feedback, which is a known component of closed-loop motor control and of motor cortex inputs and will have a large impact on the control policy.</p>
</disp-quote>
<p>This comment resonates well with Reviewer 3's remark regarding the autonomous nature (or not) of M1 during movement. Rather than thinking of our RNN models as anatomically confined models of M1 alone, we think of them as models of the dynamics which M1 implements possibly as part of a broader network involving “inter-area loops and (at some latency) sensory feedback”, and whose state appears to be near-fully decodable from M1 activity alone. We have added a paragraph of Discussion on this important point.</p>
<disp-quote content-type="editor-comment">
<p>(5) A key feature determining the usefulness of preparation is the direction of the readout dimension. However, all readouts had a similar structure (random Gaussian initialization). Therefore, it would be useful to have more discussion regarding how the structure of the output connectivity would affect preparation, since the motor cortex certainly does not follow this output scheme.</p>
</disp-quote>
<p>We agree with this limitation of our model — indeed one key message of Figure 4 is that the degree of reliance on preparatory inputs depends strongly on how the dynamics align with the readout. However, this strong dependence is somewhat specific to low-dimensional models; in higher-dimensional models (most of our paper), one expects that any random readout matrix C will pick out activity dimensions in the RNN that are sufficiently aligned with the most controllable directions of the dynamics to encourage preparation.</p>
<p>We did consider optimizing C away (which required differentiating through the iLQR optimizer, which is possible but very costly), but the question inevitably arises what exactly should C be optimized for, and under what constraints (e.g fixed norm or not). One possibility is to optimize C with respect to the same control objective that the control inputs are optimized for, and constrain its norm (otherwise, inputs to the M1 model, and its internal activity, could become arbitrarily small as C can grow to compensate). We performed this experiment (new Supplementary Figure S7) and obtained a similar preparation index; there was one notable difference, namely that the optimized readout modes led to greater observability compared to a random readout; thus, the same amount of “muscle energy” required for a given movement could now be produced by a smaller initial condition. In turn, this led to smaller control inputs, consistent with a lower control cost overall.</p>
<p>Whilst we could have systematically optimized C away, we reasoned that (i) it is computationally expensive, and (ii) the way M1 affects downstream effectors is presumably “optimized” for much richer motor tasks than simple 2D reaching, such that optimizing C for a fixed set of simple reaches could lead to misleading conclusions. We therefore decided to stick with random readouts.</p>
<disp-quote content-type="editor-comment">
<p>Additional comments :</p>
<p>(1) The choice of cost function seems very important. Is it? For example, penalising the square of u(t) may produce very different results than penalising the absolute value.</p>
</disp-quote>
<p>Yes, the choice of cost function does affect the results, at least qualitatively. The absolute value of the inputs is a challenging cost to use, as iLQR relies on a local quadratic approximation of the cost function. However, we have included additional experiments in which we penalized the squared <italic>derivative</italic> of the inputs (Supplementary Figure S8; see also our response to Reviewer 3's suggestion on this topic), and we do see differences in the qualitative behavior of the model (though the main takeaway, i.e. the reliance on preparation, continues to hold). This is now referred to and discussed in the Discussion section.</p>
<disp-quote content-type="editor-comment">
<p>(2) In future work it would be useful to consider the role of spinal networks, which are known to contribute to preparation in some cases (e.g. Prut and Fetz, 1999).</p>
<p>(3) The control signal magnitude is penalised, but not the output torque magnitude, which highlights the fact that control in the model is quite different from muscle control, where co-contraction would be a possibility and therefore a penalty of muscle activation would be necessary. Future work should consider the role of these differences in control policy.</p>
</disp-quote>
<p>Thank you for pointing us to this reference! Regarding both of these concerns, we agree that the model could be greatly improved and made more realistic in future work (another avenue for this would be to consider a more realistic biophysical model, e.g. using the MotorNet library). We hope that the current Discussion, which highlights the various limitations of our modeling choices, makes it clear that a lot of these choices could easily be modified depending on the specific assumptions/investigation being performed.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2:</bold></p>
</disp-quote>
<p>Thank you for your positive review! We very much agree with the limitations you pointed out, some of which overlapped with the comments of the other reviewers. We have done our best to address them through additional discussion and new supplementary figures. We briefly highlight below where those changes can be found.</p>
<disp-quote content-type="editor-comment">
<p>(1) Though the optimal control theory framework is ideal to determine inputs that minimize output error while regularizing the input norm, it however cannot easily account for some other varied types of objectives especially those that may lead to a complex optimization landscape. For instance, the reusability of parts of the circuit, sparse use of additional neurons when learning many movements, and ease of planning (especially under uncertainty about when to start the movement), may be alternative or additional reasons that could help explain the preparatory activity observed in the brain. It is interesting to note that inputs that optimize the objective chosen by the authors arguably lead to a trade-off in terms of other desirable objectives. Specifically, the inputs the authors derive are time-dependent, so a recurrent network would be needed to produce them and it may not be easy to interpolate between them to drive new movement variants. In addition, these inputs depend on the desired time of output and therefore make it difficult to plan, e.g. in circumstances when timing should be decided depending on sensory signals. Finally, these inputs are specific to the full movement chain that will unfold, so they do not permit reuse of the inputs e.g. in movement sequences of different orders.</p>
</disp-quote>
<p>Yes, that is a good point! We have incorporated further Discussion related to this point. We have additionally included a new example in which we regularize the temporal complexity of the inputs (see also our response to Reviewer 3's suggestion on this topic), which leads to more slowly varying inputs, and may indeed represent a more realistic constraint and lead to simpler inputs that can more easily be interpolated between. We also agree that uncertainty about the upcoming go cue may play an important role in the strategy adopted by the animals. While we have not performed an extensive investigation of the topic, we have included a Supplementary Figure (S9) in which we used Model Predictive Control to investigate the effect of planning under uncertainty about the go cue arrival time. We hope that this will give the reader a better sense of what sort of model extensions are possible within our framework.</p>
<disp-quote content-type="editor-comment">
<p>(2) Relatedly, if the motor circuits were to balance different types of objectives, the activity and inputs occurring before each movement may be broken down into different categories that may each specialize into one objective. For instance, previous work (Kaufman et al. eNeuron 2016, Iganaki et al., Cell 2022, Zimnik and Churchland, Nature Neuroscience 2021) has suggested that inputs occurring before the movement could be broken down into preparatory inputs 'stricto sensu' - relating to the planned characteristics of the movement - and a trigger signal, relating to the transition from planning to execution - irrespective of whether the movement is internally timed or triggered by an external event. The current work does not address which type(s) of early input may be labeled as 'preparatory' or may be thought of as a part of 'planning' computations.</p>
</disp-quote>
<p>Yes, our model does indeed treat inputs in a very general way, and does not distinguish between the different types of processes they may be composed of. This is partly because we do not explicitly model where the inputs come from, such that our inputs likely englobe multiple processes. We have added discussion related to this point.</p>
<disp-quote content-type="editor-comment">
<p>(3) While the authors rightly point out some similarities between the inputs that they derive and observed preparatory activity in the brain, notably during motor sequences, there are also some differences. For instance, while both the derived inputs and the data show two peaks during sequences, the data reproduced from Zimnik and Churchland show preparatory inputs that have a very asymmetric shape that really plummets before the start of the next movement, whereas the derived inputs have larger amplitude during the movement period - especially for the second movement of the sequence. In addition, the data show trigger-like signals before each of the two reaches. Finally, while the data show a very high correlation between the pattern of preparatory activity of the second reach in the double reach and compound reach conditions, the derived inputs appear to be more different between the two conditions. Note that the data would be consistent with separate planning of the two reaches even in the compound reach condition, as well as the re-use of the preparatory input between the compound and double reach conditions. Therefore, different motor sequence datasets - notably, those that would show even more coarticulation between submovements - may be more promising to find a tight match between the data and the author's inputs. Further analyses in these datasets could help determine whether the coarticulation could be due to simple filtering by the circuits and muscles downstream of M1, planning of movements with adjusted curvature to mitigate the work performed by the muscles while permitting some amount of re-use across different sequences, or - as suggested by the authors - inputs fully tailored to one specific movement sequence that maximize accuracy and minimize the M1 input magnitude.</p>
</disp-quote>
<p>Regarding the exact shape of the occupancy plots, it is important to note that some of the more qualitative aspects (e.g the relative height of the two peaks) will change if we change the parameters of the cost function. Right now, we have chosen the parameters to ensure that both reaches would be performed at roughly the same speed (as a way to very loosely constrain the parameters based on the observed behavior). However, small changes to the hyperparameters can lead to changes in the model output (e.g one of the two consecutive reaches being performed using greater acceleration than the other), and since our biophysical model is fairly simple, changes in the behavior are directly reflected in the network activity. Essentially, what this means is that while the double occupancy is a consistent feature of the model, the exact shape of the peaks is more sensitive to hyperparameters, and we do not wish to draw any strong conclusions from them, given the simplicity of the biophysical model. However, we do agree that our model exhibits some differences with the data. As discussed above, we have included additional discussion regarding the potential existence of separate inputs for planning vs triggering the movement in the context of single reaches.</p>
<p>Overall, we are excited about the suggestions made by the Reviewer here about using our approach to analyze other motor sequence datasets, but we think that in order to do this properly, one would need to adopt a more realistic musculo-skeletal model (such as one provided by MotorNet).</p>
<disp-quote content-type="editor-comment">
<p>(4) Though iLQR is a powerful optimization method to find inputs optimizing the author's cost function, it also has some limitations. First, given that it relies on a linearization of the dynamics at each timestep, it has a limited ability to leverage potential advantages of nonlinearities in the dynamics. Second, the iLQR algorithm is not a biologically plausible learning rule and therefore it might be difficult for the brain to learn to produce the inputs that it finds. It remains unclear whether using alternative algorithms with different limitations - for instance, using variants of BPTT to train a separate RNN to produce the inputs in question - could impact some of the results.</p>
</disp-quote>
<p>We agree that our choice of iLQR has limitations: while it offers the advantage of convergence guarantees, it does indeed restrict the choice of cost function and dynamics that we can use. We have now included extensive discussion of how the modeling choices affect our results.</p>
<p>We do not view the lack of biological plausibility of iLQR as an issue, as the results are agnostic to the algorithm used for optimization. However, we agree that any structure imposed on the inputs (e.g by enforcing them to be the output of a self-contained dynamical system) would likely alter the results. A potentially interesting extension of our model would be to do just what the reviewer suggested, and try to learn a network that can generate the optimal inputs. However, this is outside the scope of our investigation, as it would then lead to new questions (e.g what brain region would that other RNN represent?).</p>
<disp-quote content-type="editor-comment">
<p>(5) Under the objective considered by the authors, the amount of input occurring before the movement might be impacted by the presence of online sensory signals for closed-loop control. It is therefore an open question whether the objective and network characteristics suggested by the authors could also explain the presence of preparatory activity before e.g. grasping movements that are thought to be more sensory-driven (Meirhaeghe et al., Cell Reports 2023).</p>
</disp-quote>
<p>It is true that we aren’t currently modeling sensory signals explicitly. However, some of the optimal inputs we infer may be capturing upstream information which could englobe some sensory information. This is currently unclear, and would likely depend on how exactly the model is specified. We have added new discussion to emphasize that our dynamics should not be understood as just representing M1, but more general circuits whose state can be decoded from M1.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
</disp-quote>
<p>Additionally, thank you for pointing out various typos in the manuscript, we have fixed those!</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 3:</bold></p>
</disp-quote>
<p>Thank you very much for your review, which makes a lot of very insightful points, and raises several interesting questions. In summary, we very much agree with the limitations you pointed out. In particular, the choice of input cost is something we had previously discussed, but we had found it challenging to decide on what a reasonable cost for “complexity” could be. Following your comment, we have however added a first attempt at penalizing “temporal complexity”, which shows promising behavior. We have only included those additional analyses as supplementary figures, and we have included new discussion, which hopefully highlights what we meant by the different model components, and how the model behavior may change as we vary some of our choices. We hope this can be informative for future models that may use a similar approach. Below, we highlight the changes that we have made to address your comments.</p>
<disp-quote content-type="editor-comment">
<p>The main limitation of the study is that it focuses exclusively on one specific constraint - magnitude - that could limit motor-cortex inputs. This isn't unreasonable, but other constraints are at least as likely, if less mathematically tractable. The basic results of this study will probably be robust with regard such issues - generally speaking, any constraint on what can be delivered during execution will favor the strategy of preparing - but this robustness cuts both ways. It isn't clear that the constraint used in the present study - minimizing upstream energy costs - is the one that really matters. Upstream areas are likely to be limited in a variety of ways, including the complexity of inputs they can deliver. Indeed, one generally assumes that there are things that motor cortex can do that upstream areas can't do, which is where the real limitations should come from. Yet in the interest of a tractable cost function, the authors have built a system where motor cortex actually doesn't do anything that couldn't be done equally well by its inputs. The system might actually be better off if motor cortex were removed. About the only thing that motor cortex appears to contribute is some amplification, which is 'good' from the standpoint of the cost function (inputs can be smaller) but hardly satisfying from a scientific standpoint.</p>
<p>The use of a term that punishes the squared magnitude of control signals has a long history, both because it creates mathematical tractability and because it (somewhat) maps onto the idea that one should minimize the energy expended by muscles and the possibility of damaging them with large inputs. One could make a case that those things apply to neural activity as well, and while that isn't unreasonable, it is far from clear whether this is actually true (and if it were, why punish the square if you are concerned about ATP expenditure?). Even if neural activity magnitude an important cost, any costs should pertain not just to inputs but to motor cortex activity itself. I don't think the authors really wish to propose that squared input magnitude is the key thing to be regularized. Instead, this is simply an easily imposed constraint that is tractable and acts as a stand-in for other forms of regularization / other types of constraints. Put differently, if one could write down the 'true' cost function, it might contain a term related to squared magnitude, but other regularizing terms would by very likely to dominate. Using only squared magnitude is a reasonable way to get started, but there are also ways in which it appears to be limiting the results (see below).</p>
<p>I would suggest that the study explore this topic a bit. Is it possible to use other forms of regularization? One appealing option is to constrain the complexity of inputs; a long-standing idea is that the role of motor cortex is to take relatively simple inputs and convert them to complex time-evolving inputs suitable for driving outputs. I realize that exploring this idea is not necessarily trivial. The right cost-function term is not clear (should it relate to low-dimensionality across conditions, or to smoothness across time?) and even if it were, it might not produce a convex cost function. Yet while exploring this possibility might be difficult, I think it is important for two reasons.</p>
<p>First, this study is an elegant exploration of how preparation emerges due to constraints on inputs, but at present that exploration focuses exclusively on one constraint. Second, at present there are a variety of aspects of the model responses that appear somewhat unrealistic. I suspect most of these flow from the fact that while the magnitude of inputs is constrained, their complexity is not (they can control every motor cortex neuron at both low and high frequencies). Because inputs are not complexity-constrained, preparatory activity appears overly complex and never 'settles' into the plateaus that one often sees in data. To be fair, even in data these plateaus are often imperfect, but they are still a very noticeable feature in the response of many neurons. Furthermore, the top PCs usually contain a nice plateau. Yet we never get to see this in the present study. In part this is because the authors never simulate the situation of an unpredictable delay (more on this below) but it also seems to be because preparatory inputs are themselves strongly time-varying. More realistic forms of regularization would likely remedy this.</p>
</disp-quote>
<p>That is a very good point, and it mirrors several concerns that we had in the past. While we did focus on the input norm for the sake of simplicity, and because it represents a very natural way to regularize our control solutions, we agree that a “complexity cost” may be better suited to models of brain circuits. We have addressed this in a supplementary investigation. We chose to focus on a cost that penalizes the temporal complexity of the inputs, as ||u(t+1) - u(t)||^2. Note that this required augmenting the state of the model, making the computations quite a bit slower; while it is doable if we only penalize the first temporal derivative, it would not scale well to higher orders.</p>
<p>Interestingly, we did find that the activity in that setting was somewhat more realistic (see new Supplementary Figure S8), with more sustained inputs and plateauing activity. While we have kept the original model for most of the investigations, the somewhat more realistic nature of the results under that setting suggests that further exploration of penalties of that sort could represent a promising avenue to improve the model.</p>
<p>We also found the idea of a cost that would ensure low-dimensionality of the inputs across conditions very interesting. However, it is challenging to investigate with iLQR as we perform the optimization separately for each condition; nevertheless, it could be investigated using a different optimizer.</p>
<disp-quote content-type="editor-comment">
<p>At present, it is also not clear whether preparation always occurs even with no delay. Given only magnitude-based regularization, it wouldn't necessarily have to be. The authors should perform a subspace-based analysis like that in Figure 6, but for different delay durations. I think it is critical to explore whether the model, like monkeys, uses preparation even for zero-delay trials. At present it might or might not. If not, it may be because of the lack of more realistic constraints on inputs. One might then either need to include more realistic constraints to induce zero-delay preparation, or propose that the brain basically never uses a zero delay (it always delays the internal go cue after the preparatory inputs) and that this is a mechanism separate from that being modeled.</p>
<p>I agree with the authors that the present version of the model, where optimization knows the exact time of movement onset, produces a reasonably realistic timecourse of preparation when compared to data from self-paced movements. At the same time, most readers will want to see that the model can produce realistic looking preparatory activity when presented with an unpredictable delay. I realize this may be an optimization nightmare, but there are probably ways to trick the model into optimizing to move soon, but then forcing it to wait (which is actually what monkeys are probably doing). Doing so would allow the model to produce preparation under the circumstances where most studies have examined it. In some ways this is just window-dressing (showing people something in a format they are used to and can digest) but it is actually more than that, because it would show that the model can produce a reasonable plateau of sustained preparation. At present it isn't clear it can do this, for the reasons noted above. If it can't, regularizing complexity might help (and even if this can't be shown, it could be discussed).</p>
<p>In summary, I found this to be a very strong study overall, with a conceptually timely message that was well-explained and nicely documented by thorough simulations. I think it is critical to perform the test, noted above, of examining preparatory subspace activity across a range of delay durations (including zero) to see whether preparation endures as it does empirically. I think the issue of a more realistic cost function is also important, both in terms of the conceptual message and in terms of inducing the model to produce more realistic activity. Conceptually it matters because I don't think the central message should be 'preparation reduces upstream ATP usage by allowing motor cortex to be an amplifier'. I think the central message the authors wish to convey is that constraints on inputs make preparation a good strategy. Many of those constraints likely relate to the fact that upstream areas can't do things that motor cortex can do (else you wouldn't need a motor cortex) and it would be good if regularization reflected that assumption. Furthermore, additional forms of regularization would likely improve the realism of model responses, in ways that matter both aesthetically and conceptually. Yet while I think this is an important issue, it is also a deep and tricky one, and I think the authors need considerable leeway in how they address it. Many of the cost-function terms one might want to use may be intractable. The authors may have to do what makes sense given technical limitations. If some things can't be done technically, they may need to be addressed in words or via some other sort of non-optimization-based simulation.</p>
<p>Specific comments</p>
<p>As noted above, it would be good to show that preparatory subspace activity occurs similarly across delay durations. It actually might not, at present. For a zero ms delay, the simple magnitude-based regularization may be insufficient to induce preparation. If so, then the authors would either have to argue that a zero delay is actually never used internally (which is a reasonable argument) or show that other forms of regularization can induce zero-delay preparation.</p>
</disp-quote>
<p>Yes, that is a very interesting analysis to perform, which we had not considered before! When investigating this, we found that the zero-delay strategy does not rely on preparation in the same way as is seen in the monkeys. This seems to be a reflection of the fact that our “Go cue” corresponds to an “internal” go cue which would likely come after the true, “external go cue” – such that we would indeed never actually be in the zero delay setting. This is not something we had addressed (or really considered) before, although we had tried to ensure we referred to “delta prep” as the duration of the preparatory period but not necessarily the delay period. We have now included more discussion on this topic, as well as a new Supplementary Figure S10.</p>
<disp-quote content-type="editor-comment">
<p>I agree with the authors that prior modeling work was limited by assuming the inputs to M1, which meant that prior work couldn't address the deep issue (tackled here) of why there should be any preparatory inputs at all. At the same time, the ability to hand-select inputs did provide some advantages. A strong assumption of prior work is that the inputs are 'simple', such that motor cortex must perform meaningful computations to convert them to outputs. This matters because if inputs can be anything, then they can just be the final outputs themselves, and motor cortex would have no job to do. Thus, prior work tried to assume the simplest inputs possible to motor cortex that could still explain the data. Most likely this went too far in the 'simple' direction, yet aspects of the simplicity were important for endowing responses with realistic properties. One such property is a large condition-invariant response just before movement onset. This is a very robust aspect of the data, and is explained by the assumption of a simple trigger signal that conveys information about when to move but is otherwise invariant to condition. Note that this is an implicit form of regularization, and one very different from that used in the present study: the input is allowed to be large, but constrained to be simple. Preparatory inputs are similarly constrained to be simple in the sense that they carry only information about which condition should be executed, but otherwise have little temporal structure. Arguably this produces slightly too simple preparatory-period responses, but the present study appears to go too far in the opposite direction. I would suggest that the authors do what they can to address these issue via simulations and/or discussion. I think it is fine if the conclusion is that there exist many constraints that tend to favor preparation, and that regularizing magnitude is just one easy way of demonstrating that. Ideally, other constraints would be explored. But even if they can't be, there should be some discussion of what is missing - preparatory plateaus, a realistic condition-invariant signal tied to movement onset - under the present modeling assumptions.</p>
</disp-quote>
<p>As described above, we have now included two additional figures. In the first one (S8, already discussed above), we used a temporal smoothness prior, and we indeed get slightly more realistic activity plateaus. In a second supplementary figure (S9), we have also considered using model predictive control (MPC) to optimize the inputs under an uncertain go cue arrival time. There, we found that removing the assumption that the delay period is known came with new challenges: in particular, it requires the specification of a “mental model” of when the Go cue will arrive. While it is reasonable to expect that monkeys will have a prior over the go time arrival cue that will be shaped by the design of the experiment, some assumptions must be made about the utility functions that should be used to weigh this prior. For instance, if we imagine that monkeys carry a model of the possible arrival time of the go cue that is updated online, they could nonetheless act differently based on this information, for instance by either preparing so as to be ready for the earliest go cue possible or alternatively to be ready for the average go cue. This will likely depend on the exact task design and reward/penalty structure. Here, we added simulations with those two cases (making simplifying assumptions to make the problem tractable/solvable using model predictive control), and found that the “earliest preparation” strategy gives rise to more realistic plateauing activity, while the model where planning is done for the “most likely go time” does not. We suspect that more realistic activity patterns could be obtained by e.g combining this framework with the temporal smoothness cost. However, the main point we wished to make with this new supplementary figure is that it is possible to model the task in a slightly more realistic way (although here it comes at the cost of additional model assumptions). We have now added more discussion related to those points. Note that we have kept our analyses on these new models to a minimum, as the main takeaway we wish to convey from them is that most components of the model could be modified/made more realistic. This would impact the qualitative behavior of the system and match to data but – in the examples we have so far considered – does not appear to modify the general strategy of networks relying on preparation.</p>
<disp-quote content-type="editor-comment">
<p>On line 161, and in a few other places, the authors cite prior work as arguing for &quot;autonomous internal dynamics in M1&quot;. I think it is worth being careful here because most of that work specifically stated that the dynamics are likely not internal to M1, and presumably involve inter-area loops and (at some latency) sensory feedback. The real claim of such work is that one can observe most of the key state variables in M1, such that there are periods of time where the dynamics are reasonably approximated as autonomous from a mathematical standpoint. This means that you can estimate the state from M1, and then there is some function that predicts the future state. This formal definition of autonomous shouldn't be conflated with an anatomical definition.</p>
</disp-quote>
<p>Yes, that is a good point, thank you for making it so clearly! Indeed, as previous work, we do not think of our “M1 dynamics” as being internal to M1, but they may instead include sensory feedback / inter-area loops, which we summarize into the connectivity, that we chose to have dynamics that qualitatively resemble data. We have now incorporated more discussion regarding what exactly the dynamics in our model represent.</p>
</body>
</sub-article>
</article>