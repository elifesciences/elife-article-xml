<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">91601</article-id>
<article-id pub-id-type="doi">10.7554/eLife.91601</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91601.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A previously undescribed scene-selective site is the key to encoding ego-motion in natural environments</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kennedy</surname>
<given-names>Bryan</given-names>
</name>
<xref ref-type="aff" rid="a1"/>
<xref ref-type="aff" rid="a2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Malladi</surname>
<given-names>Sarala N.</given-names>
</name>
<xref ref-type="aff" rid="a1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tootell</surname>
<given-names>Roger BH</given-names>
</name>
<xref ref-type="aff" rid="a1"/>
<xref ref-type="aff" rid="a2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Nasr</surname>
<given-names>Shahin</given-names>
</name>
<xref ref-type="aff" rid="a1"/>
<xref ref-type="aff" rid="a2"/>
<email xlink:href="mailto:shahin.nasr@mgh.harvard.edu">shahin.nasr@mgh.harvard.edu</email>
</contrib>
<aff id="a1"><addr-line>Massachusetts General Hospital</addr-line></aff>
<aff id="a2"><addr-line>Harvard Medical School</addr-line></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Schlichting</surname>
<given-names>Margaret L</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Toronto</institution>
</institution-wrap>
<city>Toronto</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<pub-date pub-type="epub">
<day>25</day>
<month>09</month>
<year>2023</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2024-01-16">
<day>16</day>
<month>01</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP91601</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2023-08-28">
<day>28</day>
<month>08</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-09-25">
<day>25</day>
<month>09</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.21203/rs.3.rs-3378081/v1"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Kennedy et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Kennedy et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-91601-v1.pdf"/>
<abstract>
<p>Current models of scene processing in human brain include three scene-selective areas: the Parahippocampal Place Area (or the temporal place areas; PPA/TPA), the restrosplenial cortex (or the medial place area; RSC/MPA) and the transverse occipital sulcus (or the occipital place area; TOS/OPA). Here, we challenged this simplistic model by showing that another scene-selective site can also be detected within the posterior intraparietal gyrus. Despite the smaller size of this site compared to the other scene-selective areas, the posterior intraparietal gyrus scene-selective (PIGS) site was detected consistently in a large pool of subjects (<italic>n</italic> = 59; 33 females). The reproducibility of this finding was tested based on multiple criteria, including comparing the results across sessions, utilizing different scanners (3T and 7T) and stimulus sets. Furthermore, we found that this site (but not the other three scene-selective areas) is significantly sensitive to ego-motion in scenes, thus distinguishing the role of PIGS in scene perception relative to other scene-selective areas. These results highlight the importance of including finer scale scene-selective sites in models of scene processing – a crucial step toward a more comprehensive understanding of how scenes are encoded under dynamic conditions.</p>
</abstract>
<kwd-group>
<kwd>Scene perception</kwd>
<kwd>ego-motion</kwd>
<kwd>intraparietal gyrus</kwd>
<kwd>fine-scale functional organization</kwd>
</kwd-group>

</article-meta>
<notes>
<notes notes-type="conflict-of-interest-statement">
<title>Conflict of interest statement</title>
<p>Authors declare no competing financial interests.</p>
</notes>
</notes>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>1. Introduction</title>
<p>In human and non-human primates (NHPs), fMRI has been used for many decades to localize the cortical regions that are preferentially involved in scene perception (<xref ref-type="bibr" rid="c13">Epstein and Kanwisher, 1998</xref>; <xref ref-type="bibr" rid="c57">Tsao et al., 2008</xref>; <xref ref-type="bibr" rid="c50">Rajimehr et al., 2009</xref>; <xref ref-type="bibr" rid="c38">Nasr et al., 2011</xref>). Early studies focused mainly on larger activity sites that were more easily reproducible across sessions and individuals, ignoring smaller sites that were not easily detectable in all subjects and/or were not reproducible across scan sessions, based on the techniques available at that time. This led to relatively simple models of neuronal processing solely based on larger visual areas.</p>
<p>Specifically, these models suggested three scene-selective areas within the human visual cortex, with possible homologues in NHPs (<xref ref-type="bibr" rid="c38">Nasr et al., 2011</xref>; <xref ref-type="bibr" rid="c29">Kornblith et al., 2013</xref>; <xref ref-type="bibr" rid="c32">Li et al., 2022</xref>). The human cortical areas were originally named parahippocampal place area (PPA) (<xref ref-type="bibr" rid="c13">Epstein and Kanwisher, 1998</xref>), retrosplenial cortex (RSC) (<xref ref-type="bibr" rid="c33">Maguire, 2001</xref>) and transverse occipital sulcus (TOS) (<xref ref-type="bibr" rid="c19">Grill-Spector, 2003</xref>), based the local anatomical landmarks. However, subsequent studies noticed the discrepancy between the location of these functionally-defined areas and the anatomical landmarked, and instead named those regions temporal, medial and occipital place areas or TPA, MPA and OPA (<xref ref-type="bibr" rid="c38">Nasr et al., 2011</xref>; <xref ref-type="bibr" rid="c12">Dilks et al., 2013</xref>; <xref ref-type="bibr" rid="c51">Silson et al., 2016</xref>).</p>
<p>The idea that scene-selective areas are limited to these three regions is based largely on group-averaged activity maps, generated after applying large surface/volume-based smoothing to the data from individual subjects. In such group-averaged data, originally based on fixed-rather than random-effects, thresholds tended to be high to reduce the impact of nuisance artifacts (<xref ref-type="bibr" rid="c38">Nasr et al., 2011</xref>). Thus, though well founded, this approach conceivably may not have identified smaller scene-selective areas (<xref ref-type="fig" rid="fig1">Fig. 1A</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><title>Distribution of scene-selective areas within the human visual cortex.</title>
<p>Panel A shows the group-averaged (n=14) response to ‘scenes &gt; faces’ contrast (Experiment 1). Areas PPA/TPA, RSC/MPA and TOS/OPA are localized within the temporal, medial and posterior-lateral brain surfaces, respectively. To show consistency with our previous reports (<xref ref-type="bibr" rid="c38">Nasr et al., 2011</xref>), data from individual subjects was largely smoothed (FWHM=5mm) and the group-averaged maps were generated based on fixed-rather than random-effects (see also <xref ref-type="fig" rid="fig3">Figure 3</xref>). The resultant map was thresholded at <italic>p</italic>&lt;10<sup>-25</sup> and overlaid on the common brain template (fsaverage). Panel B shows the activity map in one randomly-selected subject (see also <xref ref-type="fig" rid="fig2">Figure 2</xref>), evoked in response to the same stimulus contrast as in Panel A. Here, the activity map was only minimally smoothed (FWHM=2mm). Consequently, multiple smaller scene-selective sites could be detected across the cortex, including PIGS (black arrowhead), located within the posterior intraparietal gyrus. Traditionally, these smaller activity patches are treated as noise in measurement and discarded. For ease in comparing the two panels, the individual’s data was also overlaid on the fsaverage.</p></caption>
<graphic xlink:href="3378081v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>However, at the single subject level, multiple smaller scene-selective sites can be detected outside these scene-selective areas, especially when drastic spatial smoothing is avoided (<xref ref-type="fig" rid="fig1">Fig. 1B</xref>). This phenomenon is highlighted in a recent neuroimaging study in NHPs (<xref ref-type="bibr" rid="c32">Li et al., 2022</xref>) in which authors took advantage of high-resolution neuroimaging techniques using implanted head coils. Their findings suggested that scene-selective areas are likely not limited to the three expected sites, and that other, smaller, scene-selective areas may also be detected across the brain. Still, the reliability in detection of these smaller sites, their spatial consistency across large populations and their specific role in scene perception that distinguishes them from the other scene-selective areas, remain unclear.</p>
<p>Here, we used conventional (based on a 3T scanner) and high-resolution (based on a 7T scanner) fMRI to localize and study additional scene-selective site(s) that were detected outside PPA/TPA, RSC/MPA and TOS/OPA. We focused our efforts on the intraparietal region mainly because multiple previous studies reported indirect evidence for scene and/or scene-related information processing within this region (<xref ref-type="bibr" rid="c30">Lescroart and Gallant, 2019</xref>; <xref ref-type="bibr" rid="c45">Pitzalis et al., 2020</xref>; <xref ref-type="bibr" rid="c52">Sulpizio et al., 2020</xref>; <xref ref-type="bibr" rid="c40">Park et al., 2022</xref>). Consistent with these studies, we found at least one additional scene-selective area within the posterior intraparietal gyrus, adjacent to the motion-selective area V6 (<xref ref-type="bibr" rid="c11">Dechent and Frahm, 2003</xref>; <xref ref-type="bibr" rid="c44">Pitzalis et al., 2009</xref>). This site was termed PIGS, reflecting its location (posterior intraparietal gyrus) and function (scene-selectivity). PIGS was detected consistently across individual subjects and populations and localized reliably across scan sessions. Moreover, it showed sensitivity to ego-motion within visual scenes, a phenomenon not detectable in other scene-selective areas.</p>
</sec>
<sec id="s2" sec-type="methods">
<title>2. Methods</title>
<sec id="s2-1">
<title>2.1. Participants</title>
<p>Fifty-nine human subjects (33 females), aged 22–68 years, participated in this study. All subjects had normal or corrected-to-normal vision and radiologically normal brains, without any history of neuropsychological disorder. All experimental procedures conformed to NIH guidelines and were approved by Massachusetts General Hospital protocols. Written informed consent was obtained from all subjects before the experiments.</p>
</sec>
<sec id="s2-2">
<title>2.2. General procedure</title>
<p>This study consists of 7 experiments during which we used fMRI to localize and study the evoked scene-selective responses. During these experiments, stimuli were presented via a projector (1024 × 768 pixel resolution, 60 Hz refresh rate) onto a rear-projection screen. Subjects viewed the stimuli through a mirror mounted on the receive coil array. Details of these stimuli are described in the following sections.</p>
<p>During all experiments, to ensure that subjects were attending to the screen, they were instructed to report color changes (red to blue and vice versa) for a centrally presented fixation object (0.1° × 0.1°) by pressing a key on the keypad. Subject detection accuracy remained above 75%, and showed no significant difference in color change detection performance across experimental conditions (<italic>p</italic>&gt;0.10). MATLAB (MathWorks; Natick, MA, USA) and the Psychophysics Toolbox (<xref ref-type="bibr" rid="c7">Brainard, 1997</xref>; <xref ref-type="bibr" rid="c41">Pelli, 1997</xref>) were used to control stimulus presentation.</p>
<sec id="s2-2-1">
<title>2.2.1. Experiment 1 – Localization of scene-selective areas</title>
<p>In fourteen subjects (6 females), we localized scene-selective areas PPA/TPA, RSC/MPA and TOS/OPA by measuring their evoked brain activity, using a 3T fMRI scanner, as they were presented with 8 colorful images of real-world (indoor) scenes vs. (group) faces. Scene and face images were retinotopically centered and subtended 20° × 26° of visual field without any significant differences between their root mean square (RMS) contrast (t(14) =1.10, <italic>p</italic>=0.29). Scene and face stimuli were presented in different blocks (16 s per block and 1 s per image). Each subject participated in 4 runs and each run, consisted of 10 blocks plus 32 s of blank presentation at the beginning and at the end of each block. Within each run, the sequence of blocks and the sequence of images within them was randomized.</p>
</sec>
<sec id="s2-2-2">
<title>2.2.2. Experiment 2 – Reproducibility of PIGS across scan sessions (3T vs. 7T)</title>
<p>To localize PIGS with higher spatial resolution and to enhance the signal/contrast to noise ratio (relative to Experiment 1), four subjects were randomly selected from those who participated in Experiment 1 and were scanned in a 7T scanner. These individuals were presented with 300 grayscale images of scenes and 48 grayscale images of (single) faces other than those used in Experiment 1. Here, scene images included pictures of indoor (100 images), manmade outdoor (100 images) and natural outdoor (100 images) scenes, selected from the Southampton-York Natural Scenes (SYNS) dataset (<xref ref-type="bibr" rid="c1">Adams et al., 2016</xref>).</p>
<p>As in Experiment 1, all images were retinotopically centered, and subtended 20° × 26° of visual field and there was no significant difference between the RMS contrast across the two categories (t(346) =0.75, <italic>p</italic>=0.38). Scene and face images were presented across different blocks. Each block contained 24 stimuli (1 s per stimuli), with no blank presentation between the stimuli. The sequence of stimuli was randomized within the blocks. Each subject participated in 12 runs (11 blocks per run; 24 s per block; 1 s per stimulus), beginning and ending with an additional block (12 s) of uniform black presentation. In each run, the sequence of blocks and the sequence of images within them was randomized.</p>
</sec>
<sec id="s2-2-3">
<title>2.2.3. Experiment 3 – Localization of area V6</title>
<p>This experiment was designed to clarify the relative localization of PIGS vs. area V6 (<xref ref-type="bibr" rid="c44">Pitzalis et al., 2009</xref>). All fourteen subjects who participated in Experiment 1 were examined again in a separate scan session using a 3T scanner. During this scan session, we localized area V6 by contrasting the response evoked by coherent radially-moving (optic flow) vs. randomly-moving white dots (20° × 26°), presented against a black background. The experiment was block-designed, and each block took 16 s, beginning and ending with an additional block of 16 s uniform black presentation. Other details of the experiment were similar to those in Experiment 1.</p>
</sec>
<sec id="s2-2-4">
<title>2.2.4. Experiment 4 – Localization of PIGS in a larger population</title>
<p>Considering the small size of PIGS, it was important to show that this area could survive group-averaging over larger populations, compared to Experiment 1. Accordingly, Experiment 4 localized this area in a large pool of subjects, consisted of thirty-one individuals (19 females) other than those who participated in Experiment 1. The stimuli and procedure were identical to Experiment 1.</p>
</sec>
<sec id="s2-2-5">
<title>2.2.5. Experiment 5 – Response to two independent set of scenes and non-scene objects</title>
<p>Experiments 1–4 used the response evoked by scenes vs. faces to localize PIGS. However, it remained unknown whether PIGS also showed a selective response to the ‘scenes vs. objects’ contrast. Accordingly, in two independent groups of subjects (no overlap), Experiment 5 tested the response evoked by scenes vs. nonscene objects in PIGS and the adjacent areas (i.e. V6, TOS/OPA and RSC/MPA).</p>
<p>Specifically, in Experiment 5a, thirteen subjects (7 females), other than those who participated in Experiment 1, were scanned in a 3T scanner. They were presented with 22 grayscale images of indoor/outdoor scenes, other than those presented in Experiments 1–4, and 88 grayscale images that included either a single or multiple everyday non-animate (non-face) objects. All stimuli were retinotopically centered and presented within a circular aperture (diameter=20°). The RMS contrast of the objects was significantly higher than the scenes (t(108)=3.72, <italic>p</italic>&lt;10<sup>-3</sup>). Scene and object images were presented in different blocks according to their category (22 s per block and 1 s per image). Each subject participated in 12 runs and each run consisted of 9 blocks, plus 16 s of blank presentation at the beginning and the end of each block. As in other experiments, the sequence of blocks and the sequence of images within them was randomized.</p>
<p>In Experiment 5b, fourteen subjects (8 females), other than those who participated in Experiment 1 and 5a, were scanned in a 3T scanner. Each subject was presented with 32 grayscales images of indoor/outdoor scenes, 32 images of everyday (non-face) objects plus also their scrambled versions, and 32 images of single faces. Scene and non-scene stimuli were different than those used in Experiment 1–4 and 5a. In contrast to Experiment 5a, all non-scene images included only one single object and there was no significant difference between the RMS contrasts of scenes and the three object categories (F(3, 111)=0.42, <italic>p</italic>=0.74). Other details were similar to those in Experiment 5a.</p>
</sec>
<sec id="s2-2-6">
<title>2.2.6. Experiment 6 – Coherently- vs. incoherently-changing scenes</title>
<p>This experiment was designed to differentiate the role of PIGS in scene perception from TOS/OPA, RSC/MPA and PPA/TPA. Twelve subjects, from the fourteen subjects who participated in Experiment 1, participated in this experiment. The excluded two subjects could not participate further in our tests for personal reasons. Subjects were scanned in a 3T scanner on a different day relative to Experiments 1–3. During this scan, they were presented with rapidly ‘coherently- vs. incoherently-changing scenes’ (100 ms per image), across different blocks (16 s per block).</p>
<p>Coherently-changing scenes implied ego-motion (fast walking) along 3 different outdoor natural trails. Stimuli (20° × 26°) were generated as one of the experimenters walked through the trails while carrying a camera mounted on his forehead, taking pictures every 2 meters. Incoherently-changing scenes consisted of the same images as the coherently-changing blocks, but with randomized order. In other words, the only difference between the coherently- vs. incoherently-changing scenes was the sequence of stimuli within the block. For both coherently- and incoherently-changing scenes, images from different trails were presented across different blocks.</p>
<p>In separate blocks, subjects were also presented with 80 images that included multiple faces (20° × 26°) with the same timing as the scene images (i.e., 100 ms per image; 16 s per block). All stimuli were grayscaled. Each subject participated in 6 runs and each run consisted of 9 blocks, plus 8 s of blank presentation at the beginning and the end of each block and 4 s of blank presentation between blocks.</p>
<p>On different runs (within the same session), subjects were also presented with concentric rings, extending 20° × 26° (height × width) in the visual field, presented against a light gray background (40 cd/m<sup>2</sup>). In half of the blocks (16 s per block), rings moved radially (centrifugally vs. centripetally; 4°/s) and the direction of motion changed every 4 s to reduce the impact of motion after-effects. In the remaining half of the blocks, rings remained stationary throughout the whole block. Each subject participated in 2 runs and each run consisted of 8 blocks, plus 16 s of uniform gray presentation at the beginning and the end of each run. The sequence of moving and stationary blocks was pseudo-randomized across runs.</p>
</sec>
<sec id="s2-2-7">
<title>Experiment 7 – Response to biological motion</title>
<p><underline>To test whether PIGS also responds selectively to biological motion, twelve</underline> individuals were selected randomly and were scanned in a 3T scanner while they were presented with the moving point-lights that represented complex biological movements such as crawling, cycling, jumping, paddling, walking, etc. (<xref ref-type="bibr" rid="c23">Jastorff and Orban, 2009</xref>). Each action was presented for 2 s and the sequence of actions was randomized across the blocks (20 s per block). As a control, in different blocks, the subjects were shown the same stimuli when all of the point-lights moved in the same direction (i.e., translation motion). Each subject participated in 11 runs and each run consisted of 12 blocks, plus 10 s of blank presentation at the beginning and the end of each run.</p>
</sec>
</sec>
<sec id="s2-3">
<title>2.3. Imaging</title>
<sec id="s2-3-1">
<title>2.3.1. 3T scans</title>
<p>In Experiments 1 and 3–6, subjects were scanned in a horizontal 3T scanner (Tim Trio, Siemens Healthcare, Erlangen, Germany). Gradient echo EPI sequences were used for functional imaging. Functional data were acquired using single-shot gradient echo EPI with nominally 3.0 mm isotropic voxels (TR=2000 ms; TE=30 ms; flip angle=90°; band width (BW)=2298 Hz/pix; echo-spacing= 0.5 ms; no partial Fourier; 33 axial slices covering the entire brain; and no acceleration). During the first 3T scan (see the General Procedure), structural (anatomical) data were acquired for each subject using a 3D T1-weighted MPRAGE sequence (TR=2530 ms; TE=3.39 ms; TI=1100 ms; flip angle=7°; BW=200 Hz/pix; echo-spacing=8.2 ms; voxel size=1.0×1.0×1.33 mm).</p>
</sec>
<sec id="s2-3-2">
<title>2.3.2. 7T scans</title>
<p>In Experiment 2, subjects were scanned in a 7T Siemens whole-body scanner (Siemens Healthcare, Erlangen, Germany) equipped with SC72 body gradients (maximum gradient strength, 70 mT/m; maximum slew rate, 200 T/m/s) using a custom-built 32-channel helmet receive coil array and a birdcage volume transmit coil. Voxel dimensions were nominally 1.0 mm, isotropic. Single-shot gradientecho EPI was used to acquire functional images with the following protocol parameter values: TR=3000 ms; TE=28 ms; flip angle=78°; BW=1184 Hz/pix; echo-spacing=1 ms; 7/8 phase partial Fourier; 44 oblique-coronal slices; and acceleration factor r=4 with GRAPPA reconstruction and FLEET-ACS data (<xref ref-type="bibr" rid="c47">Polimeni et al., 2015</xref>) with 10° flip angle. The field of view included the occipital-parietal brain areas to cover PIGS, RSC/MPA and TOS/OPA (but not PPA/TPA).</p>
</sec>
</sec>
<sec id="s2-4">
<title>2.4. Data Analysis</title>
<sec id="s2-4-1">
<title>2.4.1. Structural data analysis</title>
<p>For each subject, inflated and flattened cortical surfaces were reconstructed based on the high-resolution anatomical data (<xref ref-type="bibr" rid="c9">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="c15">Fischl et al., 1999</xref>; <xref ref-type="bibr" rid="c16">Fischl et al., 2002</xref>), during which the standard pial surface was generated as the gray matter border with the surrounding cerebrospinal fluid or CSF (i.e., the GM-CSF interface). The white matter surface was also generated as the interface between white and gray matter (i.e. WM-GM interface). In addition, an extra surface was generated at 50% of the depth of the local gray matter (<xref ref-type="bibr" rid="c9">Dale et al., 1999</xref>).</p>
</sec>
<sec id="s2-4-2">
<title>2.4.2. Individual-level functional data analysis</title>
<p>All functional data were rigidly aligned (6 df) relative to subject’s own structural scan, using rigid Boundary-Based Registration (<xref ref-type="bibr" rid="c18">Greve and Fischl, 2009</xref>), and then were motion corrected. Data collected in the 3T (but not 7T) scanner was spatially smoothed using a 3D Gaussian kernel (2 mm FWHM). To preserve the spatial resolution, data collected within the 7T scanner was not spatially smoothed.</p>
<p>Subsequently, a standard hemodynamic model based on a gamma function was fit to the fMRI signal to estimate the amplitude of the BOLD response. For each individual subject, the average BOLD response maps were calculated for each condition (<xref ref-type="bibr" rid="c17">Friston et al., 1999</xref>). Finally, voxel-wise statistical tests were conducted by computing contrasts based on a univariate general linear model.</p>
<p>The resultant significance maps based on 3T scans were sampled from the middle of cortical gray matter (defined for each subject based on their structural scan (see section 2.4.1)). For 7T scans, the resultant significance maps were sampled from deep cortical layers at the gray-white matter interface. This procedure reduced the spatial blurring caused by superficial veins (<xref ref-type="bibr" rid="c28">Koopmans et al., 2010</xref>; <xref ref-type="bibr" rid="c46">Polimeni et al., 2010</xref>; <xref ref-type="bibr" rid="c10">De Martino et al., 2013</xref>; <xref ref-type="bibr" rid="c37">Nasr et al., 2016</xref>). For presentation, the resultant maps were projected either onto the subject’s reconstructed cortical surfaces or onto a common template (fsaverage; Freesurfer (<xref ref-type="bibr" rid="c14">Fischl, 2012</xref>)).</p>
</sec>
<sec id="s2-4-3">
<title>2.4.3. Group-level functional data analysis</title>
<p>To generate group-averaged maps, functional maps were spatially normalized across subjects, then averaged using random-effects models and corrected for multiple comparisons (<xref ref-type="bibr" rid="c17">Friston et al., 1999</xref>). For <xref ref-type="fig" rid="fig1">Figure 1A</xref> and to replicate our original finding (<xref ref-type="bibr" rid="c38">Nasr et al., 2011</xref>), the group-average maps were generated using fixed-effects. The resultant significance maps were projected onto a common human brain template (fsaverage).</p>
</sec>
<sec id="s2-4-4">
<title>2.4.4. Region of interest (ROI) analysis</title>
<p>The main ROIs included area PIGS, the two neighboring scene-selective areas (RSC/MPA, TOS/OPA), and area V6. In Experiment 6, we also included area PPA/TPA in our analysis. These ROIs were localized in two different ways: (1) functionally, for each subject based on their own evoked activity (section 2.4.4.1), and (2) probabilistically, based on activity measured in a different group of subjects (section 2.4.4.2).</p>
<sec id="s2-4-4-1">
<title>2.4.4.1. Functionally-localized ROIs</title>
<p>For those subjects who participated in Experiments 6 and 7, we localized scene-selective areas PIGS, TOS/OPA, RSC/MPA, and PPA/TPA based on their stronger response to scenes compared to faces at a threshold level of <italic>p</italic>&lt;10<sup>-2</sup>, using the method described in Experiment 1. For subjects in Experiment 6, we also localized area V6 based on the expected selective response in this region to coherent radially- vs. incoherently-moving random dots (see Section 2.2.3). In those subjects in which PIGS and V6 showed partial overlap, the overlapping parts were excluded for the analysis.</p>
</sec>
<sec id="s2-4-4-2">
<title>2.4.4.2. Probabilistically-localized ROIs</title>
<p>For those subjects who participated in Experiments 4 and 5, we tested the consistency of PIGS locations across populations, using probabilistic labels for areas PIGS, TOS/OPA, RSC/MPA and V6. These labels were generated based on the results of Experiment 1 (for PIGS, TOS/OPA and RSC/MPA) and Experiment 3 (for V6). Specifically, we localized the ROIs separately for the individual subjects who participated in Experiments 1 and 3. Then the labels were overlaid on a common brain template (fsaverage). We computed the probability that each vortex within the cortical surface belonged to one of the ROIs. The labels for PIGS, TOS/OPA, RSC/MPA and V6 were generated based on those vertices that showed a probability higher than 20%. This method assured us that our measurements were not biased by those subjects who showed stronger scene-selective responses. Moreover, by selecting a relatively low threshold (i.e., 20%), we avoided confining our ROIs to the center of activity sites.</p>
</sec>
</sec>
<sec id="s2-4-5">
<title>2.4.5. Statistical tests</title>
<p>To test the effect of independent parameters, we applied paired t-tests and/or a repeated-measures ANOVA, with Greenhouse-Geisser correction whenever the sphericity assumption was violated.</p>
</sec>
</sec>
<sec id="s2-5">
<title>2.5. Data sharing statement</title>
<p>All data, codes and stimuli are ready to be shared upon request.</p>
<p>MATLAB (RRID: SCR_001622; <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com">https://www.mathworks.com</ext-link>).</p>
<p>FreeSurfer (RRID:SCR_001847; <ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsFast">https://surfer.nmr.mgh.harvard.edu/fswiki/FsFast</ext-link>).</p>
<p>Psychophysics Toolbox (RRID:SCR_002881; <ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/docs/Psychtoolbox">http://psychtoolbox.org/docs/Psychtoolbox</ext-link>).</p>
</sec>
</sec>
<sec id="s3" sec-type="results">
<title>3. Results</title>
<p>This study consists of seven experiments. Experiment 1 focused on localizing the scene-selective site (PIGS) within the posterior intraparietal region. Experiment 2 showed consistency in the spatial location of PIGS across sessions. Experiment 3 examined PIGS location relative to V6, an area involved in motion coherency and optic flow encoding. Experiment 4 showed that, despite its small size, PIGS is detectable in group-averaged maps in large populations. Experiment 5 showed that scene and non-scene objects are differentiable from each other based on the evoked response evoked within PIGS. Experiment 6 tested the response in PIGS to ego-motion in scenes, yielding a result that differentiated PIGS from the other scene-selective regions. Finally, Experiment 7 showed that PIGS does not respond selectively to biological motion.</p>
<sec id="s3-1">
<title>3.1. Experiment 1 – Small scene-selective sites are detectable within the posterior intraparietal gyrus</title>
<p>When the level of spatial smoothing is relatively low, scene-selective sites (other than PPA/TPA, TOS/OPA and RSC/MPA) are detectable across the brain, especially within the posterior intraparietal gyrus (<xref ref-type="fig" rid="fig1">Fig. 1B</xref>). To test the consistency in location of these scene-selective sites across individuals, fourteen subjects were presented with scene and face stimuli while we collected their fMRI activity. Considering the expected small size of the scene-selective sites within the intraparietal region, we used limited signal smoothing in our analysis (FWHM = 2 mm; see Methods) to increase the chance of detecting these sites.</p>
<p><xref ref-type="fig" rid="fig2">Figure 2</xref> shows the activity maps evoked by the ‘scenes &gt; faces’ contrast in seven exemplar subjects. All activity maps were overlaid on a common brain template to clarify the consistency in location of scene-selective sites across individuals. In all tested individuals, besides areas RSC/MPA and TOS/OPA, we detected at least one scene-selective site within the posterior portion of the intraparietal gyrus, close to (but outside) the parieto-occipital sulcus (POS). Accordingly, we named this site the posterior interparietal gyrus scene-selective site or PIGS.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><title>Activity evoked by ‘scene &gt; face’ contrast in seven individual subjects, other than the one shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</title>
<p>Panel A shows the evoked activity in the right hemisphere of one individual subject. The inset shows the enlarged activity map within the intraparietal region. The three scene-selective areas, along with area PIGS, are indicated in the map with arrowheads. The location of adjacent sulci the parieto-occipital sulcus (POS), the intraparietal sulcus (IPS) and the calcarine sulcus (CS)) are also indicated in the inset. Panel B shows the result from six other individuals. In this panel, the first two columns show the activity within the left hemisphere, while the next two columns show the activity within the right hemisphere of the same subjects. In all subjects, PIGS is detectable bilaterally within the posterior portion of the intraparietal gyrus, near (but outside) the POS. For all of the subjects, threshold level was set at <italic>p</italic>&lt;10<sup>-4</sup>. All activity maps were overlaid on the fsaverage to highlight the consistency in PIGS location across the subjects.</p></caption>
<graphic xlink:href="3378081v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>When measured at the same threshold levels (<italic>p</italic>&lt; 10<sup>-2</sup>), the relative size of PIGS was 73.86% ± 49.01% (mean ± S. D.) of RSC/MPA, 28.26% ± 15.67% of TOS/OPA, and 19.45% ± 8.43% of PPA/TPA. Considering the proximity of PIGS to the skull and head coil surface (<xref ref-type="fig" rid="fig1">Fig. 1</xref>), the relatively small size of PIGS could not be ascribe to the lower signal/contrast to noise ratio in that region.</p>
<p>To better clarify the consistency of PIGS localization across subjects, we also generated group-averaged activity maps based on random effects, and after correction for multiple comparisons. As demonstrated in <xref ref-type="fig" rid="fig3">Fig. 3A</xref>, PIGS was also detectable in the group-averaged activity maps, in almost the same location as in the individual subject maps. Overall, these results suggest that, despite the relatively small size of this scene-selective site, PIGS is consistently detectable across subjects in the same cortical location.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
<caption><title>PIGS was detected in group-averaged activity maps across two non-overlapping populations.</title>
<p>Panel A shows the group-averaged activity, evoked within the intraparietal region of fourteen subjects who participated in Experiment 1. Panel B shows the group-averaged activity, evoked within the intraparietal region of thirty-one subjects who participated in Experiment 4. Importantly, PIGS was evident in both groups bilaterally in the corresponding location (black arrows). Thus, despite its small size, this area was detectable even in the group-averaged activity maps based on large populations. Notably, in both panels, maps were generated based on random-effects, after correction for multiple comparisons. In both maps, the location of RSC/MPA and TOS/OPA are respectively indicated with white and green arrowheads.</p></caption>
<graphic xlink:href="3378081v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3-2">
<title>3.2. Experiment 2 – PIGS reproducibility across scan sessions</title>
<p>To test the reproducibility of our results, four subjects were selected randomly among those who participated in Experiment 1. These subjects were scanned again (on a different day), using a 7T (rather than a 3T) scanner, and a different set of scenes and faces (<xref ref-type="fig" rid="fig4">Fig. 4A</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4</label>
<caption><title>PIGS was detected consistently across sessions.</title>
<p>Panel A shows the stimuli used for localizing PIGS during 7T scans. Stimuli including indoor, manmade outdoor and natural outdoor scenes and faces other than those used in Experiment 1. Panels B and C show the activity evoked by ‘scene &gt; face’ contrast in the 3T scans (Experiment 1), overlaid on subjects own reconstructed brain (left hemisphere). Panel D shows the activity evoked by ‘scene &gt; face’ contrast during 7T scans (Experiment 2). Despite the difference in scanners (3T vs. 7T) and stimuli, the location of PIGS remained mostly unchanged. Panel E shows the location of PIGS, measured in 3T (black dashed lines) and 7T (green dashed lines) relative to the location of area V6 (white arrowhead), localized functionally based on the response to ‘optic-flow &gt; random motion’ (Experiment 3). In all subjects, the center of scene- and optic-flow-selective responses was adjacent, but not overlapping.</p></caption>
<graphic xlink:href="3378081v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As demonstrated in <xref ref-type="fig" rid="fig4">Fig. 4</xref>, despite utilizing a different scanner and a different set of stimuli, PIGS was still detectable in the same location (<xref ref-type="fig" rid="fig4">Fig. 4B-D</xref>). Here again, PIGS was localized within the posterior portion of the intraparietal gyrus and close to the posterior lip of parieto-occipital sulcus. Considering the higher contrast/signal to noise ratio of 7T (compared to 3T) scans, this result strongly suggested that the PIGS evidence was not simply a nuisance artifact in fMRI measurements.</p>
</sec>
<sec id="s3-3">
<title>3.3. Experiment 3 – Localization of areas PIGS vs. V6</title>
<p>Posterior intraparietal cortex also accommodates area V6, which is involved in motion coherency (opticflow) encoding (<xref ref-type="bibr" rid="c44">Pitzalis et al., 2009</xref>). Recent studies have suggested that scene stimuli evoke a strong response within V6 (<xref ref-type="bibr" rid="c52">Sulpizio et al., 2020</xref>). To test whether PIGS overlaps with area V6, we localized V6 in all subjects who participated in Experiment 1, based on visual presentation of random vs. radially moving dots (see Methods).</p>
<p><xref ref-type="fig" rid="fig4">Figure 4D</xref> shows the co-localization of V6 and PIGS in four individual subjects. Consistent with previous studies (<xref ref-type="bibr" rid="c44">Pitzalis et al., 2009</xref>; <xref ref-type="bibr" rid="c43">Pitzalis et al., 2015</xref>), V6 was localized <italic>within</italic> the posterior portion of the POS without any overlap between its center and PIGS.</p>
<p>To test the relative localization of these two regions at the group level, we generated probabilistic labels for PIGS and V6 (see Methods). As demonstrated in <xref ref-type="fig" rid="fig5">Fig. 5</xref>, the probabilistic label for PIGS was localized within the intraparietal gyrus and outside the POS (<xref ref-type="fig" rid="fig5">Fig. 5A</xref>), while V6 was located within the POS (<xref ref-type="fig" rid="fig5">Fig. 5B</xref>). We also did not find any overlap between area V6 and areas RSC/MPA and TOS/OPA (<xref ref-type="fig" rid="fig5">Fig. 5C</xref>). Thus, despite the low threshold level used to generate these labels (probability &gt; 20%), the areas PIGS and V6 were located side-by-side (<xref ref-type="fig" rid="fig5">Fig. 5D</xref>), without any overlapping between their centers.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><title>Area PIGS is located outside the POS and adjacent to the functionally-localized area V6.</title>
<p>Panels A and B show the probabilistic localization of areas PIGS and V6, respectively (see Methods). Panel C shows the probabilistic localization of areas RSC/MPA and TOS/OPA. All probability maps are thresholded at 20%-50% (red-to-yellow) and overlaid on the fsaverage. Panel D shows the relative location of these sites. Consistent with the results from the individual maps (<xref ref-type="fig" rid="fig4">Figure 4E</xref>), PIGS and V6 were located adjacent to each other, such that V6 was located within the POS and PIGS located outside the POS (within the intraparietal gyrus) with minimal overlap between the two regions.</p></caption>
<graphic xlink:href="3378081v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3-4">
<title>3.4. Experiment 4 – PIGS localization in a larger population</title>
<p>The results of Experiments 1–3 suggest that PIGS can be localized consistently across individual subjects, and this area appears to be distinguishable from the adjacent area V6. However, considering the small size of this area, it appears necessary to test whether this area was detectable based on group averaging in a larger population. Accordingly, in Experiment 4 we scanned thirty-one individuals (other than those who participated in Experiments 1–3) while they were presented with the same stimuli as in Experiment 1 (<xref ref-type="fig" rid="fig2">Fig. 2</xref>).</p>
<p>As demonstrated in <xref ref-type="fig" rid="fig3">Fig. 3B</xref>, PIGS was also detectable in this new population in almost the same location as in Experiment 1. Specifically, PIGS was detected bilaterally within the posterior portion of the intraparietal gyrus, adjacent to the POS. We did not find a significant difference between the two populations in the size of PIGS when normalized either relative to the size of RSC/MPA (t(43) = 0.98, <italic>p</italic> = 0.33), or TOS/OPA (t(43) = 0.26, <italic>p</italic> = 0.80) or PPA/TPA (t(43) = 0.52, <italic>p</italic> = 0.61). Thus, the location and relative size of PIGS appeared to remain unchanged across populations.</p>
<p>These results suggest that one may rely on the probabilistically-generated labels to examine the evoked activity within PIGS. To test this hypothesis, we measured the level of scene-selective activity in PIGS, along with the areas TOS/OPA, RSC/MPA and V6, using the probabilistic labels generated based on the results of Experiments 1 and 3 (see Methods and <xref ref-type="fig" rid="fig5">Fig. 5</xref>). As demonstrated in <xref ref-type="fig" rid="fig6">Fig. 6A-B</xref>, results of this ROI analysis showed a significant scene-selective activity in PIGS (t(31) = 8.11, <italic>p</italic>&lt; 10<sup>-8</sup>), TOS/OPA (t(31) = 7.91, <italic>p</italic>&lt; 10<sup>-7</sup>) and RSC/MPA (t(31) = 9.11, <italic>p</italic>&lt; 10<sup>-8</sup>). Importantly, despite the proximity of PIGS and V6, the level of scene-selective activity in PIGS was significantly higher than that in V6 (t(11) = 5.03, <italic>p</italic>&lt; 10<sup>-4</sup>). Thus, it appears that the probabilistically-generated ROIs can be used to examine PIGS response, and to differentiate it from adjacent areas such as V6 (see also Experiment 5).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6</label>
<caption><title>Probabilistically generated labels can be used to detect PIGS.</title>
<p>Panel A shows the activity evoked by ‘scenes vs. faces’ stimuli, across PIGS, V6, RSC/MPA and TOS/OPA. Panel B shows the level of scene-selective activity, measured as ‘scene – face’, within these regions. Despite the small size of PIGS, the probabilistic label could detect the scene-selective activity within this area and the level of this activity was significantly higher than the adjacent area V6. In all panels, each dot represents the activity measured in one subject.</p></caption>
<graphic xlink:href="3378081v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3-5">
<title>3.5. Experiment 5 – Selective response to scenes compared to non-scene objects in PIGS</title>
<p>Thus far, we localized PIGs in multiple experiments by contrasting the response evoked by scenes vs. faces. In Experiments 5a and 5b, we examined whether PIGS also showed a selective response to scenes compared to objects (not just faces). In Experiment 5a, twelve individuals, other than those who participated in Experiments 1–3, were scanned while viewing pictures of scenes (other than those used to localize PIGS) and everyday objects (<xref ref-type="fig" rid="fig7">Fig. 7A</xref>) (see Methods).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7</label>
<caption><title>PIGS could also be detected based on the ‘scene &gt; object’ contrast.</title>
<p>Panels A and D show the stimuli used in Experiments 5a and 5b respectively. Panels B and E show the activity maps evoked by ‘scene &gt; object’ contrast in two different individuals who participated in Experiment 5a and 5b. Panels C and F show the activity maps evoked by and a different set of scenes and faces (used in Experiments 1 and 4) in the same individuals. The location of PIGS remained unchanged between the two maps.</p></caption>
<graphic xlink:href="3378081v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As demonstrated in <xref ref-type="fig" rid="fig7">Figs. 7B and 7C</xref> for one individual subject, ‘scenes vs. objects’ and ‘scenes vs. faces’ (Experiment 4) contrasts generated similar activity maps. Importantly, in both maps, PIGS was detectable in a consistent location adjacent to (but outside) the parieto-occipital sulcus. Moreover, results of an ROI analysis, using the probabilistically-generated labels based on the results of Experiments 1 and 3, yielded significant scene-selective activity within PIGS (t(11) = 6.57, <italic>p</italic>&lt; 10<sup>-4</sup>), RSC/MPA (t(12) = 11.00, <italic>p</italic>&lt; 10<sup>-6</sup>) and TOS/OPA (t(12) = 6.26, <italic>p</italic>&lt; 10<sup>-3</sup>) (<xref ref-type="fig" rid="fig8">Figs. 8A and 8B</xref>). We also found that the level of scene-selective activity within PIGS is significantly higher than that in the adjacent area V6 (t(11) = 2.42, <italic>p</italic>= 0.03). Thus, scenes and (non-face) objects are differentiable from each other, based on the activity evoked within PIGS.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8</label>
<caption><title>The application of probabilistically generated labels to measure the PIGS response to ‘scene vs. object’ stimuli.</title>
<p>Panels A and C show the activity evoked by ‘scenes vs. object’ stimuli in Experiments 5a and 5b, respectively. Panels B and D show the level of scene-selective activity within the regions of interest. As in Experiment 4, the probabilistic label detected the scene-selective activity within PIGS and the level of this activity was significantly higher than the adjacent area V6. Other details are similar to <xref ref-type="fig" rid="fig6">Figure 6</xref>.</p></caption>
<graphic xlink:href="3378081v1_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In Experiment 5b, fifteen individuals (other than those who participated in Experiments 1 and 5a), were scanned while viewing a new set of stimuli that included pictures of scenes, faces, everyday objects and scrambled objects (<xref ref-type="fig" rid="fig7">Fig. 7D</xref>). In contrast to Experiment 5a in which the number of objects within each image could vary, here, each image contained only one object (see Methods). Despite this change, contrasting the response to scene vs. non-scene images (averaged over objects, scrambled objects and faces) evoked a similar activity pattern, as Scene vs. Faces (<xref ref-type="fig" rid="fig7">Figs. 7E and 7F</xref>). Moreover, the ROI analysis yielded a significant scene-selective activity within PIGS (t(14) = 2.37, <italic>p</italic>= 0.03), RSC/MPA (t(14) = 10.33, <italic>p</italic>&lt; 10<sup>-7</sup>) and TOS/OPA (t(14) = 4.79, <italic>p</italic>&lt; 10<sup>-3</sup>) (<xref ref-type="fig" rid="fig8">Figs. 8</xref>). Here again, the level of scene-selective activity within PIGS was higher than V6 (t(14) = 2.27, <italic>p</italic>= 0.04). Together, results of Experiments 1–5 suggest that PIGS responds selectively to a wide range of scenes compared to non-scene objects, and that the level of this activity is higher than in the adjacent area V6.</p>
</sec>
<sec id="s3-6">
<title>3.6. Experiment 6 – PIGS response to ego-motion</title>
<p>Experiments 1–5 clarified the location of PIGS, and its general functional selectivity for scenes. However, a more specific role of this area in scene perception remains undefined. Experiment 6 tested the hypothesis that area PIGS is involved in encoding ego-motion within scenes. This hypothesis was motivated by the fact that PIGS is located adjacent to V6 (<xref ref-type="fig" rid="fig5">Fig. 5D</xref>), an area involved in encoding optic flow. Other studies have also suggested that ego-motion may influence the scene-selective activity within this region, without clarifying whether this activity was centered either within or outside V6 (<xref ref-type="bibr" rid="c45">Pitzalis et al., 2020</xref>; <xref ref-type="bibr" rid="c52">Sulpizio et al., 2020</xref>).</p>
<p>Twelve individuals, from those who participated in Experiment 1, took part in this experiment (see Methods). These subjects were presented with coherently-changing scene stimuli that implied ego-motion across different outdoor trails (<xref ref-type="fig" rid="fig9">Fig. 9</xref>). In separate blocks, they were also presented with incoherently-changing scenes and faces. <xref ref-type="fig" rid="fig8">Figure 8</xref> shows the group-averaged scene-selective activity, evoked by coherently- (<xref ref-type="fig" rid="fig10">Fig. 10A</xref>) and incoherently-changing scene stimuli (<xref ref-type="fig" rid="fig10">Fig. 10B</xref>). Consistent with our hypothesis, PIGS showed a significantly stronger response (bilaterally) to coherently- (compared to incoherently-) changing scenes that implied ego-motion (<xref ref-type="fig" rid="fig10">Fig. 10C</xref>). However, the level of activity within RSC/MPA and TOS/OPA did not change significantly between these two conditions.</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9</label>
<caption><title>Example of stimuli used in Experiment 6.</title>
<p>Coherently-changing scenes implied ego-motion, as if the observer was jogging through a trail. Incoherently-changing scenes consisted of the same scene images as the coherently-changing scenes, but presented in a pseudo-random order. Face stimuli consisted of a mosaic of faces. These stimuli were different than those used in the previous experiments.</p></caption>
<graphic xlink:href="3378081v1_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10</label>
<caption><title>Scene-selective response to coherently- vs. incoherently-changing scenes within the intraparietal region (Experiment 6).</title>
<p>Panels A and B show respectively the group-averaged activity evoked by coherently- and incoherently-changing scenes relative to faces. Panel Cshows the group-averaged response evoked by the ‘coherently- &gt; incoherently-changing scenes’ contrast. Among scene-selective areas, only PIGS showed significant sensitivity to the observer ego-motion. Besides PIGS, this contrast also evoked activity within area MT (cyan arrowhead), also more dorsal portions of the parietal cortex. Panel D shows the location of scene-selective areas in the same group of subjects based on an independent set of scene and face stimuli (Experiment 1), and generated based on random-effects. The location of PIGS (outside the POS) and RSC/MPA (within the POS) are indicated by black and white arrowheads, respectively. All maps were generated based on random-effects, after correction for multiple comparisons.</p></caption>
<graphic xlink:href="3378081v1_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Consistent with the group-averaged activity maps, results of an ROI analysis (<xref ref-type="fig" rid="fig11">Fig. 11</xref>) yielded a significantly stronger response to coherently- (vs. incoherently-) changing scenes in PIGS (t(11) = 5.97, <italic>p&lt;</italic>10<sup>-4</sup>) but not in RSC/MPA (t(11) = 0.12, <italic>p</italic> = 0.90) and TOS/OPA (t(11) = 0.48, <italic>p</italic> = 0.64). Interestingly, area PPA/TPA showed a stronger response to incoherently- (compared to coherently-) changing scenes (t(11) = 3.48, <italic>p</italic>&lt; 0.01). To better clarify the difference between scene-selective areas, we repeated this test by applying a one-way repeated measures ANOVA to the differential response to ‘coherently- vs. incoherently-changing scenes’, measured across these four scene-selective areas. This test yielded a significant effect of area on the evoked differential activity (F(3, 11) = 53.89, <italic>p</italic>&lt; 10<sup>-10</sup>). Post hoc analysis, with Bonferroni correction, showed that the level of differential activity evoked by ‘coherently- vs. incoherently-changing scenes’ was significantly higher within PIGS than all other scene-selective areas (<italic>p</italic> &lt; 10<sup>-6</sup>). These results suggest a distinctive role for area PIGS in ego-motion encoding, that differentiates it from the other scene-selective areas. The absence of activity modulation in the other scene-selective areas also ruled out the possibility that the activity increase in PIGS was simply due to attentional modulation during coherently- vs. incoherently-changing scenes (see Discussion).</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure 11</label>
<caption><title>The scene-selective activity evoked within PIGS is influenced by the observer ego-motion.</title>
<p>Panel A shows the scene-selective activity evoked by the coherently- (red) and incoherently-changing scenes (blue), measured relative to the response to the faces, across areas PIGS, V6, RSC/MPA, TOS/OPA and PPA/TPA. Panel B shows the level of difference between the response evoked by ‘coherently- – incoherently-’ changing scenes across the regions of interest. While all regions showed a significantly stronger response to scenes compared to faces, PIGS showed the strongest impact of ego-motion on the scene-selective response. Other details are similar to <xref ref-type="fig" rid="fig6">Figure 6</xref>.</p></caption>
<graphic xlink:href="3378081v1_fig11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In addition to PIGS, we also found a significantly stronger response to coherently- (rather than incoherently-) changing scenes in area V6 (t(11) = 3.57, <italic>p</italic>&lt; 0.01). However, the level of this selectivity was significantly weaker in V6 compared to that in PIGS (t(11) = 2.63, <italic>p</italic>= 0.02). Moreover, in the group-averaged activity maps, the contrast between coherently- vs. incoherently-changing scenes yielded a stronger response outside (rather than inside) the POS and also in area MT, located at the tip of medial temporal sulcus (<xref ref-type="fig" rid="fig10">Fig. 10C</xref>). Together, these results suggest that the impact of ego-motion on scene processing is stronger in PIGS than that in V6.</p>
<p>In the same session (but different runs), we also tested the selectivity of the PIGS response for simpler forms of motion. In different blocks, subjects were presented with radially moving vs. stationary concentric rings (see Methods). Consistent with the previous studies of motion perception (<xref ref-type="bibr" rid="c44">Pitzalis et al., 2009</xref>; <xref ref-type="bibr" rid="c20">Hacialihafiz and Bartels, 2015</xref>), the results of an ROI analysis here, did not yield any strong (significant) motion-selective activity within PIGS (t(11) = 1.84, <italic>p</italic>= 0.10), RSC/MPA (t(11) = 1.97, <italic>p</italic>= 0.08), PPA/TPA (t(11) = 1.93, <italic>p</italic>= 0.08) and V6 (t(11) = 2.03, <italic>p</italic>= 0.07). In contrast, we found a strong motion selectivity within area TOS/OPA (t(11) = 4.57, <italic>p</italic>&lt; 10<sup>-3</sup>), likely due to its overlap with the motion-selective area V3A/B (<xref ref-type="bibr" rid="c38">Nasr et al., 2011</xref>). Thus, in contrast to optic flow and ego-motion, simpler forms of motion only evoke weak-to-no selective activity within PIGS and V6.</p>
</sec>
<sec id="s3-7">
<title>3.7. Experiment 7 – PIGS response to biological motion</title>
<p>The results of Experiment 6 showed that PIGS responds selectively to ego-motion in scenes, but not strongly to radially moving rings. However, it could be argued that PIGS may also respond to the other types of complex motion, e.g., biological motion. To test this hypothesis, we measured the PIGS response to biological vs. translational motion in twelve subjects (see Methods). As illustrated in <xref ref-type="fig" rid="fig12">Fig. 12</xref>, and consistent with the previous studies of biological motion (<xref ref-type="bibr" rid="c49">Puce et al., 1998</xref>; <xref ref-type="bibr" rid="c5">Beauchamp et al., 2003</xref>; <xref ref-type="bibr" rid="c48">Puce and Perrett, 2003</xref>; <xref ref-type="bibr" rid="c42">Pelphrey et al., 2005</xref>; <xref ref-type="bibr" rid="c23">Jastorff and Orban, 2009</xref>; <xref ref-type="bibr" rid="c25">Kamps et al., 2016</xref>), biological motion evoked a stronger response bilaterally within area MT and superior temporal sulcus but not within the posterior intraparietal gyrus. Consistent with the maps, an ROI analysis (based on the functionally-defined labels) showed no significant difference between the response to biological vs. translational motion within PIGS (t(11) = 1.27, <italic>p</italic> = 0.23), TOS/OPA (t(11) = 1.63, <italic>p</italic> = 0.13), RSC/MPA (t(11) = 1.40, <italic>p</italic> = 0.18), and PPA/TPA (t(11) = 0.41, <italic>p</italic>= 0.69). These results indicated that PIGS does not respond to all types of complex motion.</p>
<fig id="fig12" position="float" fig-type="figure">
<label>Figure 12</label>
<caption><title>The group-averaged activity map evoked by the ‘biological &gt; translational motion’ contrast.</title>
<p>Despite the low threshold used to generate these maps, we did not detect any significant activity evoked by the ‘biological &gt; translational motion’ contrast within the PIGS and/or the other scene-selective areas. Rather, this contrast evoked a significant activity mainly within the inferior temporal sulcus (ITS), medial temporal sulcus (MTS) and superior temporal sulcus (STS).</p></caption>
<graphic xlink:href="3378081v1_fig12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s4" sec-type="discussion">
<title>4. Discussion</title>
<p>These data suggest that selective scene processing is not limited to areas PPA/TPA, RSC/MPA and TOS/OPA, and that additional smaller scene-selective sites can also be found across the visual system. By focusing on one small scene-selective site, we showed that this site (PIGS) was consistently identifiable across individuals and groups. We also showed that inclusion of this site in the models of scene processing may clarify how ego-motion influences scene perception.</p>
<sec id="s4-1">
<title>4.1. FMRI and all that “noise, noise, noise”!</title>
<p>The early fMRI studies dealt with a considerable amount of noise in measurements, partly due to using lower magnetic field scanners and imperfect hardware and software. This noise in measurements affected the reliability of the findings. Consequently, those early studies focused on larger activity sites that were more reliably detectable across subjects/sessions. The smaller sites were either ignored or eliminated by excessive signal smoothing, applied to enhance the level of contrast to noise ratio.</p>
<p>However, advances in neuroimaging techniques have now made it possible to detect and distinguish fMRI activity at the spatial scale of cortical columns (<xref ref-type="bibr" rid="c58">Yacoub et al., 2007</xref>; <xref ref-type="bibr" rid="c61">Zimmermann et al., 2011</xref>; <xref ref-type="bibr" rid="c37">Nasr et al., 2016</xref>). Although the reliability of the fMRI signal still depends on the number of trial repetitions, a spatially confined, but extensively repeated, evoked response can be detected reliably across different sessions (<xref ref-type="bibr" rid="c37">Nasr et al., 2016</xref>; <xref ref-type="bibr" rid="c27">Kennedy et al., 2023</xref>).</p>
<p>The present data shows that PIGS could be localized consistently across multiple subjects and across different sessions and scanners. Furthermore, our results indicated that the probabilistic labels, generated based on one population, can be used to localize PIGS, and to distinguish its function from the adjacent regions (e.g., V6) in a second population. Together, these results highlight the reliability of current fMRI techniques in detecting smaller cortical regions, in the level of individual subjects.</p>
</sec>
<sec id="s4-2">
<title>4.2. PIGS responds selectively to a variety of scene stimuli</title>
<p>To establish a true category-selective response, the stimulus set should sample enough variety to reflect the range and variability among the category members. Consistent with this are the many (and continuing) studies seeking to define the range and fundamental aspects of ‘place selective’ (<xref ref-type="bibr" rid="c13">Epstein and Kanwisher, 1998</xref>; <xref ref-type="bibr" rid="c56">Troiani et al., 2014</xref>) and ‘face selective’ (<xref ref-type="bibr" rid="c26">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="c60">Yue et al., 2011</xref>) stimuli in extrastriate visual cortex, decades after their first discovery.</p>
<p>Accordingly, here we tested five different scene stimulus sets across our experiments, including a wide variety of indoor/outdoor and natural/manmade scenes. In all cases, we were able to evoke a selective response within PIGS, and the level of this response was comparable to that in the adjacent scene-selective areas RSC/MPA and TOS/OPA. Thus, the scene-selective response in PIGS appeared not to be limited to a single subset of scenes. However, it remains unclear whether scene stimuli are differentiable from each other based on the pattern of evoked response in this region. More experiments are necessary to test this hypothesis (see also the Limitations).</p>
</sec>
<sec id="s4-3">
<title>4.3. PIGS is not just another scene selective area</title>
<p>Our results (Experiment 6) suggest that ego-motion can significantly influence the activity evoked within PIGS. This phenomenon distinguishes the role of PIGS in scene perception, relative to other scene-selective regions. Specifically, previous studies have shown that PPA/TPA and RSC/MPA show weak-to-no sensitivity to motion <italic>per se</italic> (<xref ref-type="bibr" rid="c20">Hacialihafiz and Bartels, 2015</xref>). In comparison, area TOS/OPA shows a stronger motion-selective response, presumably related to its (partial) overlap with area V3A/B (<xref ref-type="bibr" rid="c55">Tootell et al., 1997</xref>; <xref ref-type="bibr" rid="c38">Nasr et al., 2011</xref>). Instead, the current data show that the ego-motion related activity within PIGS is stronger than in TOS/OPA.</p>
<p>This finding is consistent with the fact that PIGS is located adjacent to area V6 (<xref ref-type="fig" rid="fig4">Figs. 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>), an area that contributes to encoding optic flow (<xref ref-type="bibr" rid="c44">Pitzalis et al., 2009</xref>). Considering PIGS and V6 proximity, hypothetical inputs from V6 may contribute to the strong ego-motion selective response in PIGS. This said, the current data also suggests that the role of PIGS differs from that in V6, in terms of ego motion encoding. Compared to V6, PIGS showed a stronger impact of ego-motion on scene processing, while V6 shows a stronger response to optic flow induced by random dot arrays. Thus, PIGS contributes to scene encoding and ego motion within scenes, while V6 is likely involved in detecting optic flow caused by egomotion.</p>
</sec>
<sec id="s4-4">
<title>4.4. Ego-motion Encoding in PIGS vs. TOS/OPA</title>
<p>We showed that PIGS and TOS/OPA are located on two different sides of the IPS with TOS/OPA located more ventrally compared to PIGS. We also showed a stronger of ego-motion on activity within PIGS compared to TOS/OPA. In contrast, TOS/OPA (but not PIGS) responded selectively to simpler forms of motion. These results suggest that PIGS and TOS/OPA are likely two different visual areas, with PIGS being involved in encoding higher-level ego-motion cues.</p>
<p>However, at least two previous studies suggested that area TOS/OPA may also contribute to ego-motion encoding in scenes. Specifically, Kamps and colleagues have shown increased response in TOS/OPA during ego-motion vs. static scene presentation (<xref ref-type="bibr" rid="c25">Kamps et al., 2016</xref>). Jones et al. have also shown that ego-motion (and not other types of movements) enhances TOS/OPA activity when compared to scrambled scenes (<xref ref-type="bibr" rid="c24">Jones et al., 2023</xref>). In contrast to these findings, our tests showed weak-to-no egomotion related activity enhancement in area TOS/OPA.</p>
<p>This difference may well reflect methodological discrepancies. Specifically, in the study by Kamps et al., the static and ego-motion stimuli were presented with two different refresh rates. While in our study, the coherently- and incoherently-changing stimuli were refreshed with the same temporal frequency (see Methods). In the study by Jones et al., the response to scrambled scenes was used as a control condition, whereas our stimuli were more equivalent, differing only in the sequence of image presentation. Moreover, these studies used higher levels of spatial smoothing (FWHM = 5 mm), compared to the values we used here during pre-processing. Also, for understandable reasons, they limited their analysis to previously known scene-selective areas. These technical differences make it difficult to directly compare the two sets of results.</p>
</sec>
<sec id="s4-5">
<title>4.5. Ego-motion but not attention</title>
<p>Experiment 6 showed stronger scene-selective activity within PIGS when subjects were presented with coherently- (compared to incoherently-) changing scenes. It could be argued that coherently-changing scenes attract more attention compared to incoherently-changing scenes. On the face of it, this hypothesis appears to be consistent with the intraparietal role in controlling the spatial attention (<xref ref-type="bibr" rid="c6">Behrmann et al., 2004</xref>; <xref ref-type="bibr" rid="c53">Szczepanski et al., 2010</xref>). However, multiple studies have shown that attention to scenes increases the level of activity within the scene-selective areas (<xref ref-type="bibr" rid="c39">O’craven et al., 1999</xref>; <xref ref-type="bibr" rid="c34">Nasr and Tootell, 2012a</xref>; <xref ref-type="bibr" rid="c3">Baldauf and Desimone, 2014</xref>). However, we did not find any significant activity increases in response to coherently- (vs. incoherently-) changing scenes in PPA/TPA, RSC/MPA and TOS/OPA. Thus, modulation of attention, per se, could not be responsible for the enhanced activity within PIGS in response to coherently- (compared to incoherently-) changing scenes.</p>
</sec>
<sec id="s4-6">
<title>4.6. Direction-selective response within the intraparietal cortex</title>
<p>Motion-selective sites are expected to show at least some level of sensitivity to motion direction (<xref ref-type="bibr" rid="c2">Albright et al., 1984</xref>; <xref ref-type="bibr" rid="c61">Zimmermann et al., 2011</xref>). We did not test the sensitivity of PIGS to the direction of ego motion. However, Pitzalis et al. have shown evidence for motion direction encoding within the V6 + region (<xref ref-type="bibr" rid="c45">Pitzalis et al., 2020</xref>). Furthermore, Tootell et al. reported evidence for motion direction (approaching vs. withdrawing) encoding within posterior intraparietal cortex (<xref ref-type="bibr" rid="c54">Tootell et al., 2022</xref>). Although none of these studies showed any evidence for a new scene-selective area, they raised the possibility that PIGS may also contribute towards encoding ego-motion direction, and even higher level cognitive concepts such as detecting an intrusion to personal space (<xref ref-type="bibr" rid="c22">Holt et al., 2014</xref>).</p>
</sec>
<sec id="s4-7">
<title>4.7. Limitations</title>
<p>In the past, many studies have scrutinized the response function of scene-selective areas to numerous stimulus contrasts. According to these studies, scene-selective areas can differentiate many object categories based on their low-, mid-, and/or higher-level visual features such as their natural size (Konkle and Oliva, 2012), (non-)animacy (<xref ref-type="bibr" rid="c59">Yue et al., 2020</xref>; <xref ref-type="bibr" rid="c8">Coggan and Tong, 2023</xref>), rectilinearity (<xref ref-type="bibr" rid="c36">Nasr et al., 2014</xref>), spatial layout (<xref ref-type="bibr" rid="c21">Harel et al., 2013</xref>), orientation (<xref ref-type="bibr" rid="c35">Nasr and Tootell, 2012b</xref>), spikiness (<xref ref-type="bibr" rid="c8">Coggan and Tong, 2023</xref>), location within the visual field (<xref ref-type="bibr" rid="c31">Levy et al., 2001</xref>), and spatial content (<xref ref-type="bibr" rid="c4">Bar et al., 2008</xref>). Our findings are only a first step toward characterizing PIGS in greater detail. More tests are required to reach the current (yet incomplete) knowledge about the response function of PIGS.</p>
</sec>
</sec>
<sec id="s5" sec-type="conclusion">
<title>5. Conclusion</title>
<p>Neuroimaging studies of scene perception have typically focused on linking scene perception to the evoked activity within PPA/TPA, TOS/OPA and RSC/MPA. Although other scene-selective sites are detectable across the visual cortex, they are largely ignored because of their relatively small size. Our data suggests that the future inclusion of these small sites in models of scene perception may help clarify current models of scene processing in dynamic environments.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by NIH NEI (grants R01 EY017081 and R01 EY030434), and by the MGH/HST Athinoula A. Martinos Center for Biomedical Imaging. Crucial resources were made available by a NIH Shared Instrumentation Grant S10-RR019371. We thank Ms. Azma Mareyam for help with hardware maintenance during this study. We also thank Dr. Claudio Galletti for his helpful comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adams</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Elder</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Graf</surname> <given-names>EW</given-names></string-name>, <string-name><surname>Leyland</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lugtigheid</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Muryy</surname> <given-names>A</given-names></string-name></person-group> (<year>2016</year>) <article-title>The southampton-york natural scenes (syns) dataset: Statistics of surface attitude</article-title>. <source>Sci Rep</source> <volume>6</volume>:<fpage>1</fpage>-<lpage>17</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Albright</surname> <given-names>TD</given-names></string-name>, <string-name><surname>Desimone</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>CG</given-names></string-name></person-group> (<year>1984</year>) <article-title>Columnar organization of directionally selective cells in visual area MT of the macaque</article-title>. <source>J Neurophysiol</source> <volume>51</volume>:<fpage>16</fpage>-<lpage>31</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baldauf</surname> <given-names>D</given-names></string-name>, <string-name><surname>Desimone</surname> <given-names>R</given-names></string-name></person-group> (<year>2014</year>) <article-title>Neural mechanisms of object-based attention</article-title>. <source>Science</source> <volume>344</volume>:<fpage>424</fpage>-<lpage>427</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Aminoff</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schacter</surname> <given-names>DL</given-names></string-name></person-group> (<year>2008</year>) <article-title>Scenes unseen: the parahippocampal cortex intrinsically subserves contextual associations, not scenes or places per se</article-title>. <source>J Neurosci</source> <volume>28</volume>:<fpage>8539</fpage>-<lpage>8544</lpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beauchamp</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Haxby</surname> <given-names>JV</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>A</given-names></string-name></person-group> (<year>2003</year>) <article-title>FMRI responses to video and point-light displays of moving humans and manipulable objects</article-title>. <source>J Cogn Neurosci</source> <volume>15</volume>:<fpage>991</fpage>-<lpage>1001</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Behrmann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Geng</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Shomstein</surname> <given-names>S</given-names></string-name></person-group> (<year>2004</year>) <article-title>Parietal cortex and attention</article-title>. <source>Curr Opin Neurobiol</source> <volume>14</volume>:<fpage>212</fpage>-<lpage>217</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name></person-group> (<year>1997</year>) <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source> <volume>10</volume>:<fpage>433</fpage>-<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Coggan</surname> <given-names>DD</given-names></string-name>, <string-name><surname>Tong</surname> <given-names>F</given-names></string-name></person-group> (<year>2023</year>) <article-title>Spikiness and animacy as potential organizing principles of human ventral visual cortex</article-title>. <source>Cereb Cortex</source>:<fpage>bhad108</fpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dale</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name></person-group> (<year>1999</year>) <article-title>Cortical surface-based analysis. I. Segmentation and surface reconstruction</article-title>. <source>Neuroimage</source> <volume>9</volume>:<fpage>179</fpage>-<lpage>194</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Martino</surname> <given-names>F</given-names></string-name>, <string-name><surname>Zimmermann</surname> <given-names>J</given-names></string-name>, <string-name><surname>Muckli</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K</given-names></string-name>, <string-name><surname>Yacoub</surname> <given-names>E</given-names></string-name>, <string-name><surname>Goebel</surname> <given-names>R</given-names></string-name></person-group> (<year>2013</year>) <article-title>Cortical depth dependent functional responses in humans at 7T: improved specificity with 3D GRASE</article-title>. <source>PLoS One</source> <volume>8</volume>:<fpage>e60514</fpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dechent</surname> <given-names>P</given-names></string-name> <string-name><surname>Frahm</surname> <given-names>J</given-names></string-name></person-group> (<year>2003</year>) <article-title>Characterization of the human visual V6 complex by functional magnetic resonance imaging</article-title>. <source>Eur J Neurosci</source> <volume>17</volume>:<fpage>2201</fpage>-<lpage>2211</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dilks</surname> <given-names>DD</given-names></string-name>, <string-name><surname>Julian</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Paunov</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name></person-group> (<year>2013</year>) <article-title>The occipital place area is causally and selectively involved in scene perception</article-title>. <source>J Neurosci</source> <volume>33</volume>:<fpage>1331</fpage>-<lpage>1336</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Epstein</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name></person-group> (<year>1998</year>) <article-title>A cortical representation of the local visual environment</article-title>. <source>Nature</source> <volume>392</volume>:<fpage>598</fpage>-<lpage>601</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischl</surname> <given-names>B</given-names></string-name></person-group> (<year>2012</year>) <article-title>FreeSurfer</article-title>. <source>Neuroimage</source> <volume>62</volume>:<fpage>774</fpage>-<lpage>781</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Dale</surname> <given-names>AM</given-names></string-name></person-group> (<year>1999</year>) <article-title>Cortical surface-based analysis. II: Inflation, flattening, and a surface-based coordinate system</article-title>. <source>Neuroimage</source> <volume>9</volume>:<fpage>195</fpage>-<lpage>207</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Salat</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Busa</surname> <given-names>E</given-names></string-name>, <string-name><surname>Albert</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dieterich</surname> <given-names>M</given-names></string-name>, <string-name><surname>Haselgrove</surname> <given-names>C</given-names></string-name>, <string-name><surname>van der Kouwe</surname> <given-names>A</given-names></string-name>, <string-name><surname>Killiany</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kennedy</surname> <given-names>D</given-names></string-name>, <string-name><surname>Klaveness</surname> <given-names>S</given-names></string-name>, <string-name><surname>Montillo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Makris</surname> <given-names>N</given-names></string-name>, <string-name><surname>Rosen</surname> <given-names>B</given-names></string-name>, <string-name><surname>Dale</surname> <given-names>AM</given-names></string-name></person-group> (<year>2002</year>) <article-title>Whole brain segmentation: automated labeling of neuroanatomical structures in the human brain</article-title>. <source>Neuron</source> <volume>33</volume>:<fpage>341</fpage>-<lpage>355</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Holmes</surname> <given-names>AP</given-names></string-name> <string-name><surname>Price</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Buchel</surname> <given-names>C</given-names></string-name>, <string-name><surname>Worsley</surname> <given-names>KJ</given-names></string-name></person-group> (<year>1999</year>) <article-title>Multisubject fMRI studies and conjunction analyses</article-title>. <source>Neuroimage</source> <volume>10</volume>:<fpage>385</fpage>-<lpage>396</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greve</surname> <given-names>DN</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B</given-names></string-name></person-group> (<year>2009</year>) <article-title>Accurate and robust brain image alignment using boundary-based registration</article-title>. <source>Neuroimage</source> <volume>48</volume>:<fpage>63</fpage>-<lpage>72</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname> <given-names>K</given-names></string-name></person-group> (<year>2003</year>) <article-title>The neural basis of object perception</article-title>. <source>Curr Opin Neurobiol</source> <volume>13</volume>:<fpage>159</fpage>-<lpage>166</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hacialihafiz</surname> <given-names>DK</given-names></string-name>, <string-name><surname>Bartels</surname> <given-names>A</given-names></string-name></person-group> (<year>2015</year>) <article-title>Motion responses in scene-selective regions</article-title>. <source>Neuroimage</source> <volume>118</volume>:<fpage>438</fpage>-<lpage>444</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kravitz</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Baker</surname> <given-names>CI</given-names></string-name></person-group> (<year>2013</year>) <article-title>Deconstructing visual scenes in cortex: gradients of object and spatial layout information</article-title>. <source>Cereb Cortex</source> <volume>23</volume>:<fpage>947</fpage>-<lpage>957</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holt</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Cassidy</surname> <given-names>BS</given-names></string-name>, <string-name><surname>Yue</surname> <given-names>X</given-names></string-name>, <string-name><surname>Rauch</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Boeke</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Nasr</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Coombs</surname> <given-names>G</given-names> <suffix>3rd</suffix></string-name></person-group> (<year>2014</year>) <article-title>Neural correlates of personal space intrusion</article-title>. <source>J Neurosci</source> <volume>34</volume>:<fpage>4123</fpage>-<lpage>4134</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jastorff</surname> <given-names>J</given-names></string-name>, <string-name><surname>Orban</surname> <given-names>GA</given-names></string-name></person-group> (<year>2009</year>) <article-title>Human functional magnetic resonance imaging reveals separation and integration of shape and motion cues in biological motion processing</article-title>. <source>J Neurosci</source> <volume>29</volume>:<fpage>7315</fpage>-<lpage>7329</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jones</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Byland</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dilks</surname> <given-names>DD</given-names></string-name></person-group> (<year>2023</year>) <article-title>The occipital place area represents visual information about walking, not crawling</article-title>. <source>Cereb Cortex</source>:<fpage>bhad055</fpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kamps</surname> <given-names>FS</given-names></string-name>, <string-name><surname>Lall</surname> <given-names>V</given-names></string-name>, <string-name><surname>Dilks</surname> <given-names>DD</given-names></string-name></person-group> (<year>2016</year>) <article-title>The occipital place area represents first-person perspective motion information through scenes</article-title>. <source>Cortex</source> <volume>83</volume>:<fpage>17</fpage>-<lpage>26</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name>, <string-name><surname>McDermott</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chun</surname> <given-names>MM</given-names></string-name></person-group> (<year>1997</year>) <article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>. <source>J Neurosci</source> <volume>17</volume>:<fpage>4302</fpage>-<lpage>4311</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kennedy</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bex</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hunter</surname> <given-names>D</given-names></string-name>, <string-name><surname>Nasr</surname> <given-names>S</given-names></string-name></person-group> (<year>2023</year>) <article-title>Two fine-scale channels for encoding motion and stereopsis within the human magnocellular stream</article-title>. <source>Prog Neurobiol</source>:<fpage>102374</fpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koopmans</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Barth</surname> <given-names>M</given-names></string-name>, <string-name><surname>Norris</surname> <given-names>DG</given-names></string-name></person-group> (<year>2010</year>) <article-title>Layer-specific BOLD activation in human V1</article-title>. <source>Hum Brain Mapp</source> <volume>31</volume>:<fpage>1297</fpage>-<lpage>1304</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kornblith</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cheng</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ohayon</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tsao</surname> <given-names>DY</given-names></string-name></person-group> (<year>2013</year>) <article-title>A network for scene processing in the macaque temporal lobe</article-title>. <source>Neuron</source> <volume>79</volume>:<fpage>766</fpage>-<lpage>781</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lescroart</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group> (<year>2019</year>) <article-title>Human scene-selective areas represent 3D configurations of surfaces</article-title>. <source>Neuron</source> <volume>101</volume>:<fpage>178</fpage>-<lpage>192</lpage>. <fpage>e177</fpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levy</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hasson</surname> <given-names>U</given-names></string-name>, <string-name><surname>Avidan</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hendler</surname> <given-names>T</given-names></string-name>, <string-name><surname>Malach</surname> <given-names>R</given-names></string-name></person-group> (<year>2001</year>) <article-title>Center–periphery organization of human object areas</article-title>. <source>Nat Neurosci</source> <volume>4</volume>:<fpage>533</fpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>X</given-names></string-name>, <string-name><surname>Zhu</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Vanduffel</surname> <given-names>W</given-names></string-name></person-group> (<year>2022</year>) <article-title>Submillimeter fMRI reveals an extensive, fine-grained and functionally-relevant scene-processing network in monkeys</article-title>. <source>Prog Neurobiol</source> <volume>211</volume>:<fpage>102230</fpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maguire</surname> <given-names>E</given-names></string-name></person-group> (<year>2001</year>) <article-title>The retrosplenial contribution to human navigation: a review of lesion and neuroimaging findings</article-title>. <source>Scand J Psychol</source> <volume>42</volume>:<fpage>225</fpage>-<lpage>238</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nasr</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name></person-group> (<year>2012a</year>) <article-title>Role of fusiform and anterior temporal cortical areas in facial recognition</article-title>. <source>Neuroimage</source> <volume>63</volume>:<fpage>1743</fpage>-<lpage>1753</lpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nasr</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name></person-group> (<year>2012b</year>) <article-title>A cardinal orientation bias in scene-selective visual cortex</article-title>. <source>J Neurosci</source> <volume>32</volume>:<fpage>14921</fpage>-<lpage>14926</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nasr</surname> <given-names>S</given-names></string-name>, <string-name><surname>Echavarria</surname> <given-names>CE</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name></person-group> (<year>2014</year>) <article-title>Thinking outside the box: rectilinear shapes selectively activate scene-selective cortex</article-title>. <source>J Neurosci</source> <volume>34</volume>:<fpage>6721</fpage>-<lpage>6735</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nasr</surname> <given-names>S</given-names></string-name>, <string-name><surname>Polimeni</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name></person-group> (<year>2016</year>) <article-title>Interdigitated Color-and Disparity-Selective Columns within Human Visual Cortical Areas V2 and V3</article-title>. <source>J Neurosci</source> <volume>36</volume>:<fpage>1841</fpage>-<lpage>1857</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nasr</surname> <given-names>S</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>N</given-names></string-name>, <string-name><surname>Devaney</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Yue</surname> <given-names>X</given-names></string-name>, <string-name><surname>Rajimehr</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ungerleider</surname> <given-names>LG</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name></person-group> (<year>2011</year>) <article-title>Scene-selective cortical regions in human and nonhuman primates</article-title>. <source>J Neurosci</source> <volume>31</volume>:<fpage>13771</fpage>-<lpage>13785</lpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’craven</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Downing</surname> <given-names>PE</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name></person-group> (<year>1999</year>) <article-title>fMRI evidence for objects as the units of attentional selection</article-title>. <source>Nature</source> <volume>401</volume>:<fpage>584</fpage>-<lpage>587</lpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Park</surname> <given-names>J</given-names></string-name>, <string-name><surname>Josephs</surname> <given-names>E</given-names></string-name>, <string-name><surname>Konkle</surname> <given-names>T</given-names></string-name></person-group> (<year>2022</year>) <article-title>Ramp-shaped neural tuning supports graded population-level representation of the object-to-scene continuum</article-title>. <source>Sci Rep</source> <volume>12</volume>:<fpage>18081</fpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name></person-group> (<year>1997</year>) <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spat Vis</source> <volume>10</volume>:<fpage>437</fpage>-<lpage>442</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pelphrey</surname> <given-names>KA</given-names></string-name>, <string-name><surname>Morris</surname> <given-names>JP</given-names></string-name> <string-name><surname>Michelich</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Allison</surname> <given-names>T</given-names></string-name>, <string-name><surname>McCarthy</surname> <given-names>G</given-names></string-name></person-group> (<year>2005</year>) <article-title>Functional anatomy of biological motion perception in posterior temporal cortex: an fMRI study of eye, mouth and hand movements</article-title>. <source>Cereb Cortex</source> <volume>15</volume>:<fpage>1866</fpage>-<lpage>1876</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pitzalis</surname> <given-names>S</given-names></string-name>, <string-name><surname>Fattori</surname> <given-names>P</given-names></string-name> <string-name><surname>Galletti</surname> <given-names>C</given-names></string-name></person-group> (<year>2015</year>) <article-title>The human cortical areas V6 and V6A</article-title>. <source>Vis Neurosci</source> <fpage>32</fpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pitzalis</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Committeri</surname> <given-names>G</given-names></string-name>, <string-name><surname>Fattori</surname> <given-names>P</given-names></string-name> <string-name><surname>Galati</surname> <given-names>G</given-names></string-name>, <string-name><surname>Patria</surname> <given-names>F</given-names></string-name>, <string-name><surname>Galletti</surname> <given-names>C</given-names></string-name></person-group> (<year>2009</year>) <article-title>Human V6: the medial motion area</article-title>. <source>Cereb Cortex</source> <volume>20</volume>:<fpage>411</fpage>-<lpage>424</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pitzalis</surname> <given-names>S</given-names></string-name>, <string-name><surname>Serra</surname> <given-names>C</given-names></string-name>, <string-name><surname>Sulpizio</surname> <given-names>V</given-names></string-name>, <string-name><surname>Committeri</surname> <given-names>G</given-names></string-name>, <string-name><surname>de Pasquale</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fattori</surname> <given-names>P</given-names></string-name> <string-name><surname>Galletti</surname> <given-names>C</given-names></string-name>, <string-name><surname>Sepe</surname> <given-names>R</given-names></string-name>, <string-name><surname>Galati</surname> <given-names>G</given-names></string-name></person-group> (<year>2020</year>) <article-title>Neural bases of self-and object-motion in a naturalistic vision</article-title>. <source>Hum Brain Mapp</source> <volume>41</volume>:<fpage>1084</fpage>-<lpage>1111</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Polimeni</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Greve</surname> <given-names>DN</given-names></string-name>, <string-name><surname>Wald</surname> <given-names>LL</given-names></string-name></person-group> (<year>2010</year>) <article-title>Laminar analysis of 7T BOLD using an imposed spatial activation pattern in human V1</article-title>. <source>Neuroimage</source> <volume>52</volume>:<fpage>1334</fpage>-<lpage>1346</lpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Polimeni</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Bhat</surname> <given-names>H</given-names></string-name>, <string-name><surname>Witzel</surname> <given-names>T</given-names></string-name>, <string-name><surname>Benner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Feiweier</surname> <given-names>T</given-names></string-name>, <string-name><surname>Inati</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Renvall</surname> <given-names>V</given-names></string-name>, <string-name><surname>Heberlein</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wald</surname> <given-names>LL</given-names></string-name></person-group> (<year>2015</year>) <article-title>Reducing sensitivity losses due to respiration and motion in accelerated echo planar imaging by reordering the autocalibration data acquisition</article-title>. <source>Magn Reson Med</source>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Puce</surname> <given-names>A</given-names></string-name>, <string-name><surname>Perrett</surname> <given-names>D</given-names></string-name></person-group> (<year>2003</year>) <article-title>Electrophysiology and brain imaging of biological motion</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>358</volume>:<fpage>435</fpage>-<lpage>445</lpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Puce</surname> <given-names>A</given-names></string-name>, <string-name><surname>Allison</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bentin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gore</surname> <given-names>JC</given-names></string-name>, <string-name><surname>McCarthy</surname> <given-names>G</given-names></string-name></person-group> (<year>1998</year>) <article-title>Temporal cortex activation in humans viewing eye and mouth movements</article-title>. <source>J Neurosci</source> <volume>18</volume>:<fpage>2188</fpage>-<lpage>2199</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rajimehr</surname> <given-names>R</given-names></string-name>, <string-name><surname>Young</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name></person-group> (<year>2009</year>) <article-title>An anterior temporal face patch in human cortex, predicted by macaque maps</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>106</volume>:<fpage>1995</fpage>-<lpage>2000</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silson</surname> <given-names>EH</given-names></string-name>, <string-name><surname>Steel</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Baker</surname> <given-names>CI</given-names></string-name></person-group> (<year>2016</year>) <article-title>Scene-selectivity and retinotopy in medial parietal cortex</article-title>. <source>Front Hum Neurosci</source> <volume>10</volume>:<fpage>412</fpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sulpizio</surname> <given-names>V</given-names></string-name>, <string-name><surname>Galati</surname> <given-names>G</given-names></string-name>, <string-name><surname>Fattori</surname> <given-names>P</given-names></string-name> <string-name><surname>Galletti</surname> <given-names>C</given-names></string-name>, <string-name><surname>Pitzalis</surname> <given-names>S</given-names></string-name></person-group> (<year>2020</year>) <article-title>A common neural substrate for processing scenes and egomotion-compatible visual motion</article-title>. <source>Brain structure and Function</source> <volume>225</volume>:<fpage>2091</fpage>-<lpage>2110</lpage>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szczepanski</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Konen</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Kastner</surname> <given-names>S</given-names></string-name></person-group> (<year>2010</year>) <article-title>Mechanisms of spatial attention control in frontal and parietal cortex</article-title>. <source>J Neurosci</source> <volume>30</volume>:<fpage>148</fpage>-<lpage>160</lpage>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Nasiriavanaki</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Babadi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Greve</surname> <given-names>DN</given-names></string-name>, <string-name><surname>Nasr</surname> <given-names>S</given-names></string-name>, <string-name><surname>Holt</surname> <given-names>DJ</given-names></string-name></person-group> (<year>2022</year>) <article-title>Interdigitated Columnar Representation of Personal Space and Visual Space in Human Parietal Cortex</article-title>. <source>J Neurosci</source> <volume>42</volume>:<fpage>9011</fpage>-<lpage>9029</lpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Mendola</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Hadjikhani</surname> <given-names>NK</given-names></string-name>, <string-name><surname>Ledden</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Reppas</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Dale</surname> <given-names>AM</given-names></string-name></person-group> (<year>1997</year>) <article-title>Functional analysis of V3A and related areas in human visual cortex</article-title>. <source>J Neurosci</source> <volume>17</volume>:<fpage>7060</fpage>-<lpage>7078</lpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Troiani</surname> <given-names>V</given-names></string-name>, <string-name><surname>Stigliani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>ME</given-names></string-name>, <string-name><surname>Epstein</surname> <given-names>RA</given-names></string-name></person-group> (<year>2014</year>) <article-title>Multiple object properties drive scene-selective regions</article-title>. <source>Cereb Cortex</source> <volume>24</volume>:<fpage>883</fpage>-<lpage>897</lpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsao</surname> <given-names>DY</given-names></string-name>, <string-name><surname>Moeller</surname> <given-names>S</given-names></string-name>, <string-name><surname>Freiwald</surname> <given-names>WA</given-names></string-name></person-group> (<year>2008</year>) <article-title>Comparing face patch systems in macaques and humans</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>105</volume>:<fpage>19514</fpage>-<lpage>19519</lpage>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yacoub</surname> <given-names>E</given-names></string-name>, <string-name><surname>Shmuel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Logothetis</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K</given-names></string-name></person-group> (<year>2007</year>) <article-title>Robust detection of ocular dominance columns in humans using Hahn Spin Echo BOLD functional MRI at 7 Tesla</article-title>. <source>Neuroimage</source> <volume>37</volume>:<fpage>1161</fpage>-<lpage>1177</lpage>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yue</surname> <given-names>X</given-names></string-name>, <string-name><surname>Robert</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ungerleider</surname> <given-names>LG</given-names></string-name></person-group> (<year>2020</year>) <article-title>Curvature processing in human visual cortical areas</article-title>. <source>Neuroimage</source> <volume>222</volume>:<fpage>117295</fpage>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yue</surname> <given-names>X</given-names></string-name>, <string-name><surname>Cassidy</surname> <given-names>BS</given-names></string-name>, <string-name><surname>Devaney</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Holt</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name></person-group> (<year>2011</year>) <article-title>Lower-level stimulus features strongly influence responses in the fusiform face area</article-title>. <source>Cereb Cortex</source> <volume>21</volume>:<fpage>35</fpage>-<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zimmermann</surname> <given-names>J</given-names></string-name>, <string-name><surname>Goebel</surname> <given-names>R</given-names></string-name>, <string-name><surname>De Martino</surname> <given-names>F</given-names></string-name>, <string-name><surname>van de Moortele</surname> <given-names>PF</given-names></string-name>, <string-name><surname>Feinberg</surname> <given-names>D</given-names></string-name>, <string-name><surname>Adriany</surname> <given-names>G</given-names></string-name>, <string-name><surname>Chaimow</surname> <given-names>D</given-names></string-name>, <string-name><surname>Shmuel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K</given-names></string-name>, <string-name><surname>Yacoub</surname> <given-names>E</given-names></string-name></person-group> (<year>2011</year>) <article-title>Mapping the organization of axis of motion selective features in human area MT using high-field fMRI</article-title>. <source>PLoS One</source> <volume>6</volume>:<fpage>e28716</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91601.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Schlichting</surname>
<given-names>Margaret L</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Toronto</institution>
</institution-wrap>
<city>Toronto</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>In this manuscript, the authors present a wealth of fMRI data at both 3T and 7T to identify a scene-selective region of the intraparietal gyrus (&quot;PIGS&quot;) that appears to have some responsivity to characteristics of ego-motion. In a series of experiments, they delineate the anatomical location of PIGS and functionally differentiate it from nearby V6 and OPA. Evidence for these <bold>valuable</bold> findings is <bold>solid</bold>, but further (a) consideration of whether this region overlaps with others reported previously and (b) support for, or tempering of, the ego-motion claim may be warranted.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91601.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The authors ran a series of experiments with separate subject populations, different stimuli, and on two different MRI scanners (one 3T, one 7T) to establish a scenes-selective region on the intraparietal gyrus that they decided to name PIGS. I think that IPA (intraparietal place area) would also have been a good choice with an allusion to a beverage rather than a domestic animal. The authors show that PIGS can be detected robustly through a series of experiments. They anatomically and functionally separate PIGS from nearby V6, which encodes optic flow. The authors determined that PIGS encodes ego-motion.</p>
<p>Strengths:</p>
<p>
The robust detection of PIGS in several experiments with different sets of participants and on different scanners makes these results convincing. The functional differentiation is well executed.</p>
<p>Weaknesses:</p>
<p>
The distinction of PIGS from nearby OPA, which has also been implied in navigation and ego-motion, is not as clear as it could be.</p>
<p>Impact:</p>
<p>
Overall, this is a valuable contribution to the cognitive neuroscience of the visual system. It shows that there is still room for discovering details of visual processing, given recent advances in scanning technology, statistical methods, and larger sample sizes.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91601.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>
The authors report an extensive series of neuroimaging experiments (at both 3T and 7T) to provide evidence for a scene-selective visual area in the human posterior parietal cortex (PIGS) that is distinct from the main three (parahippocampal place area, PPA; occipital place area, OPA; medial place area, MPA) typically reported in the literature. Further, they argue that in comparison with the other three, this region may specifically be involved in representing ego-motion in natural contexts. The characterization of this scene-selective region provides a useful reference point for studies of scene processing in humans.</p>
<p>Strengths</p>
<p>
One of the major strengths of the work is the extensive series of experiments reported, showing clear reproducibility of the main finding and providing functional insight into the region studied. The results are clearly presented and for the most part, convincing.</p>
<p>Weaknesses</p>
<p>
One of the major weaknesses of the work is the failure to relate the current results to other findings in the literature, making it hard to assess whether it is is a &quot;previously undescribed scene-selective site&quot;.</p>
<p>First, the scene-selective region identified appears to overlap with regions that have previously been identified in terms of their retinotopic properties. In particular, it is unclear whether this region overlaps with V7/IPS0 and/or IPS1. This is particularly important since prior work has shown that OPA often overlaps with v7/IPS0 (Silson et al, 2016, Journal of Vision). The findings would be much stronger if the authors could show how the location of PIGS relates to retinotopic areas (other than V6, which they do currently consider). I wonder if the authors have retinotopic mapping data for any of the participants included in this study. If not, the authors could always show atlas-based definitions of these areas (e.g. Wang et al, 2015, Cerebral Cortex).</p>
<p>Second, recent studies have reported a region anterior to OPA that seems to be involved in scene memory (Steel et al, 2021, Nature Communications; Steel et al, 2023, The Journal of Neuroscience; Steel et al, 2023, biorXiv). Is this region distinct from PIGS? Based on the figures in those papers, the scene memory-related region is inferior to V7/IPS0, so characterizing the location of PIGS to V7/IPS0 as suggested above would be very helpful here as well.</p>
<p>If PIGS overlaps with either of V7/IPS0 or the scene memory-related area described by Steel and colleagues, then arguably it is not a newly defined region (although the characterization provided here still provides new information).</p>
<p>Another reason that it would be helpful to relate PIGS to this scene memory area is that this scene memory area has been shown to have activity related to the amount of visuospatial context (Steel et al, 2023, The Journal of Neuroscience). The conditions used to show the sensitivity of PIGS to ego-motion also differ in the visuospatial context that can be accessed from the stimuli. Even if PIGS appears distinct from the scene memory area, the degree of visuospatial context is an alternative account of what might be represented in PIGS.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91601.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The authors report a scene-selective area in the posterior intraparietal gyrus (PIGS). This area lies outside the classical three scene-selective regions (PPA/TPA, RSC/MPA, TOS/OPA), and is selective for ego-motion.</p>
<p>Strengths:</p>
<p>
The authors firmly establish the location and selectivity of the new area through a series of well-crafted controlled experiments. They show that the area can be missed with too much smoothing, thus providing a case for why it has not been previously described. They show that it appears in much the same location in different subjects, with different magnetic field strengths, and with different stimulus sets. Finally, they show that it is selective for ego-motion - defined as a series of sequential photographs of an egocentric trajectory along a path. They further clarify that the area is not generically motion-selective by showing that it does not respond to biological motion without an ego-motion component to it. All statistics are standard and sound; the evidence presented is strong.</p>
<p>Weaknesses:</p>
<p>
There are few weaknesses in this work. If pressed, I might say that the stimuli depicting ego-motion do not, strictly speaking, depict motion, but only apparent motion between 2m apart photographs. However, this choice was made to equate frame rates and motion contrast between the 'ego-motion' and a control condition, which is a useful and valid approach to the problem. Some choices for visualization of the results might be made differently; for example, outlines of the regions might be shown in more plots for easier comparison of activation locations, but this is a minor issue.</p>
<p>This is a very strong paper.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91601.1.sa4</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kennedy</surname>
<given-names>Bryan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Malladi</surname>
<given-names>Sarala N.</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tootell</surname>
<given-names>Roger BH</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Nasr</surname>
<given-names>Shahin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 1:</bold></p>
<p>Comment 1.1: The distinction of PIGS from nearby OPA, which has also been implied in navigation and ego-motion, is not as clear as it could be.</p>
</disp-quote>
<p>Response1.1: The main functional distinction between TOS/OPA and PIGS is that TOS/OPA responds preferentially to moving vs. stationary stimuli (even concentric rings), likely due to its overlap with the retinotopic motion-selective visual area V3A, for which this is a defining functional property (e.g. Tootell et al., 1997, J Neurosci). In comparison, PIGS does not show such a motion-selectivity.  Instead, PIGS responds preferentially to more complex forms of motion within scenes. In this revision, we tried to better highlight this point in the Discussion (see also the response to the first comment from Reviewer #2).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2:</bold></p>
<p>Comment 2.1: First, the scene-selective region identified appears to overlap with regions that have previously been identified in terms of their retinotopic properties. In particular, it is unclear whether this region overlaps with V7/IPS0 and/or IPS1. This is particularly important since prior work has shown that OPA often overlaps with v7/IPS0 (Silson et al, 2016, Journal of Vision). The findings would be much stronger if the authors could show how the location of PIGS relates to retinotopic areas (other than V6, which they do currently consider). I wonder if the authors have retinotopic mapping data for any of the participants included in this study. If not, the authors could always show atlas-based definitions of these areas (e.g. Wang et al, 2015, Cerebral Cortex).</p>
</disp-quote>
<p>Response 2.1: We thank the reviewers for reminding us to more clearly delineate this issue of possible overlap, including the information provided by Silson et al, 2016.  The issue of possible overlap between area TOS/OPA and the retinotopic visual areas, both in humans and non-human primates, was also clarified by our team in 2011 (Nasr et al., 2011). As you can see in the enclosed figure, and consistent with those previous studies, TOS/OPA overlaps with visual areas V3A/B and V7. Whereas PIGS is located more dorsally close to IPS2-4. As shown here, there is no overlap between PIGS and TOS/OPA and there is no overlap between PIGS and areas V3A/B and V7. To more directly address the reviewer’s concern, in the next revision, we will show the relative position of PIGS and the retinotopic areas (at least) in one individual subject.</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<caption>
<title>The relative location of PIGS, TOS/OPA and the retinotopic visual areas.</title>
<p>The left panel showed the result of high-resolution (7T; voxel size = 1 mm; no spatial smoothing) polar angle mapping in one individual. The right panel shows the location of scene-selective areas PIGS and TOS/OPA in the same subject (7T; voxel size = 1 mm; no spatial smoothing).  While area TOS/OPA shows some overlap with the retinotopic visual areas V3A/B and V7, PIGS shows partial overlap with area IPS2-4.  In both panels, the activity maps are overlaid on the subjects’ own reconstructed brain surface.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-91601-sa4-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Comment 2.2: Second, recent studies have reported a region anterior to OPA that seems to be involved in scene memory (Steel et al, 2021, Nature Communications; Steel et al, 2023, The Journal of Neuroscience; Steel et al, 2023, biorXiv). Is this region distinct from PIGS? Based on the figures in those papers, the scene memory-related region is inferior to V7/IPS0, so characterizing the location of PIGS to V7/IPS0 as suggested above would be very helpful here as well. If PIGS overlaps with either of V7/IPS0 or the scene memory-related area described by Steel and colleagues, then arguably it is not a newly defined region (although the characterization provided here still provides new information).</p>
</disp-quote>
<p>Response 2.2: The lateral-place memory area (LPMA) is located on the lateral brain surface, anterior relative to the IPS (see Figure 1 from Steel et al., 2021 and Figure 3 from Steel et al., 2023). In contrast, PIGS is located on the posterior brain surface, also posterior relative to the IPS. In other words, they are located on two different sides of a major brain sulcus. In this revision we have clarified this point, including the citations by Steel and colleagues.</p>
<disp-quote content-type="editor-comment">
<p>Comments 2.3: Another reason that it would be helpful to relate PIGS to this scene memory area is that this scene memory area has been shown to have activity related to the amount of visuospatial context (Steel et al, 2023, The Journal of Neuroscience). The conditions used to show the sensitivity of PIGS to ego-motion also differ in the visuospatial context that can be accessed from the stimuli. Even if PIGS appears distinct from the scene memory area, the degree of visuospatial context is an alternative account of what might be represented in PIGS.</p>
</disp-quote>
<p>Response 2.3: The reviewer raises an interesting point. One minor confusion is that we may be inadvertently referring to two slightly different types of “visuospatial context”. Specifically, the stimuli used in the ego-motion experiment here (i.e. coherently vs. incoherently changing scenes) represent the same scenes, and the only difference between the two conditions is the sequence of images across the experimental blocks. In that sense, the two experimental conditions may be considered to have the same visuospatial context. However, it could be also argued that the coherently changing scenes provide more information about the environmental layout. In that case, considering the previous reports that PPA/TPA and RSC/MPA may also be involved in layout encoding (Epstein and Kanwisher 1998; Wolbers et al. 2011), we expected to see more activity within those regions in response to coherently compared incoherently changing scenes. These issues are now more explicitly discussed in the revised article.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 3:</bold></p>
<p>Comment 3.1: There are few weaknesses in this work. If pressed, I might say that the stimuli depicting ego-motion do not, strictly speaking, depict motion, but only apparent motion between 2s apart photographs. However, this choice was made to equate frame rates and motion contrast between the 'ego-motion' and a control condition, which is a useful and valid approach to the problem. Some choices for visualization of the results might be made differently; for example, outlines of the regions might be shown in more plots for easier comparison of activation locations, but this is a minor issue.</p>
</disp-quote>
<p>Response 3.1: We thank the reviewer for these constructive suggestions, and we agree with their comment that the ego-motion stimuli are not smooth, even though they were refreshed every 100 ms.  However, the stimuli were nevertheless coherent enough to activate areas V6 and MT, two major areas known to respond preferentially to coherent compared to incoherent motion.</p>
<p>Epstein, R., and N. Kanwisher. 1998. 'A cortical representation of the local visual environment', Nature, 392: 598-601.</p>
<p>Wolbers, T., R. L. Klatzky, J. M. Loomis, M. G. Wutte, and N. A. Giudice. 2011. 'Modality-independent coding of spatial layout in the human brain', Curr Biol, 21: 984-9.</p>
</body>
</sub-article>
</article>