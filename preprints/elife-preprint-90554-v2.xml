<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">90554</article-id>
<article-id pub-id-type="doi">10.7554/eLife.90554</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90554.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.7</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Foveated metamers of the early visual system</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8999-9003</contrib-id>
<name>
<surname>Broderick</surname>
<given-names>William F</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>billbrod@gmail.com</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rufo</surname>
<given-names>Gizem</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7475-5586</contrib-id>
<name>
<surname>Winawer</surname>
<given-names>Jonathan</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1206-527X</contrib-id>
<name>
<surname>Simoncelli</surname>
<given-names>Eero P</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00sekdz59</institution-id><institution>Flatiron Institute, Simons Foundation</institution></institution-wrap>, <city>New York</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zbnvs85</institution-id><institution>Meta, Inc.</institution></institution-wrap>, <city>Menlo Park</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Psychology, New York University</institution></institution-wrap>, <city>New York</city>, <country country="US">United States</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap>, <city>New York</city>, <country country="US">United States</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Courant Institute for Mathematical Sciences, New York University</institution></institution-wrap>, <city>New York</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-11-01">
<day>01</day>
<month>11</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-10-14">
<day>14</day>
<month>10</month>
<year>2025</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP90554</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-08-02">
<day>02</day>
<month>08</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-08-02">
<day>02</day>
<month>08</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.05.18.541306"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-11-01">
<day>01</day>
<month>11</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.90554.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.90554.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.90554.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.90554.1.sa0">Reviewer #2 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>¬© 2023, Broderick et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Broderick et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-90554-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>The ability of humans to discriminate and identify spatial patterns varies across the visual field, and is generally worse in the periphery than in the fovea. This decline in performance is revealed in many kinds of tasks, from detection to recognition. A parsimonious hypothesis is that the representation of any visual feature is blurred (spatially averaged) by an amount that differs for each feature, but that in all cases increases with eccentricity. Here, we examine models for two such features: local luminance and spectral energy. Each model averages the corresponding feature in pooling windows whose diameters scale linearly with eccentricity. We performed perceptual experiments with synthetic stimuli to determine the largest window scaling for which human and model discrimination abilities match (the ‚Äúcritical‚Äù scaling). We used much larger stimuli than those of previous studies, subtending 53.6 by 42.2 degrees of visual angle. We found that the critical scaling for the luminance model was approximately one-fourth that of the energy model and, consistent with earlier studies, that the estimated critical scaling value was smaller when discriminating a synthesized stimulus from a natural image than when discriminating two synthesized stimuli. Moreover, we found that initializing the generation of the synthesized images with natural images reduced the critical scaling value when discriminating two synthesized stimuli, but not when discriminating a synthesized from a natural image stimulus. Together, the results show that critical scaling is strongly affected by the image statistic (pooled luminance vs. spectral energy), the comparison type (synthesized vs. synthesized or synthesized vs. natural), and the initialization image for synthesis (white noise vs natural image). We offer a coherent explanation for these results in terms of alignments and misalignments of the models with human perceptual representations.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Minor text updates in response to reviewer comments.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Vision science is often concerned with what things look like (‚Äúappearance‚Äù), but a long and fruitful thread of research has investigated what humans cannot see, that is, the information they are insensitive to. Perceptual metamers ‚Äî images that are physically distinct but perceptually indistinguishable ‚Äî provide a classic example of such research, used to elucidate loss of visual information. <xref ref-type="bibr" rid="c14">Cohen and Kappauf (1985)</xref> identified this concept in the writings of Isaac Newton, who noted that the color percept elicited by a single wavelength of light could also be created by mixing multiple wavelengths. Cohen and Kappauf traced the perceptual meaning of the word ‚Äúmetamer‚Äù to a 1919 chapter by Wilhelm Ostwald, the first Nobel laureate in chemistry. Color metamers were instrumental in the development of the Young-Helmholtz theory of trichromacy (<xref ref-type="bibr" rid="c28">Helmholtz, 1852</xref>). Specifically, the experimental observation of metamers clarified human sensitivity to light wavelengths, and led to the hypothesis that the human visual system projects the infinite-dimensional physical signal to three dimensions. It took more than a century before the physiological basis for this ‚Äî the three classes of cone photoreceptor ‚Äî was revealed experimentally (<xref ref-type="bibr" rid="c55">Schnapf et al., 1987</xref>).</p>
<p>The visual system also discards a great deal of spatial detail, more so in portions of the visual field farthest from the center of gaze. Specifically, the reduction of visual capabilities with increasing eccentricity has been demonstrated for both acuity (<xref ref-type="bibr" rid="c22">Frisen and Glansholm, 1975</xref>) and contrast sensitivity (<xref ref-type="bibr" rid="c5">Banks et al., 1987</xref>; <xref ref-type="bibr" rid="c53">Robson and Graham, 1981</xref>; <xref ref-type="bibr" rid="c54">Rovamo et al., 1978</xref>), and is reflected in the physiology: fewer cortical resources are dedicated to the periphery (<xref ref-type="bibr" rid="c56">Schwartz, 1977</xref>) and receptive fields in all stages of the visual hierarchy grow with eccentricity (e.g., <xref ref-type="bibr" rid="c16">Daniel and Whitteridge (1961)</xref>; <xref ref-type="bibr" rid="c15">Dacey and Petersen (1992)</xref>; <xref ref-type="bibr" rid="c24">Gattass et al. (1981</xref>, <xref ref-type="bibr" rid="c25">1988</xref>); <xref ref-type="bibr" rid="c41">Maunsell and Newsome (1987)</xref>; <xref ref-type="bibr" rid="c67">Wandell and Winawer (2015</xref>)). This decrease in acuity has been demonstrated by either scaling the size of features, as in the Anstis eye chart (<xref ref-type="bibr" rid="c2">Anstis, 1974</xref>), or by progressively blurring the image, as in <xref ref-type="bibr" rid="c3">Anstis (1998)</xref>; <xref ref-type="bibr" rid="c59">Thibos (2020</xref>). More generally, one can explain this decreasing sensitivity to spatial information with ‚Äúpooling models‚Äù, which compute local averages of image features in windows that grow larger with eccentricity (<xref ref-type="bibr" rid="c4">Balas et al., 2009</xref>; <xref ref-type="bibr" rid="c21">Freeman and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="c33">Keshvari and Rosenholtz, 2016</xref>). These models assume that peripheral representations are qualitatively similar to those of the fovea: the same local computations are performed over larger regions.</p>
<p>Such pooling at increasingly larger scales is ubiquitous throughout the many stages of visual processing. One hypothesis is that each stage performs essentially the same canonical computation, ‚Äúextract features and pool‚Äù, differing based on what features are extracted and the spatial extent of the pooling. Here, we test models based on two kinds of image features ‚Äî one model that averages local luminance (<bold>luminance model</bold>) and one that averages both local spectral energy and luminance (<bold>energy model</bold>). We hypothesize that the more complex feature, spectral energy, is pooled over a larger spatial extent than luminance. We test the spatial pooling extent of these image features using a metamer paradigm. Specifically, we generate images pairs in which one or both images have been manipulated such that the two are <bold>model metamers</bold> (images that are physically distinct but with identical model representations). The pair of model metamers are also perceptual metamers if the human visual system is insensitive to the differences between them, as schematized in <xref rid="fig1" ref-type="fig">figure 1</xref> (see <xref ref-type="bibr" rid="c71">Watson et al. (1986</xref>) for an analogous presentation with respect to sensitivity to spatial and temporal frequency). By comparing model and human perceptual metamers, we investigate how well the models‚Äô sensitivities (and insensitivities) align with those of the human visual system. Note that throughout much of this paper, we refer to ‚Äúvisual stimuli‚Äù to emphasize that our results and interpretation depend on the viewing conditions (distance from viewer, relative location of the fovea, and linear relationship between pixel values and output luminance).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Schematic diagram of perceptual metamers.</title> <p>Each panel contains a two-dimensional depiction of the set of all visual images: every image corresponds to a point in this space and every point in this space represents an image. Perceptual metamers ‚Äî visual stimuli that cannot be reliably distinguished ‚Äî are equivalence classes in this space, which we depict with a set of distinct non-overlapping regions, within which all stimuli are perceptually indistinguishable. <bold>Left</bold>: example stimulus (black point), and surrounding metameric stimuli (region enclosed by black polygon). <bold>Center</bold>: In a hierarchical visual system, in which each stage transforms the signals of the previous stage and discards additional information, every pair of stimuli that are metamers for an early visual area. ùí© <sub>1</sub> are also metamers for the later visual area. ùí©<sub>2</sub>. Thus, metamers of earlier stages are nested within metamers of later stages. The converse does not hold: there are stimuli that. ùí©<sub>1</sub> can distinguish but that. ùí©<sub>2</sub> cannot. <bold>Right</bold>: Two stimulus families used for initialization of our metamer synthesis algorithm: white noise (distribution represented as a grayscale intensity map in the lower right corner) and natural images (distribution represented by the curved gray manifold). Typical white noise samples fall within a single perceptual metamer class (humans are unable to distinguish them). Natural images, on the other hand, are generally distinguishable from each other, as well as from un-natural images (those that lie off the manifold).</p></caption>
<graphic xlink:href="541306v7_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>This procedure rests on the assumption that the visual system processes information hierarchically, in a sequence of stages, and information discarded in early stages cannot be recovered by later stages (the ‚Äúdata-processing inequality‚Äù; note that this assumption does not preclude recurrent processing or feedback from later stages or other modalities. It only asserts that these signals do not carry additional information about the current visual stimulus). For example, metameric color stimuli produce identical cone responses and thus cannot be distinguished by any additional downstream neural processing. Similarly, if two visual stimuli generate identical responses in all neurons at a subsequent stage of processing (e.g., the retinal ganglion cells), the stimuli will appear identical, even if their cone responses differ. This is schematized in the central panel of <xref rid="fig1" ref-type="fig">figure 1</xref>: two stimuli are perceptual metamers if they are indistinguishable in an early visual area, such as ùí©<sub>1</sub> or ùí©<sub>2</sub>.</p>
<p>Combining these pooling models with the metamer paradigm allows us to ask, for a given statistic, ‚Äúover which spatial scales are humans sensitive to changes in this statistic‚Äù. The model metamers are defined as the set of visual stimuli that match a target visual stimulus in the specified image statistic averaged at the spatial scale defined by the model‚Äôs scaling parameter, with (ideally) all other aspects of image content as random as possible. If humans are sensitive to the image statistic at a scale smaller than the one used to synthesize the model metamer, then they may be able to tell the difference between the model metamer and the target image, or to pairs of model metamers that are physically distinct. If humans are sensitive to the image statistic only at the same or larger spatial scale, then they will be unable to distinguish between the set of stimuli, and thus the stimuli are also perceptual metamers.</p>
<p>A number of authors have investigated perceptual metamers using pooling models, using a particular family of ‚Äútexture statistics‚Äù capturing joint responses of oriented filter responses and their nonlinear combinations (e.g., <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011)</xref>; <xref ref-type="bibr" rid="c33">Keshvari and Rosenholtz (2016)</xref>; <xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref>; <xref ref-type="bibr" rid="c17">Deza et al. (2019)</xref>; <xref ref-type="bibr" rid="c10">Brown et al. (2023</xref>)). In this study, we instead investigate models based on local pooling of either luminance or spectral energy. These are arguably the two most fundamental attributes of visual stimuli, and they are commonly used to characterize inputs (visual stimuli) and outputs (neural responses and behavior, as in neurometric and psychometric functions of luminance and contrast). Moreover, these two measures correspond to the mean (luminance) and the variance within frequency channels (spectral energy), the two most common choices for statistical summary of central tendency and dispersion. While humans are sensitive to both measures, neither is intended as a model of specific neural responses in the human visual system. The use of these relatively simple statistics also facilitates experimental design and interpretation. For example: how should one best construct the set of model metamer stimuli to use in the experiment? How should one reconcile differing results that arise from comparing two synthesized stimuli or one synthesized stimulus with the original image stimulus?</p>
<p>The importance of these questions has been highlighted in two previous studies: <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>) found that performance was poorer when participants compared two synthesized stimuli than when they compared a synthesized stimulus to the original image, and <xref ref-type="bibr" rid="c10">Brown et al. (2023</xref>) observed performance differences dependent on base image characteristics. Here, we examine both of these factors, as well as the influence of the distribution of ‚Äúseed‚Äù images used to initialize the stochastic metamer synthesis algorithm. In general, the model metamers synthesized for use in any experiment represent only a small subset of all such metamers, and the procedure by which those samples are generated can have a strong influence on the outcome.</p>
<p>In this study, we synthesized metamers for two different models at multiple spatial scales and measured their perceptual discriminability. For a set of 20 natural images, we used a stochastic gradient descent method to generate metamers for both models, and measured discrimination capabilities of human observers when comparing these with their corresponding original images, as well as with each other. Finally, we compared our results with those of previous studies that used a texture model whose statistics were pooled in eccentricity-scaled regions (<xref ref-type="bibr" rid="c21">Freeman and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="c65">Wallis et al., 2019</xref>), and found that they were consistent.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Foveated pooling models</title>
<p>We constructed foveated models of human perception that capture sensitivity to local luminance and spectral energy (see <xref rid="fig2" ref-type="fig">figure 2</xref>). Both models are ‚Äúpooling models‚Äù (<xref ref-type="bibr" rid="c4">Balas et al., 2009</xref>; <xref ref-type="bibr" rid="c21">Freeman and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="c33">Keshvari and Rosenholtz, 2016</xref>; <xref ref-type="bibr" rid="c65">Wallis et al., 2019</xref>), which compute statistics as weighted averages within overlapping local windows. A specific pooling model is characterized by both the quantities that are pooled and the shapes/sizes of the pooling windows. These regions are analogous to neural receptive fields, whose sizes grow proportionally with distance from the fovea, as documented in monkey physiology and human fMRI (e.g., <xref ref-type="bibr" rid="c24">Gattass et al. (1981)</xref>; <xref ref-type="bibr" rid="c67">Wandell and Winawer (2015</xref>)).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Two pooling models.</title> <p>Both models compute local image statistics within a Gaussian-weighted window that is separable in a log-polar coordinate system, such that radial extent is approximately twice the angular extent (red contours indicate half-maximum levels). Windows are uniformly spaced in a log-polar grid, with centers separated by one standard deviation. A single scaling factor governs the size of all pooling windows. The luminance model (left) computes average luminance. The spectral energy model (right)computes average spectral energy at 4 orientation and 6 scales, as well as luminance, for a total of 25 statistics per window. Spectral energy is computed using the complex steerable pyramid constructed in the Fourier domain (<xref ref-type="bibr" rid="c57">Simoncelli and Freeman, 1995</xref>), squaring and summing across the real and imaginary components. Full resolution version of this figure can found on the OSF.</p></caption>
<graphic xlink:href="541306v7_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As in previous studies (<xref ref-type="bibr" rid="c21">Freeman and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="c33">Keshvari and Rosenholtz, 2016</xref>; <xref ref-type="bibr" rid="c65">Wallis et al., 2019</xref>), we used arrays of smooth overlapping windows that are separable and of constant size when expressed in polar angle and log-eccentricity (consistent with the approximate log-polar geometry of visual cortical maps (<xref ref-type="bibr" rid="c56">Schwartz, 1977</xref>)), as illustrated in <xref rid="fig2" ref-type="fig">figure 2</xref>. Like these previous studies, our windows were radially-elongated (the radial extent is roughly twice the angular extent), but unlike the previous studies, we used Gaussian profiles separated by one standard deviation, yielding a smoother representation and minimal ringing and blocking artifacts in the synthesized stimuli. The log-polar mapping ensures that the size of the pooling windows, in both radial and angular directions, is proportional to their distance from the fovea, and this proportionality is controlled by a single <bold>scaling parameter</bold>, <italic>s</italic>. Increasing this value increases the size of all pooling regions, resulting in a larger and more diverse set of metameric stimuli, in a nested manner (<xref rid="fig3" ref-type="fig">figure 3</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Metamers in human perception and pooling models.</title> <p>Panels depict a two-dimensional stimulus space (see figure 1), with polygonal regions indicating groups of perceptually indistinguishable stimuli (human metamers). Pooling models ‚Ñ≥<sup>(<italic>i</italic>)</sup> determine the statistics that are pooled, with pooling extent controlled by scaling parameter, <italic>s</italic>. Each pooling model defines a set of <italic>model metamers</italic>: sets of stimuli that are physically different but whose model outputs are identical to those of the target (original) image <italic>T</italic> (example sets indicated by ellipses). We generate model metamer samples using an optimization procedure: starting from initial image <italic>I</italic><sub><italic>k</italic></sub> we adjust the pixel values until their pooled statistics match those of the target. The shapes and sizes of metameric regions (ellipses) depend on the model (<italic>i</italic>), the scaling parameter (<italic>s</italic>), the statistics of the target imagW <inline-formula><inline-graphic xlink:href="541306v7_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, as well as the initial images (<italic>I</italic>) and the synthesis algorithm. For a given set of statistics <italic>Œ∏</italic>, increasing the scaling value by factor <italic>Œ± &gt;</italic> 1 increases the size of the metamer set, and any stimulus that is a metamer for <inline-formula><inline-graphic xlink:href="541306v7_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> will also be a metamer for <inline-formula><inline-graphic xlink:href="541306v7_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (i.e., larger desaturated ellipses contain the smaller saturated ellipses). <italic>Critical scaling</italic> is the largest <italic>s</italic> for which all model metamers are human metamers (smaller ellipses). <bold>Left</bold>: Model with parameters that produce approximate alignment with human metamers. <bold>Right</bold>: Model with different pooled statistics that yield metamers (blue ellipses) that are poorly aligned with human metamers: at critical scaling, there are human metamers that are not model metamers. This mismatch cannot be resolved by adjusting the scaling parameter: increasing <italic>s</italic> such that all human metamers are also model metamers (larger ellipse) will also yield model metamers that are not human metamers. See also <xref ref-type="bibr" rid="c20">Feather et al. (2023</xref>), figure 1.</p></caption>
<graphic xlink:href="541306v7_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the current study, we examine two models that compute different statistics within their pooling windows. The <bold>luminance model</bold> pools pixel intensities, and thus, two luminance model metamers have the same average luminance within corresponding pooling windows. This model serves as a baseline for comparison to models that pool more image statistics, such as the energy model used here and the texture model used in <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011)</xref>; <xref ref-type="bibr" rid="c65">Wallis et al.(2019</xref>). Since the model‚Äôs responses are insensitive to the highest frequencies, luminance model metamers include blurred versions of the target image (in which high frequencies are discarded), but also variants of the target image in which high frequencies are randomized or even amplified. In general, synthesized luminance model metamers inherit the high frequency content of their initialization image, as can be seen in <xref rid="fig4" ref-type="fig">figure 4</xref>, middle row. While the high-scaling model metamer is clearly perceptually distinct from the target image (regardless of observer fixation location), the low-scaling stimulus is diffcult to discriminate from the target when fixating at the center of the stimulus (i.e., when the human observer‚Äôs fovea is aligned with the model fovea).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Example synthesized model metamers.</title> <p><bold>Top:</bold> Target image. <bold>Middle:</bold> Luminance model metamers, computed for two different scaling values (values as indicated, red ellipses to right of fixation indicate pooling window contours at half-max at that eccentricity). The left image is computed with a small scaling value, and is a perceptual metamer for most subjects: when fixating at the cross in the center of the stimulus, the two stimuli appear perceptually identical to the target image. Note, however, that when fixating in the periphery (e.g., the blue box), one can clearly see that the stimulus differs from the target (see enlarged versions of the foveal and peripheral neighborhoods to right). The right image is computed with a larger scaling value, and is no longer a erceptual metamer (for any choice of observer fixation). <bold>Bottom:</bold> Energy model metamers. Again, the left image is computed with a small scaling value and is a perceptual metamer for most observers when fixated on the center cross. Peripheral content (e.g., blue box) contains more complex distortions, readily visible when viewed directly. The right image, computed with a large scaling value, differs in appearance from the target regardless of observer fixation. Full resolution version of this figure can be found on the OSF.</p></caption>
<graphic xlink:href="541306v7_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The <bold>spectral energy model</bold> pools the squared outputs of oriented bandpass filter responses at multiple scales and orientations, as well as the pixel intensities. As such, its statistics are a superset of the luminance model, and its metamers are a subset. The energies are computed using a complex steerable pyramid, which decomposes images into frequency channels selective for 6 different scales and 4 different orientations. Energy is computed by squaring and summing the real and imaginary responses (arising from evenand odd-symmetric filters) within each channel. These energies, along with the luminances, are then averaged within the spatial pooling windows. Thus, a pair of energy model metamers have the same average oriented energy and luminance within each of these windows. The bottom row of <xref rid="fig4" ref-type="fig">figure 4</xref> shows energy model metamers for two different scaling values. The low scaling value for the energy model is approximately matched to the higher scaling value for the luminance model, while the higher scaling value is approximately that of the energy model from <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>). The high-scaling model metamer is perceptually distinct from the target stimulus, and also perceptually distinct from the high-scaling luminance model metamer. The low-scaling model metamer, on the other hand, is diffcult to distinguish from the original image (when fixating at the center), but is readily distinguished when one fixates peripherally.</p>
<p>The appearance of these two model metamers reflects both the measurements that are being matched and the seed images used to initialize synthesis. The luminance model matches average pixel intensity, but has no constraints on spatial frequency, and thus its metamers retain the high frequency content of the initial white-noise images. The energy model, on the other hand, matches the average contrast energy at all scales and orientations, but discards exact position information (which depends on phase structure). Hence, unlike the luminance model metamers, it reduces the high frequency power to match the typical content of natural images, and essentially scrambles the phase spectrum, leading to the cloud-like appearance of its metamers.</p>
<p>As can be seen in <xref rid="fig4" ref-type="fig">figure 4</xref>, both models can generate perceptual metamers. More generally, all pooling models can generate perceptual metamers if the scaling value is made suiciently small (in the limit as scaling goes to zero, the model metamers must be identical to the target, in every pixel). For statistics that capture features relevant to human perception, metamers can be achieved with windows whose size is matched to the scale of human perceptual sensitivity. The maximal scaling at which synthetic stimuli are perceptual metamers is thus highly dependent on the choice of underlying statistics: in our examples, the energy model perceptual metamer (<xref rid="fig4" ref-type="fig">figure 4</xref>, bottom left) is generated with a scaling value about six times larger than that for the luminance model perceptual metamer (middle left), and about five times smaller than those reported in <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>) using higher-order texture statistics. The goal of the present study is to use psychophysics to find the largest scaling value for which these two models generate perceptual metamers, known as the <bold>critical scaling</bold>.</p>
</sec>
<sec id="s2b">
<title>Psychophysical experiment</title>
<p>We synthesized model metamers matching 20 different natural images (the <bold>target images</bold>) collected from the authors‚Äô personal collections, as well as from the UPenn Natural Image Database (<xref ref-type="bibr" rid="c60">Tkaƒçik et al. (2011</xref>), extended dataset of urban images provided by David Brainard). The images were chosen to span a variety of natural image content types, including buildings, animals, and natural textures (<xref rid="fig5" ref-type="fig">figure 5</xref>). Model metamers were generated via gradient descent on the squared error between target and synthetic pooled statistics, and initialized with either an image of white noise or another image drawn from the set of target images.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Target images used in the experiments.</title> <p>Images contain a variety of content, including textures, objects, and scenes. All are RAW camera images, with values proportional to luminance and quantized to 16 bits. Images were converted to grayscale, cropped to 2048 √ó 2600 pixels, displayed at 53.6 √ó 42.2 degrees, with intensity values rescaled to lie within the range of [0.05, 0.95] of the display intensities. All subjects saw target images 1‚Äì10, half saw 11‚Äì15, and half saw 16‚Äì20. A full resolution version of this figure can be found on the OSF.</p></caption>
<graphic xlink:href="541306v7_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the experiments, observers discriminated two grayscale stimuli, of size 53.6 by 42.2 degrees, sequentially displayed. Each stimulus was separated into two halves by a superimposed vertical bar (mid gray, 2 deg wide, see <xref rid="fig15" ref-type="fig">figure 15</xref>). One side, selected at random on each trial, was identical in the two intervals, while the other differed (e.g., during the second interval, one half of the stimulus contains the target image, the other a synthesized model metamer). Each stimulus was presented for 200 msecs, separated by a 500 msec blank (mid gray) interval, and followed by a a blank screen with text prompting the observer to report which side of the stimulus had changed.</p>
</sec>
<sec id="s2c">
<title>Critical scaling is four times smaller for the luminance than the energy model</title>
<p>We fit the behavioral data using the 2-parameter function introduced in <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>), estimating the critical scaling (<italic>s</italic><sub><italic>c</italic></sub>) and maximum <italic>d</italic><sup>‚Ä≤</sup> (<italic>a</italic>) parameters with a Markov Chain Monte Carlo procedure and a hierarchical, partial-pooling model similar to that used by <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>). For a given model and comparison, performance increases monotonically with scaling, and is fit well by this particular psychometric function (<xref rid="fig6" ref-type="fig">figure 6A</xref>). The exception is the synthesized vs. synthesized comparison for the luminance model, for which performance remains poor at all scales (see next section). In the original vs. synthesized cases (for both models), performance is near chance for the smallest tested scaling values and exceeds 90% for the largest. The critical scaling values, as seen in <xref rid="fig6" ref-type="fig">figure 6B</xref>, are approximately 0.016 for the luminance model and 0.06 for the energy model. Thus, as expected, the critical scaling for the model depends on the statistic being pooled: humans are sensitive to changes in local luminance at a much smaller scale than local spectral energy.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Performance curves and their corresponding parameter values for different models and stimulus comparisons.</title> <p>The luminance model has a substantially smaller critical scaling than the energy model, and original vs. synthesized comparisons yield smaller critical scaling values than synthesized vs. synthesized comparisons. (A) Psychometric functions, expressing probability correct as a function of scaling parameter, for both energy and luminance models (aqua and beige, respectively), and original vs. synthesized (solid line) and synthesized vs. synthesized (dashed line) comparisons. Data points represent average values across subjects and target images, 4320 trials per data point except for luminance model synthesized vs. synthesized comparison, which have only 180 trials per data point (one subject, five target images). Lines represent the posterior predictive means of fitted curves across subjects and target images, with the shaded region indicating the 95% high-density interval (HDI, <xref ref-type="bibr" rid="c36">Kruschke (2015</xref>)). (B) Estimated parameter values, separated by target image (left) or subject (right). Top row shows the critical scaling value and the bottom the value of the maximum <italic>d</italic><sup>‚Ä≤</sup> parameter. Points represent the posterior means, shaded regions the 95% HDI, and horizontal dashed lines and shaded regions the global means and 95% HDI. Note that the luminance model, synthesized vs. synthesized comparison is not shown, because the data are poorly fit (panel A, beige dashed line).</p></caption>
<graphic xlink:href="541306v7_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>Critical scaling is smaller for original vs. synthesized comparisons than synthesized vs. synthesized comparisons</title>
<p>For both luminance and energy models, it is generally easier to distinguish an original image stimulus from a synthesized stimulus than to distinguish two synthesized stimuli initialized with different white noise seeds (with same target image and scaling value), as also reported in <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>) for their pooled texture model. For the luminance model, discrimination of two synthesized stimuli is nearly impossible at all scaling values (note that we only have data for one participant, an author, for this comparison; we did not wish to subject the other participants to this nearly impossible task). For the energy model, discriminating two synthesized stimuli is possible but diffcult, with performance only approaching 60%, on average (although note that there are substantial differences across subjects, see <xref rid="fig6" ref-type="fig">figure 6B</xref> and appendix 5). The critical scaling value for this comparison, approximately 0.25, is comparable to that reported in <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>) for their pooled energy model. The asymptotic performance, however, is much lower in our data. We attribute this to experimental differences (see appendix section 3).</p>
<p>The diffculty of differentiating between two synthesized stimuli is striking, as illustrated in <xref rid="fig7" ref-type="fig">figure 7</xref>. In the limit of global pooling windows, luminance metamers are samples of white noise, which cannot be distinguished when presented at the resolution and extent used for the stimuli in this study (<xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>) made a similar point when discussing their use of the original vs. synthesized task). Analogously, synthesis with the energy model forces local orientated spectral energy to match, without explicitly constraining the phase. Two instances of phase scrambling within peripheral windows are not easily discriminable, even though either of the two might be discriminable from a stimulus with more structure. The diffculty discriminating between stimulus pairs like those shown in <xref rid="fig7" ref-type="fig">figure 7</xref> may arise early or late in processing, up to and including memory. Wherever this diffculty arises, it is more pronounced for the synth vs. synth comparisons. These results complicate the interpretation of the critical scaling value, which we return to in the discussion.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Comparison of two synthesized metamers is more diffcult than comparison of a synthesized metamer with the original image.</title> <p>For the highest tested scaling value (1.5) the original vs. synthesized comparison is trivial while the synthesized vs. synthesized comparison is diffcult (energy model) or impossible (luminance model). <bold>Top:</bold> target image. <bold>Middle:</bold> Two luminance model metamers, generated from different initial uniform noise images. <bold>Bottom:</bold> Two energy model metamers, generated from different initial uniform noise images. All four of the model metamers can be easily distinguished from the natural image at top (original vs. synthesized), but are diffcult to distinguish from each other, despite the fact that their pooling windows have grown very large (synthesized vs. synthesized). Full resolution version of this figure can be found on the OSF.</p></caption>
<graphic xlink:href="541306v7_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2e">
<title>Asymptotic discrimination performance depends on target image content, but critical scaling does not</title>
<p>To the extent that the models (at critical scaling) capture something important about human perception, stimulus pairs that are model metamers will be perceptual metamers, and hence discrimination should be at chance. Neither model offers predictions of perceptual discriminability (they are deterministic, and do not specify any method of decoding or comparing stimuli). Consistent with this, the critical scaling, which measures the point at which stimulus pairs become indistinguishable, does not vary much across target images for a given model and comparison, unlike performance at super-threshold scaling values and the asymptotic levels of <italic>d</italic><sup>‚Ä≤</sup> (<xref rid="fig6" ref-type="fig">figure 6B</xref>). Variations in max <italic>d</italic><sup>‚Ä≤</sup> are especially clear in the target image-specific psychometric functions for the original vs. synthesized energy model comparison (<xref rid="fig8" ref-type="fig">figure 8</xref>). Specifically, for the llama target image, performance only rises slightly above chance, even at very large scaling windows. The respective target images in panel B suggest an explanation: much of the llama image is cloud-like, while the nyc image is full of sharp edges in the cardinal directions, with arise from precise alignment of phases across positions and scales. As discussed above, synthetic energy model metamers have matching local oriented spectral energy, with randomized phase information; in order to generate sharp, elongated contours for the buildings of nyc, the windows must be very small. Conversely, the appearance of the llama is captured even when the pooling windows are large. Thus, when scaling is larger than critical scaling, some comparisons become easy and some do not. However, this pattern depends on the interaction between the model‚Äôs sensitivities and the target image content; this pattern does not hold for the luminance model, or for synthesized vs. synthesized comparisons, for which both the llama and nyc target images exhibit typical performance. See appendix 5 for more details.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><title>The interaction between image content and model sensitivities greatly affects asymptotic performance (especially for the synthesized vs. synthesized comparison using the energy model) while critical scaling does not vary as much.</title> <p>(A) Performance for each target image, averaged across subjects, comparing synthesized stimuli to natural image stimuli. Most target images show similar performance, with one obvious outlier whose performance never rises above 60%. Data points represent the average across subjects, 288 trials per data point for half the images, 144 per data point for the other half. Lines represent the posterior predictive means across subjects, with the shaded region giving the 95% HDI. (B) Example energy model metamers for two extreme target images. The top row (nyc) is the target image with the best performance (purple line in panel A), while the bottom row (llama) has the worst performance (red line in panel A). In each row, the leftmost image is the target image, and the next two show model metamers with the lowest and highest tested scaling values for this comparison. Full resolution version of this figure can be found on the OSF</p></caption>
<graphic xlink:href="541306v7_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We thus find evidence that although super-threshold performance depends on the interaction between target image content and model sensitivities, critical scaling is consistent across target images, in contrast with <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>).</p>
</sec>
<sec id="s2f">
<title>Performance in synthesized vs. synthesized comparisons is affected by the distribution of synthesis initialization images</title>
<p>The two types of comparisons shown in <xref rid="fig6" ref-type="fig">figure 6</xref> ‚Äî original vs. synthesized and synthesized vs. synthesized ‚Äî show very different critical scaling values. Specifically, for a particular scaling value and set of image statistics, some stimulus pairs are much easier to discriminate than others. We hypothesize that metamers synthesized from white noise seeds are restricted to a relatively small region of the full set of model metamers. As a result, these stimuli are more perceptually similar to each other than they are to the target stimulus. To generate metamers outside of this set, we also used other natural images from our data set to initialize the synthesis procedure (which was not done in previous studies, <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011)</xref>; <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>)).</p>
<p><xref rid="fig9" ref-type="fig">Figure 9A</xref> shows behavior for these additional comparisons in a single subject, sub-00. Changing the initialization image has a large effect on the synthesized vs. synthesized comparison but little-to-no effect on the original vs. synthesized comparison. For synthesized vs. synthesized, initializing with a different natural image improves performance compared to initializing with white noise, but is still worse than performance for original vs. synthesized.</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9.</label>
<caption><title>Initializing model metamers with natural images does not affect performance in the original vs. synthesized comparison, but reduces critical scaling and increases max <italic>d</italic><sup>‚Ä≤</sup> for the synthesized vs. synthesized comparison.</title> <p>Note all data in this figure is for a single subject and 15 of the 20 target images. (A) Probability correct for one subject (sub-00), as a function of scaling. Each point represents the average of 540 trials (over all fifteen target images), except for the synthesized vs. synthesized luminance model white noise comparison (averaged over 5 target images). Vertical black line indicates scaling value where diffculty ran from chance to 100%, based on initialization and comparison, as discussed in panel B. (B) Three comparisons corresponding to the three psychophysical curves intersected by the vertical black line in panel A. The small image next to each model metamer shows the image used to initialize synthesis. See text for details. Full resolution version of this figure can be found on the OSF.</p></caption>
<graphic xlink:href="541306v7_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10.</label>
<caption><title>Parameter values for the comparisons shown in figure 9A (Top: critical scaling value; Bottom: max <italic>d</italic><sup>‚Ä≤</sup>).</title> <p>Data shown is subject who completed all comparisons. Points represent the posterior means, shaded regions the 95% HDI, and horizontal dashed shaded regions average across all shown stimuli for this subject. Note that the luminance model, synthesized vs. synthesized: white comparison is not shown in this panel, because the data was poorly fit by this curve.</p></caption>
<graphic xlink:href="541306v7_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig9" ref-type="fig">Figure 9B</xref> shows three comparisons involving five metamers arising from different initializations, each with scaling corresponding to the vertical line in panel A, but with dramatically different human performance. The top row shows the easiest comparison, between the original image stimulus and a synthesized stimulus initialized with a different natural image (bike); the subject was able to distinguish these two stimuli with near-perfect accuracy (in this case, when comparing against a natural image stimulus, performance is identical regardless of whether the metamer was initialized with white noise or natural image). The bottom row shows the hardest comparison, between two synthesized stimuli initialized with different samples of white noise. As discussed above, comparing two stimuli of this type is diffcult even with large pooling windows; at this scaling level, humans are insensitive to the differences between them, and so performance was at chance. The middle row shows two synthesized stimuli, initialized with different natural images, which the subject was able to distinguish with moderate accuracy. When comparing these two stimuli, one can see features in the periphery that remain from the initial image (e.g., traces of the lines between tiles are present in the bottom right corner of the tile-initialized stimulus, while remnants of the power lines are present in the top right of the highway-initialized stimulus). Even when fixating, the subject was able to use these features to distinguish the two stimuli, i.e., the human was sensitive to them while the model was not. This reinforces the notion that the initialization of the synthesis process is important. In both the middle and bottom row, both stimuli are synthesized (i.e., neither row contains the target image) yet one comparison is much harder than the other.</p>
<p>Note that the experiments discussed in this section were only carried out by a single participant. We do not believe this substantially affects our conclusions. This subject was an author (and thus an expert observer), and has an average critical scaling and the highest max <italic>d</italic><sup>‚Ä≤</sup> value for the comparisons we have multiple subjects‚Äô data (see <xref rid="fig6" ref-type="fig">figure 6</xref>); as we largely interpret the critical scaling, values of which are quite consistent across observers, their results are therefore likely typical. While our ability to precisely estimate this critical scaling is limited by the number of subjects, we believe that the broad trends that we focus on are likely robust. That is, while the exact critical scaling values will likely vary across participants, we believe that starting from a natural image will have a negligible effect on the critical scaling of the energy model original vs. synth task, while doing so for the energy model synth vs. synth task will lead to a result intermediate between the two energy model white-noise initialization comparisons. Finally, we gathered three hours of data for these two energy model natural image comparisons, allowing us to fit the full psychophysical curve. See the methods section Session and block organization for a full enumeration of the trials.</p>
<p>As with the comparison (but unlike target image content), the initialization of the synthesis procedure has a substantial effect on our experimental estimates of critical scaling.</p>
</sec>
<sec id="s2g">
<title>Critical scaling is not determined by model dimensionality</title>
<p>For each model, the number of statistics is proportional to the number of pooling regions, and thus decreases quadratically with scaling. <xref ref-type="table" rid="tbl1">Table 1</xref> shows average critical scaling values across all conditions, along with the corresponding number of model statistics. We can see that critical scaling does not correspond to a fixed number of statistics. We should also note that if one were to use the model outputs as a compressed representation of the image, the number of statistics in each representation is almost certainly an overcount, for several reasons. First, in order to ensure that the Gaussian pooling windows uniformly tile the image, the most peripheral windows in the model have the majority of their mass off the image, which is necessary to avoid synthesis artifacts. Second, for the energy model, we did not attempt to determine how the precise number of scales or orientations affected metamer synthesis, and currently all scales are equally-weighted across the image. As the human visual system is insensitive to high frequencies in the periphery and low frequencies in the fovea, this is probably unnecessary, and so some of these statistics can likely be discarded. Finally, our pooling windows are highly overlapping and thus the pooled statistics are far from independent; this redundancy means that the effective dimensionality of our model representations is less than the quoted number of statistics.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Critical scaling (posterior mean over all subjects and target images) and number of statistics (as a percentage of number of input image pixels - a type of &quot;compression rate&quot;), for each model and comparison.</title> <p>Note that the critical scaling for original vs. synthesized comparisons does not result in the same number of statistics across models and, in particular, at their critical scaling values, all models have dimensionality less than that of the input image.</p></caption>
<graphic xlink:href="541306v7_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>In summary, critical scaling does not reflect a fixed number of statistics. We instead believe it reflects the spatial scale at which human perception is sensitive to changes in each of these statistics.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We measured perceptual discriminability of wide field-of-view metamers arising from foveated pooling of two visual attributes. We found that performance depended on the model type (i.e., the type of image features that are pooled), the nature of the comparison (original vs. synthesized, or synthesized vs. synthesized), the seed image used for metamer synthesis, and to a lesser extent, the natural image target. Specifically, critical scaling was much smaller for the luminance than for the energy model, and much smaller for original vs. synthesized than for synthesized vs. synthesized comparisons. For original vs. synthesized comparisons, critical scaling values were also reduced when the synthesis procedure was initialized from another natural image rather than white noise. Finally, asymptotic performance, but not critical scaling, was affected by target image content. Below, we consider why each of these factors affect performance.</p>
<sec id="s3a">
<title>Why does critical scaling depend on the feature that is pooled?</title>
<p>Visual representations are formed through a cascade of transformations. An appealing hypothesis is that each of these is comprised of the same canonical ‚Äúblurred feature extraction‚Äù computation, differing only in the choice of feature and the spatial extent of the blurring (<xref ref-type="bibr" rid="c23">Fukushima, 1980</xref>; <xref ref-type="bibr" rid="c18">Douglas et al., 1989</xref>; <xref ref-type="bibr" rid="c39">LeCun et al., 1989</xref>; <xref ref-type="bibr" rid="c27">Heeger et al., 1996</xref>; <xref ref-type="bibr" rid="c52">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="c11">Bruna and Mallat, 2013</xref>). In the perception literature, <xref ref-type="bibr" rid="c40">Lettvin (1976</xref>) provides an early, informal description of this ‚Äúcompulsory feature averaging‚Äù as an explanation for the degradation of peripheral percepts, non-foveated versions have been described in <xref ref-type="bibr" rid="c44">Parkes et al. (2001)</xref>; <xref ref-type="bibr" rid="c47">Pelli et al. (2004)</xref>; <xref ref-type="bibr" rid="c26">Greenwood et al. (2009</xref>), and foveated proposals appear in <xref ref-type="bibr" rid="c4">Balas et al. (2009)</xref>; <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>). In these, stages are distinguished by their features and the scaling of the pooling regions with eccentricity. Similarly, in the primate visual system, features represented in successive stages of processing become more complex and receptive fields increase in size. In particular, the optics and photoreceptors pool the incident light over small regions, V1 pools spectral energy over larger regions, V2 pools texture-related statistics over yet larger regions, and so forth. Consistent with this view, we find that for synthetic metamer stimuli, the features pooled by the model have a dramatic effect on the critical scaling value, which is approximately four times larger for the energy model than for the luminance model in the original vs. synthesized comparison (<xref rid="fig6" ref-type="fig">figure 6</xref> and <xref ref-type="table" rid="tbl1">table 1</xref>). The values of critical scaling as a function of image statistic and comparison type are summarized in <xref rid="fig11" ref-type="fig">figure 11</xref>, along with the results of <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>) and <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>) obtained for model of texture (<xref ref-type="bibr" rid="c49">Portilla and Simoncelli, 2000</xref>). Overall, critical scaling for original vs. synthetic comparisons follows approximate ratios of 1:4:12 for luminance:spectral-energy:texture features (filled circles, <xref rid="fig11" ref-type="fig">figure 11</xref>).</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure 11.</label>
<caption><title>Critical scaling values for the two pooling models presented in this paper (Luminance and Energy) along with a Texture model (data from Freeman and Simoncelli (2011) and Wallis et al. (2019), averaging across the two image classes).</title> <p>Filled points indicate original vs. synthesized comparisons, while unfilled points indicate synthesized vs. synthesized comparisons (for the luminance model, this is effectively infinite, since participants were unable to perform the task for any scaling value). For all three models, critical values are smaller in the original vs. synthesized comparison than the synthesized vs. synthesized one, but this effect decreases with increasing complexity of image statistics. Our critical scaling values for synth vs. synth comparisons of the energy model are consistent with those reported by <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>).</p></caption>
<graphic xlink:href="541306v7_fig11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3b">
<title>Why does critical scaling depend on comparison type?</title>
<p>We found large effects of comparison type on performance (<xref rid="fig11" ref-type="fig">figure 11</xref>, filled vs. unfilled). Specifically, for the energy model the critical scaling for synthesized vs. synthesized was about four times larger than that for original vs. synthesized. For the luminance model, participants were generally unable to discriminate any pairs for the synthesized vs. synthesized comparison. These effects arise from an interaction between human perception, the model, and the synthesis process.</p>
<p>As shown in <xref rid="fig12" ref-type="fig">figure 12</xref>, the synthesis procedure can bias the sampling of model metamers such that two synthesized stimuli are metameric with each other, but distinguishable from the target stimulus. The idealized version implicitly assumes that the synthesized stimuli sample the manifold of possible model metamers broadly, but our synthesis procedure (as with that of <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>) and <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>)) does not guarantee this.</p>
<fig id="fig12" position="float" fig-type="figure">
<label>Figure 12.</label>
<caption><title>Illustration of how critical scaling value can depend on comparison type.</title> <p>Left: For the original vs. synthetic comparison, critical scaling corresponds to the largest ellipse such that the synthetic image is indistinguishable from (i.e., lies within the same perceptual region as) the target (original) image <italic>T</italic>. Right: Example configuration in which synthetic vs. synthetic comparisons lead to a larger critical scaling (with correspondingly larger ellipse). The two synthesized images are indistinguishable from each other, but not from the target.</p></caption>
<graphic xlink:href="541306v7_fig12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The conceptual framework we outlined in this paper emphasizes deterministic information loss, with representations projected from a high-dimensional space onto a lower-dimensional manifold. Under these conditions, metamerism is transitive: all stimuli within an equivalence class are indistinguishable from each other, and all stimulus pairs from different classes are distinguishable. However, the inability to discriminate stimuli is due both to reductions of dimensionality as well as noise in neural responses, a conceptualization underlying the use of signal detection theory and ideal observer models in perception. Stochastic processes can cause a lack of transitivity: two stimuli might each differ from a target by less than the discrimination threshold, but still be discriminable from each other. This could contribute to the differences arising between comparison types, and is not captured by the metamer paradigm.</p>
</sec>
<sec id="s3c">
<title>Why does this comparison-type dependency decrease with feature complexity?</title>
<p>The effect of comparison type was previously reported by <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>) and <xref ref-type="bibr" rid="c17">Deza et al. (2019</xref>) in models pooling texture-like statistics. As seen in <xref rid="fig11" ref-type="fig">figure 11</xref>, the ratio in critical scaling between the two types of comparisons decreases as the model features become more complex: infinite for luminance, roughly quadruple for energy, and less than double for texture. One potential explanation for this observation is that these computations are being performed in deeper stages of the visual hierarchy and there are progressively fewer opportunities to discard information later in the hierarchy. For example, the difference in V1 responses for a pair of stimuli may be discarded in later stages, whereas in a later area (e.g., area IT), there are fewer remaining stages of processing in which information can be discarded. This may explain why we see no overlap between the critical scaling values for original vs. synthesized and synthesized vs. synthesized comparisons across target images in <xref rid="fig6" ref-type="fig">figure 6B</xref>, whereas <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>) find substantial overlap for the texture model.</p>
<p>Another potential explanation is schematized in <xref rid="fig13" ref-type="fig">figure 13</xref>: for the luminance model, the model metamer classes grow with scaling <italic>s</italic> in such a way that model metamers generated from white noise initialization fall within the same perceptual metamer class for a wide range of <italic>s</italic> values. This is an extreme case of the synthesis issue described in the previous section. Conversely, as the texture model‚Äôs critical scaling dependence on comparison type is much weaker, we can hypothesize that its metamer classes are such that model metamers synthesized from white noise readily fall into distinct perceptual metamer classes, leading to a critical scaling value that is similar to the value for the original vs. synth comparison.</p>
<fig id="fig13" position="float" fig-type="figure">
<label>Figure 13.</label>
<caption><title>Illustration of how discrepancies between the original vs. synth and synth vs. synth comparisons can be model-dependent.</title> <p>Upper panels depict a hypothetical luminance model, for which pairs of synthesized stimuli are indistinguishable for all scaling values (i.e., they lie within the same perceptual metamer class for both the small ellipse on the left, and for larger ellipse on the right). In this case, critical scaling cannot be estimated (indicated by dashes in ellipse). Lower panels depict a hypothetical texture model, for which synth vs. synth comparisons yield a critical scaling value (right) only slightly larger than that obtained for original vs. synth (left). Energy model metamers (not shown) lie between these two extremes (see figure 11).</p></caption>
<graphic xlink:href="541306v7_fig13.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3d">
<title>Why does critical scaling sometimes depend on synthesis initialization?</title>
<p>Our experimental results demonstrate that critical scaling depends on the comparison type and feature complexity. We also find that it depends on synthesis initialization. Specifically, the set of metamer stimuli generated for our experiments is a biased sample from the space of all possible model metamers, and is particularly dependent on the distribution of initialization images. This effect is especially noticeable in the case of the luminance model, as discussed in the previous section. We conjectured that natural images are more likely than synthetic images to include information that the human visual system has evolved to discriminate and hence doesn‚Äôt discard even in later processing. As such, initialization with natural images can provide an intuitive method of sampling from a different portion of the manifold of possible model metamers. The schematic presented in <xref rid="fig14" ref-type="fig">figure 14</xref> provides an illustration of how this might occur.</p>
<fig id="fig14" position="float" fig-type="figure">
<label>Figure 14.</label>
<caption><title>Illustration of how discrepancies between original vs. synth and synth vs. synth comparisons can depend on the image used to initialize synthesis.</title> <p>Initializing with white noise (as is commonly done) can lead to a large difference between the two comparisons (see also figures 12 and 13). Initializing with a natural image can reduce the magnitude of the difference.</p></caption>
<graphic xlink:href="541306v7_fig14.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig15" position="float" fig-type="figure">
<label>Figure 15.</label>
<caption><title>Schematic of psychophysics task.</title> <p>Top shows the structure for a single trial: a single stimulus is presented for 200 msec, bisected in the center by a gray bar, followed by a blank screen for 500 msec. The stimulus is re-displayed, with a random half of the stimulus changed to the comparison stimulus, for 200 msec. The participants then have as long as needed to say which half of the stimulus changed, followed by a 500 msec intertrial interval. Bottom table shows possible comparisons. In original vs. synthesized, one stimulus was the target image stimulus whose model representation the synthesized stimuli match (see figure 5), and the other was one such synthesized stimulus. In synthesized vs. synthesized, both were synthesized stimuli targeting the same target image stimulus, with the same model scaling, but different initialization. In experiments, dividing bar, blanks, and backgrounds were all midgray. For more details see text.</p></caption>
<graphic xlink:href="541306v7_fig15.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We controlled the generation of metamer stimuli in our experiments by selecting one of two initialization distributions: white noise and natural images. A natural intermediate choice would be pink noise, with frequency spectrum matched to that of natural images. Such initialization would reduce the high frequencies found in the super-threshold luminance model metamers (e.g., <xref rid="fig4" ref-type="fig">figure 4</xref>), while avoiding the detectable features found in those initialized with natural images (e.g., appendix <xref rid="fig1" ref-type="fig">figure 1</xref>), likely leading to higher critical scaling values. The logic of our experiments, however, relies on finding the <italic>smallest</italic> critical scaling value, rather than generating stimuli that look as similar to the target as possible. Thus, we expect such initializations would be uninformative. A more principled statistical sampling approach could result in metamers that better reflect natural image properties, or metamers that are more likely to be discriminable by human observers.</p>
</sec>
<sec id="s3e">
<title>Why does asymptotic performance, but not critical scaling, depends on image content?</title>
<p>Similar to <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>) and <xref ref-type="bibr" rid="c10">Brown et al. (2023</xref>), we find that metamer discrimination performance is somewhat dependent on image content. Both of those studies synthesize model metamers based on pooled texture statistics, and <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>) shows that texture-like target image stimuli are harder to distinguish from their synthesized stimuli than scene-like ones, while <xref ref-type="bibr" rid="c10">Brown et al. (2023</xref>) shows that original textures with higher global and local regularity (e.g., woven baskets) are easier to distinguish from their synthesized stimuli than those with low regularity (e.g., animal fur). This aligns with our result: the most distinguishable pairs include natural image stimuli features not well-captured by the synthesizing model, whereas the least distinguishable include those natural image stimuli whose features are all adequately captured.</p>
<p>However, we should note that we found this image-level variability largely in super-threshold performance, and this variability does not constitute a failure of these pooling models. As pointed out by <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>), asymptotic performance also varies with experimental manipulation, while critical scaling remains relatively unaffected. The metamer paradigm makes strong predictions about what happens when the representation of two stimuli are matched: they are indistinguishable, and so performance on a discrimination task will be at chance, as captured by the critical scaling value. That is, any differences are subthreshold. However, it makes <italic>no</italic> predictions about performance at super-threshold levels. An analogy with color vision seems apt: color matching experiments provide evidence for what spectral distributions of light are perceived as identical colors, but provide no information about whether humans consider blue more similar to green or to red; further measurements are necessary to understand color appearance.</p>
<p>By investigating how such differences are handled by later brain areas, this complementary approach could gain a better understanding of stimulus discriminability beyond ‚Äúidentical or not‚Äù. Such work could use the models presented here as a starting point and could draw on the substantial literature of observer models in vision science and image processing; see <xref ref-type="bibr" rid="c72">Ziemba and Simoncelli (2021)</xref>; <xref ref-type="bibr" rid="c38">Kurzawski et al. (2024</xref>) for examples applying this approach to pooling model metamer images. Combining and extending the synthesis-focused metamer approach with observer models‚Äô attention to super-threshold performance would help lead to a fuller understanding of human perceptual sensitivities and insensitivities.</p>
</sec>
<sec id="s3f">
<title>Conclusion</title>
<p>In summary, we‚Äôve used large field-of-view stimuli matched to the foveation properties of the human visual system in order to estimate the spatial precision of the perceptual representation of low-level image features. By improving our understanding of the information that is discarded, these measurements provide a starting point for building a foveated model of the visual system‚Äôs front end. We‚Äôve also provided a unified explanation of the variations in critical scaling that arise from changes in image content, comparison type, and synthesis initialization, providing guidance on how to use model and perceptual metamers for understanding visual representations.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<p>All experimental materials, data, and code for this project are available online under the MIT or similarly permissive licenses. Specifically, software is on GitHub, synthesized metamers can be browsed on this website, and all stimuli and data can be downloaded from the OSF. The GitHub site provides instructions for downloading and using data.</p>
<sec id="s4a">
<title>Synthesis</title>
<p>We synthesized model metamers matching 20 different natural images (the <bold>target images</bold>) from the authors‚Äô personal collections, as well as from the UPenn Natural Image Database (<xref ref-type="bibr" rid="c60">Tkaƒçik et al. (2011</xref>), extended dataset of urban images provided by David Brainard). The selected photos were high-resolution with 16-bit pixel intensities proportional to luminance, that had not undergone lossy compression (which could result in artifacts). They were converted to grayscale using scikitimage‚Äôs &lt;monospace&gt;color.rgb2gray &lt;/monospace&gt;function (<xref ref-type="bibr" rid="c66">van der Walt et al., 2014</xref>), cropped to 2048 by 2600 pixels (the Brainard photos were 2014 pixels tall, so a small amount of reflection padding was used to reach 2048 pixels), and had their pixel values rescaled to lie between 0.05 and 0.95. Synthesized stimuli were still allowed to have pixel values between 0 and 1; without rescaling the target images, synthesis resulted in strange artifacts with pixels near 0, as this was the minimum allowed value. The target images were chosen to span a variety of natural image content types, including buildings, animals, and natural textures (see <xref rid="fig5" ref-type="fig">figure 5</xref>).</p>
<p>We synthesized the model metamers using custom software written in PyTorch (<xref ref-type="bibr" rid="c45">Paszke et al., 2019</xref>), using the AMSGrad variant of the Adam optimization algorithm (<xref ref-type="bibr" rid="c34">Kingma and Ba, 2014</xref>; <xref ref-type="bibr" rid="c51">Reddi et al., 2018</xref>), with learning rate 0.01. Slightly different approaches were used for the luminance and energy model metamers. For the luminance model metamers, the objective function was to minimize the mean-squared error between the model representation of the target and synthesized stimuli, <italic><inline-formula><inline-graphic xlink:href="541306v7_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula></italic>, and synthesis was run for 5000 iterations. For the energy model metamers, the objective function also contained a quadratic range penalty term, which penalized any pixel values outside of [0, 1], <inline-formula><inline-graphic xlink:href="541306v7_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where
<disp-formula id="ueqn1">
<graphic xlink:href="541306v7_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Synthesis was run for 15000 iterations. Additionally, energy model metamer synthesis used stochastic weight averaging (<xref ref-type="bibr" rid="c32">Izmailov et al., 2018</xref>), which helped avoid local optima by averaging over pixel values as synthesis neared convergence, and used coarse-to-fine optimization (<xref ref-type="bibr" rid="c49">Portilla and Simoncelli, 2000</xref>). Additionally, each statistic (in both models) was z-scored using the average statistic value computed across the entire image on a selection of grayscale texture images. For both models, synthesis terminated early if the loss had not decreased by more than 1<italic>e</italic> ‚àí 9 over the past 50 iterations. While not all model metamers achieved the same loss values, with differences in synthesis loss across target images, there was no relationship between the remaining loss and behavioral performance.</p>
<p>For each model, its windows were represented as two tensors, one for angular slices and one for annuli, which, when multiplied together, would give the individual windows, with separate sets of windows for each scale in the energy model. This required a large amount of memory, and so for scaling values below 0.09, models were too large to perform synthesis on the available NVIDIA GPUs with 32GB of memory. Thus, all luminance model metamers were computed on the CPU, and synthesis of a single stimulus took from about an hour for scaling 1.5 to 2 days for scaling 0.058 to 14 days for scaling 0.01. For the energy model metamers, the lowest two scaling values were computed on the CPU, with synthesis taking about a week. For those energy model metamers which were able to be computed on the GPU, synthesis took from 5 hours for scaling 0.095 to 1.5 hours for scaling 0.27 and above. This synthesis procedure was completed in parallel using the high-performance computing cluster at the Flatiron Institute.</p>
<p>Synthesized stimuli for original vs. synthesized and synthesized vs. synthesized white noise comparisons (see Psychophysical experiment) were initialized with full-field patches of white noise (each pixel sampled from a uniform distribution between 0 and 1). For each model, scaling value, and target image, three different initialization seeds were used. A unique set of three seeds was used for each scaling value and target image, except for the following, which all used seeds {0, 1, 2}:</p>
<list list-type="bullet">
<list-item><p>Luminance model: azulejos, bike, graffti, llama, terraces, tiles; scaling 0.01, 0.013, 0.017, 0.021, 0.027, 0.035, 0.045, 0.058, 0.075 and 0.5.</p></list-item>
<list-item><p>Energy model: azulejos, bike, graffti, llama, terraces, tiles; scaling 0.095, 0.12, 0.14, 0.18, 0.22, 0.27, 0.33, 0.4, and 0.5.</p></list-item>
</list>
<p>For original vs. synthesized and synthesized vs. synthesized natural image comparison, synthesized stimuli for each model, scaling value, and target image were initialized with three random choices from among the rest of the target images.</p>
</sec>
<sec id="s4b">
<title>Pooling windows</title>
<p>Pooling model windows are laid out in a log-polar grid, with peaks spaced one standard deviation apart, such that adjacent window functions cross at a value of 0.352 (relative to max of 0.4). They have a single parameter, <bold>scaling</bold>, which specifies the ratio of the pooling window diameter at fullwidth half-max in the radial direction and the distance of its center from the fovea, both in degrees. For example, the pooling windows of a model with scaling factor 0.1 have a radial diameter of 1 degree at 10 degrees eccentricity, 2 at 20 degrees, etc.</p>
<p>Stimulus pixels within 0.5 degree from the fixation point were exactly matched in our synthesized stimuli, approximating the fovea, where no pooling occurs. Additionally, for small scaling values, windows for some distance beyond this region would be smaller than a pixel and so the only solution is to match the pixel values in that region directly. For example, with image resolution of 2048 by 2600 and display size of 53.6 by 42.2 degrees, models with scaling value of 0.063 have windows whose diameter at FWHM is smaller than a pixel out to 0.52 degrees, with this number increasing quadratically as scaling decreases, reaching 3.29 degrees for scaling 0.01 (see appendix 4 for more discussion).</p>
</sec>
<sec id="s4c">
<title>Observers</title>
<p>Eight participants (5 women and 3 men, aged 24 to 33), including an author (W.F.B.), participated in the study and were recruited from New York University. All subjects had normal or corrected- to-normal vision. Each subject completed nine one-hour sessions. One subject (sub-00, author W.F.B.) also performed seven additional sessions. All subjects provided informed consent before participating in the study. The experiment was conducted in accordance with the Declaration of Helsinki and was approved by the New York University ethics committee on activities involving human subjects.</p>
</sec>
<sec id="s4d">
<title>Psychophysical experiment</title>
<p>A psychophysical experiment was run in order to determine which of the synthesized model metamers were also perceptual metamers. We first describe the structure of a single trial, then how the trials were organized into blocks and sessions.</p>
<sec id="s4d1">
<title>Trial structure</title>
<p>See <xref rid="fig15" ref-type="fig">figure 15</xref> for schematic. Observers viewed a series of grayscale image stimuli on a monitor, at a size of 53.6 by 42.2 degrees. An initial stimulus, divided in half by a vertical midgray bar 2 degrees wide, was displayed for 200 msecs, before being replaced by a midgray screen for 500 msecs, followed by a second stimulus for another 200 msecs (also divided by a vertical midgray bar). Stimuli were presented for 200 msecs to minimize the possibility of eye movements. The dividing bar prevented participants‚Äô use of discontinuities between the two stimulus halves to perform the task. One side of the second stimulus (left half or right half) was identical to the first stimulus, and the other side changed. After the second stimulus was viewed, a midgray screen appeared with text prompting a response, and the observer‚Äôs task was to report which half had changed. The observer had as much time as necessary to respond. The two compared stimuli were either two synthesized stimuli (synthesized for identical models with the same scaling value and target image, but different initializations) or one synthesized stimulus and its target image stimulus. Either stimulus could be presented first.</p>
<p>The midgray blank screen presented between the two stimulus presentations reduces motion cues participants could use to discriminate the two stimuli. Our models aim to capture the steady state response to the stimuli, not the transient response. The mask forces the participants to use the image content to discriminate between the two stimuli, rather than relying on temporal edges (analogous to our use of the vertical bar to prevent the use of spatial edges). This introduces a shortterm memory component in the task (participants must remember the first stimulus in order to compare it to the second stimulus), as in previous metamer discrimination experiments (<xref ref-type="bibr" rid="c21">Freeman and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="c17">Deza et al., 2019</xref>; <xref ref-type="bibr" rid="c65">Wallis et al., 2019</xref>). We believe the precise duration of this mask is unimportant for our results: first, <xref ref-type="bibr" rid="c6">Bennett and Cortese (1996</xref>) found the duration of a blank screen did not affect thresholds in a spatial frequency discrimination task over a range from 200 to 10,000 msec, and second, mask duration is likely to have a similar effect on performance as stimulus presentation duration, which <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>) found affected asymptotic performance but not critical scaling.</p>
</sec>
<sec id="s4d2">
<title>Session and block organization</title>
<p>Across 9 sessions, each subject completed a total of 12,960 trials, factored into 3 model/comparison combinations by 8 scaling values by 15 target images by 36 repetitions. This large number of trials enabled us to check individual differences within both target images and observers for each model/comparison combination. The 3 model/comparison combinations were 1) luminance model, original vs. synth, white noise; 2) energy model, original vs. synth, white noise; and 3) energy model, synth vs. synth, white noise. The 8 scaling values were logarithmically spaced, with the range chosen separately for each model/comparison to span an appropriate range of values. For luminance model, original vs. synth, white noise, the scaling endpoints were 0.01 and 0.058; for energy model, original vs. synth, white noise, the endpoints were 0.063 and 0.27; and energy model, synth vs. synth, white noise, the endpoints were 0.27 and 1.5. There were a total of 20 target images, but each subject only saw 15. Every subject saw images 1 through 10. Half the subjects also saw images 11 through 15, and half saw images 16 through 20 (see <xref rid="fig5" ref-type="fig">figure 5</xref>). The 36 repetitions were averaged for analysis and included 12 trials for each of 3 synthesis seeds. For the white noiseinitialized comparisons, these seeds were independent samples of white noise used to initialize the synthesis procedure, resulting in 3 distinct model metamers.</p>
<p>Each of the above model/comparisons was tested across 3 sessions, each lasting approximately one hour. Each subject started with either the luminance or energy model, original vs. synth, white noise. The 3 sessions required for the model/comparison tested first were completed before moving onto 3 sessions testing the other model. The order of the two models was randomized across subjects. After completing these 6 sessions, the subject completed 3 sessions testing the energy model, synth vs. synth, white noise. This comparison was last as it was the most diffcult.</p>
<p>Each of the 9 sessions consisted of 1,440 trials, containing all 36 repetitions for all 8 scaling values for 5 of the 15 target images viewed by the subject (target images were randomly assigned to sessions, independently for each subject). The 1,440 trials per session were broken up into 5 blocks of 288 trials each. Each block took about 8 to 12 minutes, and consisted of 12 repetitions for all 8 scaling values for 3 of the 5 target images.</p>
<p>In addition, one subject (sub-00) completed 7 additional sessions (10,080 additional trials). This included 1 session for luminance model, synth vs. synth, white noise; 3 for energy model, original vs. synth, natural image; and 3 for energy model, synth vs. synth, natural image. As with the comparisons that all subjects completed, these sessions each included 1,440 trials, factored into 5 target images by 8 scaling values by 36 repetitions. Only 1 session was included for luminance model, original vs. synth, white noise because performance was at chance for all target images and all scaling values (see <xref rid="fig6" ref-type="fig">figures 6</xref> and <xref rid="fig7" ref-type="fig">7</xref>). No sessions were completed for the luminance model, natural image comparisons due to the time required for synthesis; see appendix <xref ref-type="sec" rid="s1">section 1</xref> for more information.</p>
<p>The four types of comparisons are explained in full below:</p>
<list list-type="order">
<list-item><p>Original vs. synthesized, white noise: the two stimuli being compared were always one synthesized stimulus and its target stimulus, and the synthesized stimulus was initialized with a sample of white noise.</p></list-item>
<list-item><p>Synthesized vs. synthesized, white noise: both stimuli were synthesized, with the same model, scaling value, and target stimulus, but different white noise seeds as synthesis initialization.</p></list-item>
<list-item><p>Original vs. synthesized, natural image: the two stimuli being compared were always one synthesized stimulus and its target stimulus, and the synthesized stimulus was initialized with a different natural image drawn randomly from our set.</p></list-item>
<list-item><p>Synthesized vs. synthesized, natural image: both stimuli were synthesized, with the same model, scaling value, and target stimulus, but initialized with different natural images from our set.</p></list-item>
</list>
<p>Subjects completed several training blocks. Before their first session, they completed an initial training block, comparing two natural image stimuli and two noise samples (one white, one pink). Before their first session of each comparison type including a natural image stimulus, they completed a secondary training block showing two natural image stimuli and two synthesized stimuli of the type included in the session, one with the largest scaling included in the task and one with the smallest. Before the session comparing two synthesized stimuli, they similarly completed a training block comparing four synthesized stimuli, two with a low scaling value and two with a high scaling value, for each of two target images. Each training block took one to two minutes and was repeated if performance on the high scaling synthesized stimuli was below 90% or subjects expressed uncertainty about their ability to perform the task (participants were expected to perform close to chance for the low scaling synthesized stimuli). Additionally, before each session which included a natural image stimulus (the original vs. synthesized comparisons), subjects were shown the five natural images that would be part of that session, as well as two example synthesized stimuli per target image, one with a low scaling value, one with a high scaling value. Before each session comparing two synthesized stimuli (the synthesized vs. synthesized comparison), subjects were shown four example synthesized stimuli per target image, two with the lowest scaling value and two with the highest scaling value for that comparison. A video of a single energy model training block, original vs. synthesized: white noise comparison, can be found on the OSF.</p>
</sec>
</sec>
<sec id="s4e">
<title>Apparatus</title>
<p>The stimuli were displayed on an Eizo CS2740 LED flat monitor running at 60 Hz with resolution 3840√ó2160. The monitor was gamma-corrected to yield a linear relationship between luminance and pixel value. The maximum, minimum, and mean luminances were 147.73, .3939, and 77.31 cd/m<sup>2</sup>, respectively.</p>
<p>The experiment was run with a viewing distance of 40 cm, giving 48.5 pixels per degree of visual angle. A chin and forehead rest was used to maintain head position, but the subjects‚Äô eyes were not tracked.</p>
<p>The experiment was run using custom code written in Python 3.7.0 using PsychoPy 3.1.5 (<xref ref-type="bibr" rid="c46">Peirce et al., 2019</xref>), run on an Ubuntu 20.04 LTS desktop. A button box was used to record the psychophysical response data. All stimuli were presented as 8-bit grayscale images.</p>
<p>Eye tracking was considered unnecessary for several reasons. First, each stimulus was only presented for 200 msecs, which is the typical latency of saccades (<xref ref-type="bibr" rid="c13">Carpenter, 1988</xref>). Thus, participants were unlikely to saccade during stimulus presentation. Second, our participants were trained observers and so have experience fixating for the duration of hour-long experimental blocks. Moreover, fixation failures would likely reveal themselves in variability in estimated psychometric curves. But the consistency of estimated parameters (indicated both by their relatively narrow credible intervals and similarity to results reported in <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>)), suggest that this does not greatly affect our results. Finally, the two-degree wide vertical bar at the center of the screen ensures that minor errors in fixation will have no effect.</p>
</sec>
<sec id="s4f">
<title>Data analysis</title>
<p>All trials were analyzed, a total of 4,320 trials per subject per model per comparison (across 15 target images and 8 scaling values) for all energy model comparisons and for luminance model original vs. synthesized white noise comparison. Luminance model synthesized vs. synthesized, white noise comparison had 1,440 trials (across 5 target images and 8 scaling values) for a single subject. Where behavioral data is plotted in this paper, the proportion correct is the average across all relevant trials.</p>
<p>We fit psychophysical curves describing proportion correct as a function of model scaling using the two-parameter function for discriminability <italic>d</italic><sup>‚Ä≤</sup> derived in <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>):
<disp-formula id="eqn1">
<graphic xlink:href="541306v7_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>s</italic><sub><italic>c</italic></sub> is the critical scaling value (performance is at chance for scaling values at or below <italic>s</italic><sub><italic>c</italic></sub>) and <italic>a</italic> is the max <sup>‚Ä≤</sup> value (called the ‚Äúproportionality factor‚Äù in <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>)).</p>
<p>Psychophysical curves were constructed by converting this <sup>‚Ä≤</sup> into the probability correct using the same function as in <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>):
<disp-formula id="eqn2">
<graphic xlink:href="541306v7_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where Œ¶ is the cumulative of the normal distribution. The probability correct is 50% when <italic>d</italic> <sup>‚Ä≤</sup> = 0 (and thus when scaling is at or below the critical scaling), reaches about 79% when <italic>d</italic><sup>‚Ä≤</sup> = 2 and 98% when <italic>d</italic><sup>‚Ä≤</sup> = 4. As the <italic>a</italic> parameter above gives the maximum <italic>d</italic> <sup>‚Ä≤</sup> value, it has a monotonic relationship with the asymptotic performance, which can be seen in <xref rid="fig16" ref-type="fig">figure 16</xref>.</p>
<fig id="fig16" position="float" fig-type="figure">
<label>Figure 16.</label>
<caption><title>Relationship between the max <italic>d</italic> <sup>‚Ä≤</sup> parameter, <italic>Œ±</italic> and asymptotic performance.</title> <p>As max <italic>d</italic> <sup>‚Ä≤</sup> increases beyond approximately 5 (where asymptotic performance is at ceiling), the slope of the psychophysical curve continues to increase (for example, compare the slope of the luminance and energy model original vs. synth white noise comparisons in figure 9A).</p></caption>
<graphic xlink:href="541306v7_fig16.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The posterior distribution over parameters <italic>s</italic><sub><italic>c</italic></sub> and <italic>a</italic> was estimated using a hierarchical, partialpooling model, with independent subject- and target image-level effects for both <italic>s</italic><sub><italic>c</italic></sub> and <italic>Œ±</italic>, with each model and comparison estimated separately, following the procedure used in <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>). Subject responses were modeled as samples from a Bernoulli distribution with probability (1 ‚àí <italic>œÄ</italic>)<italic>P</italic> (<italic>s</italic>) + .5<italic>œÄ</italic>, where <italic>œÄ</italic> is the lapse rate, estimated independently for each subject. Estimates were obtained using a Markov Chain Monte Carlo (MCMC) procedure written in Python 3.7.10 (<xref ref-type="bibr" rid="c61">Van Rossum and Drake, 2009</xref>) using the <monospace>numpyro</monospace>package, version 0.8.0 (<xref ref-type="bibr" rid="c48">Phan et al., 2019</xref>; <xref ref-type="bibr" rid="c7">Bingham et al., 2018</xref>). MCMC sampling was conducted using the No U-Turn Sampler algorithm (<xref ref-type="bibr" rid="c29">Hoffman and Gelman, 2014</xref>), with parameters selected to ensure convergence, which was assessed using the <italic><inline-formula><inline-graphic xlink:href="541306v7_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula></italic> statistics (<xref ref-type="bibr" rid="c9">Brooks and Gelman (1998</xref>), looking for <italic><inline-formula><inline-graphic xlink:href="541306v7_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula></italic>, <xref ref-type="bibr" rid="c63">Vehtari et al. (2021</xref>)) and by examining traceplots. Parameters were given weakly-informative priors and both <italic>s</italic><sub><italic>c</italic></sub> and <italic>a</italic> were estimated on natural logarithmic scales.</p>
<p>In sum, for model <italic>m</italic> E {<italic>E, L</italic>}, comparison <italic>t</italic>, subject, target image <italic>i</italic>, and scaling <italic>s</italic>:
<disp-formula id="eqn3">
<graphic xlink:href="541306v7_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn4">
<graphic xlink:href="541306v7_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="541306v7_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with the following priors:
<disp-formula id="ueqn2">
<graphic xlink:href="541306v7_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The priors for <italic>s</italic><sub><italic>c,mt</italic></sub> of the energy and luminance models correspond to critical scales of 0.25 and 0.018, respectively, the centers of the V1 physiological range provided in <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>) <xref rid="fig5" ref-type="fig">figure 5</xref>, and from the slope of a line fit to the dendritic field diameter vs. eccentricity of midget retinal ganglion cells in <xref ref-type="bibr" rid="c15">Dacey and Petersen (1992</xref>) <xref rid="fig2" ref-type="fig">figure 2B</xref>. These priors reflect our original prediction that the models‚Äô critical scaling values should be similar to those of the physiological scaling in the brain area sensitive to the same image features. While we are not committed to this link to physiology, it is reasonable to expect the energy model‚Äôs critical scaling to be similar to that of <xref ref-type="bibr" rid="c21">Freeman and Simoncelli (2011</xref>), who tested the same model, and to assume the critical scaling for the luminance model to be a good deal smaller. The priors also reflect our prediction that critical scaling should be independent of comparison type and consistent across target images and subjects, while not placing too much of a constraint on the parameters.</p>
<p>The posterior distribution represents the model‚Äôs beliefs about the parameters given the priors and data and is summarized throughout this paper as the posterior mean and 95% high density intervals. The latter represents the range of values containing 95% of the distribution with the highest probability, as opposed to the more common 95% confidence interval, which is symmetrically arranged around the mean. The two are identical for symmetric distributions, but can diverge markedly if the distribution is highly skewed (<xref ref-type="bibr" rid="c36">Kruschke, 2015</xref>)).</p>
</sec>
<sec id="s4g">
<title>Software</title>
<p>These experiments relied on a variety of custom scripts written in Python 3.7.10 (<xref ref-type="bibr" rid="c61">Van Rossum and Drake, 2009</xref>), all found in the GitHub repository associated with this paper. The following packages were used: <monospace>snakemake</monospace>(<xref ref-type="bibr" rid="c43">M√∂lder et al., 2021</xref>), <monospace>JAX</monospace>(<xref ref-type="bibr" rid="c8">Bradbury et al., 2018</xref>), <monospace>matplotlib</monospace>(<xref ref-type="bibr" rid="c31">Hunter, 2007</xref>), <monospace>psychopy</monospace>(<xref ref-type="bibr" rid="c46">Peirce et al., 2019</xref>), <monospace>scipy</monospace>(<xref ref-type="bibr" rid="c64">Virtanen et al., 2020</xref>), <monospace>scikit-image</monospace>(<xref ref-type="bibr" rid="c66">van der Walt et al., 2014</xref>), <monospace>pytorch</monospace>(<xref ref-type="bibr" rid="c45">Paszke et al., 2019</xref>), <monospace>arviz</monospace>(<xref ref-type="bibr" rid="c37">Kumar et al., 2019</xref>), <monospace>numpyro</monospace>(<xref ref-type="bibr" rid="c48">Phan et al., 2019</xref>; <xref ref-type="bibr" rid="c7">Bingham et al., 2018</xref>), <monospace>pandas</monospace>(<xref ref-type="bibr" rid="c50">Reback et al., 2021</xref>; <xref ref-type="bibr" rid="c42">McKinney, 2010</xref>), <monospace>seaborn</monospace>(<xref ref-type="bibr" rid="c69">Waskom, 2021</xref>), <monospace>jupyterlab</monospace>(<xref ref-type="bibr" rid="c35">Kluyver et al., 2016</xref>), and <monospace>xarray</monospace>(<xref ref-type="bibr" rid="c30">Hoyer and Hamman, 2017</xref>).</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>The authors would like to thank David Brainard for the use of his photographs, from the UPenn Natural Image Database (<xref ref-type="bibr" rid="c60">Tkaƒçik et al., 2011</xref>) and the extended dataset of urban images. They would also like to thank Tony Movshon, David Heeger, David Brainard, Corey Ziemba, and Colin Bredenberg for their feedback on the manuscript, Mike Landy for his assistance with the design of the psychophysical task and feedback on the manuscript, Heiko Sch√ºtt for his assistance with the Markov Chain Monte Carlo analysis, and the authors of <xref ref-type="bibr" rid="c65">Wallis et al. (2019</xref>) for sharing their code and data. Furthermore, they would like to thank Liz Lovero, Paul Murray, Dylan Simon, and Aaron Watters for their work in creating the metamer browser website.</p>
</ack>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Appendix</label>
<media xlink:href="supplements/541306_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Mullen</surname> <given-names>KT</given-names></string-name>, <string-name><surname>Hess</surname> <given-names>RF</given-names></string-name></person-group>. <article-title>Human Peripheral Spatial Resolution for Achromatic and Chromatic Stimuli: Limits Imposed By Optical and Retinal Factors</article-title>. <source>The Journal of Physiology</source>. <year>1991</year>; <volume>442</volume>(<issue>1</issue>):<fpage>47</fpage>‚Äì<lpage>64</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anstis</surname> <given-names>SM</given-names></string-name></person-group>. <article-title>A Chart Demonstrating Variations in Acuity With Retinal Position</article-title>. <source>Vision Research</source>. <year>1974</year> <month>jul</month>; <volume>14</volume>(<issue>7</issue>):<fpage>589</fpage>‚Äì<lpage>592</lpage>. <pub-id pub-id-type="doi">10.1016/0042-6989(74)90049-2</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/0042-6989(74)90049-2</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anstis</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Picturing Peripheral Acuity</article-title>. <source>Perception</source>. <year>1998</year> <month>jul</month>; <volume>27</volume>(<issue>7</issue>):<fpage>817</fpage>‚Äì<lpage>825</lpage>. <pub-id pub-id-type="doi">10.1068/p270817</pub-id>, doi: <pub-id pub-id-type="doi">10.1068/p270817</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Balas</surname> <given-names>B</given-names></string-name>, <string-name><surname>Nakano</surname> <given-names>L</given-names></string-name>, <string-name><surname>Rosenholtz</surname> <given-names>R.</given-names></string-name></person-group> <article-title>A Summary-Statistic Representation in Peripheral Vision Explains Visual Crowding</article-title>. <source>Journal of Vision</source>. <year>2009</year> <month>nov</month>; <volume>9</volume>(<issue>12</issue>):<fpage>13</fpage>‚Äì<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1167/9.12.13</pub-id>, doi: <pub-id pub-id-type="doi">10.1167/9.12.13</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Banks</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Geisler</surname> <given-names>WS</given-names></string-name>, <string-name><surname>Bennett</surname> <given-names>PJ</given-names></string-name></person-group>. <article-title>The Physical Limits of Grating Visibility</article-title>. <source>Vision Research</source>. <year>1987</year> <month>jan</month>; <volume>27</volume>(<issue>11</issue>):<fpage>1915</fpage>‚Äì<lpage>1924</lpage>. <pub-id pub-id-type="doi">10.1016/0042-6989(87)90057-5</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bennett</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Cortese</surname> <given-names>F.</given-names></string-name></person-group> <article-title>Masking of Spatial Frequency in Visual Memory Depends on Distal, Not Retinal, Frequency</article-title>. <source>Vision Research</source>. <year>1996</year> <month>jan</month>; <volume>36</volume>(<issue>2</issue>):<fpage>233</fpage>‚Äì<lpage>238</lpage>. <pub-id pub-id-type="doi">10.1016/0042-6989(95)00085-e</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/0042-6989(95)00085-e</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Bingham</surname> <given-names>E</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Jankowiak</surname> <given-names>M</given-names></string-name>, <string-name><surname>Obermeyer</surname> <given-names>F</given-names></string-name>, <string-name><surname>Pradhan</surname> <given-names>N</given-names></string-name>, <string-name><surname>Karaletsos</surname> <given-names>T</given-names></string-name>, <string-name><surname>Singh</surname> <given-names>R</given-names></string-name>, <string-name><surname>Szerlip</surname> <given-names>P</given-names></string-name>, <string-name><surname>Horsfall</surname> <given-names>P</given-names></string-name>, <string-name><surname>Goodman</surname> <given-names>ND</given-names></string-name></person-group>. <article-title>Pyro: Deep Universal Probabilistic Programming</article-title>. <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.1810.09538</pub-id>. <year>2018</year>;.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Bradbury</surname> <given-names>J</given-names></string-name>, <string-name><surname>Frostig</surname> <given-names>R</given-names></string-name>, <string-name><surname>Hawkins</surname> <given-names>P</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Leary</surname> <given-names>C</given-names></string-name>, <string-name><surname>Maclaurin</surname> <given-names>D</given-names></string-name>, <string-name><surname>Necula</surname> <given-names>G</given-names></string-name>, <string-name><surname>Paszke</surname> <given-names>A</given-names></string-name>, <string-name><surname>VanderPlas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wanderman-Milne</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Q</given-names></string-name></person-group>, <source>JAX: composable transformations of Python+NumPy programs</source>; <year>2018</year>. <ext-link ext-link-type="uri" xlink:href="http://github.com/google/jax">http://github.com/google/jax</ext-link>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brooks</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Gelman</surname> <given-names>A.</given-names></string-name></person-group> <article-title>General Methods for Monitoring Convergence of Iterative Simulations</article-title>. <source>Journal of Computational and Graphical Statistics</source>. <year>1998</year> <conf-date>ec</conf-date>; <volume>7</volume>(<issue>4</issue>):<fpage>434</fpage>‚Äì<lpage>455</lpage>. <pub-id pub-id-type="doi">10.1080/10618600.1998.10474787</pub-id>, doi: <pub-id pub-id-type="doi">10.1080/10618600.1998.10474787</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brown</surname> <given-names>R</given-names></string-name>, <string-name><surname>Dutell</surname> <given-names>V</given-names></string-name>, <string-name><surname>Walter</surname> <given-names>B</given-names></string-name>, <string-name><surname>Rosenholtz</surname> <given-names>R</given-names></string-name>, <string-name><surname>Shirley</surname> <given-names>P</given-names></string-name>, <string-name><surname>McGuire</surname> <given-names>M</given-names></string-name>, <string-name><surname>Luebke</surname> <given-names>D.</given-names></string-name></person-group> <article-title>Effcient Dataflow Modeling of Peripheral Encoding, in the Human Visual System</article-title>. <source>ACM Transactions on Applied Perception</source>. <year>2023</year> <month>Jan</month>; <volume>20</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1145/3564605</pub-id>, doi: <pub-id pub-id-type="doi">10.1145/3564605</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bruna</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mallat</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Invariant Scattering Convolution Networks</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2013</year> <month>aug</month>; <volume>35</volume>(<issue>8</issue>):<fpage>1872</fpage>‚Äì<lpage>1886</lpage>. doi: <pub-id pub-id-type="doi">10.1109/tpami.2012.230</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Burt</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Adelson</surname> <given-names>EH</given-names></string-name></person-group>. <chapter-title>The Laplacian pyramid as a compact image code</chapter-title>. <person-group person-group-type="editor"><string-name><surname>Kaufmann</surname> <given-names>M</given-names></string-name></person-group> <source>Readings in Computer Vision</source> <publisher-name>Elsevier</publisher-name>; <year>1987</year>.p. <fpage>671</fpage>‚Äì<lpage>679</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Carpenter</surname> <given-names>RH</given-names></string-name></person-group>. <source>Movements of the Eyes, 2nd Rev</source>. <publisher-name>Pion Limited</publisher-name>; <year>1988</year>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Kappauf</surname> <given-names>WE</given-names></string-name></person-group>. <article-title>Color Mixture and Fundamental Metamers: Theory, Algebra, Geometry, Application</article-title>. <source>The American Journal of Psychology</source>. <year>1985</year>; <volume>98</volume>(<issue>2</issue>):<fpage>171</fpage>. <pub-id pub-id-type="doi">10.2307/1422442</pub-id>, doi: <pub-id pub-id-type="doi">10.2307/1422442</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dacey</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>MR</given-names></string-name></person-group>. <article-title>Dendritic Field Size and Morphology of Midget and Parasol Ganglion Cells of the Human Retina</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1992</year> <month>oct</month>; <volume>89</volume>(<issue>20</issue>):<fpage>9666</fpage>‚Äì<lpage>9670</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.89.20.9666</pub-id>, doi: <pub-id pub-id-type="doi">10.1073/pnas.89.20.9666</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Daniel</surname> <given-names>PM</given-names></string-name>, <string-name><surname>Whitteridge</surname> <given-names>D.</given-names></string-name></person-group> <article-title>The Representation of the Visual Field on the Cerebral Cortex in Monkeys</article-title>. <source>The Journal of Physiology</source>. <year>1961</year> <conf-date>ec</conf-date>; <volume>159</volume>(<issue>2</issue>):<fpage>203</fpage>‚Äì<lpage>221</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.1961.sp006803</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Deza</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jonnalagadda</surname> <given-names>A</given-names></string-name>, <string-name><surname>Eckstein</surname> <given-names>MP</given-names></string-name></person-group>. <article-title>Towards Metamerism via Foveated Style Transfer</article-title>. <source>In: International Conference on Learning Representations</source>; <year>2019</year>. <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=BJzbG20cFQ">https://openreview.net/forum?id=BJzbG20cFQ</ext-link>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Douglas</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>KAC</given-names></string-name>, <string-name><surname>Whitteridge</surname> <given-names>D.</given-names></string-name></person-group> <article-title>A Canonical Microcircuit for Neocortex</article-title>. <source>Neural Computation</source>. <year>1989</year> <conf-date>ec</conf-date>; <volume>1</volume>(<issue>4</issue>):<fpage>480</fpage>‚Äì<lpage>488</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1989.1.4.480</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duncan</surname> <given-names>RO</given-names></string-name>, <string-name><surname>Boynton</surname> <given-names>GM</given-names></string-name></person-group>. <article-title>Cortical Magnification Within Human Primary Visual Cortex Correlates With Acuity Thresholds</article-title>. <source>Neuron</source>. <year>2003</year> <month>may</month>; <volume>38</volume>(<issue>4</issue>):<fpage>659</fpage>‚Äì<lpage>671</lpage>. <pub-id pub-id-type="doi">10.1016/s0896-6273(03)00265-4</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/s0896-6273(03)00265-4</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feather</surname> <given-names>J</given-names></string-name>, <string-name><surname>Leclerc</surname> <given-names>G</given-names></string-name>, <string-name><surname>MƒÖdry</surname> <given-names>A</given-names></string-name>, <string-name><surname>McDermott</surname> <given-names>JH</given-names></string-name></person-group>. <article-title>Model Metamers Reveal Divergent Invariances Between Biological and Artificial Neural Networks</article-title>. <source>Nature Neuroscience</source>. <year>2023</year> <month>oct</month>; <volume>26</volume>(<issue>11</issue>):<fpage>2017</fpage>‚Äì<lpage>2034</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-023-01442-0</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41593-023-01442-0</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Freeman</surname> <given-names>J</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name></person-group>. <article-title>Metamers of the ventral stream</article-title>. <source>Nature Neuroscience</source>. <year>2011</year> <month>aug</month>; <volume>14</volume>(<issue>9</issue>):<fpage>1195</fpage>‚Äì<lpage>1201</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn.2889</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frisen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Glansholm</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Optical and Neural Resolution in Peripheral Vision</article-title>. <source>Investigative Ophthalmology &amp; Visual Science</source>. <year>1975</year>; <volume>14</volume>(<issue>7</issue>):<fpage>528</fpage>‚Äì<lpage>536</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fukushima</surname> <given-names>K.</given-names></string-name></person-group> <article-title>Neocognitron: a Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected By Shift in Position</article-title>. <source>Biological Cybernetics</source>. <year>1980</year> <month>apr</month>; <volume>36</volume>(<issue>4</issue>):<fpage>193</fpage>‚Äì<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1007/bf00344251</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gattass</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>CG</given-names></string-name>, <string-name><surname>Sandell</surname> <given-names>JH</given-names></string-name></person-group>. <article-title>Visual Topography of V2 in the Macaque</article-title>. <source>The Journal of Comparative Neurology</source>. <year>1981</year>; <volume>201</volume>(<issue>4</issue>):<fpage>519</fpage>‚Äì<lpage>539</lpage>. <pub-id pub-id-type="doi">10.1002/cne.902010405</pub-id>, doi: <pub-id pub-id-type="doi">10.1002/cne.902010405</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gattass</surname> <given-names>R</given-names></string-name>, <string-name><surname>Sousa</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>C.</given-names></string-name></person-group> <article-title>Visuotopic Organization and Extent of V3 and V4 of the Macaque</article-title>. <source>Journal of Neuroscience</source>. <year>1988</year>; <volume>8</volume>(<issue>6</issue>):<fpage>1831</fpage>‚Äì<lpage>1845</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/content/8/6/1831">http://www.jneurosci.org/content/8/6/1831</ext-link>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenwood</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Bex</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Dakin</surname> <given-names>SC</given-names></string-name></person-group>. <article-title>Positional Averaging Explains Crowding With Letter-Like Stimuli</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2009</year> <month>aug</month>; <volume>106</volume>(<issue>31</issue>):<fpage>13130</fpage>‚Äì<lpage>13135</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0901352106</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Movshon</surname> <given-names>JA</given-names></string-name></person-group>. <article-title>Computational Models of Cortical Visual Processing</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1996</year>; <volume>93</volume>(<issue>2</issue>):<fpage>623</fpage>‚Äì<lpage>627</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Helmholtz</surname> <given-names>H.</given-names></string-name></person-group> <article-title>LXXXI. On the Theory of Compound Colours</article-title>. <source>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</source>. <year>1852</year>; <volume>4</volume>(<issue>28</issue>):<fpage>519</fpage>‚Äì<lpage>534</lpage>. <pub-id pub-id-type="doi">10.1080/14786445208647175</pub-id>, doi: <pub-id pub-id-type="doi">10.1080/14786445208647175</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoffman</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Gelman</surname> <given-names>A.</given-names></string-name></person-group> <article-title>The No-U-turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo</article-title>. <source>Journal of Machine Learning Research</source>. <year>2014</year>; <volume>15</volume>(<issue>1</issue>):<fpage>1593</fpage>‚Äì<lpage>1623</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoyer</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hamman</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Xarray: N-D Labeled Arrays and Datasets in Python</article-title>. <source>Journal of Open Research Software</source>. <year>2017</year>; <volume>5</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.5334/jors.148</pub-id>, doi: <pub-id pub-id-type="doi">10.5334/jors.148</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hunter</surname> <given-names>JD</given-names></string-name></person-group>. <article-title>Matplotlib: a 2d Graphics Environment</article-title>. <source>Computing in Science &amp; Engineering</source>. <year>2007</year>; <volume>9</volume>(<issue>3</issue>):<fpage>90</fpage>‚Äì<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>, doi: <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Izmailov</surname> <given-names>P</given-names></string-name>, <string-name><surname>Podoprikhin</surname> <given-names>D</given-names></string-name>, <string-name><surname>Garipov</surname> <given-names>T</given-names></string-name>, <string-name><surname>Vetrov</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>AG</given-names></string-name></person-group>. <article-title>Averaging Weights Leads To Wider Optima and Better Generalization</article-title>. <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.1803.05407</pub-id> <year>2018</year>;.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keshvari</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rosenholtz</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Pooling of Continuous Features Provides a Unifying Account of Crowding</article-title>. <source>Journal of Vision</source>. <year>2016</year> <month>feb</month>; <volume>16</volume>(<issue>3</issue>):<fpage>39</fpage>. <pub-id pub-id-type="doi">10.1167/16.3.39</pub-id>, doi: <pub-id pub-id-type="doi">10.1167/16.3.39</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Kingma</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Ba</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Adam: A Method for Stochastic Optimization</article-title>. <source>ArXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id> <year>2014</year>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kluyver</surname> <given-names>T</given-names></string-name>, <string-name><surname>Ragan-Kelley</surname> <given-names>B</given-names></string-name>, <string-name><surname>P√©rez</surname> <given-names>F</given-names></string-name>, <string-name><surname>Granger</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bussonnier</surname> <given-names>M</given-names></string-name>, <string-name><surname>Frederic</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kelley</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hamrick</surname> <given-names>J</given-names></string-name>, <string-name><surname>Grout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Corlay</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ivanov</surname> <given-names>P</given-names></string-name>, <string-name><surname>Avila</surname> <given-names>D</given-names></string-name>, <string-name><surname>Abdalla</surname> <given-names>S</given-names></string-name>, <string-name><surname>Willing</surname> <given-names>C</given-names></string-name>, <string-name><given-names>development</given-names> <surname>team</surname></string-name></person-group> J. <chapter-title>Jupyter Notebooks - a publishing format for reproducible computational workflows</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Loizides</surname> <given-names>F</given-names></string-name>, <string-name><surname>Scmidt</surname> <given-names>B</given-names></string-name></person-group>, editors. <source>Positioning and Power in Academic Publishing: Players, Agents and Agendas</source> <publisher-loc>Netherlands</publisher-loc>: <publisher-name>IOS Press</publisher-name>; <year>2016</year>. p. <fpage>87</fpage>‚Äì<lpage>90</lpage>. <ext-link ext-link-type="uri" xlink:href="https://eprints.soton.ac.uk/403913/">https://eprints.soton.ac.uk/403913/</ext-link>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kruschke</surname> <given-names>JK</given-names></string-name></person-group>. <source>Doing Bayesian Data Analysis</source>. <edition>Second</edition> ed. <publisher-name>Elsevier</publisher-name>; <year>2015</year>. <pub-id pub-id-type="doi">10.1016/c2012-0-00477-2</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/c2012-0-00477-2</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kumar</surname> <given-names>R</given-names></string-name>, <string-name><surname>Carroll</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hartikainen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>O.</given-names></string-name></person-group> <article-title>Arviz a Unified Library for Exploratory Analysis of Bayesian Models in Python</article-title>. <source>Journal of Open Source Software</source>. <year>2019</year>; <volume>4</volume>(<issue>33</issue>):<fpage>1143</fpage>. <pub-id pub-id-type="doi">10.21105/joss.01143</pub-id>, doi: <pub-id pub-id-type="doi">10.21105/joss.01143</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="report"><person-group person-group-type="author"><string-name><surname>Kurzawski</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Broderick</surname> <given-names>WF</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Winawer</surname> <given-names>J</given-names></string-name></person-group>, <source>A Foveated Model Of Visual Discrimination Based On Windowed Texture Statistics</source>; <publisher-name>Computational and Mathematical Models in Vision</publisher-name> <year>2024</year>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Boser</surname> <given-names>B</given-names></string-name>, <string-name><surname>Denker</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Henderson</surname> <given-names>D</given-names></string-name>, <string-name><surname>Howard</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Hubbard</surname> <given-names>W</given-names></string-name>, <string-name><surname>Jackel</surname> <given-names>LD</given-names></string-name></person-group>. <article-title>Backpropagation Applied To Handwritten Zip Code Recognition</article-title>. <source>Neural Computation</source>. <year>1989</year> <conf-date>ec</conf-date>; <volume>1</volume>(<issue>4</issue>):<fpage>541</fpage>‚Äì<lpage>551</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id>, doi: <pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lettvin</surname> <given-names>JY</given-names></string-name></person-group>. <article-title>On Seeing Sidelong</article-title>. <source>The Sciences</source>. <year>1976</year> <month>jul</month>; <volume>16</volume>(<issue>4</issue>):<fpage>10</fpage>‚Äì<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1002/j.2326-1951.1976.tb01231.x</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maunsell</surname> <given-names>JHR</given-names></string-name>, <string-name><surname>Newsome</surname> <given-names>WT</given-names></string-name></person-group>. <article-title>Visual Processing in Monkey Extrastriate Cortex</article-title>. <source>Annual Review of Neuroscience</source>. <year>1987</year> <month>mar</month>; <volume>10</volume>(<issue>1</issue>):<fpage>363</fpage>‚Äì<lpage>401</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.ne.10.030187.002051</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>McKinney</surname> <given-names>W.</given-names></string-name></person-group> <article-title>Data Structures for Statistical Computing in Python</article-title>. <conf-name>Proceedings of the 9th Python in Science Conference</conf-name>; <year>2010</year>. p. <fpage>56</fpage> ‚Äì <lpage>61</lpage>. <pub-id pub-id-type="doi">10.25080/Majora-92bf1922-00a</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>M√∂lder</surname> <given-names>F</given-names></string-name>, <string-name><surname>Jablonski</surname> <given-names>KP</given-names></string-name>, <string-name><surname>Letcher</surname> <given-names>B</given-names></string-name>, <string-name><surname>Hall</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Tomkins-Tinch</surname> <given-names>CH</given-names></string-name>, <string-name><surname>Sochat</surname> <given-names>V</given-names></string-name>, <string-name><surname>Forster</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>S</given-names></string-name>, <string-name><surname>Twardziok</surname> <given-names>SO</given-names></string-name>, <string-name><surname>Kanitz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wilm</surname> <given-names>A</given-names></string-name>, <string-name><surname>Holtgrewe</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rahmann</surname> <given-names>S</given-names></string-name>, <string-name><surname>Nahnsen</surname> <given-names>S</given-names></string-name>, <string-name><surname>K√∂ster</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Sustainable Data Analysis With Snakemake</article-title>. <source>F1000Research</source>. <year>2021</year> <month>apr</month>; <volume>10</volume>:<fpage>33</fpage>. <pub-id pub-id-type="doi">10.12688/f1000research.29032.2</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parkes</surname> <given-names>L</given-names></string-name>, <string-name><surname>Lund</surname> <given-names>J</given-names></string-name>, <string-name><surname>Angelucci</surname> <given-names>A</given-names></string-name>, <string-name><surname>Solomon</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Morgan</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Compulsory Averaging of Crowded Orientation Signals in Human Vision</article-title>. <source>Nature Neuroscience</source>. <year>2001</year> <month>jul</month>; <volume>4</volume>(<issue>7</issue>):<fpage>739</fpage>‚Äì<lpage>744</lpage>. <pub-id pub-id-type="doi">10.1038/89532</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Paszke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>S</given-names></string-name>, <string-name><surname>Massa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lerer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bradbury</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chanan</surname> <given-names>G</given-names></string-name>, <string-name><surname>Killeen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Gimelshein</surname> <given-names>N</given-names></string-name>, <string-name><surname>Antiga</surname> <given-names>L</given-names></string-name>, <string-name><surname>Desmaison</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kopf</surname> <given-names>A</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>E</given-names></string-name>, <string-name><surname>DeVito</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Raison</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tejani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chilamkurthy</surname> <given-names>S</given-names></string-name>, <string-name><surname>Steiner</surname> <given-names>B</given-names></string-name>, <string-name><surname>Fang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bai</surname> <given-names>J</given-names></string-name>, <etal>et al.</etal></person-group> <chapter-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Wallach</surname> <given-names>H</given-names></string-name>, <string-name><surname>Larochelle</surname> <given-names>H</given-names></string-name>, <string-name><surname>Beygelzimer</surname> <given-names>A</given-names></string-name>, <string-name><surname>d‚ÄôAlch√©-Buc</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fox</surname> <given-names>E</given-names></string-name>, <string-name><surname>Garnett</surname> <given-names>R</given-names></string-name></person-group>, editors. <source>Advances in Neural Information Processing Systems</source> <volume>32</volume> <publisher-name>Curran Associates, Inc</publisher-name>.; <year>2019</year>.p. <fpage>8024</fpage>‚Äì<lpage>8035</lpage>. <ext-link ext-link-type="uri" xlink:href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</ext-link>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peirce</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gray</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Simpson</surname> <given-names>S</given-names></string-name>, <string-name><surname>MacAskill</surname> <given-names>M</given-names></string-name>, <string-name><surname>H√∂chenberger</surname> <given-names>R</given-names></string-name>, <string-name><surname>Sogo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kastman</surname> <given-names>E</given-names></string-name>, <string-name><surname>Lindel√∏v</surname> <given-names>JK.</given-names></string-name></person-group> <article-title>PsychoPy2: Experiments in Behavior Made Easy</article-title>. <source>Behavior Research Methods</source>. <year>2019</year> <month>feb</month>; <volume>51</volume>(<issue>1</issue>):<fpage>195</fpage>‚Äì<lpage>203</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id>, doi: <pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name>, <string-name><surname>Palomares</surname> <given-names>M</given-names></string-name>, <string-name><surname>Majaj</surname> <given-names>NJ</given-names></string-name></person-group>. <article-title>Crowding Is Unlike Ordinary Masking: Distinguishing Feature Integration From Detection</article-title>. <source>Journal of Vision</source>. <year>2004</year> <conf-date>ec</conf-date>; <volume>4</volume>(<issue>12</issue>):<fpage>12</fpage>. <pub-id pub-id-type="doi">10.1167/4.12.12</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Phan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Pradhan</surname> <given-names>N</given-names></string-name>, <string-name><surname>Jankowiak</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Composable Effects for Flexible and Accelerated Probabilistic Programming in Numpyro</article-title>. <source>arXiv preprint</source> <pub-id pub-id-type="doi">10.48550/arXiv.1912.11554</pub-id>. <year>2019</year>;.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Portilla</surname> <given-names>J</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name></person-group>. <article-title>A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coeffcients</article-title>. <source>International journal of computer vision</source>. <year>2000</year>; <volume>40</volume>(<issue>1</issue>):<fpage>49</fpage>‚Äì<lpage>70</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Reback</surname> <given-names>J</given-names></string-name>, <collab>jbrockmendel</collab>, <string-name><surname>McKinney</surname> <given-names>W</given-names></string-name>, <string-name><surname>den Bossche</surname> <given-names>JV</given-names></string-name>, <string-name><surname>Augspurger</surname> <given-names>T</given-names></string-name>, <string-name><surname>Cloud</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hawkins</surname> <given-names>S</given-names></string-name>, <collab>gfyoung</collab>, <string-name><surname>Roeschke</surname> <given-names>M</given-names></string-name>, <collab>Sinhrks</collab>, <etal>et al</etal></person-group>, <source>pandas-dev/pandas: Pandas 1.2.3</source>. <publisher-name>Zenodo</publisher-name>; <year>2021</year>. doi: <pub-id pub-id-type="doi">10.5281/zenodo.4572994</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Reddi</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Kale</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kumar</surname> <given-names>S.</given-names></string-name></person-group> <article-title>On the Convergence of Adam and Beyond</article-title>. <source>In: International Conference on Learning Representations</source>; <year>2018</year>. <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=ryQu7f-RZ">https://openreview.net/forum?id=ryQu7f-RZ</ext-link>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riesenhuber</surname> <given-names>M</given-names></string-name>, <string-name><surname>Poggio</surname> <given-names>T.</given-names></string-name></person-group> <article-title>Hierarchical Models of Object Recognition in Cortex</article-title>. <source>Nature Neuroscience</source>. <year>1999</year> <month>nov</month>; <volume>2</volume>(<issue>11</issue>):<fpage>1019</fpage>‚Äì<lpage>1025</lpage>. <pub-id pub-id-type="doi">10.1038/14819</pub-id>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Robson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Graham</surname> <given-names>N.</given-names></string-name></person-group> <article-title>Probability Summation and Regional Variation in Contrast Sensitivity Across the Visual Field</article-title>. <source>Vision research</source>. <year>1981</year>; <volume>21</volume>(<issue>3</issue>):<fpage>409</fpage>‚Äì<lpage>418</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rovamo</surname> <given-names>J</given-names></string-name>, <string-name><surname>Virsu</surname> <given-names>V</given-names></string-name>, <string-name><surname>N√§s√§nen</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Cortical Magnification Factor Predicts the Photopic Contrast Sensitivity of Peripheral Vision</article-title>. <source>Nature</source>. <year>1978</year> <month>jan</month>; <volume>271</volume>(<issue>5640</issue>):<fpage>54</fpage>‚Äì<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1038/271054a0</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/271054a0</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schnapf</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Kraft</surname> <given-names>TW</given-names></string-name>, <string-name><surname>Baylor</surname> <given-names>DA</given-names></string-name></person-group>. <article-title>Spectral Sensitivity of Human Cone Photoreceptors</article-title>. <source>Nature</source>. <year>1987</year> <month>jan</month>; <volume>325</volume>(<issue>6103</issue>):<fpage>439</fpage>‚Äì<lpage>441</lpage>. <pub-id pub-id-type="doi">10.1038/325439a0</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwartz</surname> <given-names>EL</given-names></string-name></person-group>. <article-title>Spatial Mapping in the Primate Sensory Projection: Analytic Structure and Relevance To Perception</article-title>. <source>Biological Cybernetics</source>. <year>1977</year> <conf-date>ec</conf-date>; <volume>25</volume>(<issue>4</issue>):<fpage>181</fpage>‚Äì<lpage>194</lpage>. <pub-id pub-id-type="doi">10.1007/bf01885636</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Freeman</surname> <given-names>WT</given-names></string-name></person-group>. <article-title>The Steerable Pyramid: A flexible architecture for multi-scale derivative computation</article-title>. <conf-name>Proc 2nd IEEE Int‚Äôl Conf on Image Proc (ICIP), vol. III</conf-name>; <year>1995</year>. p. <fpage>444</fpage>‚Äì<lpage>447</lpage>. doi: <pub-id pub-id-type="doi">10.1109/ICIP.1995.537667</pub-id>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname> <given-names>S</given-names></string-name>, <string-name><surname>Levi</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name></person-group>. <article-title>A Double Dissociation of the Acuity and Crowding Limits To Letter Identification, and the Promise of Improved Visual Screening</article-title>. <source>Journal of Vision</source>. <year>2014</year> <month>may</month>; <volume>14</volume>(<issue>5</issue>):<fpage>3</fpage>‚Äì<lpage>3</lpage>. <pub-id pub-id-type="doi">10.1167/14.5.3</pub-id>, doi: <pub-id pub-id-type="doi">10.1167/14.5.3</pub-id>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thibos</surname> <given-names>LN</given-names></string-name></person-group>. <article-title>Retinal Image Formation and Sampling in a Three-Dimensional World</article-title>. <source>Annual Review of Vision Science</source>. <year>2020</year> <month>sep</month>; <volume>6</volume>(<issue>1</issue>):<fpage>469</fpage>‚Äì<lpage>489</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-121219-081840</pub-id>, doi: <pub-id pub-id-type="doi">10.1146/annurev-vision-121219-081840</pub-id>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tkaƒçik</surname> <given-names>G</given-names></string-name>, <string-name><surname>Garrigan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ratliff</surname> <given-names>C</given-names></string-name>, <string-name><surname>Milƒçinski</surname> <given-names>G</given-names></string-name>, <string-name><surname>Klein</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Seyfarth</surname> <given-names>LH</given-names></string-name>, <string-name><surname>Sterling</surname> <given-names>P</given-names></string-name>, <string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Balasubramanian</surname> <given-names>V.</given-names></string-name></person-group> <article-title>Natural Images From the Birthplace of the Human Eye</article-title>. <source>PLoS ONE</source>. <year>2011</year> <month>jun</month>; <volume>6</volume>(<issue>6</issue>):<fpage>e20409</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0020409</pub-id>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Van Rossum</surname> <given-names>G</given-names></string-name>, <string-name><surname>Drake</surname> <given-names>FL</given-names></string-name></person-group>. <source>Python 3 Reference Manual</source>. <publisher-loc>Scotts Valley, CA</publisher-loc>: <publisher-name>CreateSpace</publisher-name>; <year>2009</year>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vehtari</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gelman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gabry</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC</article-title>. <source>Statistics and Computing</source>. <year>2016</year> <month>aug</month>; <volume>27</volume>(<issue>5</issue>):<fpage>1413</fpage>‚Äì<lpage>1432</lpage>. <pub-id pub-id-type="doi">10.1007/s11222-016-9696-4</pub-id>, doi: <pub-id pub-id-type="doi">10.1007/s11222-016-9696-4</pub-id>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vehtari</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gelman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Simpson</surname> <given-names>D</given-names></string-name>, <string-name><surname>Carpenter</surname> <given-names>B</given-names></string-name>, <string-name><surname>B√ºrkner</surname> <given-names>PC</given-names></string-name></person-group>. <article-title>Rank-Normalization, Folding, and Localization: an Improved R‚Äù for Assessing Convergence of Mcmc (with Discussion)</article-title>. <source>Bayesian Analysis</source>. <year>2021</year>; <volume>16</volume>(<issue>2</issue>):<fpage>667</fpage> ‚Äì <lpage>718</lpage>. <pub-id pub-id-type="doi">10.1214/20-BA1221</pub-id>, doi: <pub-id pub-id-type="doi">10.1214/20-BA1221</pub-id>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Virtanen</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gommers</surname> <given-names>R</given-names></string-name>, <string-name><surname>Oliphant</surname> <given-names>TE</given-names></string-name>, <string-name><surname>Haberland</surname> <given-names>M</given-names></string-name>, <string-name><surname>Reddy</surname> <given-names>T</given-names></string-name>, <string-name><surname>Cournapeau</surname> <given-names>D</given-names></string-name>, <string-name><surname>Burovski</surname> <given-names>E</given-names></string-name>, <string-name><surname>Peterson</surname> <given-names>P</given-names></string-name>, <string-name><surname>Weckesser</surname> <given-names>W</given-names></string-name>, <string-name><surname>Bright</surname> <given-names>J</given-names></string-name>, <string-name><surname>van der Walt</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Brett</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jarrod Millman</surname> <given-names>K</given-names></string-name>, <string-name><surname>Mayorov</surname> <given-names>N</given-names></string-name>, <string-name><surname>Nelson</surname> <given-names>ARJ</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kern</surname> <given-names>R</given-names></string-name>, <string-name><surname>Larson</surname> <given-names>E</given-names></string-name>, <string-name><surname>Carey</surname> <given-names>C</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</article-title>. <source>Nature Methods</source>. <year>2020</year>; <volume>17</volume>:<fpage>261</fpage>‚Äì<lpage>272</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wallis</surname> <given-names>TS</given-names></string-name>, <string-name><surname>Funke</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Ecker</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Gatys</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Wichmann</surname> <given-names>FA</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Image Content Is More Important Than Bouma‚Äôs Law for Scene Metamers</article-title>. <source>eLife</source>. <year>2019</year> <month>apr</month>; <volume>8</volume>. <pub-id pub-id-type="doi">10.7554/elife.42512</pub-id>, doi: <pub-id pub-id-type="doi">10.7554/elife.42512</pub-id>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van der Walt</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sch√∂nberger</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Nunez-Iglesias</surname> <given-names>J</given-names></string-name>, <string-name><surname>Boulogne</surname> <given-names>F</given-names></string-name>, <string-name><surname>Warner</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Yager</surname> <given-names>N</given-names></string-name>, <string-name><surname>Gouillart</surname> <given-names>E</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>T</given-names></string-name></person-group>, <article-title>the scikit-image contributors. Scikit-Image: Image Processing in Python</article-title>. <source>PeerJ</source>. <year>2014</year> 6; <volume>2</volume>:<fpage>e453</fpage>. <pub-id pub-id-type="doi">10.7717/peerj.453</pub-id>, doi: <pub-id pub-id-type="doi">10.7717/peerj.453</pub-id>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Winawer</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Computational Neuroimaging and Population Receptive Fields</article-title>. <source>Trends in cognitive sciences</source>. <year>2015</year>; <volume>19</volume>(<issue>6</issue>):<fpage>349</fpage>‚Äì<lpage>357</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Bovik</surname> <given-names>AC</given-names></string-name></person-group>. <article-title>Mean Squared Error: Love It Or Leave It? A New Look At Signal Fidelity Measures</article-title>. <source>IEEE Signal Processing Magazine</source>. <year>2009</year> <month>jan</month>; <volume>26</volume>(<issue>1</issue>):<fpage>98</fpage>‚Äì<lpage>117</lpage>. <pub-id pub-id-type="doi">10.1109/msp.2008.930649</pub-id>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Waskom</surname> <given-names>ML</given-names></string-name></person-group>. <article-title>Seaborn: Statistical Data Visualization</article-title>. <source>Journal of Open Source Software</source>. <year>2021</year>; <volume>6</volume>(<issue>60</issue>):<fpage>3021</fpage>. <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id>, doi: <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watanabe</surname> <given-names>S.</given-names></string-name></person-group> <article-title>A Widely Applicable Bayesian Information Criterion</article-title>. <source>Journal of Machine Learning Research</source>. <year>2013</year>; <volume>14</volume>(<month>Mar</month>):<fpage>867</fpage>‚Äì<lpage>897</lpage>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watson</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Ahumada</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Farrell</surname> <given-names>JE</given-names></string-name></person-group>. <article-title>Window of Visibility: a Psychophysical Theory of Fidelity in Time-Sampled Visual Motion Displays</article-title>. <source>Josa a</source>. <year>1986</year>; <volume>3</volume>(<issue>3</issue>):<fpage>300</fpage>‚Äì<lpage>307</lpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ziemba</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name></person-group>. <article-title>Opposing Effects of Selectivity and Invariance in Peripheral Vision</article-title>. <source>Nature Communications</source>. <year>2021</year> <month>jul</month>; <volume>12</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1038/s41467-021-24880-5</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41467-021-24880-5</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90554.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study provides <bold>important</bold> insights into how researchers can use perceptual metamers to formally explore the limits of visual representations at different processing stages. The framework is <bold>compelling</bold> and the data largely support the claims, subject to minor caveats.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90554.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This is an interesting study on the nature of representations across the visual field. The question of how peripheral vision differs from foveal vision is a fascinating and important one. The majority of our visual field is extra-foveal yet our sensory and perceptual capabilities decline in pronounced and well-documented ways away from the fovea. Part of the decline is thought to be due to spatial averaging ('pooling') of features. Here, the authors contrast two models of such feature pooling with human judgments of image content. They use much larger visual stimuli than in most previous studies, and some sophisticated image synthesis methods to tease apart the prediction of the distinct models.</p>
<p>More importantly, in so doing, the researchers thoroughly explore the general approach of probing visual representations through metamers-stimuli that are physically distinct but perceptually indistinguishable. The work is embedded within a rigorous and general mathematical framework for expressing equivalence classes of images and how visual representations influence these. They describe how image-computable models can be used to make predictions about metamers, which can then be compared to make inferences about the underlying sensory representations. The main merit of the work lies in providing a formal framework for reasoning about metamers and their implications, for comparing models of sensory processing in terms of the metamers that they predict, and for mapping such models onto physiology. Importantly, they also consider the limits of what can be inferred about sensory processing from metamers derived from different models.</p>
<p>Overall, the work is of a very high standard and represents a significant advance over our current understanding of perceptual representations of image structure at different locations across the visual field. The authors do a good job of capturing the limits of their approach I particularly appreciated the detailed and thoughtful Discussion section and the suggestion to extend the metamer-based approach described in the MS with observer models. The work will have an impact on researchers studying many different aspects of visual function including texture perception, crowding, natural image statistics and the physiology of low- and mid-level vision.</p>
<p>The main weaknesses of the original submission relate to the writing. A clearer motivation could have been provided for the specific models that they consider, and the text could have been written in a more didactic and easy to follow manner. The authors could also have been more explicit about the assumptions that they make.</p>
<p>Comments following re-submission:</p>
<p>Overall, I think the authors have done a satisfactory job of addressing most of the points I raised.</p>
<p>There's one final issue which I think still needs better discussion.</p>
<p>I think reviewer 2 articulated better than I have the point I was concerned about: the relationship between JNDs and metamers as depicted in the schematics and indeed in the whole conceptualization.</p>
<p>I think the issue here is that there seems to be a conflating of two concepts- 'subthreshold' and 'metamer'-and I'm not convinced it is entirely unproblematic. It's true that two stimuli that cannot be discriminated from one another due to the physical differences being too small to detect reliably by the visual system are a form of metamer in the strict definition 'physically different, but perceptually the same'.</p>
<p>
However, I don't think this is the scientifically substantial notion of metamer that enabled insights into trichromacy. That form of metamerism is due to the principle of univariance in feature encoding, and involves conditions in which physically very different stimuli are mapped to one and the same point in sensory encoding space whether or not there is any noise in the system. When I say 'physically very different' I mean different by a large enough amount that they would be far above threshold, potentially orders of magnitude larger than a JND if the system's noise properties were identical but the system used a different sensory basis set to measure them. This seems to be a very different kind of 'physically different, but perceptually the same'.</p>
<p>I do think the notion of metamerism can obviously be very usefully extended beyond photoreceptors and photon absorptions. In the interesting case of texture metamers, what I think is meant is that stimuli would be discriminable if scrutinised in the fovea, but because they have the same statistics they are treated as equivalent. I think the discussion of this could still be clearly articulated in the manuscript. It would benefit from a more thorough discussion of the difference between metamerism and subthreshold, especially in the context of the Voronoi diagrams at the beginning.</p>
<p>It needs to be made clear to the reader why it is that two stimuli that are physically similar (e.g., just spanning one of the edges in the diagram) can be discriminable, while at the same time, two stimuli that are very different (e.g., at opposite ends of a cell) can't.</p>
<p>Do the cells include BOTH those sets of stimuli that cannot be discriminated just because of internal noise AND those that can't be discriminated because they are projected to literally the same point in the sensory encoding space? What are the strengths and limits of models that involve the strict binarization of sensory representations, and how can they be integrated with models dealing with continuous differences? These seem like important background concepts that ought to be included in either the introduction of discussion sections. In this context it might also be helpful to refer to the notion of 'visual equivalence' as described by:</p>
<p>Ramanarayanan, G., Ferwerda, J., Walter, B., &amp; Bala, K. (2007). Visual equivalence: towards a new standard for image fidelity. ACM Transactions on Graphics (TOG), 26(3), 76-es.</p>
<p>Other than that, I congratulate the authors on a very interesting study, and look forward to reading the final version.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90554.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors have improved clarity overall and have spoken to most of the issues raised by the reviewers. There are still two outstanding problems however, where issues raised during the review were inappropriately dismissed in the manuscript. These should be explicitly addressed as limitations to the results presented (no eye tracking), and early pilot experiments that informed the experiments as presented (pink noise) rather than brushed off as 'unnecessary' and 'would be uninformative'.</p>
<p>Eye tracking:</p>
<p>It is generally accepted that experiments testing stimuli presented at specific locations in peripheral vision require eye tracking to ensure that the stimulus is presented as expected, in particular, in the correct location. As I stated in the previous round of review, while a stimulus presentation time of 200ms does help eliminate some saccades, it does not eliminate the possibility that subjects were not fixating well during stimulus onset. I am also unclear what the authors mean by 'trained observer' in this context, though the authors state that an author subject in a different portion of the paper is an 'expert observer'. Does this mean the 'trained observers' are non-expert recruited subjects? Given the conditions tested differ from previous work (Freeman &amp; Simoncelli, 2011) *these differences are a main contribution of the paper!* which DID include eye tracking in a subset of subjects, it is entirely possible to get similar results to this work in the context of non eye-tracking controlled stimulus presentation. The reasons now in the manuscript are not reasons that make eye tracking 'considered unnecessary'.</p>
<p>I appreciate that the authors now state the lack of eye tracking explicitly, but believe the paper needs to at least state that this is a limitation of the results reported, and eyetracking being 'considered unnecessary' is unreasonable, nor a norm in this subfield.</p>
<p>N=1: The authors now state clearly the limitations of a single subject in the manuscript, and state the expertise level of this subject.</p>
<p>Large number of trials: The authors now address this and include an enumeration of the large number of trials.</p>
<p>Simple Models / Physiology comparison: I support the choice to reduce claims regarding tight connections to physiology, and appreciate the explanation of the luminance model.</p>
<p>Previous Work: I appreciate the author's changes to the introduction, both in discussing previous work and citation fixes.</p>
<p>Blurred White, Pink Noise: While the authors now address pink noise, the explanation for such stimuli being expected to be uninformative is confusing to me. The manuscript now first states that pink noise is a natural choice, then claims it would be uninformative, while also stating in the rebuttal (not the manuscript) that they tried it and it indeed reduced the artifacts they note. The logic of the experiments indeed relies on finding the smallest critical scaling value, which is measured by subjects determining if a synthesis is similar or different to a target or second synth. A synthesis free from artifacts would surely affect the subjects responses and the smallest critical scaling measured.</p>
<p>The statement that the authors experimented with pink noise early on and found this able to address the artifacts should be stated in the manuscript itself, not just in the rebuttal, and the blanket statement that this experiment would be 'uninformative' is incorrect. Surely this early pilot the authors mention in the rebuttal was informative to designing the experiments that appear in the final paper, and would be an informative experiment to include.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90554.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Broderick</surname>
<given-names>William F</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8999-9003</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Rufo</surname>
<given-names>Gizem</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Winawer</surname>
<given-names>Jonathan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7475-5586</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Simoncelli</surname>
<given-names>Eero P</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1206-527X</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors‚Äô response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>This is an interesting study of the nature of representations across the visual field. The question of how peripheral vision differs from foveal vision is a fascinating and important one. The majority of our visual field is extra-foveal yet our sensory and perceptual capabilities decline in pronounced and well-documented ways away from the fovea. Part of the decline is thought to be due to spatial averaging (‚Äôpooling‚Äô) of features. Here, the authors contrast two models of such feature pooling with human judgments of image content. They use much larger visual stimuli than in most previous studies, and some sophisticated image synthesis methods to tease apart the prediction of the distinct models.</p>
<p>More importantly, in so doing, the researchers thoroughly explore the general approach of probing visual representations through metamers-stimuli that are physically distinct but perceptually indistinguishable. The work is embedded within a rigorous and general mathematical framework for expressing equivalence classes of images and how visual representations influence these. They describe how image-computable models can be used to make predictions about metamers, which can then be compared to make inferences about the underlying sensory representations. The main merit of the work lies in providing a formal framework for reasoning about metamers and their implications, for comparing models of sensory processing in terms of the metamers that they predict, and for mapping such models onto physiology. Importantly, they also consider the limits of what can be inferred about sensory processing from metamers derived from different models.</p>
<p>Overall, the work is of a very high standard and represents a significant advance over our current understanding of perceptual representations of image structure at different locations across the visual field. The authors do a good job of capturing the limits of their approach and I particularly appreciated the detailed and thoughtful Discussion section and the suggestion to extend the metamer-based approach described in the MS with observer models. The work will have an impact on researchers studying many different aspects of visual function including texture perception, crowding, natural image statistics, and the physiology of low- and mid-level vision.</p>
<p>The main weaknesses of the original submission relate to the writing. A clearer motivation could have been provided for the specific models that they consider, and the text could have been written in a more didactic and easy-to-follow manner. The authors could also have been more explicit about the assumptions that they make.</p>
</disp-quote>
<p>Thank you for the summary. We appreciate the positives noted above. We address the weaknesses point by point below.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary</p>
<p>This paper expands on the literature on spatial metamers, evaluating different aspects of spatial metamers including the effect of different models and initialization conditions, as well as the relationship between metamers of the human visual system and metamers for a model. The authors conduct psychophysics experiments testing variations of metamer synthesis parameters including type of target image, scaling factor, and initialization parameters, and also compare two different metamer models (luminance vs energy). An additional contribution is doing this for a field of view larger than has been explored previously</p>
<p>General Comments</p>
<p>Overall, this paper addresses some important outstanding questions regarding comparing original to synthesized images in metamer experiments and begins to explore the effect of noise vs image seed on the resulting syntheses. While the paper tests some model classes that could be better motivated, and the results are not particularly groundbreaking, the contributions are convincing and undoubtedly important to the field. The paper includes an interesting Voronoi-like schematic of how to think about perceptual metamers, which I found helpful, but for which I do have some questions and suggestions. I also have some major concerns regarding incomplete psychophysical methodology including lack of eye-tracking, results inferred from a single subject, and a huge number of trials. I have only minor typographical criticisms and suggestions to improve clarity. The authors also use very good data reproducibility practices.</p>
</disp-quote>
<p>Thank you for the summary. We appreciate the positives noted above. We address the weaknesses point by point below.</p>
<disp-quote content-type="editor-comment">
<p>Specific Comments</p>
<p>Experimental Setup</p>
<p>Firstly, the experiments do not appear to utilize an eye tracker to monitor fixation. Without eye tracking or another manipulation to ensure fixation, we cannot ensure the subjects were fixating the center of the image, and viewing the metamer as intended. While the short stimulus time (200ms) can help minimize eye movements, this does not guarantee that subjects began the trial with correct fixation, especially in such a long experiment. While Covid-19 did at one point limit in-person eye-tracked experiments, the paper reports no such restrictions that would have made the addition of eye-tracking impossible. While such a large-scale experiment may be difficult to repeat with the addition of eye tracking, the paper would be greatly improved with, at a minimum, an explanation as to why eye tracking was not included.</p>
</disp-quote>
<p>Addressed on pg. 25, starting on line 658.</p>
<disp-quote content-type="editor-comment">
<p>Secondly, many of the comparisons later in the paper (Figures 9,10) are made from a single subject. N=1 is not typically accepted as sufficient to draw conclusions in such a psychophysics experiment. Again, if there were restrictions limiting this it should be discussed. Also (P11) Is subject sub-00 is this an author? Other expert? A naive subject? The subject‚Äôs expertise in viewing metamers will likely affect their performance.</p>
</disp-quote>
<p>Addressed on pg. 14, starting on line 308.</p>
<disp-quote content-type="editor-comment">
<p>Finally, the number of trials per subject is quite large. 13,000 over 9 sessions is much larger than most human experiments in this area. The reason for this should be justified.</p>
</disp-quote>
<p>In general, we needed a large number of trials to fit full psychometric functions for stimuli derived for both models, with both types of comparison, both initializations, and over many target images. We could have eliminated some of these, but feel that having a consistent dataset across all these conditions is a strength of the paper.</p>
<disp-quote content-type="editor-comment">
<p>In addition to the sentence on pg. 14, line 318, a full enumeration of trials is now described on pg. 23, starting on line 580.</p>
<p>Model</p>
<p>For the main experiment, the authors compare the results of two models: a ‚Äôluminance model‚Äô that spatially pools mean luminance values, and an ‚Äôenergy model‚Äô that spatially pools energy calculated from a multi-scale pyramid decomposition. They show that these models create metamers that result in different thresholds for human performance, and therefore different critical scaling parameters, with the basic luminance pooling model producing a scaling factor 1/4 that of the energy model. While this is certain to be true, due to the luminance model being so much simpler, the motivation for the simple luminance-based model as a comparison is unclear.</p>
</disp-quote>
<p>The use of simple models is now addressed on pg. 3, starting on line 98, as well as the sentence starting on pg. 4 line 148: the luminance model is intended as the simplest possible pooling model.</p>
<disp-quote content-type="editor-comment">
<p>The authors claim that this luminance model captures the response of retinal ganglion cells, often modeled as a center-surround operation (Rodieck, 1964). I am unclear in what aspect(s) the authors claim these center-surround neurons mimic a simple mean luminance, especially in the context of evidence supporting a much more complex role of RGCs in vision (Atick &amp; Redlich, 1992). Why do the authors not compare the energy model to a model that captures center-surround responses instead? Do the authors mean to claim that the luminance model captures only the pooling aspects of an RGC model? This is particularly confusing as Figures 6 and 9 show the luminance and energy models for original vs synth aligning with the scaling of Midget and Parasol RGCs, respectively. These claims should be more clearly stated, and citations included to motivate this. Similarly, with the energy model, the physiological evidence is very loosely connected to the model discussed.</p>
</disp-quote>
<p>We have removed the bars showing potential scaling values measured by electrophysiology in the primate visual system and attempted to clarify our language around the relationship between these models and physiology. Our metamer models are only loosely connected to the physiology, and we‚Äôve decided in revision not to imply any direct connection between the model parameters and physiological measurements. The models should instead be understood as loosely inspired by physiology, but not as a tool to localize the representation (as was done in the Freeman paper).</p>
<p>The physiological scaling values are still used as the mean of the priors on the critical scaling value for model fitting, as described on pg. 27, starting on line 698.</p>
<disp-quote content-type="editor-comment">
<p>Prior Work:</p>
<p>While the explorations in this paper clearly have value, it does not present any particularly groundbreaking results, and those reported are consistent with previous literature.The explorations around critical eccentricity measurement have been done for texture models (Figure 11) in multiple papers (Freeman 2011, Wallis, 2019, Balas 2009). In particular, Freeman 20111 demonstrated that simpler models, representing measurements presumed to occur earlier in visual processing need smaller pooling regions to achieve metamerism. This work‚Äôs measurements for the simpler models tested here are consistent with those results, though the model details are different. In addition, Brown, 2023 (which is miscited) also used an extended field of view (though not as large as in this work). Both Brown 2023, and Wallis 2019 performed an exploration of the effect of the target image. Also, much of the more recent previous work uses color images, while the author‚Äôs exploration is only done for greyscale.</p>
</disp-quote>
<p>We were pleased to find consistency of our results with previous studies, given the (many) differences in stimuli and experimental conditions (especially viewing angle), while also extending to new results with the luminance model, and the effects of initialization. Note that only one of the previous studies (Freeman and Simoncelli, 2011) used a pooled spectral energy model. Moreover, of the previous studies, only one (Brown et al., 2023) used color images (we have corrected that citation - thanks for catching the error).</p>
<disp-quote content-type="editor-comment">
<p>Discussion of Prior Work:</p>
<p>The prior work on testing metamerism between original vs. synthesized and synthesized vs. synthesized images is presented in a misleading way. Wallis et al.‚Äôs prior work on this should not be a minor remark in the post-experiment discussion. Rather, it was surely a motivation for the experiment. The text should make this clear; a discussion of Wallis et al. should appear at the start of that section. The authors similarly cite much of the most relevant literature in this area as a minor remark at the end of the introduction (P3L72).</p>
</disp-quote>
<p>The large differences we observed between comparison types (original vs synthesized, compared to synthesized vs synthesized) surprised us. Understanding such difference was not a primary motivation for the work, but it is certainly an important component of our results. In the introduction, we thought it best to lay out the basic logic of the metamer paradigm for foveated vision before mentioning the complications that are introduced in both the Wallis and Brown papers (paragraph beginning p. 3, line 109). Our results confirm and bolster the results of both of those earlier works, which are now discussed more fully in the Introduction (lines 109 and following).</p>
<disp-quote content-type="editor-comment">
<p>White Noise: The authors make an analogy to the inability of humans to distinguish samples of white noise. It is unclear however that human difficulty distinguishing samples of white noise is a perceptual issue- It could instead perhaps be due to cognitive/memory limitations. If one concentrates on an individual patch one can usually tell apart two samples. Support for these difficulties emerging from perceptual limitations, or a discussion of the possibility of these limitations being more cognitive should be discussed, or a different analogy employed.</p>
</disp-quote>
<p>We now note the possibility of cognitive limits on pg. 8, starting on line 243, as well as pg. 22, line 571. The ability of observers to distinguish samples of white noise is highly dependent on display conditions. A small patch of noise (i.e., large pixels, not too many) can be distinguished, but a larger patch cannot, especially when presented in the periphery. This is more generally true for textures (as shown in Ziemba and Simoncelli (2021)). Samples of white noise at the resolution used in our study are indistinguishable.</p>
<disp-quote content-type="editor-comment">
<p>Relatedly, in Figure 14, the authors do not explain why the white noise seeds would be more likely to produce syntheses that end up in different human equivalence classes.</p>
</disp-quote>
<p>In figure 14, we claim that white noise seeds are more likely to end up in the same human equivalence classes than natural image seeds. The explanation as to why we think this may be the case is now addressed on pg. 19, starting on line 423.</p>
<disp-quote content-type="editor-comment">
<p>It would be nice to see the effect of pink noise seeds, which mirror the power spectrum of natural images, but do not contain the same structure as natural images - this may address the artifacts noted in Figure 9b.</p>
</disp-quote>
<p>The lack of pink noise seeds is now addressed on pg. 19, starting on line 429.</p>
<disp-quote content-type="editor-comment">
<p>Finally, the authors note high-frequency artifacts in Figure 4 &amp; P5L135, that remain after syntheses from the luminance model. They hypothesize that this is due to a lack of constraints on frequencies above that defined by the pooling region size. Could these be addressed with a white noise image seed that is pre-blurred with a low pass filter removing the frequencies above the spatial frequency constrained at the given eccentricity?</p>
</disp-quote>
<p>The explanation for this is similar to the lack of pink noise seeds in the previous point: the goal of metamer synthesis is model testing, and so for a given model, we want to find model metamers that result in the smallest possible critical scaling value. Taking white noise seed images and blurring them will almost certainly remove the high frequencies visible in luminance metamers in figure 4 and thus result in a larger critical scaling value, as the reviewer points out. However, the logic of the experiments requires finding the smallest critical scaling value, and so these model metamers would be uninformative. In an early stage of the project, we did indeed synthesize model metamers using pink noise seeds, and observed that the high frequency artifacts were less prominent.</p>
<disp-quote content-type="editor-comment">
<p>Schematic of metamerism: Figures 1,2,12, and 13 show a visual schematic of the state space of images, and their relationship to both model and human metamers. This is depicted as a Voronoi diagram, with individual images near the center of each shape, and other images that fall at different locations within the same cell producing the same human visual system response. I felt this conceptualization was helpful. However, implicitly it seems to make a distinction between metamerism and JND (just noticeable difference). I felt this would be better made explicit. In the case of JND, neighboring points, despite having different visual system responses, might not be distinguishable to a human observer.</p>
</disp-quote>
<p>Thanks for noting this ‚Äì in general, metamers are subthreshold, and for the purpose of the diagram, we had to discretize the space showing metameric regions (Voronoi regions) around a set of stimuli. We‚Äôve rewritten the captions to explain this better. We address the binary subthreshold nature of the metamer paradigm in the discussion section (pg. 19, line 438).</p>
<disp-quote content-type="editor-comment">
<p>In these diagrams and throughout the paper, the phrase ‚Äôvisual stimulus‚Äô rather than ‚Äôimage‚Äô would improve clarity, because the location of the stimulus in relation to the fovea matters whereas the image can be interpreted as the pixels displayed on the computer.</p>
</disp-quote>
<p>We agree and have tried to make this change, describing this choice on pg. 3 line 73.</p>
<disp-quote content-type="editor-comment">
<p>Other</p>
<p>The authors show good reproducibility practices with links to relevant code, datasets, and figures.</p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>In its current form, I found the introduction to be too cursory. I felt that the article would benefit from a clearer motivation for the two models that are considered as the reader is left unclear why these particular models are of special scientific significance. The luminance model is intended to capture some aspects of retinal ganglion cells response characteristics and the spectral energy model is intended to capture some aspects of the primary visual cortex. However, one can easily imagine models that include the pooling of other kinds of features, and it would be helpful to get an idea of why these are not considered. Which aspects of processing in the retina and V1 are being considered and which are being left out, and why? Why not consider representations that capture even higher-order statistical structure than those covered by the spectral energy model (or even semantics)? I think a bit of rewriting with this in mind could improve the introduction.</p>
<p>Along similar lines, I would have appreciated having the logic of the study explained more explicitly and didactically: which overarching research question is being asked, how it is operationalised in the models and experiments, and what are the predictions of the different models. Figures 2 and 3 are certainly helpful, but I felt further explanations would have made it easier for the reader to follow. Throughout, the writing could be improved by a careful re-reading with a view to making it easier to understand. For example, where results are presented, a sentence or two expanding on the implications would be helpful.</p>
<p>I think the authors could also be more explicit about the assumptions they make. While these are obviously (tacitly) included in the description of the models themselves, it would be helpful to state them more openly. To give one example, when introducing the notion of critical scaling, on p.6 the authors state as if it is a self-evident fact that &quot;metamers can be achieved with windows whose size is matched to that of the underlying visual neurons&quot;. This presumably is true only under particular conditions, or when specific assumptions about readout from populations of neurons are invoked. It would be good to identify and state such assumptions more directly (this is partly covered in the Discussion section ‚ÄôThe linking proposition underlying the metamer paradigm‚Äô, but this should be anticipated or moved earlier in the text).</p>
</disp-quote>
<p>We agree that our introduction was too cursory and have reworked it. We have also backed off of the direct comparison to physiology and clarified that we chose these two as the simplest possible pooling models. We have also added sentences at the end of each result section attempting to summarize the implication (before discussing them fully in the discussion). Hopefully the logic and assumptions are now clearer.</p>
<disp-quote content-type="editor-comment">
<p>There are also some findings that warrant a more extensive discussion. For example, what is the broader implication of the finding that original vs. synthesised and synthesised vs. synthesised comparisons exhibit very different scaling values? Does this tell us something about internal visual representations, or is it simply capturing something about the stimuli?</p>
</disp-quote>
<p>We believe this difference is a result of the stimuli that are used in the experiment and thus the synthesis procedure itself, which interacts with the model‚Äôs pooled image feature. We have attempted to update the relevant figures and discussions to clarify this, in the sections starting on pg 17 line 396 and pg. 19 line 417.</p>
<disp-quote content-type="editor-comment">
<p>At some points in the paper, a third model (‚Äôtexture model‚Äô) creeps into the discussion, without much explanation. I assume that this refers to models that consider joint (rather than marginal) statistics of wavelet responses, as in the famous Portilla &amp; Simoncelli texture model. However, it would be helpful to the reader if the authors could explain this.</p>
</disp-quote>
<p>Addressed on pg. 3, starting on line 94.</p>
<disp-quote content-type="editor-comment">
<p>Minor corrections.</p>
<p>Caption of Figure 3: ‚Äôtop‚Äô and ‚Äôbottom‚Äô should be ‚Äôleft‚Äô and ‚Äôright‚Äô</p>
<p>Line 177: ‚Äôsmallest tested scaling values tested‚Äô. Remove one instance of ‚Äôtested‚Äô</p>
<p>Line 212: ‚Äôthe images-specific psychometric functions‚Äô -&gt; ‚Äôimage-specific‚Äô</p>
<p>Line 215: ‚Äôcloud-like pink noise‚Äô. It‚Äôs not literally pink noise, so I would drop this.</p>
<p>Line 236: ‚ÄôImportantly, these results cannot be predicted from the model, which gives no specific insight as to why some pairs are more discriminable than others‚Äô. The authors should specify what we do learn from the model if it fails to provide insight into why some image pairs are more discriminable than others.</p>
<p>Figure 9: it might be helpful to include small insets with the ‚Äôhighway‚Äô and ‚Äôtiles‚Äô source images to aid the reader in understanding how the images in 9B were generated.</p>
<p>Table 1 placement should be after it is first referred to on line 258.</p>
<p>In the Discussion section &quot;Why does critical scaling depend on the comparison being performed&quot;, it would be helpful to consider the case where the two model metamers *are* distinguishable from each other even though each is indistinguishable from the target image. I would assume that this is possible (e.g., if the target image is at the midpoint between the two model images in image space and each of the stimuli is just below 1 JND away from the target). Or is this not possible for some reason?</p>
</disp-quote>
<p>Regarding line 236: this specific line has been removed, and the discussion about this issue has all been consolidated in the final section of the discussion, starting on pg. 19 line 438.</p>
<p>Regarding the final comment: this is addressed in the paragraph starting on pg. 16 line 386. To expand upon that: the situation laid out by the reviewer is not possible in our conceptualization, in which metamerism is transitive and image discriminability is binary. In order to investigate situations like the one laid out by the reviewer, one needs models whose representations have metric properties, i.e., which allow you to measure and reason about perceptual distance, which we refer to in the paragraph starting on pg. 20 line 460. We also note that this situation has not been observed in this or any other pooling model metamer study that we are aware of. All other minor changes have been addressed.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>Original image T should be marked in the Voronoi diagrams.</p>
<p>Brown et al is miscited as 2021 should be ACM Transactions on Applied Perception 2023.</p>
<p>Figure 3 caption: models are left and right, not top and bottom.</p>
</disp-quote>
<p>Thanks, all of the above have been addressed.</p>
<p>References</p>
<p>BrownReral Encoding, in the Human Visual System. ACM Transactions on Applied Perception. 2023 Jan; 20(1):1‚Äì22.<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1145/356460">http://dx.doi.org/10.1145/356460</ext-link>, Dutell V, Walter B, Rosenholtz R, Shirley P, McGuire M, Luebke D. Efficient Dataflow Modeling of Periph-<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1145/3564605">5</ext-link>, doi: 10.1145/3564605.</p>
<p>Freeman Jdoi: 10.1038/nn.2889, Simoncelli EP. Metamers of the ventral stream. Nature Neuroscience. 2011 aug; 14(9):1195‚Äì1201..</p>
<p>Ziemba CMnications. 2021 jul; 12(1)., Simoncelli EP. Opposing Effects of Selectivity and Invariance in Peripheral Vision. Nature Commu-<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-021-24880-5">https://doi.org/10.1038/s41467-021-24880-5</ext-link>, doi: 10.1038/s41467-021-24880-5.</p>
</body>
</sub-article>
</article>