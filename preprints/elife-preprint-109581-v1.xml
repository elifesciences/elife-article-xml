<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">109581</article-id>
<article-id pub-id-type="doi">10.7554/eLife.109581</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.109581.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Dynamic updating of spatial working memory across eye movements: a computational investigation of transsaccadic integration</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6246-0702</contrib-id>
<name>
<surname>Zhao</surname>
<given-names>Sijia</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">‡</xref>
<email>sijia.zhao@psy.ox.ac.uk</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5108-5743</contrib-id>
<name>
<surname>Parr</surname>
<given-names>Thomas</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">‡</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1704-554X</contrib-id>
<name>
<surname>Udale</surname>
<given-names>Rob</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1901-2485</contrib-id>
<name>
<surname>Klar</surname>
<given-names>Verena</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jones</surname>
<given-names>Gabriel David</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Scholcz</surname>
<given-names>Anna</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1833-4995</contrib-id>
<name>
<surname>Toniolo</surname>
<given-names>Sofia</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0735-4349</contrib-id>
<name>
<surname>Manohar</surname>
<given-names>Sanjay G</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6850-9255</contrib-id>
<name>
<surname>Husain</surname>
<given-names>Masud</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Department of Experimental Psychology, University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">United Kingdom</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Nuffield Department of Clinical Neurosciences, University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">United Kingdom</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Nuffield Department of Women’s and Reproductive Health, University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">United Kingdom</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0080acb59</institution-id><institution>Cognitive Disorders Clinic, John Radcliffe hospital, University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Wang</surname>
<given-names>Shuo</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Washington University in St. Louis</institution>
</institution-wrap>
<city>St. Louis</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>‡</label><p>SZ and TP had equal contribution to this paper.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-12-31">
<day>31</day>
<month>12</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP109581</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-10-27">
<day>27</day>
<month>10</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-10-30">
<day>30</day>
<month>10</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.07.26.666983"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Zhao et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Zhao et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-109581-v1.pdf"/>
<abstract>
<p>The brain continuously integrates rapidly changing visual input across eye movements to maintain stable perception, yet the precise mechanisms underpinning dynamic working memory and how these break down in brain diseases remain unclear. We developed a novel eye-tracking paradigm and computational models to investigate how spatial and colour information are updated across saccades. Our findings reveal that saccades selectively impair spatial but not colour memory. Computational modelling identified that spatial representations are maintained in a dual eye-centred frame of reference which is actively updated by a noisy memory of saccades but is vulnerable to interference. Using this model, we found that specific mechanistic failures in initial encoding and memory decay, rather than the saccadic updating process itself, account for spatial working memory deficits in Alzheimer’s and Parkinson’s disease. These results provide a mechanistic understanding of how dynamic spatial memory operates in health and its disruption in neurodegenerative disorders.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>remapping</kwd>
<kwd>spatiotopic memory</kwd>
<kwd>retinotopic memory</kwd>
<kwd>drawing</kwd>
<kwd>eye movements</kwd>
</kwd-group>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id>
<institution>Wellcome Trust (WT)</institution>
</institution-wrap>
</funding-source>
<award-id>206330/Z/17/Z</award-id>
<principal-award-recipient>
<name>
<surname>Toniolo</surname>
<given-names>Sofia</given-names>
</name>
<name>
<surname>Husain</surname>
<given-names>Masud</given-names>
</name>
<name>
<surname>Udale</surname>
<given-names>Rob</given-names>
</name>
<name>
<surname>Jones</surname>
<given-names>Gabriel David</given-names>
</name>
<name>
<surname>Zhao</surname>
<given-names>Sijia</given-names>
</name>
<name>
<surname>Scholcz</surname>
<given-names>Anna</given-names>
</name>
<name>
<surname>Klar</surname>
<given-names>Verena</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-8">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id>
<institution>Wellcome Trust (WT)</institution>
</institution-wrap>
</funding-source>
<award-id>226645/Z/22/Z</award-id>
<principal-award-recipient>
<name>
<surname>Husain</surname>
<given-names>Masud</given-names>
</name>
<name>
<surname>Toniolo</surname>
<given-names>Sofia</given-names>
</name>
<name>
<surname>Zhao</surname>
<given-names>Sijia</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-11">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id>
<institution>UKRI | Medical Research Council (MRC)</institution>
</institution-wrap>
</funding-source>
<award-id>MR/P00878/X</award-id>
<principal-award-recipient>
<name>
<surname>Manohar</surname>
<given-names>Sanjay G</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-12">
<funding-source>
<institution-wrap>
<institution>NIHR Academic Clinical Fellowship</institution>
</institution-wrap>
</funding-source>
<award-id>ACF-2023-13-013</award-id>
<principal-award-recipient>
<name>
<surname>Parr</surname>
<given-names>Thomas</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-13">
<funding-source>
<institution-wrap>
<institution>NIHR Oxford Health BRC</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Husain</surname>
<given-names>Masud</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-14">
<funding-source>
<institution-wrap>
<institution>NIHR Oxford Health BRC</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Toniolo</surname>
<given-names>Sofia</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-15">
<funding-source>
<institution-wrap>
<institution>NIHR Oxford Health BRC</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Manohar</surname>
<given-names>Sanjay G</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-16">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100013373</institution-id>
<institution>NIHR | NIHR Oxford Biomedical Research Centre (OxBRC)</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Manohar</surname>
<given-names>Sanjay G</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Updated the corresponding author's affiliation address due to a recent change of departmental address.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Natural vision is profoundly dynamic, relying upon sequences of saccades. Everyday tasks, such as making a cup of tea,<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup> arranging the layout of objects,<sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref></sup> or copying a simple line drawing <sup><xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c7">7</xref></sup>, require us to precisely track spatial locations—which appear at different retinotopic locations with each fixation—across saccades.<sup><xref ref-type="bibr" rid="c8">8</xref></sup> This requires a dynamic updating process, termed “remapping”, that involves updating our internal representation of object locations to maintain a stable world-centred frame of reference.<sup><xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c12">12</xref></sup> One key hypothesis is that we use working memory across visual fixations to update perception dynamically.<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref></sup> However, despite the apparent stability of our visual world, it has proven difficult to separate the distinct contributions of retinal memory from transsaccadic memory. To precisely characterise the interplay of factors contributing to dynamic spatial working memory— particularly amidst the complexities of multiple fixations as well as the potential for memory decay and interference—and to move beyond purely descriptive observations, fundamental questions regarding the underlying mechanisms and sources of potential errors need to be addressed.</p>
<p>Central to such an endeavour is understanding the nature of the brain’s internal spatial representation: Is it a single, world-centred (allocentric) map, or does it rely on a fixation-centred (retinotopic) representation that must be actively updated after each saccade?<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup> If the latter, how do we keep track of the fixation locations from which the allocentric spatial location can be reconstructed? Do our brains retain memories for recent fixation locations, or do we reconstruct these from memories of recent saccades? One way of addressing these questions is to examine the spatiotemporal patterns of errors made during working memory tasks. Under the assumption that imperfect memory traces are subject to time-dependent decay,<sup><xref ref-type="bibr" rid="c17">17</xref></sup> the patterns of errors made should differ depending upon the spatial variables represented.</p>
<p>In addition to these unresolved questions in healthy people, it has become apparent that neurodegenerative conditions such as Alzheimer’s disease (AD) and Parkinson’s disease (PD) can be associated with significant disruptions to visuospatial processing and constructional apraxia (a disorder characterised by impaired drawing and copying abilities).<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref></sup> This raises a crucial question: Could specific deficits in transsaccadic remapping mechanisms explain the impairments in complex visuospatial tasks, such as copying drawings?</p>
<p>To address these questions and move beyond purely descriptive observations towards mechanistic models, we developed a novel eye-tracking paradigm — the LOcation and Colour Updating across Saccades (LOCUS) paradigm (<xref rid="fig1" ref-type="fig">Figure 1</xref>)—to directly compare retinotopic and transsaccadic working memory for both spatial location and colour. Combined with computational modelling, our approach allowed us to precisely characterize the interplay of factors contributing to dynamic spatial working memory, isolate the impact of saccades, and test specific, competing hypotheses regarding transsaccadic memory. Including young and older healthy adults as well as patients with Parkinson’s and Alzheimer’s disease, our findings reveal that saccades selectively disrupt spatial but not colour memory. Across all healthy and patient groups, the winning computational model indicated that spatial representations are maintained in a dual (retinotopic) reference frame, updated based on a noisy memory of the saccade vector, and susceptible to interference from other items in memory. Importantly, we identified specific model parameters—related to angular encoding, memory decay, and interference—that differentiated the patient groups from healthy controls, providing a mechanistic account of their visuospatial deficits.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Location and Colour Updating across Saccades (LOCUS) Task.</title>
<p>Participants were asked to fixate a white cross, wherever it appeared. They had to remember the colour and location of a sequence of two briefly presented coloured squares (Item 1 and 2), each appearing within a white square frame. They then fixated a colour wheel wherever it appeared on the screen. This cued recall of a specific square (Item 1 or Item 2 labelled within the colour wheel). Participants selected the remembered colour on the colour wheel which led to a square of that colour appearing on the screen. They then dragged this square to its remembered location on the screen. There were four conditions: <bold>1</bold>) No-Saccade condition providing a measure of retinotopic memory; <bold>2</bold>) Saccade After Item 1; <bold>3</bold>) Saccade After Item 2; <bold>4</bold>) Saccades after both items (Two Saccades condition). Conditions 2)-4) provide a measure of the impact of 1 or 2 saccades on recall, in addition to any retinotopic contribution.</p></caption>
<graphic xlink:href="666983v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Saccades disrupt spatial memory but spare colour</title>
<p>In the LOCUS (Location and Colour Updating across Saccades) paradigm (<xref rid="fig1" ref-type="fig"><italic>Figure 1</italic></xref>), participants memorised the colour and location of two sequentially presented squares. Subsequently, they were cued to recall the colour and location of one of these squares, i.e. either the first or the second. Colour recall involved clicking on a colour wheel to indicate the remembered colour using the mouse. Location recall required dragging a colour-matched square to the remembered position on the screen.</p>
<p>By manipulating the spatial location of the squares and also that of the colour wheel, we controlled saccadic demands on every trial, creating four conditions with equal probability: no-saccade, saccade after Item 1, saccade after Item 2 (to the colour wheel), or a saccade after both items. The no-saccade trials provided a measure of retinotopic working memory, while the two-saccade trials assessed transsaccadic working memory. To mitigate potential confounds, we monitored eye position throughout the experiment. Eye-tracking analysis confirmed high compliance: participants correctly maintained fixation or executed saccades as instructed on the vast majority of trials (83% ± 14%). Non-compliant trials were excluded from further analysis.</p>
<p>This design allowed us to isolate and quantify the unique impact of saccades on spatial memory, enabling us to test competing hypotheses regarding spatial representation. If an allocentric mechanism solely underpinned spatial memory, we would anticipate no difference in memory precision across saccade conditions, as the spatial representation would be world-centred and unaffected by eye movements. Thus, performance in the no-saccade condition should be comparable to the two-saccade condition.</p>
<p>Conversely, if spatial memory relies on a retinotopic representation that requires active updating with eye movements, a different pattern or error would be predicted. The two-saccade condition was anticipated to be the most challenging due to cumulative decay in the memory traces used for stimulus reconstruction after each saccade.<sup><xref ref-type="bibr" rid="c20">20</xref></sup> In contrast, the no-saccade condition was expected to yield the most accurate localisation, relying solely on retinotopic information. Additionally, given well-known recency effects in working memory, recall performance for the second item was also expected to be better than for the first stimulus in each condition.<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup> This is implicit in all of the models we tested because the variance of responses grows with time since the stimulus was observed. Importantly, we also hypothesised that saccades would selectively disrupt location memory while leaving colour memory intact.</p>
<p>These predictions were confirmed in young healthy adults (<italic>N</italic> = 21, mean age = 24.1 years, ranged between 19 and 34). A repeated measures ANOVA revealed a significant main effect of saccades on location memory (F(2.2,43.9)=33.2, p&lt;0.001, partial η²=0.62), indicating substantial impairment after eye movements (<xref rid="fig2" ref-type="fig"><italic>Figure 2A</italic></xref>). The saccades affected both items similarly (no interaction between saccades and item (F(2.6,52.7)=0.64, p=0.57, partial η²=0.03). However, as expected, spatial memory for Item 1 was significantly less accurate than for Item 2 (F(1,20) = 12.5, <italic>p</italic> = 0.002, partial η² = 0.38). Crucially, even a single saccade after Item 2 significantly reduced location memory accuracy compared to the no-saccade condition (t=-7.64, p(bonf)&lt;0.001).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Saccades lead to increased recall error for location but not colour.</title>
<p><bold>(A)</bold> Mean location error for each saccade condition, and for each probed item (Item 1: solid lines; Item 2: dashed lines). <bold>(B)</bold> Colour error for the same conditions. Shaded error represents ±1 standard error from mean. <bold>(C)</bold> Beta coefficients from a multiple linear regression model predicting location error, illustrating the effect of item order (1 or 2), saccade after the probed item, and saccade after the other item, for young (blue) and elderly (orange) participants. Error bars representL±L1 standard errors. (D) The same model applied to colour errors, revealing a recency effect but no impact of saccade demands. ** means significantly different from zero p&lt;0.01, *** means significantly different from zero p&lt;0.001.</p></caption>
<graphic xlink:href="666983v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Saccadic interference with location memory is particularly striking because even eye movements directed towards the colour wheel to report the colour remembered had a detrimental effect on the preservation of precise spatial representations. In contrast, colour memory itself was not affected by saccades (<xref rid="fig2" ref-type="fig"><italic>Figure 2B</italic></xref>; F(2.2, 44.7) = 0.68, <italic>p</italic>=0.53, partial η² =0.03), confirming the specificity of saccadic interference on spatial memory. The expected recency effect was also observed in colour report, with better memory for the second item (F(1,20) = 6.52, <italic>p</italic> = 0.02, partial η² = 0.25).</p>
<p>Further analysis indicated that larger saccades were associated with increased spatial memory disruption (see Supplementary Materials for “Larger saccades disrupt memory more”). A multiple linear regression predicting location error showed that only the vertical distance between stimulus frames significantly predicted error (t(41)=5.73,p&lt;0.0001), with marginal significance for horizontal saccade direction (t(41)=−1.94,p=0.059). This suggests that greater displacement, particularly vertical, exacerbates spatial memory impairment, irrespective of saccade direction. However, there was no evidence that crossing saccades (e.g., rightward then leftward) or specific horizontal/vertical saccade directions differentially affected spatial memory precision, as revealed by repeated measures ANOVAs (F(1.3,49.8)=0.04, p=0.89, partial η2=0.001 for horizontal crossing; F(1.7,62.8)=0.29,p=0.71, partial η2=0.008 for vertical crossing).</p>
</sec>
<sec id="s2b">
<title>Saccadic interference with spatial memory is age-independent</title>
<p>To investigate the impact of aging, we replicated the experiment in a group of healthy adults above 60 (Elderly Healthy Adults; <italic>N</italic> = 21; mean age = 72.4 years, ranged between 60 and 80). A three-way ANOVA (4 saccade conditions x 2 items x 2 age groups) revealed a main effect of age, with older participants demonstrating lower overall accuracy compared to their younger counterparts (<xref rid="fig2" ref-type="fig"><italic>Figure 2A</italic></xref>, F(1,40) = 22.73, p &lt; 0.001, partial η² = 0.36). This age-related decline was evident in both transsaccadic working memory (Two-Saccades Location Error, rho=0.57, p &lt; 0.001, Fisher’s z = 0.64; partial rho=0.55, p&lt;0.001, z=0.62 after controlling gender and ACE total score) and retinotopic working memory (No-Saccade Location Error, rho=0.70, p&lt;0.001, Fisher’s z = 0.86; partial rho=0.69, p&lt;0.001, z=0.85).</p>
<p>These findings suggest that while both retinotopic and transsaccadic working memory decline with age, the relative impact of saccades on spatial memory remains stable across age groups. This was also supported by the lack of significant interaction between age group, saccade condition, and item type (all <italic>p</italic> &gt; 0.23), suggesting that the detrimental effect of saccades on spatial memory is constant across both young and older adults. This observation was further supported by post-hoc tests, which confirmed the <italic>same</italic> pattern of location errors across saccade conditions in both groups: the no-saccade condition exhibited the least location error (p(bonf) &lt; 0.001, |t| ≥ 5.94), followed by the one-saccade conditions (no difference between them: t = 0.02, p(bonf) = 1.00, Cohen’s d = 0.002), and with the highest error on the two-saccade condition (|t|≥3.53, p(bonf) ≤ 0.006).</p>
<p>A multiple linear regression model, incorporating data from both age groups further corroborated these findings. Both earlier item presentation and the presence of saccades after either item predicted increased location error (<xref rid="fig2" ref-type="fig"><italic>Figure 2C</italic></xref>) but not colour error (<xref rid="fig2" ref-type="fig"><italic>Figure 2D</italic></xref>). There was no difference in the beta coefficients between two age groups.</p>
<p>In summary, while healthy aging was associated with a general decline in spatial memory accuracy, the specific disruptive effect of saccades on spatial – but not colour – memory remained constant across age groups. Thus, the mechanisms underlying transsaccadic remapping and its impact on spatial working memory are generally preserved in healthy aging, despite an overall reduction in memory performance.</p>
</sec>
<sec id="s2c">
<title>Computational model comparison reveals the mechanisms of transsaccadic memory</title>
<p>To dissect the specific cognitive processes underlying the observed saccade cost on spatial memory, we turned to computational modelling with a suite of seven computational models designed to test specific, competing hypotheses about how the brain represents and updates visual information across saccades (<xref rid="tbl1" ref-type="table">Table 1</xref>). The models are organised hierarchically, starting from a simple, single-representation model and progressively incorporating more complex mechanisms. This allows us to systematically identify which mechanisms are necessary to account for the observed behaviour. The model, data, and analysis scripts are all available in the open dataset: <ext-link ext-link-type="uri" xlink:href="https://osf.io/95ecp/">https://osf.io/95ecp/</ext-link>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Hypotheses expressed as seven competing models.</title></caption>
<graphic xlink:href="666983v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Demographics of Participants.</title><p>The value in brackets indicate one standard deviation or proportion of the sample. YC = Young healthy control; EC = Elderly healthy control; PD = Parkinson’s Disease; AD = Alzheimer’s Disease; ACE = the Addenbrooke’s Cognitive Examination III.</p></caption>
<graphic xlink:href="666983v2_tbl1a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Our modelling framework addresses three core conceptual distinctions:
<list list-type="order">
<list-item><p><bold>Reference frame.</bold> Whether spatial memory is maintained in a single, world-centred (allocentric) map (<xref rid="fig3" ref-type="fig">Figure 3A</xref>), or relies on a dual, fixation-centred (retinotopic) reference frame (<xref rid="fig3" ref-type="fig">Figure 3B</xref>).</p></list-item>
<list-item><p><bold>Distractor interference.</bold> Whether memory is susceptible to interference from other items held in memory (<xref rid="fig3" ref-type="fig">Figure 3C</xref>).</p></list-item>
<list-item><p><bold>Remapping source.</bold> Within a dual-frame system, whether error arises from a passive decay of the remembered fixation point (<xref rid="fig3" ref-type="fig">Figure 3D</xref>), or from an active but noisy reconstruction based on the saccade vector itself (<xref rid="fig3" ref-type="fig">Figure 3E</xref>).</p></list-item>
</list>
</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Conceptual illustration of allocentric vs retinotopic spatial representations.</title>
<p>This figure depicts the distinction between various representations of spatial memory. These include allocentric (world-centred) and retinotopic (eye-centred) spatial representation. <bold>(A)</bold> The <bold>Allocentric</bold> panel illustrates a world-centred reference frame, where a stimulus location (x, y) is represented independently of eye position. The spread of the grey shaded area around the stimulus indicates for this representation the encoding error and subsequent decay since the stimulus was viewed. In this framework, memory error is assumed to increase over time due to a general decay of the stored coordinates, which is treated as a decay in Euclidean space, such that the probability density of a remembered location (coordinates (x,y)) is a normal distribution whose covariance grows with time centred on the stimulus. This model corresponds to Model 1 in <xref rid="tbl1" ref-type="table">Table 1</xref>. <bold>(B)</bold> The <bold>Retinotopic</bold> panel presents an eye-centred reference frame, where the stimulus location is encoded relative to the current fixation point (x, y) using polar coordinates: radial distance (r) and angular direction (θ). The orange arrow indicates the vector from fixation to the stimulus, representing these encoded retinotopic coordinates. The grey shaded area here again represents the initial encoding error and subsequent decay, which can be decomposed into radial and angular components. This <bold>dual model</bold>, incorporating both encoding error and decay in radial and angular components, corresponds to Model 2 in <xref rid="tbl1" ref-type="table">Table 1</xref>. It forms the basis for more complex models (Models 3-7 in <xref rid="tbl1" ref-type="table">Table 1</xref>) that differentiate between decay of the reference frame itself (e.g., recall of fixation point) and updating based on saccade information. <bold>(C)</bold> The <bold>Interference</bold> panel shows the combination of a stimulus and distractor (translated into the same retinotopic coordinates), with some probability associated with a response to the memory of the distractor stimulus. The lower two panels distinguish between whether the origin of the retinotopic plot is recalled based upon a remembered fixation location <bold>(D)</bold>, or whether it is based upon a reconstruction after propagating back through remembered saccade vectors <bold>(E)</bold> — conceptually shown with arrows whose base is large to indicate the (cumulative) uncertainty in the origin points of each vector.</p></caption>
<graphic xlink:href="666983v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>All models assume that memory for location decays over time, formalised as a diffusion (Brownian motion) process, where the precision of the remembered location decreases with increasing retention interval. The key differences between the models lie in what is decaying and when (see Methods for mathematical details and detailed variance functions).</p>
<p>These conceptual distinctions give rise to a family of seven specific computational models (<xref rid="tbl1" ref-type="table">Table 1</xref>):
<list list-type="order">
<list-item><p><bold>Allocentric model</bold>: This model assumes the brain stores locations in a single, stable, world-centred (allocentric) reference frame. In this model, memory error arises solely from a time-dependent decay of the stored coordinates, independent of any eye movements.</p></list-item>
<list-item><p><bold>Dual model</bold>: This foundational model introduces the core hypothesis of a dual-reference frame. It posits that locations are initially encoded in eye-centred (retinotopic) coordinates (<bold>radial</bold> distance and <bold>angular</bold> direction from fixation), which are then subject to both initial encoding errors and time-dependent decay. This forms the basis for more complex models that explicitly distinguish between decay in the reference frame’s centre and coordinates within that frame.</p></list-item>
<list-item><p><bold>Dual (fixation decay) model</bold>: Building on the dual-frame hypothesis, this model proposes that to remap a location after a saccade, the brain uses its memory of the <italic>original fixation point</italic>. This memory trace of the eye’s position itself decays over time, introducing a specific source of error into the remapped location. Of note, the time at which the memory of the fixation location starts to decay is the time of the first saccade following the disappearance of the stimulus. This means that, depending upon the experimental condition, the fixation memory starts to decay <italic>after</italic> the retinotopic memories start to decay.</p></list-item>
<list-item><p><bold>Dual (saccade update) model</bold>: This model offers an alternative updating mechanism. Instead of relying on a memory of the absolute fixation location, it proposes that the brain uses a memory of the <italic>saccade vector</italic> itself (the direction and amplitude of the eye movement). The original location is then reconstructed based on this imperfect, noisy memory of the saccade, providing a different source of remapping error. Here, each saccade between the fixation at which the stimulus was shown and the response time injects an additional encoding error which have cumulative effects, in addition to the cumulative memory decay from decays in each of the saccade vector memories in the time since the saccade.</p></list-item>
<list-item><p><bold>Dual + interference model</bold>: This model tests the influence of cognitive load by incorporating crosstalk between items in memory. It suggests that the memory for the target location is partially corrupted by the presence of the other, distracting item’s location, adding a source of error independent of remapping. In effect, this means the response distribution is assumed to be a weighted sum between the distributions anticipated if responding to the target or to the distractor.</p></list-item>
<list-item><p><bold>Dual (fixation decay) + interference model</bold>: This model combines two potential sources of error: the passive decay of the remembered fixation point (Fixation Decay) and corruption from the distractor item (Interference).</p></list-item>
<list-item><p><bold>Dual (saccade update) + interference model</bold>: This is the most complex model, combining the active, saccade-based updating mechanism with distractor interference. It proposes that memory error arises from both an imperfect remapping process based on a noisy saccade signal and from confusion with the other item held in memory.</p></list-item>
</list>
By fitting these models to our participants’ response data, we inferred which set of mechanisms best explains their performance. We fit all seven models to the trial-by-trial response data from all healthy participants (N=42) (<xref rid="fig4" ref-type="fig">Figure 4</xref>) and used random-effects Bayesian model selection (BMS) to identify the most plausible generative model, a process that inherently balances model fit with complexity <sup><xref ref-type="bibr" rid="c23">23</xref>–<xref ref-type="bibr" rid="c25">25</xref></sup>. The analysis yielded a strong result: the “<bold>Dual (Saccade) + Interference</bold>” model (<bold>Model 7 in </bold><xref rid="tbl1" ref-type="table">Table 1</xref>) emerged as the winning model, providing substantial evidence against the next best alternative with a Bayes Factor of 6.11. This indicates substantial evidence that saccade-based updating with distractor interference is the most prevalent generative process in the healthy population.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Model comparison results in healthy control (HC) participants (N=42).</title>
<p><bold>(A)</bold> Individual participant posterior probabilities. Heatmap illustrates the posterior probability for each of the seven competing models across individual healthy participants. The red dashed vertical line separates the age groups, with young controls on the left and elderly healthy controls on the right. This panel indicates that the ‘Dual (Saccade Update) + Interference’ model frequently shows the highest posterior probability for individual participants. <bold>(B)</bold> Aggregate model posterior probabilities. Bar chart showing the posterior probability for each competing model, aggregated across all healthy participants.</p></caption>
<graphic xlink:href="666983v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The superiority of this specific model provides three critical insights. Firstly, the victory of a dual-frame model over the simpler allocentric model strongly supports the hypothesis that spatial memory is fundamentally reliant on eye-centred representations that must be actively updated. Secondly, the model comparison decisively adjudicates the source of remapping error: the clear win for the ‘Saccade Update’ mechanism suggests that transsaccadic error arises not from a passive forgetting of where the eyes were, but from an active, yet imperfect, process of remapping based on a noisy internal copy of the saccade command—or, perhaps, from the proprioceptive signal associated with that movement. Finally, the inclusion of the ‘Interference’ term was essential for the winning model, demonstrating that this remapping process is also susceptible to confusion from other items held in memory. This winning model was therefore used for all subsequent analyses.</p>
</sec>
<sec id="s2d">
<title>Patients with AD and PD show preserved saccadic interference but greater overall error</title>
<p>Having established the mechanisms in healthy individuals, we next investigated how these processes are affected in patients with Alzheimer’s disease (AD) and Parkinson’s disease (PD), comparing their performance to that of both young (YC) and elderly (EC) controls. As anticipated, there was a significant main effect of group on overall memory performance. A two-way ANOVA revealed that patient groups were less accurate on both location (F(3, 318) = 59.71, p &lt; 0.001) and colour memory (F(3, 318) = 47.85, p &lt; 0.001). Post-hoc analyses confirmed that AD patients exhibited significantly greater errors than all other groups across every condition (e.g., on No-Saccade Location Error vs YC, EC, and PD: all p &lt; 0.001; see <xref rid="fig5" ref-type="fig">Figure 5</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Saccadic interference with spatial memory is preserved across healthy aging and neurodegenerative conditions, despite group differences in recall error.</title>
<p>Saccades lead to increased recall error for location, but not colour, in healthy young (YC) and elderly (EC) controls, as well as in patients with PD and AD. While patient groups exhibit greater overall memory errors, the fundamental pattern of saccadic disruption remains consistent across all cohorts. (<bold>A</bold>) Location error. Mean location error (in degrees of visual angle) for each saccade condition (No-Saccade, 1 Irrelevant Saccade, 1 Saccade, 2 Saccades) for all participant groups. A two-way ANOVA revealed a significant main effect of Group and Condition, but there was no significant Group × Condition interaction (F(9, 318) = 0.46, p = 0.900), indicating that the magnitude of saccadic disruption to spatial memory is constant across groups. Post-hoc Tukey-Kramer comparisons within each condition between groups confirmed a graded impairment in overall spatial accuracy: AD patients exhibited significantly greater errors than all other groups (e.g., AD vs. YC, EC, PD: all p &lt; 0.001), PD patients performed worse than both YC and EC (all p &lt; 0.001), and EC performed worse than YC (all p &lt; 0.001). The significant post-hoc between group comparisons are shown above the bar plots, with Bonferroni multiple comparison correction applied. (<bold>B</bold>) Colour error. Mean colour error (in radians) for each saccade condition across participant groups. For colour memory, a two-way ANOVA revealed a significant main effect of Group but no significant main effect of Condition or Group × Condition interaction. Post-hoc comparisons for colour error showed a similar pattern of overall group differences as observed for location memory (all group comparisons p &lt; 0.001). Error bars represent ±1 standard error of the mean (SEM). Asterisks denote statistical significance from post-hoc comparisons: *: p&lt;0.05, **: p&lt;0.01; ***: p&lt;0.001.</p></caption>
<graphic xlink:href="666983v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Across all groups, saccades selectively disrupted spatial memory—shown by a significant main effect of condition on location error (F(3, 318) = 4.64, p = 0.003)—but not on colour error (F(3, 318) = 0.80, p = 0.49). Most strikingly, the fundamental pattern of saccadic interference seen in healthy controls was fully replicated in the patient cohorts. In other words, the magnitude of this saccadic disruption did not differ between groups. We found no significant interaction between group and condition for either location (F(9, 318) = 0.46, p = 0.90) or colour memory (F(9, 318) = 0.26, p = 0.98). This critical null-interaction indicates that while overall spatial precision is lower in patients, the additional computational cost imposed by a saccade is not disproportionately larger.</p>
<p>The pattern of performance decay from the no-saccade to the two-saccade condition was remarkably consistent across all groups (<xref rid="fig5" ref-type="fig">Figure 5A</xref>), suggesting that the core mechanisms for updating spatial representations across eye movements are surprisingly resilient to the neurodegenerative processes in both PD and AD.</p>
</sec>
<sec id="s2e">
<title>Modelling reveals the sources of spatial memory deficits in AD and PD</title>
<p>To understand the source of the patients’ deficits, we applied the winning ‘Dual (Saccade) + Interference’ model the data from all participants (YC, EC, AD, and PD). By fitting the model to the entire dataset, we obtained estimates of the parameters for each individual, which then formed the basis for our group-level analysis. To formally test for group differences, we used Parametric Empirical Bayes (PEB), a hierarchical Bayesian approach that compares parameter estimates across groups while accounting for the uncertainty of each estimate <sup><xref ref-type="bibr" rid="c26">26</xref></sup>. This allowed us to identify which specific cognitive mechanisms, as formalised by the model parameters, were affected in the patient cohorts.</p>
<p>The analysis revealed a nuanced pattern of deficits, with four parameters differentiating the groups (<xref rid="fig6" ref-type="fig">Figure 6</xref>). The most prominent deficits were observed in memory decay and interference. The Bayesian inversion used here allows us to quantify the posterior mode and variance for each parameter and the covariance for each parameter. From these, we can compute the probabilities that pairs of parameters differ from one another, which we report as P(A&gt;B)—meaning the posterior probability that the parameter for group A was greater than that for group B.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Group differences in winning model parameters.</title>
<p>The plots show group-level parameter estimates from the Parametric Empirical Bayes (PEB) analysis for the winning ‘Dual (Saccade) + Interference’ model. The y-axis represents the posterior mean of the log-scaled parameter, where a more positive value indicates a greater influence of that parameter on performance (i.e., greater error or faster decay). The exception is the interference parameter, which is effectively a logit parameter, meaning the probability of selecting the distractor is obtained by a sigmoid transform. Error bars represent the 90% posterior confidence intervals. Group differences are indicated by an asterisk (*) if the posterior probability of a difference between the respective parameter estimates is greater than 95%, as calculated from their posterior distributions (i.e., if P(A&gt;B)&gt;0.95 or P(B&gt;A)&gt;0.95). The analysis reveals that Angular Encoding, Angular Decay, Radial Decay, and Interference parameters differentiate the groups. Notably, parameters related to saccade processing (Saccade Encoding and Saccade Decay) and Radial Encoding do not differ between groups.</p></caption>
<graphic xlink:href="666983v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><bold>Radial decay</bold> showed a clear, graded impairment: AD patients had a greater decay rate than PD patients (P(AD &gt; PD) = 1.000), who in turn were more impaired than EC (P(PD &gt; EC) = 0.996), and who were more impaired than YC (P(EC &gt; YC) = 0.995). A similar graded pattern was observed for Interference, where AD patients were most susceptible (P(AD &gt; PD) = 0.999), followed by EC (P(EC &gt; YC) = 1.000), while PD and EC groups did not significantly differ (P(PD &gt; EC) = 0.532). Patients with AD also showed a tendency towards greater angular decay than controls (P(AD &gt; YC) = 0.924; P(AD &gt; EC) = 0.772), although this fell below 95% probability. This effect was partly influenced by a surprisingly lower decay rate in the PD group compared to EC (P(PD &lt; EC) = 0.037, indicating P(EC &gt; PD) = 0.963), but not YC (P(PD &lt; YC) = 0.140).</p>
<p>In contrast, group differences in encoding were less pronounced. Only angular encoding error significantly (applying a 95% threshold) differentiated the groups, an effect largely driven by the particularly high precision of the YC group compared to AD (P(YC &lt; AD) = 0.000), PD (P(YC &lt; PD) = 0.019), and EC (P(YC &lt; EC) = 0.002). Among the patient and elderly control groups, AD showed significantly higher angular encoding error than PD (P(AD &gt; PD) = 0.985), but AD and PD did not significantly differ from EC (P(AD &gt; EC) = 0.909; P(PD &gt; EC) = 0.787).</p>
<p>Crucially, and in line with our behavioural findings, parameters related to the saccade itself— saccade encoding and saccade decay—did not differentiate between groups. This modelling result reinforces the conclusion that the core computational process of updating spatial information based on an eye movement is not the primary locus of impairment in PD or AD. Instead, this approach pinpoints specific mechanistic failures—most clearly, a faster decay of radial position information and increased susceptibility to interference—that underlie the characteristic visuospatial deficits in these conditions.</p>
</sec>
<sec id="s2f">
<title>Transsaccadic working memory predicts copying deficits</title>
<p>To assess how the mechanistic parameters derived from the LOCUS task predict real-world visuospatial abilities, we also instructed all participants to complete the Rey-Osterrieth Complex Figure copy task (ROCF; <xref rid="fig7" ref-type="fig"><italic>Figure 7A</italic></xref>) on an Android tablet using a digital pen (see examples in <xref rid="fig7" ref-type="fig"><italic>Figure 7B</italic></xref>; all Copy data are available in the open dataset: <ext-link ext-link-type="uri" xlink:href="https://osf.io/95ecp/">https://osf.io/95ecp/</ext-link>).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Rey-Osterrieth Complex Figure (ROCF) copying performance.</title>
<p>(A) The original figure that participants copied and recalled. (B) Examples of ROCF copy and immediate recall tasks from a young and an elderly healthy participant, both with normal cognitive function (ACE total score &gt; 88). ACE = Addenbrooke’s Cognitive Examination-III.</p></caption>
<graphic xlink:href="666983v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To assess the ecological validity of the identified saccade-updating retinotopic mechanism, we modelled individual ROCF copy scores across all four groups using the estimated (maximum a posteriori) parameters from the winning “Dual (Saccade) + Interference” model (Model 7; <xref rid="fig8" ref-type="fig">Figure 8</xref>) as regressors in a Bayesian linear model. Each regressor was normalised by dividing by the square root of its variance. The model successfully explained a significant proportion of the variance in ROCF copy scores (61.99%), indicating that the identified computational mechanisms are strong predictors of constructional ability (<xref rid="fig8" ref-type="fig">Figure 8A</xref>). The posterior covariance matrix of the regression coefficients associated with each of the model parameters is shown in <xref rid="fig8" ref-type="fig">Figure 8B</xref>.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8:</label>
<caption><title>Effect of model parameters on Rey-Osterrieth Complex Figure copy scores.</title>
<p>How model parameters from the wining “Dual (Saccade) + Interference” model explain variance in ROCF copy performance? (A) Predicted versus empirical scores. A scatter plot of empirical ROCF scores against scores predicted by the fitted linear model. Each point represents an individual participant, coloured by diagnostic group: young controls (YC; light green), elderly controls (EC; dark green), Parkinson’s disease (PD; yellow), and Alzheimer’s disease (AD; purple). A black line denotes the linear fit across all data. ROCF scores were demeaned across participants. Scores outside of 2 standard deviations from the mean were omitted. The plot title indicates the percentage of variance in ROCF copy scores explained by the model. (B) Posterior covariance matrix. A heatmap depicting the posterior covariance between the estimated model parameters (i.e., between the regression coefficients that determine the contribution of each of the parameters from our previous winning model to the ROCF score). Diagonal elements represent individual parameter variances; off-diagonal elements show covariances between parameter pairs. The colour bar provides the scale. (C) Estimated parameter contributions. Bar plot displaying the estimated contribution (beta coefficients) of each LOCUS model parameter to the predicted ROCF copy score. Error bars represent 90% credible intervals (1.65 standard deviations of the posterior variance). Asterisks (∗) denote parameters whose 90% confidence interval does not encompass zero.</p></caption>
<graphic xlink:href="666983v2_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig8" ref-type="fig">Figure 8C</xref> indicates how specific mechanistic parameters during transsaccadic working memory contributed to explaining ROCF copy performance. Although as shown in <xref rid="fig6" ref-type="fig">Figure 6</xref>, the saccade encoding error did not differentiate across these four groups, it does contribute to a worse ROCF copying performance. This highlights the critical role of accurate remapping based on saccadic information; even if the core saccadic update mechanism is preserved across groups (as shown in previous analyses), the precision of this updating process is crucial for complex visuospatial tasks. Moreover, worse ROCF copy performance is associated particularly with higher initial angular encoding error. This indicates that imprecision in the initial registration of angular spatial information contributes to difficulties in accurately reproducing complex visual stimuli.</p>
<p>Surprisingly, <bold>less</bold> angular decay was associated with <bold>poorer</bold> ROCF copy scores. However, a plausible explanation for this could be related to the trend seen in angular encoding error. People who have smaller angular encoding error have more “room” for memory decay before angular encoding error accumulates. Evidence for this explanation is evident in the (weak) negative correlation evident in the covariance matrix in <xref rid="fig8" ref-type="fig">Figure 8</xref> between the coefficients for angular encoding and decay rates. The implication is that the results would be similar if we slightly increased the encoding variance while reducing the decay rate, and vice versa.</p>
<p>Conversely, parameters such as radial encoding error, radial decay, saccade decay and interference did not predict ROCF copy scores. This differential predictive power underscores the specific relevance of angular and saccade-related precision for the demanding task of copying complex figures, which inherently requires continuous and accurate spatial updating across multiple fixations.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>This study introduced a novel paradigm, the LOCUS task (<xref rid="fig1" ref-type="fig"><italic>Figure 1</italic></xref>), and combined this with computational modelling to investigate the underlying mechanisms and sources of potential errors in dynamic spatial working memory, particularly how the brain maintains a stable world-centred frame of reference across saccades. We investigated this across four cohorts: young healthy adults (YC), healthy elderly adults (EC), and patients with and Alzheimer’s disease (AD) and Parkinson’s disease (PD). The two clinical groups are known to are frequently associated with significant disruptions to visuospatial processing and oculomotor control.<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref></sup></p>
<p>The LOCUS task was designed to dissociate retinotopic (within-fixation) and transsaccadic (across-fixation) working memory, examining the precision of both location and colour information maintained across saccades. Our findings reveal a “saccade cost”—a reduction in spatial memory precision following eye movements, consistent with previous research on healthy young adults <sup><xref ref-type="bibr" rid="c20">20</xref></sup>. Unexpectedly, this saccade cost was remarkably consistent across all four populations, demonstrating its stability with aging and neurodegeneration. This finding implies that any observed reduction in transsaccadic performance in older adults and neurodegenerative patients is primarily attributable to lower baseline retinotopic memory rather than an increased cost associated with saccadic remapping.</p>
<p>The conclusion was further supported by our computational modelling. The winning model, “Dual (Saccade) + Interference”, posits an eye-centred, retinotopic representation (encoding stimulus as angular and radial displacement from fixation) that also incorporates saccade-based updating and interference. Although saccade updating was an essential component of the winning model, its two key parameters—initial encoding error and decay rate during maintenance—did not significantly differ across groups. This indicates that the core computational process of updating spatial information based on eye movements is largely preserved in healthy aging and neurodegeneration. Group differences were instead driven by deficits in angular encoding error (precision of initial angle from fixation), angular decay, radial decay (decay in memory of distance from fixation), and interference susceptibility. The eye-centred retinotopic representation is consistent with findings in posterior parietal cortex neurons <sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup> which are considered to play a crucial role in visual localisation as well as remapping across eye movements.<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup> In using this computational approach, we align our work with established practices in computational psychiatry: Our framework employs Variational Laplace, a method used to recover computational phenotypes in clinical populations like those with substance use disorders,<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup> and features a time-dependent parameterisation of variance conceptually similar to the widely-used Hierarchical Gaussian Filter.<sup><xref ref-type="bibr" rid="c32">32</xref>–<xref ref-type="bibr" rid="c35">35</xref></sup></p>
<p>A clear finding was the specificity of the saccade cost to spatial features; it was not observed for non-spatial features like colour, even in neurodegenerative conditions. This discrepancy challenges notions of fixed visual working memory capacity unaffected by saccades.<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c36">36</xref>–<xref ref-type="bibr" rid="c38">38</xref></sup> The differential impact on spatial versus non-spatial features in transsaccadic memory aligns with the established “what” and “where” pathways in visual processing.<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c40">40</xref></sup> For objects to remain unified, object features must be bound to stable representations of location across saccades.<sup><xref ref-type="bibr" rid="c41">41</xref></sup> One possibility is that remapping updates both features and location through a shared mechanism, predicting equal saccadic interference for both colour and location in the present study. However, our findings suggest otherwise. An alternative is that features like colour might be automatically remapped, preserving a coarse representation of the remembered item across saccades.<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c42">42</xref></sup> Our paradigm offers a tool to further investigate transsaccadic working memory and its capacity across different object features.</p>
<p>Furthermore, we demonstrated that a combination of transsaccadic and retinotopic working memory precision predicts drawing performance on the Rey-Osterrieth Complex Figure (ROCF) copying task. Specifically, higher saccade encoding error and angular encoding error significantly contributed to poorer ROCF copy performance. This highlights the crucial role of accurate spatial updating and remapping visual representations across retinotopic coordinates for successful visuomotor integration, such as drawing. This link provides insights into the functional role of spatial remapping in everyday cognition. While spatial remapping is often invoked to explain perceptual stability,<sup><xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c14">14</xref></sup> the necessity of high-resolution transsaccadic memory for basic visual perception is debated.<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c43">43</xref>–<xref ref-type="bibr" rid="c45">45</xref></sup> A prevailing view suggests that detailed internal models are unnecessary for perception, given the continuous availability of visual information in the external world.<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup> Our findings support an alternative perspective, aligning with the proposal that high-resolution transsaccadic memory primarily serves action rather than perception.<sup><xref ref-type="bibr" rid="c13">13</xref></sup> This is consistent with the need for precise localisation in eye-hand coordination tasks such as pointing or grasping.<sup><xref ref-type="bibr" rid="c46">46</xref></sup> Even when unaware of intrasaccadic target displacements, individuals rapidly adjust their reaching movements, suggesting direct access of the motor system to remapping signals.<sup><xref ref-type="bibr" rid="c47">47</xref></sup> Further support comes from evidence that pointing to remembered locations is biased by changes in eye position,<sup><xref ref-type="bibr" rid="c48">48</xref></sup> and that remapping neurons reside within the dorsal “action” visual pathway, rather than the ventral “perception” visual pathway.<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c49">49</xref>,<xref ref-type="bibr" rid="c50">50</xref></sup> By demonstrating a strong link between transsaccadic working memory and drawing (a complex fine motor skill), our findings suggest that precise visual working memory across eye movements plays an important role in complex fine motor control.</p>
<p>The results of this study also provide novel insights into the cognitive processes underlying drawing, specifically highlighting the role of transsaccadic working memory in the ability to copy a drawing. Previous research has primarily focused on the roles of fine motor control and eye-hand coordination in this skill.<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c51">51</xref>–<xref ref-type="bibr" rid="c56">56</xref></sup> This is partly because of consistent failure to find a strong relation between memory and drawing or copying ability.<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c57">57</xref></sup> A recent study highlighted this lack of relationship, by examining ROCF copy performance and various cognitive functions using established neuropsychological battery amongst 142 young children.<sup><xref ref-type="bibr" rid="c57">57</xref></sup> Despite finding a strong correlation between eye-hand coordination and copying performance, common measures of working memory, such as digit span and Corsi block tasks, did not directly predict copying performance. This seemingly counterintuitive finding suggests that the capacity to maintain information about digits or spatial positions in a sequence does not significantly explain the variance in drawing ability.</p>
<p>This aligns with the seminal study on 3D-block-copying, where participants were found to rarely memorise the entire model.<sup><xref ref-type="bibr" rid="c4">4</xref></sup> Instead, they focused on information around the block they were actively working on, relying on frequent eye and hand movements to complete the copy. This strategy minimizes the load on traditional working memory but increases the demand for precise eye-hand coordination, a pattern also observed in art students copying complex lines.<sup><xref ref-type="bibr" rid="c58">58</xref></sup> A commonly accepted explanation for this strategy is that, essentially, when a model is constantly available “out there,” there is no need to construct and maintain a detailed internal representation in memory. Nevertheless, the findings presented here indicate that even with the visual information readily available, drawing still might depend on working memory <italic>across saccades</italic>. In general, such a proposal is consistent with the idea that individuals adapt their gaze and memory strategies based on task demands.<sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup> The results of the studies presented here suggest that there is a specific contribution of working memory to the complex skill of copying a drawing. This may have important implications for understanding how drawing skills are acquired and develop, and for investigating mechanisms underlying drawing deficits observed in clinical populations, e.g. with constructional apraxia, characterised by impaired drawing and copying abilities.<sup><xref ref-type="bibr" rid="c60">60</xref>,<xref ref-type="bibr" rid="c61">61</xref></sup></p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a" sec-type="ethics-statement">
<title>Ethics</title>
<p>The study was performed in accordance with the ethical standards as laid down in the 1964 Declaration of Helsinki and its later amendments. Ethical approval was granted by the University of Oxford ethics committee (IRAS ID: 248379, Ethics Approval Reference: 18/SC/0448). All participants gave written informed consent prior to the start of the study.</p>
</sec>
<sec id="s4b">
<title>Participants</title>
<p>A total of 42 healthy volunteers (21 young adults, mean age = 24.1 years; 21 older adults, mean age = 72.4 years) participated in this study. Their demographics are shown in <xref rid="tbl1" ref-type="table">Table 1</xref>. All participants were recruited locally in Oxford, UK, and none were professional artists. Young participants were recruited through the online system of the Department of Experimental Psychology at the University of Oxford. Older adults were recruited from a pool of volunteers registered with the Oxford Dementia and Ageing Research (OxDARE). None of the participants had a psychiatric or neurological illness or was on psychoactive medications. Older adult participants were all over 50 years of age. All participants completed the Addenbrooke’s Cognitive Examination-III (ACE) to assess their overall cognitive function <sup><xref ref-type="bibr" rid="c62">62</xref></sup>. Except for one elderly participant who scored 85, all participants scored above the standard normal cut-off 88.</p>
</sec>
<sec id="s4c">
<title>Experimental Procedure</title>
<p>The LOCUS task (LOcation and COlour Updating across Saccades, <xref rid="fig1" ref-type="fig"><italic>Figure 1</italic></xref>) assessed visuospatial working memory across eye movements. Participants viewed two sequentially presented coloured squares (1-second duration each, with a 0.5-second inter-stimulus interval), memorising their location and colour within frames cantered on a fixation cross. A labelled colour wheel then cued recall of a specific square. Participants selected the remembered colour and dragged a corresponding square to its original screen location.</p>
<p>Saccade demands were manipulated by varying the location of the second frame and colour wheel. In 50% of trials, the second frame appeared in the same location as the first (no saccade required), while in the other 50%, it appeared in a different location, requiring a saccade of at least 8.5° (degrees of visual angle). The colour wheel also appeared in the same or a different location as the second frame with equal probability. This creates four conditions: No-saccade, Saccade-After-Item 1, Saccade-After-Item-2, or Two-Saccades (<xref rid="fig1" ref-type="fig"><italic>Figure 1</italic></xref>). Each condition comprised 25% of the 160 trials, presented in 16 randomised blocks.</p>
<p>The coloured square was 0.66° × 0.66° visual angle, the fixation cross was 1° × 1° visual angle, and the frame was 9.8° × 9.8° visual angle with a 0.1° linewidth. The location of the square was randomised within the frame with a fixed minimal distance from fixation cross of 3.82° visual angle. The screen had 3 x 5 possible frame locations with a 0.3° gap between frames. The colour wheel was 4.5° wide, with a 1.5°-wide inner black circle displaying a 1° tall label (probing “1” or “2”). The colour and location of squares were generated pseudorandomly for each participant just before the starting experiment. To avoid the potential bias to certain locations on the colour wheel, the colour wheel’s rotation was randomised for every trial.</p>
<p>Participants were instructed to maintain fixation on a cross when it was present, and to execute saccades to peripheral targets when prompted by appearance of a new frame at a different location or the appearance of the colour wheel. When item 1 or 2 was displayed, it was positioned within a square frame with a central fixation cross (<xref rid="fig2" ref-type="fig"><italic>Figure 2</italic></xref>). The use of a square frame and central fixation cross served to emphasise the retinotopic reference frame, highlighting no need to move the eyes unless these elements were extinguished, and a new stimulus appeared in the periphery. Thus, in the no-saccade condition, participants did not need to make eye movements and therefore respond without having to rely on transsaccadic memory.</p>
<p>To mitigate potential confounds related to visual impairment or inability to fixate with task instructions, we monitored eye position throughout the experiment. Eye-tracking analysis confirmed high compliance: participants fixated the cross in the majority of trials (81% ± 10%), with no significant difference between age groups (t(40) = 0.76, <italic>p</italic> = 0.45). Fixation durations averaged 0.21s (± 0.05s), sufficient for encoding visual information. Furthermore, in the no-saccade condition, participants successfully maintained fixation in 80% of trials (± 10%). Similarly, in the saccade conditions, participants accurately executed saccades away when required on 86% of trials (± 18%). Non-compliance trials were excluded from further analysis.</p>
</sec>
<sec id="s4d">
<title>Measurement of Saccade Cost</title>
<p>The LOCUS task allows for the quantification of several key metrics: (1) retinotopic working memory: the location error in no-saccade condition, (2) transsaccadic working memory: the location error in two-saccades condition, and (3) “saccade cost”: the reduction in spatial memory accuracy attributable to saccades. Whie a straightforward approach to quantify saccade cost would be to directly compare performance in the two-saccades condition to the no-saccade condition, this method confounds saccade cost with retinotopic working memory. To isolate the effect of saccades on memory independent of retinotopic condition, we calculated saccade cost as the difference in performance between the Two-Saccade condition and the Saccade-After-The-Other-Item condition. This yields a specific measure of the additional memory error due to a saccade made immediately after the probed item.</p>
<p>Furthermore, we conducted a secondary analysis using the beta coefficient derived from a multiple linear regression model to estimate the effect of saccades and item order on location errors. This approach confirmed the relationship between saccade cost and ROCF copy performance, demonstrating the robustness of our findings.</p>
</sec>
<sec id="s4e">
<title>Rey-Osterrieth Complex Figure (ROCF)</title>
<p>Participants completed the ROCF copy and immediate recall tasks on a Galaxy Tab S3 tablet (SM-T820) using the corresponding digital pen (Galaxy Tab S3 S Pen). The tablet’s screen dimensions were 148 x 197 mm (1536 x 2048 pixels, 264 ppi density). Participants placed the tablet on the table upright. During the copy phase, the ROCF figure displayed on the top of the tablet <sup><xref ref-type="bibr" rid="c63">63</xref></sup>. They first copied the figure in the lower portion of the screen. Upon completion, they were instructed to reproduce the ROCF from memory, without the figure present. Drawing traces were recorded and printed for blind scoring by two experienced raters using the standard 18-element scoring system.<sup><xref ref-type="bibr" rid="c64">64</xref></sup></p>
<p>For further analysis, the drawing paths were analysed in MATLAB using in-house scripts. We extracted the total time taken to complete each task, calculated from the first moment the digital pen touched the screen to the final pen lift. In addition, we extracted total distance drawn (total path length in pixels), path drawing speed (mean derivative of path distance per second), and mean waiting time (duration between completing the last path and starting the next path).</p>
</sec>
<sec id="s4f">
<title>Methods – Computational Modelling</title>
<p>We used of series of simple computational models to test a set of hypotheses about the mechanisms that underwrite the measured data. All hypotheses rely upon the idea that memory decays with time since the stimulus was presented, modelled here using a diffusion (or Brownian motion) like process. Under this formulation, the shape of the distribution of expected responses, and its change over time, will depend upon the way in which our brains represent the variables that we expect to decay. Conceptually, our hypotheses distinguished between allocentric (screen centred) coordinates and retinotopic coordinates, whether target and distractor representations do or do not interfere with one another, whether there was evidence for an embodied representation in which the current position of the eyes helps reinforce a memory and, if so, whether remembered locations are centred on remembered fixations or on relative position based upon memories of the vectors of saccades performed since that location.</p>
<p><xref rid="fig9" ref-type="fig">Figure 9</xref> shows the form of these hypotheses, each of which can be expressed with a time-dependent likelihood function. Each of these models was fit to the responses made by each participant, where the response data were rotated and translated such that the target stimulus was always in the same angle, and the origin was always set to be the fixation location associated with the probed stimulus. The model fits were performed using a Variational Laplace procedure (a form of approximate Bayesian inference) using a Newton optimisation scheme as implemented in SPM25 using the spm_nlsi_Newton.m MATLAB routine.<sup><xref ref-type="bibr" rid="c65">65</xref>,<xref ref-type="bibr" rid="c66">66</xref></sup> This relies upon a specification of a log likelihood function which gives the distribution, given the parameters, of the recorded responses. In the allocentric model, this log likelihood function had the form:
<disp-formula id="disp-eqn-1">
<graphic xlink:href="666983v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9:</label>
<caption><title>Models and hypotheses.</title>
<p>This figure illustrates the form of the models used for our model comparisons. The upper panel shows the distinction between allocentric screen centred coordinates, which we treat as a decay in Euclidean space, such that the probability density of a response (coordinates (u,v)) is a normal distribution whose covariance grows with time centred on the stimulus. In contrast, the retinotopic representation uses polar coordinates with both radial and angular decays. There are several ways in which the associated covariance can be formulated, and this depends upon the different times over which the decay in memory might occur. The different timings in the alternative experimental conditions are shown schematically in the middle panel, dealing with the time since the stimulus was visible (τ), the time since the first saccade (if any) since the stimulus was present (t), and the time since the second saccade (if any) after the stimulus (T). The variations, based on these timings, for the dynamics of the covariance for the retinotopic model, are shown in the lower panel. The simple model assumes that any decay in the fixation location around which the retinotopic coordinates can be captured by decays in the radial and angular coordinates. This implies there is no difference in the form of the decay when the same fixation location is sustained after the stimulus disappears compared to when a subsequent saccade is made. The fixation-dependent model assumes instead that there is an independent decay in the fixation location, which only begins once we have moved our eyes from the location at which we saw the stimulus. In contrast, the saccade dependent framing assumes that, rather than retain a memory of fixation location, we reconstruct the fixation location based upon our memory of the saccade vectors made, since that fixation. Here, the n variable represents the number of saccades made since the relevant fixation. The interference effects are mediated by computing the distribution we would expect from a response to the target stimulus and the distribution we would expect from a response to the distractor stimulus (translated into the same retinotopic space) and taking a weighted sum of the two.</p></caption>
<graphic xlink:href="666983v2_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Here, (<italic>u,v</italic>) are the Euclidean coordinates of the response to a stimulus presented at coordinates (<italic>x</italic>,<italic>y</italic>), with an encoding variance of κ<sub>2</sub> and a decay constant of κ<sub>1</sub> which scales with the time τ between the stimulus presentation and the response. This is summed over all responses a participant has made accounting for the time at which they are made to give the accumulated log likelihood for the parameters for that dataset.</p>
<p>The dual models, with a retinotopic component, all have the form:
<disp-formula id="disp-eqn-2">
<graphic xlink:href="666983v2_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, the principle is similar, but the distributions are over the polar coordinates of the response, with (<italic>r</italic>,θ) representing the polar coordinates of the stimulus. The variances, Σ, are functions now of the parameter set and (up to) three different time variables. These different times are illustrated schematically in Figure X (middle panel) and are disambiguated by the different experimental conditions. They are:
<list list-type="order">
<list-item><p>the time since the stimulus was visible (τ)</p></list-item>
<list-item><p>the time since the first saccade (if any) since the stimulus was present (<italic>t</italic>)</p></list-item>
<list-item><p>the time since the second saccade (if any) after the stimulus (<italic>T</italic>).</p></list-item>
</list>
The forms of the variance functions are shown in Figure X. Here we see a combination of encoding parameters (time-independent), and decay parameters for retinotopic stimulus memories (multiplied by τ), fixation locations (multiplied by <italic>t</italic>) and saccade vectors (multiplied by <italic>t</italic> + <italic>T</italic>). The reason for multiplying saccade decay constants by the sum of <italic>t</italic> and <italic>T</italic> is that a reconstructed fixation location based upon saccade vectors will have a cumulative increase in uncertainty with each saccade since that fixation.</p>
<p>Finally, the likelihood for the interference models is the weighted sum of two likelihood (<italic>e</italic><sup>L(K)</sup>) functions of the form above, but with one computed from the target location and the other from the distractor location. The interference parameter determined the weighting towards the distractor.</p>
<p>We used relatively conservative priors, which were (for all parameters) normal distributions centred on zero, with a spherical covariance of 1/8. This ensured we needed good evidence for the parameters to be estimated as being confidently non-zero. All parameters were log scaling parameters (ensuring the scaling parameters were positive) except for the interference parameter, when present, which was passed through a sigmoid function to constrain the effect of the parameter to lie between zero and one.</p>
<p>Variational Laplace provides approximate Bayesian model evidence values, and posterior probability distributions for each parameter, for each model and participant. By accumulating the log model evidence across all participants and applying a softmax function, we arrived at posterior probabilities for each model. We selected the winning model for subsequent steps of the analysis. Our next stage was to use Parametric Empirical Bayes (PEB)<sup><xref ref-type="bibr" rid="c67">67</xref></sup> to assess the effect of diagnostic group on the individual parameter estimates. PEB is effectively a second level linear model. We used the SPM25 implementation spm_dcm_peb.m.<sup><xref ref-type="bibr" rid="c65">65</xref></sup></p>
<p>Finally, to test hypotheses about the mechanisms that explain performance on complex figure drawing, we took the maximum a posteriori estimates (MAP) for all parameters in the winning model for all participants and used these, normalised by their variance, as regressors for a linear model to predict mean-centred copy scores (omitting those scores &gt; 2 SD from the mean) and changes in score on immediate recall. Again, we used spm_nlsi_Newton.m to invert this linear model. Following inversion, we computed the predicted scores based upon the MAP values of the regression coefficients and assessed the percentage of empirical variance explained. The percentage of empirical variance explained by the model was assessed using the formula: 100×(1− var(residuals)/var(empirical_scores)).</p>
</sec>
<sec id="s4g">
<title>Statistical Analysis</title>
<p>All statistical analyses were conducted in MATLAB R2025a and JASP.<sup><xref ref-type="bibr" rid="c68">68</xref></sup> For correlations, Spearman’s rho was reported with two-tailed p-values (alpha = 0.05) and effect size (Fisher’s z). Bayesian comparisons were also performed, with Bayes Factor 10 (BF<sub>10</sub>) reported when applicable. For ANOVAs, Mauchly’s test of sphericity was applied, and Greenhouse-Geisser correction was used when sphericity was violated. F-stats, p values, and partial eta-squared were reported. Note that sphericity correction is not available for factors with two levels, such as the item factor (only two items). For multiple comparisons, Bonferroni-corrected p-values (<italic>p</italic>(bonf)) were reported.</p>
<p>Multiple linear regression models were fit for each participant using all trials. The regression analyses were conducted via the MATLAB function <italic>regress</italic>. The resulting beta coefficients were then averaged across participants. A one-sample t-test (two-tailed) was used to determine if the group average beta coefficient for each predictor was significantly different from zero. The t-statistic and p-value (two-tailed) are reported for each predictor.</p>
<p>Mediation analysis with multiple mediators was conducted in JASP, using standardised regression coefficients <sup><xref ref-type="bibr" rid="c68">68</xref></sup>. The bias-corrected bootstrap method with 1000 iterations was used to estimate confidence intervals for the indirect and direct effects.<sup><xref ref-type="bibr" rid="c69">69</xref></sup></p>
<p>The bayes factor was computed as the division of accumulated posterior probability between the winning model and the next best alternative.</p>
<p>For comparing group differences in the estimated parameters of the winning model, a parametric empirical Bayes (PEB) approach was utilised, employing spm_dcm_peb in SPM25. This hierarchical Bayesian method allows for robust group-level inference by modelling individual parameter estimates as samples drawn from a group distribution, thereby accounting for inter-subject variability and propagating uncertainty from the first (individual) level. For each parameter of interest, the group-level posterior mean and covariance were obtained from the PEB analysis. To assess statistically significant differences between groups for a given parameter, the probability of a directional difference between the posterior estimates of any two groups was calculated using their respective posterior distributions. A difference was deemed statistically significant if this calculated probability was greater than 0.95 (corresponding to strong evidence in favour of a directional difference).</p>
</sec>
<sec id="s4h">
<title>Eyetracking Recording</title>
<p>Participants sat in front of a 21” CRT computer screen (1024L×L768 pixels; 100 Hz refresh rate) at a viewing distance of 60 cm in a dimly lit quiet room with their head supported on a chinrest. Visual stimuli were presented using MATLAB (The MathWorks) and Psychophysics Toolbox <sup><xref ref-type="bibr" rid="c70">70</xref>,<xref ref-type="bibr" rid="c71">71</xref></sup> on a Windows-XP computer. Eye positions during the LOCUS task were monitored using a frame-mounted infrared eye-tracking camera (Eyelink 1000 Tower Mounted, SR Research Ltd.). Right eye was recorded at a sampling rate of 1000 Hz. A nine-point calibration procedure for the Eyelink system was conducted prior the experiment.</p>
</sec>
<sec id="s4i">
<title>Eyetracking Data Analysis</title>
<p>All eyertracking analysis was performed offline. Interval where the eye tracker detected full and partial eye closures were automatically treated as missing data. Furthermore, intervals where pupil diameter was recorded as 0 or change dramatically immediately pre- or post-blink were also automatically treated as blinks and removed. Gaze positions were epoched relative to stimulus onset (item 1, item 2 and colour wheel) and analysed for 1 second post-stimulus-onset. Euclidean distance that the gaze travelled to the target was computed and converted to visual degrees. Fixations were defined as gaze remaining within 2.5° of the target for at least 150 ms. Saccades were defined as eye movements exceeding 150° visual angle per second, lasting longer than 10 ms in duration, and traversing a distance greater than 9.2° from the initial fixation. The distance of 9.2° was defined as it is the radius of perifoveal visual field <sup><xref ref-type="bibr" rid="c72">72</xref>,<xref ref-type="bibr" rid="c73">73</xref></sup>.</p>
<p>Task compliance was assessed based on fixation and saccade criteria specific to each condition. For example, in the Saccade-After-Item-1 condition, compliance required fixations on both Item 1 and Item 2, with no saccade exceeding 9.2° after Item 2 measured in the epoch after the onset of colour wheel.</p>
</sec>
</sec>

</body>
<back>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>All experimental data and healthy participants' Rey-Osterrieth Complex Figure drawings are publicly available on the Open Science Framework (OSF) at <ext-link ext-link-type="uri" xlink:href="https://osf.io/95ecp/">https://osf.io/95ecp/</ext-link>. All scripts used for the computational modelling are included in the same repository.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This research was supported by funding from the Wellcome Trust and National Institute of Health and Care Research (NIHR) Oxford Health Biomedical Research Centre (BRC). S.Z., R.U., V.K., G.D.J., A.S., S.T. and M.H. were funded by the Wellcome Trust (206330/Z/17/Z and 226645/Z/22/Z). S.G.M. was funded by a Medical Research Council (MRC) Clinician Scientist Fellowship (MR/P00878/X), NIHR BRC and NIHR Oxford Health BRC. T.P. is supported by an NIHR Academic Clinical Fellowship (ref: ACF-2023-13-013).</p>
</ack>
<sec id="additional-information" sec-type="additional-information">
<title>Addtional information</title>
<sec id="ai1">
<title>Author Contributions</title>
<p><bold>S.Z.</bold>: Conceptualization, Methodology, Formal analysis, Data Curation, Writing - Original Draft, Writing – Review &amp; Editing, Visualization, Project administration. <bold>T.P.</bold>: Methodology, Formal analysis, Writing - Original Draft, Writing – Review &amp; Editing, Visualization. <bold>R.U.</bold>: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Resources, Data Curation, Project administration. <bold>V.K.:</bold> Methodology, Resources, Investigation, Data Curation, Writing - Review &amp; Editing, Project administration. <bold>G.D.J.</bold>: Resources, Data Curation. <bold>A.S.</bold>: Data Curation. <bold>S.T.</bold>: Conceptualization, Resources, Data Curation, Writing - Review &amp; Editing. <bold>S.M.</bold>: Conceptualization, Methodology, Writing – Review &amp; Editing, Supervision. <bold>M.H.</bold>: Conceptualization, Methodology, Resources, Writing – Review &amp; Editing, Project administration, Supervision, Funding acquisition.</p>
</sec>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplementary Material</label>
<media xlink:href="supplements/666983_file02.docx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Land</surname>, <given-names>M. F.</given-names></string-name> &amp; <string-name><surname>Hayhoe</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>In what ways do eye movements contribute to everyday activities?</article-title> <source>Vision Research</source> <volume>41</volume>, <fpage>3559</fpage>–<lpage>3565</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crawford</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Medendorp</surname>, <given-names>W. P.</given-names></string-name> &amp; <string-name><surname>Marotta</surname>, <given-names>J. J</given-names></string-name></person-group>. <article-title>Spatial Transformations for Eye–Hand Coordination</article-title>. <source>Journal of Neurophysiology</source> <volume>92</volume>, <fpage>10</fpage>–<lpage>19</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Draschkow</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kallmayer</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Nobre</surname>, <given-names>A. C</given-names></string-name></person-group>. <article-title>When Natural Behavior Engages Working Memory</article-title>. <source>Current Biology</source> <volume>31</volume>, <fpage>869</fpage>–<lpage>874.e5</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ballard</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Hayhoe</surname>, <given-names>M. M.</given-names></string-name> &amp; <string-name><surname>Pelz</surname>, <given-names>J. B</given-names></string-name></person-group>. <article-title>Memory Representations in Natural Tasks</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>7</volume>, <fpage>66</fpage>–<lpage>80</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>D. J</given-names></string-name></person-group>. <article-title>Look little, look often: The influence of gaze frequency on drawing accuracy</article-title>. <source>Perception &amp; Psychophysics</source> <volume>67</volume>, <fpage>997</fpage>–<lpage>1009</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McManus</surname>, <given-names>I. C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Art students who cannot draw: Exploring the relations between drawing ability, visual memory, accuracy of copying, and dyslexia</article-title>. <source>Psychology of Aesthetics, Creativity, and the Arts</source> <volume>4</volume>, <fpage>18</fpage>–<lpage>30</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perdreau</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Cavanagh</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Drawing experts have better visual memory while drawing</article-title>. <source>Journal of Vision</source> <volume>15</volume>, <issue>5</issue> (<year>2015</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grzeczkowski</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Shi</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Rolfs</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Deubel</surname>, <given-names>H</given-names></string-name></person-group>. <article-title>Perceptual learning across saccades: Feature but not location specific</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>120</volume>, <fpage>e2303763120</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duhamel</surname>, <given-names>J.-R.</given-names></string-name>, <string-name><surname>Colby</surname>, <given-names>C. L.</given-names></string-name> &amp; <string-name><surname>Goldberg</surname>, <given-names>M. E</given-names></string-name></person-group>. <article-title>The Updating of the Representation of Visual Space in Parietal Cortex by Intended Eye Movements</article-title>. <source>Science</source> <volume>255</volume>, <fpage>90</fpage>–<lpage>92</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Colby</surname>, <given-names>C. L.</given-names></string-name> &amp; <string-name><surname>Goldberg</surname>, <given-names>M. E.</given-names></string-name></person-group> <article-title>Space And Attention In Parietal Cortex</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>22</volume>, <fpage>319</fpage>–<lpage>349</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Golomb</surname>, <given-names>J. D.</given-names></string-name> &amp; <string-name><surname>Mazer</surname>, <given-names>J. A.</given-names></string-name></person-group> <article-title>Visual Remapping</article-title>. <source>Annual Review of Vision Science</source> <volume>7</volume>, <fpage>257</fpage>–<lpage>277</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Melcher</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Colby</surname>, <given-names>C. L.</given-names></string-name></person-group> <article-title>Trans-saccadic perception</article-title>. <source>Trends in Cognitive Sciences</source> <volume>12</volume>, <fpage>466</fpage>–<lpage>473</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bays</surname>, <given-names>P. M.</given-names></string-name> &amp; <string-name><surname>Husain</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Spatial remapping of the visual world across saccades</article-title>. <source>NeuroReport</source> <volume>18</volume>, <fpage>1207</fpage>–<lpage>1213</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harrison</surname>, <given-names>W. J.</given-names></string-name>, <string-name><surname>Stead</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Wallis</surname>, <given-names>T. S. A.</given-names></string-name>, <string-name><surname>Bex</surname>, <given-names>P. J.</given-names></string-name> &amp; <string-name><surname>Mattingley</surname>, <given-names>J. B</given-names></string-name></person-group>. <article-title>A computational account of transsaccadic attentional allocation based on visual gain fields</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>121</volume>, <fpage>e2316608121</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irwin</surname>, <given-names>D. E</given-names></string-name></person-group>. <article-title>Memory for position and identity across eye movements</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>18</volume>, <fpage>307</fpage>–<lpage>317</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Golomb</surname>, <given-names>J. D</given-names></string-name></person-group>. <article-title>Remapping locations and features across saccades: a dual-spotlight theory of attentional updating</article-title>. <source>Current Opinion in Psychology</source> <volume>29</volume>, <fpage>211</fpage>–<lpage>218</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneegans</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Bays</surname>, <given-names>P. M</given-names></string-name></person-group>. <article-title>Drift in Neural Population Activity Causes Working Memory to Deteriorate Over Time</article-title>. <source>J Neurosci</source> <volume>38</volume>, <fpage>4859</fpage>–<lpage>4869</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cormack</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Aarsland</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ballard</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Tovée</surname>, <given-names>M. J</given-names></string-name></person-group>. <article-title>Pentagon drawing and neuropsychological performance in Dementia with Lewy Bodies, Alzheimer’s disease, Parkinson’s disease and Parkinson’s disease with dementia</article-title>. <source>Int J Geriatr Psychiatry</source> <volume>19</volume>, <fpage>371</fpage>–<lpage>377</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trojano</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Gainotti</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>Drawing Disorders in Alzheimer’s Disease and Other Forms of Dementia</article-title>. <source>J Alzheimers Dis</source> <volume>53</volume>, <fpage>31</fpage>–<lpage>52</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Golomb</surname>, <given-names>J. D.</given-names></string-name> &amp; <string-name><surname>Kanwisher</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>Retinotopic memory is more precise than spatiotopic memory</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>109</volume>, <fpage>1796</fpage>–<lpage>1801</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Broadbent</surname>, <given-names>D. E.</given-names></string-name> &amp; <string-name><surname>Broadbent</surname>, <given-names>M. H. P</given-names></string-name></person-group>. <article-title>Recency effects in visual memory</article-title>. <source>The Quarterly Journal of Experimental Psychology Section A</source> <volume>33</volume>, <fpage>1</fpage>–<lpage>15</lpage> (<year>1981</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murdock</surname>, <given-names>B. B</given-names></string-name></person-group>. <article-title>The serial position effect of free recall</article-title>. <source>Journal of Experimental Psychology</source> <volume>64</volume>, <fpage>482</fpage>–<lpage>488</lpage> (<year>1962</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Penny</surname>, <given-names>W. D</given-names></string-name></person-group>. <article-title>Comparing Dynamic Causal Models using AIC, BIC and Free Energy</article-title>. <source>Neuroimage</source> <volume>59</volume>, <fpage>319</fpage>–<lpage>330</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rasmussen</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Ghahramani</surname>, <given-names>Z. Occam’ s Razor</given-names></string-name></person-group>. In <source>Advances in Neural Information Processing Systems</source> vol. <volume>13</volume> (<publisher-name>MIT Press</publisher-name>, <year>2000</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>MacKay</surname>, <given-names>D. J. C.</given-names></string-name></person-group> <source>Information Theory, Inference and Learning Algorithms</source>. (<publisher-name>Cambridge University Press</publisher-name>, <year>2003</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeidman</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A guide to group effective connectivity analysis, part 2: Second level analysis with PEB</article-title>. <source>Neuroimage</source> <volume>200</volume>, <fpage>12</fpage>–<lpage>25</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>Y. E.</given-names></string-name> &amp; <string-name><surname>Andersen</surname>, <given-names>R. A</given-names></string-name></person-group>. <article-title>A common reference frame for movement plans in the posterior parietal cortex</article-title>. <source>Nat Rev Neurosci</source> <volume>3</volume>, <fpage>553</fpage>–<lpage>562</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Land</surname>, <given-names>M. F</given-names></string-name></person-group>. <article-title>The Operation of the Visual System in Relation to Action</article-title>. <source>Current Biology</source> <volume>22</volume>, <fpage>R811</fpage>–<lpage>R817</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ten Brink</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Fabius</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Weaver</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Nijboer</surname>, <given-names>T. C. W.</given-names></string-name> &amp; <string-name><surname>Van der Stigchel</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Trans-saccadic memory after right parietal brain damage</article-title>. <source>Cortex</source> <volume>120</volume>, <fpage>284</fpage>–<lpage>297</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Elevated decision uncertainty and reduced avoidance drives in depression, anxiety and substance use disorders during approach–avoidance conflict: a replication study</article-title>. <source>Journal of Psychiatry and Neuroscience</source> <volume>48</volume>, <fpage>E217</fpage>–<lpage>E231</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taylor</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Active learning impairments in substance use disorders when resolving the explore-exploit dilemma: A replication and extension of previous computational modeling results</article-title>. <source>Drug and Alcohol Dependence</source> <volume>252</volume>, <issue>110945</issue> (<year>2023</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathys</surname>, <given-names>C. D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Uncertainty in perception and the Hierarchical Gaussian Filter</article-title>. <source>Front Hum Neurosci</source> <volume>8</volume>, (<year>2014</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hess</surname>, <given-names>A. J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Bayesian Workflow for Generative Modeling in Computational Psychiatry</article-title>. <source>Comput Psychiatr</source> <volume>9</volume>, <fpage>76</fpage>–<lpage>99</lpage> (<year>2025</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marshall</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Pharmacological Fingerprints of Contextual Uncertainty</article-title>. <source>PLOS Biology</source> <volume>14</volume>, <fpage>e1002575</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kirsch</surname>, <given-names>L. P.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Updating beliefs beyond the here-and-now: the counter-factual self in anosognosia for hemiplegia</article-title>. <source>Brain Communications</source> <volume>3</volume>, (<year>2021</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prime</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Vesia</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Crawford</surname>, <given-names>J. D</given-names></string-name></person-group>. <article-title>Cortical mechanisms for trans-saccadic memory and integration of multiple object features</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> (<year>2011</year>) doi:<pub-id pub-id-type="doi">10.1098/rstb.2010.0184</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prime</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Tsotsos</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Keith</surname>, <given-names>G. P.</given-names></string-name> &amp; <string-name><surname>Crawford</surname>, <given-names>J. D</given-names></string-name></person-group>. <article-title>Visual memory capacity in transsaccadic integration</article-title>. <source>Exp Brain Res</source> <volume>180</volume>, <fpage>609</fpage>–<lpage>628</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irwin</surname>, <given-names>D. E.</given-names></string-name> &amp; <string-name><surname>Gordon</surname>, <given-names>R. D.</given-names></string-name></person-group> <article-title>Eye Movements, Attention and Trans-saccadic Memory</article-title>. <source>Visual Cognition</source> <volume>5</volume>, <fpage>127</fpage>–<lpage>155</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name> &amp; <string-name><surname>Mishkin</surname>, <given-names>M.</given-names></string-name></person-group> <chapter-title>Two cortical visual systems</chapter-title>. In <source>Analysis of visual behavior</source> <fpage>549</fpage>–<lpage>586</lpage> (<publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA, USA</publisher-loc>, <year>1982</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mishkin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name> &amp; <string-name><surname>Macko</surname>, <given-names>K. A</given-names></string-name></person-group>. <article-title>Object vision and spatial vision: two cortical pathways</article-title>. <source>Trends in Neurosciences</source> <volume>6</volume>, <fpage>414</fpage>–<lpage>417</lpage> (<year>1983</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cavanagh</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hunt</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Afraz</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Rolfs</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Visual stability based on remapping of attention pointers</article-title>. <source>Trends in Cognitive Sciences</source> <volume>14</volume>, <fpage>147</fpage>–<lpage>153</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fabius</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Fracasso</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Acunzo</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Stigchel</surname>, <given-names>S. V. der</given-names></string-name> &amp; <string-name><surname>Melcher</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Low-Level Visual Information Is Maintained across Saccades, Allowing for a Postsaccadic Handoff between Visual Areas</article-title>. <source>J. Neurosci</source>. <volume>40</volume>, <fpage>9476</fpage>–<lpage>9486</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irwin</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Yantis</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Jonides</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Evidence against visual integration across saccadic eye movements</article-title>. <source>Perception &amp; Psychophysics</source> <volume>34</volume>, <fpage>49</fpage>–<lpage>57</lpage> (<year>1983</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irwin</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>J. S.</given-names></string-name> &amp; <string-name><surname>Sun</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Visual masking and visual integration across saccadic eye movements</article-title>. <source>Journal of Experimental Psychology: General</source> <volume>117</volume>, <fpage>276</fpage>–<lpage>287</lpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rayner</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>McConkie</surname>, <given-names>G. W.</given-names></string-name> &amp; <string-name><surname>Zola</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>Integrating information across eye movements</article-title>. <source>Cognitive Psychology</source> <volume>12</volume>, <fpage>206</fpage>–<lpage>226</lpage> (<year>1980</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baltaretu</surname>, <given-names>B. R.</given-names></string-name>, <string-name><surname>Monaco</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Velji-Ibrahim</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Luabeya</surname>, <given-names>G. N.</given-names></string-name> &amp; <string-name><surname>Crawford</surname>, <given-names>J. D</given-names></string-name></person-group>. <article-title>Parietal Cortex Integrates Saccade and Object Orientation Signals to Update Grasp Plans</article-title>. <source>The Journal of Neuroscience</source> <volume>40</volume>, (<year>2020</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prablanc</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Martin</surname>, <given-names>O</given-names></string-name></person-group>. <article-title>Automatic control during hand reaching at undetected two-dimensional target displacements</article-title>. <source>J Neurophysiol</source> <volume>67</volume>, <fpage>455</fpage>–<lpage>469</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henriques</surname>, <given-names>D. Y. P.</given-names></string-name>, <string-name><surname>Klier</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Lowy</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Crawford</surname>, <given-names>J. D</given-names></string-name></person-group>. <article-title>Gaze-Centered Remapping of Remembered Visual Space in an Open-Loop Pointing Task</article-title>. <source>J. Neurosci</source>. <volume>18</volume>, <fpage>1583</fpage>–<lpage>1594</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Husain</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Nachev</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Space and the parietal cortex</article-title>. <source>Trends in Cognitive Sciences</source> <volume>11</volume>, <fpage>30</fpage>–<lpage>36</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodale</surname>, <given-names>M. A.</given-names></string-name> &amp; <string-name><surname>Westwood</surname>, <given-names>D. A</given-names></string-name></person-group>. <article-title>An evolving view of duplex vision: separate but interacting cortical pathways for perception and action</article-title>. <source>Current Opinion in Neurobiology</source> <volume>14</volume>, <fpage>203</fpage>–<lpage>211</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Bravi</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Minciacchi</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>Assessing the Development of Fine Motor Control in Elementary School Children Using Drawing and Tracing Tasks</article-title>. <source>Percept Mot Skills</source> <volume>128</volume>, <fpage>605</fpage>–<lpage>624</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raimo</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Santangelo</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Trojano</surname>, <given-names>L</given-names></string-name></person-group>. <article-title>The Neural Bases of Drawing. A Meta-analysis and a Systematic Literature Review of Neurofunctional Studies in Healthy Individuals</article-title>. <source>Neuropsychol Rev</source> <volume>31</volume>, <fpage>689</fpage>–<lpage>702</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trojano</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Grossi</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Flash</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Cognitive neuroscience of drawing: Contributions of neuropsychological, experimental and neurofunctional studies</article-title>. <source>Cortex</source> <volume>45</volume>, <fpage>269</fpage>–<lpage>277</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bai</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Guan</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>The Visuospatial and Sensorimotor Functions of Posterior Parietal Cortex in Drawing Tasks: A Review</article-title>. <source>Front. Aging Neurosci</source>. <volume>13</volume>, (<year>2021</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gowen</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Miall</surname>, <given-names>R. C</given-names></string-name></person-group>. <article-title>Eye–hand interactions in tracing and drawing tasks</article-title>. <source>Human Movement Science</source> <volume>25</volume>, <fpage>568</fpage>–<lpage>585</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petilli</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Daini</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Saibene</surname>, <given-names>F. L.</given-names></string-name> &amp; <string-name><surname>Rabuffetti</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Automated scoring for a Tablet-based Rey Figure copy task differentiates constructional, organisational, and motor abilities</article-title>. <source>Sci Rep</source> <volume>11</volume>, <issue>14895</issue> (<year>2021</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Senese</surname>, <given-names>V. P.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Identifying neuropsychological predictors of drawing skills in elementary school children</article-title>. <source>Child Neuropsychology</source> <volume>26</volume>, <fpage>345</fpage>–<lpage>361</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tchalenko</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Chris Miall</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Eye–hand strategies in copying complex lines</article-title>. <source>Cortex</source> <volume>45</volume>, <fpage>368</fpage>–<lpage>376</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Droll</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Hayhoe</surname>, <given-names>M. M</given-names></string-name></person-group>. <article-title>Trade-offs between gaze and working memory use</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>33</volume>, <fpage>1352</fpage>–<lpage>1365</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Russell</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A deficit of spatial remapping in constructional apraxia after right-hemisphere stroke</article-title>. <source>Brain</source> <volume>133</volume>, <fpage>1239</fpage>–<lpage>1251</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van der Stigchel</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Parietal Involvement in Constructional Apraxia as Measured Using the Pentagon Copying Task</article-title>. <source>Dementia and Geriatric Cognitive Disorders</source> <volume>46</volume>, <fpage>50</fpage>– <lpage>59</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathuranath</surname>, <given-names>P. S.</given-names></string-name>, <string-name><surname>Nestor</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Berrios</surname>, <given-names>G. E.</given-names></string-name>, <string-name><surname>Rakowicz</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Hodges</surname>, <given-names>J. R</given-names></string-name></person-group>. <article-title>A brief cognitive test battery to differentiate Alzheimer’s disease and frontotemporal dementia</article-title>. <source>Neurology</source> <volume>55</volume>, <fpage>1613</fpage>–<lpage>1620</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rey</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>L’examen psychologique dans les cas d’encephalopathie traumatique</article-title>. <source>Archives de Psychologie</source> <volume>28</volume>, <fpage>286</fpage>–<lpage>340</lpage> (<year>1941</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Osterrieth</surname>, <given-names>P. A</given-names></string-name></person-group>. <article-title>Le test de copie d’une figure complexe; contribution à l’étude de la perception et de la mémoire</article-title>. <source>Archives de Psychologie</source> <volume>30</volume>, <fpage>206</fpage>–<lpage>356</lpage> (<year>1944</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Tierney</surname>, <given-names>T. M.</given-names></string-name>, <etal>et al.</etal> . <collab>Preprint at</collab></person-group><article-title>SPM 25: open source neuroimaging analysis software</article-title>. <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.2501.12081</pub-id> (<year>2025</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeidman</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Parr</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>A primer on Variational Laplace (VL)</article-title>. <source>NeuroImage</source> <volume>279</volume>, <issue>120310</issue> (<year>2023</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Bayesian model reduction and empirical Bayes for group (DCM) studies</article-title>. <source>NeuroImage</source> <volume>128</volume>, <fpage>413</fpage>–<lpage>431</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><collab>JASP Team. JASP</collab></person-group>. (<year>2024</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biesanz</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Falk</surname>, <given-names>C. F.</given-names></string-name> &amp; <string-name><surname>Savalei</surname>, <given-names>V</given-names></string-name></person-group>. <article-title>Assessing Mediational Models: Testing and Interval Estimation for Indirect Effects</article-title>. <source>Multivariate Behavioral Research</source> <volume>45</volume>, <fpage>661</fpage>–<lpage>701</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name></person-group> <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source> <volume>10</volume>, <fpage>433</fpage>–<lpage>436</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kleiner</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>What’s new in psychtoolbox-3</article-title>. <source>Perception</source> <volume>36</volume>, <fpage>1</fpage>–<lpage>16</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Polyak</surname>, <given-names>S. L</given-names></string-name></person-group>. <source>The Retina</source>. (<publisher-name>University of Chicago Press</publisher-name>, <publisher-loc>Chicago</publisher-loc>, <year>1941</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sakurai</surname>, <given-names>M</given-names></string-name></person-group>. <chapter-title>Parafovea</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Shamey</surname>, <given-names>R</given-names></string-name></person-group> <source>Encyclopedia of Color Science and Technology</source> <fpage>1</fpage>–<lpage>3</lpage> (<publisher-name>Springer</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, <year>2020</year>). doi:<pub-id pub-id-type="doi">10.1007/978-3-642-27851-8_215-2</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109581.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Shuo</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Washington University in St. Louis</institution>
</institution-wrap>
<city>St. Louis</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study makes an <bold>important</bold> contribution by revealing how saccades selectively disrupt spatial working memory while sparing other object features, and by demonstrating how this mechanism is altered in aging and neurodegeneration. The findings are supported by <bold>convincing</bold> evidence derived from well-controlled eye-tracking experiments and systematic generative model comparisons. Together, the work provides a computationally grounded framework that is of importance for understanding trans-saccadic memory and its clinical relevance.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109581.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study employed a saccade-shifting sequential working memory paradigm, manipulating whether a saccade occurred after each memory array to directly compare retinotopic and transsaccadic working memory for both spatial location and color. Across four participant groups (young and older healthy adults, and patients with Parkinson's disease and Alzheimer's disease), the authors found a consistent saccade-related cost specifically for spatial memory - but not for color - regardless of differences in memory precision. Using computational modeling, they demonstrate that data from healthy participants are best explained by a complex saccade-based updating model that incorporates distractor interference. Applying this model to the patient groups further elucidates the sources of spatial memory deficits in PD and AD. The authors then extend the model to explain copying deficits in these patient groups, providing evidence for the ecological validity of the proposed saccade-updating retinotopic mechanism.</p>
<p>Strengths:</p>
<p>Overall, the manuscript is well written, and the experimental design is both novel and appropriate for addressing the authors' key research questions. I found the study to be particularly comprehensive: it first characterizes saccade-related costs in healthy young adults, then replicates these findings in healthy older adults, demonstrating how this &quot;remapping&quot; cost in spatial working memory is age-independent. After establishing and validating the best-fitting model using data from both healthy groups, the authors apply this model to clinical populations to identify potential mechanisms underlying their spatial memory impairments. The computational modeling results offer a clearer framework for interpreting ambiguities between allocentric and retinotopic spatial representations, providing valuable insight into how the brain represents and updates visual information across saccades. Moreover, the findings from the older adult and patient groups highlight factors that may contribute to spatial working memory deficits in aging and neurological disease, underscoring the broader translational significance of this work.</p>
<p>Weaknesses:</p>
<p>Several concerns should be addressed to enhance the clarity of the manuscript:</p>
<p>(1) Relevance of the figure-copy results (pp. 13-15).</p>
<p>Is it necessary to include the figure-copy task results within the main text? The manuscript already presents a clear and coherent narrative without this section. The figure-copy task represents a substantial shift from the LOCUS paradigm to an entirely different task that does not measure the same construct. Moreover, the ROCF findings are not fully consistent with the LOCUS results, which introduces confusion and weakens the manuscript's coherence. While I understand the authors' intention to assess the ecological validity of their model, this section does not effectively strengthen the manuscript and may be better removed or placed in the Supplementary Materials.</p>
<p>(2) Model fitting across age groups (p. 9).</p>
<p>It is unclear whether it is appropriate to fit healthy young and healthy elderly participants' data to the same model simultaneously. If the goal of the model fitting is to account for behavioral performance across all conditions, combining these groups may be problematic, as the groups differ significantly in overall performance despite showing similar remapping costs. This suggests that model performance might differ meaningfully between age groups. For example, in Figure 4A, participants 22-42 (presumably the elderly group) show the best fit for the Dual (Saccade) model, implying that the Interference component may contribute less to explaining elderly performance.</p>
<p>Furthermore, although the most complex model emerges as the best-fitting model, the manuscript should explain how model complexity is penalized or balanced in the model comparison procedure. Additionally, are Fixation Decay and Saccade Update necessarily alternative mechanisms? Could both contribute simultaneously to spatial memory representation? A model that includes both mechanisms-e.g., Dual (Fixation) + Dual (Saccade) + Interference-could be tested to determine whether it outperforms Model 7 to rule out the sole contribution of complexity.</p>
<p>Minor point: On p. 9, line 336, Figure 4A does not appear to include the red dashed vertical line that is mentioned as separating the age groups.</p>
<p>(3) Clarification of conceptual terminology.</p>
<p>Some conceptual distinctions are unclear. For example, the relationship between &quot;retinal memory&quot; and &quot;transsaccadic memory,&quot; as well as between &quot;allocentric map&quot; and &quot;retinotopic representation,&quot; is not fully explained. Are these constructs related or distinct? Additionally, the manuscript uses terms such as &quot;allocentric map,&quot; &quot;retinotopic representation,&quot; and &quot;reference frame&quot; interchangeably, which creates ambiguity. It would be helpful for the authors to clarify the relationships among these terms and apply them consistently.</p>
<p>(4) Rationale for the selective disruption hypothesis (p. 4, lines 153-154).</p>
<p>The authors hypothesize that &quot;saccades would selectively disrupt location memory while leaving colour memory intact.&quot; Providing theoretical or empirical justification for this prediction would strengthen the argument.</p>
<p>(5) Relationship between saccade cost and individual memory performance (p. 4, last paragraph).</p>
<p>The authors report that larger saccades were associated with greater spatial memory disruption. It would be informative to examine whether individual differences in the magnitude of saccade cost correlate with participants' overall/baseline memory performance (e.g. their memory precision in the no-saccade condition). Such analyses might offer insights into how memory capacity/ability relates to resilience against saccade-induced updating.</p>
<p>(6) Model fitting for the healthy elderly group to reveal memory-deficit factors (pp. 11-12).</p>
<p>The manuscript discusses model-based insights into components that contribute to spatial memory deficits in AD and PD, but does not discuss components that contribute to spatial memory deficits in the healthy elderly group. Given that the EC group also shows impairments in certain parameters, explaining and discussing these outcomes of the EC group could provide additional insights into age-related memory decline, which would strengthen the study's broader conclusions.</p>
<p>(7) Presentation of saccade conditions in Figure 5 (p. 11).</p>
<p>In Figure 5, it may be clearer to group the four saccade conditions together within each patient group. Since the main point is that saccadic interference on spatial memory remains robust across patient groups, grouping conditions by patient type rather than intermixing conditions would emphasize this interpretation.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109581.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Zhao et al investigate how object location and colour are degraded across saccadic eye movements. They employ an eye-tracking task that requires participants to remember two sequentially presented items and subsequently report the colour and position of either one of these. Through counterbalancing of the presence or absence of saccades across items, the authors endeavour to dissect the impact of saccades independently on item location or colour. These behavioural findings form the basis of generative models designed to test competing, nested accounts of how stored information is stored and updated across saccades.</p>
<p>Strengths:</p>
<p>The combination of eye-tracking and generative modelling is a strength of the paper, which opens new perspectives into the impact of Alzheimer's and Parkinson's disease on the performance of visuospatial cognitive tests. The finding that the model parameters covary with clinical performance on the ROCF test is a nice example of a &quot;computational assay&quot; of disease.</p>
<p>Weaknesses:</p>
<p>I have a number of substantial and minor concerns for the authors to consider in a revision:</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109581.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The manuscript introduces a visual paradigm aimed at studying trans-saccadic memory.</p>
<p>The authors observe how memory of object location is selectively impaired across eye movements, whereas object colour memory is relatively immune to intervening eye movements.</p>
<p>
Results are reported for young and elderly healthy controls, as well as PD and AD participants.</p>
<p>A computational model is introduced to account for these results, indicating how early differences in memory encoding and decay (but not trans-saccadic updating per se) can account for the observed differences between healthy controls and clinical groups.</p>
<p>Strengths:</p>
<p>The data presented encompasses healthy and elderly controls, as well as clinical groups.</p>
<p>The authors introduce an interesting modelling strategy, aimed at isolating and identifying the main components behind the observed pattern of results.</p>
<p>Weaknesses:</p>
<p>The models tested differ in terms of the number of parameters. In general, a larger number of parameters leads to a better goodness of fit. It is not clear how the difference in the number of parameters between the models was taken into account.</p>
<p>It is not clear whether the modelling results could be influenced by overfitting (it is not clear how well the model can generalize to new observations).</p>
<p>Results specificity: it is not clear how specific the modelling results are with respect to constructional ability (measured via the Rey-Osterrieth Complex Figure test). As with any cognitive test, performance can also be influenced by general, non-specific abilities that contribute broadly to test success.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109581.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhao</surname>
<given-names>Sijia</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6246-0702</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Parr</surname>
<given-names>Thomas</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5108-5743</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Udale</surname>
<given-names>Rob</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1704-554X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Klar</surname>
<given-names>Verena</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1901-2485</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Jones</surname>
<given-names>Gabriel David</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Scholcz</surname>
<given-names>Anna</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Toniolo</surname>
<given-names>Sofia</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1833-4995</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Manohar</surname>
<given-names>Sanjay G</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0735-4349</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Husain</surname>
<given-names>Masud</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6850-9255</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>(1) About ROCF figure-copy results</bold></p>
</disp-quote>
<p>Reviewer #1 queried the necessity of including the Rey-Osterrieth Complex Figure (ROCF) results in the main text. We appreciate the reviewer’s perspective on the narrative flow and the transition between the LOCUS paradigm and the ROCF results. However, we remain keen to retain these findings in the main tex, as they provide critical ecological and clinical validation for the computational mechanisms identified in our study.</p>
<p>We argue that the following points support the retention of these results:</p>
<p>(1)  The ROCF we used is a standard neuropsychological tool for identifying constructional apraxia. Our results bridge the gap between basic cognitive neuroscience and clinical application by demonstrating that specific remapping parameters—rather than general memory precision—predict real-world deficits in patients.</p>
<p>(2)  The finding that our winning model explains approximately 62% of the variance in ROCF copy scores across all diagnostic groups further indicates that these parameters from the LOCUS task represent core computational phenotypes that underpin complex, real-life visuospatial construction (copying drawings).</p>
<p>(3)  Previous research has often observed only a weak or indirect link between drawing ability and traditional working memory measures, such as digit span  (Senese et al., 2020). This was previously attributed to “deictic” strategies—like frequent eye movements—that minimise the need to hold large amounts of information in memory (Ballard et al., 1995; Cohen, 2005; Draschkow et al., 2021). While our study was not exclusively designed to catalogue all cognitive contributions to drawing, our findings provide significant and novel evidence indicating that transsaccadic integration is a critical driver of constructional (copying drawing) ability. By demonstrating this link, we offer a new direction for future research, shifting the focus from general memory capacity toward the precision of spatial updating across eye movements.</p>
<p>By including the ROCF results in the main text, we provide evidence for a functional role for spatial remapping that extends beyond perceptual stability into the domain of complex visuomotor control. We will expand on these points in the Discussion in our revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>(2) Model complexity and overfitting</bold></p>
</disp-quote>
<p>We would like to clarify that the Bayesian model selection (BMS) procedure utilised in this manuscript inherently balances model fit with parsimony. Unlike maximum likelihood inference, where overfitting is a primary concern often requiring cross-validation via out-of-sample prediction, our approach depends upon the comparison of marginal likelihoods. This method directly penalises model complexity — a principle often described as the “Bayesian Occam’s Razor” (Rasmussen and Ghahramani, 2000). This means that a model is only favoured if the improvement in fit justifies the additional parameter space. If a parameter were redundant, it would lower the model's evidence by “diluting” the probability mass over the parameter space. The emergence of the “Dual (Saccade) + Interference” model as the winning candidate suggests it offers the most plausible generative account of the data while maintaining necessary parsimony. We would be happy to point toward literature that discusses how these marginal likelihood approximations provide a more robust guard against overfitting than standard metrics like BIC or AIC (MacKay, 2003; Murray and Ghahramani, 2005; Penny, 2012).</p>
<disp-quote content-type="editor-comment">
<p><bold>(3) On model fitting across age groups</bold></p>
</disp-quote>
<p>This approach is primarily supported by our empirical findings: there was no significant interaction between age group and saccade condition for either location or colour memory. While older adults demonstrated lower baseline precision, the specific disruptive effect of saccades (the “saccade cost”) was remarkably consistent across cohorts. This justifies the use of a common generative model to assess quantitative differences in parameter estimates.</p>
<p>This approach does implicitly assume that participants perform the task in a qualitatively similar way. However, as this assumption is mitigated by the fact that our winning model nests simpler models as special cases, it supports the assessment of group differences in parameters that play consistent mechanistic roles. This flexibility allows the model to naturally accommodate groups where certain components—such as interference—may play a reduced role, while remaining sensitive to the specific mechanistic failures that differentiate healthy aging from neurodegeneration.</p>
<disp-quote content-type="editor-comment">
<p><bold>(4) Conceptual terminology and patient group descriptions</bold></p>
</disp-quote>
<p>We will clarify our conceptual terminology, explicitly defining the relationships between retinotopic (eye-centred), transsaccadic (across-saccade), and spatiotopic (world-centred) representations.</p>
<p>Regarding the demographics of the clinical cohorts, we apologise for any lack of clarity in our initial presentation. The patient demographics for both the Parkinson’s disease (PD) and Alzheimer’s disease (AD) groups—including age, gender, education, and ACE-III scores—are currently detailed alongside the healthy control data (two groups: Young Healthy Controls and Elderly Healthy Controls) in the table within the Participants section of the Materials and Methods. In our revision. We will ensure that this table is correctly labelled as Table 2 and will provide more comprehensive recruitment and characterisation details for both patient groups within the main text. Finally, we will include a detailed discussion in the Supplementary Materials regarding eye-tracking data quality across all cohorts, specifically comparing calibration accuracy, trace stability, and trial rejection rates to demonstrate that our findings are not confounded by differences in recording quality between healthy and clinical populations.</p>
<p>References</p>
<p>Ballard DH, Hayhoe MM, Pelz JB. 1995. Memory Representations in Natural Tasks. Journal of Cognitive Neuroscience 7:66–80. DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.1995.7.1.66">https://doi.org/10.1162/jocn.1995.7.1.66</ext-link></p>
<p>Cohen DJ. 2005. Look little, look often: The influence of gaze frequency on drawing accuracy. Perception &amp; Psychophysics 67:997–1009. DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03193626">https://doi.org/10.3758/BF03193626</ext-link></p>
<p>Draschkow D, Kallmayer M, Nobre AC. 2021. When Natural Behavior Engages Working Memory. Current Biology 31:869-874.e5. DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2020.11.013">https://doi.org/10.1016/j.cub.2020.11.013</ext-link>, PMID: 33278355</p>
<p>MacKay DJC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge University Press.</p>
<p>Murray I, Ghahramani Z. 2005. A note on the evidence and Bayesian Occam’s razor (Technical report No. GCNU TR 2005-003). Gatsby Unit.</p>
<p>Penny WD. 2012. Comparing Dynamic Causal Models using AIC, BIC and Free Energy. Neuroimage 59:319–330. DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2011.07.039">https://doi.org/10.1016/j.neuroimage.2011.07.039</ext-link>, PMID: 21864690</p>
<p>Rasmussen C, Ghahramani Z. 2000. Occam’ s Razor. Advances in Neural Information Processing Systems. MIT Press.</p>
<p>Senese VP, Zappullo I, Baiano C, Zoccolotti P, Monaco M, Conson M. 2020. Identifying neuropsychological predictors of drawing skills in elementary school children. Child Neuropsychology 26:345–361. DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/09297049.2019.1651834">https://doi.org/10.1080/09297049.2019.1651834</ext-link>, PMID: 31390949</p>
</body>
</sub-article>
</article>