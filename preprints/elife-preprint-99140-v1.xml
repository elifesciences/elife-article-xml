<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99140</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99140</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99140.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Switching perspective: Comparing ground-level and bird’s-eye views for bees navigating clutter</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3139-3981</contrib-id>
<name>
<surname>Sonntag</surname>
<given-names>Annkathrin</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1029-8846</contrib-id>
<name>
<surname>Sauzet</surname>
<given-names>Odile</given-names>
</name>
<xref ref-type="aff" rid="a2">b</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2463-2040</contrib-id>
<name>
<surname>Lihoreau</surname>
<given-names>Mathieu</given-names>
</name>
<xref ref-type="aff" rid="a3">c</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9336-4270</contrib-id>
<name>
<surname>Egelhaaf</surname>
<given-names>Martin</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0889-4550</contrib-id>
<name>
<surname>Bertrand</surname>
<given-names>Olivier</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
</contrib>
<aff id="a1"><label>a</label><institution>Neurobiology, Faculty of Biology, Bielefeld University</institution>, 33615 Bielefeld, <country>Germany</country></aff>
<aff id="a2"><label>b</label><institution>School of Public Health and Department of Business Administration and Economics, Bielefeld University</institution>, 33615 Bielefeld, <country>Germany</country></aff>
<aff id="a3"><label>c</label><institution>Research Center on Animal Cognition (CRCA), Center for Integrative Biology (CBI); CNRS, University Paul Sabatier - Toulouse III</institution>, 31062 Toulouse, <country>France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Desplan</surname>
<given-names>Claude</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>New York University</institution>
</institution-wrap>
<city>New York</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>a.sonntaguni-bielefeld.de <email>a.sonntag@uni-bielefeld.de</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-07-24">
<day>24</day>
<month>07</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99140</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-08">
<day>08</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-05-09">
<day>09</day>
<month>05</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.21.572344"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Sonntag et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Sonntag et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99140-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Animals navigating in three dimensions encounter different perspectives of their world, often transitioning from bird’s eye views at higher altitudes to frog’s-eye views closer to the ground. How they integrate this information to pinpoint a goal location is virtually unknown. Here we tested the ability of bumblebees to use both types of views when homing in a cluttered environment. Our combined modelling and experimental approach examined various views for goal location in cluttered settings. Whereas, bird’s-eye views performed best in simulations of current snapshot homing models, behavioural experiments revealed that bumblebees predominantly relied on frog’s eye views when pinpointing nest entrances in cluttered environments. These findings reveal the limitations of snapshot-homing models and suggest that bumblebees use a combination of navigational tools to successfully find their way home in cluttered environments. This is not only relevant for understanding bee movements, but also for other animals and humans navigating in 3D as well as the development of technologies inspired by natural systems, such as autonomous flying robots.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Captions of the figures to the model results in the supporting information have been updated.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Numerous animals, spanning diverse taxa, navigate within a three-dimensional world where changes in altitude are common. Birds, fish, mammals and insects change their flight altitude, climbing height, or swimming depth from near the ground to great heights above or depths below the water surface during foraging, nesting, or in search for shelter [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c16">16</xref>]. Also humans experience that the landscapes and views of the environment change with increasing altitude during activities like hiking, climbing, or aeroplane travel. This prompts us to question whether animals that have evolved to solve three-dimensional challenges are also adapted to efficiently use bird’ s-eye view perspectives for navigational purposes. Over the past decades, researchers have delved into the navigational strategies employed by insects, commonly referred to as their navigational toolkit [<xref ref-type="bibr" rid="c46">46</xref>]. This toolkit primarily comprises a compass and an odometer, which can be synergistically employed as a path integrator—enabling the integration of distance and direction travelled. Additionally, insects utilise landmark guidance and exploration behaviour in their navigation. Taken all together, the navigational toolkit has been studied by analysing the insects’ walking or flight paths mostly in two dimensions (e.g. [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c43">43</xref>]).</p>
<p>Accordingly, the visual mechanisms that have been considered are primarily based on the information that can be seen from close to the ground (frog’s-eye view) [<xref ref-type="bibr" rid="c50">50</xref>]. However, when flying insects exit their nest for the first time and are not yet familiar with their surroundings, they increase the distance to the nest during loops, arcs and spirals and vary their flight altitude during so-called learning flights [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c47">47</xref>]. They may therefore learn and use visual sceneries at different altitudes. The visual scenery may drastically change when insects change their flight altitude in cluttered environments. Flying insects use views from above the clutter, i.e., bird’ s-eye views, to recognize ground-level landmarks and locations for large-scale navigation [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c30">30</xref>]. Such bird’s-eye views might not only be relevant for high altitudes and navigation on a large spatial scale, but also on smaller spatial scales and altitudes in the context of local homing.</p>
<p>This might be especially helpful for underground nesting species, such as bumblebees <italic>Bombus ter-restris</italic>, whose nest entrance is often inconspicuously located within the undergrowth. In such cluttered environments, bumblebees need to change their flight altitude both when learning the surroundings and when homing. Bird’s eye views might be helpful for guiding homing behaviour in the near-range of the hidden nest hole by providing a kind of overview. In contrast, frog’s eye views might help pinpointing the nest entrance. Computational models suggest such views of the visual scenery from within cluttered environments is sufficient to explain the returning journey of ants [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c31">31</xref>] and bees [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c50">50</xref>] under a variety of experimental conditions. These models rely on visual memories of the environments acquired during previous journeys or learning flights at the nest site or a flower. To navigate visually, the modelled insect compares its current view with the memorised ones and steers toward the most familiar memories (e.g. active scanning [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c1">1</xref>], rotation-invariant comparison [<xref ref-type="bibr" rid="c41">41</xref>], while oscillating by left and right turns [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>]). This class of models can simulate route-following behaviour within clutter, even at different flight altitudes [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c7">7</xref>]. Despite all these results at the model level, how insects use views at different altitudes to navigate has never been studied experimentally. Because this is a particular challenge for bumblebees in finding their nest hole, we investigated this in a combined experimental and model analysis.</p>
<p>We addressed the question of whether bumblebees, <italic>Bombus terrestris</italic>, learn views at different altitudes and if they can return home by just using either frog’s or bird’s eye views. We first investigated the navigation of modelled bees guided by standard models (multi-snapshot model), which are broadly used in the literature for homing [<xref ref-type="bibr" rid="c51">51</xref>]. We then designed behavioural experiments to challenge bees in the same environment. The analysis was based on homing experiments in cluttered laboratory environments, where the flight altitude during the learning and homing flights was systematically constrained by manipulating the environment. We related our results to predictions of snapshot models [<xref ref-type="bibr" rid="c52">52</xref>, <xref ref-type="bibr" rid="c50">50</xref>] comparing the performance of bird’s and frog’s eye view snapshots in the same cluttered environment like the behavioural experiments.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Snapshot models perform best with bird’s eye views</title>
<p>To guide our hypotheses about which views bees should prioritise during their homing flight, we compared predictions of homing models based on either bird’s eye views or frog’s eye views. In previous studies on the mechanism of visual homing, models could replicate the insects’ homing behaviour at least in spatially relatively simple situations [<xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c25">25</xref>]. These models assume that insects learn one or multiple views, so-called <italic>snapshots</italic>, around their goal location, like their nest. During their return, they compare their current views with the stored snapshot(s). Therefore we tested the homing performance in a cluttered meadow (clutter) of rotational image difference models based on two parameters: brightness values of the pixels [<xref ref-type="bibr" rid="c52">52</xref>, <xref ref-type="bibr" rid="c12">12</xref>] or contrast-weighed-nearness values encoding the depth and contrast of the environment based on optic flow information [<xref ref-type="bibr" rid="c13">13</xref>] (for details see Methods). The environment for the model simulations was the same as that later used in the experimental analysis (<xref rid="fig2" ref-type="fig">Fig. 2</xref> A&amp;B). It consisted of an inconspicuous goal on the floor, i.e. the nest hole, surrounded by multiple, similarly-looking objects creating an artificial meadow around the nest. Thus, we investigated the model performance of homing in a cluttered environment and carried out an image comparison of the snapshots taken either above the meadow or within it.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Brightness based homing model at four altitude layers in the environment (0.02 m, 0.15 m, 0.32 m, and 0.45 m) for the area around the artificial meadow (clutter). The rows show different parameters for the memorised snapshots (eight positions taken either outside or inside the meadow and either above the meadow, bird’s eye view, or close to the ground, frog’s eye views). <bold>A&amp;B:</bold> Examples of panoramic snapshots in A from the bird’s eye view outside the clutter and in B frog’s eye views inside the clutter. The axes of the panoramic images refer to the azimuthal directions (x-axis) and to the elevational directions from the simulated bee’s point of view (y-axis, dorsal meaning upwards, ventral downwards, equatorial towards the horizon). <bold>C:</bold> Rendered layers of the environment for a comparison of the current view of the simulated bee. The layers are at 0.02 m(orange), 0.15 m (blue), 0.32 m(green) and 0.45 m (red) heights. <bold>D&amp;E:</bold> The first column shows were the snapshots were taken in relation to the nest position (nest position in black, objects in red and snapshot positions indicated by coloured arrows). The other two columns show the comparison of memorised snapshot for two layers of the environment (0.02 m and 0.45 m as shown in C). The heatmaps show the image similarity between the current view at the position in the arena and the memorised snapshots taken around the nest (blue = very similar, white = very different). Additionally, white lines and arrows present the vector field from which the homing potential is derived. Red circles indicate the positions of the objects and the white dot indicates the nest position. The background colour of each column indicates the height of the current views that the snapshots are compared to. <bold>D:</bold> Memorised bird’s eye view snapshots taken outside (distance to the nest = 0.55 m) and above the clutter (height = 0.45 m) can guide the model at the highest altitude (red background) to the nest but fails to do so at the three lower altitudes. <bold>E:</bold> Memorised frog’s eye view snapshots taken outside (distance to the nest = 0.55 m) the clutter and close to the floor (height = 0.02 m) can only guide the model towards the center.</p></caption>
<graphic xlink:href="572344v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p><bold>A:</bold> We trained two groups of bees in a cylindrical flight arena with cylindrical cluttered objects (‘artificial meadow’) around the nest entrance. The first group, <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup>, was trained with a ceiling height of the arena twice the height of the objects providing space to fly above the objects. They might have memorised both a frog’s (<italic>F</italic><sup>+</sup>) as well as a bird’s eye (<italic>B</italic><sup>+</sup>) view when leaving the nest. The second group, <italic>B<sup>−</sup> F</italic><sup>+</sup>, was trained with an arena height restricted to the height of the objects, allowing the bees to only use frog’s eye views <italic>B<sup>−</sup> F</italic><sup>+</sup>. Both groups were tested to return home in three test conditions: HighCeiling (BF), Covered (B) and LowCeilng (F). In test BF the artificial meadow was shifted from the training position to another position in the arena to exclude the use of potential external cues; the bees could use both, a frog’s and a bird’s eye view, during return. In test B a partial ceiling above and a transparent wall was placed around the objects preventing the bees from entering the artificial clutter during return. In test F the ceiling was lowered to the top of the objects allowing the bees to use only frog’s eye views during return. <bold>B:</bold> 3D view of the setup with the hive and the foraging chamber.</p></caption>
<graphic xlink:href="572344v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Eight snapshots around the nest position were compared to panoramic images either based on brightness values or contrast-weighted nearness of the rendered environment at four altitudes. These altitudes were chosen to provide information related to ecologically relevant heights for the bees during their return home (<xref rid="fig5" ref-type="fig">Fig. 5</xref>). These were either close to the ground (0.02 m) resembling walking or flying close the floor, half the height of the objects (0.15 m), at the maximum height of the objects (0.32 m) or above the objects (0.45 m). We hypothesised that the simulated bee could memorise panoramic snapshots either at high altitude (bird’s eye views) or close to the floor (frog’s eye views). Further, these snapshots were taken either close to the nest (0.1 m) or just outside the clutter (0.55 m) (<xref rid="fig1" ref-type="fig">Fig.1</xref> positions of snapshots). Previous behavioural studies showed that views close to the goal are acquired during learning walks or flight [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c42">42</xref>] and modelling studies could emphasise the successful use during homing [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. Views just at the border of the clutter display the change of the environment from within the clutter surrounded by many objects to a clearance outside the clutter. Those abrupt changes of the environment have been shown to trigger reorientation by convoluted flight pattern [<xref ref-type="bibr" rid="c8">8</xref>]. The comparison of simulated homing either with memorised snapshots taken from the bird’s eye or the frog’s eye views, revealed the best performance, highest image similarity and homing vectors point towards the nest, with bird’s eye view snapshots outside the clutter (brightness model: <xref rid="fig1" ref-type="fig">Fig. 1</xref> and <xref rid="fig9" ref-type="fig">Fig. 9</xref>, contrast-weighted nearness model: <xref rid="fig13" ref-type="fig">Fig.13</xref>). Bird’s eye view snapshots led the simulated bee very close to the nest (brightness model: <xref rid="fig1" ref-type="fig">Fig. 1</xref> D&amp;E and <xref rid="fig9" ref-type="fig">Fig. 9</xref> - <xref rid="fig10" ref-type="fig">10</xref>, contrast-weighted nearness model: <xref rid="fig13" ref-type="fig">Fig. 13</xref> - <xref rid="fig14" ref-type="fig">14</xref>) while frog’s eye view snapshots inside the clutter could only lead into the clutter but not to the nest (brightness model: <xref rid="fig1" ref-type="fig">Fig. 1</xref> F&amp;G and <xref rid="fig11" ref-type="fig">Fig. 11</xref> - <xref rid="fig12" ref-type="fig">12</xref>, contrast-weighted nearness model: <xref rid="fig15" ref-type="fig">Fig. 15</xref> and <xref rid="fig16" ref-type="fig">16</xref>). Snapshots above the clutter showed the best homing performance when compared to images above the clutter. Based on these results we made qualitative predictions on the bumblebees homing behaviour, assuming they are guided by homing mechanisms akin to the one tested in simulation. First, this suggests bumblebees would return to their environment by flying above the clutter. Second, bees that could not acquire views above the clutter, would not be able to locate their nest entrance.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Examples of return flights and bees’ search for their nest in a cluttered environment (N = 26). <bold>A&amp;B:</bold>Exemplary flight trajectories in 3D (left column) and a top view in 2D (right column) from the group <italic>B</italic><sup>+</sup><italic>F</italic><sup>+</sup> in the BF condition (<bold>A</bold>), and from the group <italic>B<sup>−</sup> F</italic><sup>+</sup> in the B condition (<bold>B</bold>). The colour indicates the time, blue the start and red the end of the flight. The objects are depicted by red cylinders in the 3D plot and as red circles in the 2D plot. The black dots in the 2D plot shows the visual nest position within the clutter. <bold>A:</bold> The bee searches for the nest within the clutter at a low flight altitude. <bold>B:</bold> The bee is mainly trying to enter the covered clutter from the side. <bold>C:</bold> Spatial search distribution represented by hexagonal binning of the percentage of visits of all bees (relative to each bee’s total flight time) in the BF condition from group <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup>. Orange circles indicate the true and the visual nest position. Black circles indicate the object positions of the clutter. <bold>D:</bold> Percentage searching for the two groups <italic>B</italic><sup>+</sup><italic>F</italic><sup>+</sup> (filled boxes, N = 26) and <italic>B<sup>−</sup> F</italic><sup>+</sup> (hatched boxes, N = 26) in the tests BF, F and B relative to the total flight time. The search percentage at the true nest is given in blue and at the visual nest in green. For all tested conditions and both groups, the bees searched more at the visual nest within the clutter than at the true nest location (refer to SI <xref rid="tbl1" ref-type="table">Table 1</xref> for statistical tests).</p></caption>
<graphic xlink:href="572344v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Entry points of the bees (N = 26) to the clutter, from the side (<bold>A</bold>, circular histogram in grey) and from the top (<bold>B</bold>, scatter plot in blue) of the clutter. The direction of the direct path from the arena entrance to the nest is given by the green triangle. The kernel density estimation (KDE) of the entries from the side of the clutter is shown as a black, dashed line in <bold>A</bold>. The radial axes represents the normalized magnitude of the KDE.</p></caption>
<graphic xlink:href="572344v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Probability density distribution of the flight altitude and the search distribution for the B condition of the groups <italic>B<sup>−</sup> F</italic><sup>+</sup> (<bold>A&amp;B</bold>) and <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> <bold>C&amp;D</bold>). A flight altitude of 0 is at the floor, of 300mm is at the height of the objects and 600mm at the ceiling. <bold>A</bold>: The group <italic>B<sup>−</sup> F</italic><sup>+</sup> constrained to a low ceiling during training shows two peaks for a low altitude and for a high altitude. <bold>C</bold>: The group <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> shows a broader distribution, with three peaks either half the height of the objects, just above the height of the objects, or close below the ceiling. <bold>B&amp;D</bold>: The search distributions reveal that the bees tried to enter the clutter between the objects.</p></caption>
<graphic xlink:href="572344v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Schematic of a multi-step process with visually triggered reloading of memorised home vectors. A bee entering the arena (grey circle) could have experienced the normal path integration vector (black arrow) pointing towards the true nest location at the training position in clutter (black dot within the light red circle). However, the visual scene changed drastically in the test. Hence, a vector memory might point coarsely to the clutter (light green arrow) triggered by the prominent visual cue of the clutter shifted to the test position (dark red circle). Since this vector would not point precisely towards the nest, the bee could search at the border of the clutter (curved, yellow arrow) for a previously experienced entry position to the clutter. This position could reload a refined clutter vector pointing to the more precise nest position within the clutter (blue arrow).</p></caption>
<graphic xlink:href="572344v3_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Examples of the memorised snapshots based on brightness values for inside and outside the clutter as well as from the bird’s and frog’s eye view perspective. The axes of the panoramic images refer to the azimuthal directions (x-axis) and to the elevational directions from the simulated bee’s point of view (y-axis, dorsal meaning upwards, ventral downwards, equatorial towards the horizon).</p></caption>
<graphic xlink:href="572344v3_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><p>Examples of the memorised snapshots based on contrast-weighted nearness for inside and outside the clutter as well as from the bird’s and frog’s eye view perspective. The axes of the panoramic images refer to the azimuthal directions (x-axis) and to the elevational directions from the simulated bee’s point of view (y-axis, dorsal meaning upwards, ventral downwards, equatorial towards the horizon).</p></caption>
<graphic xlink:href="572344v3_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><p>Brightness-based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with frog’s eye view snapshots taken outside the clutter (distance = 0.55 m, height = 0.45 m). <bold>A:</bold> We applied the model to a list of images, one of them (memory 0) being the memory inside the model. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these images, where the local minima are shifted according the nest bearing. <bold>B-C:</bold> Heatmaps of full environment (<bold>B</bold>) and only the cluttered area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees with bird’s eye view snapshots outside the clutter to the nest position when the images are compared to images at an altitude of 0.45 m.</p></caption>
<graphic xlink:href="572344v3_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure">
<label>Figure 10.</label>
<caption><p>Brightness-based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with bird’s eye view snapshots taken inside the clutter (distance = 0.1 m, height = 0.45 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, one of them (memory 0) being the memory inside the model. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these images, where the local minima are shifted according the nest bearing. <bold>B:</bold> Heatmaps of full environment (<bold>B</bold>) and only the cluttered area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees, with bird’s eye view snapshots inside the clutter, only coarsely to the clutter but not to the nest when the images are compared to images at an altitude of 0.45 m.</p></caption>
<graphic xlink:href="572344v3_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig11" position="float" orientation="portrait" fig-type="figure">
<label>Figure 11.</label>
<caption><p>Brightness-based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with frog’s eye view snapshots taken inside the clutter (distance = 0.1 m, height = 0.02 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, one of them (memory 0) being the memory inside the model. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these images, where the local minima are shifted according the nest bearing. <bold>B:</bold> Heatmaps of full environment (<bold>B</bold>) and only the cluttered area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees, with frog’s eye view snapshots inside the clutter, only to the center of the clutter (altitude of 0.32 m) or shifted away from the nest (altitude of 0.45 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v3_fig11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig12" position="float" orientation="portrait" fig-type="figure">
<label>Figure 12.</label>
<caption><p>Brightness-based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with frog’s eye view snapshots taken outside the clutter (distance = 0.55 m, height = 0.02 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, one of them (memory 0) being the memory inside the model. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these image, where the local minima are shifted according the nest bearing. <bold>B:</bold> Heatmaps of full environment (<bold>B</bold>) and only the cluttered area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees, with frog’s eye view snapshots outside the clutter, only to the center of the clutter (altitude of 0.32 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v3_fig12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig13" position="float" orientation="portrait" fig-type="figure">
<label>Figure 13.</label>
<caption><p>Contrast-weighted nearness based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with bird’s eye snapshots taken outside the clutter (distance = 0.55 m, height = 0.45 m). <bold>A:</bold> We applied the model to a list of images, one of them (memory 0) being the memory inside the model. We observe that the image similarity as based on [<xref ref-type="bibr" rid="c13">13</xref>] is the maximum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local maxima for each of these image, where the local maxima are shifted according the nest bearing. <bold>B:</bold> Heatmaps of full environment (<bold>B</bold>) and only the cluttered area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image similarity between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees, with bird’s eye view snapshots outside the clutter, only to the center of the clutter (altitude of 0.32 m) or shifted away from the nest (altitude of 0.45 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v3_fig13.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig14" position="float" orientation="portrait" fig-type="figure">
<label>Figure 14.</label>
<caption><p>Contrast-weighted nearness based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with bird’s eye view snapshots taken inside the clutter (distance = 0.1 m, height = 0.45 m). <bold>A:</bold> We applied the model to a list of images, one of them (memory 0) being the memory inside the model. We observe that the image similarity as based on [<xref ref-type="bibr" rid="c13">13</xref>] is the maximum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local maxima for each of these image, where the local maxima are shifted according the nest bearing. <bold>B-D:</bold> Heatmaps of full environment (<bold>B</bold>) and only the cluttered area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image similarity between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees with bird’s eye view snapshots inside the clutter only to the center of the clutter (altitude of 0.45 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v3_fig14.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig15" position="float" orientation="portrait" fig-type="figure">
<label>Figure 15.</label>
<caption><p>Contrast-weighted nearness based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with frog’s eye view snapshots taken inside the clutter (distance = 0.1 m, height = 0.02 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, one of them (memory 0) being the memory inside the model. We observe that the image similarity as based on [<xref ref-type="bibr" rid="c13">13</xref>] is the maximum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local maxima for each of these images, where the local maxima are shifted according the nest bearing. <bold>B-D:</bold> Heatmaps of full environment (<bold>B</bold>) and only the cluttered area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image similarity between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees with frog’s eye view snapshots inside the clutter only to the center clutter (altitude of 0.45 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v3_fig15.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig16" position="float" orientation="portrait" fig-type="figure">
<label>Figure 16.</label>
<caption><p>Contrast-weighted nearness based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with frog’s eye view snapshots taken outside the clutter (distance = 0.55 m, height = 0.02 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, one of them (memory 0) being the memory inside the model. We observe that the image similarity as based on [<xref ref-type="bibr" rid="c13">13</xref>] is the maximum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local maxima for each of these images, where the local maxima are shifted according the nest bearing. <bold>B-D:</bold> Heatmaps of full environment (<bold>B</bold>) and only the cluttered area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image similarity between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees with frog’s eye view snapshots outside the clutter only to the center clutter (altitude of 0.32 m) or slightly shifted away from center (altitude of 0.45 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v3_fig16.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Frog’s eye views are sufficient for bees’ homing in clutter</title>
<p>Having shown that snapshot models perform best with bird’s eye views, we tested whether homing bees employed this strategy accordingly. Based on the model results, we hypothesised that bees should show the best homing performance when they can learn bird’s eye views and perform worse when only having access to frog’s eye view during learning. We first needed to assess their ability to return by using only visual cues provided by the clutter. Two groups of bees were trained to find their way in a cylindrical flight arena (with a diameter of 1.5 m and a height of 0.8 m) from the nest entrance to the foraging entrance and back (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). The group <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> was trained with unrestricted access to bird’s eye views (<italic>B</italic><sup>+</sup>) above the clutter and frog’s eye views ( <italic>F</italic><sup>+</sup>) within the clutter. The group <italic>B<sup>−</sup> F</italic><sup>+</sup> only had access to frog’s eye views within the clutter during the training period. To test if the bees associated the clutter with their nest location and did not use non-intentional visual cues, e.g., outside above the flight arena (even though we tried to avoid such cues), the clutter was shifted in the tests to another position in the arena, creating two possible nest locations: the true nest location in an external reference system as during the training condition and a visual nest location relative to the clutter (see Materials and Methods). Further, the ceiling of the arena was either placed at the height of the clutter, allowing only frog’s eye views (F), or high above the clutter (BF), allowing the bees to get both: frog’s and bird’s eye views.</p>
<p>In the tests BF and F where the bees had physical access to the clutter, they searched for their nest within the clutter. This search was in the vicinity of the visual nest entrance, though it was not always precisely centred at the nest entrance; instead, it showed some spatial spread around this location. A comparison of the time spent at the two possible nest entrance locations showed that the bees were able to find back to the visual nest in the clutter even when the clutter was shifted relative to an external reference frame (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). Furthermore, we obtained a higher search percentage at the visual nest in the clutter for the BF and F tests as well as for both groups trained to either <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> or <italic>B<sup>−</sup> F</italic><sup>+</sup> (<xref rid="fig3" ref-type="fig">Fig. 3</xref> and <xref rid="fig18" ref-type="fig">Fig. 18</xref>, statistical results in SI <xref rid="tbl1" ref-type="table">Table 1</xref>). Most time was spent within the arena close to the visual nest location. Still, the spatial distribution of search locations also shows high search percentages in two to three other areas near the nest location (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). These other search areas can be explained by locations similar-looking to the visual nest location between the objects as the simulations depicted a rather broad area with a high image similarity (<xref rid="fig1" ref-type="fig">Fig. 1</xref>, <xref rid="fig13" ref-type="fig">Fig. 13</xref> - <xref rid="fig14" ref-type="fig">14</xref> for the contrast-weighted nearness model). Both groups, <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> and <italic>B<sup>−</sup> F</italic><sup>+</sup>, showed similar search distribution during F and BF tests, indicating that the ceiling height does not change the bees’ search distributions (t-test results with Bonferroni-correction: SI <xref rid="tbl1" ref-type="table">Table 1</xref>). In conclusion, when the bees had physical access to the clutter during the test, i.e. they could fly between the objects in the BF and F test, they were able to find back the nest location within the clutter by using only frog’s eye views.</p>
<fig id="fig17" position="float" orientation="portrait" fig-type="figure">
<label>Figure 17.</label>
<caption><p>Four exemplary flight trajectories of the first outbound flight of bees in 3D (left subplot) and a top view in 2D (right subplot). The colour indicates the time, blue the start of the and red the end of the flight. The objects are depicted by red cylinders in the 3D plot and as red circles in the 2D plot. The black dots in the 2D plot shows the nest position within the clutter.</p></caption>
<graphic xlink:href="572344v3_fig17.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig18" position="float" orientation="portrait" fig-type="figure">
<label>Figure 18.</label>
<caption><p>Search distributions for the group <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> (left column) in the condition F (condiditon BF is shown in the main results in <xref rid="fig2" ref-type="fig">Fig.2D</xref>) and for group <italic>B<sup>−</sup> F</italic><sup>+</sup> (right column) in the tests BF and F (N = 26 for each plot). The bees of group <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> in the condition F searched for the nest inside the clutter and spent more time at the visual nest (in the clutter) than at the true nest (as during training). The bees of group <italic>B<sup>−</sup> F</italic><sup>+</sup> in the condition F and BF searched for the nest within the clutter and spent more time at the visual nest than at the true nest.</p></caption>
<graphic xlink:href="572344v3_fig18.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Statistical results of t-tests</title></caption>
<graphic xlink:href="572344v3_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2c">
<title>Bird’s eye views are not sufficient for bees to return</title>
<p>Based on the results of the simulations (<xref rid="fig1" ref-type="fig">Fig. 1</xref>), we hypothesised that bees could pinpoint their nest position by using only bird’s eye views. In addition, we observed that during the first outbound flights, bees quickly increased their altitude to fly above objects surrounding the nest entrance (<xref rid="fig17" ref-type="fig">Fig. 17</xref>). Therefore, we restricted the bees’ access to the clutter by a transparent cover around it, so they did not have access to the clutter from the side but only from above (see test B, <xref rid="fig2" ref-type="fig">Fig. 2</xref> A). In this test, the bees tried to enter the clutter sideways between the objects as the search distributions show (<xref rid="fig5" ref-type="fig">Fig. 5</xref> B&amp;D). The bees did not search at the nest location above the clutter (<xref rid="fig5" ref-type="fig">Fig. 5</xref> B&amp;D) which they could have done if they would have learned to enter the clutter from above. We can therefore reject the hypothesis that bees use bird’s eye view to return home in a cluttered environment. Rather frog’s eye views seem to be sufficient for homing. Nevertheless, an interesting aspect with respect to finding the nest hole is revealed by the entry positions into the clutter. Entry points from all three tests (B, BF and F for both groups) from the top and the side to clutter supported the finding that most bees tried to cross the boundary to the clutter from the side while only very few tried to cross the clutter from the top (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). The entry points from the side concentrate mainly on three locations indicating that the bees learned similar entrance points to the clutter (<xref rid="fig4" ref-type="fig">Fig. 4</xref> A). Taken together, these results show that the bees learned certain positions where they enter the clutter indicating that these positions might be used to store snapshots for the return.</p>
</sec>
<sec id="s2d">
<title>Flight altitude changes with training views</title>
<p>We investigated the influence of the potentially available views during training on the bees’ altitude during homing by comparing the employed flight altitudes. The flight altitude probability density distributions for the F and BF tests showed for both groups that the bees flew mostly very low searching for the nest entrance (<xref rid="fig19" ref-type="fig">Fig. 19</xref>). The distributions, however, looked very different for the test B. The flight altitude distribution of the group <italic>B<sup>−</sup> F</italic><sup>+</sup> shows two peaks, one around 130mm and one around 550mm (<xref rid="fig5" ref-type="fig">Fig. 5</xref> A). These peaks indicate that the bees either flew just below half the object height, probably trying to enter the covered clutter from the side or they flew close to the ceiling of the arena. The flights close to the ceiling might reflect exploratory behaviour in a section of the flight arena the bees could not experience before as they were trained to a ceiling constrained to the height of the objects. The behaviour of flying at a low altitude, trying to enter the clutter from the side, is fitting to the peaks of search time around the covered clutter (<xref rid="fig2" ref-type="fig">Fig. 2</xref> A&amp;B). The group <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> showed a broader probability density distribution indicating three shallower peaks, one at an altitude around 170mm, which refers to half the height of the objects, a second peak around 370mm, referring to just above the objects and a third peak just below the ceiling where the bees might have explored the boundaries of the arena. This indicates that the bees trained with <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> seemed to have learned to fly above the objects and used this information when the cover blocked the entrance to the clutter. The group <italic>B<sup>−</sup> F</italic><sup>+</sup>, constrained to fly only up to the top of the objects but not above, seemed to be unable to search for another way to enter the clutter from above. Although the bees exposed to <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> during training, did not search above the nest position, they seemed to have learned the height of the objects which the altitude-constrained group, <italic>B<sup>−</sup> F</italic><sup>+</sup>, could not learn because the bees flew more at the height of the objects. Overall, there is a clear difference in the flight altitude distributions between the two groups and between the tests. The group trained with <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> flew more at the height of the objects when the clutter was covered, while the group trained with a <italic>B<sup>−</sup> F</italic><sup>+</sup> seemed to just explore the newly available space at a very high altitude with flying little at the height of the objects. This indicates that the flight altitude during learning plays a role in what altitude the bees fly during the return flights, and that this could influence what aspects are learned.</p>
<fig id="fig19" position="float" orientation="portrait" fig-type="figure">
<label>Figure 19.</label>
<caption><p>Flight altitude distributions for the tests BF and F for the groups <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> and <italic>B<sup>−</sup> F</italic><sup>+</sup> (N = 26 for each plot).</p></caption>
<graphic xlink:href="572344v3_fig19.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig20" position="float" orientation="portrait" fig-type="figure">
<label>Figure 20.</label>
<caption><p>Exemplary flight trajectories in 3D (left column) and a top view in 2D (right column) from the group <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> in the B test. The colour indicates the time, blue the start of the and red the end of the flight. The objects are depicted by red cylinders in the 3D plot and as red circles in the 2D plot. The black dots in the 2D plot shows the nest position within the clutter. The bee is trying to enter the covered clutter from the side but, eventually, it is increasing its altitude and it flies above the clutter. However, the bee is not searching for the visual nest above the covered clutter.</p></caption>
<graphic xlink:href="572344v3_fig20.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We investigated how bumblebees find home in a cluttered environment using views at different altitudes. The results of simulations of snapshot-based homing models [<xref ref-type="bibr" rid="c51">51</xref>] suggested the best homing performance with bird’s eye views from above the clutter. Surprisingly, bees performed equally well whether or not they could experience bird’s eye views during training or just frog’s eye views, i.e. views taken close to the ground. Even bees trained with both frog’s and bird’s eye views, predominantly used frog’s eye views for homing. Thus, instead of solely relying on snapshots, we are suggesting a multi-step process for homing in clutter the bees could have used using a variety of tools from the navigational toolkit supported by the bees’ behaviour.</p>
<sec id="s3a">
<title>Working range of snapshot model is limited inside clutter</title>
<p>Snapshot matching is widely assumed to be one of the most prominent navigational strategies of central-place foraging insects [<xref ref-type="bibr" rid="c50">50</xref>]. In the simulations of this study, we found that bird’s eye view snapshots led to the best homing performance, while frog’s eye view snapshots could only find the clutter but not the nest position within the clutter. Additionally, the model steered only to the nest when the current views were above the objects (altitudes 0.32 m and 0.45 m). Previous studies showed that snapshots at higher altitudes result in larger catchment areas than snapshots close to the ground [<xref ref-type="bibr" rid="c32">32</xref>]. Furthermore, in a particular form of snapshot model, i.e. skyline snapshots, occluding objects lead to smaller catchment areas [<xref ref-type="bibr" rid="c33">33</xref>]. Our model simulations support both conclusions, as the bird’s eye views above the clutter were less occluded. Behavioural studies also confirm the advantage of higher altitude snapshots as it was found that honeybees and bumblebees may use ground features of the environment while navigating on a large spatial scale (i.e. of hundreds of metres) [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c3">3</xref>]. A modelling and robotic approach of Stankiewicz and Webb [<xref ref-type="bibr" rid="c40">40</xref>] could also confirm these findings. Overall, our model analysis could show that snapshot models are not able to find home with views within a cluttered environment but only with views from above it.</p>
</sec>
<sec id="s3b">
<title>Bees’ homing performance in a cluttered environment</title>
<p>To our great surprise, we found that the behavioural performance of bumblebees differed quite substantially from our snapshot-based model predictions, when we tested bees either trained with bird’s and frog’s eye views or trained only with frog’s eye views. Both groups of bees performed equally well in finding the nest position between the cluttered objects. However, bees that could experience heights above the objects during training also used bird’s eye views during homing to fly at the altitude of the objects. This finding indicates that they learned to fly above and find other possibilities to enter the clutter. From a flight control perspective, changing between flying up and down could be energetically more demanding than between left and right. Large-scale studies (e.g. [<xref ref-type="bibr" rid="c30">30</xref>]) state that bees fly more in a plane than up/down. Moreover, from a flight control perspective, it might be difficult for the bees to approach the clutter from above and then descend to the nest between the objects. This hypothesis is supported by a study showing that bees prefer to avoid flying above obstacles at short distances when they have the choice to fly around [<xref ref-type="bibr" rid="c44">44</xref>].</p>
</sec>
<sec id="s3c">
<title>A multi-step process as a hypothetical homing mechanism in clutter</title>
<p>Since snapshot models could not explain the bees’ homing behaviour in clutter, we propose an alternative explanatory hypothesis for the bees’ behaviour by reloading a path integration vector as described by Webb [<xref ref-type="bibr" rid="c45">45</xref>]. This alternative suggests that the bee can reload a memorised home vector triggered by external stimuli, such as visual cues, to navigate back home. In our study, a standard path integration vector of a bee entering the arena to return home (as illustrated by the bee’s position in <xref rid="fig6" ref-type="fig">Fig. 6</xref>) would point toward the true nest hole at the training position in the flight arena (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, indicated by the black arrow). Following the conceptual framework proposed by Webb [<xref ref-type="bibr" rid="c45">45</xref>], a vector memory could be triggered by the presence of the cluttered objects around the nest entrance as a visual cue (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, depicted by the light green arrow). In the training situation, both vectors were congruent, pointing in the same direction. However, they would point in different directions in the test situation, as the clutter and, thus, the visual nest hole were displaced within the flight arena. Given that the cluttered objects were the most salient cues within the cylindrical environment, it is plausible that the bees relied on them as robust cues for homing. Based on the association between the clutter and the nest, this vector could be used by bees that learned to use the shortest path between the entrance to the flight arena and the nest to enter the clutter. The behavioural findings support the use of the shortest path by a clear peak in the entrance position around the clutter in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. However, we also found two other entry positions that many bees used, which could be explained by locations the bees memorised to enter the clutter. These memories might be triggered by visual cues, e.g. a memorised snapshot, at the border of the clutter (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, denoted by the yellow arrow and could be used to refine the coarse vector of the clutter vector memory. As the bee traverses along the clutter’s border, it eventually reaches a location where it learned to enter the clutter during training. This hypothesis is supported by the observed similarity in entry points into the clutter exhibited by the bees (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). Once within the clutter, the bee might utilise the distance between the nest and the wall as a reference, coupled with the direction memorised at the entry point, to locate the nest (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, represented by the blue arrow). This hypothetical multi-step process underlines the complexity of the bees’ navigation strategy in cluttered environments, involving the integration of various environmental cues and memories to achieve precise homing.</p>
</sec>
<sec id="s3d">
<title>Outlook</title>
<p>Our study underscores the limitations inherent in snapshot models, revealing their inability to provide precise positional estimates within densely cluttered environments, especially when compared to the navi-gational abilities of bees using frog’s-eye views. Notably, bees trained with bird’s-eye views demonstrated adaptability in spatially constrained situations, although this strategy was not employed explicitly for searching nests above clutter. Future research should extend these findings on a larger scale and explore the development of 3D snapshot models that account for altitude variations. Furthermore, these insights extend beyond bees and may have implications for other species, like birds with altitude fluctuations during foraging or nesting [<xref ref-type="bibr" rid="c4">4</xref>]. The use of bird’s-eye views in densely cluttered forests, akin to our findings with bees, prompts consideration of similar behaviours in other flying animals, but also for walking animals, such as ants such navigating varied vegetation heights [<xref ref-type="bibr" rid="c17">17</xref>]. Switching views might considerably affect the ability of animals to solve spatial problems, as shown in humans [<xref ref-type="bibr" rid="c21">21</xref>], and understanding how best to combine these information for navigation for navigation will no doubt benefit the development autonomously flying robots, for instance to help them navigating in cluttered environments.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>View-based homing models</title>
<p>We rendered an environment with 40 randomly placed red cylinders, creating an artificial meadow (clutter), surrounding the nest entrance placed on a floor with a black and white pattern with a 1/f spatial frequency distribution (a distribution observed in nature [<xref ref-type="bibr" rid="c39">39</xref>]) in a graphic software (Blender, 2.82a). A panoramic image was taken on a 1 cm spaced grid along the x-axis of the arena. The arena was 1.5 m in diameter like for the following behavioural experiments. For the rendering we focused only on relevant cues like the objects and the patterned floor. We did not render the nest hole which was visually covered during the behavioural experiments. In addition, the arena wall was also not rendered as it did not contain any information for bees other than for flight control and simulations with a wall led to similar results (data not shown). The grid-spacing was used at four altitudes: 2 cm for walking or hovering bees, 15 cm for bees flying half of the object height, 32 cm for bees flying just above the objects and 45 cm for bees flying very high to compare how the calculated similarity changes regarding the altitude. In the next step, eight images around the nest were taken and stored as memorised snapshots, and the current views were compared (examples in <xref rid="fig7" ref-type="fig">Fig. 7</xref>-<xref rid="fig8" ref-type="fig">8</xref>).</p>
<p>We used the method presented in Doussot et al. [<xref ref-type="bibr" rid="c14">14</xref>] to calculate the image differences according to the brightness and the contrast-weighted-nearness method, each with eight snapshots. The image difference was calculated either by the brightness method relying on the brightness value of each pixel in one panoramic image or the contrast-weighted nearness method (see SI Methods) relying on the contrast calculated by the Michelson contrast (ratio of the luminance-amplitude (<italic>I<sub>max</sub></italic> − <italic>I<sub>min</sub></italic>)) and the luminance-background (<italic>I<sub>max</sub></italic> + <italic>I<sub>min</sub></italic>)) weighted by the inverse of the distance [<xref ref-type="bibr" rid="c13">13</xref>]. We wanted to predict the simulated bee’s probable endpoint by analyzing vector fields from different homing models. Convergence points in these vector fields were determined using the Helmholtz-Hodge Decomposition, focusing on the curl-free component, representing the potential <italic>ϕ</italic> ([<xref ref-type="bibr" rid="c14">14</xref>]). After applying the Helmholtz-Hodge Decomposition to each vector field, we scaled the resulting potential between 0 and 1 for all homing models. With these potentials, we were able to plot heatmaps accordingly to estimate the areas in the arena where view-based agents/ simulated bees would most likely steer towards. These heatmaps were compared to the bees’ search distributions in the arena.</p>
</sec>
<sec id="s4b">
<title>Animal handling</title>
<p>We used four <italic>B. terrestris</italic> hives provided by Koppert B.V., The Netherlands, that were ordered sequentially to test one colony at a time. The bee hives arrived in small boxes and were transferred to acrylic nest boxes under red light (non-visible to bees [<xref ref-type="bibr" rid="c15">15</xref>]) to 30 × 30 × 30 cm. The nest box was placed on the floor and connected to an experimental arena. We covered the nest box with a black cloth to mimic the natural lighting of underground habitats of <italic>B. terrestris</italic> [<xref ref-type="bibr" rid="c19">19</xref>]. The bee colonies were provided with pollen balls <italic>ad libitum</italic> directly within the nest boxes. The pollen balls were made of 50 mL ground, commercial pollen collected by honeybees (W. Seip, Germany), and 10 mL water. The bees reached a foraging chamber via the experimental arena containing gravity feeders. These are small bottles with a plate at the bottom so that the bees can access the sugar solution through small slots in the plate. The feeders were filled with a sweet aqueous solution (30% saccharose, 70% water in volume). Lighting from above was provided in a 12 h/12 h cycle and the temperature was kept constantly at 20<italic><sup>◦</sup></italic>. Throughout the experiments, foraging bees were individually marked using coloured, numbered plastic tags glued with melted resin on their thorax.</p>
</sec>
<sec id="s4c">
<title>Experimental arena</title>
<p>The experimental arena was a cylinder with a diameter of 1.5 m and a height of 80 cm as in [<xref ref-type="bibr" rid="c14">14</xref>]. A red and white pattern, perceived black and white by bumblebees [<xref ref-type="bibr" rid="c15">15</xref>], with a 1/f spatial frequency distribution (a distribution observed in nature [<xref ref-type="bibr" rid="c39">39</xref>]) covered the wall and floor of the cylindrical arena; the bees were provided with enough contrast to allow them to use optical flow. Red light came from 18 neon tubes (36 W Osram + Triconic) filtered by a red acrylic plate (Antiflex ac red 1600 ttv). Bees did not see these lights and perceived the red-white pattern as black and white. An adjustable transparent ceiling prevented the bees from exiting the arena. It allowed lighting from 8 neon tubes (52 W Osram + Triconic) and 8 LEDs (5 W GreenLED) as in [<xref ref-type="bibr" rid="c14">14</xref>], and recording from five high-speed cameras with different viewing angles. A foraging bee exiting the hive crossed three small acrylic boxes (inner dimension of 8 <italic>×</italic> 8<italic>×</italic> 8 cm) with closable exits to select the bee to be tested. The bee then walked through a plastic tube with 2.5 cm in diameter and entered the cylindrical arena. This nest exit was surrounded by visual objects. The foraging chamber was reached through a hole at a height of 28 cm above the floor in the arena wall. After foraging the bees exited the foraging chamber and entered the flight arena through a hole in the arena wall at a height of 28 cm above the floor. The nest entrance within the arena was surrounded by visual objects. When it found this nest entrance, the bee walked through a plastic tube with 2.5 cm in diameter and reached the hive by crossing three small acrylic boxes (inner dimension of 8 <italic>×</italic> 8<italic>×</italic> 8 cm) with closable exits which were used during the experiments to block bees from entering the arena. The objects surrounding the nest exit in the arena consisted of 40 randomly placed red cylinders (2 cm in diameter and 30 cm in height), creating an artificial meadow. The size of the artificial meadow was 80 cm in diameter to allow its displacement to a different location in the flight arena. We considered that the artificial meadow was dense enough to pose a challenge in finding the nest within the cylinder constellations, as we could show in the snapshot-model comparison. For example, if only three cylinders were used, the bee may only search there because these would be the only conspicuous landmarks. Additionally, the configuration had to be sufficiently sparse for the bee to fly through [<xref ref-type="bibr" rid="c37">37</xref>]. We used the object density and object distances of Gonsek et al. [<xref ref-type="bibr" rid="c18">18</xref>] as a reference to find a randomly distributed object configuration. Red-lighting from below was used only during recordings. The cylindrical arena had a door, allowing the experimenter to change the objects within the arena.</p>
</sec>
<sec id="s4d">
<title>Experimental design</title>
<p>We tested two groups of bees in three tests trained with the objects of the artificial meadow surrounding the nest entrance. The two groups differed in the training condition. For the group <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup>, the flight altitude was unconstrained, and they could experience bird’s eye views above the objects. The second group, <italic>B<sup>−</sup> F</italic><sup>+</sup>, were restricted during training to a maximum flight altitude of the height of the objects, so the frog’s eye views.</p>
<p>We tested 26 bees per group (4 colonies were used, 2 per group, from each colony 13 bees are included in the analysis resulting in a total of 52 individuals). In both groups, the foraging bees travelled between their nest and the foraging chamber. The return flight of each bee was recorded in all tests of a given experiment. Between individual tests, the bees were allowed to forage <italic>ad libitum</italic>. An artificial meadow surrounded the nest, and the area around the nest positions was cleaned with 70% ethanol between the tests to avoid chemical markings.</p>
<p>To test the behaviour of the bees, we locked up to six individually marked bees in the foraging chamber at a time. Each bee participated either in group <italic>B</italic><sup>+</sup> <italic>F</italic><sup>+</sup> (high ceiling during training, resulting in available frog’s and bird’s eye views) or <italic>B<sup>−</sup> F</italic><sup>+</sup> (low ceiling during training, resulting in only available frog’s eye views) and was tested once in all tests of the respective experiment. The order of the tests was pseudo-random only that in the group <italic>B<sup>−</sup> F</italic><sup>+</sup> the test BF was always tested last to not let the bees experience bird’s eye views before testing the B test. Before the tests, the cylindrical arena was emptied of bees, the spatial arrangement of objects was shifted, and the nest entrance closed. One bee at a time was allowed to search for its home for three minutes after take-off. After this time, the spatial arrangement of the ceiling height and the artificial meadow was placed back in the training condition, and the nest entrance opened. The bees had up to two minutes to take off. Otherwise, they were captured and released close to the nest. Between tests, the bees could fly <italic>ad libitum</italic> between the nest and foraging chamber under the training conditions.</p>
<p>For the tests, the artificial meadow was placed at a different location than during the training condition, and thus, it did not surround the true nest entrance leading to the hive. The artificial meadow indicated the location of a visual nest entrance. The true nest entrance and the visual nest entrance were covered by a piece of paper with the same texture as the arena floor so that they were not discernible by the bees. For the test BF, no other constraint was added to the general tests to test if the bees associated their nest entrance with the artificial meadow. The F test consisted of a transparent wall and ceiling on top of the objects, which prevented the bees from entering the meadow. To return to the nest location in the meadow, they were only able to pinpoint the position from above if they were using only bird’s eye views. In the F test, the flight altitude was constrained to the height of the artificial meadow so that bees could no longer experience bird’s eye views from above the meadow. Thus, the bees had to use the frog’s eye view to return to their nest.</p>
</sec>
<sec id="s4e">
<title>Flight trajectories</title>
<p>Bee trajectories were recorded at 62.5 Hz (16 ms between two consecutive frames) with five synchronised Basler cameras (Basler acA 2040um-NIR) with different viewing angles (similar to [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c34">34</xref>]). One camera was placed on top of the middle of the arena to track the bumblebees’ movements in the plane, and the other four cameras were distributed around the arena to record the position of the bees from different angles to minimise the error during triangulation of the bee’s position in 3D. Before the bees entered the setup, the recording had already started, and the first 60 frames were used to calculate a bee-less image of the arena (background image). During the rest of the recording, only crops, i.e. image sections (40x40 pixels), containing large differences between the background image and the current image (i.e. potentially containing the bee) were saved to the hard drive together with the position of the crop in the image. The recording scripts were written in C++. The image crops were analysed by a custom-written neural network to classify the crops in bees or not a bee. When non-biological speed (speed above 4 m/s [<xref ref-type="bibr" rid="c19">19</xref>]) or implausible positions (outside the arena) were observed, the crops neighbouring these time points were manually reviewed.</p>
<p>The trajectories were analysed in Python (version 3.8.17), notably with OpenCV. A detailed list of packages used is published in the data publication. The time course of the positions of the bees in 3D within the arena is shown for a selection of flights (<xref rid="fig2" ref-type="fig">Fig. 2</xref> and S13). For each of the tests, a distribution of presence in the flight arena was computed by using hexagonal binning of the 2D positions to show the search areas of the bees qualitatively. Bees that collected food in the foraging chamber returned home and searched for their nest entrance. Even when objects are displaced to a novel location [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c28">28</xref>], replaced by smaller, differently coloured, or camouflaged objects [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c13">13</xref>], or moved to create visual conflict between several visual features [<xref ref-type="bibr" rid="c14">14</xref>], bees search for their nest entrance associated to such objects. Therefore, we assumed that bees entering the arena after visiting a foraging chamber will search for their nest location. The returning bees could spend an equal amount of time in any location in the arena or concentrate their search around the physical position of their nest or around visual objects associated with the nest entrance [<xref ref-type="bibr" rid="c14">14</xref>]. In our experiments, the nest entrance was surrounded by cylindrical objects. During the tests, when the objects were shifted to a novel location, a bee guided by the objects might have searched for its nest around the objects. In contrast, they might not have used the objects to navigate and might have searched for their nest at the original location. We, therefore, quantified the time spent around two locations: at the true nest and the nest according to the objects (‘visual nest’). Additionally, the positions of the bees crossing the boundaries of the cluttered objects for the first time at the side or at the top of the clutter were visualised a circular histogram (entries from the top) and a scatter plot (entries from the side). These were used to describe where the bees entered the cluttered area. As the clutter was shifted to another position in the arena, we can exclude the use of compass, odour or magnetic cues.</p>
</sec>
<sec id="s4f">
<title>Statistical analysis for hypotheses testing</title>
<p>Hypotheses about the time spent in one area compared to another were tested using the dependent t-test for paired samples. Hypotheses involving multiple comparisons were tested using a Bonferroni correction for the significance level. As long as a hypothesis concerned only two areas, no adjustment was made to the significance level. With a sample size of 26 bees, we were able to detect a time difference spent in two areas (e.g. fictive and true nest locations) of 0.25 s assuming a standard deviation of 0.38 s (estimated from [<xref ref-type="bibr" rid="c14">14</xref>]) with a power of 80% at a significance level of 0.05. The analysis was performed with Python using the Scipy library for statistical analyses.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We would like to thank Vedant Dixit, Maximilian Stahlsmeier, Pia Hippel and Helene Schnellenberg for their help during the data collection. Additionally, we would like to thank Sina Mews for helpful discussions on statistical models. This project was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) and the Agence Nationale de la Recherche (ANR, French National Research Agency). We also acknowledge support for the publication costs by the Open Access Publication Fund of Bielefeld University.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Ardin</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Peng</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Lagogiannis</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Webb</surname></string-name>. <article-title>Using an insect mushroom body circuit to encode route memory in complex natural environments</article-title>. <source>PLoS Computational Biology</source>, <volume>12</volume>(<issue>2</issue>), <month>feb</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Baddeley</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Husbands</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Philippides</surname></string-name>. <article-title>A model of ant route navigation driven by scene familiarity</article-title>. <source>PLoS Computational Biology</source>, <volume>8</volume>(<issue>1</issue>), <month>jan</month> <year>2012</year>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><given-names>J. S.</given-names> <surname>Brebner</surname></string-name>, <string-name><given-names>J. C.</given-names> <surname>Makinson</surname></string-name>, <string-name><given-names>O. K.</given-names> <surname>Bates</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Rossi</surname></string-name>, <string-name><given-names>K. S.</given-names> <surname>Lim</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Dubois</surname></string-name>, <collab>T. Gómez-Moracho, M. Lihoreau, L. Chittka, and J. L. Woodgate</collab>. <article-title>Bumble bees strategically use ground level linear features in navigation</article-title>. <source>Animal Behaviour</source>, <volume>179</volume>, Sept. <year>2021</year>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Bruderer</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Peter</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>Korner-Nievergelt</surname></string-name>. <article-title>Vertical distribution of bird migration between the Baltic Sea and the Sahara</article-title>. <source>Journal of Ornithology</source>, <volume>159</volume>(<issue>2</issue>), <month>Apr.</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Buehlmann</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name>. <article-title>Multimodal interactions in insect navigation</article-title>. <source>Animal Cognition</source>, <volume>23</volume>(<issue>6</issue>), <month>Nov.</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><given-names>B. A.</given-names> <surname>Cartwright</surname></string-name> and <string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name>. <article-title>Landmark learning in bees</article-title>. <source>Journal of Comparative Physiology A</source>, <volume>151</volume>(<issue>4</issue>), <month>Dec.</month> <year>1983</year>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Cheung</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Collett</surname></string-name>, <string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Dewar</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Dyer</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Narendra</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Philippides</surname></string-name>, W. Stürzl, B. Web, A. Wystrach, and J. Zeil. <article-title>Still no convincing evidence for cognitive map use by honeybees</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>111</volume>(<issue>42</issue>), <month>oct</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name>, <string-name><given-names>S. N.</given-names> <surname>Fry</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Wehner</surname></string-name>. <article-title>Sequence learning by honeybees</article-title>. <source>Journal of Comparative Physiology A</source>, <volume>172</volume>(<issue>6</issue>), June <year>1993</year>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name>. <article-title>Insect navigation: Do honeybees learn to follow highways?</article-title> <source>Current Biology</source>, <volume>25</volume>(<issue>6</issue>), <month>mar</month> <year>2015</year>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name> and N. Hempel de Ibarra. <article-title>An ‘instinct for learning’: the learning flights and walks of bees, wasps and ants from the 1850s to now</article-title>. <source>Journal of Experimental Biology</source>, <volume>226</volume>(<issue>6</issue>), <month>Apr.</month> <year>2023</year>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Degen</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kirbach</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Reiter</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Lehmann</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Norton</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Storms</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Koblofsky</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Winter</surname></string-name>, <string-name><given-names>P. B.</given-names> <surname>Georgieva</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Chamkhi</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Meyer</surname></string-name>, <string-name><given-names>P. K.</given-names> <surname>Singh</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Manz</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Greggers</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Menzel</surname></string-name>. <article-title>Honeybees learn landscape features during exploratory orientation flights</article-title>. <source>Current Biology</source>, <volume>26</volume>(<issue>20</issue>), <month>oct</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><given-names>A. D.</given-names> <surname>Dewar</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Philippides</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name>. <article-title>What is the relationship between visual environment and the form of ant learning-walks? An in silico investigation of insect navigation</article-title>. <source>Adaptive Behavior</source>, <volume>22</volume>(<issue>3</issue>), June <year>2014</year>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Dittmar</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Stuerzl</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Baird</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Boeddeker</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name>. <article-title>Goal seeking in honeybees: Matching of optic flow snapshots?</article-title> <source>Journal of Experimental Biology</source>, <volume>213</volume>, <year>2010</year>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Doussot</surname></string-name>, <string-name><given-names>O. J. N.</given-names> <surname>Bertrand</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name>. <article-title>Visually guided homing of bumblebees in ambiguous situations: A behavioural and modelling study</article-title>. <source>PLOS Computational Biology</source>, <volume>16</volume>(<issue>10</issue>), <month>oct</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><given-names>A. G.</given-names> <surname>Dyer</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Chittka</surname></string-name>. <article-title>Biological significance of distinguishing between similar colours in spectrally variable illumination: Bumblebees (Bombus terrestris) as a case study</article-title>. <source>Journal of Comparative Physiology A: Neuroethology, Sensory, Neural, and Behavioral Physiology</source>, <volume>190</volume>(<issue>2</issue>), <month>feb</month> <year>2004</year>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Finkelstein</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Las</surname></string-name>, and <string-name><given-names>N.</given-names> <surname>Ulanovsky</surname></string-name>. <article-title>3-D Maps and Compasses in the Brain</article-title>. <source>Annual Review of Neuroscience</source>, <volume>39</volume>, <year>2016</year>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><given-names>C. A.</given-names> <surname>Freas</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Narendra</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Cheng</surname></string-name>. <article-title>The view from the trees: Nocturnal bull ants, Myrmecia midas, use the surrounding panorama while descending from trees</article-title>. <source>Frontiers in Psychology</source>, <volume>9</volume>(<issue>JAN</issue>), <month>Jan.</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Gonsek</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Jeschke</surname></string-name>, S. Rönnau, and O. J. Bertrand. <article-title>From Paths to Routes: A Method for Path Classification</article-title>. <source>Frontiers in Behavioral Neuroscience</source>, <volume>14</volume>, <month>jan</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="book"><string-name><given-names>D</given-names>. <surname>Goulson</surname></string-name>. <source>Bumblebees: behaviour, ecology, and conservation</source>. <publisher-name>Oxford University Press</publisher-name>, <year>2010</year>. </mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><given-names>R. M.</given-names> <surname>Grieves</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Jedidi-Ayoub</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Mishchanchuk</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Renaudineau</surname></string-name>, and <string-name><given-names>K. J.</given-names> <surname>Jeffery</surname></string-name>. <article-title>The place-cell representation of volumetric space in rats</article-title>. <source>Nature Communications</source>, <volume>11</volume>(<issue>1</issue>), <month>Dec.</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Haxhimusa</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Carpenter</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Catrambone</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Foldes</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Stefanov</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Arns</surname></string-name>, and <string-name><given-names>Z.</given-names> <surname>Pizlo</surname></string-name>. <article-title>2D and 3D Traveling Salesman Problem</article-title>. <source>The Journal of Problem Solving</source>, <volume>3</volume>(<issue>2</issue>), <month>Feb.</month> <year>2011</year>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><given-names>J. A.</given-names> <surname>Helms</surname></string-name>, <string-name><given-names>A. P.</given-names> <surname>Godfrey</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Ames</surname></string-name>, and <string-name><given-names>E. S.</given-names> <surname>Bridge</surname></string-name>. <article-title>Predator foraging altitudes reveal the structure of aerial insect communities</article-title>. <source>Scientific Reports</source>, <volume>6</volume>(<issue>1</issue>), June <year>2016</year>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Khuong</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Gautrais</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Perna</surname></string-name>, C. Sbäı, M. Combe, <string-name><given-names>P.</given-names> <surname>Kuntz</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Jost</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Theraulaz</surname></string-name>. <article-title>Stigmergic construction and topochemical information shape ant nest architecture</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>(<issue>5</issue>), <month>Feb.</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="confproc"><string-name><given-names>A.</given-names> <surname>Kodzhabashev</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>. <article-title>Route following without scanning</article-title>. <conf-name>Conference on Biomimetic and Biohybrid Systems</conf-name>. <year>2015</year>. <pub-id pub-id-type="doi">10.1007/978-3-319-22979-9_20</pub-id></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Le Möel</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name>. <article-title>Opponent processes in visual memories: A model of attraction and repulsion in navigating insects’ mushroom bodies</article-title>. <source>PLoS Computational Biology</source>, <volume>16</volume>(<issue>2</issue>), <year>2020</year>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Lehrer</surname></string-name>. <article-title>Why do bees turn back and look?</article-title> <source>Journal of Comparative Physiology A</source>, <volume>172</volume>(<issue>5</issue>), <month>may</month> <year>1993</year>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Lehrer</surname></string-name> and <string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name>. <article-title>Approaching and departing bees learn different cues to the distance of a landmark</article-title>. <source>Journal of Comparative Physiology A</source>, <volume>175</volume>(<issue>2</issue>), <month>aug</month> <year>1994</year>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="thesis"><string-name><given-names>A.</given-names> <surname>Lobecke</surname></string-name>. <article-title>Local homing of the bumblebee, Bombus terrestris</article-title>. <publisher-name>University of Bielefeld</publisher-name>, <publisher-loc>Bielefeld, Germany</publisher-loc>, <year>2018</year>. <pub-id pub-id-type="doi">10.4119/unibi/2930455</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Lobecke</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Kern</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name>. <article-title>Taking a goal-centred dynamic snapshot as a possibility for local homing in initially näıve bumblebees</article-title>. <source>Journal of Experimental Biology</source>, <volume>221</volume>(<issue>2</issue>), <month>jan</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Menzel</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Tison</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Fischer-Nakai</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Cheeseman</surname></string-name>, <string-name><given-names>M. S.</given-names> <surname>Balbuena</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>T</given-names>. <surname>Landgraf</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Petrasch</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Polster</surname></string-name>, and <string-name><given-names>U.</given-names> <surname>Greggers</surname></string-name>. <article-title>Guidance of navigating honeybees by learned elongated ground structures</article-title>. <source>Frontiers in Behavioral Neuroscience</source>, <volume>12</volume>:<issue>322</issue>, <month>jan</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><collab>R. Möller</collab>. <article-title>A model of ant navigation based on visual prediction</article-title>. <source>Journal of Theoretical Biology</source>, <volume>305</volume>, <month>jul</month> <year>2012</year>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Murray</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Zeil</surname></string-name>. <article-title>Quantifying navigational information: The catchment volumes of panoramic snapshots in outdoor scenes</article-title>. <source>PLoS One</source>, <volume>12</volume>(<issue>10</issue>), <month>Oct.</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><given-names>M. M.</given-names> <surname>Müller</surname></string-name>, <string-name><given-names>O. J. N.</given-names> <surname>Bertrand</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Differt</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name>. <article-title>The problem of home choice in skyline-based homing</article-title>. <source>PLoS One</source>, <volume>13</volume>(<issue>3</issue>), <month>Mar.</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Odenthal</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Doussot</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Meyer</surname></string-name>, and <string-name><given-names>O. J. N.</given-names> <surname>Bertrand</surname></string-name>. <article-title>Analysing Head-Thorax Choreography During Free-Flights in Bumblebees</article-title>. <source>Frontiers in Behavioral Neuroscience</source>, <volume>14</volume>, <year>2021</year>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><given-names>J. L.</given-names> <surname>Osborne</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Smith</surname></string-name>, <string-name><given-names>S. J.</given-names> <surname>Clark</surname></string-name>, <string-name><given-names>D. R.</given-names> <surname>Reynolds</surname></string-name>, <string-name><given-names>M. C.</given-names> <surname>Barron</surname></string-name>, <string-name><given-names>K. S.</given-names> <surname>Lim</surname></string-name>, and <string-name><given-names>A. M.</given-names> <surname>Reynolds</surname></string-name>. <article-title>The ontogeny of bumblebee flight trajectories: From Näıve explorers to experienced foragers</article-title>. <source>PLoS One</source>, <volume>8</volume>(<issue>11</issue>), <month>Nov.</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="confproc"><string-name><given-names>A.</given-names> <surname>Philippides</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Steadman</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Dewar</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Walker</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name>. <article-title>Insect-inspired visual navigation for flying robots</article-title>, <conf-name>Conference on Biomimetic and Biohybrid Systems</conf-name>, <conf-loc>Edinburgh, United Kingdom</conf-loc>, <year>2016</year>. <pub-id pub-id-type="doi">10.1007/978-3-319-42417-0_24</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Ravi</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Siesenop</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Bertrand</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Doussot</surname></string-name>, <string-name><given-names>W. H.</given-names> <surname>Warren</surname></string-name>, <string-name><given-names>S. A.</given-names> <surname>Combes</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name>. <article-title>Bumblebees perceive the spatial layout of their environment in relation to their body size and form to minimize inflight collisions</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>117</volume>(<issue>49</issue>), <month>Dec.</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Robert</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Frasnelli</surname></string-name>, <string-name><given-names>N. H.</given-names> <surname>De Ibarra</surname></string-name>, and <string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name>. <article-title>Variations on a theme: Bumblebee learning flights from the nest and from flowers</article-title>. <source>Journal of Experimental Biology</source>, <volume>221</volume>(<issue>4</issue>), <month>feb</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Schwegmann</surname></string-name>, <string-name><given-names>J. P.</given-names> <surname>Lindemann</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name>. <article-title>Temporal Statistics of Natural Image Sequences Generated by Movements with Insect Flight Characteristics</article-title>. <source>PLoS One</source>, <volume>9</volume>(<issue>10</issue>), <month>oct</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Stankiewicz</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Webb</surname></string-name>. <article-title>Looking down: a model for visual route following in flying insects</article-title>. <source>Bioinspiration &amp; Biomimetics</source>, <volume>16</volume>(<issue>5</issue>), Sept. <year>2021</year>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Stone</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Webb</surname></string-name>. <article-title>Rotation invariant visual processing for spatial memory in insects</article-title>. <source>Interface Focus</source>, <volume>8</volume>(<issue>4</issue>), <month>aug</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><collab>W. Stürzl, J. Zeil, N. Boeddeker, and J. M. Hemmi</collab>. <article-title>How wasps acquire and use views for homing</article-title>. <source>Current Biology</source>, <volume>26</volume>(<issue>4</issue>), <month>Feb.</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><given-names>X.</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Yue</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>. <article-title>A decentralised neural model explaining optimal integration of navigational strategies in insects</article-title>. <source>eLife</source>, <volume>9</volume>, June <year>2020</year>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Thoma</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Fisher</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Bertrand</surname></string-name>, and <string-name><given-names>C.</given-names> <surname>Braun</surname></string-name>. <article-title>Evaluation of possible flight strategies for close object evasion from bumblebee experiments. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), volume 12413 LNAI</article-title>. <source>Springer Science and Business Media Deutschland GmbH</source>, <year>2021</year>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Webb</surname></string-name>. <article-title>The internal maps of insects</article-title>. <source>Journal of Experimental Biology</source>, <volume>222</volume>, <year>2019</year>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Wehner</surname></string-name>. <article-title>The architecture of the desert ant’s navigational toolkit (hymenoptera: Formicidae)</article-title>. <source>Myrmecological News</source>, <volume>12</volume>, <year>2009</year>. </mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><given-names>J. L.</given-names> <surname>Woodgate</surname></string-name>, <string-name><given-names>J. C.</given-names> <surname>Makinson</surname></string-name>, <string-name><given-names>K. S.</given-names> <surname>Lim</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Reynolds</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Chittka</surname></string-name>. <article-title>Life-long radar tracking of bumblebees</article-title>. <source>PLoS One</source>, <volume>11</volume>(<issue>8</issue>), <month>Aug.</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Philippides</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name>. <article-title>Snapshots in ants? New interpretations of paradigmatic experiments</article-title>. <source>Journal of Experimental Biology</source>, <volume>216</volume>(<issue>10</issue>), <month>may</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Schwarz</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Schultheiss</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Beugnon</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Cheng</surname></string-name>. <article-title>Views, landmarks, and routes: How do desert ants negotiate an obstacle course? Journal of Comparative Physiology A: Neuroethology</article-title>, <source>Sensory, Neural, and Behavioral Physiology</source>, <volume>197</volume>, <year>2011</year>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Zeil</surname></string-name>. <article-title>Visual homing: an insect perspective</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>22</volume>(<issue>2</issue>), <month>Apr.</month> <year>2012</year>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Zeil</surname></string-name>. <article-title>Visual navigation: properties, acquisition and use of views</article-title>. <source>Journal of Comparative Physiology A</source>, <volume>209</volume>, <month>Dec.</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Zeil</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Hofmann</surname></string-name>, and <string-name><given-names>J. S.</given-names> <surname>Chahl</surname></string-name>. <article-title>Catchment areas of panoramic snapshots in outdoor scenes</article-title>. <source>Journal of the Optical Society of America A</source>, <volume>20</volume>(<issue>3</issue>), <month>Mar.</month> <year>2003</year>.</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Supporting Information</title>
<sec id="s5aa">
<title>Methods</title>
<sec id="s5a">
<title>Rotational Image Difference Functions</title>
<p>For each position (<italic>x, y</italic>) in the arena an equi-rectangular panoramic image (360deg along the azimuth, 180 deg along the elevation) was acquired. To determine the most familiar direction, each snapshot in the arena were compared to memorised snapshot. This comparison was based on the minimum rotational image difference function (RIDF, <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref>) for the brightness model. The minimum RIDF is the minimum root mean squared image difference <italic>d<sub>x,y</sub></italic> between two views (the current view <italic>I<sub>x,y</sub></italic> and the view at the nest <italic>I<sub>N</sub></italic>) for different azimuthal viewing directions <italic>α</italic> weighted by <italic>w</italic>(<italic>v</italic>). <italic>w</italic>(<italic>v</italic>) is a sine wave along the y-axis counterbalancing the oversampling of the poles at the transformation of the 3D sphere mimicking the bee’s eye to 2D equirectangular images by giving values of 1 at the equator and 0 at the poles (<xref rid="eqn2" ref-type="disp-formula">Eq. 2</xref>). In the equation below, (<italic>u, v</italic>) corresponds to the viewing direction in the azimuthal direction u, and direction along the elevation <italic>v</italic>. The images resolution were (<italic>N<sub>u</sub></italic>, <italic>N<sub>v</sub></italic>) = (360, 180) pixels.</p>
<disp-formula id="eqn1">
<graphic xlink:href="572344v3_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn2">
<graphic xlink:href="572344v3_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<p>The RIDF <italic>d<sub>s</sub>i<sub>,x,y</sub></italic> is calculated for each snapshot <italic>s<sub>i</sub></italic> in <italic>S</italic> = <italic>s</italic><sub>0</sub>, <italic>s</italic><sub>1</sub>, …, <italic>s<sub>n</sub></italic> around the nest location and the heading direction <italic>h<sub>s</sub>i<sub>,x,y</sub></italic> at each grid location x, y and each snapshot <italic>s<sub>i</sub></italic> is determined by taking the location of the minimum RIDF (<xref rid="eqn3" ref-type="disp-formula">Eq. 3</xref>). To weigh the heading direction the ratio <italic>w<sub>s</sub>i</italic> was calculated between the minimum RIDF of all snapshots <italic>s<sub>i</sub></italic> in <italic>S<sub>dmin</sub></italic> and the current RIDF <italic>d<sub>s</sub>i</italic> (<xref rid="eqn5" ref-type="disp-formula">Eq. 5</xref>). The homing vector <italic>H⃗V</italic> results from the weighted circular mean of the different heading directions <italic>h<sub>s</sub>i</italic> (<xref rid="eqn6" ref-type="disp-formula">Eq. 6</xref>).</p>
<disp-formula id="eqn3">
<graphic xlink:href="572344v3_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn4">
<graphic xlink:href="572344v3_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="572344v3_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn6">
<graphic xlink:href="572344v3_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<p>The contrast-weighted-nearness model will use the contrast weighted by the depth of the environment to steer an agent home. The contrast was calculated as the ratio of luminance-amplitude (i.e. standard deviation of the luminance) and luminance-background (i.e. average of the luminance) within a 3x3 pixel window of the snapshot image (i.e. Michelson contrast). As described in [<xref ref-type="bibr" rid="c13">13</xref>], we used the rotational similarity function between the current view <italic>I<sub>x,y</sub></italic> and the memorized view <italic>I<sub>N</sub></italic> (<xref rid="eqn7" ref-type="disp-formula">Eq. 7</xref>). As for the brightness-based model, the homing vector (<xref rid="eqn6" ref-type="disp-formula">Eq. 6</xref>) was computed by the weighted circular means of each vectors derived from each memorised views (<xref rid="eqn9" ref-type="disp-formula">Eq. 9</xref>).</p>
<disp-formula id="eqn7">
<graphic xlink:href="572344v3_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn8">
<graphic xlink:href="572344v3_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn9">
<graphic xlink:href="572344v3_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</sec>
</sec>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99140.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>In this <bold>useful</bold> study, the authors tested the ability of bumblebees to use bird-view and ground-view for homing in cluttered landscapes using modeling and behavioral experiments, claiming that bumblebees rely most on ground-views for homing. However, due to a lack of analysis of the bees' behavior during training and a lack of information as to how the homing behavior of bees develops over time, the evidence supporting their claims is currently <bold>incomplete</bold>. Moreover, there was concern that the experimental environment was not representative of natural scenes, thus limiting the findings of the study.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99140.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this paper, the authors aimed to test the ability of bumblebees to use bird-view and ground-view for homing in cluttered landscapes. Using modelling and behavioural experiments, the authors showed that bumblebees rely most on ground-views for homing.</p>
<p>Strengths:</p>
<p>The behavioural experiments are well-designed, and the statistical analyses are appropriate for the data presented.</p>
<p>Weaknesses:</p>
<p>Views of animals are from a rather small catchment area.</p>
<p>Missing a discussion on why image difference functions were sufficient to explain homing in wasps (Murray and Zeil 2017).</p>
<p>The artificial habitat is not really 'cluttered' since landmarks are quite uniform, making it difficult to infer ecological relevance.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99140.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In a 1.5m diameter, 0.8m high circular arena bumblebees were accustomed to exiting the entrance to their nest on the floor surrounded by an array of identical cylindrical landmarks and to forage in an adjacent compartment which they could reach through an exit tube in the arena wall at a height of 28cm. The movements of one group of bees were restricted to a height of 30cm, the height of the landmark array, while the other group was able to move up to heights of 80cm, thus being able to see the landmark array from above.</p>
<p>During one series of tests, the flights of bees returning from the foraging compartment were recorded as they tried to reach the nest entrance on the floor of the arena with the landmark array shifted to various positions away from the true nest entrance location. The results of these tests showed that the bees searched for the net entrance in the location that was defined by the landmark array.</p>
<p>In a second series of tests, access to the landmark array was prevented from the side, but not from the top, by a transparent screen surrounding the landmark array. These tests showed that the bees of both groups rarely entered the array from above, but kept trying to enter it from the side.</p>
<p>
The authors express surprise at this result because modelling the navigational information supplied by panoramic snapshots in this arena had indicated that the most robust information about the location of the nest entrance within the landmark array was supplied by views of the array from above, leading to the following strong conclusions:</p>
<p>
line 51: &quot;Snapshot models perform best with bird's eye views&quot;;</p>
<p>
line 188: &quot;Overall, our model analysis could show that snapshot models are not able to find home with views within a cluttered environment but only with views from above it.&quot;;</p>
<p>
line 231: &quot;Our study underscores the limitations inherent in snapshot models, revealing their inability to provide precise positional estimates within densely cluttered environments, especially when compared to the navigational abilities of bees using frog's-eye views.&quot;</p>
<p>Strengths:</p>
<p>The experimental set-up allows for the recording of flight behaviour in bees, in great spatial and temporal detail. In principle, it also allows for the reconstruction of the visual information available to the bees throughout the arena.</p>
<p>Weaknesses:</p>
<p>Modelling:</p>
<p>
Modelling left out information potentially available to the bees from the arena wall and in particular from the top edge of the arena and cues such as cameras outside the arena. For instance, modelled IDF gradients within the landmark array degrade so rapidly in this environment, because distant visual features, which are available to bees, are lacking in the modelling. Modelling furthermore did not consider catchment volumes, but only horizontal slices through these volumes.</p>
<p>Behavioural analysis:</p>
<p>
The full potential of the set-up was not used to understand how the bees' navigation behaviour develops over time in this arena and what opportunities the bees have had to learn the location of the nest entrance during repeated learning flights and return flights.</p>
<p>Without a detailed analysis of the bees' behaviour during 'training', including learning flights and return flights, it is very hard to follow the authors' conclusions. The behaviour that is observed in the tests may be the result of the bees' extended experience shuttling between the nest and the entry to the foraging arena at 28cm height in the arena wall. For instance, it would have been important to see the return flights of bees following the learning flights shown in Figure 17.</p>
<p>Basically, both groups of bees (constrained to fly below the height of landmarks (F) or throughout the height of the arena (B)) had ample opportunities to learn that the nest entrance lies on the floor of the landmark array. The only reason why B-bees may not have entered the array from above when access from the side was prevented, may simply be that bumblebees, because they bumble, find it hard to perform a hovering descent into the array.</p>
<p>General:</p>
<p>The most serious weakness of the set-up is that it is spatially and visually constrained, in particular lacking a distant visual panorama, which under natural conditions is crucial for the range over which rotational image difference functions provide navigational guidance. In addition, the array of identical landmarks is not representative of natural clutter and, because it is visually repetitive, poses un-natural problems for view-based homing algorithms. This is the reason why the functions degrade so quickly from one position to the next (Figures 9-12), although it is not clear what these positions are (memory0-memory7).</p>
<p>
In conclusion, I do not feel that I have learnt anything useful from this experiment; it does suggest, however, that to fully appreciate and understand the homing abilities of insects, there is no alternative but to investigate these abilities in the natural conditions in which they have evolved.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99140.1.sa3</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sonntag</surname>
<given-names>Annkathrin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3139-3981</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Sauzet</surname>
<given-names>Odile</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1029-8846</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Lihoreau</surname>
<given-names>Mathieu</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2463-2040</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Egelhaaf</surname>
<given-names>Martin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9336-4270</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Bertrand</surname>
<given-names>Olivier</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0889-4550</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 1 (Public Review):</bold></p>
<p>“Summary:</p>
<p>In this paper, the authors aimed to test the ability of bumblebees to use bird-view and ground-view for homing in cluttered landscapes. Using modelling and behavioural experiments, the authors showed that bumblebees rely most on ground-views for homing.</p>
<p>Strengths:</p>
<p>The behavioural experiments are well-designed, and the statistical analyses are appropriate for the data presented.</p>
<p>Weaknesses:</p>
<p>Views of animals are from a rather small catchment area.</p>
<p>Missing a discussion on why image difference functions were sufficient to explain homing in wasps (Murray and Zeil 2017).</p>
<p>The artificial habitat is not really 'cluttered' since landmarks are quite uniform, making it difficult to infer ecological relevance.”</p>
</disp-quote>
<p>Thank you for your thorough evaluation of our study. We aimed to investigate local homing behaviour on a small scale, which is ecologically relevant given that the entrance of bumblebee nests is often inconspicuously hidden within the vegetation. This requires bees to locate their nest entrance using views within a confined area. While many studies have focused on larger scales using radar tracking (e.g. <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">Capaldi et</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">al.</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">2000;</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">Osborne</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">et</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">al.</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">2013;</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">Woodgate</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">et</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">al.</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?6oMrq0">2016)</ext-link>, there is limited understanding of the mechanisms behind local homing on a smaller scale, especially in dense environments.</p>
<p>We appreciate your suggestion to include the study by <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?39npQk">Murray</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?39npQk">and</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?39npQk">Zeil</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?39npQk">(2017)</ext-link> in our discussion. Their research explored the catchment areas of image difference functions on a larger spatial scale with a cubic volume of 5m x 5m x 5m. Aligned with their results, we found that image difference functions pointed towards the location of the objects surrounding the nest when the images were taken above the objects. However, within the clutter, i.e. the dense set of objects surrounding the nest, the model did not perform well in pinpointing the nest position.</p>
<p>We agree with your comment about the term &quot;clutter&quot;. Therefore, we will refer to our landmark arrangement as a &quot;dense environment&quot; instead. Uniformly distributed objects do indeed occur in nature, as seen in grasslands, flower meadows, or forests populated with similar plants.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2 (Public Review):</bold></p>
<p>Summary:</p>
<p>In a 1.5m diameter, 0.8m high circular arena bumblebees were accustomed to exiting the entrance to their nest on the floor surrounded by an array of identical cylindrical landmarks and to forage in an adjacent compartment which they could reach through an exit tube in the arena wall at a height of 28cm. The movements of one group of bees were restricted to a height of 30cm, the height of the landmark array, while the other group was able to move up to heights of 80cm, thus being able to see the landmark array from above.</p>
<p>During one series of tests, the flights of bees returning from the foraging compartment were recorded as they tried to reach the nest entrance on the floor of the arena with the landmark array shifted to various positions away from the true nest entrance location. The results of these tests showed that the bees searched for the net entrance in the location that was defined by the landmark array.</p>
<p>In a second series of tests, access to the landmark array was prevented from the side, but not from the top, by a transparent screen surrounding the landmark array. These tests showed that the bees of both groups rarely entered the array from above, but kept trying to enter it from the side.</p>
<p>The authors express surprise at this result because modelling the navigational information supplied by panoramic snapshots in this arena had indicated that the most robust information about the location of the nest entrance within the landmark array was supplied by views of the array from above, leading to the following strong conclusions:</p>
<p>line 51: &quot;Snapshot models perform best with bird's eye views&quot;; line 188: &quot;Overall, our model analysis could show that snapshot models are not able to find home with views within a cluttered environment but only with views from above it.&quot;; line 231: &quot;Our study underscores the limitations inherent in snapshot models, revealing their inability to provide precise positional estimates within densely cluttered environments, especially when compared to the navigational abilities of bees using frog's-eye views.&quot; Strengths:</p>
<p>The experimental set-up allows for the recording of flight behaviour in bees, in great spatial and temporal detail. In principle, it also allows for the reconstruction of the visual information available to the bees throughout the arena.</p>
<p>The experimental set-up allows for the recording of flight behaviour in bees, in great spatial and temporal detail. In principle, it also allows for the reconstruction of the visual information available to the bees throughout the arena.</p>
<p>Weaknesses:</p>
<p>Modelling:</p>
<p>Modelling left out information potentially available to the bees from the arena wall and in particular from the top edge of the arena and cues such as cameras outside the arena. For instance, modelled IDF gradients within the landmark array degrade so rapidly in this environment, because distant visual features, which are available to bees, are lacking in the modelling. Modelling furthermore did not consider catchment volumes, but only horizontal slices through these volumes.</p>
</disp-quote>
<p>When we started modelling the bees’ homing based on image-matching, we included the arena wall. However, the model simulations pointed only coarsely towards the clutter but not toward the nest position. We hypothesised that the arena wall and object location created ambiguity. Doussot et al. (2020) showed that such a model can yield two different homing locations when distant and local cues are independently moved. Therefore, we reduced the complexity of the environment by concentrating on the visual features, which were moved between training and testing. (Neither the camera nor the wall were moved between training and test). We acknowledge that this information should have been provided to substantiate our reasoning. As such, we will include model results with the arena wall in the revised paper.</p>
<p>As we wanted to investigate if bees would use ground views or bird’s eye views to home in a dense environment, we think the catchment volumes would provide qualitatively similar, though quantitatively more detailed information as catchment slices. Our approach of catchment slices is sufficient to predict whether ground or bird' s-eye views perform better in leading to the nest, and we will, therefore, not include further computations of catchment volumes.</p>
<disp-quote content-type="editor-comment">
<p>Behavioural analysis:</p>
<p>The full potential of the set-up was not used to understand how the bees' navigation behaviour develops over time in this arena and what opportunities the bees have had to learn the location of the nest entrance during repeated learning flights and return flights.</p>
<p>Without a detailed analysis of the bees' behaviour during 'training', including learning flights and return flights, it is very hard to follow the authors' conclusions. The behaviour that is observed in the tests may be the result of the bees' extended experience shuttling between the nest and the entry to the foraging arena at 28cm height in the arena wall. For instance, it would have been important to see the return flights of bees following the learning flights shown in Figure 17.</p>
<p>Basically, both groups of bees (constrained to fly below the height of landmarks (F) or throughout the height of the arena (B)) had ample opportunities to learn that the nest entrance lies on the floor of the landmark array. The only reason why B-bees may not have entered the array from above when access from the side was prevented, may simply be that bumblebees, because they bumble, find it hard to perform a hovering descent into the array.</p>
</disp-quote>
<p>A prerequisite for studying the learning flight in a given environment is showing that the bees manage to return to their home. Here, our primary goal was to demonstrate this within a dense environment. While we understand that a detailed analysis of the learning and return flights would be valuable, we feel this is outside the scope of this particular study.</p>
<p>Multi-snapshot models have been repeatedly shown to be sufficient to explain the homing behaviour in natural as well as artificial environments. A model can not only be used to replicate but also to predict a given outcome and shape the design of experiments. Here, we used the models to shape the experimental design, as it does not require the entire history of the bee's trajectory to be tested and provides interesting insight into homing in diverse environments.</p>
<p>Our current knowledge of learning flights did not permit these investigations of bee training. Firstly, our setup does not allow us to record each inbound and outbound flight of the bumblebees during training. Doing so would require blocking the entire colony for extended time periods, potentially impairing the motivation of the bees to forage or the survival and development of the colony. Secondly, the exact locations where bees learn or if and whether they continuously learn by weighting the visual experience based on their positions and orientations is not always clear. It makes it difficult to categorise these flights accurately in learning and return flights. Additionally, homing models remain elusive on the learning mechanisms at play during the learning flights. Therefore, we believe that continuous effort must be made to understand bees' learning and homing ability. We felt it was necessary first to establish that bees could navigate back to the nest in a dense, cluttered environment. With this understanding, we are currently conducting a detailed study of the bees' learning flights in various dense environments and provide these results in a separate article.</p>
<p>While we acknowledge that the bees had ample opportunities to learn the location of the nest entrance, we believe that their behaviour of entering the dense environment at a very low altitude cannot be solely explained by extended experience. It is possible that the bees could have also learned to enter at the edge of the objects or above the objects before descending within the clutter.</p>
<disp-quote content-type="editor-comment">
<p>General:</p>
<p>The most serious weakness of the set-up is that it is spatially and visually constrained, in particular lacking a distant visual panorama, which under natural conditions is crucial for the range over which rotational image difference functions provide navigational guidance. In addition, the array of identical landmarks is not representative of natural clutter and, because it is visually repetitive, poses un-natural problems for view-based homing algorithms. This is the reason why the functions degrade so quickly from one position to the next (Figures 9-12), although it is not clear what these positions are (memory0-memory7).</p>
<p>In conclusion, I do not feel that I have learnt anything useful from this experiment; it does suggest, however, that to fully appreciate and understand the homing abilities of insects, there is no alternative but to investigate these abilities in the natural conditions in which they have evolved.</p>
</disp-quote>
<p>We respectfully disagree with the evaluation that our study does not provide new insights due to the controlled lab conditions. Both field and lab research are absolutely necessary and should feed each other. Dismissing the value of controlled lab experiments would overlook the contributions of previous lab-based research, which has significantly advanced our understanding of animal behaviour. It is only possible to precisely define the visual test environments under laboratory conditions and to identify the role of these components for the behaviour through targeted variation of individual components of the environment. These results should guide field-based experiments for validation.</p>
<p>Our lab settings are a kind of abstraction of natural situations focusing on those aspects that are at the centre of the research question. Our approach here was that bumblebees have to find their inconspicuous nest hole in nature, which is difficult to find in often highly dense environments, and ultimately on a spatial scale in the metre range. We first wanted to find out if bumblebees can find their nest hole under the particularly challenging condition that all objects surrounding the nest hole are the same. This was not yet clear. Uniformly distributed objects may, however, also occur in nature, as seen with visually inconspicuous nest entrances of bumblebees in grass meadows, flower meadows, or forests with similar plants. We agree that the term &quot;clutter&quot; is not well-defined in the literature and will refer to our environment as a &quot;dense environment.&quot;</p>
<p>Despite the lack of a distant visual panorama, or also UV light, wind, or other confounding factor inherent to field work, the bees successfully located the nest position even when we shifted the dense environment within the flight arena. We used rotational-image difference functions based on snapshots taken around the nest position to predict the bees' behaviour, as this is one of the most widely accepted and computationally most parsimonious</p>
<p>mechanisms for homing. This approach also proved effective in our more restricted conditions, where the bees still managed to pinpoint their home.</p>
</body>
</sub-article>
</article>