<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108495</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108495</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108495.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The view-tolerance of human identity recognition depends on horizontal face information</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Roux-Sibilon</surname>
<given-names>Alexia</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dumont</surname>
<given-names>Hélène</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bremhorst</surname>
<given-names>Vincent</given-names>
</name>
<xref ref-type="aff" rid="A3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jacobs</surname>
<given-names>Christianne</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Goffaux</surname>
<given-names>Valérie</given-names>
</name>
<email>valerie.goffaux@uclouvain.be</email>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A4">4</xref>
</contrib>
<aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02495e989</institution-id><institution>Psychological Sciences Research Institute (IPSY), UCLouvain</institution></institution-wrap>, <city>Louvain-la-Neuve</city>, <country country="BE">Belgium</country></aff>
<aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01a8ajp46</institution-id><institution>Université Clermont-Auvergne, CNRS, LAPSCO</institution></institution-wrap>, <city>Clermont-Ferrand</city>, <country country="FR">France</country></aff>
<aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02495e989</institution-id><institution>Statistical Methodology and Computing Service (SMCS), UCLouvain</institution></institution-wrap>, <city>Louvain-la-Neuve</city>, <country country="BE">Belgium</country></aff>
<aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02495e989</institution-id><institution>Institute of Neurosciences (IoNS), UCLouvain</institution></institution-wrap>, <city>Louvain-la-Neuve</city>, <country country="BE">Belgium</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Dekker</surname>
<given-names>Tessa</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<pub-date pub-type="epub">
<day>01</day>
<month>08</month>
<year>2025</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2025-10-22">
<day>22</day>
<month>10</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108495</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2025-08-14">
<day>14</day>
<month>08</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-07-17">
<day>17</day>
<month>07</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.31234/osf.io/8au9j_v3"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Roux-Sibilon et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Roux-Sibilon et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108495-v1.pdf"/>
<self-uri xlink:href="8au9j.pdf" content-type="docx" xlink:role="full-text"/>
<abstract>
<p>How we recognise objects and people despite their physical appearance can change dramatically across encounters is a central yet unresolved question in vision science. In particular, the visual information that supports the human ability to recognize face identity across views is unknown. Past research suggests horizontally oriented face information plays a key role. We tested this hypothesis by characterizing the orientation of the visual information physically available in the face stimulus to support view-tolerant face recognition and how human observers make use of it.</p>
<p>Human observers performed an old/new identity recognition task with face stimuli presented under different viewpoints, achieved by rotating the faces in yaw (from full-frontal to profile) and filtered to preserve contrast in selective orientation ranges. Human performance remained tuned to the horizontal range of face information irrespective of yaw.</p>
<p>We used a model observer approach to define the information physically available in the stimulus for matching face identity within each viewpoint (view-selective model observer) or across different viewpoints (view-tolerant model observer). The view-selective model indicated that face identity is carried by orientation ranges shifting from horizontal in frontal views to vertical in profile views. In contrast, the view-tolerant model showed that the horizontal range provides the most stable identity cues across views. The horizontally-tuned orientation profile of human recognition performance was predicted by the high diagnosticity of horizontal information in frontal views and the stability of the horizontal identity cues across views.</p>
<p>Our findings indicate that the invariant representation of a face, gradually learned through repeated exposure to its natural appearance statistics, relies primarily on horizontal facial information. By identifying the spatial information supporting view-tolerant face recognition in humans, the present work yields concrete, data-driven constraints for the refinement of visual recognition models.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p>The way in which we recognize objects and people is a central but as yet unresolved question in vision sciences. Our understanding of visual perception is still limited by our ability to account for how we recognize objects and people despite the sometimes radically different images they project onto the retinae, due to varying lighting, distance, depth rotation, etc (<xref ref-type="bibr" rid="c20">DiCarlo &amp; Cox, 2007</xref>). We experience faces under a broad range of depth rotations mainly along the x axis, i.e., from left to right profile (i.e., yaw; <xref ref-type="bibr" rid="c25">Favelle &amp; Palmisano, 2012</xref>) due to biomechanical constrains favoring x-axis head rotations and the vantage point on our conspecifics’ faces also varying more along the x than the y axis (i.e., when moving around people). While rotating in depth, a given face projects retinal images that are more dissimilar than the ones that different faces under the same viewpoint would project (<xref ref-type="bibr" rid="c1">Adini et al., 1997</xref>; <xref ref-type="bibr" rid="c12">Burton et al., 2016</xref>; <xref ref-type="bibr" rid="c40">Hill et al., 1997</xref>). Yet humans generally have no difficulty in recognizing a familiar face across views, which implies the joint ability to differentiate its identity from others and to generalize it from one view to another, i.e., with tolerance to variations (<xref ref-type="bibr" rid="c76">Ritchie &amp; Burton, 2017</xref>).</p>
<p>The tolerance of face identity recognition culminates when faces are familiar to the observer (e.g., <xref ref-type="bibr" rid="c26">Favelle &amp; Palmisano, 2018</xref>; <xref ref-type="bibr" rid="c35">Hancock et al., 2000</xref>; <xref ref-type="bibr" rid="c40">Hill et al., 1997</xref>; <xref ref-type="bibr" rid="c42">Jeffery et al., 2006</xref>; <xref ref-type="bibr" rid="c45">Johnston &amp; Edmonds, 2009</xref>; <xref ref-type="bibr" rid="c46">Jones et al., 2017</xref>; <xref ref-type="bibr" rid="c53">Liu, 2002</xref>; <xref ref-type="bibr" rid="c63">Newell et al., 1999</xref>; <xref ref-type="bibr" rid="c65">O’Toole et al., 1998</xref>, <xref ref-type="bibr" rid="c66">1999</xref>; <xref ref-type="bibr" rid="c88">Troje &amp; Bulthoff, 1996</xref>), suggesting that a core determinant of tolerant recognition is the repeated exposure to the natural statistics of a person’s face (e.g., to the variability of a person appearance across different views; <xref ref-type="bibr" rid="c86">Tian &amp; Grill-Spector, 2015a</xref>; <xref ref-type="bibr" rid="c88">Troje &amp; Bulthoff, 1996</xref>; <xref ref-type="bibr" rid="c89">Van Meel &amp; Op de Beeck, 2018</xref>; <xref ref-type="bibr" rid="c94">Wallis &amp; Bulthoff, 2001</xref>). Such statistical learning is assumed to be the main unsupervised learning route for tolerant face/object recognition in humans and animals (<xref ref-type="bibr" rid="c21">DiCarlo &amp; Cox, 2007b</xref>; <xref ref-type="bibr" rid="c27">Fiser &amp; Aslin, 2002</xref>; <xref ref-type="bibr" rid="c37">Hauser et al., 2001</xref>; <xref ref-type="bibr" rid="c41">Huber et al., 2023</xref>; <xref ref-type="bibr" rid="c50">Li &amp; DiCarlo, 2008</xref>, <xref ref-type="bibr" rid="c51">2010</xref>, <xref ref-type="bibr" rid="c52">2012</xref>; <xref ref-type="bibr" rid="c87">Tian &amp; Grill-Spector, 2015b</xref>). Since exposure to the natural variations of a given person is typically prolonged (several seconds), it has been proposed that temporal contiguity contributes to the generation of a multi-view representation of identity. The importance of temporal contiguity in identity recognition is supported by findings that human observers tend to confound identities if learnt in the same temporal sequence of views (<xref ref-type="bibr" rid="c51">Li &amp; DiCarlo, 2010</xref>; <xref ref-type="bibr" rid="c59">Miyashita, 1993</xref>; <xref ref-type="bibr" rid="c73">Pitts &amp; McCulloch, 1947</xref>; <xref ref-type="bibr" rid="c89">Van Meel &amp; Op De Beeck, 2018</xref>; <xref ref-type="bibr" rid="c92">Wallis et al., 2009</xref>; <xref ref-type="bibr" rid="c93">Wallis &amp; Bulthoff, 1999</xref>). Other findings suggest that temporal contiguity is not necessary and that the important contributing factor is the number of learnt views, be them seen randomly or in sequence (<xref ref-type="bibr" rid="c55">T. Liu, 2007</xref>; <xref ref-type="bibr" rid="c87">Tian &amp; Grill-Spector, 2015b</xref>). Yet, since the different views of a given person are usually seen in temporal contiguity in natural viewing, it seems likely that temporal contiguity contributes to statistical learning of face identity.</p>
<p>While some attention has been devoted to the contribution of temporal contiguity in view-tolerant recognition, the <italic>spatial</italic> aspects of the natural statistics supporting view-tolerant face identity recognition are still largely elusive. Face appearance results from the complex interaction between extrinsic viewing conditions and the intrinsic 3D shape and reflectance properties of the face (determined e.g., by viewpoint and lighting; <xref ref-type="bibr" rid="c24">Favelle et al., 2017</xref>; <xref ref-type="bibr" rid="c39">Hill &amp; Bruce, 1996</xref>; <xref ref-type="bibr" rid="c54">Liu et al., 2000</xref>). A study by <xref ref-type="bibr" rid="c40">Hill et al. (1997)</xref> suggested that shape is the primary determinant of view-tolerant recognition, and that surface cues such as reflectance and texture contribute less. Burton and colleagues (<xref ref-type="bibr" rid="c11">Burton et al., 2005</xref>; <xref ref-type="bibr" rid="c10">Burton, 2013</xref>) suggested that as exposure to multiple appearances of a person increases, the accidental, irrelevant variations would be progressively whitened (i.e., averaged out) and reveal the stable cues to identity. <xref ref-type="fig" rid="fig1">Figure 1</xref> simulates such averaging using the pictures of a given identity taken from drastically variable poses, as experienced in natural viewing. It can be seen that additionally to the whitening of accidental properties, the resulting average contains a strong horizontal structure; namely it results in the emergence of the so-called (horizontal) bar code of the face (<xref ref-type="bibr" rid="c18">Dakin &amp; Watt, 2009</xref>). We, as others, proposed that the horizontal content of the face stimulus may drive the visual mechanisms engaged for the view-tolerant recognition of face identity (<xref ref-type="bibr" rid="c15">Caldara &amp; Seghier, 2009</xref>; <xref ref-type="bibr" rid="c18">Dakin &amp; Watt, 2009</xref>; <xref ref-type="bibr" rid="c28">Gilad-Gutnick et al., 2018</xref>; <xref ref-type="bibr" rid="c29">Goffaux, 2008</xref>; <xref ref-type="bibr" rid="c32">Goffaux &amp; Dakin, 2010</xref>). Several lines of empirical evidence indicate that the horizontally-oriented face information conveys optimal cues to identity. First, the visual mechanisms specialized for the identity recognition of faces’ frontal view were found to rely preferentially on the horizontal structure of the face image, indicating a better sensitivity to identity in the horizontal range of face information (e.g., <xref ref-type="bibr" rid="c4">Balas et al., 2015</xref>; <xref ref-type="bibr" rid="c22">Dumont et al., 2024</xref>; <xref ref-type="bibr" rid="c30">Goffaux, 2019</xref>; <xref ref-type="bibr" rid="c32">Goffaux &amp; Dakin, 2010</xref>; <xref ref-type="bibr" rid="c68">Pachai et al., 2013</xref>, <xref ref-type="bibr" rid="c69">2018</xref>). The horizontal dependence of face identity recognition has also been shown to predict face identification accuracy at the individual observer level (<xref ref-type="bibr" rid="c23">Duncan et al., 2019</xref>; <xref ref-type="bibr" rid="c68">M. V. Pachai et al., 2013</xref>). Furthermore, there is indirect evidence that horizontal face information may optimally drive the tolerance of face identity recognition. For example, while the horizontal dependence manifests from three months of age (<xref ref-type="bibr" rid="c19">De Heering et al., 2016</xref>), it strengthens over the lifespan as individuals accumulate experience with the natural statistics of face appearance (<xref ref-type="bibr" rid="c34">Goffaux et al., 2015</xref>). Moreover, and considering that the recognition of familiar faces differs from unfamiliar face recognition by its stronger tolerance to retinal variations due to e.g., a change in view (<xref ref-type="bibr" rid="c7">Bruce et al., 1999</xref>; <xref ref-type="bibr" rid="c14">Burton et al., 1999</xref>, <xref ref-type="bibr" rid="c13">2010</xref>; <xref ref-type="bibr" rid="c39">Hill &amp; Bruce, 1996</xref>; <xref ref-type="bibr" rid="c56">Megreya &amp; Burton, 2006</xref>; <xref ref-type="bibr" rid="c65">O’Toole et al., 1998</xref>; <xref ref-type="bibr" rid="c75">Ramon, 2015b</xref>), the finding by Pachai and colleagues that the recognition of a face from % to full-frontal view increasingly relies on horizontal information as a function of familiarity supports the notion that not only the accuracy but also the tolerance of identity recognition most crucially depends on horizontal face cues (<xref ref-type="bibr" rid="c67">Pachai et al., 2017</xref>; see also <xref ref-type="bibr" rid="c74">Ramon, 2015a</xref>). In one of the <xref ref-type="bibr" rid="c32">Goffaux and Dakin (2010)</xref>’s experiments, the matching of unfamiliar faces from frontal to % view was more largely disrupted by horizontal than vertical noise masking. Yet, whether this effect reflects view-tolerant recognition or whether it is primarily due to the encoding of the frontal view probe is unclear. The important question of whether the horizontal range of face information is a privileged informational avenue for view-tolerant identity recognition is therefore still unanswered.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Graphic illustration of the horizontal smearing occurring when averaging multiple views of a face.</title> <p>The tolerance of human face identity recognition to drastic appearance variations caused by varying lighting, viewpoint, facial expression, etc. has been proposed to emerge thanks to an averaging mechanism that would progressively whiten these accidental variations and preserve the stable identity cues to identity. Past illustrations (<xref ref-type="bibr" rid="c10">Burton, 2013</xref>) used varying lighting and expressions but moderate pose variations. Here we show that when pose varies from left to right profile, averaging results in horizontally smeared face cues suggesting that across encounters with a face, cues at most orientations except horizontal are whitened. As the observer learns the natural statistics of a person’s face, it seems plausible that they increasingly rely on horizontal cues for identity recognition (see <xref ref-type="bibr" rid="c67">Pachai et al., 2017</xref> for supporting evidence). Images of two celebrities (George Clooney and Daniel Ratclife) were sampled from the internet and sorted into three view categories: frontal, left-, and right-averted. In order to illustrate the horizontal smearing due to view variations, the averages were made of 40% left-averted, 40% right-averted, and 20% frontal views. The luminance and RMS contrast of the averaged faces were set to a luminance of .5 and contrast of .4. Using this procedure, one can appreciate the emergence of the so-called bar code, namely the vertical arrangement of horizontally-oriented cues typical of the face at basic and individual levels of categorization (<xref ref-type="bibr" rid="c18">Dakin &amp; Watt, 2009</xref>).</p></caption>
<graphic xlink:href="8au9jv3_fig1.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>The present study aims at directly investigating the hypothesis that the tolerance of human face identity recognition is supported by the horizontal range of face information. We familiarized a sample of human observers with the multiple views of a set of face identities. In an old-new recognition task involving face stimuli presented in various views, we demonstrated that humans stay broadly tuned to the horizontal range of face information irrespective of yaw, with a stronger tuning observed for frontal views. We used a model observer approach to define the information physically available in the stimulus for the matching of face identity within a given viewpoint and across different viewpoints. A model observer is a basic image processing algorithm that cross-correlates a target image with probe images at the pixel level, and selects the probe with the highest correlation, i.e., the most likely match. Combined with orientation filtering, this method provides a formal way to describe how the information most useful for matching identity is distributed in the orientation domain of the face image (e.g., <xref ref-type="bibr" rid="c17">Collin et al., 2014</xref> for a similar approach in the spatial frequency domain).</p>
<p>We tested two model observers on the same multiple views of face identities as used in the human old-new recognition task: a <italic>view-selective</italic> model observer, which matched identities within the same view (e.g., matching a profile view of identity A with profile views of all identities), and a <italic>view-tolerant</italic> model observer, which matched identities across different views (e.g., matching a profile view of identity A with the other views of all identities). This approach revealed a substantial difference in the orientation distribution of view-specific and view-tolerant information. The view-selective model indicated that, at the image level, face identity is conveyed by a broad view-dependent spectrum of orientations: identity cues were distributed in the horizontal range at frontal views (in line with <xref ref-type="bibr" rid="c68">Pachai et al., 2013</xref>) but shifted to the vertical range as the face turned to profile. In contrast, the view-tolerant model indicated that the horizontal structure of the face stimulus provides the most useful cues for matching identity across yaws. Partial correlations between model and human observer performance suggest that the horizontal tuning of human face identity recognition is due to the high diagnosticity of horizontal information at frontal view and the stability of the horizontal identity cues across views.</p>
</sec>
<sec id="s2" sec-type="methods">
<title>Methods</title>
<sec id="s2-1">
<title>Subjects</title>
<p>Twenty-two healthy young adults took part in this experiment in exchange for monetary compensation (8 euros per hour of testing). They were 14 females and 8 males, aged 23.5 (± 3.4) on average (4 were left-handed), recruited via Facebook advertisement. They received a written description of the experiment protocol and gave their written informed consent. Participants had normal vision as verified by a Landolt C acuity test (conducted using FRACT; <xref ref-type="bibr" rid="c3">Bach, 1996</xref>). Participants wore optical corrections when necessary. The experimental protocol was approved by the local ethical committee (Psychological Sciences Research Institute, UCLouvain).</p>
</sec>
<sec id="s2-2">
<title>Stimuli</title>
<p>We selected 30 face identities (15 male, 15 female) from the 3D laser-scanned face database of the Max-Planck Institute for Biological Cybernetics (Tuebingen, Germany; <xref ref-type="bibr" rid="c88">Troje &amp; Bulthoff, 1996</xref>). Faces were viewed under seven different viewpoints ranging from −75° to +75° in steps of 25° (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). We first converted all images into grayscales and resized them so that all faces subtended a height of 210 pixels. All images were padded into 400x400 pixels gray canvas and alpha-blended with a viewspecific aperture designed to cover the hair and neck of the average of all face images at a given viewpoint (using Adobe Photoshop).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Stimulus conditions.</title> 
<p>Columns. Each identity was viewed from seven different viewpoints ranging from +75° to −75° in steps of 25°. Rows. All images were filtered in the Fourier domain to preserve only a selective range of orientation, from 0° (vertical) to 157.5° in steps of 22.5°.</p></caption>
<graphic xlink:href="8au9jv3_fig2.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Next, images were normalized to obtain a mean luminance of 0 and root-mean square (RMS) contrast of 1 and submitted to a fast two-dimensional Fourier transform to manipulate orientation content. Since manipulations in the Fourier domain apply to the whole image, they encompass both the face and background pixels. When the image of a face on a plain background is filtered in the Fourier orientation domain, energy belonging to the face therefore smears to the background and vice-versa, resulting in an oriented halo. To minimize this smearing, we applied an iterative phase-scrambling procedure (as in <xref ref-type="bibr" rid="c16">Canoluk et al., 2023</xref>; <xref ref-type="bibr" rid="c72">Petras et al., 2019</xref>; <xref ref-type="bibr" rid="c78">Roux-Sibilon et al., 2023</xref>; <xref ref-type="bibr" rid="c81">Schuurmans et al., 2023</xref>), which consists in iteratively phase-scrambling the image, pasting the original face pixels, and phase-scrambling again (50 iterations). By making the power spectra of the face and background pixels more comparable, this procedure minimizes smearing during orientation filtering. The amplitude spectra of the resulting images were then multiplied with wrapped Gaussian filters (standard deviation of 20°) centered on orientations ranging from 0° (vertical) to 157.5° in steps of 22.5°. After inverse-Fourier transform, the filtered images were combined with the view-specific aperture.</p>
<p>In all images, the luminance and RMS contrast of the face pixels were fixed to 0.55 and 0.15, respectively, and background pixels were uniformly set to 0.55. The percentage of clipped pixel values (below 0 or above 1) per image did not exceed 3%. We used custom-written scripts in Matlab 2014a (Mathworks Inc, Natick, MA) for stimulus preparation.</p>
<p>Stimuli were displayed against a grey background (0.55 luminance across RGB channels) at a viewing distance of57 cm on a Viewpixx monitor (VPixx Technologies Inc., Saint-Bruno, Canada) with a resolution of 1920 x 1080 pixels and a 70Hz refresh rate using PsychToolbox (<xref ref-type="bibr" rid="c5">Brainard, 1997</xref>). With this display, face area subtended 5° (width) by 8° (height) of visual angle (approximately corresponding to conversational distance). At the start of the experiment lighting was switched off and the testing area of the lab was closed off separately with light-draining black curtains.</p>
</sec>
<sec id="s2-3">
<title>Procedure</title>
<p>Face identification performance was measured with an old/new recognition task. Faces were presented one by one at screen center and participants were instructed to determine for each of them whether it was a face with which they had been a priori familiarized (‘old’ face) or not (‘new’ face). They answered using a button box by pressing ‘1’ for ‘old’ and ‘2’ for ‘new’. Out of the 30 face identities used in the main experiment, participants were familiarized with 5 female and 5 male faces. The familiar identities were randomly sampled from the stimulus set on the first visit of the participant and were kept identical throughout the experiment.</p>
<p>In the initial session, we familiarized each participant with their selection of to-be-learnt ‘old’ faces, presented in their full spectrum version. To engage participants in the face learning procedure, they were asked to remember a face-name pairing, although during the main experiment they were only asked to judge whether the face was seen during familiarization (‘old’) or new (i.e., incidental learning method as in <xref ref-type="bibr" rid="c53">Liu and Chaudhuri, 2002</xref>). Each identity was first shown centrally rotating from left profile (+75°) to right profile (-75°) pseudo-dynamically along with its assigned first name on top of the screen (400ms per frame). The various views of the given face (from +75° to −75° in steps of 25°, i.e. seven views) were then presented one by one for 400ms. After each identity presentation, a recap screen appeared with all the faces learned in previous trials, clustered by identity and shown under the various viewpoints side by side. All learnt identities were randomly presented one-by-one in the second familiarization phase, each under one of the seven viewpoints. The face appeared in isolation for 1000ms. Then the name associated with the face was added and both name and face were shown together for another 2000ms.</p>
<p>Participants were then evaluated on their ability to name the learnt so-called ‘old’ faces at various viewpoints (+75° to −75°, in steps of 25°). A trial started with a 500ms fixation, then the stimulus was presented at screen center along with the name options (the five names of the ‘old’ faces of the same gender of the shown face), numbered from 1 to 5, at the top of the screen until participant response (maximum of 3000ms). Participants responded by pressing on the corresponding key (1 to 5). The fixation turned green in case of a correct response. When incorrect, the fixation turned red. In both cases, the correct name appeared along with feedback. Feedback lasted for 500ms. Familiarization and test were looped until naming accuracy reached 80 %.</p>
<p>The main experiment was divided into 32 runs of 40 trials. A run started with recap screens for all learned identities under the seven viewpoints (from +75° to −75° in 25° steps) along with their associated name. In each main experiment trial, a face stimulus was presented centrally at a specified viewpoint and orientation range (from 0° to 157.5° in 22.5° steps). Viewpoint and orientation range varied randomly from one trial to the other. A trial started with a 1s fixation, next the face stimulus appeared until response or for 3s maximum. At the end of a run, participants received feedback on their average accuracy. To avoid inducing a response bias, there were as many ‘old’ as ‘new’ trials, making the 10 ‘old’ faces twice as frequent as the 20 ‘new’ ones, which were each presented only once per condition. There were 40 trials per condition and a total of 1280 trials.</p>
<p>Participants first practiced the main experimental task on 20 randomly selected trials with full spectrum stimuli. If they reached 80% accuracy, they could start the main experiment with the filtered stimuli. If practice accuracy was lower than 80%, participants were invited to run the familiarization (learning and test) again. Whenever accuracy in an experiment run dropped below 55% correct, and every third run regardless of performance, participants were presented with the recap screens again. All through the experiment, they were encouraged to respond as accurately and rapidly as possible.</p>
<p>The total experiment lasted for 1.5 hours on average, split into three testing sessions.</p>
</sec>
<sec id="s2-4">
<title>Human data analysis</title>
<p>To prevent outlier responses from contaminating the results, we applied a log10 transformation on response latencies at the trial level and excluded trials with latencies at more than 2.5 times the standard deviation above and below the individual mean, in each participant. This procedure resulted in the exclusion of 1.81% of the trials on average.</p>
<p>In order to estimate the orientation dependence of human sensitivity to identity across views, we derived individual <italic>d’</italic> at each viewpoint and orientation. To do so, we determined hit and false alarm rates (<xref ref-type="bibr" rid="c83">Tanner &amp; Birdsall, 1958</xref>) from the ‘old’/’new’ response in each participant, at each orientation and for each viewpoint. Following the log-linear rule (<xref ref-type="bibr" rid="c38">Hautus, 1995</xref>), we added a 0.5 correction to both before calculating the z scores. Performance at the 0° filter was duplicated to have circular filter values from 0° to 180°. As expected from previous studies (<xref ref-type="bibr" rid="c22">Dumont et al., 2024</xref>; <xref ref-type="bibr" rid="c33">Goffaux &amp; Greenwood, 2016</xref>; <xref ref-type="bibr" rid="c69">M. V. Pachai et al., 2018</xref>), the <italic>d’</italic> plotted as a function of orientation in the frontal view condition (yaw = 0°) depicted a bell-shaped function centred on horizontal orientation (i.e., 90°; see <xref ref-type="fig" rid="fig3">Figure 3A</xref>, central panel). At the other viewpoints, d’ followed a very similar shape, with maximum sensitivity roughly centred on horizontal orientation. Therefore, human sensitivity data could be fitted using a simple Gaussian model. The Gaussian model is defined as:
<disp-formula id="FD1">
<alternatives>
<mml:math id="M1" display="block"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>A</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mi>A</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mo>.</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>S</mml:mi><mml:mi>tan</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
<graphic xlink:href="8au9jv3_eqn1.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p><bold>A.</bold> Sensitivity of human observers to facial identity (d’) as a function of the orientation filter (0° to 180° in 22.5° steps) and face viewpoint (yaw: +75° to −75° in 25° steps). Dots and error bars represent mean d’ values and 95% confidence intervals across participants. Solid lines and shaded areas indicate the mean posterior predictions and 95% credible intervals from the Gaussian Bayesian multilevel model. <bold>B.</bold> Population-level mean parameters of the Gaussian Bayesian Multilevel model, plotted with 95% credible intervals as a function of face viewpoint. The 95% credible intervals reflect the uncertainty of the model and can be interpreted as follows: there is a 95% chance that the value of the population parameter lies within this interval.</p></caption>
<graphic xlink:href="8au9jv3_fig3.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>with <italic>orientation</italic> being the orientation of the filter, ranging from 0 to 180°. The model estimates four free parameters. The <italic>Peak Location</italic> is the orientation on which the gaussian curve is centred. The <italic>Standard Deviation</italic> is the width of the Gaussian curve (i.e., strength of the tuning). The <italic>Base Amplitude</italic> is the height of the gaussian curve base (i.e., the minimum sensitivity, typically found near vertical orientations). <italic>Peak Amplitude</italic> refers to the height of the Gaussian curve relative to its baseline — that is, it reflects the advantage of horizontal over vertical orientations.</p>
<p>We used the package <italic>brms</italic> (<xref ref-type="bibr" rid="c9">Burkner, 2018</xref>) in R to implement this model in the Bayesian framework, using a multilevel modelling approach (<xref ref-type="bibr" rid="c22">Dumont et al., 2024</xref>; <xref ref-type="bibr" rid="c60">Moors et al., 2020</xref>; <xref ref-type="bibr" rid="c61">Nalborczyk et al., 2019</xref>). The four parameters <italic>Peak Location, Standard Deviation, Base Amplitude,</italic> and <italic>Peak Amplitude</italic> were conjointly estimated by linear predictor terms which included an intercept and the effect of Viewpoint. We also estimated a subject-level intercept of <italic>Standard Deviation</italic> and <italic>Base Amplitude</italic> as random effects, to allow the shape of the gaussian to vary across participants. This multilevel structure provides a more accurate estimation of population-level parameters by accounting for subject variability. The prior distributions of the different parameters of the model were specified based on a compromise between (1) using knowledge from previous research (e.g., we used a normal distribution centred on horizontal orientation – 90° – for <italic>Peak Location,</italic> in line with the well-established horizontal tuning of face identification), (2) keeping unbiased uncertainty when previous research was not informative (e.g., for the effect of Viewpoint on the different parameters), and (3) allowing the convergence of the model. The details of the prior distributions can be found in Supplementary <xref ref-type="table" rid="tbl1">Table 1</xref>. We ran four Markov Chain Monte-Carlo simulations, with 20,000 iterations including 3,000 warm-up iterations.</p>
<table-wrap id="tbl1" position="float" orientation="portrait">
<label>Table 1.</label>
<caption><title>Posterior mean and 95% credible interval for each parameter of the Gaussian model, at each viewpoint.</title></caption>
<alternatives>
<graphic xlink:href="8au9jv3_tbl1.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Parameter</th>
<th align="left" valign="top">Viewpoint</th>
<th align="left" valign="top">Estimate (posterior mean)</th>
<th align="left" valign="top">95% credible interval – lower bound</th>
<th align="left" valign="top">95% credible interval – upper bound</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top" rowspan="7">Peak Location</td>
<td align="left" valign="top">-75 (left profile)</td>
<td align="left" valign="top">99.5</td>
<td align="left" valign="top">90.02</td>
<td align="left" valign="top">108.98</td>
</tr>
<tr>
<td align="left" valign="top">-50</td>
<td align="left" valign="top">94.4</td>
<td align="left" valign="top">86.98</td>
<td align="left" valign="top">101.87</td>
</tr>
<tr>
<td align="left" valign="top">-25</td>
<td align="left" valign="top">90.62</td>
<td align="left" valign="top">83.58</td>
<td align="left" valign="top">97.73</td>
</tr>
<tr>
<td align="left" valign="top">0 (full front view)</td>
<td align="left" valign="top">90.99</td>
<td align="left" valign="top">88.22</td>
<td align="left" valign="top">93.78</td>
</tr>
<tr>
<td align="left" valign="top">25</td>
<td align="left" valign="top">91.98</td>
<td align="left" valign="top">85.00</td>
<td align="left" valign="top">99.01</td>
</tr>
<tr>
<td align="left" valign="top">50</td>
<td align="left" valign="top">88.19</td>
<td align="left" valign="top">80.82</td>
<td align="left" valign="top">95.51</td>
</tr>
<tr>
<td align="left" valign="top">75 (right profile)</td>
<td align="left" valign="top">80.24</td>
<td align="left" valign="top">70.73</td>
<td align="left" valign="top">89.76</td>
</tr>
<tr>
<td align="left" valign="top" rowspan="7">Standard Deviation</td>
<td align="left" valign="top">-75 (left profile)</td>
<td align="left" valign="top">40.37</td>
<td align="left" valign="top">27.40</td>
<td align="left" valign="top">54.4</td>
</tr>
<tr>
<td align="left" valign="top">-50</td>
<td align="left" valign="top">46.51</td>
<td align="left" valign="top">34.51</td>
<td align="left" valign="top">59.29</td>
</tr>
<tr>
<td align="left" valign="top">-25</td>
<td align="left" valign="top">47.2</td>
<td align="left" valign="top">35.75</td>
<td align="left" valign="top">59.07</td>
</tr>
<tr>
<td align="left" valign="top">0 (full front view)</td>
<td align="left" valign="top">44.94</td>
<td align="left" valign="top">40.55</td>
<td align="left" valign="top">49.41</td>
</tr>
<tr>
<td align="left" valign="top">25</td>
<td align="left" valign="top">51.36</td>
<td align="left" valign="top">40.04</td>
<td align="left" valign="top">63.14</td>
</tr>
<tr>
<td align="left" valign="top">50</td>
<td align="left" valign="top">50.02</td>
<td align="left" valign="top">38.18</td>
<td align="left" valign="top">62.23</td>
</tr>
<tr>
<td align="left" valign="top">75 (right profile)</td>
<td align="left" valign="top">45.06</td>
<td align="left" valign="top">32.5</td>
<td align="left" valign="top">58.12</td>
</tr>
<tr>
<td align="left" valign="top" rowspan="7">Base Amplitude</td>
<td align="left" valign="top">-75 (left profile)</td>
<td align="left" valign="top">0.81</td>
<td align="left" valign="top">0.29</td>
<td align="left" valign="top">1.31</td>
</tr>
<tr>
<td align="left" valign="top">-50</td>
<td align="left" valign="top">0.44</td>
<td align="left" valign="top">-0.16</td>
<td align="left" valign="top">0.99</td>
</tr>
<tr>
<td align="left" valign="top">-25</td>
<td align="left" valign="top">0.11</td>
<td align="left" valign="top">-0.49</td>
<td align="left" valign="top">0.66</td>
</tr>
<tr>
<td align="left" valign="top">0 (full front view)</td>
<td align="left" valign="top">0.07</td>
<td align="left" valign="top">-0.2</td>
<td align="left" valign="top">0.34</td>
</tr>
<tr>
<td align="left" valign="top">25</td>
<td align="left" valign="top">-0.09</td>
<td align="left" valign="top">-0.72</td>
<td align="left" valign="top">0.5</td>
</tr>
<tr>
<td align="left" valign="top">50</td>
<td align="left" valign="top">0.27</td>
<td align="left" valign="top">-0.34</td>
<td align="left" valign="top">0.84</td>
</tr>
<tr>
<td align="left" valign="top">75 (right profile)</td>
<td align="left" valign="top">0.65</td>
<td align="left" valign="top">0.11</td>
<td align="left" valign="top">1.16</td>
</tr>
<tr>
<td align="left" valign="top" rowspan="7">Peak Amplitude</td>
<td align="left" valign="top">-75 (left profile)</td>
<td align="left" valign="top">1.01</td>
<td align="left" valign="top">0.47</td>
<td align="left" valign="top">1.58</td>
</tr>
<tr>
<td align="left" valign="top">-50</td>
<td align="left" valign="top">1.68</td>
<td align="left" valign="top">1.10</td>
<td align="left" valign="top">2.30</td>
</tr>
<tr>
<td align="left" valign="top">-25</td>
<td align="left" valign="top">1.87</td>
<td align="left" valign="top">1.29</td>
<td align="left" valign="top">2.48</td>
</tr>
<tr>
<td align="left" valign="top">0 (full front view)</td>
<td align="left" valign="top">1.72</td>
<td align="left" valign="top">1.49</td>
<td align="left" valign="top">1.97</td>
</tr>
<tr>
<td align="left" valign="top">25</td>
<td align="left" valign="top">2.07</td>
<td align="left" valign="top">1.45</td>
<td align="left" valign="top">2.72</td>
</tr>
<tr>
<td align="left" valign="top">50</td>
<td align="left" valign="top">1.81</td>
<td align="left" valign="top">1.22</td>
<td align="left" valign="top">2.45</td>
</tr>
<tr>
<td align="left" valign="top">75 (right profile)</td>
<td align="left" valign="top">1.12</td>
<td align="left" valign="top">0.56</td>
<td align="left" valign="top">1.71</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Model diagnostics of the model were checked and indicated a good convergence: The potential scale reduction factor (R-hat) was of 1.00 for all parameters, the Bulk Effective Sample Size (ESS) was superior to 10.000 for all but four parameters (Intercept of the Base Amplitude: Bulk ESS = 5504; Intercept of the Peak Amplitude: Bulk ESS = 8494; Subject-level random effect of the Standard Deviation: Bulk ESS = 5965; Subject-level random effect of the Base Amplitude: Bulk ESS = 6308), and the Tail ESS was superior to 10.000 for all but one parameters (Subject-level random effect of the Standard Deviation: Bulk ESS = 5321).</p>
</sec>
<sec id="s2-5">
<title>Image analysis</title>
<p>We investigated how the distribution of energy across orientations in the experimental stimuli varied as a function of viewpoint. The amplitude and phase spectra for the image of each full spectrum face were obtained by means of a two-dimensional fast Fourier Transform and multiplied with wrapped Gaussian filters, with peak orientations centred on orientation values from 0° to 157.5° in steps of 22.5° (20° standard deviation; as in <xref ref-type="bibr" rid="c33">Goffaux &amp; Greenwood, 2016</xref>) and with peak spatial frequencies ranging from 1 to 200 cycles per image in 20 logarithmic steps. Amplitude values within each spatial frequency and orientation bin were squared and summed, then averaged across spatial frequencies per orientation bin.</p>
<p>Note that because the Fourier transform represents image energy in a discrete manner, energy at the lowest spatial frequency components can only be reliably sampled at the main cardinal and oblique ranges (i.e., 0°, 45°, 90°, 135°) especially when very narrow orientation filters are used (<xref ref-type="bibr" rid="c36">Hansen &amp; Essock, 2004</xref>). However, because orientation filters were broad and we interpret relative differences, and not absolute values, of energy distribution across viewpoints, the influence of this issue is minimal. We obtained one mean energy value per orientation band by averaging across identities.</p>
<p>We found that while face images contained most of their energy in the horizontal range, irrespective of viewpoint (<xref ref-type="fig" rid="fig3">Figure 3</xref>), the amplitude of the horizontal energy peak decreased as the face viewpoint moved away from frontal. For profile views, there was a plateau covering the horizontal plus the adjacent left and right oblique orientations for the left- and right-pointing profile views, respectively. In comparison, vertical energy was lower than any other range regardless of viewpoint. These findings replicate past evidence that most of the energy in a face image is contained in a range centred over the horizontal angle (<xref ref-type="bibr" rid="c30">Goffaux, 2019a</xref>; <xref ref-type="bibr" rid="c33">Goffaux &amp; Greenwood, 2016</xref>; <xref ref-type="bibr" rid="c47">Keil, 2009</xref>).</p>
<p>Yet the distribution of image energy as a function of orientation does not provide any insight about its potential usefulness for face identity recognition; we addressed this using model observers.</p>
</sec>
<sec id="s2-6">
<title>Model observers</title>
<p>To systematically quantify the orientation profile of the stimulus information physically available (1) to identify faces seen under a fixed viewpoint and (2) to match face identities across viewpoints (requesting tolerance to change in views), we designed two different model observers: a view-selective model observer and a view-tolerant model observer, respectively. By characterizing the orientation profile of the information available to recognize face identity in a view-selective and -tolerant manner, this approach enables emitting formal, stimulus-driven hypotheses on the information as used by humans (<xref ref-type="bibr" rid="c17">Collin et al., 2014</xref>; <xref ref-type="bibr" rid="c68">M. V. Pachai et al., 2013</xref>). Namely, it allows addressing the extent to which human view-tolerant identity recognition relies on the orientation range that is the most informative for the view at stake, or the one most stable across views.</p>
<p>Model observers performed the task on the same 10 “old” faces and 20 “new” faces as randomly selected for the human observers; we ran 22 view-selective and 22 view-tolerant model observers to match the number of tested human subjects. We presented each of the 30 faces once per condition. In each trial, the models computed the pixelwise similarity based on the calculation of a cross-correlation between a target image (either from the ‘old’ or ‘new’ set) and each of the possible exemplars of the same gender (i.e., probes) filtered at the same orientation as the target image. The distributions of cross-correlation coefficients across viewpoints and orientations are shown for each model separately in the supplementary section. The probe face with the highest correlation (i.e., the more similar) to the target image was selected as the model response (winner-take-all scheme). Depending on the correspondence between the “winner” probe and the actual target, the responses of the model observer were categorized as hit or false alarm, allowing for the computation of a sensitivity d’ in each orientation and viewpoint condition along a procedure like the one used to compute human performance.</p>
<p>In both model types, target and probes were of the same orientation range (see <xref ref-type="bibr" rid="c17">Collin et al., 2014</xref>; <xref ref-type="bibr" rid="c62">Nasanen, 1999</xref> for a similar method applied to the spatial frequency domain). Targets and probes in the view-selective model observer stemmed from a fixed viewpoint whereas the view-tolerant model observer matched target and probe separated by more than one viewpoint step to force tolerance to viewpoint in this model performance (e.g., a face at +025 of yaw was matched to faces at +075, −025, – 050, and −075). Since profile views stood at the viewpoint continuum extrema, one extra viewpoint was available for comparison. We therefore decided to drop the mirror profile view to match the number of comparisons across profile and non-profile viewpoints. Performance of the view-tolerant model was averaged across comparison viewpoints.</p>
<p>In a pilot phase, we measured the overall identification performance of each model. In the view- selective model, we had to decrease signal-to-noise ratio (SNR) of the target and probe images to .125 (face RMS contrast: .01; noise RMS contrast: .08) to avoid ceiling effects and keep overall performance close to human levels. Sensitivity d’ of the view-tolerant model was much lower than human sensitivity, even without noise. The view-tolerant model therefore processed fully visible stimuli (SNR of 1). This decreased sensitivity in the view-tolerant compared to the view-selective model is expected as none of the probes exactly matched the target at the pixel level due to viewpoint differences. In contrast to humans who rely on internally stored representations to match identity across views, the model observer lacks such internal representations and entirely relies on (inefficient) pixelwise comparisons. The distribution of d’ and 95% confidence intervals for each model observer are displayed in the supplementary section.</p>
<p>The main objective of running these model observers is to interpret human orientation sensitivity profiles considering the available information in the stimulus; following this reasoning, model observer performance is only interesting when compared to human performance. Therefore, we investigated which model observer best predicted the orientation dependence profile of human face recognition using partial Pearson correlation analyses. We controlled for the variance in image energy across orientations and viewpoints, in all partial correlation analyses to exclude the possibility that view- selective/tolerant model performance is a mere epiphenomenon of absolute oriented energy. The Fisher-Z transform of perfect correlations leading to infinite values, we replaced all −1 and 1 partial correlation coefficients to −.99 and .99, respectively, before applying Fisher Z-transformation and computing 95% confidence intervals (see <xref ref-type="table" rid="tbl1">Table 1</xref>). We (partial) correlated the orientation profiles of human and each model observer at each viewpoint separately (eight orientation vectors). For each specific viewpoint, the orientation sensitivity profile of each human observer was correlated to the average orientation sensitivity profile of either model observer, while controlling for the variance explained by (1) the average orientation sensitivity profile of the alternate model observer and (2) the average profile of orientation energy. The variability in human individual orientation profiles was taken as an estimate of the maximally achievable data-model correlation. We computed the cross-correlation between each human individual orientation sensitivity profile and the average sensitivity profile of the remaining participants. The maximally achievable correlation was the mean of these individual-to-group correlations.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s2-7">
<title>Results – Human observers</title>
<p>Upon visual inspection, group-averaged d’ plotted as a function of orientation depicted a bell-shaped function roughly centred on horizontal orientation in the frontal view condition as in the other Viewpoint conditions (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The fitting confirmed that sensitivity to identity shows a roughly similar Gaussian orientation tuning profile across viewpoints (see bell-shaped curves on <xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p>
<p>The relative stability of the human orientation sensitivity profile was corroborated by the stable and significantly positive correlations of the orientation tuning profile across viewpoints (rs&gt; .67, ps &lt; .05; <xref ref-type="fig" rid="fig4">Figure 4</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Sensitivity (i.e., performance at the recognition task) of human and model observers and image energy across viewpoints and orientations.</title>
<p><italic>Left column</italic>. 3D surf plots of the normalized energy/sensitivy across orientation and viewpoints. <italic>Middle column</italic>. Matrix representations of the normalized energy/sensitivity across orientation and viewpoints. <italic>Right column</italic>. Matrix representations of the Pearson correlation (non Fisher-z transformed) of the normalized orientation distributions of energy/sensitivity across viewpoints.</p></caption>
<graphic xlink:href="8au9jv3_fig4.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Despite human sensitivity for face identity always being best around horizontal orientations and worse around vertical ones, there were notable fluctuations in the orientation sensitivity profile across viewpoints. To better grasp these variations, we plotted the population-level estimates of the four parameters of the Gaussian curve as a function of viewpoint in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. Population-level estimates and 95% credible intervals can also be found in <xref ref-type="table" rid="tbl2">Table 2</xref>.</p>
<table-wrap id="tbl2" position="float" orientation="portrait">
<label>Table 2.</label>
<caption><title>Mean and 95% confidence interval of the (Fisher Z-transformed) partial correlation coefficients between human and each model orientation d’ profiles while controlling for the variance in image energy and alternate model.</title></caption>
<alternatives>
<graphic xlink:href="8au9jv3_tbl2.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top">Model</th>
<th align="left" valign="top" colspan="2">Viewpoint-specific</th>
<th align="left" valign="top" colspan="2">Viewpoint-tolerant</th>
</tr>
<tr>
<th align="left" valign="top"/>
<th align="left" valign="top">mean</th>
<th align="left" valign="top">95% ci</th>
<th align="left" valign="top">mean</th>
<th align="left" valign="top">95% ci</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">+075</td>
<td align="right" valign="top">0.06</td>
<td align="left" valign="top">[-0.09 0.2]</td>
<td align="right" valign="top">0.49</td>
<td align="left" valign="top">[0.30 0.67]</td>
</tr>
<tr>
<td align="left" valign="top">+050</td>
<td align="right" valign="top">0.04</td>
<td align="left" valign="top">[-0.10 0.18]</td>
<td align="right" valign="top">0.5</td>
<td align="left" valign="top">[0.34 0.66]</td>
</tr>
<tr>
<td align="left" valign="top">+025</td>
<td align="right" valign="top">0.56</td>
<td align="left" valign="top">[0.42 0.71]</td>
<td align="right" valign="top">0.3</td>
<td align="left" valign="top">[0.17 0.42]</td>
</tr>
<tr>
<td align="left" valign="top">000</td>
<td align="right" valign="top">0.79</td>
<td align="left" valign="top">[0.62 0.97]</td>
<td align="right" valign="top">0.19</td>
<td align="left" valign="top">[0.09 0.30]</td>
</tr>
<tr>
<td align="left" valign="top">-025</td>
<td align="right" valign="top">0.36</td>
<td align="left" valign="top">[0.18 0.54]</td>
<td align="right" valign="top">0.34</td>
<td align="left" valign="top">[0.17 0.51]</td>
</tr>
<tr>
<td align="left" valign="top">-050</td>
<td align="right" valign="top">0.1</td>
<td align="left" valign="top">[-0.06 0.25]</td>
<td align="right" valign="top">0.59</td>
<td align="left" valign="top">[0.45 0.72]</td>
</tr>
<tr>
<td align="left" valign="top">-075</td>
<td align="right" valign="top">-0.05</td>
<td align="left" valign="top">[-0.17 0.08]</td>
<td align="right" valign="top">0.56</td>
<td align="left" valign="top">[0.35 0.77]</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p><italic>Peak Location</italic> is estimated to lie close to 90° (grey horizontal line in <xref ref-type="fig" rid="fig3">Figure 3B</xref>) in all viewpoints except at the two profile views where peak location shifts toward adjacent obliques orientations (yaw = −75/75; see <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Specifically, peak human sensitivity tended to shift towards left and right oblique for the most extreme leftward and rightward deviations in viewpoint, respectively. <italic>Peak amplitude,</italic> which corresponds to the difference in sensitivity between the vertical and horizontal ranges, is relatively stable across all viewpoints except for the two profile views where it is lower. <italic>Base amplitude</italic>, reflecting sensitivity in the vertical range, is highest for the two profile views and progressively decreases towards the full-frontal view. This pattern suggests that vertically-oriented face content is more diagnostic for profile than for other viewpoints. The variation of <italic>Standard Deviation</italic> across viewpoints can hardly be interpreted because of the high uncertainty of the estimations; the 95% credible intervals of the <italic>Standard Deviation</italic> for the different viewpoints mostly overlap.</p>
</sec>
<sec id="s2-8">
<title>Results – View-selective model observer</title>
<p>The performance of the model observer matching faces at specific viewpoints indicates that the most diagnostic orientation range varies as a function of viewpoint (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p>
<p>Within the frontal view, optimal cues to identity are conveyed by orientations close to the horizontal angle (between 90° and 112.5°). The striking similarity of the orientation tuning profile of the view- selective model observer with the human performance indicates that at frontal view, the human visual system makes an efficient use of the information as made available in the stimulus. The peak of model performance shifted to the left and right oblique orientations closest to horizontal angle (67.5° and 112.5°) for leftward and rightward deviations in viewpoint, respectively. Additionally, the view-selective model observer increasingly relied on vertical cues as the face is turned towards its profile, in line with human performance.</p>
<p>We found only (significant) weakly positive correlations of the orientation sensitivity profiles across adjacent viewpoints (e.g., +075 and +050) confirming the orientation tuning variations across views (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p>
<p>Except for the frontal view, the orientation tuning of the view-selective model observer differs from human orientation profiles, which kept tuned to horizontal and adjacent oblique ranges irrespective of viewpoint.</p>
</sec>
<sec id="s2-9">
<title>Results – View-tolerant model observer</title>
<p>The model observer matching face identity across viewpoints performed in a drastically different way from the view-selective model observer. Recognition performance peaked sharply in horizontal angles and dropped abruptly at other ranges (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Such orientation profile was relatively stable across viewpoints as shown by the homogeneous matrix of large positive and significant Pearson correlation coefficients for sensitivity profiles across viewpoints (rs&gt; .88, ps&lt; .002).</p>
<p>Such horizontal tuning was sharper than the one observed for humans with a much more severe drop of sensitivity in the vertical orientation range. The horizontal tuning of human performance is much shallower even at frontal view. It shows a similar dip in sensitivity for the vertical range but the latter however tends to reduce when moving away from the frontal view, which does not happen for the view-tolerant model observer that keeps sharply horizontally-tuned.</p>
</sec>
<sec id="s2-10">
<title>Results – Human Versus Model observers</title>
<p>What model observer best predicts the orientation tuning profile of human face recognition? Is it the view-selective model taking advantage of the identity cues that are optimal for the viewpoint at stake, or the view-tolerant model the performance of which performance relies on the identity cues most stable across viewpoints? To address this question directly, human orientation <italic>d’</italic> profiles were correlated at the individual level with each model orientation <italic>d’</italic> profile, when partialling out the variance explained by the alternate model and image energy. We submitted the so-obtained individual Fisher-Z transformed partial correlation coefficients to a repeated-measure ANOVA with Model and Viewpoint as within-subject factors.</p>
<p>These partial correlations showed that the orientation tuning profiles of human and view-selective model observers correlated strongly for frontal and near-frontal views, approaching the maximally achievable correlation. However, partial correlations dropped steeply as the views deviated further from frontal (<xref ref-type="fig" rid="fig5">Figure 5</xref>; <xref ref-type="table" rid="tbl2">Table 2</xref>). The correlation of orientation tuning profiles between human and view-tolerant model observers was lower overall but significant across all viewpoints.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><title>Fisher-z transformed Pearson partial correlation of the orientation sensitivity profiles between humans and each model, while controlling for the alternate model and image energy.</title>
<p>Error bars show the 95% confidence intervals. The fade grey line depicts the maximally achievable correlation for the separate viewpoint conditions in the human dataset (see Methods for details).</p></caption>
<graphic xlink:href="8au9jv3_fig5.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>The repeated-measure ANOVA confirmed that the human-model correlation of orientation sensitivity profiles differed depending on Model (F(1,21)= 5.21, p = .033, η<sup>2</sup> = .017) and Viewpoint (F(3.85,80.9)= 11.064, p&lt; .001, η<sup>2</sup> = .11). The interaction between Model and Viewpoint was also significant (F(4.2,88.35)= 13.035, p&lt; .001, η<sup>2</sup> = .23).</p>
<p>We explored the impact of viewpoint on human-model correlation for the view-selective and view-tolerant models separately using a repeated-measure ANOVA with Viewpoint as a within-subject factor.</p>
<p>For the view-selective model, this analysis revealed a robust effect of Viewpoint on the human-model partial correlation (F(3.86,80.99)= 22.795, p&lt; .001, n<sup>2</sup>= −52)· For the view-tolerant model, the effect of Viewpoint on the human-model partial correlation was not significant (F(6,126)= 1.2, p= .32, η<sup>2</sup> = .054). This confirms the relatively stable human-model partial correlation coefficients across viewpoints observed for the view-tolerant model and the fluctuant profile of human-model partial correlation coefficients for the view-specific model (peaking at frontal views and decreasing as moving toward profile views).</p>
<p>Furthermore, at each viewpoint, we examined which model observer best correlated with human orientation sensitivity profile (using Holm-corrected post-hoc tests on Fisher Z-transformed partial correlation coefficients; <xref ref-type="table" rid="tbl3">Table 3</xref>). The view-tolerant model predicted a significantly larger portion of the variance in human orientation sensitivity profile at +/-050 and +/-075 viewpoints, this difference was marginal for the +050 viewpoint. It is only for the identification of frontal views of faces that the view- selective model correlated best with human data. Correlations were of similar strength for the +075, +025, −025 (and +050) viewpoints (<xref ref-type="table" rid="tbl3">Table 3</xref>).</p>
<table-wrap id="tbl3" position="float" orientation="portrait">
<label>Table 3.</label>
<caption>
<title>Difference of the human-model correlation coefficients between viewpoint-selective and view-tolerant model observers.</title>
<p>Positive t values indicate a stronger correlation with the viewpoint-selective model and negative t values stronger correlation with the view-tolerant model.</p>
</caption>
<alternatives>
<graphic xlink:href="8au9jv3_tbl3.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="left" valign="top"/>
<th align="left" valign="top" colspan="2">95% Cl for Mean Difference</th>
<th align="left" valign="top"/>
<th align="left" valign="top"/>
</tr>
<tr>
<th align="left" valign="top">View</th>
<th align="left" valign="top">Mean Difference</th>
<th align="left" valign="top">Lower</th>
<th align="left" valign="top">Upper</th>
<th align="left" valign="top">t</th>
<th align="left" valign="top">p Holm</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">+075</td>
<td align="left" valign="top">-0.47</td>
<td align="left" valign="top">-0.86</td>
<td align="left" valign="top">-0.07</td>
<td align="left" valign="top">-4.20</td>
<td align="left" valign="top">0.002</td>
</tr>
<tr>
<td align="left" valign="top">+050</td>
<td align="left" valign="top">-0.43</td>
<td align="left" valign="top">-0.82</td>
<td align="left" valign="top">-0.03</td>
<td align="left" valign="top">-3.82</td>
<td align="left" valign="top">0.01</td>
</tr>
<tr>
<td align="left" valign="top">+025</td>
<td align="left" valign="top">0.23</td>
<td align="left" valign="top">-0.16</td>
<td align="left" valign="top">0.63</td>
<td align="left" valign="top">2.08</td>
<td align="left" valign="top">1</td>
</tr>
<tr>
<td align="left" valign="top">+000</td>
<td align="left" valign="top">0.69</td>
<td align="left" valign="top">0.30</td>
<td align="left" valign="top">1.09</td>
<td align="left" valign="top">6.21</td>
<td align="left" valign="top">&lt; .001</td>
</tr>
<tr>
<td align="left" valign="top">-025</td>
<td align="left" valign="top">0.03</td>
<td align="left" valign="top">-0.37</td>
<td align="left" valign="top">0.42</td>
<td align="left" valign="top">0.23</td>
<td align="left" valign="top">1</td>
</tr>
<tr>
<td align="left" valign="top">-050</td>
<td align="left" valign="top">-0.61</td>
<td align="left" valign="top">-1.00</td>
<td align="left" valign="top">-0.21</td>
<td align="left" valign="top">-5.44</td>
<td align="left" valign="top">&lt; .001</td>
</tr>
<tr>
<td align="left" valign="top">-075</td>
<td align="left" valign="top">-0.78</td>
<td align="left" valign="top">-1.18</td>
<td align="left" valign="top">-0.39</td>
<td align="left" valign="top">-7.02</td>
<td align="left" valign="top">&lt; .001</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="s2-11">
<title>Results – Is model observers’ performance predicted by horizontal energy predominance?</title>
<p>In the above correlation analyses, we controlled the variance in image energy to yield a clean measure of the functional link between human and model observer performance. Here, we explore the possibility that the horizontal reliance of either model observer performance scales with the energy predominance of this orientation range in the stimulus image.</p>
<p>To analyse the influence of horizontal predominance on the model’s performance, we computed for each image the horizontal minus vertical difference in terms of energy and model observer sensitivity. For each identity and at each viewpoint, energy and model sensitivity difference values were submitted to a Pearson correlation. We found a positive modest correlation between view-selective model observer and image energy (Pearson r = .24, p &lt; .0005). There was no similar correlation for the view-tolerant model (Pearson rho = .025, p = .72) despite energy and view-tolerant model performance similarly peaked in the horizontal range across viewpoints.</p>
<p>What this analysis shows is that, in a given face, the stronger the predominance in horizontal energy (relative to the vertical energy), the more diagnostic the horizontally-oriented identity cues for view- selective recognition. However, horizontal energy predominance in a given face does not account for this range carrying the most stable cues across viewpoints.</p>
<p>This image-level analysis did not include human data since human sensitivity necessarily aggregates performance across trials. However, it would likely show a similar detachment from horizontal energy predominance as the view-tolerant model. Namely, humans are expected to rely on horizontal cues to match faces across views no matter the amount of horizontal energy predominance in the face at stake (e.g., George Clooney versus Brad Pitt).</p>
</sec>
</sec>
<sec id="s4" sec-type="discussion">
<title>Discussion</title>
<p>Tolerant face identity recognition is defined as the ability to extract idiosyncratic face cues despite the large variability in any given face appearance (e.g., <xref ref-type="bibr" rid="c12">Burton et al., 2016</xref>; <xref ref-type="bibr" rid="c49">Kramer et al., 2018</xref>). The spatial information supporting the tolerance of visual recognition is a central and debated topic in the field of the visual and computational neurosciences (e.g, <xref ref-type="bibr" rid="c2">Andrews et al., 2023</xref>; <xref ref-type="bibr" rid="c20">DiCarlo &amp; Cox, 2007a</xref>). By showing that the information supporting tolerance in face identity recognition is image-computable, i.e. that it can be objectively defined in the orientation domain of the face image, the present work makes a decisive advance on this question. Our finding that view-tolerant face identity recognition is driven by the horizontal range of face information yields concrete, image-driven constraints for the development of theoretical models of visual recognition.</p>
<p>Human participants performed an old/new identity recognition task on face stimuli presented under a variable view (from left to right profile) and filtered to contain a restricted range of oriented content (from 0° to 157.5° in steps of 22.5°; see <xref ref-type="fig" rid="fig2">Figure 2</xref>). For each view, recognition performance followed a Gaussian profile with a peak in the horizontal range and declining progressively when shifting towards the vertical range (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Yet, while identity recognition stayed broadly tuned to the horizontal range irrespective of the vantage view, there were moderate but notable fluctuations in the tuning profiles. First, the peak location of the Gaussian tuning profile slightly and gradually shifted away from horizontal towards right and left adjacent obliques as the face turned to left or right profile. Second, the base amplitude of the Gaussian increased drastically towards profile views. The U-shaped profile of base amplitude as a function of viewpoint shows the increasing contribution of vertically oriented information as the face moves away from the frontal view (see <xref ref-type="fig" rid="fig3">Figure 3B</xref>). In other words, the horizontal advantage of human identity recognition is largest at full-frontal views and attenuates for non-frontal views due to the increased contribution of vertical (and close-to-vertical) orientations. In other words, while human identity recognition stays tuned to the horizontal range irrespective viewpoints, it tends to increasingly rely on non-horizontal orientations as the face shifts to profile.</p>
<p>As the vantage point of a face shifts away from frontal view, morphological features related to the 3D structure of the face become more apparent (e.g., nose and cheek protuberances, jaw line, nose bridge, eyebrow head...; see e.g., <xref ref-type="bibr" rid="c82">Stephan &amp; Caine, 2007</xref> for evidence that the nose gains in informativeness from frontal to profile view). Our finding that non-horizontal ranges, particularly vertical, gain importance in non-frontal views suggests that these orientations may facilitate access to such features.</p>
<p>In contrast, other sources of information are lost when shifting towards profile views, such as the bilateral symmetric organization of the face as well as the 2D shape of features and their configuration along the x axis (e.g., size of eyebrows, interocular distance; <xref ref-type="bibr" rid="c79">Royer et al., 2016</xref>; <xref ref-type="bibr" rid="c88">Troje &amp; Bulthoff, 1996</xref>). In a way, it is surprising that this shift in accessible information did not manifest in a substantial variation of the peak location of the orientation tuning profile. Instead, the relatively stable preference for horizontal information across views suggests that it is not solely due to this range facilitating access to the 2D properties and bilateral symmetry (<xref ref-type="bibr" rid="c18">Dakin &amp; Watt, 2009</xref>), features that are most available in frontal views. Our recent work (<xref ref-type="bibr" rid="c22">Dumont et al., 2024</xref>) showed that the inversion and negation effects on identity recognition, presumed to reflect a disrupted access to 2D shape and surface cues, respectively (for shape: <xref ref-type="bibr" rid="c43">Jiang et al., 2011</xref>; <xref ref-type="bibr" rid="c57">Meinhardt-Injac et al., 2013</xref>; <xref ref-type="bibr" rid="c77">Rossion, 2009</xref>; for surface: <xref ref-type="bibr" rid="c8">Bruce &amp; Langton, 1994</xref>; <xref ref-type="bibr" rid="c48">Kemp et al., 1996</xref>; <xref ref-type="bibr" rid="c54">C. H. Liu et al., 2000</xref>; <xref ref-type="bibr" rid="c80">Russell et al., 2006</xref>; <xref ref-type="bibr" rid="c91">Vuong et al., 2005</xref>), are strongest in the horizontal range. This supports the notion that the contribution of the horizontal face information to identification goes beyond the carriage of 2D shape cues. Alternatively, the stable horizontal tuning may be due to identity recognition relying mostly on the eye region irrespective of view. Despite the eye appearance being strongly affected by changes in view, the identity information extracted from the eye region is the most diagnostic across views (<xref ref-type="bibr" rid="c82">Stephan &amp; Caine, 2007</xref>; but see <xref ref-type="bibr" rid="c79">Royer et al., 2016</xref> for conflicting results) and stays best defined in the horizontal range.</p>
<p>Using a model observer approach, we investigated how the human observer makes use of the identity cues physically available in the image across orientations and views. It is indeed important to compare the human tuning profile to a pixel-based quantification of the information that is available in the stimulus in order to characterize more formally the human sampling specificities potentially at stake when recognizing face identity. We designed two model observers to measure the information available in the stimulus to “recognize” (i.e., cross-correlate) face identity within and across views and disentangle stimulus information available for view-selective and view-tolerant recognition, respectively.</p>
<p>Let’s first summarize the findings related to the view-selective model observer. It showed that identity cues most diagnostic to discriminate face identity in a view-specific manner is highly variable across views (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The view-selective model observer was horizontally-tuned for frontal views of faces, in a manner strikingly similar to the human performance profile. This suggests that human observers make a close-to-optimal use of the orientation information in face images when identifying frontal face views. Akin to human recognition, the view-selective model progressively increased its reliance on vertically- oriented cues when the face moved from frontal to profile. However, for the view-selective model – and in contrast to human performance – this came to the expense of the horizontal tuning, which vanished completely. What these findings suggest is that the robustness of the horizontal tuning of face identity recognition at frontal view is due to this range conveying the optimal cues to identity in these specific conditions. However, they also show that the horizontal range loses its view-specific informativeness in non-frontal views; namely, differences in face identity at non-frontal views are predominantly carried by non-horizontal ranges of information. Therefore, view-specific informativeness cannot account for the generalization of the horizontal tuning profile of human identity recognition across views. In other words, the reason human recognition remains tuned to the horizontal range from frontal to profile views cannot be that this range provides optimal identity cues at each view.</p>
<p>In contrast to the view-selective model observer, the view-tolerant model observer kept sharply tuned to the horizontal range irrespective of view (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The horizontal range resulted in the highest crosscorrelation among the different views of a given face indicating that this range yields the identity cues that are the most stable across views, those that enable binding different face views into a unique representation of identity (<xref ref-type="bibr" rid="c10">Burton, 2013</xref>). This physical property of the face image likely explains why human identity recognition keeps horizontally-tuned across views (see <xref ref-type="bibr" rid="c32">Goffaux &amp; Dakin, 2010</xref> for a similar suggestion based on empirical evidence in a viewpoint-generalization identity matching task).</p>
<p>For a direct comparison of human and model performance, we quantified the variance shared between each model observer and human orientation tuning profile while controlling for image energy and the alternate model (<xref ref-type="fig" rid="fig5">Figure 5</xref>). These analyses confirmed that the view-selective model best predicted the orientation tuning of human identity recognition at frontal and close-to-frontal views, that are typically experienced during so-called face-to-face conversations, but not at profile and close-to-profile views. In contrast, the variance explained by the view-tolerant model was relatively stable across viewpoints. The particularly strong horizontal tuning of human identity recognition is thus likely due to the visual system extracting a representation that simultaneously prioritizes the orientation range conveying the cues that are the richest at frontal view and the most stable across views.</p>
<p>The horizontal tuning of human face recognition was found to be relatively broad across views, compared with the sharp tuning of the view-tolerant model observer. This tuning breadth may serve to retain information of the complex manifold of a given face appearance variability across views. Indeed, each face identity has its own specific way to vary across expression, illumination, view, etc. Such idiosyncratic within-person variability was proposed to drive familiar face identity recognition as much as the stable (horizontal) identity information (e.g., <xref ref-type="bibr" rid="c12">Burton et al., 2016</xref>; <xref ref-type="bibr" rid="c76">Ritchie &amp; Burton, 2017</xref>). Moreover, accidental properties such as head and gaze orientation carry important cues for the regulation of social interactions. Our past evidence shows that the fine discrimination of gaze direction is best supported in the vertical range (<xref ref-type="bibr" rid="c30">Goffaux, 2019</xref>). The vertical range also likely carries most of the information about the head direction. Thus, the broad horizontal tuning of identity recognition by humans may allow for the integration of such accidental properties of a face with the (more) stable identity (<xref ref-type="bibr" rid="c64">Or&amp; Wilson, 2010</xref>). For functional social interactions, it may be necessary to retain the dynamic and variable signals emitted by a face as much as its invariant aspects, and this would entail a relatively broad tuning to orientation.</p>
<p>Effect of lighting are even more disruptive than view changes (<xref ref-type="bibr" rid="c1">Adini et al., 1997</xref>; <xref ref-type="bibr" rid="c6">Braje et al., 1998</xref>; <xref ref-type="bibr" rid="c24">S. Favelle et al., 2017</xref>; <xref ref-type="bibr" rid="c39">Hill &amp; Bruce, 1996</xref>; <xref ref-type="bibr" rid="c44">A. Johnston et al., 2013</xref>; <xref ref-type="bibr" rid="c85">Tarr et al., 1998</xref>, <xref ref-type="bibr" rid="c84">2008</xref>). Image representations that emphasized the horizontal features were found to be less sensitive to changes in the direction of illumination (<xref ref-type="bibr" rid="c1">Adini et al., 1997</xref>). Future research should test whether tolerance of human face identity recognition to lighting is also supported by the horizontal range.</p>
<p>To conclude, this study demonstrates that the horizontal range carries the richest identity cues in frontal views, and the most stable across views. The orientation tuning profile of human identity recognition aligns with this combination of high diagnostic value in frontal views and cross-view stability. Taken together, this body of evidence suggests that the invariant representation of a given face, gradually learned through repeated exposure to its natural appearance statistics, relies heavily on horizontal facial information (<xref ref-type="fig" rid="fig1">Figure 1</xref>; (<xref ref-type="bibr" rid="c12">Burton et al., 2016</xref>; <xref ref-type="bibr" rid="c18">Dakin &amp; Watt, 2009</xref>; <xref ref-type="bibr" rid="c76">Ritchie &amp; Burton, 2017</xref>).</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Julie Juaneda, Zoe Strapazzon, Hana Zjakic, and Stien Van de Plas for their help with the data collection. H.D. is supported by the Belgian National Fund for Scientific Research (FRS-FNRS). V.G. is a research associate of the FRS-FNRS.</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Contributor Roles</title>
<p>A.R.-S.: Formal analysis, Conceptualization, Visualization, Writing – review &amp; editing;</p>
<p>H.D.: Formal analysis, Data curation, Visualization;</p>
<p>V.B.: Formal analysis;</p>
<p>C.J.: Methodology, Investigation;</p>
<p>V.G.: Conceptualization, Methodology, Formal analysis, Funding acquisition, Visualization, writing – original draft, Project administration, Supervision.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adini</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Moses</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Ullman</surname>, <given-names>S.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Face recognition: The problem of compensating for changes in illumination direction</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>19</volume>(<issue>7</issue>), <fpage>721</fpage>–<lpage>732</lpage>. <pub-id pub-id-type="doi">10.1109/34.598229</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andrews</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Rogers</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mileva</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Watson</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name></person-group> (<year>2023</year>). <article-title>A narrow band of image dimensions is critical for face recognition</article-title>. <source>Vision Research</source>, <volume>212</volume>, <fpage>108297</fpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2023.108297</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bach</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1996</year>). <article-title>The Freiburg Visual Acuity Test??? Automatic Measurement of Visual Acuity</article-title>: <source>Optometry and Vision Science</source>, <volume>73</volume>(<issue>1</issue>), <fpage>49</fpage>–<lpage>53</lpage>. <pub-id pub-id-type="doi">10.1097/00006324-199601000-00008</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Balas</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Saville</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>A face detection bias for horizontal orientations develops in middle childhood</article-title>. <source>Frontiers in Psychology</source>, <fpage>06</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2015.00772</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>1997</year>). <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), <fpage>433</fpage>–<lpage>436</lpage>. <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Braje</surname>, <given-names>W. L.</given-names></string-name>, <string-name><surname>Kersten</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Troje</surname>, <given-names>N. F.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Illumination effects in face recognition</article-title>. <source>Psychobiology</source>, <volume>26</volume>(<issue>4</issue>), <fpage>371</fpage>–<lpage>380</lpage>. <pub-id pub-id-type="doi">10.3758/BF03330623</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bruce</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Henderson</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Greenwood</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Hancock</surname>, <given-names>P. J. B.</given-names></string-name>, <string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Miller</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Verification of face identities from images captured on video</article-title>. <source>Journal of Experimental Psychology: Applied</source>, <volume>5</volume>(<issue>4</issue>), <fpage>339</fpage>–<lpage>360</lpage>. <pub-id pub-id-type="doi">10.1037/1076-898X.5.4.339</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bruce</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Langton</surname>, <given-names>S.</given-names></string-name></person-group> (<year>1994</year>). <article-title>The Use of Pigmentation and Shading Information in Recognising the Sex and Identities of Faces</article-title>. <source>Perception</source>, <volume>23</volume>(<issue>7</issue>), <fpage>803</fpage>–<lpage>822</lpage>. <pub-id pub-id-type="doi">10.1068/p230803</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burkner</surname>, <given-names>P.-C.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Advanced Bayesian Multilevel Modeling with the R Package brms</article-title>. <source>The R Journal</source>, <volume>10</volume>(<issue>1</issue>), <fpage>395</fpage>. <pub-id pub-id-type="doi">10.32614/RJ-2018-017</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Why has research in face recognition progressed so slowly? The importance of variability</article-title>. <source>Quarterly Journal of Experimental Psychology</source>, <volume>66</volume>(<issue>8</issue>), <fpage>1467</fpage>–<lpage>1485</lpage>. <pub-id pub-id-type="doi">10.1080/17470218.2013.800125</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Jenkins</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Hancock</surname>, <given-names>P. J. B.</given-names></string-name>, &amp; <string-name><surname>White</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Robust representations for face recognition: The power of averages</article-title>. <source>Cognitive Psychology</source>, <volume>51</volume>(<issue>3</issue>), <fpage>256</fpage>–<lpage>284</lpage>. <pub-id pub-id-type="doi">10.1016/j.cogpsych.2005.06.003</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Kramer</surname>, <given-names>R. S. S.</given-names></string-name>, <string-name><surname>Ritchie</surname>, <given-names>K. L.</given-names></string-name>, &amp; <string-name><surname>Jenkins</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Identity From Variation: Representations of Faces Derived From Multiple Instances</article-title>. <source>Cognitive Science</source>, <volume>40</volume>(<issue>1</issue>), <fpage>202</fpage>–<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1111/cogs.12231</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>White</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>McNeill</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2010</year>). <article-title>The Glasgow Face Matching Test</article-title>. <source>Behavior Research Methods</source>, <volume>42</volume>(<issue>1</issue>), <fpage>286</fpage>–<lpage>291</lpage>. <pub-id pub-id-type="doi">10.3758/BRM.42.1.286</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Cowan</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Bruce</surname>, <given-names>V.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Face Recognition in Poor-Quality Video: Evidence From Security Surveillance</article-title>. <source>Psychological Science</source>, <volume>10</volume>(<issue>3</issue>), <fpage>243</fpage>–<lpage>248</lpage>. <pub-id pub-id-type="doi">10.1111/1467-9280.00144</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caldara</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Seghier</surname>, <given-names>M. L.</given-names></string-name></person-group> (<year>2009</year>). <article-title>The Fusiform Face Area responds automatically to statistical regularities optimal for face categorization</article-title>. <source>Human Brain Mapping</source>, <volume>30</volume>(<issue>5</issue>), <fpage>1615</fpage>–<lpage>1625</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.20626</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Canoluk</surname>, <given-names>M. U.</given-names></string-name>, <string-name><surname>Moors</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Contributions of low- and high-level contextual mechanisms to human face perception</article-title>. <source>PLOS One</source>, <volume>18</volume>(<issue>5</issue>), <elocation-id>e0285255</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0285255</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Collin</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Rainville</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Watier</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Boutet</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Configural and Featural Discriminations Use the Same Spatial Frequencies: A Model Observer versus Human Observer Analysis</article-title>. <source>Perception</source>, <volume>43</volume>(<issue>6</issue>), <fpage>509</fpage>–<lpage>526</lpage>. <pub-id pub-id-type="doi">10.1068/p7531</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dakin</surname>, <given-names>S. C.</given-names></string-name>, &amp; <string-name><surname>Watt</surname>, <given-names>R. J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Biological ‘bar codes’ in human faces</article-title>. <source>Journal of Vision</source>, <volume>9</volume>(<issue>4</issue>), <elocation-id>2.1–10</elocation-id>. <pub-id pub-id-type="doi">10.1167/9.4.2</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Heering</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Dollion</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Godard</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Durand</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Baudouin</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Three-month- old infants’ sensitivity to horizontal information within faces</article-title>. <source>Developmental Psychobiology</source>, <volume>58</volume>(<issue>4</issue>), <fpage>536</fpage>–<lpage>542</lpage>. <pub-id pub-id-type="doi">10.1002/dev.21396</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Cox</surname>, <given-names>D. D.</given-names></string-name></person-group> (<year>2007a</year>). <article-title>Untangling invariant object recognition</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>11</volume>(<issue>8</issue>), <fpage>333</fpage>–<lpage>341</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Cox</surname>, <given-names>D. D.</given-names></string-name></person-group> (<year>2007b</year>). <article-title>Untangling invariant object recognition</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>11</volume>(<issue>8</issue>), <fpage>333</fpage>–<lpage>341</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dumont</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Roux-Sibilon</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Horizontal face information is the main gateway to the shape and surface cues to familiar face identity</article-title>. <source>PLOS One</source>, <volume>19</volume>(<issue>10</issue>), <elocation-id>e0311225</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0311225</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duncan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Royer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dugas</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Blais</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Fiset</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Revisiting the link between horizontal tuning and face processing ability with independent measures</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>45</volume>(<issue>11</issue>), <fpage>1429</fpage>–<lpage>1435</lpage>. <pub-id pub-id-type="doi">10.1037/xhp0000684</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Favelle</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hill</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Claes</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2017</year>). <article-title>About Face: Matching Unfamiliar Faces Across Rotations of View and Lighting</article-title>. <source>I-Perception</source>, <volume>8</volume>(<issue>6</issue>), <fpage>2041669517744221</fpage>. <pub-id pub-id-type="doi">10.1177/2041669517744221</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Favelle</surname>, <given-names>S. K.</given-names></string-name>, &amp; <string-name><surname>Palmisano</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2012</year>). <article-title>The Face Inversion Effect Following Pitch and Yaw Rotations: Investigating the Boundaries of Holistic Processing</article-title>. <source>Frontiers in Psychology</source>, <fpage>3</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2012.00563</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Favelle</surname>, <given-names>S. K.</given-names></string-name>, &amp; <string-name><surname>Palmisano</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2018</year>). <article-title>View specific generalisation effects in face recognition: Front and yaw comparison views are better than pitch</article-title>. <source>PLOS One</source>, <volume>13</volume>(<issue>12</issue>), <elocation-id>e0209927</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0209927</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiser</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Statistical learning of higher-order temporal structure from visual shape sequences</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>28</volume>(<issue>3</issue>), <fpage>458467</fpage>. <pub-id pub-id-type="doi">10.1037/0278-7393.28.3.458</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilad-Gutnick</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Harmatz</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Tsourides</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Yovel</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Sinha</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Recognizing Facial Slivers</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>30</volume>(<issue>7</issue>), <fpage>951</fpage>–<lpage>962</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01265</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2008</year>). <article-title>The horizontal and vertical relations in upright faces are transmitted by different spatial frequency ranges</article-title>. <source>Acta Psychologica</source>, <volume>128</volume>(<issue>1</issue>), <fpage>119</fpage>–<lpage>126</lpage>. <pub-id pub-id-type="doi">10.1016/j.actpsy.2007.11.005</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2019a</year>). <article-title>Fixed or flexible? Orientation preference in identity and gaze processing in humans</article-title>. <source>PLOS One</source>, <volume>14</volume>(<issue>1</issue>), <elocation-id>e0210503</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0210503</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2019b</year>). <article-title>Fixed or flexible? Orientation preference in identity and gaze processing in humans</article-title>. <source>PLOS One</source>, <volume>14</volume>(<issue>1</issue>), <elocation-id>e0210503</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0210503</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Dakin</surname>, <given-names>S. C.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Horizontal information drives the behavioral signatures of face processing</article-title>. <source>Frontiers in Psychology</source>, <fpage>1</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2010.00143</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Greenwood</surname>, <given-names>J. A.</given-names></string-name></person-group> (<year>2016</year>). <article-title>The orientation selectivity of face identification</article-title>. <source>Scientific Reports</source>, <volume>6</volume>(<issue>1</issue>), <fpage>34204</fpage>. <pub-id pub-id-type="doi">10.1038/srep34204</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Poncin</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Schiltz</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Selectivity of Face Perception to Horizontal Information over Lifespan (from 6 to 74 Year Old)</article-title>. <source>PLOS One</source>, <volume>10</volume>(<issue>9</issue>), <elocation-id>e0138812</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0138812</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hancock</surname>, <given-names>P. J. B.</given-names></string-name>, <string-name><surname>Bruce</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Recognition of unfamiliar faces</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>4</volume>(<issue>9</issue>), <fpage>330</fpage>–<lpage>337</lpage>. <pub-id pub-id-type="doi">10.1016/S1364-6613(00)01519-9</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hansen</surname>, <given-names>B. C.</given-names></string-name>, &amp; <string-name><surname>Essock</surname>, <given-names>E. A.</given-names></string-name></person-group> (<year>2004</year>). <article-title>A horizontal bias in human visual processing of orientation and its correspondence to the structural components of natural scenes</article-title>. <source>Journal of Vision</source>, <volume>4</volume>(<issue>12</issue>), <fpage>5</fpage>. <pub-id pub-id-type="doi">10.1167/4.12.5</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hauser</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Segmentation of the speech stream in a non-human primate: Statistical learning in cotton-top tamarins</article-title>. <source>Cognition</source>, <volume>78</volume>(<issue>3</issue>), <fpage>B53</fpage>–<lpage>B64</lpage>. <pub-id pub-id-type="doi">10.1016/S0010-0277(00)00132-3</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hautus</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>1995</year>). <article-title>Corrections for extreme proportions and their biasing effects on estimated values ofd’</article-title>. <source>Behavior Research Methods, Instruments, &amp; Computers</source>, <volume>27</volume>(<issue>1</issue>), <fpage>46</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.3758/BF03203619</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hill</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Bruce</surname>, <given-names>V.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Effects of lighting on the perception of facial surfaces</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>22</volume>(<issue>4</issue>), <fpage>986</fpage>–<lpage>1004</lpage>. <pub-id pub-id-type="doi">10.1037/0096-1523.22.4.986</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hill</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name>, &amp; <string-name><surname>Akamatsu</surname>, <given-names>S.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Information and viewpoint dependence in face recognition</article-title>. <source>Cognition</source>, <volume>62</volume>(<issue>2</issue>), <fpage>201</fpage>–<lpage>222</lpage>. <pub-id pub-id-type="doi">10.1016/S0010-0277(96)00785-8</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huber</surname>, <given-names>L. S.</given-names></string-name>, <string-name><surname>Geirhos</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The developmental trajectory of object recognition robustness: Children are like small adults but unlike big deep neural networks</article-title>. <source>Journal of Vision</source>, <volume>23</volume>(<issue>7</issue>), <fpage>4</fpage>. <pub-id pub-id-type="doi">10.1167/jov.23.7.4</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jeffery</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Rhodes</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Busey</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2006</year>). <article-title>View-Specific Coding of Face Shape</article-title>. <source>Psychological Science</source>, <volume>17</volume>(<issue>6</issue>), <fpage>501</fpage>–<lpage>505</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9280.2006.01735.x</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jiang</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Blanz</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Holistic processing of shape cues in face identification: Evidence from face inversion, composite faces, and acquired prosopagnosia</article-title>. <source>Visual Cognition</source>, <volume>19</volume>(<issue>8</issue>), <fpage>1003</fpage>–<lpage>1034</lpage>. <pub-id pub-id-type="doi">10.1080/13506285.2011.604360</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnston</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hill</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Carman</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Recognising Faces: Effects of Lighting Direction, Inversion, and Brightness Reversal</article-title>. <source>Perception</source>, <volume>42</volume>(<issue>11</issue>), <fpage>1227</fpage>–<lpage>1237</lpage>. <pub-id pub-id-type="doi">10.1068/p210365n</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnston</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>Edmonds</surname>, <given-names>A. J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Familiar and unfamiliar face recognition: A review</article-title>. <source>Memory</source>, <volume>17</volume>(<issue>5</issue>), <fpage>577</fpage>–<lpage>596</lpage>. <pub-id pub-id-type="doi">10.1080/09658210902976969</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jones</surname>, <given-names>S. P.</given-names></string-name>, <string-name><surname>Dwyer</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Lewis</surname>, <given-names>M. B.</given-names></string-name></person-group> (<year>2017</year>). <article-title>The Utility of Multiple Synthesized Views in the Recognition of Unfamiliar Faces</article-title>. <source>Quarterly Journal of Experimental Psychology</source>, <volume>70</volume>(<issue>5</issue>), <fpage>906918</fpage>. <pub-id pub-id-type="doi">10.1080/17470218.2016.1158302</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keil</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2009</year>). <article-title>“I Look in Your Eyes, Honey”: Internal Face Features Induce Spatial Frequency Preference for Human Face Processing</article-title>. <source>PLoS Computational Biology</source>, <volume>5</volume>(<issue>3</issue>), <elocation-id>e1000329</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000329</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kemp</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Pike</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>White</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Musselman</surname>, <given-names>A.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Perception and Recognition of Normal and Negative Faces: The Role of Shape from Shading and Pigmentation Cues</article-title>. <source>Perception</source>, <volume>25</volume>(<issue>1</issue>), <fpage>3752</fpage>. <pub-id pub-id-type="doi">10.1068/p250037</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kramer</surname>, <given-names>R. S. S.</given-names></string-name>, <string-name><surname>Young</surname>, <given-names>A. W.</given-names></string-name>, &amp; <string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Understanding face familiarity</article-title>. <source>Cognition</source>, <volume>172</volume>, <fpage>46</fpage>–<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2017.12.005</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Unsupervised Natural Experience Rapidly Alters Invariant Object Representation in Visual Cortex</article-title>. <source>Science</source>, <volume>321</volume>(<issue>5895</issue>), <fpage>1502</fpage>–<lpage>1507</lpage>. <pub-id pub-id-type="doi">10.1126/science.1160028</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Unsupervised Natural Visual Experience Rapidly Reshapes Size-Invariant Object Representation in Inferior Temporal Cortex</article-title>. <source>Neuron</source>, <volume>67</volume>(<issue>6</issue>), <fpage>1062</fpage>–<lpage>1075</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2010.08.029</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Neuronal Learning of Invariant Object Representation in the Ventral Visual Stream Is Not Dependent on Reward</article-title>. <source>The Journal of Neuroscience</source>, <volume>32</volume>(<issue>19</issue>), <fpage>6611</fpage>–<lpage>6620</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3786-11.2012</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Reassessing the 3/4 view effect in face recognition</article-title>. <source>Cognition</source>, <volume>83</volume>(<issue>1</issue>), <fpage>31</fpage>–<lpage>48</lpage>. <pub-id pub-id-type="doi">10.1016/S0010-0277(01)00164-0</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>C. H.</given-names></string-name>, <string-name><surname>Collin</surname>, <given-names>C. A.</given-names></string-name>, &amp; <string-name><surname>Chaudhuri</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Does Face Recognition Rely on Encoding of 3-D Surface? Examining the Role of Shape-from-Shading and Shape-from-Stereo</article-title>. <source>Perception</source>, <volume>29</volume>(<issue>6</issue>), <fpage>729743</fpage>. <pub-id pub-id-type="doi">10.1068/p3065</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Learning Sequence of Views of Three-Dimensional Objects: The Effect of Temporal Coherence on Object Memory</article-title>. <source>Perception</source>, <volume>36</volume>(<issue>9</issue>), <fpage>1320</fpage>–<lpage>1333</lpage>. <pub-id pub-id-type="doi">10.1068/p5778</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Megreya</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Unfamiliar faces are not faces: Evidence from a matching task</article-title>. <source>Memory &amp; Cognition</source>, <volume>34</volume>(<issue>4</issue>), <fpage>865</fpage>–<lpage>876</lpage>. <pub-id pub-id-type="doi">10.3758/BF03193433</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meinhardt-Injac</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Persike</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Meinhardt</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Holistic Face Processing is Induced by Shape and Texture</article-title>. <source>Perception</source>, <volume>42</volume>(<issue>7</issue>), <fpage>716</fpage>–<lpage>732</lpage>. <pub-id pub-id-type="doi">10.1068/p7462</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mike Burton</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Why has research in face recognition progressed so slowly? The importance of variability</article-title>. <source>Quarterly Journal of Experimental Psychology</source>, <volume>66</volume>(<issue>8</issue>), <fpage>1467</fpage>–<lpage>1485</lpage>. <pub-id pub-id-type="doi">10.1080/17470218.2013.800125</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miyashita</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>1993</year>). <article-title>Inferior Temporal Cortex: Where Visual Perception Meets Memory</article-title>. <source>Annual Review of Neuroscience</source>, <volume>16</volume>(<issue>1</issue>), <fpage>245</fpage>–<lpage>263</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.ne.16.030193.001333</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moors</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>T. L.</given-names></string-name>, &amp; <string-name><surname>Wagemans</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Configural superiority for varying contrast levels</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>82</volume>(<issue>3</issue>), <fpage>1355</fpage>–<lpage>1367</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-019-01917-y</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nalborczyk</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Batailler</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lœvenbruck</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Vilain</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bürkner</surname>, <given-names>P.-C.</given-names></string-name></person-group> (<year>2019</year>). <article-title>An Introduction to Bayesian Multilevel Models Using brms: A Case Study of Gender Effects on Vowel Variability in Standard Indonesian</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>, <volume>62</volume>(<issue>5</issue>), <fpage>1225</fpage>–<lpage>1242</lpage>. <pub-id pub-id-type="doi">10.1044/2018_JSLHR-S-18-0006</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nasanen</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Spatial frequency bandwidth used in the recognition of facial images</article-title>. <source>Vision Research</source>, <volume>39</volume>(<issue>23</issue>), <fpage>3824</fpage>–<lpage>3833</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(99)00096-6</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Newell</surname>, <given-names>F. N.</given-names></string-name>, <string-name><surname>Chiroro</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Valentine</surname>, <given-names>T.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Recognizing Unfamiliar Faces: The Effects of Distinctiveness and View</article-title>. <source>The Quarterly Journal of Experimental Psychology Section A</source>, <volume>52</volume>(<issue>2</issue>), <fpage>509</fpage>–<lpage>534</lpage>. <pub-id pub-id-type="doi">10.1080/713755813</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Or</surname>, <given-names>C. C.-F.</given-names></string-name>, &amp; <string-name><surname>Wilson</surname>, <given-names>H. R.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Face recognition: Are viewpoint and identity processed after face detection?</article-title> <source>Vision Research</source>, <volume>50</volume>(<issue>16</issue>), <fpage>1581</fpage>–<lpage>1589</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2010.05.016</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Toole</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Edelman</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Bulthoff</surname>, <given-names>H. H.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Stimulus-specific effects in face recognition over changes in viewpoint</article-title>. <source>Vision Research</source>, <volume>38</volume>(<issue>15-16</issue>), <fpage>2351</fpage>–<lpage>2363</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(98)00042-X</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Toole</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Vetter</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Blanz</surname>, <given-names>V.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Three-dimensional shape and two-dimensional surface reflectance contributions to face recognition: An application of three-dimensional morphing</article-title>. <source>Vision Research</source>, <volume>39</volume>(<issue>18</issue>), <fpage>3145</fpage>–<lpage>3155</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(99)00034-6</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pachai</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sekuler</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name>, &amp; <string-name><surname>Ramon</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Personal familiarity enhances sensitivity to horizontal structure during processing of face identity</article-title>. <source>Journal of Vision</source>, <volume>17</volume>(<issue>6</issue>), <fpage>5</fpage>. <pub-id pub-id-type="doi">10.1167/17.6.5</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pachai</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sekuler</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bennett</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Sensitivity to Information Conveyed by Horizontal Contours is Correlated with Face Identification Accuracy</article-title>. <source>Frontiers in Psychology</source>, <fpage>4</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00074</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pachai</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>P. J.</given-names></string-name>, &amp; <string-name><surname>Sekuler</surname>, <given-names>A. B.</given-names></string-name></person-group> (<year>2018</year>). <article-title>The Bandwidth of Diagnostic Horizontal Structure for Face Identification</article-title>. <source>Perception</source>, <volume>47</volume>(<issue>4</issue>), <fpage>397</fpage>–<lpage>413</lpage>. <pub-id pub-id-type="doi">10.1177/0301006618754479</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pachai</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Sekuler</surname>, <given-names>A. B.</given-names></string-name>, &amp; <string-name><surname>Bennett</surname>, <given-names>P. J.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Sensitivity to Information Conveyed by Horizontal Contours is Correlated with Face Identification Accuracy</article-title>. <source>Frontiers in Psychology</source>, <fpage>4</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00074</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pachai</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Sekuler</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name>, &amp; <string-name><surname>Ramon</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Personal familiarity enhances sensitivity to horizontal structure during processing of face identity</article-title>. <source>Journal of Vision</source>, <volume>17</volume>(<issue>6</issue>), <fpage>5</fpage>. <pub-id pub-id-type="doi">10.1167/17.6.5</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petras</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>ten Oever</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Jacobs</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Coarse-to-fine information integration in human vision</article-title>. <source>NeuroImage</source>, <volume>186</volume>, <fpage>103</fpage>–<lpage>112</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.10.086</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pitts</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>McCulloch</surname>, <given-names>W. S.</given-names></string-name></person-group> (<year>1947</year>). <article-title>How we know universals the perception of auditory and visual forms</article-title>. <source>The Bulletin of Mathematical Biophysics</source>, <volume>9</volume>(<issue>3</issue>), <fpage>127</fpage>–<lpage>147</lpage>. <pub-id pub-id-type="doi">10.1007/BF02478291</pub-id></mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramon</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2015a</year>). <article-title>Differential Processing of Vertical Interfeature Relations Due to Real-Life Experience with Personally Familiar Faces</article-title>. <source>Perception</source>, <volume>44</volume>(<issue>4</issue>), <fpage>368</fpage>–<lpage>382</lpage>. <pub-id pub-id-type="doi">10.1068/p7909</pub-id></mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramon</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2015b</year>). <article-title>Perception of global facial geometry is modulated through experience</article-title>. <source>PeerJ</source>, <volume>3</volume>, <elocation-id>e850</elocation-id>. <pub-id pub-id-type="doi">10.7717/peerj.850</pub-id></mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ritchie</surname>, <given-names>K. L.</given-names></string-name>, &amp; <string-name><surname>Burton</surname>, <given-names>A. M.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Learning faces from variability</article-title>. <source>Quarterly Journal of Experimental Psychology</source>, <volume>70</volume>(<issue>5</issue>), <fpage>897</fpage>–<lpage>905</lpage>. <pub-id pub-id-type="doi">10.1080/17470218.2015.1136656</pub-id></mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Distinguishing the cause and consequence of face inversion: The perceptual field hypothesis</article-title>. <source>Acta Psychologica</source>, <volume>132</volume>(<issue>3</issue>), <fpage>300</fpage>–<lpage>312</lpage>. <pub-id pub-id-type="doi">10.1016/j.actpsy.2009.08.002</pub-id></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roux-Sibilon</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Peyrin</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Greenwood</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Radial bias in face identification</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>, <volume>290</volume>(<issue>2001</issue>), <fpage>20231118</fpage>. <pub-id pub-id-type="doi">10.1098/rspb.2023.1118</pub-id></mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Royer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Blais</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Barnabé-Lortie</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Carré</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Leclerc</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Fiset</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Efficient visual information for unfamiliar face matching despite viewpoint variations: It’s not in the eyes!</article-title> <source>Vision Research</source>, <volume>123</volume>, <fpage>33</fpage>–<lpage>40</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2016.04.004</pub-id></mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Russell</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sinha</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Biederman</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Nederhouser</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Is Pigmentation Important for Face Recognition? Evidence from Contrast Negation</article-title>. <source>Perception</source>, <volume>35</volume>(<issue>6</issue>), <fpage>749</fpage>–<lpage>759</lpage>. <pub-id pub-id-type="doi">10.1068/p5490</pub-id></mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schuurmans</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Petras</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Backward masking reveals coarse- to-fine dynamics in human V1</article-title>. <source>NeuroImage</source>, <volume>274</volume>, <fpage>120139</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120139</pub-id></mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stephan</surname>, <given-names>B. C. M.</given-names></string-name>, &amp; <string-name><surname>Caine</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2007</year>). <article-title>What is in a View? The Role of Featural Information in the Recognition of Unfamiliar Faces across Viewpoint Transformation</article-title>. <source>Perception</source>, <volume>36</volume>(<issue>2</issue>), <fpage>189</fpage>–<lpage>198</lpage>. <pub-id pub-id-type="doi">10.1068/p5627</pub-id></mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanner</surname>, <given-names>W. P.</given-names></string-name>, &amp; <string-name><surname>Birdsall</surname>, <given-names>T. G.</given-names></string-name></person-group> (<year>1958</year>). <article-title>Definitions of <italic>d</italic> ‘ and n as Psychophysical Measures</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>30</volume>(<issue>10</issue>), <fpage>922</fpage>–<lpage>928</lpage>. <pub-id pub-id-type="doi">10.1121/1.1909408</pub-id></mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Georghiades</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Jackson</surname>, <given-names>C. D.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Identifying faces across variations in lighting: Psychophysics and computation</article-title>. <source>ACM Transactions on Applied Perception</source>, <volume>5</volume>(<issue>2</issue>), <fpage>1</fpage>–<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1145/1279920.1279924</pub-id></mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Kersten</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Bülthoff</surname>, <given-names>H. H.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Why the visual recognition system might encode the effects of illumination</article-title>. <source>Vision Research</source>, <volume>38</volume>(<issue>15-16</issue>), <fpage>2259</fpage>–<lpage>2275</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(98)00041-8</pub-id></mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2015a</year>). <article-title>Spatiotemporal information during unsupervised learning enhances viewpoint invariant object recognition</article-title>. <source>Journal of Vision</source>, <volume>15</volume>(<issue>6</issue>), <fpage>7</fpage>. <pub-id pub-id-type="doi">10.1167/15.6.7</pub-id></mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2015b</year>). <article-title>Spatiotemporal information during unsupervised learning enhances viewpoint invariant object recognition</article-title>. <source>Journal of Vision</source>, <volume>15</volume>(<issue>6</issue>), <fpage>7</fpage>. <pub-id pub-id-type="doi">10.1167/15.6.7</pub-id></mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Troje</surname>, <given-names>N. F.</given-names></string-name>, &amp; <string-name><surname>Bülthoff</surname>, <given-names>H. H.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Face recognition under varying poses: The role of texture and shape</article-title>. <source>Vision Research</source>, <volume>36</volume>(<issue>12</issue>), <fpage>1761</fpage>–<lpage>1771</lpage>. <pub-id pub-id-type="doi">10.1016/0042-6989(95)00230-8</pub-id></mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Meel</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Op de Beeck</surname>, <given-names>H. P.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Temporal Contiguity Training Influences Behavioral and Neural Measures of Viewpoint Tolerance</article-title>. <source>Frontiers in Human Neuroscience</source>, <fpage>12</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2018.00013</pub-id></mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Meel</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Op De Beeck</surname>, <given-names>H. P.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Temporal Contiguity Training Influences Behavioral and Neural Measures of Viewpoint Tolerance</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>12</volume>, <fpage>13</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2018.00013</pub-id></mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vuong</surname>, <given-names>Q. C.</given-names></string-name>, <string-name><surname>Peissig</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Harrison</surname>, <given-names>M. C.</given-names></string-name>, &amp; <string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>2005</year>). <article-title>The role of surface pigmentation for recognition revealed by contrast reversal in faces and Greebles</article-title>. <source>Vision Research</source>, <fpage>11</fpage>.</mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wallis</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Backus</surname>, <given-names>B. T.</given-names></string-name>, <string-name><surname>Langer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Huebner</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Bulthoff</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Learning illumination- and orientation-invariant representations of objects throughtemporal association</article-title>. <source>Journal of Vision</source>, <volume>9</volume>(<issue>7</issue>), <fpage>6</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1167/9.7.6</pub-id></mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wallis</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Bülthoff</surname>, <given-names>H.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Learning to recognize objects</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>3</volume>(<issue>1</issue>), <fpage>2231</fpage>. <pub-id pub-id-type="doi">10.1016/S1364-6613(98)01261-3</pub-id></mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wallis</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Bülthoff</surname>, <given-names>H. H.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Effects of temporal association on recognition memory</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>98</volume>(<issue>8</issue>), <fpage>4800</fpage>–<lpage>4804</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.071028598</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108495.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Dekker</surname>
<given-names>Tessa</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study combines behavioural psychophysics with image-computable models to contrast a view-selective model of face recognition with a view-tolerant process. Although diagnostic orientations vary with viewpoint (horizontal for frontal, vertical for profile), human recognition remains consistently tuned to horizontal information, aligning with the view-tolerant model's predictions. The evidence for view-invariant recognition is <bold>solid</bold>, though testing more plausible model variants and considering generalisability to more naturalistic face stimuli would strengthen the conclusions.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108495.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors describe the results of a single study designed to investigate the extent to which horizontal orientation energy plays a key role in supporting view-invariant face recognition. The authors collected behavioral data from adult observers who were asked to complete an old/new face matching task by learning broad-spectrum faces (not orientation filtered) during a familiarization phase and subsequently trying to label filtered faces as previously seen or novel at test. This data revealed a clear bias favoring the use of horizontal orientation energy across viewpoint changes in the target images. The authors then compared different ideal observer models (cross-correlations between target and probe stimuli) to examine how this profile might be reflected in the image-level appearance of their filtered images. This revealed that a model looking for the best matching face within a viewpoint differed substantially from human data, exhibiting a vertical orientation bias for extreme profiles. However, a model forced to match targets to probes at different viewing angles exhibited a consistent horizontal bias in much the same manner as human observers.</p>
<p>Strengths:</p>
<p>I think the question is an important one: The horizontal orientation bias is a great example of a low-level image property being linked to high-level recognition outcomes, and understanding the nature of that connection is important. I found the old/new task to be a straightforward task that was implemented ably and that has the benefit of being simple for participants to carry out and simple to analyze. I particularly appreciated that the authors chose to describe human data via a lower-dimensional model (their Gaussian fits to individual data) for further analysis. This was a nice way to express the nature of the tuning function, favoring horizontal orientation bias in a way that makes key parameters explicit. Broadly speaking, I also thought that the model comparison they include between the view-selective and view-tolerant models was a great next step. This analysis has the potential to reveal some good insights into how this bias emerges and ask fine-grained questions about the parameters in their model fits to the behavioral data.</p>
<p>Weaknesses:</p>
<p>I will start with what I think is the biggest difficulty I had with the paper. Much as I liked the model comparison analysis, I also don't quite know what to make of the view-tolerant model. As I understand the authors' description, the key feature of this model is that it does not get to compare the target and probe at the same yaw angle, but must instead pick a best match from candidates that are at different yaws. While it is interesting to see that this leads to a very different orientation profile, it also isn't obvious to me why such a comparison would be reflective of what the visual system is probably doing. I can see that the view-specific model is more or less assuming something like an exemplar representation of each face: You have the opportunity to compare a new image to a whole library of viewpoints, and presumably it isn't hard to start with some kind of first pass that identifies the best matching view first before trying to identify/match the individual in question. What I don't get about the view-tolerant model is that it seems almost like an anti-exemplar model: You specifically lack the best viewpoint in the library but have to make do with the other options. Again, this is sort of interesting and the very different behavior of the model is neat to discuss, but it doesn't seem easy to align with any theoretical perspective on face recognition. My thinking here is that it might be useful to consider an additional alternate model that doesn't specifically exclude the best-matching viewpoint, but perhaps condenses appearance across views into something like a prototype. I could even see an argument for something like the yaw-averages presented earlier in the manuscript as the basis for such a model, but this might be too much of a stretch. Overall, what I'd like to see is some kind of alternate model that incorporates the existence of the best-match viewpoint somehow, but without the explicit exemplar structure of the view-specific model.</p>
<p>Besides this larger issue, I would also like to see some more details about the nature of the cross-correlation that is the basis for this model comparison. I mostly think I get what is happening, but I think the authors could expand more on the nature of their noise model to make more explicit what is happening before these cross-correlations are taken. I infer that there is a noise-addition step to get them off the ceiling, but I felt that I had to read between the lines a bit to determine this.</p>
<p>Another thing that I think is worth considering and commenting on is the stimuli themselves and the extent to which this may limit the outcomes of their behavioral task. The use of the 3D laser-scanned faces has some obvious advantages, but also (I think) removes the possibility for pigmentation to contribute to recognition, removes the contribution of varying illumination and expression to appearance variability, and perhaps presents observers with more homogeneous faces than one typically has to worry about. I don't think these negate the current results, but I'd like the authors to expand on their discussion of these factors, particularly pigmentation. Naively, surface color and texture seem like they could offer diagnostic cues to identity that don't rely so critically on horizontal orientations, so removing these may mean that horizontal bias is particularly evident when face shape is the critical cue for recognition.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108495.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study investigates the visual information that is used for the recognition of faces. This is an important question in vision research and is critical for social interactions more generally. The authors ask whether our ability to recognise faces, across different viewpoints, varies as a function of the orientation information available in the image. Consistent with previous findings from this group and others, they find that horizontally filtered faces were recognised better than vertically filtered faces. Next, they probe the mechanism underlying this pattern of data by designing two model observers. The first was optimised for faces at a specific viewpoint (view-selective). The second was generalised across viewpoints (view-tolerant). In contrast to the human data, the view-specific model shows that the information that is useful for identity judgements varies according to viewpoint. For example, frontal face identities are again optimally discriminated with horizontal orientation information, but profiles are optimally discriminated with more vertical orientation information. These findings show human face recognition is biased toward horizontal orientation information, even though this may be suboptimal for the recognition of profile views of the face.</p>
<p>One issue in the design of this study was the lowering of the signal-to-noise ratio in the view-selective observer. This decision was taken to avoid ceiling effects. However, it is not clear how this affects the similarity with the human observers.</p>
<p>Another issue is the decision to normalise image energy across orientations and viewpoints. I can see the logic in wanting to control for these effects, but this does reflect natural variation in image properties. So, again, I wonder what the results would look like without this step.</p>
<p>Despite the bias toward horizontal orientations in human observers, there were some differences in the orientation preference at each viewpoint. For example, frontal faces were biased to horizontal (90 degrees), but other viewpoints had biases that were slightly off horizontal (e.g., right profile: 80 degrees, left profile: 100 degrees). This does seem to show that differences in statistical information at different viewpoints (more horizontal information for frontal and more vertical information for profile) do influence human perception. It would be good to reflect on this nuance in the data.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108495.1.sa3</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Roux-Sibilon</surname>
<given-names>Alexia</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dumont</surname>
<given-names>Hélène</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bremhorst</surname>
<given-names>Vincent</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jacobs</surname>
<given-names>Christianne</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Goffaux</surname>
<given-names>Valérie</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>The authors describe the results of a single study designed to investigate the extent to which horizontal orientation energy plays a key role in supporting view-invariant face recognition. The authors collected behavioral data from adult observers who were asked to complete an old/new face matching task by learning broad-spectrum faces (not orientation filtered) during a familiarization phase and subsequently trying to label filtered faces as previously seen or novel at test. This data revealed a clear bias favoring the use of horizontal orientation energy across viewpoint changes in the target images. The authors then compared different ideal observer models (cross-correlations between target and probe stimuli) to examine how this profile might be reflected in the image-level appearance of their filtered images. This revealed that a model looking for the best matching face within a viewpoint differed substantially from human data, exhibiting a vertical orientation bias for extreme profiles. However, a model forced to match targets to probes at different viewing angles exhibited a consistent horizontal bias in much the same manner as human observers.</p>
<p>Strengths:</p>
<p>I think the question is an important one: The horizontal orientation bias is a great example of a low-level image property being linked to high-level recognition outcomes, and understanding the nature of that connection is important. I found the old/new task to be a straightforward task that was implemented ably and that has the benefit of being simple for participants to carry out and simple to analyze. I particularly appreciated that the authors chose to describe human data via a lower-dimensional model (their Gaussian fits to individual data) for further analysis. This was a nice way to express the nature of the tuning function, favoring horizontal orientation bias in a way that makes key parameters explicit. Broadly speaking, I also thought that the model comparison they include between the view-selective and view-tolerant models was a great next step. This analysis has the potential to reveal some good insights into how this bias emerges and ask finegrained questions about the parameters in their model fits to the behavioral data.</p>
</disp-quote>
<p>We thank the reviewer for their positive appraisal of the importance of our research question as well as of the soundness of our approach to it.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>I will start with what I think is the biggest difficulty I had with the paper. Much as I liked the model comparison analysis, I also don't quite know what to make of the view-tolerant model. As I understand the authors' description, the key feature of this model is that it does not get to compare the target and probe at the same yaw angle, but must instead pick a best match from candidates that are at different yaws. While it is interesting to see that this leads to a very different orientation profile, it also isn't obvious to me why such a comparison would be reflective of what the visual system is probably doing. I can see that the view-specific model is more or less assuming something like an exemplar representation of each face: You have the opportunity to compare a new image to a whole library of viewpoints, and presumably it isn't hard to start with some kind of first pass that identifies the best matching view first before trying to identify/match the individual in question. What I don't get about the view-tolerant model is that it seems almost like an anti-exemplar model: You specifically lack the best viewpoint in the library but have to make do with the other options. Again, this is sort of interesting and the very different behavior of the model is neat to discuss, but it doesn't seem easy to align with any theoretical perspective on face recognition. My thinking here is that it might be useful to consider an additional alternate model that doesn't specifically exclude the best-matching viewpoint, but perhaps condenses appearance across views into something like a prototype. I could even see an argument for something like the yaw-averages presented earlier in the manuscript as the basis for such a model, but this might be too much of a stretch. Overall, what I'd like to see is some kind of alternate model that incorporates the existence of the best-match viewpoint somehow, but without the explicit exemplar structure of the view-specific model.</p>
</disp-quote>
<p>The view-tolerant model was designed so that identity needed to be abstracted away from variations in yaw to support face recognition. We believe this model aligns with the notion of tolerant recognition.</p>
<p>The tolerance of identity recognition is presumably empowered by the internal representation of the natural statistics of identity, i.e. the stable traits and (idiosyncratic) variability of a face, which builds up through the varied encounters with a given face (Burton, Jenkins et al. 2005, Burton, Jenkins and Schweinberger 2011, Jenkins and Burton 2011, Jenkins, White et al. 2011, Burton, Kramer et al. 2016, Menon, Kemp and White 2018).</p>
<p>The average of various images of a face provides its appearance distribution (i.e., variability) and central tendency (i.e., stable properties; Figure 1) and could be used as a reasonable proxy of its natural statistical properties (Burton, Jenkins et al. 2005). We thus believe that the alternate model proposed by the reviewer is relevant to existing theories of face identity recognition and agree that our current model observers do not fully capture this aspect. It is thus an excellent idea to examine the orientation tuning profile of a model observer that compares a specific view of a face to the average encompassing all views of a face identity. Since the horizontal range is proposed to carry the view-stable cues to identity, we expect that such a ‘viewpoint-average’ model observer will perform best with horizontally filtered faces and that its orientation tuning profile will significantly predict human performance across views. We expect the viewpointtolerant and viewpoint-average observers will behave similarly as they manifest the stability of the horizontal identity cues across variations in viewpoint.</p>
<disp-quote content-type="editor-comment">
<p>Besides this larger issue, I would also like to see some more details about the nature of the crosscorrelation that is the basis for this model comparison. I mostly think I get what is happening, but I think the authors could expand more on the nature of their noise model to make more explicit what is happening before these cross-correlations are taken. I infer that there is a noise-addition step to get them off the ceiling, but I felt that I had to read between the lines a bit to determine this.</p>
</disp-quote>
<p>The view-selective model responded correctly whenever successfully matching a given face identity at a specific viewpoint to itself. Since there was an exact match in each trial, resulting in uninformative ceiling performance, we decreased the signal-to-noise ratio (SNR) of the target and probe images to .125 (face RMS contrast: .01; noise RMS contrast: .08). In every trial, target and probe faces were each combined with 10 different random noise patterns. SNR was adjusted so that the overall performance of the view-selective model was in the range of human performance. We will describe these important aspects in the methods and add a supplemental with the graphic illustration of the d’ distributions of each model and human observers.</p>
<disp-quote content-type="editor-comment">
<p>Another thing that I think is worth considering and commenting on is the stimuli themselves and the extent to which this may limit the outcomes of their behavioral task. The use of the 3D laserscanned faces has some obvious advantages, but also (I think) removes the possibility for pigmentation to contribute to recognition, removes the contribution of varying illumination and expression to appearance variability, and perhaps presents observers with more homogeneous faces than one typically has to worry about. I don't think these negate the current results, but I'd like the authors to expand on their discussion of these factors, particularly pigmentation. Naively, surface color and texture seem like they could offer diagnostic cues to identity that don't rely so critically on horizontal orientations, so removing these may mean that horizontal bias is particularly evident when face shape is the critical cue for recognition.</p>
</disp-quote>
<p>We indeed got rid of surface color by converting images to gray scales. While we acknowledge that the conversion to grayscales may have removed one potential source of surface information, it is unlikely that our stimuli fully eliminated the contribution of surface pigmentation in our study. Pigmentation refers to all surface reflectance property (Russell, Sinha et al. 2006) and hue (color) is only one surface cue among others. The grayscaled 3D laser scanned faces used here still contained natural variations in crucial surface cues such as skin albedo (i.e., how light or dark the surface appears) and texture (i.e., spatial variation in how light is reflected). Both color and grayscale stimuli (2D face pictures or 3D laser scanned faces like ours) have actually been used to disentangle the role of shape and surface cues to identity recognition (e.g., Troje and Bulthoff 1996, Vuong, Peissig et al. 2005, Russell, Sinha et al. 2006, Russell, Biederman et al. 2007, Jiang, Dricot et al. 2009).</p>
<p>More fundamentally, we demonstrated that the diagnosticity of the horizontal range of face information is not restricted to the transmission of shape cues. Our recent work has indeed shown that the processing of both face shape and surface most critically relies on horizontal information (Dumont, Roux-Sibilon and Goffaux 2024).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>This study investigates the visual information that is used for the recognition of faces. This is an important question in vision research and is critical for social interactions more generally. The authors ask whether our ability to recognise faces, across different viewpoints, varies as a function of the orientation information available in the image. Consistent with previous findings from this group and others, they find that horizontally filtered faces were recognised better than vertically filtered faces. Next, they probe the mechanism underlying this pattern of data by designing two model observers. The first was optimised for faces at a specific viewpoint (viewselective). The second was generalised across viewpoints (view-tolerant). In contrast to the human data, the view-specific model shows that the information that is useful for identity judgements varies according to viewpoint. For example, frontal face identities are again optimally discriminated with horizontal orientation information, but profiles are optimally discriminated with more vertical orientation information. These findings show human face recognition is biased toward horizontal orientation information, even though this may be suboptimal for the recognition of profile views of the face.</p>
<p>One issue in the design of this study was the lowering of the signal-to-noise ratio in the viewselective observer. This decision was taken to avoid ceiling effects. However, it is not clear how this affects the similarity with the human observers.</p>
</disp-quote>
<p>The view-selective model responded correctly whenever successfully matching a given face identity at a specific viewpoint to itself. Since there was an exact match in each trial, resulting in uninformative ceiling performance, we decreased the signal-to-noise ratio (SNR) of the target and probe images to .125 (face RMS contrast: .01; noise RMS contrast: .08). In every trial, target and probe faces were each combined with 10 different random noise patterns. SNR was adjusted so that the overall performance of the view-selective model was in the range of human performance. We will describe these important aspects in the methods and add a supplemental with the graphic illustration of the d’ distributions of each model and human observers.</p>
<disp-quote content-type="editor-comment">
<p>Another issue is the decision to normalise image energy across orientations and viewpoints. I can see the logic in wanting to control for these effects, but this does reflect natural variation in image properties. So, again, I wonder what the results would look like without this step.</p>
</disp-quote>
<p>Energy of natural images is disproportionately distributed across orientations (e.g., Hansen, Essock et al. 2003). Images of faces cropped from their background as used here contain most of their energy in the horizontal range (Keil 2009, Goffaux and Greenwood 2016, Goffaux 2019). If not normalized after orientation filtering, such uneven distribution of energy would boost recognition performance in the horizontal range across views. Normalization was performed across our experimental conditions merely to avoid energy from explaining the influence of viewpoint on the orientation tuning profile.</p>
<p>We are not aware of any systematic natural variations of energy across face views. To address this, we measured face average energy (i.e., RMS contrast) in the original stimulus set, i.e., before the application of any image processing or manipulation. Background pixels were excluded from these image analyses. Across yaws, we found energy to range between .11 and .14 on a 0 to 1 grayscale. This is moderate compared to the range of energy variations we measured across identities (from .08 to .18). This suggests that variations in energy across viewpoints are moderate compared to variations related to identity. It is unclear whether these observations are specific to our stimulus set or whether they are generalizable to faces we encounter in everyday life. They, however, indicate that RMS contrast did not substantially vary across views in the present study and suggest that RMS normalization is unlikely to have affected the influence of viewpoint on recognition performance.</p>
<p>Nonetheless, we acknowledge the importance of this issue regarding the trade-off between experimental control and stimulus naturalness, and we will refer to it explicitly in the methods section.</p>
<disp-quote content-type="editor-comment">
<p>Despite the bias toward horizontal orientations in human observers, there were some differences in the orientation preference at each viewpoint. For example, frontal faces were biased to horizontal (90 degrees), but other viewpoints had biases that were slightly off horizontal (e.g., right profile: 80 degrees, left profile: 100 degrees). This does seem to show that differences in statistical information at different viewpoints (more horizontal information for frontal and more vertical information for profile) do influence human perception. It would be good to reflect on this nuance in the data.</p>
</disp-quote>
<p>Indeed, human performance data indicates that while identity recognition remains tuned to horizontal information, horizontal tuning shows some variation across viewpoints. We primarily focused on the first aspect because of its direct relevance to our research objective, but also discussed the second aspect: with yaw rotation, certain non-horizontal morphological features such as the jaw line or nose bridge, etc. may increasingly contribute to identity recognition, whereas at frontal or near frontal views, features are mostly horizontally-oriented (e.g., Keil 2008, Keil 2009). We will relate this part of the discussion more explicitly to the observation of the fluctuation of the peak location as a function of yaw.</p>
</body>
</sub-article>
</article>