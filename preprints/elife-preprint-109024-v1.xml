<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">109024</article-id>
<article-id pub-id-type="doi">10.7554/eLife.109024</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.109024.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Developmental Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>From salience to semantics: multilevel hierarchical contingencies organise parent-infant joint attention</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<name>
<surname>Labendzki</surname>
<given-names>Pierre</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="author-notes" rid="FN1">*</xref>
<email>u2176677@uel.ac.uk</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Perapoch Amadó</surname>
<given-names>Marta</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="author-notes" rid="FN1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Viswanathan</surname>
<given-names>Narain K</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Northrop</surname>
<given-names>Tom J</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ives</surname>
<given-names>James</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lancaster</surname>
<given-names>Katie L</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Greenwood</surname>
<given-names>Emily</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Esposito</surname>
<given-names>Giovanni</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Phillips</surname>
<given-names>Emily AM</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jones</surname>
<given-names>Emily JH</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Goupil</surname>
<given-names>Louise</given-names>
</name>
<xref ref-type="aff" rid="A3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wass</surname>
<given-names>Sam V</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/057jrqr44</institution-id><institution>University of East London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02mb95055</institution-id><institution>Birkbeck University of London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02rx3b187</institution-id><institution>University Grenoble Alpes (FR, LPNC)</institution></institution-wrap>, <city>Grenoble</city>, <country country="FR">France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="FN1" fn-type="equal"><label>*</label><p>Joint first authors</p></fn>
</author-notes>
<pub-date pub-type="epub">
<day>01</day>
<month>03</month>
<year>2025</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2025-11-25">
<day>25</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP109024</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2025-09-10">
<day>10</day>
<month>09</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-08-29">
<day>29</day>
<month>08</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.13140/RG.2.2.26638.47686"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Labendzki et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Labendzki et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-109024-v1.pdf"/>
<abstract>
<p>Despite strong evidence that children learn more effectively from face-to-face interactions than from screens, we still understand relatively little about the dynamic, adaptive processes through which inter-personal contingency enhances attention and learning during live interactions. In this study, we investigate how social signals during early interactions operate across multiple hierarchical levels, ranging from low-level salience cues to higher-order features. Specifically, we examine how mothers dynamically and reciprocally adjust their behaviours across these levels in response to their infants’ attention during play. To achieve this, we developed a suite of novel, information-theory-based methods to quantify naturalistic audio-visual-semantic behaviours. Using time-series analyses, we assessed moment-by-moment associations between infant attention and both lower-order features (e.g., spectral flux of ambient noise and maternal vocalizations, maternal face and hand movement) and higher-order features (e.g., speech information rate, facial expression novelty, semantic surprisal, and toy naming) in tabletop interactions involving 67 mother-infant dyads (5- and 15-month-olds). Our findings suggest that, from early infancy, the information infants perceive is continuously and dynamically modulated across multiple hierarchical levels, contingent on their behaviour and attention. When infants focus on objects, mothers reduce low-level sensory input, minimising distractions. Conversely, increases in object naming and high-level information content associate with increases in sustained attention. These results indicate that maternal behaviours are both driven by and predictive of infant attention, and that, even from early development, attention involves interactive processes which unfold across multiple levels, from salience to semantics.</p>
</abstract>
<kwd-group>
<title>Keywords</title>
<kwd>Infant attention</kwd>
<kwd>salience</kwd>
<kwd>semantics</kwd>
<kwd>development</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>1. Introduction</title>
<p>Infants construct and refine predictive models of their world, seeking out stimuli that maximize their learning potential (<xref ref-type="bibr" rid="c33">Gerken et al., 2011</xref>; <xref ref-type="bibr" rid="c50">Kidd et al., 2014</xref>; <xref ref-type="bibr" rid="c80">Poli et al., 2020</xref>). These predictions allow them to navigate their environments more effectively (<xref ref-type="bibr" rid="c52">Köster et al., 2020</xref>), orienting their attention toward information that optimizes their ability to learn (<xref ref-type="bibr" rid="c12">Berger &amp; Posner, 2022</xref>). Maximizing learning from the environment requires infants to build hierarchical predictive models that span a broad range of representations, from low-level features such as fast-varying fluctuations in amplitude, pitch and luminance, through to higher-order features which operate over slower time-scales, and which arise from the hierarchical integration of low-level features into cognitively meaningful information (<xref ref-type="bibr" rid="c42">Heilbron et al., 2022</xref>; <xref ref-type="bibr" rid="c53">Kothinti &amp; Elhilali, 2023</xref>).</p>
<p>When viewing TV and movie clips, for example, neuroimaging evidence suggests that adult brains differentially track low- and higher-level features (<xref ref-type="bibr" rid="c18">Chang et al., 2022</xref>). Temporally coarse-grained predictions originating in the default mode network inform temporally fine-grained predictions in primary auditory and motor areas (<xref ref-type="bibr" rid="c8">Baldassano et al., 2017</xref>; <xref ref-type="bibr" rid="c18">Chang et al., 2022</xref>; <xref ref-type="bibr" rid="c40">Hasson et al., 2015</xref>). In infants, these processes are thought to be more rudimentary. Infant brains represent only longer events, even in early visual regions, with no time-scale hierarchy (<xref ref-type="bibr" rid="c115">Yates et al., 2022</xref>). Whereas younger infants’ gaze is better predicted by layers of a neural network model corresponding to lower-level areas of the ventral visual stream, older infants’ gaze was better predicted by higher-level layers (<xref ref-type="bibr" rid="c48">Kiat et al., 2022</xref>). Over time, information increasingly becomes integrated over more coarse-grained spatial and temporal scales (<xref ref-type="bibr" rid="c76">Pempek et al., 2010</xref>). As multiple physical aspects of an event reach an infant, they must be related to each other for the infant to model a complete representation of that event (<xref ref-type="bibr" rid="c5">Bahrick &amp; Lickliter, 2000</xref>; <xref ref-type="bibr" rid="c25">Dionne-Dostie et al., 2015</xref>).</p>
<sec id="s1-1">
<title>1.1 The Role of Caregivers in Scaffolding Infant Attention</title>
<p>However, these previous studies do not account for the fact that most early real-world experiences are embedded in social interaction (<xref ref-type="bibr" rid="c17">Carretero &amp; Español, 2016</xref>; <xref ref-type="bibr" rid="c26">Español &amp; Shifres, 2015</xref>). Traditional approaches use experimental tasks that conceptualise attention as a passive response to incoming sensory information (<xref ref-type="bibr" rid="c92">L. Smith &amp; Gasser, 2005</xref>; <xref ref-type="bibr" rid="c106">Wass &amp; Jones, 2023</xref>; <xref ref-type="bibr" rid="c107">Wass, 2014</xref>; <xref ref-type="bibr" rid="c108">Wass &amp; Goupil, 2022</xref>). But in reality, infants actively modify their environments through their interactions, shaping the information they receive based on their own behaviour and experiences (<xref ref-type="bibr" rid="c3">Anderson et al., 2022</xref>; <xref ref-type="bibr" rid="c66">Mendez et al., 2024</xref>; <xref ref-type="bibr" rid="c92">L. Smith &amp; Gasser, 2005</xref>; <xref ref-type="bibr" rid="c108">Wass &amp; Goupil, 2022</xref>). This is true of children’s interactions with physical objects and features in their environment (<xref ref-type="bibr" rid="c3">Anderson et al., 2022</xref>, <xref ref-type="bibr" rid="c2">2024</xref>) but it is especially true of their early social interactions.</p>
<p>Extensive evidence already shows that caregivers shape infants’ attention and learning by adapting their behaviour to align with the developmental needs of the infant (<xref ref-type="bibr" rid="c15">Brennan et al., 2010</xref>; <xref ref-type="bibr" rid="c85">Schick et al., 2022</xref>; <xref ref-type="bibr" rid="c102">Tippenhauer et al., 2020</xref>). One well-studied example is the use of infant-directed (ID) speech, which is characterized by exaggerated acoustic features such as higher pitch, greater pitch variability, and slower amplitude modulation (<xref ref-type="bibr" rid="c22">M. Cooke et al., 2014</xref>; <xref ref-type="bibr" rid="c43">Hilton et al., 2022</xref>). These features, although not completely universal, act to capture and maintain infant attention effectively while facilitating language learning (<xref ref-type="bibr" rid="c75">Nencheva &amp; Lew-Williams, 2022</xref>; <xref ref-type="bibr" rid="c82">Räsänen et al., 2018</xref>). Additionally, caregivers simplify semantic complexity in their speech, adjusting it to match the infant’s vocabulary development (<xref ref-type="bibr" rid="c89">Schwab et al., 2018</xref>; <xref ref-type="bibr" rid="c88">Schwab &amp; Lew-Williams, 2016</xref>) and further supporting word learning. Caregivers also modify their facial expressions and hand movements to maintain infant engagement and visual attention (<xref ref-type="bibr" rid="c20">Chong et al., 2003</xref>; <xref ref-type="bibr" rid="c51">Kliesch et al., 2022</xref>; <xref ref-type="bibr" rid="c94">Stern, 1974</xref>; <xref ref-type="bibr" rid="c103">van Schaik et al., 2020</xref>). These adaptations are thought to be constrained by diverging need (<xref ref-type="bibr" rid="c19">Chater &amp; Vitányi, 2002</xref>; <xref ref-type="bibr" rid="c113">Woźniak &amp; Knoblich, 2022</xref>): predictable elements help infants integrate upcoming stimuli, while novel stimuli capture attention by introducing uncertainty (<xref ref-type="bibr" rid="c49">Kidd et al., 2012</xref>, <xref ref-type="bibr" rid="c50">2014</xref>; <xref ref-type="bibr" rid="c68">Meyer et al., 2023</xref>; <xref ref-type="bibr" rid="c55">Labendzki et al., pre-print</xref>).</p>
</sec>
<sec id="s1-2">
<title>1.2 “Alive” joint attention</title>
<p>Most current quantitative research on infant attention has focused on global differences, such as average comparisons between infant-directed versus adult-directed communication. These approaches miss the dynamic, reciprocal, “alive” (<xref ref-type="bibr" rid="c28">Fogel &amp; Garvey, 2007</xref>) nature of social interactions. Although countless studies have used observer ratings to measure maternal sensitivity and dyadic mutuality (<xref ref-type="bibr" rid="c21">J. E. Cooke et al., 2022</xref>; <xref ref-type="bibr" rid="c27">Feldman, 2007</xref>; <xref ref-type="bibr" rid="c74">Murray et al., 2016</xref>), relatively few studies have quantitatively observed how infants and caregivers continuously respond and adapt to each other’s behaviours in a bidirectional exchange (<xref ref-type="bibr" rid="c10">Beebe et al., 2016</xref>; <xref ref-type="bibr" rid="c27">Feldman, 2007</xref>; <xref ref-type="bibr" rid="c47">Jaffe et al., 2001</xref>).</p>
<p>Previous research has shown that mothers dynamically adjust both the pitch of their speech and their modulation patterns (rate of change of pitch) based on infants’ attentional states (<xref ref-type="bibr" rid="c79">Phillips et al., 2023</xref>; <xref ref-type="bibr" rid="c84">Reisner et al., 2024</xref>; <xref ref-type="bibr" rid="c93">N. A. Smith &amp; Trainor, 2008</xref>). We also know that caregivers modulate their gaze contingent on infant behaviours (<xref ref-type="bibr" rid="c77">Perapoch Amadó et al., 2025</xref>), and that mothers’ object labelling and handling are often contingent on infants’ gaze (<xref ref-type="bibr" rid="c36">Goupil et al., 2024</xref>; <xref ref-type="bibr" rid="c97">Sun &amp; Yoshida, 2022</xref>). From previous research we also know that object naming enhances infant attention during joint play (<xref ref-type="bibr" rid="c65">Mendez et al., 2023</xref>; <xref ref-type="bibr" rid="c98">Sun &amp; Yoshida, 2024</xref>). This creates a shared history of interaction, enabling both partners to predict and adapt to each other’s actions, ultimately fostering a dynamic communication system (<xref ref-type="bibr" rid="c16">Bruner, 1974</xref>; <xref ref-type="bibr" rid="c29">Fogel et al., 1992</xref>; <xref ref-type="bibr" rid="c37">Gratier &amp; Magnier, 2012</xref>; <xref ref-type="bibr" rid="c41">Hasson &amp; Frith, 2016</xref>; <xref ref-type="bibr" rid="c62">Malloch &amp; Trevarthen, 2009</xref>; <xref ref-type="bibr" rid="c73">Murray, 2014</xref>; <xref ref-type="bibr" rid="c83">Ravreby et al., 2022</xref>; <xref ref-type="bibr" rid="c105">Vygotsky et al., 1978</xref>).</p>
<p>As yet, though, most previous that has examined dynamical processes during real-time caregiver-infant interactions has concentrated on examining different individual features in isolation (<xref ref-type="bibr" rid="c10">Beebe et al., 2016</xref>; <xref ref-type="bibr" rid="c39">Ham &amp; Tronick, 2009</xref>; <xref ref-type="bibr" rid="c57">Lavelli &amp; Fogel, 2013</xref>; <xref ref-type="bibr" rid="c79">Phillips et al., 2023</xref>). No previous research has examined how these reciprocal interactive influences operate across multiple hierarchical layers, ranging from low-level salience cues (such as physical movement, and vocal pitch and amplitude fluctuations) through to higher-level features that represent meaning and context. Understanding this is crucial from both a practical perspective, for improving interventions that target interaction dynamics to improve long-term infant outcomes (e.g. <xref ref-type="bibr" rid="c74">Murray et al., 2016</xref>); and from a theoretical perspective, for enriching our understanding of how multimodal and hierarchical features interactively influence social attention, and how these influences change and develop with age.</p>
</sec>
<sec id="s1-3">
<title>1.3 Current Study</title>
<p>The present study examines how infant-caregiver interaction change between 5 and 15 months, which is the period when the capacity for infant-led joint attention is thought to emerge (<xref ref-type="bibr" rid="c72">Mundy et al., 2009</xref>). We manually coded the gaze of mothers and their infants as they played together with toys (see <xref ref-type="section" rid="s2-2">2.2</xref> for more details) and calculated a range of lower-level and higher-level features of the interaction. Lower-level features included ‘spectral flux’, i.e. the instantaneous change in audio spectral content, which was calculated separately for ambient noise and for sections where the mother was vocalising; and differentials (i.e. rate of change) of maternal face and hand movement, which drive low-level visual and auditory salience (<xref ref-type="bibr" rid="c46">Itti &amp; Baldi, 2009</xref>). Higher-level features operate over longer timescales and involve the integration of low-level features into information that is cognitively meaningful. These included information rate (i.e., the rate of meaningful data transmitted per time unit), semantic surprisal (i.e., log probability of the upcoming word given the preceding words), toy naming, and facial novelty (i.e., the predictability of mothers’ facial expression given previous expression) (see <xref ref-type="section" rid="s2-2">2.2</xref> for equations and formal definitions). The distinction presented here between lower- and higher-level features is a practical simplification; in reality, lower- and higher-level features exist along a continuum (<xref ref-type="bibr" rid="c38">Gwilliams et al., 2024</xref>; <xref ref-type="bibr" rid="c42">Heilbron et al., 2022</xref>).</p>
<p>We had three main predictions. <bold>Prediction 1: Maternal behaviours and infant attention.</bold> Across the age groups, mothers’ behaviour will be tightly coupled to changes in infant attention to objects over modality (i.e., audio and visual) and levels (low and high order). <bold>Prediction 2: Age-dependent effectiveness of features.</bold> Lower-level features will be more effective in both capturing and maintaining younger infant’s attention to objects, while higher-level features will be more effective with older infants. <bold>Prediction 3: Changing temporal dynamics.</bold> The temporal relationship between fluctuations in maternal features and infants’ attention to objects will change with age. At 5 months, lower-level variables will more frequently precede infant attention. However, as infants increasingly take the lead in play, we predict that changes in higher-level features of the mother’s behaviour will more often follow the infant’s attention to objects.</p>
<p>To explore these three predictions, we first calculated the mean and median for all the features of the interaction we studied. Following this, we built a cross-correlation matrix to understand how our behavioural variables (e.g. the lower-level and higher-level features of the interaction) inter-relate. Finally, we performed cross-correlations to examine the temporal relationship between each lower- and higher-order feature and the infant’s attention to objects.</p>
<p>For our primary analyses, infants’ attention to objects was conceptualized as the ‘on-task’ behaviour reflecting active engagement with the central goal. In addition, however, because our tabletop play setting allowed for three gaze locations (object, partner, inattentive) we also wished to examine the possibility that decreases in infant’s attention to objects might be more readily explained as increases in infants’ attention to the mother’s face. To test this, we also include analyses in the Supplementary Materials in which we performed the same cross-correlations but with infants’ looks to the mother’s face rather than to objects as the dependent variable.</p>
</sec>
</sec>
<sec id="s2" sec-type="materials|methods">
<title>2. Materials and methods</title>
<sec id="s2-1">
<title>2.1 Participants</title>
<p>Participants were typically developing infants and their mothers. Only mothers were included because of practical difficulties in recruiting sufficient fathers to provide a gender-matched sample. The catchment area for this study was East London, including boroughs such as Tower Hamlets, Hackney and Newham. Further demographic details on the sample are given in <xref ref-type="table" rid="tblS1">Table S1</xref>.</p>
<p>Participants were recruited postnatally through advertisements at local baby groups, local preschools/nurseries, community centres and targeted social media campaigns aimed at all parents in the area, from databases of prior projects and via word-of-mouth. Informed consent and authorisation for publication has been obtained by the caregivers featured in <xref ref-type="fig" rid="fig1">Fig 1</xref> and in <xref ref-type="fig" rid="figS1">Fig.S1</xref>. Ethical approval was obtained from the University of East London ethics committee (application ID: ETH2021-0076).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Illustration of analyses with the central figure showing all low- and high-level features synced.</title>
<p>A) Spectral flux being computed as the differences between consecutive spectrum (green area between the current and previous spectrogram). B) Hand movement is computed as the average distance travelled by the right and left hands. C) Face movement is computed as the sum of derivative of the eye-to-brow distance (blue dot on the eye to red dot on the eyebrow) and the mouth opening distance (blue dot on the upper lip to red dot on the lower lip). D) Object naming was obtain using the automatic transcription and a query for the specific toys present during the interaction. E) Semantic surprisal. For each word, a probability distribution was obtain using GPT2 prompted with the previous words, the semantic surprisal is the log probability of the observed word. F) Information rate. For each word, a cumulative complexity (upper) is computed using lossless compression algorithms, taking the derivative gives the information rate (lower). G) Facial novelty. For every frame, the facial expression is estimated, and an information distance is computed using the Kullback-Leibler between consecutive frames.</p></caption>
<graphic xlink:href="26638.47686v2_fig1.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Initial exclusion criteria include complex medical conditions, known developmental delays, prematurity, uncorrected vision difficulties and parents below 18 years of age. Further exclusion criteria as well as final numbers of data included in each of the analyses for both samples are summarised in <xref ref-type="table" rid="tblS2">Table S2</xref>. The final samples included 33 5-month-old infants (15 females) and 34 15-month-old infants (17 females) and their mothers. Data were analysed in a cross-sectional manner. Average age for infants was 5.23 months (std= 0.5) and 15.75 months (std= 1.15) respectively. Average age for mothers was 35.31 years (std = 3.9, N= 29) at 5 months and 37.41 years (std= 3.82, N= 28) at 15 months. Two previous studies on joint attention and imitation have already reported analysis of the same participant cohort (<xref ref-type="bibr" rid="c77">Perapoch Amadó et al., 2025</xref>; <xref ref-type="bibr" rid="c104">Viswanathan et al., pre-print</xref>) although it is the first time that the rest of the data (i.e. lower- and higher-order variables) are analysed and reported.</p>
</sec>
<sec id="s2-2">
<title>2.2 Experimental design</title>
<p>Mothers and infants were seated facing each other on opposite sides of a table. Infants were seated either in a highchair or on a researcher’s lap, within easy reach of the toys (see <xref ref-type="fig" rid="figS1">Fig S1A</xref>). At the beginning of the joint play session, a researcher placed the toys on the table and asked the mothers to “play with their infants just as they would at home”. During the play session, researchers stayed behind a divider out of view of both the mother and the infant. The same three toys were used for each age group (see <xref ref-type="fig" rid="figS1">Fig S1D</xref>). The average duration of the joint play interactions was 5.06 minutes (std= 1.37) at 5 months and 6.17 minutes (std= 1.64) at 15 months (<xref ref-type="fig" rid="figS1">Fig S1C</xref>). Average duration differed significantly between 5- and 15-months (t(65) =-3.014, p=0.003). Given that the analyses conducted here are relative to the duration of each interaction or on specific events (e.g. infant looks at objects), variations in interaction durations should not be an issue.</p>
<p>The interactions were filmed using three Canon LEGRIA HF R806 camcorders recording at 50 frames-per-second (fps). Two cameras were placed in front of the infant, one on each side of the mother, and another one was placed in front of the mother, just behind the right side of the infant. All cameras were positioned so that the infant’s and the mother’s gaze, as well as the three toys placed on the table, were always visible (see <xref ref-type="fig" rid="figS1">Fig S1A</xref>). Microphone data were also collected using two wireless omni-directional Lavalier microphones recording at 44.1kHz in wav format, one attached to the mothers’ clothing and the other to the infant’s highchair.</p>
<p>Of note, brain activity was also recorded from both the infants and their mothers, at both ages, using a 64-channel BioSemi gel-based ActiveTwo EEG system. However, this data is not included in the current manuscript.</p>
</sec>
<sec id="s2-3">
<title>2.3 Data processing</title>
<sec id="s2-3-1">
<title>2.3.1 Synchronisation of the different datasets</title>
<p>The cameras pointing at the participants were synchronised via radio frequency (RF) receiver LED boxes attached to each camera. The RF boxes received trigger signals from a single source (computer running Matlab) at the beginning and end of the play session and concurrently triggered light pulses to LED lights visible to each camera, along with an audible beep. The synchronisation of the video coding was conducted offline by aligning the times of the LED lights of the three cameras and checking that the durations matched. The audio data from the two microphones was synchronised from the start using the Zoom H4N PRO Handy Recorder, which enables simultaneous recording. Finally, Adobe Premiere Pro was used to synchronise the video with the audio data, allowing us to align the two datasets in time and estimate the lag between them.</p>
</sec>
<sec id="s2-3-2">
<title>2.3.2 Gaze behaviour coding and processing</title>
<p>The looking behaviour of the infants and their mothers was manually coded offline on a frame-by-frame basis, at 50fps. The start of a look was the first frame in which the gaze was static after moving to a new location. The following categories of gaze were coded: looks to objects (focusing on one of the three objects), looks to partner (looking at their partner), inattentive (not looking to any of the objects nor the partner) and uncodable (see <xref ref-type="fig" rid="figS1">Fig S1B</xref>). Uncodable moments included periods where: 1) their gaze was blocked or obscured by an object and/or their own hands, 2) their eyes were outside the camera frame, and/or 3) a researcher was within the camera frame.</p>
<p>To assess inter-rater reliability, ~22% of the data (15 datasets) were double coded by a second coder and both Cohen’s kappa and observed agreement were calculated. There was substantial agreement (κ= 0.628, std= 0.134; Kappa error= 0.005, std= 0.001; observed agreement = 0.751, std= 0.092) (<xref ref-type="bibr" rid="c56">Landis &amp; Koch, 1977</xref>). Looking behaviour data was then processed such that any look preceding and following an “uncodable” period was excluded from further analyses. Similarly, both the first and the last look of every interaction were also excluded from further analyses.</p>
</sec>
<sec id="s2-3-3">
<title>2.3.3 Correlation analyses and calculation of significance</title>
<p>To calculate the associations between different features of the interaction we first performed a standard correlation analysis between all variables (<xref ref-type="fig" rid="fig3">Fig 3</xref>). Next, we conducted cross-correlation analyses to examine the dynamic associations between interaction features and infant attention (3.3) (i.e., whether one variable leads and the other follows) (<xref ref-type="fig" rid="figS5">SM Fig 5</xref>). First, we first converted the infants’ looking behaviour data into binary arrays of ones (looking at an object) and zeros (not looking at an object) (Wass et al, 2019; <xref ref-type="bibr" rid="c79">Phillips et al., 2023</xref>). Following this, we linearly detrended all the time series datasets and calculated the cross-correlations between all the different features of the interaction (3.2), and between these and infant attention (3.3) separately.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Descriptive analyses on lower- and higher-level features.</title>
<p>Violin plots showing the average at a group level of lower-level: Spectral flux - ambient (A); spectral flux - vocalisations (B); facial movement (C); hand movement (D), and higher-level features: object naming (E); information rate (F); semantic surprisal (G); facial novelty (H). Individual dots represent the data for each participant, in orange is data at 5 months, and in purple is data at 15 months. Red dots are showing the mean and red lines are showing the median. Asterisks indicate significance (* = <italic>p-adj</italic> &lt;0.05, ** = <italic>p-adj</italic> &lt;0.01, *** = <italic>p-adj</italic> &lt; 0.001).</p></caption>
<graphic xlink:href="26638.47686v2_fig2.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Correlation matrix between all features of the interaction at 5 and 15 months.</title>
<p>Each square shows the correlation between two maternal behavioural variables with a zero lag. The bottom-left triangle shows correlation values for the 5 months visit, and the top-right triangle shows correlation values for the 15 months visit.</p></caption>
<graphic xlink:href="26638.47686v2_fig3.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Because cross-correlations are not time locked to specific moments (i.e. onset of an infant look to object) but instead are conducted on two time series (e.g. infant looks to objects and information rate) as a whole (see <xref ref-type="fig" rid="fig1">Fig 1</xref>), the strength of the overall correlation is weakened by the fact that periods of expected stronger correlation are balanced by weaker correlations where we would not expect any correlation at all (<xref ref-type="bibr" rid="c114">Xu et al., 2020</xref>). This can lead to very small correlation coefficients (e.g. around r = ±0.05), albeit sometimes significant when compared against the permuted correlations (see below).</p>
<p>To assess whether the results from the cross-correlations were significantly different than chance we generated permuted data and compared it against the observed data using Cluster-Based Permutation (CBP) tests. To generate permuted data, we computed the cross-correlation between 500 random combinations of time series data from different dyads. For example, for each feature, the time series data of one participant (e.g. information rate from dyad 1) was randomly paired with the time series data of another participant (e.g. infant looking behaviour from dyad 13). We repeated this 500 times and then performed a CBP test to examine significant differences between the results from the observed (i.e. real) and the permuted data.</p>
<p>The CBP test statistic was calculated using a function from FieldTrip (<xref ref-type="bibr" rid="c63">Maris &amp; Oostenveld, 2007</xref>) called “ft_timelockstatistics”. This nonparametric framework allowed us to both control for the multiple comparison problem that arises from the fact that the effect of interest is evaluated many times (e.g. changes in mothers’ spectral flux around infant looks to objects), and to reduce the potential for false negative effects (<xref ref-type="bibr" rid="c67">Meyer et al., 2021</xref>).</p>
<p>Please refer to <xref ref-type="fig" rid="figS4">Fig S4</xref> for a visual guide on interpreting findings from a cross-correlation analysis, illustrating the interpretation of positive and negative cross-correlation values across forward and backward time-lags. Of note, we refer to ‘forward lags’ as positive lags and ‘backward lags’ as negative lags to avoid redundancy with the terms ‘positive’ and ‘negative’ used to describe correlation values.</p>
</sec>
<sec id="s2-3-4">
<title>2.3.4 Calculation of features of the interaction</title>
<p>In this section, we explain how both lower and higher-level features were calculated. Of note, these variables were all resampled to match the video sampling rate of 50Hz.</p>
<sec id="s2-3-4-1">
<title>2.3.4.1. Calculation of lower-level features</title>
<sec id="s2-3-4-1-1">
<title>2.3.4.1.1 Spectral Flux</title>
<p>Spectral flux (SF) is a measure of acoustic change over time. In this study, we opted to separate spectral flux of the ambient background (i.e., other environmental noises such as claps, toys banging on the table, etc) from spectral flux of the mothers’ vocalisations to better analyse the distinct acoustic properties and influences of each on infant attention (see <xref ref-type="fig" rid="fig1">Fig 1A and 1B</xref>). We computed spectral flux as the sum of absolute differences over frequency of successive short-time Fourier transform (<xref ref-type="bibr" rid="c69">Müller, 2015</xref>).</p>
<disp-formula id="FD1">
<alternatives>
<mml:math id="M1" display="block"><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>F</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mi>H</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>3000</mml:mn><mml:mtext> </mml:mtext><mml:mi>H</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>0.02</mml:mn><mml:mo>;</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math>
<graphic xlink:href="26638.47686v2_eqn1.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
<p>where <italic>X(t;f)</italic> is the spectral amplitude at time <italic>t</italic> and frequency <italic>f</italic> computed over a 0.05sec window with 0.02sec hop-size.</p>
<p>Spectral flux was chosen as a metric for low-level auditory saliency, as it measures both amplitude and frequency changes over time and reflects acoustic differences between consecutives time points, both important salient auditory features (<xref ref-type="bibr" rid="c44">Huang &amp; Elhilali, 2017</xref>; <xref ref-type="bibr" rid="c54">Kothinti et al., 2021</xref>). These features are modulated in Infant Directed Speech (IDS) (<xref ref-type="bibr" rid="c22">Cooke et al., 2014</xref>; <xref ref-type="bibr" rid="c59">Leong et al., 2017</xref>) and signal short term novelty in the audio stream (<xref ref-type="bibr" rid="c70">Müller &amp; Chiu, 2024</xref>). Furthermore, spectral flux has been found to be a better predictor of neural entrainment to music, as it also measures changes in the frequency domain, compared to amplitude envelope only (<xref ref-type="bibr" rid="c109">Weineck et al., 2022</xref>). This approach allows us to have a holistic approach by measuring all possible spectral changes in naturalistic speech.</p>
<p>We then spilt spectral flux into two complementary streams using OpenAI’s Whisper (<xref ref-type="bibr" rid="c81">Radford et al., 2023</xref>), an open-source automatic speech recognition system. The general architecture of the classifier as well as the process followed to train the model are presented in more detail in <xref ref-type="bibr" rid="c81">Radford et al., (2023)</xref>. WhisperX (<xref ref-type="bibr" rid="c6">Bain et al., 2023</xref>) was also used to obtain word-level onsets and offsets with millisecond precision. From there, we split the spectral flux for both vocalisations and ambient environmental sounds (i.e., all periods that occurred outside of the periods where WhisperX detected speech) separately.</p>
</sec>
<sec id="s2-3-4-1-2">
<title>2.3.4.1.2 Face movement</title>
<p>Face movement measures low-level facial movements, particularly the coordination of eyebrow and mouth movements. We used the “whole body” model from MMPose toolbox (mmpose author 2020) to extract two dimensional coordinates of mothers’ body features in every frame. The raw 2D coordinates were cleaned using a series of low-pass filters designed to reject movements that were impossibly fast. We then interpolated those short missing segment and low-pass filtered the time series to reduce the jitteriness from the feature estimation. To calculate the “Face movement” variable, we first averaged the distances between the left and right eye-to-brow measurements and then averaged this result with the mouth opening measurement. This approach captures low-level facial movements, which are considered salient during mother-infant interactions (<xref ref-type="bibr" rid="c14">Biringen, 1987</xref>), in particular mouth movements (<xref ref-type="bibr" rid="c60">Lewkowicz &amp; Hansen-Tift, 2012</xref>; <xref ref-type="bibr" rid="c119">Zhang et al., 2021</xref>) and eyebrow movements (<xref ref-type="bibr" rid="c23">de Klerk et al., 2018</xref>; <xref ref-type="bibr" rid="c45">Isomura &amp; Nakano, 2016</xref>).</p>
<p>Distances between the eye-to-brow as well as mouth opening were calculated using the Euclidian distance between the 2D coordinates of the centre of the eye, centre of the brow, and upper and lower lips (see <xref ref-type="fig" rid="fig1">Fig 1C</xref>). These distances were then normalised by the nose length at each frame to account for dynamic distance between the mothers’ head and the camera.</p>
</sec>
<sec id="s2-3-4-1-3">
<title>2.3.4.1.3 Hand movement</title>
<p>Our hand movement measure reflects the average distance travelled by the left and right hands. We use the same pipeline as for the face movement analysis (“whole body” model from MMPose with in-house cleaning) to extract the distance travelled over time by the left and right hands and then computed their average. We took the distance travelled by the hands as an index of low-level movement activity within the computed time window. The distance travelled was computed over 50 consecutive samples (1 second) and is proportional to speed.</p>
</sec>
</sec>
<sec id="s2-3-4-2">
<title>2.3.4.2 Calculation of higher-level features</title>
<sec id="s2-3-4-2-1">
<title>2.3.4.2.1 Object naming</title>
<p>Here we created a binary array where ones represented the name of the toys (i.e. panda, book and rattle; <xref ref-type="fig" rid="figS1">Fig S1D</xref>) and zeroes indicate the absence of these words. To obtain the onsets and offsets of each toy object we used Open AI’s Whisper (<xref ref-type="bibr" rid="c81">Radford et al., 2023</xref>) and WhisperX (<xref ref-type="bibr" rid="c6">Bain et al., 2023</xref>) (explained above in 2.3.4.1.1).</p>
</sec>
<sec id="s2-3-4-2-2">
<title>2.3.4.2.2 Information Rate</title>
<p>Information rate is a measure of the amount of new (uncompressible) information per unit of time. It was computed as the derivative of the cumulative compression size (<xref ref-type="bibr" rid="c86">Schmidhuber, 2009</xref>). In our case it was computed at a word level and can be expressed as:
<disp-formula id="FD2">
<alternatives>
<mml:math id="M2" display="block"><mml:mi>I</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="26638.47686v2_eqn2.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
</p>
<p>With <italic>w<sub>i</sub></italic> being the i<sup>th</sup> word and <italic>compression (w<sub>i</sub>)</italic> being the size of the losslessly compressed representation of that word. The lossless compression technique can track redundancy in text sequences in hierarchically increasing n-sequences. Compressing text in a cumulative window with PyLZMA, a Lempel-Ziv-Markov algorithm with dynamical dictionaries (<xref ref-type="bibr" rid="c9">Bauch et al., 2015</xref>), resembles how adults make predictions about upcoming events (<xref ref-type="bibr" rid="c40">Hasson et al., 2015</xref>; <xref ref-type="bibr" rid="c86">Schmidhuber, 2009</xref>). The upcoming words are predicted from the integrative posterior, i.e. the words (or sequences of words) that have already come. For instance, as more words are introduced the size of the description might increase. New words (i.e. words that have not already come before) would increase the description’s size by a larger degree when compared to the introduction of words that have already come. By taking the derivative of that increasing size, we effectively compute the rate at which new information is introduced. This integrative compression process can be compared to predictive processing (<xref ref-type="bibr" rid="c86">Schmidhuber, 2009</xref>) and to cognitive constructivism (<xref ref-type="bibr" rid="c19">Chater &amp; Vitányi, 2002</xref>; <xref ref-type="bibr" rid="c111">Wolff, 2014</xref>, <xref ref-type="bibr" rid="c112">2019</xref>) where a pattern-seeking agent compresses incoming data into a modelled representation using redundancies from the data.</p>
</sec>
<sec id="s2-3-4-2-3">
<title>2.3.4.2.3 Semantic surprisal</title>
<p>Semantic surprisal measures the linguistic probabilistic inference: it quantifies how surprising each upcoming word is given their context. This approach was chosen as it shows correlation with reading time and the N400 (<xref ref-type="bibr" rid="c32">S. L. Frank et al., 2015</xref>; <xref ref-type="bibr" rid="c90">Shain et al., 2024</xref>) and was computed for each word as the negative log probability of that word given all previous words (<xref ref-type="bibr" rid="c110">Willems et al., 2016</xref>). Specifically, this was conducted using the GPT-2 large language model, a generative transformer model that predicts the most probable next words given a text, along with their respective probabilities. We then retrieved the probability of the observed upcoming word and computed the negative log2 of that probability as the semantic surprisal (<xref ref-type="bibr" rid="c91">Shannon, 1948</xref>). The word was then added to the context window, and the prediction process was repeated for the next upcoming word (see <xref ref-type="fig" rid="fig1">Fig 1.E</xref>). Importantly, we prompted every interaction with the same sentence presenting the experiment to prevent the model from being surprised when it encountered normally unusual or unexpected words such as ‘panda’ (<italic>“The following text is about a mother and her infant playing together, as they would at home, with a toy panda, a book and a rattle, while wearing EEG hats and facial electrodes”</italic>).</p>
<disp-formula id="FD3">
<alternatives>
<mml:math id="M3" display="block"><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mtext> </mml:mtext><mml:mi>S</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋀</mml:mo><mml:mi>¿</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="26638.47686v2_eqn3.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
</sec>
<sec id="s2-3-4-2-4">
<title>2.3.4.2.4 Facial novelty</title>
<p>Our measure of facial novelty reflects an estimate of changes in the facial expressions of the mother over time. To calculate these, we used an open-source Python library, the ‘Facial Expression Recognition using Residual Masking Network’ (<xref ref-type="bibr" rid="c78">Pham et al., 2020</xref>). This classifier gives probabilities for seven canonical facial expressions (happy, sad, surprise, angry, disgust, fear, neutral) using residual neural network, a machine learning model that shows high generalisability due to “skip connections” that allows the model to ignore irrelevant features and focus on informative features for the task. On adult faces the model performs with 76.82% accuracy compared to manually labelled expressions (<xref ref-type="bibr" rid="c78">Pham et al., 2020</xref>). To measure the changes over time we computed the Kullback-Leibler divergence between consecutive frames.</p>
<disp-formula id="FD4">
<alternatives>
<mml:math id="M4" display="block"><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>exp</mml:mi></mml:mrow><mml:mo>□</mml:mo></mml:munderover><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="26638.47686v2_eqn4.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
<p>Where <italic>E<sub>x,t</sub></italic> is the expression probability for the expression <italic>x</italic> distribution at time <italic>t</italic>. The Kullback-Leibler divergence can be interpreted as the information gained from updating <italic>E<sub>x,t-1</sub></italic> to <italic>E<sub>x,t</sub></italic> , e.g. if the divergence is high, it means that the most recent facial distribution vector required a lot of information to be updated, i.e. the facial expression was novel compared to the previous frame (see <xref ref-type="fig" rid="fig1">Fig 1</xref>). We classified this as a higher-level feature as it the frame-wise difference between the last, and most abstract layer of neural network (<xref ref-type="bibr" rid="c35">Giordano et al., 2023</xref>; <xref ref-type="bibr" rid="c48">Kiat et al., 2022</xref>; <xref ref-type="bibr" rid="c53">Kothinti &amp; Elhilali, 2023</xref>; <xref ref-type="bibr" rid="c58">Lecun et al., 2015</xref>; <xref ref-type="bibr" rid="c118">Zeiler &amp; Fergus, 2013</xref>), and because we chose an information metric measure that has been used to model infant attention (<xref ref-type="bibr" rid="c31">M. C. Frank et al., 2009</xref>; <xref ref-type="bibr" rid="c80">Poli et al., 2020</xref>) to measure the model updating between consecutive frames.</p>
</sec>
</sec>
</sec>
</sec>
</sec>
<sec id="s3" sec-type="results">
<title>3. Results</title>
<sec id="s3-1">
<title>3.1 General descriptives</title>
<p>First, we explored between- and within-age group differences in how many times per minute infants and mothers engaged in looks to objects (see <xref ref-type="section" rid="s6">SM</xref>, <xref ref-type="fig" rid="figS2">Fig S2A</xref>) and for how long these episodes lasted on average (see <xref ref-type="section" rid="s6">SM</xref>, <xref ref-type="fig" rid="figS2">Fig S2B</xref>). We found no differences between age groups in the average look duration to objects or counts for either infants or mothers. However, mothers looked more frequently to objects (<italic>t</italic><sub>5M</sub>(61)= −3.01, <italic>p</italic><sub>5M</sub>=0.004; <italic>t</italic><sub>15M</sub>(63)= −6.34, <italic>p</italic><sub>15M</sub>&lt;0.001) but for shorter durations (<italic>t</italic><sub>5M</sub>(61)= 5.43, <italic>p</italic><sub>5M</sub>&lt;0.001; <italic>t</italic><sub>15M</sub>(63)= 8.47, <italic>p</italic><sub>15M</sub>&lt;0.001) compared to infants at both ages. The same analyses on looks to partner are presented in the SM (<xref ref-type="fig" rid="figS2">Fig S2C and D</xref>). Second, we quantified the amount of caregiver speech during the joint play interactions at 5 and 15 months (see <xref ref-type="section" rid="s6">SM</xref>, <xref ref-type="fig" rid="figS3">Fig S3</xref>). The amount of speech was greater at 15 months compared to 5 months (<italic>t</italic>(52)= −2.43, <italic>p</italic>= 0.018).</p>
<p>Finally, we calculated the overall mean and median values for all the lower- and higher-level features. This allowed us not only to see how these variables are distributed across participants but also to conduct age group comparisons. To assess significant differences across age groups, we first tested for normality using MATLAB’s “vartestn” function. For normally distributed data, we applied two-sided t-tests, and for non-normally distributed variables, we used two-sided Wilcoxon rank-sum tests. We adjusted for the family wise false positive discovery rate using the false discovery rate (FDR) method, using the Benjamini-Hochberg procedure (<xref ref-type="bibr" rid="c11">Benjamini &amp; Hochberg, 1995</xref>) with an alpha set at 0.05. We report adjusted p values (p-adj) that have been corrected for multiple comparisons in <xref ref-type="table" rid="tblS3">Table S3</xref> after each uncorrected p value. We observed that both the averages for “Information Rate” and “Object naming” increased significantly between 5 months and 15 months (see <xref ref-type="table" rid="tblS3">Table S3</xref>). The rest of the variables did not change significantly from 5 to 15 months.</p>
</sec>
<sec id="s3-2">
<title>3.2 Correlations between all features of the interaction</title>
<p>We performed a standard correlation analysis across all eight lower- and higher-level features to examine the associations between all the different interaction features. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows the correlation matrix at 5 and 15 months (see <xref ref-type="fig" rid="fig3">Fig. 3</xref>). We also conducted a cross-correlation analyses to capture more dynamic associations (see <xref ref-type="fig" rid="figS5">SM Fig 5)</xref>.</p>
<p>Overall, both analyses revealed that most variables were interrelated, as evidenced by significant correlations observed across multiple feature pairs (<xref ref-type="fig" rid="figS5">SM Fig 5</xref>). While the strength and timing of these correlations vary, most of them exhibit significant positive associations, particularly around lag 0, suggesting a high degree of temporal coordination (see <xref ref-type="fig" rid="fig3">Fig 3</xref> for correlation values at lag 0, and <xref ref-type="fig" rid="figS5">SM Fig 5</xref> for cross-correlation results). For example, when ‘Spectral Flux of vocalisations’ is high, ‘semantic surprisal’, ‘Information rate’ and ‘object naming’ are also high.</p>
<p>Conversely, the only pairs exhibiting negative correlations were the ones involving ‘spectral flux of ambient noise’ and the speech related variables (’spectral flux of vocalisations’, ‘object naming’, ‘information rate’ and ‘semantic surprisal’). This relationship is likely due to the way these variables are calculated, as they are inherently interdependent. The spectral flux of ambient noise and the spectral flux of vocalisations or information rate are measured in mutually exclusive contexts: when mothers are vocalising, spectral flux of ambient is set to zero and when spectral flux of ambient is being measured, it indicates that mothers were not vocalising. As a result, the observed pattern is a byproduct of our methodology rather than an intrinsic relationship between the variables.</p>
<p>The variables ‘facial movement’ and ‘facial novelty’, although both measuring the dynamics of facial feature, showed correlation values below .02, validating the theoretical differences described in the method section. Interestingly, there were few differences observed between 5 and 15 months.</p>
</sec>
<sec id="s3-3">
<title>3.3 Cross-correlations between features of the interaction and infant attention to objects</title>
<p>For our primary analyses we conducted cross-correlations to examine whether certain features of the interaction, most of them related to maternal behaviours, forward-predicted changes in infant attention to objects, or vice versa. Our predictions were that mothers would respond to decreases in infant attention by increasing their own lower-level salience (lower-level features forward-predicts infant attention); but then, when the infants’ attention is re-engaged, they would downregulate salience and upregulate higher-order semantic features (infant attention forward-predicts higher-level features).</p>
<p>Our findings are presented below, organized by each variable. The p-values resulting from statistical tests were adjusted separately for the lower-level and higher-level groups using the Benjamini-Hochberg procedure (<xref ref-type="bibr" rid="c11">Benjamini &amp; Hochberg, 1995</xref>) with an alpha set at 0.05. The adjusted p-values are reported as ‘p-adj’ in the main text alongside the corresponding uncorrected p-values. Unless otherwise specified, all reported findings reflect bidirectional relationships (i.e., if an increase in X leads to a decrease in Y, the opposite - where a decrease in X leads to an increase in Y - also holds). Please refer to <xref ref-type="fig" rid="figS4">Fig S4</xref> for a visual guide on interpreting findings from a cross-correlation analysis.</p>
<p>Since one important question in interpreting our findings is whether, instead of paying attention to objects, infants are instead paying increased attention to their parents, we have also presented in the SM (<xref ref-type="fig" rid="figS6">Figs S6</xref> and <xref ref-type="fig" rid="figS7">S7</xref>) the exact same set of analyses but examining instead the associations between features of the interaction and infant attention to mother.</p>
<sec id="s3-3-1">
<title>3.3.1 Lower-order features</title>
<sec id="s3-3-1-1">
<title>3.3.1.1 Spectral flux - ambient</title>
<p>As expected, we observed significant negative correlation values at t=0, suggesting that less ambient spectral flux associated with more infant attention to objects. When we introduced a time lag between the variables, we found that this negative association was significant from t=-1.1/-0.86sec (infant precedes ambient SF) to t=3.06/3.54sec (ambient SF precedes infant) at 5 months (p=0.008, p-adj= 0.013) and at 15 months (p=0.009, p-adj= 0.013) respectively (<xref ref-type="fig" rid="fig3">Fig 3A, 3B</xref>). The significance of both findings is stronger for positive time-lags indicating that, overall, reductions in the ambient spectral flux tended to forwards-predict increases in infant’s subsequent attention to objects (or that increases in ambient spectral flux forward predicted decreases in infant’s object attention) at both ages (<xref ref-type="fig" rid="fig3">Fig 3A, 3B</xref>).</p>
</sec>
<sec id="s3-3-1-2">
<title>3.3.1.2 Spectral flux - vocalisations</title>
<p>We found negative correlation values at t=0, suggesting that maternal speech containing less spectral flux associates with more infant attention to objects. At 5 months, this relationship was significant during two temporal windows: from t=-2.1sec (infant precedes mother) to t=1.24sec (mother precedes infant) (p=0.015, p-adj= 0.017), and from t=6.54sec to t=9.52sec (p=0.022, p-adj= 0.022) (<xref ref-type="fig" rid="fig3">Fig 3C</xref>). This suggests that, when 5-month-old infants focus on objects, mothers reduce the acoustic variability of their vocalisations, or conversely, when infants’ attention to objects decreases, mothers increase the acoustic variability of their vocalisations (backward time-lag) (see <xref ref-type="fig" rid="figS4">Figure S4</xref>). Additionally, we also found that an increase in mothers’ acoustic variability was followed by a reduction in infants’ attention to objects, or <italic>vice versa</italic>, when mothers decrease their acoustic variability, infants’ attention to objects increased (forward time-lag). At 15 months, instead, these negative associations were present at forward time-lags but not statistically significant (<xref ref-type="fig" rid="fig3">Fig 3D</xref>).</p>
<p>We considered the possibility that an increase in the spectral flux of maternal vocalisations might be associated not with an increased likelihood of looking at the object, but rather with an increased likelihood of looking at the mother. However, we found no support for this SM (<xref ref-type="fig" rid="figS6">Fig S6C and D</xref>).</p>
</sec>
<sec id="s3-3-1-3">
<title>3.3.1.3 Facial movement</title>
<p>We found negative correlations at time t=0, suggesting that increased maternal facial movement associates with less infant attention to objects, at both 5 and 15 months. This was significant from t=- 3.92/-3.46sec to t=2.48/3.06sec at 5 (p= 0.006, p-adj= 0.013) and 15 months (p&lt;0.001, p-adj= 0.007) respectively (<xref ref-type="fig" rid="fig3">Fig 3E, F</xref>). This suggests that when infants focus on the objects, mothers make fewer facial movements, potentially removing stimuli from an infant’s peripheral vision. Conversely, when mothers increase facial movements, infants tend to look less at the objects (<xref ref-type="fig" rid="fig3">Fig 3E, F</xref>). As expected, when we looked at the relationship between maternal facial movement and infants’ looks to their mother, we found the opposite pattern (see <xref ref-type="section" rid="s6">SM</xref>, <xref ref-type="fig" rid="figS6">Fig S6E, F</xref>). When infants look at their mother, there is an increase in maternal facial movement; and when mothers increase facial movements, infants look at their mothers more. Again, this pattern was observed at both ages. This relationship is independent of the relationship with vocal spectral flux documented in 3.3.1.2 (see <xref ref-type="fig" rid="fig3">Fig 3</xref>, <xref ref-type="fig" rid="figS5">Fig S5</xref>) because the direct association between the two variables is negative, but the two variables are each negatively associated with infant attention.</p>
</sec>
<sec id="s3-3-1-4">
<title>3.3.1.4 Hand movement</title>
<p>We found negative correlations at time t=0, suggesting that increased maternal hand movement associates with less infant attention to objects, at both 5 and 15 months. This was significant from t=- 3.6/-8.02sec to t=1.52/1.02sec at 5 (p=0.01, p-adj= 0.013) and 15 months (p=0.003, p-adj= 0.012) respectively (<xref ref-type="fig" rid="fig3">Fig 3G, H</xref>). The significance of both findings is stronger at backward time-lags indicating that, overall, when an infant focuses their attention on an object, it leads to a decrease in the mother’s hand movements (or that a decrease in the infants’ attention to an object leads to an increase in the mother’s hand movements) (see <xref ref-type="fig" rid="fig3">Fig 3G, H</xref>). Facial movement and hand movement were themselves positively correlated (see <xref ref-type="fig" rid="fig3">Fig 3</xref>, <xref ref-type="fig" rid="figS5">Fig S5</xref>), so the similar associations with attention likely pick up common variance across multiple variables.</p>
</sec>
</sec>
<sec id="s3-3-2">
<title>3.3.2 Higher-level features</title>
<sec id="s3-3-2-1">
<title>3.3.2.1 Object naming</title>
<p>No association between object naming and infant attention was observed at 5 months (<xref ref-type="fig" rid="fig4">Fig 4A</xref>). At 15 months, instead, we observed a positive significant correlation from t=0.24 to t=2.6sec (p=0.032, p- adj= 0.032), indicating that, when mothers name objects, it associates with subsequent increases in infants’ attention towards the named object (<xref ref-type="fig" rid="fig4">Fig 4B</xref>). These associations may be due to the shared variance between object naming and low-level features documented previously (see <xref ref-type="fig" rid="fig3">Fig 3</xref>, <xref ref-type="fig" rid="figS5">Fig S5</xref>). However, the fact that the associations with low-level features were consistent at both ages, whereas the association between object naming and attention was present only at 15 months, partially precludes this possibility.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Cross correlations between lower-level features and infant attention to objects.</title>
<p>Spectral flux of ambient background and vocalisations, facial movement, and hand movement in relation to infant attention to objects at 5 months (A, C, E, G) and 15 months (B, D, F, H), respectively. Thick orange (5 months) and purple (15 months) lines represent the observed cross-correlation results, with shaded coloured areas showing their SEM. Grey lines represent control (permuted) data, with the shaded grey area indicating its SEM. Red thick lines indicate significance from the CBP test (significance for the CBP tests was set to p&lt;0.025, two-sided, and was then FDR adjusted).</p></caption>
<graphic xlink:href="26638.47686v2_fig4.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s3-3-2-2">
<title>3.3.2.2 Information rate</title>
<p>We observed a significant positive correlation between information rate and infant attention where increased maternal information rate associated with increased infant attention to objects. This was significant from t=2.46 to 5.56sec at 5 months (p=0.023, p-adj= 0.032; <xref ref-type="fig" rid="fig4">Fig 4C</xref>), but non-significant at 15 months (<xref ref-type="fig" rid="fig4">Fig 4D</xref>). These findings are stronger at forward time-lags indicating that, overall, increases in maternal information rate more often precede infant attention to objects than follow it. Interestingly and conversely, we observed the opposite pattern for infant attention to the partner, where maternal information rate tended to precede a decrease in infant attention to partner. This pattern was present at both ages, though it was only statistically significant at 5 months (see <xref ref-type="section" rid="s6">SM</xref>, <xref ref-type="fig" rid="figS7">Fig S7C, D</xref>). This association is independent of the relationship documented between information rate and vocal spectral flux (see <xref ref-type="fig" rid="fig3">Fig 3</xref>, <xref ref-type="fig" rid="figS5">Fig S5</xref>) because those two variables are positively associated whereas the latter is positively associated with infant attention and the latter negatively.</p>
</sec>
<sec id="s3-3-2-3">
<title>3.3.2.3 Semantic surprisal</title>
<p>No significant associations were observed between semantic surprisal and infant attention to objects (<xref ref-type="fig" rid="fig4">Fig 4E, 4F</xref>) or attention to mothers (<xref ref-type="fig" rid="figS7">Figure S7E, F</xref>).</p>
</sec>
<sec id="s3-3-2-4">
<title>3.3.2.4 Facial novelty</title>
<p>We found negative correlations at t=0, indicating that increased infant attention to objects is associated with decreased maternal facial variability. These associations were significant at 15 months from t=-5.68 (infant precedes mother) to t=+1.96sec (mother precedes infant) (p&lt;0.001, p-adj= 0.003; <xref ref-type="fig" rid="fig4">Fig 4H</xref>). At 5 months, instead, these associations were non-significant (<xref ref-type="fig" rid="fig4">Fig 4G</xref>). Overall, these findings are stronger at backward time-lags indicating that increases in infant attention to objects precede decreases in maternal facial expressions (or that decreases in infant attention to objects precede increases in the variability of maternal facial expressions) (<xref ref-type="fig" rid="fig4">Fig 4G, H</xref>). As with the face movement data (<xref ref-type="fig" rid="fig3">Fig 3E, F</xref>), we expected that increases in the variability of maternal facial novelty would associate with infant attention to mothers. However, this was not the case. Although a pattern emerged, where infants appeared to pay more attention to their mothers when they increased the variability of their facial expressions, this was not significant (<xref ref-type="fig" rid="figS7">Fig SM 7G, H</xref>). The relationship between facial novelty and attention may be related to the relationships already observed between facial salience and attention (<xref ref-type="fig" rid="fig4">Fig 4E, F</xref>) as the two variables are themselves weakly positively associated (r=.007/.02 at 5/15 months).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Cross correlations between higher-level features and infant attention to objects.</title>
<p>Maternal object naming, information rate, semantic surprisal, and facial novelty in relation to infant attention to objects at 5 months (A, C, E, G) and 15 months (B, D, F, H), respectively. Thick orange (5 months) and purple (15 months) lines represent the observed cross-correlation results, with shaded coloured areas showing their SEM. Grey lines represent control (permuted) data, with the shaded grey area indicating its SEM. Red thick lines indicate significance from the CBP test (significance from CBP was set to p&lt;0.025, two-sided; of note, significance for object naming was set at p&lt;0.05, one-sided, based on the expectation that object naming would positively correlate with attention. All results were FDR adjusted).</p></caption>
<graphic xlink:href="26638.47686v2_fig5.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
</sec>
</sec>
</sec>
<sec id="s4" sec-type="discussion">
<title>4. Discussion</title>
<p>In this study we examined mother-infant interactions during dyadic object play at 5 and 15 months. We characterised eight features of the interaction, primarily derived from maternal behaviours, that were further categorized into lower-level features - namely spectral flux of ambient noise and of maternal vocalisations, and maternal face and hand movement - and higher-level features, including information rate, semantic surprisal, toy naming and facial novelty. Using time-series analyses we investigated how these features of the interaction, most of them coming from the behaviours of the mothers, dynamically modulate and are modulated by fine-grained fluctuations in the infant’s attention state.</p>
<p>Our preliminary analyses indicated no significant differences in infants’ overall attentiveness to the objects between 5 and 15 months (<xref ref-type="fig" rid="figS2">Fig S2A and B</xref>). Examining interaction features from 5 to 15 months revealed increases in mother object naming, information rate and total amount of speech (<xref ref-type="fig" rid="fig2">Fig 2E, 2F</xref>, <xref ref-type="fig" rid="figS3">S3</xref>), while all other features remained unchanged (<xref ref-type="fig" rid="fig2">Fig 2</xref>). This is in line with previous research on caregiver speech becoming more complex over infant development (<xref ref-type="bibr" rid="c89">Schwab et al., 2018</xref>) in order to accommodate for infant needs for redundancy for word learning (<xref ref-type="bibr" rid="c88">Schwab &amp; Lew-Williams, 2016</xref>; <xref ref-type="bibr" rid="c99">Tal et al., 2021</xref>).</p>
<p>Next, we examined the relationships among all eight lower- and higher-level interaction features (<xref ref-type="fig" rid="fig3">Fig 3</xref>, <xref ref-type="fig" rid="figS5">S5</xref>). The majority of the features of the interaction showed positive associations, particularly around lag 0, indicating small but significant levels of temporal coordination. This suggests that mothers engage in multiple behaviours across various modalities simultaneously when interacting with their infants, highlighting the interconnected nature of our variables. Interestingly, there were few differences observed between 5 and 15 months, implying that the features of the interaction remain relatively consistent across these time points. Importantly, while higher- and lower-level features were interrelated (<xref ref-type="fig" rid="fig3">Fig. 3</xref>, <xref ref-type="fig" rid="figS5">S5</xref>), they showed distinct patterns in relation to infant attention to objects and to the mothers. This suggests that, despite their interconnectedness, they contribute differently to dyadic interaction dynamics and the shaping of infant attention.</p>
<p>When we examined the dynamic associations between infants’ attention and lower-level features of the mothers’ behaviour, we found a range of fine-grained associations, consistent with Prediction 1, which suggests that mothers use multiple behavioural cues during play that are tightly coupled to changes in infant attention (<xref ref-type="bibr" rid="c77">Perapoch Amadó et al., 2025</xref>; <xref ref-type="bibr" rid="c79">Phillips et al., 2023</xref>; <xref ref-type="bibr" rid="c96">Suarez-Rivera et al., 2019</xref>). We found that ambient spectral flux, spectral flux of vocalisations, face and hand movements all showed negative correlations at time t=0, suggesting either that caregivers increased their saliency when infants were not paying attention to the toy objects, or that they decreased their saliency when infants were paying attention, presumably to help them focus on the toys (<xref ref-type="fig" rid="fig3">Fig 3</xref>). Contrary to Prediction 2, we found that the direction of these associations did not change with age: relationships were generally consistent between 5 months and 15 months. When we introduced a time lag between the variables we found, consistent with Prediction 3, that decreases in spectral flux (a measure of acoustic change) in both maternal vocalisations and the ambient noise was followed by subsequent increases in infants’ attention to objects (<xref ref-type="fig" rid="fig3">Fig 3 A-D</xref>). Additionally, increases in infants’ attention to objects were followed by decreases in mothers’ hand and facial movements (<xref ref-type="fig" rid="fig3">Fig 3 E-H</xref>). As expected, maternal facial movements were associated with infant attention to mother’s faces (<xref ref-type="fig" rid="figS6">Fig S6E and S6F</xref>).</p>
<p>Overall, these findings suggest that mothers adapt their low-level behaviour across different modalities to support their infant’s attentional focus. Specifically, when infants direct their attention toward an object, mothers may intentionally pause their ongoing actions or interactions to facilitate the infant’s engagement with the object of interest. Interestingly, these lower-level features were all positively correlated with infant attention to mothers’ face at t=0 (the opposite pattern observed with infant attention to objects), suggesting that mothers may reduce these behaviours not only to support infants’ attention to objects but also to avoid distracting them from the objects. This aligns with previous research demonstrating that mothers actively scaffold their infants’ attention (e.g. <xref ref-type="bibr" rid="c7">Bakeman &amp; Adamson, 1984</xref>; <xref ref-type="bibr" rid="c13">Bigelow et al., 2004</xref>; <xref ref-type="bibr" rid="c96">Suarez-Rivera et al., 2019</xref>; <xref ref-type="bibr" rid="c97">Sun &amp; Yoshida, 2022</xref>). Our findings are interesting because they highlight not only the positive associations between low-level maternal behaviours and infant attention - where increased maternal behaviours correspond to increased infant attention - but also the role of maternal behaviour reduction. More specifically, the decrease in certain maternal behaviours, potentially those that could have been acting as distractors, was also linked to greater infant attention. This suggests an adaptive parental strategy that not only involves increasing certain behaviours to support infant engagement, but also strategically reducing others, highlighting the importance of modulating low-level behaviour contingent on infant attention.</p>
<p>We also observed dynamic associations between infants’ attention and higher-level features of the interaction. Consistent with Prediction 1, we found that decreases in infant attention to objects preceded increases in the variability of maternal facial expressions; or, alternatively, that increases in infant attention preceded decreases in the variability of maternal facial expressions (<xref ref-type="fig" rid="fig4">Fig 4G, H</xref>). We also found that increases in object naming and information rate were positively associated with subsequent increases in infant attention to objects (<xref ref-type="fig" rid="fig4">Fig 4 A-D</xref>). Object naming predicted attention to objects at 15 months but not at 5 months (<xref ref-type="fig" rid="fig4">Fig 4A, B</xref>), while information rate showed this relationship at both ages but reached significance only at 5 months (<xref ref-type="fig" rid="fig4">Fig 4C, D</xref>). These findings are comparable to Suarez and colleagues who found that parental behaviours such as talking and touching were not only highly likely to occur when both the parent and infant visually attended to the same object but were also associated with longer periods of infant attention (<xref ref-type="bibr" rid="c96">Suarez-Rivera et al., 2019</xref>).</p>
<p>Similarly (and perhaps surprisingly, given that information rate and semantic surprisal were the most strongly associated of the behavioural variables (<xref ref-type="fig" rid="figS5">Fig S5</xref>)), we found associations between information rate and infant attention (<xref ref-type="fig" rid="fig4">Fig 4C, D</xref>) but no associations between semantic surprisal and infant attention (<xref ref-type="fig" rid="fig4">Fig 4E, F</xref>). This may indicate that 5- and 15-month-old infants lack of sensitivity to higher-order linguistic structures (<xref ref-type="bibr" rid="c40">Hasson et al., 2015</xref>; <xref ref-type="bibr" rid="c42">Heilbron et al., 2022</xref>). Instead, infants at 5 and 15 months might base their predictions on a simpler statistical learning process that tracks transitional probabilities between part of words (<xref ref-type="bibr" rid="c19">Chater &amp; Vitányi, 2002</xref>; <xref ref-type="bibr" rid="c86">Schmidhuber, 2009</xref>; <xref ref-type="bibr" rid="c111">Wolff, 2014</xref>, <xref ref-type="bibr" rid="c112">2019</xref>). An alternative, and complementary, explanation could be that the “semantic surprisal” variable was calculated using models trained on adult-directed text. Consequently, it is possible that the aspects deemed surprising by the model were not equally surprising to infants. However, the calculations from the model are based on what has been said within the interaction itself, which makes it unlikely that the model’s interpretation of surprisal diverges significantly from what anyone, including infants, could experience in that context. Instead, what these findings might suggest is that even at 15 months, infants are still likely in the process of developing the linguistic and cognitive frameworks necessary to actively infer meaning from language in a very coarse hierarchical scale and are not yet making predictions based on the semantic meaning of words. However, they orient their attention to speech that updates their past representation, potentially revealing that their predictions are based on a lossless compression process that needs updating when facing unseen sequences. In that regard, early attention follows the principle of least effort, where attention is allocated to speech when it offers relevant information for model updating (<xref ref-type="bibr" rid="c33">Gerken et al., 2011</xref>).</p>
<p>Overall, Prediction 2, which proposed that higher-level features would be more influential for older infants, was only partially supported. While associations between object naming and facial novelty with infant attention were stronger at 15 months, associations between information rate and infant attention were stronger at 5 months. Similarly, Prediction 3, which posited that as infants take the lead in play, higher-level maternal behaviours would more often follow infant attention to objects, was also not supported. Even at 15 months, both increased object naming and increased information rate continued to predict subsequent increases in infant attention, rather than following it. This finding is surprising given prior research suggesting that mothers often name the object their infant is already attending to (i.e. object naming/talk follows infant attention more than the other way around) (<xref ref-type="bibr" rid="c87">Schroer &amp; , 2022</xref>), and that this practice facilitates infant learning more effectively than naming objects not currently in the infant’s focus (<xref ref-type="bibr" rid="c36">Goupil et al., 2024</xref>; <xref ref-type="bibr" rid="c116">Yu &amp; Smith, 2012</xref>). However, this effect may not necessarily hold if infants already know the words.</p>
<p>Recent research in psychology has challenged traditional theoretical approaches which characterise the physical environment purely in terms of exogenous features, quantified through measures such as salience (<xref ref-type="bibr" rid="c46">Itti &amp; Baldi, 2009</xref>), and attention as the product of a push-pull between exogenous and endogenous factors (<xref ref-type="bibr" rid="c61">Luna et al., 2008</xref>). Rather, we dynamically recalibrate the salience in our environments through how we interact with them - for example, by picking up an object we are interested in and pulling it closer (<xref ref-type="bibr" rid="c3">Anderson et al., 2022</xref>; <xref ref-type="bibr" rid="c30">Franchak &amp; Yu, 2022</xref>; <xref ref-type="bibr" rid="c64">Méndez et al., 2021</xref>; <xref ref-type="bibr" rid="c87">Schroer &amp; Yu, 2022</xref>): we generate experiences through behaviours (<xref ref-type="bibr" rid="c24">Dewey, 1896</xref>; <xref ref-type="bibr" rid="c34">Gibson, 1988</xref>). In many ways, the findings from this study extend this idea into social interaction, by showing that our behaviours influence not just the low-level properties of how our partners move and talk, but also what information they present to us, and when. Already during infancy, the information that infants perceive is constantly and dynamically changing over multiple hierarchical levels, contingent on their behaviour and on their attention. Even from early development, attention involves interactive processes which unfold across multiple levels.</p>
<p>In summary, our findings describe how, even during early development, maternal behaviours both depend on, and influence, infants’ attention. This suggests that, even from early life, joint attention involves both lower- and higher-level processes. Interactive processes operate across multiple levels, from salience to semantics.</p>
<sec id="s4-1">
<title>4.1 Limitations</title>
<p>There are a number of limitations to our study. First, the experimental setup, in which a mother and her infant play together while seated across a table in the lab, cannot fully compare to a real-life interaction (<xref ref-type="bibr" rid="c100">Tamis-LeMonda, 2023</xref>), it also limits the variability of interactional patterns that the dyad can engage in, and might have put pressure on mothers to engage in the interaction more than they would have otherwise (<xref ref-type="bibr" rid="c1">Abney et al., 2020</xref>). However, despite these limitations, we believe the present set-up still preserves important characteristics of early interactions, such as responsivity (<xref ref-type="bibr" rid="c28">Fogel &amp; Garvey, 2007</xref>) and multimodality (<xref ref-type="bibr" rid="c26">Español &amp; Shifres, 2015</xref>). Second, the measures selected for this study provide only a limited representation of the diverse and complex contexts of real-world interactions. For instance, we were unable to capture narrative event structures, which are considered crucial for interpreting everyday settings (<xref ref-type="bibr" rid="c117">Zacks, 2020</xref>). Similarly, higher-order factors, such as play sequences, were beyond the scope of this study but are likely to be closely linked to infant attention, too. Third, the behaviours we quantified interact dynamically, potentially producing effects greater than the sum of their individual contributions (<xref ref-type="bibr" rid="c119">Zhang et al., 2021</xref>); however, we analysed them independently. Although we considered creating composite variables or using Principal Component Analysis (PCA) to work with components, doing so would result in the loss of the “meaning” of the individual variables. Fourth, our sample is homogeneous in terms of ethnicity, culture and socioeconomics and consists only of mothers (<xref ref-type="table" rid="tblS1">Table S1</xref>). It will be useful for future studies to include investigations from more heterogeneous groups/communities (<xref ref-type="bibr" rid="c27">Feldman, 2007</xref>; <xref ref-type="bibr" rid="c71">Mundy et al., 2007</xref>; <xref ref-type="bibr" rid="c101">Taverna et al., 2024</xref>) as well as to include fathers (<xref ref-type="bibr" rid="c4">Aureli et al., 2022</xref>).</p>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data sharing statement:</title>
<p>Partial restrictions to the data and/or materials apply. Due to the personally identifiable nature of this data (video and audio recordings from infants and their mothers) the raw data will not be made publicly accessible. Researchers who wish to access the raw data should email the lead author and permission to access the raw data will be granted as long as the applicant can guarantee that certain privacy guidelines can be provided.</p>
</sec>
<sec id="s6" sec-type="supplementary">
<title>Supplementary Materials</title>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>Demographic data at 5 and 15 months.</title></caption>
<alternatives>
<graphic xlink:href="26638.47686v2_tbl1.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all" width="100%">
<thead>
<tr style="background-color:#E8E8E8">
<th align="left" valign="top">Maternal education</th>
<th align="center" valign="top"><styled-content style="color:#E97132">5 Months</styled-content></th>
<th align="center" valign="top"><styled-content style="color:#A02B93">15 Months</styled-content></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Post-graduate degree</td>
<td align="center" valign="top">45.45</td>
<td align="center" valign="top">64.71</td>
</tr>
<tr>
<td align="left" valign="top">Degree</td>
<td align="center" valign="top">31.82</td>
<td align="center" valign="top">35.29</td>
</tr>
<tr>
<td align="left" valign="top">F.E. Qualification</td>
<td align="center" valign="top">4.55</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">A Level</td>
<td align="center" valign="top">4.55</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">GCSE’s</td>
<td align="center" valign="top">9.09</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">No formal qualification</td>
<td align="center" valign="top">0</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">Other</td>
<td align="center" valign="top">4.55</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">Prefer not to answer</td>
<td align="center" valign="top">0</td>
<td align="center" valign="top">0</td>
</tr>
</tbody>
</table>
<table frame="box" rules="all" width="100%">
<thead>
<tr style="background-color:#E8E8E8">
<th align="left" valign="top">Household income</th>
<th align="center" valign="top"><styled-content style="color:#E97132">5 Months</styled-content></th>
<th align="center" valign="top"><styled-content style="color:#A02B93">15 Months</styled-content></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Under 16k</td>
<td align="center" valign="top">5.26</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">16k – 25k</td>
<td align="center" valign="top">0.00</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">26k – 35k</td>
<td align="center" valign="top">5.26</td>
<td align="center" valign="top">6.25</td>
</tr>
<tr>
<td align="left" valign="top">36k – 50k</td>
<td align="center" valign="top">10.53</td>
<td align="center" valign="top">6.25</td>
</tr>
<tr>
<td align="left" valign="top">51k – 80k</td>
<td align="center" valign="top">10.53</td>
<td align="center" valign="top">18.75</td>
</tr>
<tr>
<td align="left" valign="top">More than 80k</td>
<td align="center" valign="top">57.89</td>
<td align="center" valign="top">62.5</td>
</tr>
<tr>
<td align="left" valign="top">Prefer not to answer</td>
<td align="center" valign="top">10.53</td>
<td align="center" valign="top">6.25</td>
</tr>
</tbody>
</table>
<table frame="box" rules="all" width="100%">
<thead>
<tr style="background-color:#E8E8E8">
<th align="left" valign="top">Maternal ethnicity</th>
<th align="center" valign="top"><styled-content style="color:#E97132">5 Months</styled-content></th>
<th align="center" valign="top"><styled-content style="color:#A02B93">15 Months</styled-content></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">White British</td>
<td align="center" valign="top">50</td>
<td align="center" valign="top">56.25</td>
</tr>
<tr>
<td align="left" valign="top">Black British</td>
<td align="center" valign="top">5</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">Other White</td>
<td align="center" valign="top">20</td>
<td align="center" valign="top">18.75</td>
</tr>
<tr>
<td align="left" valign="top">Asian, Indian</td>
<td align="center" valign="top">10</td>
<td align="center" valign="top">18.75</td>
</tr>
<tr>
<td align="left" valign="top">Black African</td>
<td align="center" valign="top">10</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">Mixed - White/Afro-Caribbean</td>
<td align="center" valign="top">5</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">Latina</td>
<td align="center" valign="top">0</td>
<td align="center" valign="top">6.25</td>
</tr>
<tr>
<td align="left" valign="top">Not answered</td>
<td align="center" valign="top">0</td>
<td align="center" valign="top">0</td>
</tr>
</tbody>
</table>
<table frame="box" rules="all" width="100%">
<thead>
<tr style="background-color:#E8E8E8">
<th align="left" valign="top">Languages spoken at home</th>
<th align="center" valign="top"><styled-content style="color:#E97132">5 Months</styled-content></th>
<th align="center" valign="top"><styled-content style="color:#A02B93">15 Months</styled-content></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">One</td>
<td align="center" valign="top">65</td>
<td align="center" valign="top">50</td>
</tr>
<tr>
<td align="left" valign="top">Two</td>
<td align="center" valign="top">30</td>
<td align="center" valign="top">43.75</td>
</tr>
<tr>
<td align="left" valign="top">Three</td>
<td align="center" valign="top">5</td>
<td align="center" valign="top">6.25</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Table S2.</label>
<caption><title>Table summarising the numbers of datasets included in the analyses for both samples as well as reason for exclusion.</title></caption>
<alternatives>
<graphic xlink:href="26638.47686v2_tbl2.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all" width="100%">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top">5 MONTHS (N)</th>
<th align="center" valign="top">15 MONTHS (N)</th>
</tr>
</thead>
<tbody>
<tr style="background-color:#E8E8E8">
<td align="left" valign="top"><bold>Behaviour coded</bold></td>
<td align="center" valign="top"><bold>33</bold></td>
<td align="center" valign="top"><bold>34</bold></td>
</tr>
<tr style="background-color:#E8E8E8">
<td align="left" valign="top" colspan="3"><bold>Audio features</bold></td>
</tr>
<tr>
<td align="left" valign="top">Missing wav file</td>
<td align="center" valign="top">2</td>
<td align="center" valign="top">4</td>
</tr>
<tr>
<td align="left" valign="top">Match cannot be found <xref ref-type="table-fn" rid="tbl2fn1">*</xref></td>
<td align="center" valign="top">1</td>
<td align="center" valign="top">0</td>
</tr>
<tr>
<td align="left" valign="top">Final N of usable datasets</td>
<td align="center" valign="top">30</td>
<td align="center" valign="top">30</td>
</tr>
<tr>
<td align="left" valign="top">Object naming</td>
<td align="center" valign="top">30</td>
<td align="center" valign="top">30</td>
</tr>
<tr>
<td align="left" valign="top">Information Rate</td>
<td align="center" valign="top">30</td>
<td align="center" valign="top">30</td>
</tr>
<tr>
<td align="left" valign="top">Semantic Surprisal</td>
<td align="center" valign="top">27</td>
<td align="center" valign="top">30</td>
</tr>
<tr>
<td align="left" valign="top">Spectral flux – ambient</td>
<td align="center" valign="top">27</td>
<td align="center" valign="top">30</td>
</tr>
<tr>
<td align="left" valign="top">Spectral flux – vocalisations</td>
<td align="center" valign="top">27</td>
<td align="center" valign="top">30</td>
</tr>
<tr style="background-color:#E8E8E8">
<td align="left" valign="top" colspan="3"><bold>Video features</bold></td>
</tr>
<tr>
<td align="left" valign="top">Final N of usable datasets</td>
<td align="center" valign="top">33</td>
<td align="center" valign="top">34</td>
</tr>
<tr>
<td align="left" valign="top">Facial Novelty (KL expression)</td>
<td align="center" valign="top">33</td>
<td align="center" valign="top">31</td>
</tr>
<tr>
<td align="left" valign="top">Facial movement</td>
<td align="center" valign="top">32</td>
<td align="center" valign="top">34</td>
</tr>
<tr>
<td align="left" valign="top">Hand movement</td>
<td align="center" valign="top">32</td>
<td align="center" valign="top">34</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="tbl2fn1"><label>*</label><p>Match between the audio streams from the microphone and the video could not be found. Thus, the audio data could not be synchronised to the video data.</p></fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>Table S3.</label>
<caption><title>Statistics for overall average levels</title></caption>
<alternatives>
<graphic xlink:href="26638.47686v2_tbl3.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all" width="100%">
<thead>
<tr style="background-color:#E8E8E8">
<th align="left" valign="top">Two-sided t-test</th>
<th align="center" valign="top">p-value/p-adj (FDR)</th>
<th align="center" valign="top">t-stat</th>
<th align="center" valign="top">df</th>
<th align="center" valign="top">sd</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Information Rate</td>
<td align="center" valign="top">0.002/0.003</td>
<td align="center" valign="top">-3.189</td>
<td align="center" valign="top">58</td>
<td align="center" valign="top">0.013</td>
</tr>
<tr>
<td align="left" valign="top">Facial movement</td>
<td align="center" valign="top">0.335</td>
<td align="center" valign="top">-0.972</td>
<td align="center" valign="top">64</td>
<td align="center" valign="top">0.004</td>
</tr>
<tr>
<td align="left" valign="top">Hand movement</td>
<td align="center" valign="top">0.104</td>
<td align="center" valign="top">-1.650</td>
<td align="center" valign="top">64</td>
<td align="center" valign="top">2.126</td>
</tr>
<tr>
<td align="left" valign="top">Spectral flux - ambient</td>
<td align="center" valign="top">0.278</td>
<td align="center" valign="top">-1.096</td>
<td align="center" valign="top">55</td>
<td align="center" valign="top">82.467</td>
</tr>
<tr>
<td align="left" valign="top">Spectral flux - vocalisations</td>
<td align="center" valign="top">0.144</td>
<td align="center" valign="top">-1.481</td>
<td align="center" valign="top">55</td>
<td align="center" valign="top">178.657</td>
</tr>
<tr>
<td align="left" valign="top">Semantic surprisal</td>
<td align="center" valign="top">0.791</td>
<td align="center" valign="top">0.267</td>
<td align="center" valign="top">55</td>
<td align="center" valign="top">1.131</td>
</tr>
</tbody>
</table>
<table frame="box" rules="all" width="100%">
<thead>
<tr style="background-color:#E8E8E8">
<th align="left" valign="top">Wilcoxon test</th>
<th align="center" valign="top">p-value/p-adj (FDR)</th>
<th align="center" valign="top">z-value</th>
<th align="center" valign="top">ranksum</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Facial novelty (expression KL)</td>
<td align="center" valign="top">0.246</td>
<td align="center" valign="top">-1.159</td>
<td align="center" valign="top">710</td>
</tr>
<tr>
<td align="left" valign="top">Object naming</td>
<td align="center" valign="top">0.002/0.003</td>
<td align="center" valign="top">-3.068</td>
<td align="center" valign="top">707</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Experimental paradigm</title>
<p><bold>Experimental paradigm.</bold> A) Top figure shows the experimental set up for the joint play condition. Two cameras pointed at the infant (view in photos 1 and 2) and one camera pointed at the mother (view in photo number 3). Looking behaviour was coded manually at 50fps for object and partner looks from both the mother and the infant. B) shows the different type of looks (i.e. looks to object 1-3, looks to partner and ‘others’. Notice that the latter category – ‘others’ – included inattention and uncodable moments). C) Plot of the average duration of the joint play interactions at 5 months (in orange) and 15 months (in purple). Asterisks indicate significance from the two-sample t-test (* = p&lt;0.05, ** = p&lt;0.01, *** = p&lt; 0.001). D) Photos of the toys employed at both time points: panda (A), book (B) and rattle (C).</p></caption>
<graphic xlink:href="26638.47686v2_fig6.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Descriptive analysis on looks to the object and to the partner.</title>
<p><bold>Descriptive analyses on looking behaviour.</bold> Figure showing average number of looks per minute (A, C) and average look duration (in seconds) (B, D) for looks to object (A, B) and looks to partner (C, D) respectively. Asterisks indicate significance (* = p&lt;0.05, ** = p&lt;0.01, *** = p&lt; 0.001).</p></caption>
<graphic xlink:href="26638.47686v2_fig7.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Average amount of speech per participant.</title>
<p><bold>Descriptive analyses on amount of speech.</bold> Figure showing average number of words. To quantify the amount of speech in each interaction, we transformed the output from Whisper Open AI (<xref ref-type="bibr" rid="c81">Radford et al., 2023</xref>) into a binary array where ones represent words and zeroes indicate the absence of words. We then summed the number of words per interaction and divided it by the length of each interaction respectively. We conducted a two-sided t-test to assess significant differences across age groups (* indicates p&lt;0.05).</p></caption>
<graphic xlink:href="26638.47686v2_fig8.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4.</label>
<caption><title>Schematic illustrating how to interpret cross-correlation results</title>
<p><bold>Visual guide on how to interpret findings from a cross-correlation analysis.</bold> Each plot illustrates the interpretation of a positive (A and B) and negative cross-correlation (C and D) values across backward (A and C) and forward (B and D) time-lags. Below each cross-correlation scheme is a illustration of the alternative possible explanations regarding the cross-correlated variables. For instance A) shows a positive correlation in backward lags between infant gaze and maternal behaviour, this can be interpreted is two complementary ways : an increased in infant gaze in followed by an increased in maternal behaviour or that a decreased in infant gaze is followed by a decreased in maternal behaviour.</p></caption>
<graphic xlink:href="26638.47686v2_fig9.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5.</label>
<caption><title>Correlations between behavioural measures.</title>
<p><bold>Matrix of pairwise cross-correlations between behavioural measures</bold>. Each subplot shows the correlation values between pairs of variables at different lags (in seconds), where backwards lags indicate that the first variable (variables at the top) precede the second one (variables on the side), and forward lags that the first variable follows the second one. Please refer to <xref ref-type="fig" rid="figS4">Fig S4</xref> for a visual guide on interpreting findings from</p></caption>
<graphic xlink:href="26638.47686v2_fig10.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6.</label>
<caption><title>Cross correlations between lower-level features and infant attention to faces.</title>
<p><bold>Cross correlations between lower-level features and infant attention to faces.</bold> Spectral flux of non-vocalisations and vocalisations, facial movement, and hand movement in relation to infant attention to faces at 5 months (A, C, E, G) and 15 months (B, D, F, H), respectively. Thick orange (5 months) and purple (15 months) lines represent the observed cross-correlation results, with shaded coloured areas showing their SEM. Grey lines represent control (permuted) data, with the shaded grey area indicating its SEM. Red thick lines indicate significance from the CBP test (p&lt;0.025, two-sided). All results were FDR adjusted.</p></caption>
<graphic xlink:href="26638.47686v2_fig11.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Figure S7.</label>
<caption><title>Cross correlations between higher-level features and infant attention to faces.</title>
<p><bold>Cross correlations between higher-level features and infant attention to faces.</bold> Maternal object naming, information rate, semantic surprisal, and facial novelty in relation to infant attention to objects at 5 months (A, C, E, G) and 15 months (B, D, F, H), respectively. Thick orange (5 months) and purple (15 months) lines represent the observed cross-correlation results, with shaded coloured areas showing their SEM. Grey lines represent control (permuted) data, with the shaded grey area indicating its SEM. Red thick lines indicate significance from the CBP test (p&lt;0.025, two-sided). All results were FDR adjusted.</p></caption>
<graphic xlink:href="26638.47686v2_fig12.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. [853251 - ONACSA]) and from Medical Research Council grant MR/X021998/1. We want to send a special thank you to María Penaherrra, Mukrime Gok, Stefanie Pow, Desirèe Cardile and Georgina Harris for their patience and dedication with the coding of our gaze data. We would also like to thank all participating infants and caregivers that took part in our study.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abney</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Suanda</surname>, <given-names>S. H.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name>, &amp; <string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2020</year>). <article-title>What are the building blocks of parent-infant coordinated attention in free-flowing interaction?</article-title> <source>Infancy</source>, <elocation-id>infa.12365</elocation-id>. <pub-id pub-id-type="doi">10.1111/infa.12365</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Rowan Candy</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Gold</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>2024</year>). <article-title>An edge-simplicity bias in the visual input to young infants</article-title>. In <source>Sci. Adv</source> (Vol. <volume>10</volume>).</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Seemiller</surname>, <given-names>E. S.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Scene saliencies in egocentric vision and their creation by parents and infants</article-title>. <source>Cognition</source>, <volume>229</volume>. <pub-id pub-id-type="doi">10.1016/j.cognition.2022.105256</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aureli</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Presaghi</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Garito</surname>, <given-names>M. C.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Mother-infant co-regulation during infancy: Developmental changes and influencing factors</article-title>. <source>Infant Behavior and Development</source>, <volume>69</volume>. <pub-id pub-id-type="doi">10.1016/j.infbeh.2022.101768</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bahrick</surname>, <given-names>L. E.</given-names></string-name>, &amp; <string-name><surname>Lickliter</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Intersensory redundancy guides attentional selectivity and perceptual learning in infancy</article-title>. <source>Developmental Psychology</source>, <volume>36</volume>(<issue>2</issue>), <fpage>190</fpage>–<lpage>201</lpage>. <pub-id pub-id-type="doi">10.1037/0012-1649.36.2.190</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bain</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Huh</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Han</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Zisserman</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>WhisperX: Time-Accurate Speech Transcription of Long-Form Audio</article-title>. <conf-name>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</conf-name>, <conf-date>2023-Augus</conf-date>, <fpage>4489</fpage>–<lpage>4493</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2023-78</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bakeman</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Adamson</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>1984</year>). <article-title>Coordinating Attention to People and Objects in Mother-Infant and Peer-Infant Interaction</article-title>. In <source>Child Development</source> (Vol. <volume>55</volume>).</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baldassano</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zadbood</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Hasson</surname>, <given-names>U.</given-names></string-name>, &amp; <string-name><surname>Norman</surname>, <given-names>K. A.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Discovering Event Structure in Continuous Narrative Perception and Memory</article-title>. <source>Neuron</source>, <volume>95</volume>(<issue>3</issue>), <fpage>709</fpage>–<lpage>721.e5</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.041</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Bauch</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Theofilis</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Rachum</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bovee</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>González</surname>, <given-names>M. P.</given-names></string-name></person-group> (<year>2015</year>). <source>PyLZMA</source>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beebe</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Messinger</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bahrick</surname>, <given-names>L. E.</given-names></string-name>, <string-name><surname>Margolis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Buvk</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Chen</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2016</year>). <article-title>A systems view of mother-infant face-to-face communication</article-title>. <source>Developmental Psychology</source>, <volume>52</volume>(<issue>4</issue>), <fpage>556</fpage>–<lpage>571</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Hochberg</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>1995</year>). <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>Journal of the Royal Statistical Society</source>, <volume>57</volume>(<issue>1</issue>), <fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berger</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Posner</surname>, <given-names>M. I.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Beyond Infant’s Looking: The Neural Basis for Infant Prediction Errors</article-title>. <source>Perspectives on Psychological Science</source>, <volume>18</volume>(<issue>3</issue>), <fpage>664</fpage>–<lpage>674</lpage>. <pub-id pub-id-type="doi">10.1177/17456916221112918</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bigelow</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Maclean</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Proctor</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2004</year>). <article-title>The role of joint attention in the development of infants’ play with objects</article-title>. <source>Developmental Science</source> <volume>7</volume> <issue>5</issue>).</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biringen</surname>, <given-names>Z. C.</given-names></string-name></person-group> (<year>1987</year>). <article-title>Infant Attention to Facial Expressions and Facial Motion</article-title>. <source>Journal of Genetic Psychology</source>, <volume>148</volume>(<issue>1</issue>), <fpage>127</fpage>–<lpage>133</lpage>. <pub-id pub-id-type="doi">10.1080/00221325.1987.9914543</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brennan</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Galati</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Kuhlen</surname>, <given-names>A. K.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Two Minds, One Dialog: Coordinating Speaking and Understanding</article-title>. <source>Psychology of Learning and Motivation - Advances in Research and Theory</source> <volume>53</volume>: <fpage>301</fpage>–<lpage>344</lpage>). <pub-id pub-id-type="doi">10.1016/S0079-7421(10)53008-1</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bruner</surname>, <given-names>J. S.</given-names></string-name></person-group> (<year>1974</year>). <article-title>From communication to language - A psychological perspective</article-title>. <source>Cognition</source>, <volume>3</volume>(<issue>3</issue>), <fpage>255</fpage>–<lpage>287</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carretero</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Español</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Multimodal Study of Adult-Infant Interaction: A Review of Its Origins and Its Current Status</article-title>. <source>Paidéia (Ribeirão Preto)</source>, <volume>26</volume>, <fpage>377</fpage>–<lpage>385</lpage>. <pub-id pub-id-type="doi">10.1590/1982-43272665201613</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>C. H. C.</given-names></string-name>, <string-name><surname>Nastase</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Hasson</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Information flow across the cortical timescale hierarchy during narrative construction</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>119</volume>(<issue>51</issue>), <elocation-id>e2209307119-e2209307119</elocation-id>. <pub-id pub-id-type="doi">10.1073/pnas.2209307119</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chater</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Vitányi</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Simplicity : A unifying principle in cognitive science?</article-title> <source>Trends in Cognitive Sciences</source>, <volume>7</volume>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chong</surname>, <given-names>S. C. F.</given-names></string-name>, <string-name><surname>Werker</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Russell</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Carroll</surname>, <given-names>J. M.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Three facial expressions mothers direct to their infants</article-title>. <source>Infant and Child Development</source>, <volume>12</volume>(<issue>3</issue>), <fpage>211</fpage>–<lpage>232</lpage>. <pub-id pub-id-type="doi">10.1002/icd.286</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cooke</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Deneault</surname>, <given-names>A.-A.</given-names></string-name>, <string-name><surname>Devereux</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Eirich</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fearon</surname>, <given-names>R. M. P.</given-names></string-name>, &amp; <string-name><surname>Madigan</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Parental sensitivity and child behavioral problems: A meta-analytic review</article-title>. <source>Child Development</source>, <volume>93</volume>(<issue>5</issue>), <fpage>1231</fpage>–<lpage>1248</lpage>. <pub-id pub-id-type="doi">10.1111/cdev.13764</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cooke</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>King</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Garnier</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Aubanel</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The listening talker: A review of human and algorithmic context-induced modifications of speech</article-title>. <source>Comput. Speech Lang.</source>, <volume>28</volume>, <fpage>543</fpage>–<lpage>571</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de Klerk</surname>, <given-names>C. C. J. M.</given-names></string-name>, <string-name><surname>Hamilton</surname>, <given-names>A. F. de C.</given-names></string-name>, &amp; <string-name><surname>Southgate</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Eye contact modulates facial mimicry in 4-month-old infants: An EMG and fNIRS study</article-title>. <source>Cortex</source>, <volume>106</volume>, <fpage>93</fpage>–<lpage>103</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2018.05.002</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dewey</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1896</year>). <article-title>The reflex arc concept in psychology</article-title>. <source>Psychological Review 3.4</source>, <volume>3</volume>(<issue>4</issue>), <fpage>357</fpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dionne-Dostie</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Paquette</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Lassonde</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Gallagher</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Multisensory Integration and Child Neurodevelopment</article-title>. <source>Brain Sciences</source>, <volume>5</volume>(<issue>1</issue>), <fpage>32</fpage>–<lpage>57</lpage>. <pub-id pub-id-type="doi">10.3390/brainsci5010032</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Español</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Shifres</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2015</year>). <article-title>The Artistic Infant Directed Performance: A Mycroanalysis of the Adult’s Movements and Sounds</article-title>. <source>Integrative Psychological and Behavioral Science</source>, <volume>49</volume>, <fpage>371</fpage>–<lpage>397</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feldman</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Parent-infant synchrony and the construction of shared timing; physiological precursors, developmental outcomes, and risk conditions</article-title>. <source>Journal of Child Psychology and Psychiatry and Allied Disciplines</source> <volume>48</volume>: <fpage>329</fpage>–<lpage>354</lpage>). <pub-id pub-id-type="doi">10.1111/j.1469-7610.2006.01701.x</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fogel</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Garvey</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Alive communication</article-title>. <source>Infant Behavior and Development</source>, <volume>30</volume>(<issue>2</issue>), <fpage>251</fpage>–<lpage>257</lpage>. <pub-id pub-id-type="doi">10.1016/j.infbeh.2007.02.007</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fogel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nwokah</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Dedo</surname>, <given-names>J. Y.</given-names></string-name>, <string-name><surname>Messinger</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Dickson</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Matusov</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Holt</surname>, <given-names>S. A.</given-names></string-name></person-group> (<year>1992</year>). <article-title>Social process theory of emotion: A dynamic systems approach</article-title>. <source>Social Development</source>, <volume>1</volume>(<issue>2</issue>), <fpage>122</fpage>–<lpage>142</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9507.1992.tb00116.x</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franchak</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Beyond screen time: Using head-mounted eye tracking to study natural behavior</article-title>. <source>Advances in Child Development and Behavior</source>, <volume>62</volume>, <fpage>61</fpage>–<lpage>91</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frank</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Vul</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>S. P.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Development of infants’ attention to faces during the first year</article-title>. <source>Cognition</source>, <volume>110</volume>(<issue>2</issue>), <fpage>160</fpage>–<lpage>170</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2008.11.010</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frank</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Otten</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Galli</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Vigliocco</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2015</year>). <article-title>The ERP response to the amount of information conveyed by words in sentences</article-title>. <source>Brain and Language</source>, <volume>140</volume>, <fpage>1</fpage>–<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2014.10.006</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gerken</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Balcomb</surname>, <given-names>F. K.</given-names></string-name>, &amp; <string-name><surname>Minton</surname>, <given-names>J. L.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Infants avoid “labouring in vain” by attending more to learnable than unlearnable linguistic patterns</article-title>. <source>Developmental Science</source>, <volume>14</volume>(<issue>5</issue>), <fpage>972</fpage>–<lpage>979</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-7687.2011.01046.x</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gibson</surname>, <given-names>E. J.</given-names></string-name></person-group> (<year>1988</year>). <article-title>Exploratory behavior in the development of perceiving, acting, and the acquiring of knowledge</article-title>. <source>Ann. Rev. Psychol</source>, <volume>39</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>41</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giordano</surname>, <given-names>B. L.</given-names></string-name>, <string-name><surname>Esposito</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Valente</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Formisano</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Intermediate acoustic-to-semantic representations link behavioral and neural responses to natural sounds</article-title>. <source>Nature Neuroscience</source>, <volume>26</volume>(<issue>4</issue>), <fpage>664</fpage>–<lpage>672</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-023-01285-9</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goupil</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Dautriche</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Denman</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Henry</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Marriott-Haresign</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Wass</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Leader-follower dynamics during early social interactions matter for infant word learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>121</volume>(<issue>38</issue>), <elocation-id>e2321008121-e2321008121</elocation-id>. <pub-id pub-id-type="doi">10.1073/pnas.2321008121</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gratier</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Magnier</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Sense and Synchrony: Infant Communication and Musical Improvisation</article-title>. <source>Intermédialités: Histoire et Théorie Des Arts, Des Lettres et Des Techniques</source>, <volume>19</volume>, <fpage>45</fpage>. <pub-id pub-id-type="doi">10.7202/1012655ar</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Gwilliams</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Marantz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>King</surname>, <given-names>J.-R.</given-names></string-name></person-group> (<year>2024</year>). <source>Hierarchical dynamic coding coordinates speech comprehension in the brain</source>. <pub-id pub-id-type="doi">10.1101/2024.04.19.590280</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ham</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Tronick</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Relational psychophysiology: Lessons from mother-nfant physiology research on dyadically expanded states of consciousness</article-title>. <source>Psychotherapy Research</source>, <volume>19</volume>(<issue>6</issue>), <fpage>619</fpage>–<lpage>632</lpage>. <pub-id pub-id-type="doi">10.1080/10503300802609672</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hasson</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Honey</surname>, <given-names>C. J.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Hierarchical process memory: Memory as an integral component of information processing</article-title>. <source>Trends in Cognitive Sciences</source> <volume>19</volume>: <fpage>304</fpage>–<lpage>313</lpage>). <pub-id pub-id-type="doi">10.1016/j.tics.2015.04.006</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hasson</surname>, <given-names>U.</given-names></string-name>, &amp; <string-name><surname>Frith</surname>, <given-names>C. D.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Mirroring and beyond: coupled dynamics as a generalized framework for modelling social interactions</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>371</volume>(<issue>1693</issue>), <fpage>20150366</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2015.0366</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heilbron</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Armeni</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schoffelen</surname>, <given-names>J.-M.</given-names></string-name>, <string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A hierarchy of linguistic predictions during natural language comprehension</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>119</volume> (<issue>32</issue>), <elocation-id>e2201968119-e2201968119</elocation-id>. <pub-id pub-id-type="doi">10.1073/pnas.2201968119</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hilton</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Bertolo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lee-Rubin</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Amir</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bainbridge</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Simson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Knox</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Glowacki</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Alemu</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Galbarczyk</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jasienska</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ross</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Neff</surname>, <given-names>M. B.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cirelli</surname>, <given-names>L. K.</given-names></string-name>, <string-name><surname>Trehub</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>M.</given-names></string-name>, … <string-name><surname>Mehr</surname>, <given-names>S. A.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Acoustic regularities in infant-directed speech and song across cultures</article-title>. <source>Nature Human Behaviour</source>, <volume>6</volume>(<issue>11</issue>), <fpage>1545</fpage>–<lpage>1556</lpage>. <pub-id pub-id-type="doi">10.1038/s41562-022-01410-x</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Elhilali</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Auditory salience using natural soundscapes</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>141</volume>(<issue>3</issue>), <fpage>2163</fpage>–<lpage>2176</lpage>. <pub-id pub-id-type="doi">10.1121/1.4979055</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Isomura</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Nakano</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Automatic facial mimicry in response to dynamic emotional stimuli in five-month-old infants</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>, <volume>283</volume>(<issue>1844</issue>), <elocation-id>20161948</elocation-id>. <pub-id pub-id-type="doi">10.1098/rspb.2016.1948</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Itti</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Baldi</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Bayesian surprise attracts human attention</article-title>. <source>Vision Research</source>, <volume>49</volume>(<issue>10</issue>), <fpage>1295</fpage>–<lpage>1306</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2008.09.007</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jaffe</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Beebe</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Feldstein</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Rhythms of Dialogue in Infancy: Coordinated Timing in Development</article-title>. <source>Monographs of the Society for Research in Child Development</source>, Vol. <volume>66</volume>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiat</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Luck</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Beckner</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Hayes</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Pomaranski</surname>, <given-names>K. I.</given-names></string-name>, <string-name><surname>Henderson</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Oakes</surname>, <given-names>L. M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Linking patterns of infant eye movements to a neural network model of the ventral stream using representational similarity analysis</article-title>. <source>Developmental Science</source>, <volume>25</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1111/desc.13155</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kidd</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Piantadosi</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2012</year>). <article-title>The Goldilocks Effect: Human Infants Allocate Attention to Visual Sequences That Are Neither Too Simple Nor Too Complex</article-title>. <source>PloS One</source>, <volume>7</volume>, <elocation-id>e36399–e36399</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0036399</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kidd</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Piantadosi</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The Goldilocks Effect in Infant Auditory Attention</article-title>. <source>Child Development</source>, <volume>85</volume>. <pub-id pub-id-type="doi">10.1111/cdev.12263</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kliesch</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Parise</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Reid</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Hoehl</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The role of social signals in segmenting observed actions in 18-month-old children</article-title>. <source>Developmental Science</source>, <volume>25</volume>(<issue>3</issue>), <elocation-id>e13198–e13198</elocation-id>. <pub-id pub-id-type="doi">10.1111/desc.13198</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Köster</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kayhan</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Langeloh</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Hoehl</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Making Sense of the World: Infant Learning From a Predictive Processing Perspective</article-title>. <source>Perspectives on Psychological Science</source>, <volume>15</volume>, <fpage>174569161989507</fpage>–<lpage>174569161989507</lpage>. <pub-id pub-id-type="doi">10.1177/1745691619895071</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kothinti</surname>, <given-names>S. R.</given-names></string-name>, &amp; <string-name><surname>Elhilali</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Are acoustics enough? Semantic effects on auditory salience in natural scenes</article-title>. <source>Frontiers in Psychology</source>, <volume>14</volume>. <pub-id pub-id-type="doi">10.3389/fpsyg.2023.1276237</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kothinti</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Elhilali</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Auditory salience using natural scenes: An online study</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>150</volume>, <fpage>2952</fpage>–<lpage>2966</lpage>. <pub-id pub-id-type="doi">10.1121/10.0006750</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Labendzki</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Goupil</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Wass</surname>, <given-names>S. V.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Temporal patterns in the complexity of child-directed song lyrics reflect their functions</article-title>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Landis</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Koch</surname>, <given-names>G. G.</given-names></string-name></person-group> (<year>1977</year>). <article-title>An Application of Hierarchical Kappa-type Statistics in the Assessment of Majority Agreement among Multiple Observers</article-title> <source>Biometrics</source> <volume>33</volume>:<fpage>363</fpage>–<lpage>374</lpage> <pub-id pub-id-type="doi">10.2307/2529786</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lavelli</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Fogel</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Interdyad Differences in Early Mother-Infant Face-to-Face Communication: Real-Time Dynamics and Developmental Pathways</article-title>. <source>Developmental Psychology</source>, <volume>49</volume>(<issue>12</issue>), <fpage>2257</fpage>–<lpage>2271</lpage>. <pub-id pub-id-type="doi">10.1037/a0032268.supp</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lecun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Deep learning</article-title>. In <source>Nature</source> <volume>521</volume> <fpage>436</fpage>–<lpage>444</lpage>). <pub-id pub-id-type="doi">10.1038/nature14539</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Leong</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Kalashnikova</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Burnham</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Goswami</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2017</year>). <source>The Temporal Modulation Structure of Infant-Directed Speech</source>. <pub-id pub-id-type="doi">10.17863/CAM.9089</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lewkowicz</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Hansen-Tift</surname>, <given-names>A. M.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Infants deploy selective attention to the mouth of a talking face when learning speech</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>109</volume>(<issue>5</issue>), <fpage>1431</fpage>–<lpage>1436</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1114783109</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luna</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Velanova</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Geier</surname>, <given-names>C. F.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Development of eye-movement control</article-title>. <source>Brain and Cognition</source>, <volume>68</volume>(<issue>3</issue>), <fpage>293</fpage>–<lpage>308</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandc.2008.08.019</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malloch</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Trevarthen</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Communicative Musicality: Exploring the Basis of Human Companionship</article-title>. <source>British Journal of Psychotherapy</source>, <volume>26</volume>, <fpage>100</fpage>–<lpage>105</lpage>. <pub-id pub-id-type="doi">10.1111/j.1752-0118.2009.01158_1.x</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>164</volume>(<issue>1</issue>), <fpage>177</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Méndez</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>2021</year>). <source>One-year old infants control bottom-up saliencies to purposely sustain attention</source>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mendez</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Controlling the input: How one-year-old infants sustain visual attention</article-title>. <source>Developmental Science</source>. <pub-id pub-id-type="doi">10.1111/desc.13445</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mendez</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Controlling the input: How one-year-old infants sustain visual attention</article-title>. <source>Developmental Science</source>, <volume>27</volume>(<issue>2</issue>), <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1111/desc.13445</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meyer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lamers</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kayhan</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Hunnius</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Enhancing reproducibility in developmental EEG research: BIDS, cluster-based permutation tests, and effect sizes</article-title>. <source>Developmental Cognitive Neuroscience</source>, <volume>52</volume>. <pub-id pub-id-type="doi">10.1016/j.dcn.2021.101036</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meyer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>van Schaik</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Poli</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Hunnius</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2023</year>). <article-title>How infant-directed actions enhance infants’ attention, learning, and exploration: Evidence from EEG and computational modeling</article-title>. <source>Developmental Science</source>, <volume>26</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1111/desc.13259</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Müller</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2015</year>). <source>Fundamentals of music processing: Audio, analysis, algorithms, applications</source> (Vol. <volume>5</volume>). <publisher-name>Springer</publisher-name>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Müller</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Chiu</surname>, <given-names>C.-Y.</given-names></string-name></person-group> (<year>2024</year>). <article-title>A Basic Tutorial on Novelty and Activation Functions for Music Signal Processing</article-title>. <source>Transactions of the International Society for Music Information Retrieval</source>. <pub-id pub-id-type="doi">10.5334/tismir.202</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mundy</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Block</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Delgado</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pomares</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Van Hecke</surname>, <given-names>A. V.</given-names></string-name>, &amp; <string-name><surname>Parlade</surname>, <given-names>M. V.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Individual differences and the development of joint attention in infancy</article-title>. <source>Child Development</source>, <volume>78</volume>(<issue>3</issue>), <fpage>938</fpage>–<lpage>954</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-8624.2007.01042.x</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mundy</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Mastergeorge</surname>, <given-names>A. M.</given-names></string-name></person-group> (<year>2009</year>). <article-title>A parallel and distributed-processing model of joint attention, social cognition and autism</article-title>. <source>Autism Research</source> <volume>2</volume>: <fpage>2</fpage>–<lpage>21</lpage>). <pub-id pub-id-type="doi">10.1002/aur.61</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Murray</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2014</year>). <source>The psychology of babies: How relationships support development from birth to two</source>. <publisher-loc>Hachette UK</publisher-loc>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>De Pascalis</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bozicevic</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hawkins</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sclafani</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Ferrari</surname>, <given-names>P. F.</given-names></string-name></person-group> (<year>2016</year>). <article-title>The functional architecture of mother-infant communication, and the development of infant social expressiveness in the first two months</article-title>. <source>Scientific Reports</source>, <volume>6</volume>. <pub-id pub-id-type="doi">10.1038/srep39019</pub-id></mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nencheva</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Lew-Williams</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Understanding why infant-directed speech supports learning: A dynamic attention perspective</article-title>. <source>Developmental Review</source>, <volume>66</volume>, <fpage>101047</fpage>. <pub-id pub-id-type="doi">10.1016/j.dr.2022.101047</pub-id></mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pempek</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Kirkorian</surname>, <given-names>H. L.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Lund</surname>, <given-names>A. F.</given-names></string-name>, &amp; <string-name><surname>Stevens</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Video comprehensibility and attention in very young children</article-title>. <source>Developmental Psychology</source>, <volume>46</volume>(<issue>5</issue>), <fpage>1283</fpage>–<lpage>1293</lpage>. <pub-id pub-id-type="doi">10.1037/a0020614</pub-id></mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perapoch Amadó</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Phillips</surname>, <given-names>E. A. M.</given-names></string-name>, <string-name><surname>Esposito</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Greenwood</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ives</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Labendzki</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lancaster</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Northrop</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Viswanathan</surname>, <given-names>N. K.</given-names></string-name>, <string-name><surname>Gök</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Peñaherrera</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>E. J. H.</given-names></string-name>, &amp; <string-name><surname>Wass</surname>, <given-names>S. V.</given-names></string-name></person-group> (<year>2025</year>). <article-title>Who Leads and Who Follows? The Pathways to Joint Attention During Free-Flowing Interactions Change Over Developmental Time</article-title>. <source>Child Development</source>. <pub-id pub-id-type="doi">10.1111/cdev.14229</pub-id></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Pham</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Vu</surname>, <given-names>T. H.</given-names></string-name>, &amp; <string-name><surname>Tran</surname>, <given-names>T. A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Facial expression recognition using residual masking network</article-title>. <conf-name>Proceedings - International Conference on Pattern Recognition</conf-name>, <fpage>4513</fpage>–<lpage>4519</lpage>. <pub-id pub-id-type="doi">10.1109/ICPR48806.2021.9411919</pub-id></mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Phillips</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Goupil</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Marriot Haresign</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Whitehorn</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Leong</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Wass</surname>, <given-names>S. V.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Proactive or reactive? Neural oscillatory insight into the leader-follower dynamics of early infant-caregiver interaction</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>120</volume>(<issue>15</issue>).</mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poli</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Serino</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Mars</surname>, <given-names>R. B.</given-names></string-name>, &amp; <string-name><surname>Hunnius</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Infants tailor their attention to maximize learning</article-title>. <source>Science Advances</source>, <volume>6</volume>, <fpage>5053</fpage>–<lpage>5076</lpage>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Radford</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Brockman</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>McLeavey</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Robust speech recognition via large-scale weak supervision</article-title>. <conf-name>Proceedings of the 40th International Conference on Machine Learning</conf-name>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Räsänen</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Kakouros</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Soderstrom</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Is infant-directed speech interesting because it is surprising? - Linking properties of IDS to statistical learning and attention at the prosodic level</article-title>. <source>Cognition</source>, <volume>178</volume>, <fpage>193</fpage>–<lpage>206</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2018.05.015</pub-id></mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ravreby</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Shilat</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Yeshurun</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Liking as a balance between synchronization, complexity and novelty</article-title>. <source>Scientific Reports</source>, <volume>12</volume>, <fpage>3181</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-022-06610-z</pub-id></mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Reisner</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Labendzki</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hoehl</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Markova</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2024</year>). <source>The reciprocal relationship between maternal infant-directed singing and infant behavior</source>. <ext-link ext-link-type="uri" xlink:href="https://pure.pmu.ac.at/en/activities/the-reciprocal-relationship-between-maternal-infant-directed-sing">https://pure.pmu.ac.at/en/activities/the-reciprocal-relationship-between-maternal-infant-directed-sing</ext-link></mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schick</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fryns</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wegdell</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Laporte</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zuberbühler</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>van Schaik</surname>, <given-names>C. P.</given-names></string-name>, <string-name><surname>Townsend</surname>, <given-names>S. W.</given-names></string-name>, &amp; <string-name><surname>Stoll</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The function and evolution of child-directed communication</article-title>. <source>PLoS Biology</source>, <volume>20</volume>(<issue>5</issue>). <pub-id pub-id-type="doi">10.1371/journal.pbio.3001630</pub-id></mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmidhuber</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Driven by Compression Progress: A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes</article-title>. <source>Anticipatory Behavior in Adaptive Learning Systems, Volume 5499 of Lecture Notes in Computer Science</source>, <volume>48</volume>. <pub-id pub-id-type="doi">10.1007/978-3-642-02565-5_4</pub-id></mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schroer</surname>, <given-names>S. E.</given-names></string-name>, &amp; <string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The real-time effects of parent speech on infants’ multimodal attention and dyadic coordination</article-title>. <source>Infancy</source>, <volume>27</volume>(<issue>6</issue>), <fpage>1154</fpage>–<lpage>1178</lpage>. <pub-id pub-id-type="doi">10.1111/infa.12500</pub-id></mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwab</surname>, <given-names>J. F.</given-names></string-name>, &amp; <string-name><surname>Lew-Williams</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Repetition across successive sentences facilitates young children’s word learning</article-title>. <source>Developmental Psychology</source>, <volume>52</volume>(<issue>6</issue>), <fpage>879</fpage>–<lpage>886</lpage>. <pub-id pub-id-type="doi">10.1037/dev0000125</pub-id></mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwab</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Rowe</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Cabrera</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Lew-Williams</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Fathers’ repetition of words is coupled with children’s vocabularies</article-title>. <source>Journal of Experimental Child Psychology</source>, <volume>166</volume>, <fpage>437</fpage>–<lpage>450</lpage>. <pub-id pub-id-type="doi">10.1016/j.jecp.2017.09.012</pub-id></mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Shain</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Meister</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pimentel</surname>, <given-names>T. I. D.</given-names></string-name>, <string-name><surname>Cotterell</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Levy</surname>, <given-names>R. I. D.</given-names></string-name></person-group> (<year>2024</year>). <source>Large-scale evidence for logarithmic effects of word predictability on reading time</source>. <volume>121</volume>, <fpage>2307876121</fpage>–<lpage>2307876121</lpage>. <pub-id pub-id-type="doi">10.1073/pnas</pub-id></mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shannon</surname>, <given-names>C. E.</given-names></string-name></person-group> (<year>1948</year>). <article-title>A mathematical theory of communication</article-title>. <source>The Bell System Technical Journal</source>, <volume>27</volume>(<issue>4</issue>), <fpage>623</fpage>–<lpage>656</lpage>. <pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb00917.x</pub-id></mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Gasser</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2005</year>). <article-title>The Development of Embodied Cognition: Six Lessons from Babies</article-title>. <source>Artificial Life</source>, <volume>11</volume>(<issue>1-2</issue>), <fpage>13</fpage>–<lpage>29</lpage>.</mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>N. A.</given-names></string-name>, &amp; <string-name><surname>Trainor</surname>, <given-names>L. J.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Infant-Directed Speech Is Modulated by Infant Feedback</article-title>. <source>Infancy</source>, <volume>13</volume>(<issue>4</issue>), <fpage>410</fpage>–<lpage>420</lpage>. <pub-id pub-id-type="doi">10.1080/15250000802188719</pub-id></mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stern</surname>, <given-names>D. N.</given-names></string-name></person-group> (<year>1974</year>). <article-title>The Goal and Structure of Mother-Infant Play</article-title>. <source>Journal of the American Academy of Child Psychiatry</source>, <volume>13</volume>(<issue>3</issue>), <fpage>402</fpage>–<lpage>421</lpage>. <pub-id pub-id-type="doi">10.1016/S0002-7138(09)61348-0</pub-id></mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suarez-Rivera</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Schatz</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Herzberg</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Tamis-LeMonda</surname>, <given-names>C. S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Joint engagement in the home environment is frequent, multimodal, timely, and structured</article-title>. <source>Infancy</source>, <volume>27</volume>(<issue>2</issue>), <fpage>232</fpage>–<lpage>254</lpage>. <pub-id pub-id-type="doi">10.1111/infa.12446</pub-id></mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suarez-Rivera</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name>, &amp; <string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Multimodal parent behaviors within joint attention support sustained attention in infants</article-title>. <source>Developmental Psychology</source>, <volume>55</volume>(<issue>1</issue>), <fpage>96</fpage>–<lpage>109</lpage>. <pub-id pub-id-type="doi">10.1037/dev0000628</pub-id></mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Yoshida</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Why the parent’s gaze is so powerful in organizing the infant’s gaze: The relationship between parental referential cues and infant object looking</article-title>. <source>Infancy</source>, <volume>27</volume>(<issue>4</issue>), <fpage>780</fpage>–<lpage>808</lpage>. <pub-id pub-id-type="doi">10.1111/infa.12475</pub-id></mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Sun</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Yoshida</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Effects of Viewed Object Size and Scene Saliency on Sustained Attention in Parent-Infant Object Play</article-title>. <conf-name>2024 IEEE International Conference on Development and Learning, ICDL 2024</conf-name>. <pub-id pub-id-type="doi">10.1109/ICDL61372.2024.10644837</pub-id></mixed-citation></ref>
<ref id="c99"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Tal</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Grossman</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Arnon</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Infant-directed speech becomes less redundant as infants grow: implications for language learning</article-title>. <source>PsyArXiv</source>. <pub-id pub-id-type="doi">10.31234/osf.io/bgtzd</pub-id></mixed-citation></ref>
<ref id="c100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tamis-LeMonda</surname>, <given-names>C. S.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The mountain stream of infant development</article-title>. <source>Infancy</source>, <volume>28</volume>(<issue>3</issue>), <fpage>468</fpage>–<lpage>491</lpage>. <pub-id pub-id-type="doi">10.1111/infa.12538</pub-id></mixed-citation></ref>
<ref id="c101"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taverna</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Padilla</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Waxman</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2024</year>). <article-title>How pervasive is joint attention? Mother-child dyads from a Wichi community reveal a different form of “togetherness</article-title>. <source>Developmental Science</source>, <volume>27</volume>(<issue>5</issue>).</mixed-citation></ref>
<ref id="c102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tippenhauer</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Fourakis</surname>, <given-names>E. R.</given-names></string-name>, <string-name><surname>Watson</surname>, <given-names>D. G.</given-names></string-name>, &amp; <string-name><surname>Lew-Williams</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2020</year>). <article-title>The scope of audience design in child-directed speech: Parents’ tailoring of word lengths for adult versus child listeners</article-title>. <source>Journal of Experimental Psychology: Learning Memory and Cognition</source>, <volume>46</volume>(<issue>11</issue>), <fpage>2163</fpage>–<lpage>2178</lpage>. <pub-id pub-id-type="doi">10.1037/xlm0000939</pub-id></mixed-citation></ref>
<ref id="c103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Schaik</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Meyer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>van Ham</surname>, <given-names>C. R.</given-names></string-name>, &amp; <string-name><surname>Hunnius</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Motion tracking of parents’ infant-versus adult-directed actions reveals general and action-specific modulations</article-title>. <source>Developmental Science</source>, <volume>23</volume>(<issue>1</issue>), <elocation-id>e12869–e12869</elocation-id>. <pub-id pub-id-type="doi">10.1111/desc.12869</pub-id></mixed-citation></ref>
<ref id="c104"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Viswanathan</surname>, <given-names>N. K.</given-names></string-name>, <string-name><surname>Esposito</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Labendzki</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Perapoch Amadó</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ives</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Greenwood</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Northrop</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Lancaster</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>de Klerk</surname>, <given-names>CJM</given-names></string-name>, <string-name><surname>Wass</surname>, <given-names>S. V.</given-names></string-name> &amp; <string-name><surname>Goupil</surname>, <given-names>L</given-names></string-name></person-group> (<year>2025</year>). <article-title>Do infants spontaneously imitate their caregivers’ voice during dyadic play?</article-title></mixed-citation></ref>
<ref id="c105"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Vygotsky</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Semenovich</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Cole</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1978</year>). <source>Mind in society: Development of higher psychological processes</source>. <publisher-name>Harvard university press</publisher-name>.</mixed-citation></ref>
<ref id="c106"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wass</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Jones</surname>, <given-names>E. J. H.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Editorial perspective: Leaving the baby in the bathwater in neurodevelopmental research</article-title>. <source>Journal of Child Psychology and Psychiatry</source>, <volume>64</volume>(<issue>8</issue>), <fpage>1256</fpage>–<lpage>1259</lpage>. <pub-id pub-id-type="doi">10.1111/jcpp.13750</pub-id></mixed-citation></ref>
<ref id="c107"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wass</surname>, <given-names>S. V.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Comparing methods for measuring peak look duration: Are individual differences observed on screen-based tasks also found in more ecologically valid contexts?</article-title> <source>Infant Behavior and Development</source>, <volume>37</volume>(<issue>3</issue>), <fpage>315</fpage>–<lpage>325</lpage>. <pub-id pub-id-type="doi">10.1016/j.infbeh.2014.04.007</pub-id></mixed-citation></ref>
<ref id="c108"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wass</surname>, <given-names>S. V.</given-names></string-name>, &amp; <string-name><surname>Goupil</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Studying the Developing Brain in Real-World Contexts: Moving From Castles in the Air to Castles on the Ground</article-title>. In <source>Frontiers in Integrative Neuroscience</source> (Vol. <volume>16</volume>). <publisher-name>Frontiers Media S.A.</publisher-name> <pub-id pub-id-type="doi">10.3389/fnint.2022.896919</pub-id></mixed-citation></ref>
<ref id="c109"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weineck</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wen</surname>, <given-names>O. X.</given-names></string-name>, &amp; <string-name><surname>Henry</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Neural synchronization is strongest to the spectral flux of slow music and depends on familiarity and beat salience</article-title>. <source>eLife</source>, <volume>11</volume>. <pub-id pub-id-type="doi">10.7554/ELIFE.75515</pub-id></mixed-citation></ref>
<ref id="c110"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Willems</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Frank</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Nijhof</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>van den Bosch</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Prediction During Natural Language Comprehension</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>(<issue>6</issue>), <fpage>2506</fpage>–<lpage>2516</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhv075</pub-id></mixed-citation></ref>
<ref id="c111"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname>, <given-names>J. G.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The SP theory of intelligence: Benefits and applications</article-title>. <source>Information (Switzerland)</source>, <volume>5</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.3390/info5010001</pub-id></mixed-citation></ref>
<ref id="c112"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname>, <given-names>J. G.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Information Compression as a Unifying Principle in Human Learning, Perception, and Cognition</article-title>. In <source>Complexity</source> (Vol. <volume>2019</volume>). <publisher-name>Hindawi Limited</publisher-name>. <pub-id pub-id-type="doi">10.1155/2019/1879746</pub-id></mixed-citation></ref>
<ref id="c113"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woźniak</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Knoblich</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Communication and action predictability: two complementary strategies for successful cooperation</article-title>. <source>Royal Society Open Science</source>, <volume>9</volume>(<issue>9</issue>). <pub-id pub-id-type="doi">10.1098/rsos.220577</pub-id></mixed-citation></ref>
<ref id="c114"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>de Barbaro</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Abney</surname>, <given-names>D. H.</given-names></string-name>, &amp; <string-name><surname>Cox</surname>, <given-names>R. F. A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Finding Structure in Time: Visualizing and Analyzing Behavioral Time Series</article-title>. <source>Frontiers in Psychology</source>, <volume>11</volume>. <pub-id pub-id-type="doi">10.3389/fpsyg.2020.01457</pub-id></mixed-citation></ref>
<ref id="c115"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yates</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Skalaban</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Ellis</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Bracher</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Baldassano</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Turk-Browne</surname>, <given-names>N. B.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Neural event segmentation of continuous experience in human infants</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>119</volume>(<issue>43</issue>), <elocation-id>e2200257119-e2200257119</elocation-id>. <pub-id pub-id-type="doi">10.1073/pnas.2200257119</pub-id></mixed-citation></ref>
<ref id="c116"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Embodied attention and word learning by toddlers</article-title>. <source>Cognition</source>, <volume>125</volume>(<issue>2</issue>), <fpage>244</fpage>–<lpage>262</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2012.06.016</pub-id></mixed-citation></ref>
<ref id="c117"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zacks</surname>, <given-names>J. M.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Event Perception and Memory</article-title>. <source>Annual Review of Psychology</source>. <volume>71</volume>:<fpage>165</fpage>–<lpage>191</lpage> <pub-id pub-id-type="doi">10.1146/annurev-psych-010419-051101</pub-id></mixed-citation></ref>
<ref id="c118"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Zeiler</surname>, <given-names>M. D.</given-names></string-name>, &amp; <string-name><surname>Fergus</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Visualizing and Understanding Convolutional Networks</article-title>. <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.1311.2901</pub-id></mixed-citation></ref>
<ref id="c119"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Frassinelli</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Tuomainen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Skipper</surname>, <given-names>J. I.</given-names></string-name>, &amp; <string-name><surname>Vigliocco</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2021</year>). <article-title>More than words: Word predictability, prosody, gesture and mouth movements in natural language comprehension</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>, <volume>288</volume>(<issue>1955</issue>). <pub-id pub-id-type="doi">10.1098/rspb.2021.0500</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109024.1.sa0</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study investigates how perceptual and semantic features of maternal behavior adapt to infants' attention during naturalistic play, providing new insights into the bidirectional and hierarchical organization of early social interaction. The methodology is innovative and overall <bold>solid</bold>, supported by comprehensive multimodal analyses and advanced information-theoretic methods, though some developmental claims warrant further tests of directionality and age effects. The work will be of interest to psychologists, cognitive scientists, and developmental researchers studying early communication, social learning, and methodological innovation in quantifying naturalistic behavior.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109024.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper investigates infants' social perception as reflected in looking behavior during face-to-face mother-infant toy play in two groups (5 and 15 months). Using information-theoretic and computer-vision methods, the authors quantify dynamic changes in lower-level (salience) and higher-level (semantic) features in the auditory and visual domains - primarily from mothers - and relate these to infants' real-time attention to toys (and to mothers). Time-lagged correlations suggest dynamic, reciprocal relations between infants' attention and maternal low-level (salience) and high-level (semantic) features at both ages, consistent with an early emergence of interpersonal social contingency based on multi-level information during interaction.</p>
<p>Strengths:</p>
<p>The study uses a naturalistic, multimodal mother-infant free-play paradigm and applies information-theoretic/AI methods to quantify both low- and high-level features of maternal behavior, enabling a fine-grained decomposition of interaction dynamics. The time-lag approach further allows examination of temporal relations between maternal signals and infants' attention.</p>
<p>Weaknesses:</p>
<p>Directionality claims from cross-correlations are sometimes unclear, especially when both positive and negative lags are significant, and the evidence for age effects is not yet convincing. Infant attention was manually coded with only moderate-substantial agreement, and handling of disagreements/uncodable periods should be clarified and acknowledged as a limitation.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109024.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study examines the dynamic interplay between infant attention and hierarchical maternal behaviors from a social information processing perspective. By employing a comprehensive naturalistic framework, the author quantified interactions across both low-level (sensory) and high-level (semantic) features. With correlation analysis with these features, they found that within social contexts, behaviors such as joint attention - shaped by mutual interaction - exhibit patterns distinct from unilateral responding or mimicry. In contrast to traditional semi-structured behavioral observation and coding, the methods employed in this study were designed to consciously and sensitively capture these dynamic features and relate them temporally. This approach contributes to a more integrated understanding of the developmental principles underlying capacities like joint action and communication.</p>
<p>Strengths:</p>
<p>The manuscript's core strength lies in its innovative, dynamic, and hierarchical framework for investigating early social attention. The findings reveal complex adaptive scaffolding strategies: for instance, when infants focus on objects, mothers reduce low-level sensory input, minimising distractions. Furthermore, the results indicate that, even from early development, maternal behaviors are both driven by and predictive of infant attention, confirming that attention involves complex interactive processes that unfold across multiple levels, from salience to semantics.</p>
<p>From a methodological standpoint, the use of unstructured play situations, combined with multi-channel, high-precision time-series analyses, undoubtedly required substantial effort in both data collection and coding. Compared to the relatively two-dimensional analytical approaches common in prior research, this study's introduction of lower-level and higher-level features to explore the hierarchical organization of processing across development is highly plausible. The psychological processes reflected by these quantified physical features span multiple domains - including emotion, motion, and phonetics - and the high temporal sampling rate ensures fine-grained resolution.</p>
<p>Critically, these features are extracted through a suite of advanced machine learning and computational methods, which automate the extraction of objective metrics from audiovisual data. Consequently, the methodological flow significantly enhances data utilization and offers valuable inspiration for future behavioral coding research aiming for high ecological validity.</p>
<p>Weaknesses:</p>
<p>The conclusion of this paper is generally supported by the data and analysis, but some aspects of data analysis need to be clarified and extended.</p>
<p>(1) A more explicit justification for the selection and theoretical categorization of the eight interaction features may be needed. The paper introduces a distinction between &quot;lower-level&quot; and &quot;higher-level&quot; features but does not clearly articulate the criteria underpinning this classification. While a continuum is acknowledged, the practical division requires a principled rationale. For instance, is the classification based on the temporal scale of the features, the degree of cognitive processing required for their integration, or their proximity to sensory input versus semantic meaning?</p>
<p>(2) The claims regarding age-related differences in Predictions 2 are not fully substantiated by the current analyses. The findings primarily rely on observing that an effect is significant in one age group but not the other (e.g., the association between object naming and attention is significant at 15 months but not at 5 months). However, this pattern alone does not constitute evidence about whether the two age groups differ significantly from each other. The absence of a direct statistical comparison (e.g., an interaction test in a model that includes age as a factor) creates an inferential gap. To robustly support developmental change, formal tests of the Age × Feature interaction on infant attention are required.</p>
<p>(3) Another potential methodological issue concerns the potential confounding effect of parents' use of the infant's name. The analysis of &quot;object naming&quot; does not clarify whether utterances containing object words (e.g., &quot;panda&quot;) were distinct from those that also incorporated the infant's name (e.g., &quot;Look, Sarah, the panda!&quot;). Given that a child's own name is a highly salient social cue known to robustly capture infant attention, its co-occurrence with object labels could potentially inflate or confound the measured effect of object naming itself. It would be important to know whether and how frequently infants' names were called, whether this variable was analyzed separately, and if its effect was statistically disentangled from that of pure object labeling.</p>
<p>(4) Interpretation of results requires clarification regarding the extended temporal lags reported, specifically the negative correlation between maternal vocal spectral flux and infant attention at 6.54 to 9.52 seconds (Figure 4C). The authors interpret this as a forward-prediction, suggesting that a decrease in acoustic variability leads to increased infant attention several seconds later. However, a lag of such duration seems unusually long for a direct, contingent infant response to a specific vocal feature. Is there existing empirical evidence from infant research to support such a prolonged response latency? Alternatively, could this signal suggest a slower, cyclical pattern of the interaction rather than a direct causal link?</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109024.1.sa3</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript presents an ambitious integration of multiple artificial intelligence technologies to examine social learning in naturalistic mother-infant interactions. The authors aimed to quantify how information flows between mothers and infants across different communicative modalities and timescales, using speech analysis (Whisper), pose detection (MMPose), facial expression recognition, and semantic modeling (GPT-2) in a unified analytical framework. Their goal was to provide unprecedented quantitative precision in measuring behavioral coordination and information transfer patterns during social learning, moving beyond traditional observational coding approaches to examine cross-modal coordination patterns and semantic contingencies in real-time across multiple temporal scales.</p>
<p>Strengths:</p>
<p>The integration of multiple AI tools into a coherent analytical framework represents a genuine methodological breakthrough that advances our capabilities for studying complex social phenomena. The authors successfully analyzed naturalistic interactions at a scale and level of detail that was not previously possible, examining 33 5-month-old and 34 15-month-old dyads across multiple modalities simultaneously. This sophisticated analytical pipeline, combining speech analysis, semantic modeling, pose detection, and facial expression recognition, provides new capabilities for studying social interactions that extend far beyond what traditional observational coding could achieve.</p>
<p>The specific findings about hierarchical information flow patterns across different timescales are particularly valuable and would not have been possible without this sophisticated analytical approach. The discovery that mothers reduce low-level sensory input when infants focus on objects, while increases in object naming and information rate associate with sustained attention, provides new empirical insights into how social learning unfolds in naturalistic settings. The temporal dynamics analyses reveal interesting patterns of behavioral coordination that extend our understanding of how caregivers adaptively modify their responses to support infant attention across multiple communicative channels simultaneously.</p>
<p>The scale of data collection and the comprehensive multi-modal approach are impressive, opening up new possibilities for understanding social learning processes. The methodological innovations demonstrate how modern computational tools can be systematically integrated to reveal new quantitative aspects of well-established developmental phenomena. The computational features developed for this study represent innovative applications of information theory and computer vision to developmental research.</p>
<p>Weaknesses:</p>
<p>Several major limitations affect the reliability and interpretability of the findings. The sample sizes of 33-34 dyads per age group are relatively modest for the complexity of analyses performed, which include eight different features examined across various time lags with extensive statistical comparisons. The study lacks adequate power analysis to demonstrate whether these sample sizes are sufficient to detect meaningful effect sizes, which is particularly concerning given the multiple comparison burden inherent in this type of multi-modal, multi-timescale analysis.</p>
<p>The statistical framework presents several concerns that limit confidence in the findings. Inter-rater reliability for gaze coding shows substantial but not excellent agreement (κ = 0.628), with only 22% of the data undergoing double coding. Given that gaze coding forms the foundation for all subsequent analyses of joint attention and information flow, this reliability level may systematically influence findings. The multiple comparison correction strategies vary inconsistently across different analyses, with some using FDR correction and others treating lower-level and higher-level features separately. Additionally, object naming analyses employed one-sided tests (p&lt;0.05) while others used two-sided tests (p&lt;0.025) without clear theoretical or methodological justification for these differences.</p>
<p>The validation of AI tools in the specific context of mother-infant interactions is insufficient and represents a critical limitation. The performance characteristics of Whisper with infant-directed speech, the precision of MMPose for detecting facial landmarks in young children, and the accuracy of facial expression recognition tools in infant contexts are not adequately validated for this population. These sophisticated tools may not perform optimally in the specific context of mother-infant interactions, where speech patterns, facial expressions, and body movements may differ substantially from their training data.</p>
<p>The theoretical positioning requires substantial refinement to better acknowledge the extensive existing literature. The authors are working within a well-established theoretical framework that has long recognized social learning as an active, bidirectional process. The joint attention literature, beginning with foundational work by Bruner (1983) and continuing through contemporary theories of social cognition by researchers like Tomasello (1995), has emphasized the communicative and adaptive nature of attentional processes. The scaffolding literature, including seminal work by Wood, Bruner, and Ross (1976), has demonstrated how parents adjust their support based on children's developing competencies. Moreover, there is a substantial body of micro-analytic research that has employed sophisticated quantitative methods to study social interactions, including work by Stern (1985) on microsecond-level interactions and research using time-series methods to examine dyadic coordination patterns.</p>
<p>The cross-correlation analyses have inherent limitations for causal inference that are not adequately acknowledged. The interpretation of temporal correlation patterns in terms of directional influence requires more cautious consideration, as observational data have fundamental constraints for establishing causality. The ecological validity is also questionable due to the laboratory tabletop interaction paradigm and the sample's demographic homogeneity, consisting primarily of white, highly educated, high-income mothers.</p>
</body>
</sub-article>
</article>