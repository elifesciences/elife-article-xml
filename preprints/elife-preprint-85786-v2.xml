<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">85786</article-id>
<article-id pub-id-type="doi">10.7554/eLife.85786</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.85786.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Pynapple, a toolbox for data analysis in neuroscience</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Viejo</surname>
<given-names>Guillaume</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Levenstein</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Carrasco</surname>
<given-names>Sofia Skromne</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5813-3218</contrib-id>
<name>
<surname>Mehrotra</surname>
<given-names>Dhruv</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mahallati</surname>
<given-names>Sara</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Vite</surname>
<given-names>Gilberto R</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Denny</surname>
<given-names>Henry</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sjulson</surname>
<given-names>Lucas</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Battaglia</surname>
<given-names>Francesco P</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Peyrache</surname>
<given-names>Adrien</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Montreal Neurological Institute and Hospital, McGill University</institution>, Montreal, QC, <country>Canada</country></aff>
<aff id="a2"><label>2</label><institution>MILA – Quebec IA Institute</institution></aff>
<aff id="a3"><label>3</label><institution>Departments of Psychiatry and Neuroscience, Albert Einstein College of Medicine</institution>, Bronx, NY</aff>
<aff id="a4"><label>4</label><institution>Donders Institute for Brain, Cognition and Behaviour, Radboud University</institution>, 6525AJ Nijmegen, <country>The Netherlands</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Kemere</surname>
<given-names>Caleb</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Rice University</institution>
</institution-wrap>
<city>Houston</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>Materials &amp; Correspondence</bold> All correspondence should be directed to Adrien Peyrache at <email>adrien.peyrache@mcgill.ca</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-05-17">
<day>17</day>
<month>05</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2023-08-25">
<day>25</day>
<month>08</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP85786</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-01-24">
<day>24</day>
<month>01</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2022-12-07">
<day>07</day>
<month>12</month>
<year>2022</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.12.06.519376"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-05-17">
<day>17</day>
<month>05</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.85786.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.85786.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.85786.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.85786.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.85786.1.sa0">Author Response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Viejo et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Viejo et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-85786-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Datasets collected in neuroscientific studies are of ever-growing complexity, often combining high dimensional time series data from multiple data acquisition modalities. Handling and manipulating these various data streams in an adequate programming environment is crucial to ensure reliable analysis, and to facilitate sharing of reproducible analysis pipelines. Here, we present Pynapple, the PYthon Neural Analysis Package, a lightweight python package designed to process a broad range of time-resolved data in systems neuroscience. The core feature of this package is a small number of versatile objects that support the manipulation of any data streams and task parameters. The package includes a set of methods to read common data formats and allows users to easily write their own. The resulting code is easy to read and write, avoids low-level data processing and other error-prone steps, and is open source. Libraries for higher-level analyses are developed within the Pynapple framework but are contained within in a collaborative repository of specialized and continuously updated analysis routines. This provides flexibility while ensuring long-term stability of the core package. In conclusion, Pynapple provides a common framework for data analysis in neuroscience.</p>
</abstract>
<abstract>
<title>Highlights</title>
<list list-type="bullet">
<list-item><p>An open-source framework for data analysis in systems neuroscience.</p></list-item>
<list-item><p>Easy-to-use object-oriented programming for data manipulation.</p></list-item>
<list-item><p>A lightweight and standalone package ensuring long-term backward compatibility.</p></list-item>
</list>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Text has been revised for clarification.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The increasing size of datasets across scientific disciplines has led to the development of specific tools to store <sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup>, analyze <sup><xref ref-type="bibr" rid="c3">3</xref></sup>, and visualize <sup><xref ref-type="bibr" rid="c4">4</xref></sup> them. While various programming environments such as Matlab and R have long been commonly used in data science, Python has progressively become one of the most popular programming languages <sup><xref ref-type="bibr" rid="c5">5</xref></sup>. This is due to its open nature, large community-driven development, and versatility of usage. As with virtually all other scientific fields, neuroscience faced the challenges of handling and analyzing large datasets by rapidly developing a wide range of specialized tools to deal with each of these types of data <sup><xref ref-type="bibr" rid="c6">6</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup> and corresponding analyses.</p>
<p>In systems neuroscience, calcium imaging and high-density electrophysiology make it possible to simultaneously monitor the activity of an increasingly large number of neurons <sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c13">13</xref></sup>. Often, this is combined with simultaneous behavioral recordings. As in all other fields, this has required the development of specific pipelines to process <sup><xref ref-type="bibr" rid="c14">14</xref>–<xref ref-type="bibr" rid="c25">25</xref></sup> and store <sup><xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup> the data. Despite this rapid progress, data analysis often relies on custom made, lab-specific code, which is susceptible to error and can be difficult to compare across research groups. While several toolboxes are available to perform neuronal data analysis <sup><xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c28">28</xref>–<xref ref-type="bibr" rid="c30">30</xref></sup> (see ref. <sup><xref ref-type="bibr" rid="c31">31</xref></sup> for review), most of these programs focus on producing high-level analysis from specified types of data, and do not offer the versatility required for rapidly-changing analytical methods and experimental methods. Users can decide to use low-level data manipulation packages such as Pandas, but in that case, the learning curve can be steep for users with low, if any, computational background.</p>
<p>The key challenge for scientific code is balancing the need for flexibility and stability. This is especially true of science because results should be reproducible (between labs, between the past and the future, and between different experimental setups) while keeping up with rapidly changing requirements (e.g., due to new kinds of data, theories, and analysis methods). To meet these needs, we designed Pynapple, a general toolbox for data analysis in systems Neuroscience with a few principles in mind.</p>
<p>The first property of such a toolbox is that it should be object-oriented, organizing software around data. This makes the programming environment very efficient for data analysis, particularly in systems neuroscience where data streams can be of very different types. For example, to compute the rate of an event, one can write a function that takes an array of event times and divides the number of elements by the time between the first to the last event. However, this approach neglects to consider that the appropriate epoch in which to calculate the rate could start earlier, or end later, than the first or last event. Addressing these concerns requires another argument, which defines the boundaries of the epoch on which the rate should be computed. Overall, this approach is error prone. The epoch boundaries and event times must be stored in the same time unit and with the same reference (i.e., simultaneous time 0) and the rate function itself can be erroneously called with arrays storing another type of data. In contrast, an object which is specifically designed to represent a series of event times can ameliorate these concerns. For example, it can be created from a specific data loader that ensures proper definition of time units and support epochs (i.e., true beginning and end of the observation time). It will then be immune to the arithmetic operations that can change the values of a generic array (for example an addition that is misplaced in the code). Further, the object can be endowed with a rate property that is specifically written for this object, reducing the odds of a coding error. While this approach may discourage users who are not familiar with this type of coding, the benefit far exceeds the effort of learning object-oriented programming, especially if the naming of the methods and properties is explicit.</p>
<p>Another property of an efficient toolbox is that a small number of objects could virtually represents all possible data streams in neuroscience, instead of objects made for specific physiological processes (e.g. spike trains). This ensures that the same code can be used for various datasets and eliminates the need of adapting the structure of the package to handle rare or yet-to-be-developed data types. Then, these objects should then be able to interact via a small number of basic and foundational operations, which are sufficient for most analyses. This allows users to quickly write new code for new use-cases, and easily understand and adapt code written by others, as the same methods can be used for any kind of data.</p>
<p>The toolbox should be able to load common data storage types, and the flexibility to create loaders for future and custom/lab-specific data. It should also support the development of yet-unknown, lab-specific, and specialized analysis methods. In other words, the customization of the package to adapt to any dataset should happen at the input stage and the development of high-level analytical methods should take place outside the core package. The properties listed above ensure the long-term stability of a toolbox, a crucial aspect for maintaining the code repository. Toolboxes built around these principles will be maximally flexible and will have the most general application.</p>
<p>In this paper we introduce the Python Neural Analysis Package (Pynapple), designed with these axioms in mind. The core of Pynapple is five versatile timeseries objects, whose methods make it possible to intuitively manipulate and analyze the data. We show how Pynapple can be used with most raw neuroscience data types to produce the most common analyses used in contemporary neuroscience. Additionally, we introduce Pynacollada, a collaborative repository for higher level analyses built from the basic functionality provided by Pynapple. A complete neuroscience data analysis pipeline using a common language supports open and reproducible code. As all users are also invited to contribute to the Pynapple ecosystem, this framework also provides a foundation upon which novel analyses can be shared and collectively built by the neuroscience community.</p>
<sec id="s1a">
<title>Core features of Pynapple</title>
<p>At its core, Pynapple is object-oriented. Because objects are designed to be self-contained and interact with each other through well-defined methods, users are less likely to make errors when using them. This is because objects can enforce their own internal consistency, reducing the chances of data inconsistencies or unexpected behavior. Overall, object-oriented programming is a powerful tool for managing complexity and reducing errors in scientific programming. Pynapple is built around only five objects that are divided into three categories: two objects represent event timestamps (one or several), two represent time-varying data (one or several time series at the same sampling times), and one represents time epochs. Raw or pre-processed data are loaded into these objects in the coding environment (<bold><xref rid="fig1" ref-type="fig">Fig. 1</xref></bold>). The data loaders ensure that all loaded objects have the same time base. Hence, once objects are constructed, the user does not have to remember properties of the data such as the sampling frequency or alignment of data indices to clock time. Then, these objects can be manipulated with their own methods (i.e., object-specific functions). A large majority of data manipulations needed for most users can be achieved with a small number of methods. From there, Pynapple offer some foundational analyses, such as cross-correlation of event times. On top of this, the user may write analytical code that is project specific.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1</label>
<caption><title>Data analysis with the Pynapple package.</title>
<p><italic>Left</italic>, any type of input data can be loaded in a small number of core objects. For example (from top to bottom): intracellular recordings in slice during which current is injected and drug is applied to the bath solution; extracellular recordings in freely moving mice whose position is video-tracked; calcium imaging in head-fixed mice during presentation of different visual stimuli and delivery of precisely timed rewards; extracellular recordings in non-human primates during the execution of cognitive tasks. <italic>Middle</italic>, object-specific methods allow the user to perform a wide variety of basic operations and to manipulate the data manipulations. <italic>Right</italic>, at a higher level, the package contains a set of foundational analysis methods such as (from top to bottom) peri-event alignment of the data (top), 1- and 2-D tuning curves, 1- and 2-D decoding; auto- and cross-correlation of event times (e.g., action potentials). These methods depend only on a few, commonly used, external packages.</p></caption>
<graphic xlink:href="519376v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The most basic objects are timestamps (<italic>Ts</italic>), which are typically used for any discrete events, for example spike or lick times. The timestamped data (<italic>Tsd</italic>) object holds timestamps and associated data associated with each timestamp. For example, this object is used to represent an animal’s position in its environment, electroencephalogram data, or average calcium fluorescence as a function of time. Two objects were designed to represent groups of <italic>Ts</italic> and <italic>Tsd</italic>, namely <italic>TsGroup</italic> and <italic>TsdFrame</italic>. The main difference between the two objects is that <italic>TsdFrame</italic> has common timestamps for all the data (and therefore, all data have the same number of samples). <italic>TsGroup</italic> is more generic as each element has its own timestamps. These objects are typically used for ensembles of simultaneously recorded spike trains (<italic>TsGroup</italic>) or simultaneously acquired calcium fluorescence (<italic>TsdFrame</italic>). They are useful when operations need to be performed on a common time basis, for example binning multiple spike trains. Note however that they can be used for many other data types, for example the position of the animal (<italic>TsdFrame</italic>). Last, <italic>IntervalSet</italic> objects represent time epochs, for example the start and end times of intervals in which the animal is running.</p>
<p>Pynapple is built with objects from the Pandas library <sup><xref ref-type="bibr" rid="c5">5</xref></sup>. As such, Pynapple objects inherit the long-term consistency of the code and the computational flexibility from this widely used package. Specifically, a <italic>Tsd</italic> object is an extension of (or “inherits” in object-orienting programming) Pandas <italic>Series</italic> object and <italic>TsdFrame</italic> of Pandas <italic>DataFrame</italic> object. A <italic>TsGroup</italic> is a child of <italic>UserDict</italic>, a built-in python object for inheriting dictionaries. Finally, <italic>IntervalSet</italic> inherits Pandas <italic>DataFrame</italic>. Timestamps are by default in units of seconds but can be readily converted to other time units using the <monospace>as_units</monospace> method in any object.</p>
<p>Pynapple objects have a limited number of core methods (<bold><xref rid="fig2" ref-type="fig">Fig. 2A</xref></bold>), which form the foundation of further operations. These operations provide a general framework by which users can manipulate the timestamps and their corresponding values as needed for analysis. For example, the time series objects have built-in methods: <monospace><italic>value_from</italic></monospace>, which gets the value from one time series object at the (closest) timestamps from another; <monospace><italic>restrict</italic></monospace>, which “restricts” a time series object; extracting only the data contained within a set of time intervals defined by an <italic>IntervalSet</italic> object; <monospace><italic>count</italic></monospace>, which counts the number of timestamps from a time series object in windows of a given bin size; <monospace><italic>threshold</italic></monospace>, which applies a threshold to the data within a <italic>Ts</italic> or <italic>Tsd</italic> object and returns a <italic>Tsd</italic> containing the data above or below the threshold. All operations can be restricted to a given epoch, specified by an <italic>IntervalSet</italic>.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2</label>
<caption><title>Core methods of the Pynapple objects.</title>
<p>A) Methods of Timestamps (<italic>Ts)</italic> and Timestamped Data (<italic>Tsd)</italic> objects. The same methods can be called for different objects, leading to qualitatively similar results. For example, <monospace>object.restrict(intervalset)</monospace> returns an object now defined on the intersection of its original time support and the input <italic>IntervalSet</italic>. Objects can be any of the timestamps and timestamped data objects. These methods can be called with only one argument, as shown here, since the default parameters are typically the same for most analyses. Yet the methods include additional arguments for more specific operations. B) Logical operations on pairs of <italic>IntervalSet</italic> objects to compute (from top to bottom) the intersection, union, and difference between epochs. These operations are commonly used to analyze data during specific epochs in a combinatorial manner, such as “exploration period AND running speed is above 5cm/s NOT left arm”. C) Methods of <italic>TsGroup</italic> objects. Each timestamp is associated by default with its occurrence rate. Additional custom metadata such as recording location can be added. These metadata can then be used to select and filter timestamps using <monospace>getby_category</monospace> for discrete labels, <monospace>getby_threshold</monospace> or <monospace>getby_intervals</monospace> for numerical values.</p></caption>
<graphic xlink:href="519376v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Furthermore, all objects have a <monospace>time_support</monospace> property, which keeps track of the time interval over which the data is valid. The time support is an <italic>IntervalSet</italic> object that is attached by default to <italic>Ts, Tsd, TsdFrame</italic> and <italic>TsGroup</italic> objects. This is a crucial property as, otherwise, it is impossible to know whether periods without data correspond to an epoch during which the underlying event was not observed or because this period has previously been excluded by a <monospace>restrict</monospace> method.</p>
<p>In addition to the ability to restrict methods of time series objects, the <italic>IntervalSet</italic> object has methods for logical operations on combinations of <italic>IntervalSets</italic>, all returning other <italic>IntervalSets</italic> (<bold><xref rid="fig2" ref-type="fig">Fig. 2B</xref></bold>): <monospace><italic>intersect</italic></monospace>, which returns the set intersection of two <italic>IntervalSet</italic> objects; <monospace><italic>union</italic></monospace>, which returns the set union of two <italic>IntervalSets</italic>; <monospace><italic>set_diff</italic></monospace>, which returns the set difference of two IntervalSet; <monospace><italic>drop_short_intervals</italic></monospace>, <monospace><italic>drop_long_intervals</italic></monospace>, which eliminate interval subsets that are shorter or longer than a desired duration; and <monospace><italic>merge_close_intervals</italic></monospace>, which merge intervals that are closer in time than a given duration.</p>
<p>Many experiments in neuroscience are based on trials, each associated with different conditions. <italic>IntervalSets</italic> are perfectly suited for this, as one <italic>IntervalSet</italic> can represent all start and end times of trials. The nature of each trial (e.g., left/right, correct/error) can be stored as a third column within the <italic>IntervalSet</italic> dataframe object. Thus, subsets of trials can be easily selected to restrict data of interest on the corresponding epochs. An alternative approach is to store different <italic>IntervalSets</italic> for different types of trials.</p>
<p>In addition to the ability to apply any methods of the <italic>Ts</italic> object to its members, <italic>TsGroup</italic> has a set of methods to calculate and store metadata about the elements of the group (<bold><xref rid="fig2" ref-type="fig">Fig. 2c</xref></bold>). For example, one can store and retrieve the anatomical structure from which a neuron was recorded, or the result from downstream analysis, perform operations on each element, and filter by various properties. These methods allow the user to, for example, calculate, store, and compare the properties of multiple neurons in a population. Additional methods for all objects are extensively documented in the documentation, and examples for usage are given in the tutorials.</p>
<p>While there are relatively few objects, and while each object has relatively few methods, they are the foundation of almost any analysis in systems neuroscience. However, if not implemented efficiently, they can be computationally intensive and if not implemented accurately, they are highly susceptible to user error. The implementation of core features in Pynapple addresses the concerns of efficiency and accuracy. Crucially, all units are indexed by seconds across the entire package, which limits the need for users to account for indexing and alignment between different streams of data at different sampling rates. For example, a user can simply use <monospace>spikes.value_from(position)</monospace> to get the animal’s position at each spike time, rather than costly and error-prone routines in which a user needs to identify matching indices for the corresponding timestamps across arrays containing spikes and behavioral information. Another common issue in data analysis is to analyze two timeseries that are not recorded at the same sampling rate. Once data are loaded in the same time base (i.e., the same time 0), they can keep their original sampling times. Using the function <monospace>value_from</monospace> from one object with the other object as argument will provide two time series with the same number of samples and the same sampling times, which will simplify further analyses. However, this means it is essential that all objects are loaded in the same time base for these methods to function correctly. Pynapple anticipates this by providing a customizable data loader, ensuring time bases are always loaded correctly.</p>
</sec>
<sec id="s1b">
<title>Importing data from common and custom pipelines</title>
<p>The proliferation of experimental methods has come with a proliferation of data formats, as well as the need to rapidly develop new formats that meet new experimental needs. Usually, these data formats are dependent on the software that was used to preprocess the raw data, making them difficult to load for further analysis. Additionally, an experimental setup can generate multiple streams of data that are saved within multiple files of various types. Thus, a universal toolbox should be able to load popular data formats into a common framework and offer the user the ability to write functions to load their own data types.</p>
<p>To ease the process of loading and synchronizing data from various streams, Pynapple includes an I/O layer that allows the user to load multiple types of datasets and write them to a common format for further analysis and sharing. The primary way by which a user interacts with the I/O layer is an object that represents an experimental session, with the properties of the object being the various time series. This I/O object is created by calling the function <monospace>load_session</monospace>, which will load all data associated with that session (<bold><xref rid="fig3" ref-type="fig">Fig. 3A</xref></bold>). For example, calling <monospace>load_session</monospace> for an <italic>in vivo</italic> electrophysiology recording would return an object called <monospace>data</monospace>, which will have properties <monospace>data.spikes</monospace>, <monospace>data.position</monospace>, and <monospace>data.epochs</monospace> which respectively store a <italic>TsGroup</italic>, containing the spike times, a <italic>TsdFrame</italic> containing the position of the animal, and an <italic>IntervalSet</italic> containing the times when the animal is on the track. With this object-oriented I/O method, the user can interact with the various data streams associated with a given experimental session and load multiple sessions at once without the risk of mixing data as each time series is attached to only one I/O object.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3</label>
<caption><title>Built-in and customizable loading function for Pynapple.</title>
<p>A) Data is originally organized as separate files in a folder. A built-in or custom-made load_session function is called to load the data into a Data class. B) Data can be loaded through a customizable GUI to enter all relevant information regarding the experiment, for example animal strain, among others. The main epochs of the recording (e.g., behavioral states, stimuli category, etc.) can be loaded from standard tabular data files (such as CSV). Behavioral tracking data extracted from various common systems and saved as a CSV file can also be loaded. C) Pynapple offers various built-in loaders for commonly used data formats, as well as a template to easily design a customizable loader to adapt to any other format or specific task design.</p></caption>
<graphic xlink:href="519376v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Data synchronization is the crux of any analysis pipeline. The <monospace>load_session</monospace> function is thus a crucial step in using the package. For unsupported data types, it is the responsibility of the users to design the preprocessing scripts that align the data streams in the same absolute time base. The data loading and synchronizing functions already included in the package for supported data types is a good starting point for any user writing a custom loading function (details this process are provided later).</p>
<p>While data types are usually specific to a recording modality (i.e., calcium imaging and electrophysiology), there are several pieces of metadata that are common to many experiments, such as the strain of the animal, age, sex, and name of the experimenter. When loading a session for the first time, the I/O process starts with a graphical user interface (GUI) in which the user can quickly and easily input the general information as well as any session epoch and behavioral tracking data (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>). This information is saved in a <italic>BaseLoader</italic> class.</p>
<p>General session information is common across experimental sessions, however specialized data streams are usually specific to recording modalities. To cover the variety of preprocessing analysis pipelines currently used in systems neuroscience, the Pynapple I/O can load data formats from popular preprocessing pipelines (e.g., CNMF-E, Phy, NeuroSuite or Suite2P). This is implemented via a set of specialized object subclasses of the BaseLoader class, avoiding the need to redefine I/O operations in each subclass. This is a core aspect of object-oriented programing, and it means that these specialized I/O classes have all the methods and properties of the parent <italic>BaseLoader</italic> objects. This ensures compatibility across various loading functions. However, once generated, these specialized I/O classes are unique and independent from each other, ensuring long-term backward compatibility. For instance, if the spike sorting tool Phy changes its output in the future, this would not affect the “Neurosuite” IO class as they are independent of each other. This allows each tool to be updated or modified independently, without requiring changes to the other tool or the overall data format.</p>
<p>Like the <italic>BaseLoader</italic> class, a specialized GUI for electrophysiology and calcium imaging is provided, with relevant metadata fields, for example electrode position in electrophysiology and type of fluorescence indicator in calcium imaging (<bold><xref rid="fig3" ref-type="fig">Fig. 3B</xref></bold>).</p>
<p>To avoid repeating the process of inputting session information and synchronization of multiple data streams, Pynapple saves all synchronized data into a unique file and can accommodate a wide range of neuroscientific data types. Recently, Neurodata Without Borders (NWB) <sup><xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup> has emerged as a flexible data format used for public data sharing and large databases such as those collected by the Allen Institute. Thus, we chose to use the NWB format for fast and universal data loading and saving with Pynapple. The <italic>BaseLoader</italic> is responsible for initializing the NWB file within the session folder (i.e. it creates a new NWB file if none is present) (<bold><xref rid="fig3" ref-type="fig">Fig. 3C</xref></bold>). Converting user’s data to NWB format encourages standardization and can facilitate sharing both data and analysis pipelines written with Pynapple.</p>
<p>Many other preprocessing pipelines exist and can often be unique to a lab or even to an individual project. To accommodate present and future needs for these specific pipelines, the documentation of Pynapple provides an easy-to-follow recipe for creating a custom I/O class that inherits the <italic>BaseLoader</italic> and can interact with a pre-existing NWB file. There are multiple benefits of the inheritance approach of data loading classes within the I/O layer of Pynapple. First, future development of new I/O classes will not affect the core and processing layers of Pynapple. This ensures long-term stability of the package. Second, users can develop their own custom I/O using available template classes. Pynapple already includes several of such templates and we expect this collection to grow in the future. Third, users can still use Pynapple without using the I/O layer of Pynapple. Last, in order to apply previous analyses, or analyses developed in another lab, to new data or data types a user only needs to develop a new I/O class for their data. This will import the data to the common Pynapple core from which the same analysis pipeline can be used.</p>
</sec>
<sec id="s1c">
<title>Foundational data processing</title>
<p>The basic methods that manipulate the core objects in Pynapple allow users to perform common, but powerful, neuroscience analyses (<bold><xref rid="fig2" ref-type="fig">Fig. 2</xref></bold>). These analyses are easy to use because they describe the relationships between time series objects, while requiring the fewest number of parameters to be set by the user. This minimizes complexity, while maximizing generalizability. The operations in Pynapple can recreate neuroscience analyses from a broad number of subdisciplines. These analyses form the foundation of neuroscience data analysis in Pynapple. To illustrate the versatility of Pynapple and how it can be used, we reanalyzed five openly available datasets.</p>
<p>The first foundational analysis is computing neural tuning curves. Tuning curves relate specific stimuli to the firing rate of neurons. To this end, Pynapple computes the firing rate of a neuron (or any other timestamped data) during each epoch in an <italic>IntervalSet</italic> object, for example for discrete conditions such as “ON/OFF’’ stimuli. Tuning curves can also be computed with respect to a continuous feature. Once computed, Pynapple is able to use tuning curves from a population of neurons to decode stimuli using a Bayesian decoder <sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup> (<bold><xref rid="fig4" ref-type="fig">Fig. 4A</xref></bold>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4</label>
<caption><title>Examples of foundational analysis across various electrophysiological datasets using Pynapple.</title>
<p>A) Analysis of an ensemble of head-direction cells. From left to right: data were collected in a freely moving mouse randomly foraging for food; all data are restricted to the wake epoch (i.e., during exploration); the tuning curve of two neurons relative to the animal’s head-direction; animal’s head-direction is decoded from the neuronal ensemble. Data from refs. <sup><xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup> B) Analysis of V1 neurons during visual stimulation. From left to right: the mouse was recorded while being head-fixed and presented with drifting gratings; spikes, stimulation and epochs are shown; example tuning curves of two V1 neurons, showing their firing rates for different grating orientations; example cross-correlation between two V1 neurons, showing an oscillatory co-modulation at about 5Hz during visual stimulation. Data from ref. <sup><xref ref-type="bibr" rid="c36">36</xref></sup>C) Analysis of medial temporal lobe neurons in human epileptic subjects. From left to right: subjects, implanted with hybrid deep electrodes, were shown a series of short clips; raster plot of a single neuron around continuous movie shot trials (green) and hard boundary trials, which are transitions between two unrelated movies (orange); peri-event neuronal firing rate for both trial types. Data from ref. <sup><xref ref-type="bibr" rid="c37">37</xref></sup> Images in panels b and c are from ref. <sup><xref ref-type="bibr" rid="c38">38</xref></sup> The analysis code used to generate this figure can be found on the Pynapple Organization Github repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/pynapple-org/pynapple-paper-2023">https://github.com/pynapple-org/pynapple-paper-2023</ext-link></p></caption>
<graphic xlink:href="519376v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The second foundational analysis is computing auto- and cross-correlograms of event data. In the most abstract sense, these correlograms show the relationship between previous and future events and a current event at time 0. In Pynapple, cross-correlograms can be generated for any two series of events by computing the event rate for each time bin of a target time series relative to each event of a reference time series. Commonly, this is used to examine the likelihood of an action potential in a neuron relating to a previous or future action potential in the same neuron (auto-correlogram) or in another neuron (cross-correlogram) (<bold><xref rid="fig4" ref-type="fig">Fig. 4B</xref></bold>). However, Pynapple does not limit this function to spiking data and correlograms may be performed on any event-based data.</p>
<p>The third and final foundational analysis is peri-event alignment. This involves aligning a specified window from <italic>Ts</italic>/<italic>Tsd</italic>/<italic>TsGroup</italic> data to a specific <italic>Ts</italic>, known as “TimeStamp Reference’’. This allows users to align data to specific points in time, and measure changes in rates around this specified time point (<bold><xref rid="fig4" ref-type="fig">Fig. 4C</xref></bold>). One example where this function is useful is aligning neuronal spikes to specific stimuli, such as optogenetic illumination, presentation of a tone, or electrical stimulation.</p>
<p>Some of the analyses presented so far are designed for spikes (and discrete events in general) and cannot be applied for continuous traces such as calcium imaging data. Pynapple includes specialized functions that can compute the tuning of a continuous value with respect to a feature, as shown for the modulation of fluorescence in calcium imaging with respect to the speed of the animal (<bold><xref rid="fig5" ref-type="fig">Fig. 5A</xref></bold>) or of the position of a vertical bar on a screen in the fly’s ellipsoid body (<bold><xref rid="fig5" ref-type="fig">Fig. 5B</xref></bold>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5</label>
<caption><title>Examples of foundational analysis across various calcium imaging datasets using Pynapple.</title>
<p>A) Analysis of a V1 neuron during visual stimulation. From left to right: the mouse was recorded while being head-fixed on a running wheel and presented with natural scene movies; fluorescence traces from a pre-processed region of interest and running speed are loaded; continuous tuning curve is directly obtained from fluorescence and speed. Data from ref. <sup><xref ref-type="bibr" rid="c39">39</xref></sup>. Image is from ref. <sup><xref ref-type="bibr" rid="c38">38</xref></sup>B) Analysis of neuronal activity in the fly central complex. From left to right: a <italic>Drosophila melanogaster</italic> is tethered to a calcium imaging setup while the position of a vertical bar is in closed loop with the fly’s movements on a ball; calcium activity in the ellipsoid body is divided into 16 wedges; example fluorescence trace and direction of the fly. Tuning curves are obtained as in <bold>A</bold>, with the direction as feature. Data from ref. <sup><xref ref-type="bibr" rid="c40">40</xref></sup></p></caption>
<graphic xlink:href="519376v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The examples shown in <xref rid="fig4" ref-type="fig">Figures 4</xref> and <xref rid="fig5" ref-type="fig">5</xref> show how these core analyses are useful for rapid data screening with just a few lines of code in a Jupyter notebook, for example. Overall, these foundational functions form the building blocks of most other analyses in systems neuroscience.</p>
<p>Importantly, they are for the most part built-in and only depend on a few widely used external packages. This ensures that the package can be used in a near stand-alone fashion, without relying on packages that are at risk of not being maintained or of not being compatible in the near future. All other developments of analysis pipelines take place outside Pynapple, ensuring the core package is only updated rarely and remains lightweight.</p>
</sec>
<sec id="s1d">
<title>Pynacollada: a collaborative library for specialized and continuously updated data analyses</title>
<p>Pynapple is designed to be stable in the foreseeable future and its core functionality is not meant to be modified. However, actual data analysis usually requires more than the available core functions. This type of data analysis is “fluid”, constantly updated by new software developments and theoretical work. Furthermore, this kind of development is collaborative in nature and the supervision of such projects is less sensitive than that of a stable package. To balance the needs for stability and flexibility, high-level functions were separated from Pynapple and included instead in Pynacollada: the Pynapple Collaborative repository hosted on GitHub.</p>
<p>Complex analyses are added to Pynacollada in the form of libraries. Each library developed for Pynacollada takes the form of a Jupyter notebook (or python scripts) which guides the user through the analysis step-by-step. As such, libraries built for Pynacollada should provide training, promote good practice in programming, and allow users to easily adapt code to their own project. Examples of complex analyses currently handled by Pynacollada are outlined below (<bold><xref rid="fig6" ref-type="fig">Fig. 6</xref></bold>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig. 6</label>
<caption><title>The Pynapple collaborative data analysis repository (Pynacollada) environment.</title>
<p>Unlike Pynapple, which is designed for long-term stability, Pynacollada is a repository of project-oriented libraries. This way, the community can collaborate on constantly evolving data analysis code without affecting the functionality of the core pipeline. Each project should include a script that can be called for specific functions and/or Jupyter notebooks to showcase the use of the code, as well as proper documentation. Pynacollada already includes several libraries and/or tutorials, including but not limited to: (1) a tutorial on manifold analysis, covering how to project neuronal data on low-dimensional subspace using various machine learning techniques; (2) a library for oscillation detection in local field potentials, which takes raw broadband traces as inputs and outputs <italic>IntervalSet</italic> objects corresponding to the start and end times of oscillation bouts.</p></caption>
<graphic xlink:href="519376v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Recent advances in the application of manifold theory to neural data analysis have allowed neuroscientists to project high-dimensional data into three or fewer dimensions <sup><xref ref-type="bibr" rid="c41">41</xref>–<xref ref-type="bibr" rid="c43">43</xref></sup>. The structure of these projections reflects the structure of these higher dimensional processes, allowing us to infer the information encoded by the population. The Pynacollada “neural_manifold” library contains a Jupyter notebook that provides a step-by-step process for recreating a ring manifold using spiking data recorded from a population of head-direction neurons (<bold><xref rid="fig6" ref-type="fig">Fig. 6</xref></bold>). This code can be adapted by the end-user for analysis of their own data by simply importing their own data and refactoring the parameters to suit their needs.</p>
<p>A second complex analysis handled by Pynacollada is sharp wave-ripple (SWR) detection. Detecting oscillatory events is a routine procedure in electrophysiology, yet usually depends on many arbitrary choices of parameters. In this case, the Jupyter notebook showcases an example of detecting SWRs, a well-characterized oscillation of the hippocampus (<bold><xref rid="fig6" ref-type="fig">Fig. 6</xref></bold>).</p>
<p>In addition, Pynacollada currently includes libraries for spike waveform processing, EEG analysis, and video tracking, among others. We invite the community to contribute to this repository by improving current libraries or upload new ones. For new libraries, only rapid screening and tests will be performed, but the code will not go through the kind of validation that is in place for Pynapple as an external library will never affect the functioning of the core package. The documentation describes what is expected in each library to simplify readability, sharing, and maintenance and, overall, how libraries should conform to Pynacollada standards. We hope this will be broadly adopted by the community, allowing researchers across labs to easily share their code.</p>
</sec>
</sec>
<sec id="s2">
<title>Discussion</title>
<p>Here we introduced Pynapple, a lightweight and open-source python toolbox for neural data analysis. The goal of this package is to offer a versatile set of tools to study typical neurophysiological and behavioral data, specifically time series (e.g., spike times, behavioral events, and continuous time series) and time intervals (e.g., trials and brain states). It also provides users with generic functions for neuroscience analyses such as tuning curves and cross-correlograms. Finally, Pynapple was designed to rely on a minimum number of dependencies, which are themselves very common and thus highly stable. As such, accessibility is the guiding axiom of Pynapple.</p>
<p>The path from data collection to reliable results involves a number of critical steps: exploratory data analysis, development of an analysis pipeline that can involve custom-made developed processing steps, and ideally the use of that pipeline and others to replicate the results. Pynapple provides a platform for these steps.</p>
<p>The design of Pynapple is centered around the manipulation of simple, abstract objects that are common to most neurophysiological and behavioral datasets. The core of Pynapple is built around five objects: Timestamps (<italic>Ts</italic>) and group of Timestamps (<italic>TsGroup</italic>), Time Series Data (<italic>Tsd</italic>) and ensemble of co-registered Tsd (<italic>TsdFrame</italic>), as well as <italic>IntervalSets</italic>. These objects can be manipulated with properties that are, in most cases, common to all objects. Building around these fundamental objects and properties means Pynapple is highly flexible and able to handle most neurophysiological and behavioral datasets, making it accessible to most systems neuroscientists.</p>
<p>Pynapple was developed to be lightweight, stable, and simple. As simplicity does not necessarily imply backward compatibility (i.e. long-term stability of the code), Pynapple main objects and their properties will remain the same for the foreseeable future, even if the code in the backend may eventually change (e.g. not relying on Pandas in future version). The small number of external dependencies also decrease the need to adapt the code to new versions of external packages. This approach favors long-term backward compatibility.</p>
<p>Data in neuroscience vary widely in their structure, size, and need for pre-processing. Pynapple is built around the idea that raw data have already been pre-processed (for example, spike sorting and detection of ROIs). According to the FAIR principles, pre-processed data should interoperate across different analysis pipelines. Pynapple makes this interoperability possible as, once the data are loaded in the Pynapple framework, the same code can be used to analyze different datasets. Specifically, to simplify analysis for users, Pynapple offers simple wrappers for loading data with popular preprocessing pipelines. However, to be fully accessible, it is not sufficient for a package’s core operations to be able to process all data types in theory. Data produced in neuroscience has a wide variety of file types, which are often only loaded by specific analysis software. Data is also largely experiment-specific. To unify these disparate file types and configurations, Pynapple’s data loader is customizable. In addition to being able to load current popular data formats, this customizable data-loader means emerging file formats may continue to be loaded in the future, without significant overhauls to the main package. This offers Pynapple long-term stability and means that Pynapple will continue to remain accessible in the foreseeable future. To note, Pynapple can be used without the I/O layer and independent of NWB for generic, on-the-fly analysis of data.</p>
<p>In further pursuit of accessibility, from these simple objects and properties, Pynapple has several built-in, foundational analyses that are common across the field of systems neuroscience. These foundational analyses include computing neural tuning curves, computing auto-/cross-correlograms, peri-event alignment, and performing Bayesian decoding. From these foundational analyses, higher order analyses can be developed. However, these higher order analyses are more prone to customization, thereby making them relatively more flexible. As such, higher order analyses are stored in the collaborative repository known as Pynacollada. This keeps the core Pynapple package stable, while allowing the user to integrate new advances in neurophysiological and behavioral analysis into their workflow.</p>
<p>Other software provide programming environments which deal with common neuroscientific data and an interface between stored data and analytical methods <sup><xref ref-type="bibr" rid="c10">10</xref></sup>. However, one problem that arises from this structure is that objects and data structures are rigidly defined, leading to a lack of versatility for new types of data or task design. In contrast, Pynapple offers a more flexible working environment and will remain accessible even as user requirements change.</p>
<p>While Pynapple expands accessibility to data analysis, it has some limitations inherent to its design. The first issue is that Pynapple is currently only available through Python. Thus, some transition is required for those primarily trained in other programming languages commonly used in neuroscience, including MATLAB and Julia. The design of the package around objects is a strength in many regards but could represent a challenge for users who are not accustomed to this programming approach. We have addressed this concern by providing users with detailed documentation, which includes a broad variety of examples. We will also keep on providing training opportunities for all future users. Last, Python code may run slower than similar code written in other languages. Pynapple is based on Pandas, whose methods are already highly optimized. Yet, current development is underway to improve computation speed and these developments are transparent for the users as they won’t change the organization of the package.</p>
<p>Soon, Pynapple will be part of an entire suite of plugin libraries that we are developing to further enhance Pynapple. To keep Pynapple robust and stable, we will develop these plugins as standalone packages. These external packages will include an automated datalogger for recapitulating analyses, an on-line visualizer for Pynapple objects, and a package for parallel computing in Pynapple. This will address the speed issue inherent to code written in Python by allowing multiple analyses to be performed simultaneously. These packages will begin to address the limitations of Pynapple we described previously, enhancing the long-term stability of Pynapple, and streamlining accessibility for its users.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="book"><string-name><surname>Folk</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Heber</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Koziol</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Pourmal</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Robinson</surname>, <given-names>D.</given-names></string-name> <chapter-title>An overview of the HDF5 technology suite and its applications</chapter-title>. <source>in Proceedings of the EDBT/ICDT 2011 Workshop on Array Databases</source> <fpage>36</fpage>–<lpage>47</lpage> (<publisher-name>Association for Computing Machinery</publisher-name>, <year>2011</year>). doi:<pub-id pub-id-type="doi">10.1145/1966895.1966900</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Wells</surname>, <given-names>D. C.</given-names></string-name> &amp; <string-name><surname>Greisen</surname>, <given-names>E. W.</given-names></string-name> <source>FITS - a Flexible Image Transport System</source>. <volume>445</volume> (<year>1979</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal> <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>J. Mach. Learn. Res</source>. <volume>12</volume>, <fpage>2825</fpage>– <lpage>2830</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>van der Maaten</surname>, <given-names>L. J. P.</given-names></string-name> &amp; <string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> <article-title>Visualizing High-Dimensional Data Using t-SNE</article-title>. <source>J. Mach. Learn. Res</source>. <volume>9</volume>, <fpage>2579</fpage>–<lpage>2605</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>McKinney</surname>, <given-names>W.</given-names></string-name> &amp; others. <article-title>pandas: a foundational Python library for data analysis and statistics</article-title>. <source>Python High Perform. Sci. Comput</source>. <volume>14</volume>, <fpage>1</fpage>–<lpage>9</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Abraham</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title>Machine learning for neuroimaging with scikit-learn</article-title>. <source>Front. Neuroinformatics</source> <volume>14</volume> (<year>2014</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Tadel</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Baillet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Mosher</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Leahy</surname>, <given-names>R. M.</given-names></string-name> <article-title>Brainstorm: A User-Friendly Application for MEG/EEG Analysis</article-title>. <source>Comput. Intell. Neurosci</source>. <volume>2011</volume>, <fpage>879716</fpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Schoffelen</surname>, <given-names>J.-M.</given-names></string-name> <article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source>Comput. Intell. Neurosci</source>. <volume>2011</volume>, <fpage>e156869</fpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Bokil</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Andrews</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kulkarni</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Mitra</surname>, <given-names>P. P.</given-names></string-name> <article-title>Chronux: A platform for analyzing neural signals</article-title>. <source>J. Neurosci. Methods</source> <volume>192</volume>, <fpage>146</fpage>–<lpage>151</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Garcia</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Neo: an object model for handling electrophysiology data in multiple formats</article-title>. <source>Front. Neuroinformatics</source> <volume>8</volume>, (<year>2014</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Freeman</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>Mapping brain activity at scale with cluster computing</article-title>. <source>Nat. Methods</source> <volume>11</volume>, <fpage>941</fpage>–<lpage>950</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Stevenson</surname>, <given-names>I. H.</given-names></string-name> &amp; <string-name><surname>Kording</surname>, <given-names>K. P.</given-names></string-name> <article-title>How advances in neural recording affect data analysis</article-title>. <source>Nat. Neurosci</source>. <volume>14</volume>, <fpage>139</fpage>–<lpage>142</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Urai</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Doiron</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Leifer</surname>, <given-names>A. M.</given-names></string-name> &amp; <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name> <article-title>Large-scale neural recordings call for new insights to link brain and behavior</article-title>. <source>Nat. Neurosci</source>. <volume>25</volume>, <fpage>11</fpage>–<lpage>19</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="other"><string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Kadir</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name> <article-title>Kilosort: realtime spike-sorting for extracellular electrophysiology with hundreds of channels</article-title>. <source>bioRxiv</source> <fpage>061481</fpage> (<year>2016</year>) doi:<pub-id pub-id-type="doi">10.1101/061481</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="web"><string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <source>Suite2p: beyond 10,000 neurons with standard two-photon microscopy. 061507 Preprint at</source> <pub-id pub-id-type="doi">10.1101/061507</pub-id> (<year>2017</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Hazan</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zugaro</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Buzsáki</surname>, <given-names>G.</given-names></string-name> <article-title>Klusters, NeuroScope, NDManager: A free software suite for neurophysiological data processing and visualization</article-title>. <source>J. Neurosci. Methods</source> <volume>155</volume>, <fpage>207</fpage>–<lpage>216</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Fee</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Mitra</surname>, <given-names>P. P.</given-names></string-name> &amp; <string-name><surname>Kleinfeld</surname>, <given-names>D.</given-names></string-name> <article-title>Automatic sorting of multiple unit neuronal signals in the presence of anisotropic and non-Gaussian variability</article-title>. <source>J. Neurosci. Methods</source> <volume>69</volume>, <fpage>175</fpage>– <lpage>188</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name>, <string-name><surname>Henze</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Csicsvari</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hirase</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Buzsáki</surname>, <given-names>G.</given-names></string-name> <article-title>Accuracy of tetrode spike separation as determined by simultaneous intracellular and extracellular measurements</article-title>. <source>J. Neurophysiol</source>. <volume>84</volume>, <fpage>401</fpage>–<lpage>414</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Yger</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title>A spike sorting toolbox for up to thousands of electrodes validated with ground truth recordings in vitro and in vivo</article-title>. <source>eLife</source> <volume>7</volume>, <fpage>e34518</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat. Neurosci</source>. <volume>21</volume>, <fpage>1281</fpage>–<lpage>1289</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title>Efficient and accurate extraction of in vivo calcium signals from microendoscopic video data</article-title>. <source>eLife</source> <volume>7</volume>, <fpage>e28728</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Mukamel</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Nimmerjahn</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Schnitzer</surname>, <given-names>M. J.</given-names></string-name> <article-title>Automated Analysis of Cellular Signals from Large-Scale Calcium Imaging Data</article-title>. <source>Neuron</source> <volume>63</volume>, <fpage>747</fpage>–<lpage>760</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Romano</surname>, <given-names>S. A.</given-names></string-name> <etal>et al.</etal> <article-title>An integrated calcium imaging processing toolbox for the analysis of neuronal population dynamics</article-title>. <source>PLOS Comput. Biol</source>. <volume>13</volume>, <fpage>e1005526</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Kaifosh</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Zaremba</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Danielson</surname>, <given-names>N. B.</given-names></string-name> &amp; <string-name><surname>Losonczy</surname>, <given-names>A.</given-names></string-name> <article-title>SIMA: Python software for analysis of dynamic fluorescence imaging data</article-title>. <source>Front. Neuroinformatics</source> <volume>8</volume>, (<year>2014</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Pnevmatikakis</surname>, <given-names>E. A.</given-names></string-name> &amp; <string-name><surname>Giovannucci</surname>, <given-names>A.</given-names></string-name> <article-title>NoRMCorre: An online algorithm for piecewise rigid motion correction of calcium imaging data</article-title>. <source>J. Neurosci. Methods</source> <volume>291</volume>, <fpage>83</fpage>–<lpage>94</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Teeters</surname>, <given-names>J. L.</given-names></string-name> <etal>et al.</etal> <article-title>Neurodata Without Borders: Creating a Common Data Format for Neurophysiology</article-title>. <source>Neuron</source> <volume>88</volume>, <fpage>629</fpage>–<lpage>634</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Rübel</surname>, <given-names>O.</given-names></string-name> <etal>et al.</etal> <article-title>The Neurodata Without Borders ecosystem for neurophysiological data science</article-title>. <source>eLife</source> <volume>11</volume>, <fpage>e78362</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Nasiotis</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> <article-title>Integrated open-source software for multiscale electrophysiology</article-title>. <source>Sci. Data</source> <volume>6</volume>, <fpage>231</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="other"><string-name><surname>Zugaro</surname>, <given-names>M.</given-names></string-name> <source>Freely Moving Animal (FMA) toolbox</source>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="other"><string-name><surname>Ackermann</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Chu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dutta</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Kemere</surname>, <given-names>C.</given-names></string-name> <source>Nelpy: Neuroelectrophysiology Object Model and Data Analysis in Python Nelpy: Neuroelectrophysiology Object Model and Data Analysis in Python</source>. (<year>2018</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Unakafova</surname>, <given-names>V. A.</given-names></string-name> &amp; <string-name><surname>Gail</surname>, <given-names>A.</given-names></string-name> <article-title>Comparing Open-Source Toolboxes for Processing and Analysis of Spike and Local Field Potentials Data</article-title>. <source>Front. Neuroinformatics</source> <volume>13</volume>, (<year>2019</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ginzburg</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>McNaughton</surname>, <given-names>B. L.</given-names></string-name> &amp; <string-name><surname>Sejnowski</surname>, <given-names>T. J.</given-names></string-name> <article-title>Interpreting Neuronal Population Activity by Reconstruction: Unified Framework With Application to Hippocampal Place Cells</article-title>. <source>J. Neurophysiol</source>. <volume>79</volume>, <fpage>1017</fpage>–<lpage>1044</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Brown</surname>, <given-names>E. N.</given-names></string-name>, <string-name><surname>Frank</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Quirk</surname>, <given-names>M. C.</given-names></string-name> &amp; <string-name><surname>Wilson</surname>, <given-names>M. A.</given-names></string-name> <article-title>A statistical paradigm for neural spike train decoding applied to position prediction from ensemble firing patterns of rat hippocampal place cells</article-title>. <source>J. Neurosci</source>. <volume>18</volume>, <fpage>7411</fpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Peyrache</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lacroix</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>P. C.</given-names></string-name> &amp; <string-name><surname>Buzsáki</surname>, <given-names>G.</given-names></string-name> <article-title>Internally organized mechanisms of the head direction sense</article-title>. <source>Nat. Neurosci</source>. <volume>18</volume>, <fpage>569</fpage>–<lpage>575</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="web"><string-name><surname>Peyrache</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>P. C.</given-names></string-name> &amp; <string-name><surname>Buzsaki</surname>, <given-names>G.</given-names></string-name> <article-title>Extracellular recordings from multi-site silicon probes in the anterior thalamus and subicular formation of freely moving mice</article-title>. <source>CRCNS.org</source> doi:<pub-id pub-id-type="doi">10.6080/K0G15XS1</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name> <etal>et al.</etal> <article-title>Survey of spiking in the mouse visual system reveals functional hierarchy</article-title>. <source>Nature</source> <volume>592</volume>, <fpage>86</fpage>–<lpage>92</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Zheng</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>Neurons detect cognitive boundaries to structure episodic memories in humans</article-title>. <source>Nat. Neurosci</source>. <volume>25</volume>, <fpage>358</fpage>–<lpage>368</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Olmos</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Kingdom</surname>, <given-names>F. A. A.</given-names></string-name> <article-title>A Biologically Inspired Algorithm for the Recovery of Shading and Reflectance Images</article-title>. <source>Perception</source> <volume>33</volume>, <fpage>1463</fpage>–<lpage>1473</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="other"><string-name><surname>Zhou</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title>EASE: EM-Assisted Source Extraction from calcium imaging data</article-title>. <source>BioRxiv</source> (<year>2020</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Turner-Evans</surname>, <given-names>D. B.</given-names></string-name> <etal>et al.</etal> <article-title>The Neuroanatomical Ultrastructure and Function of a Biological Ring Attractor</article-title>. <source>Neuron</source> <volume>108</volume>, <fpage>145</fpage>–<lpage>163</lpage>.e10 (<year>2020</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Chaudhuri</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gercek</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Pandey</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Peyrache</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Fiete</surname>, <given-names>I.</given-names></string-name> <article-title>The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep</article-title>. <source>Nat. Neurosci</source>. <volume>22</volume>, <fpage>1512</fpage>–<lpage>1520</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Viejo</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Peyrache</surname>, <given-names>A.</given-names></string-name> <article-title>Precise coupling of the thalamic head-direction system to hippocampal ripples</article-title>. <source>Nat. Commun</source>. <volume>11</volume>, <fpage>2524</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Gardner</surname>, <given-names>R. J.</given-names></string-name> <etal>et al.</etal> <article-title>Toroidal topology of population activity in grid cells</article-title>. <source>Nature</source> <volume>602</volume>, <fpage>123</fpage>– <lpage>128</lpage> (<year>2022</year>).</mixed-citation></ref>
</ref-list>
<sec id="s3">
<title>Data Availability</title>
<p>All data used in this manuscript are publicly available and were previously published.</p>
<p>Extracellular recordings from multi-site silicon probes in the anterior thalamus and subicular formation of freely moving mice.: Peyrache, A., Petersen, P. C. &amp; Buzsaki, G., 2015, <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6080/K0G15XS1">http://dx.doi.org/10.6080/K0G15XS1</ext-link>, CRCNS, SCR:006907;</p>
<p>Survey of spiking in the mouse visual system reveals functional hierarchy: Joshua H. Siegle, Xiaoxuan Jia, Séverine Durand, Sam Gale, Corbett Bennett, Nile Graddis, Greggory Heller, Tamina K. Ramirez, Hannah Choi, Jennifer A. Luviano, Peter A. Groblewski, Ruweida Ahmed, Anton Arkhipov, Amy Bernard, Yazan N. Billeh, Dillan Brown, Michael A. Buice, Nicolas Cain, Shiella Caldejon, Linzy Casal, Andrew Cho, Maggie Chvilicek, Timothy C. Cox, Kael Dai, Daniel J. Denman, Saskia E. J. de Vries, Roald Dietzman, Luke Esposito, Colin Farrell, David Feng, John Galbraith, Marina Garrett, Emily C. Gelfand, Nicole Hancock, Julie A. Harris, Robert Howard, Brian Hu, Ross Hytnen, Ramakrishnan Iyer, Erika Jessett, Katelyn Johnson, India Kato, Justin Kiggins, Sophie Lambert, Jerome Lecoq, Peter Ledochowitsch, Jung Hoon Lee, Arielle Leon, Yang Li, Elizabeth Liang, Fuhui Long, Kyla Mace, Jose Melchior, Daniel Millman, Tyler Mollenkopf, Chelsea Nayan, Lydia Ng, Kiet Ngo, Thuyahn Nguyen, Philip R. Nicovich, Kat North, Gabriel Koch Ocker, Doug Ollerenshaw, Michael Oliver, Marius Pachitariu, Jed Perkins, Melissa Reding, David Reid, Miranda Robertson, Kara Ronellenfitch, Sam Seid, Cliff Slaughterbeck, Michelle Stoecklin, David Sullivan, Ben Sutton, Jackie Swapp, Carol Thompson, Kristen Turner, Wayne Wakeman, Jennifer D. Whitesell, Derric Williams, Ali Williford, Rob Young, Hongkui Zeng, Sarah Naylor, John W. Phillips, R. Clay Reid, Stefan Mihalas, Shawn R. Olsen &amp; Christof Koch, 2021, <ext-link ext-link-type="uri" xlink:href="https://gui.dandiarchive.org/#/dandiset/000021">https://gui.dandiarchive.org/#/dandiset/000021</ext-link>, Dandiset, 000021;</p>
<p>The Neuroanatomical Ultrastructure and Function of a Biological Ring Attractor: Daniel B. Turner-Evans, Kristopher T. Jensen, Saba Ali, Tyler Paterson, Arlo Sheridan, Robert P. Ray, Tanya Wolff, J. Scott Lauritzen, Gerald M. Rubin, Davi D. Bock, and Vivek Jayaraman, 2020, <ext-link ext-link-type="uri" xlink:href="https://janelia.figshare.com/articles/dataset/OneColor_zip/12490373">https://janelia.figshare.com/articles/dataset/OneColor_zip/12490373</ext-link>, Allen, 12490373;</p>
<p>Neurons detect cognitive boundaries to structure episodic memories in humans: Zheng, Jie, Andrea GP Schjetnan, Mar Yebra, Bernard A. Gomes, Clayton P. Mosher, Suneil K. Kalia, Taufik A. Valiante, Adam N. Mamelak, Gabriel Kreiman, and Ueli Rutishauser., 2022, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48324/dandi.000207/0.220216.0323">https://doi.org/10.48324/dandi.000207/0.220216.0323</ext-link>, Dandiset, 0.220216.0323;</p>
<p>EASE: EM-Assisted Source Extraction from calcium imaging data: Pengcheng Zhou, Jacob Reimer, Ding Zhou, Amol Pasarkar, Ian Kinsella, Emmanouil Froudarakis, Dimitri V Yatsenko, Paul G Fahey, Agnes Bodor, JoAnn Buchanan, Dan Bumbarger, Gayathri Mahalingam, Russel Torres, Sven Dorkenwald, Dodam Ih, Kisuk Lee, Ran Lu, Thomas Macrina, Jingpeng Wu, Nuno da Costa, R. Clay Reid, Andreas S Tolias, Liam Paninski, 2020, <ext-link ext-link-type="uri" xlink:href="https://www.microns-explorer.org/cortical-mm3#f-data">https://www.microns-explorer.org/cortical-mm3#f-data</ext-link>, Allen, cortical-mm3#f-data</p>
</sec>
<sec id="s4">
<title>Code Availability</title>
<p>All code is publicly available online under GPL-3.0 license.</p>
<p>Pynapple: <ext-link ext-link-type="uri" xlink:href="https://github.com/pynapple-org/pynapple">https://github.com/pynapple-org/pynapple</ext-link></p>
<p>Pynacollada: <ext-link ext-link-type="uri" xlink:href="https://github.com/PeyracheLab/pynacollada">https://github.com/PeyracheLab/pynacollada</ext-link></p>
<p>Code to generate <xref rid="fig4" ref-type="fig">Figures 4</xref> and <xref rid="fig5" ref-type="fig">5</xref>: <ext-link ext-link-type="uri" xlink:href="https://github.com/pynapple-org/pynapple-paper-2023">https://github.com/pynapple-org/pynapple-paper-2023</ext-link></p>
</sec>
<ack>
<title>Acknowledgment</title>
<p>This work was supported by a Canadian Research Chair in Systems Neuroscience, CIHR Project Grant 155957 and 180330, NSERC Discovery Grant RGPIN-2018-04600, the Canada-Israel Health Research Initiative, jointly funded by the Canadian Institutes of Health Research, the Israel Science Foundation, the International Development Research Centre, Canada and the Azrieli Foundation 108877-001, and the Tanenbaum Open Science Institute (AP).</p>
</ack>
<sec id="s5">
<title>Competing Interests</title>
<p>No competing interests disclosed.</p>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.85786.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kemere</surname>
<given-names>Caleb</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Rice University</institution>
</institution-wrap>
<city>Houston</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This paper introduces the python software package Pynapple and a separate package of more advanced routines (Pynacollada) to the Neuroscience/Neural Engineering community. Pynapple provides a set of data objects and methods that have the potential to simplify data analysis for neural and behavioral data types. This represents a <bold>valuable</bold> contribution to the field. With more examples and as a live coding notebook, the evidence was judged to be <bold>compelling</bold>.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.85786.2.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>A typical path from preprocessed data to findings in systems neuroscience often includes set of analyses that often share common components. For example, an investigator might want to generate plots that relating one time series (e.g., a set of spike times) to another (measurements of a behavioral parameter such as pupil diameter or running speed). In most cases, each individual scientist writes their own code to carry out these analyses, and thus the same basic analysis is coded repeatedly. This is problematic for several reasons, including the inefficiency of different people writing the same code over and over again.</p>
<p>This paper presents Pynapple, a python package that aims to address those problems.</p>
<p>Strengths:</p>
<p>The authors have identified a key need in the community - well written analysis routines that carry out a core set of functions and can import data from multiple formats. In addition, they recognized that there are some common elements of many analyses, particularly those involving timeseries, and their object-oriented architecture takes advantage of those commonalities to simplify the overall analysis process.</p>
<p>The package is separated into a core set of applications and another with more advanced applications, with the goal of both providing a streamlined base for analyses and allowing for implementations/inclusion of more experimental approaches.</p>
<p>Weaknesses:</p>
<p>The revised version of the paper does a very good job of addressing previous concerns. It would be slightly more accurate in the Highlights section to say &quot;A lightweight and standalone package facilitating long-term backward compatibility&quot; but this is a very minor issue.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.85786.2.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The manuscript by G. Viejo et al. describes a new open-source toolbox called Pynapple, for data analysis of electrophysiological recordings, calcium imaging, and behavioral data. It is an object-oriented python package, consisting of 5 main object types: timestamps (Ts), timestamped data (Tsd), TsGroup, TsdFrame, and IntervalSet. Each object has a set of built-in core methods and import tools for common data acquisition systems and pipelines.</p>
<p>Pynapple is a low-level package that uses NWB as a file format, and further allows for other more advanced toolsets to build upon it. One of these is called Pynacollada which is a toolset for data analysis of electrophysiological, imaging, and behavioral data.</p>
<p>Pynapple and Pynacollada have the potential to become very valuable and foundational tools for the analysis of neurophysiological data. NWB still has a steep learning curve and Pynapple offers a user-friendly toolset that can also serve as a wrapper for NWB.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.85786.2.sa3</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Viejo</surname>
<given-names>Guillaume</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Levenstein</surname>
<given-names>Daniel</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Carrasco</surname>
<given-names>Sofia Skromne</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mehrotra</surname>
<given-names>Dhruv</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5813-3218</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mahallati</surname>
<given-names>Sara</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Vite</surname>
<given-names>Gilberto R</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Denny</surname>
<given-names>Henry</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sjulson</surname>
<given-names>Lucas</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Battaglia</surname>
<given-names>Francesco P</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Peyrache</surname>
<given-names>Adrien</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<p>We would ﬁrst like to thank the reviewers and the editor for their insightful comments and suggestions. We are particularly glad to read that our so&lt;ware package constitutes a set of “well-written analysis routines” which have “the potential to become very valuable and foundational tools for the analysis of neurophysiological data”. We have updated the manuscript to address their remarks where appropriate.</p>
<p>Additionally, we would like to stress that this kind of tools is in continual development. As such, the manuscript oﬀered a snapshot of the package at one point during this process, which in this case was several months ago at initial submission. Since then, several improvements were implemented. The manuscript has been further updated to reﬂect these more recent changes.</p>
<disp-quote content-type="editor-comment">
<p>From the Reviewing Editor:</p>
<p>The reviewers identiﬁed a number of fundamental weaknesses in the paper.</p>
<p>1. For a paper demonstrating a toolbox, it seems that some example analyses showing the value of the approach (and potentially the advantage in simpliﬁcation, etc over previous or other approaches) are really important to demonstrate.</p>
</disp-quote>
<p>As noted by the ﬁrst reviewer, the online repository (i.e. GitHub page) conveys a better sense of the toolboxes’ contribution to the ﬁeld than the present manuscript. This is a fair remark but at the same time, it is unclear how to illustrate this in a journal article without dedicating a great deal of page space to presenting raw code, while online tools oﬀer an easier and clearer way to do this. As a work-around, our strategy was to illustrate some examples of data analysis in Figures 4&amp;5 by comparing each illustrated processing step to the corresponding command line used by the Pynapple package. Each step requires a single line of code, meaning that one only needs to write three lines of code to decode a feature from population activity using a Bayesian decoder (Fig. 4a), compute a cross-correlograms of two neurons during speciﬁc stimulus presentation (Fig. 4b) or compute the average ﬁring rate of two neurons around a speciﬁc time of the experimental task (Fig. 4c). We believe that these visual aides make it unnecessary to add code in the main text of this manuscript. However, to aid reader understanding, we now provide clear references to online Jupyter notebooks which show how each ﬁgure was generated in ﬁgure legends as well as in the “Code Availability” section.</p>
<p><ext-link ext-link-type="uri" xlink:href="https://github.com/pynapple-org/pynapple-paper-2023">https://github.com/pynapple-org/pynapple-paper-2023</ext-link></p>
<p>Furthermore, we have opted-in for the “Executable Research Articles” feature at eLife, which will make it possible to include live scripts and ﬁgures in the manuscript once it is accepted for publication. We do not know at this stage what it entails exactly, but we hope that Figures 4&amp;5 will become live with this feature. The readers will have the possibility to see and edit the code directly within the online version of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>1. The manuscript's claims about not having dependencies seem confusing.</p>
</disp-quote>
<p>We agree that this claim was somewhat unfounded. There are virtually no Python packages that do not have dependencies. Our intention was to say that the package had no dependencies outside the most common ones, which are Numpy, Scipy, and Pandas. Too many packages in the ﬁeld tend to have long list of dependencies making long-term back-compatibility quite challenging. By keeping depencies minimal, we hope to maximise the package’'s long term back-compatibility. We have rephrased this statement in the manuscript in the following sections:</p>
<p>Figure 1, legend.</p>
<p>“These methods depend only on a few, commonly used, external packages.”</p>
<p>Section Foundational data processing:
“they are for the most part built-in and only depend on a few widely-used external packages. This ensures that the package can be used in a near stand-alone fashion, without relying on packages that are at risk of not being maintained or of not being compatible in the near future.”</p>
<disp-quote content-type="editor-comment">
<p>1. Given its signiﬁcant relevance, it seems important to cite the FMATool and describe connections between it (or analyses based on it) and the presented work.</p>
</disp-quote>
<p>Indeed, although we had already cited other toolboxes (including a review covering the topic comprehensively), we should have included this one in the original manuscript. Unfortunately, to the best of our knowledge, this toolbox is not citable (there is no companion paper). We have added a reference to it in plain text.</p>
<disp-quote content-type="editor-comment">
<p>1. Some discussion of integration between Pynapple and the rest of a full experimental data pipeline should be discussed with regard to reproducibility.</p>
</disp-quote>
<p>This is an interesting point, and the third paragraph of the discussion somewhat broached this issue. Pynapple was not originally designed to pre-process data. However, it can, in theory, load any type of data streams a&lt;er the necessary pre-processing steps. Overall, modularity is a key aspect of the Pynapple framework, and this is also the case for the integration with data pre-processing pipelines, for example spike sorting in electrophysiology and detection of region of interest in calcium imaging. We do not think there should be an integrated solution to the problem but, instead, to make it possible that any piece of code can be used for data irrespective of their origin. This is why we focused on making data loading straightforward and easy to adapt to any particular situation.
To expand on this point and make it clear that Pynapple is not meant to pre-process data but can, in theory, load any type of data streams a&lt;er the necessary pre-processing steps, we have added the following sentences to the aforementioned paragraph:</p>
<p>“Data in neuroscience vary widely in their structure, size, and need for pre-processing. Pynapple is built around the idea that raw data have already been pre-processed (for example, spike sorting and detection of ROIs).”</p>
<disp-quote content-type="editor-comment">
<p>1. Relatedly, a description of how data are stored a&lt;er processing (i.e., how precisely are processed data stored in NWB format).</p>
</disp-quote>
<p>We agree that this is a critical issue. NWB is not necessarily the best option as it is not possible to overwrite in a NWB ﬁle. This would require the creation of a new NWB ﬁle each time, which is computationally expensive and time consuming. It also further increases the odds of writing error. Theoretically, users who needs to store intermediate results in a ﬂexible way could use any methods they prefer, writing their own data ﬁles and wrappers to reload these data into Pynapple objects. Indeed, it is not easy to properly store data in an object-speciﬁc manner. This is a long-standing issue and one we are currently working to resolve.</p>
<p>To do so, we are developing I/O methods for each Pynapple core objects. We aim to provide an output format that is simple to read and backward compatible in future Pynapple releases. This feature will be available in the coming weeks. To note, while NWB may not be the central data format of Pynapple in future releases, it has become a central node in the neuroscience ecosystem of so&lt;ware. Therefore, we aim to facilitate the interaction of users with reading and writing for this format by developing a set of simple standalone functions.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>A typical path from preprocessed data to ﬁndings in systems neuroscience o&lt;en includes a set of analyses that o&lt;en share common components. For example, an investigator might want to generate plots that relate one time series (e.g., a set of spike times) to another (measurements of a behavioral parameter such as pupil diameter or running speed). In most cases, each individual scientist writes their own code to carry out these analyses, and thus the same basic analysis is coded repeatedly. This is problematic for several reasons, including the waste of time, the potential for errors, and the greater diﬃculty inherent in sharing highly customized code.</p>
<p>This paper presents Pynapple, a python package that aims to address those problems.</p>
<p>Strengths:</p>
<p>The authors have identiﬁed a key need in the community - well-written analysis routines that carry out a core set of functions and can import data from multiple formats. In addition, they recognized that there are some common elements of many analyses, particularly those involving timeseries, and their object- oriented architecture takes advantage of those commonalities to simplify the overall analysis process.</p>
<p>The package is separated into a core set of applications and another with more advanced applications, with the goal of both providing a streamlined base for analyses and allowing for implementations/inclusion of more experimental approaches.</p>
<p>Weaknesses:</p>
<p>There are two main weaknesses of the paper in its present form.</p>
<p>First, the claims relating to the value of the library in everyday use are not demonstrated clearly. There are no comparisons of, for example, the number of lines of code required to carry out a speciﬁc analysis with and without Pynapple or Pynacollada. Similarly, the paper does not give the reader a good sense of how analyses are carried out and how the object-oriented architecture provides a simpliﬁed user interaction experience. This contrasts with their GitHub page and associated notebooks which do a better job of showing the package in action.</p>
</disp-quote>
<p>As noted in the response to the Reviewing Editor and response to the reviewer’s recommendation to the authors below, we have now included links to Jupyter notebooks that highlight how panels of Figures 4 and 5 were generated (<ext-link ext-link-type="uri" xlink:href="https://github.com/pynapple-org/pynapple-paper-2023">https://github.com/pynapple-org/pynapple-paper-2023</ext-link>). However, we believe that including more code in the manuscript than what is currently shown (I.e. abbreviated call to methods on top of panels in Figs 4&amp;5) would decrease the readability of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Second, the paper makes several claims about the values of object-oriented programming and the overall design strategy that are not entirely accurate. For example, object-oriented programming does not inherently reduce coding errors, although it can be part of good so&lt;ware engineering. Similarly, there is a claim that the design strategy &quot;ensures stability&quot; when it would be much more accurate to say that these strategies make it easier to maintain the stability of the code. And the authors state that the package has no dependencies, which is not true in the codebase. These and other claims are made without a clear deﬁnition of the properties that good scientiﬁc analysis so&lt;ware should have (e.g., stability, extensibility, testing infrastructure, etc.).</p>
</disp-quote>
<p>Following thFMAe reviewer’s comment, we have rephrased and clariﬁed these claims. We provide detailed response to these remarks in the recommendations to authors below.</p>
<disp-quote content-type="editor-comment">
<p>There is also a minor issue - these packages address an important need for high-level analysis tools but do not provide associated tools for preprocessing (e.g., spike sorting) or for creating reproducible pipelines for these analyses. This is entirely reasonable, in that no one package can be expected to do everything, but a bit deeper account of the process that takes raw data and produces scientiﬁc results would be helpful. In addition, some discussion of how this package could be combined with other tools (e.g., DataJoint, Code Ocean) would help provide context for where Pynapple and Pynacollada could ﬁt into a robust and reliable data analysis ecosystem.</p>
</disp-quote>
<p>We agree the better explaining how Pynapple is integrated within data preprocessing pipelines is essential. We have clariﬁed this aspect in the manuscript and provide more details below.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>Page 1</p>
<list list-type="bullet">
<list-item><p>Title</p>
</list-item></list>
<p>The authors should note that the application name- &quot;Pynapple&quot; could be confused with something from Apple. Users may search for &quot;Pyapple&quot; as many python applications contain &quot;py&quot; like &quot;Numpy&quot;. &quot;Pyapple&quot; indeed is a Python Apple that works with Apple products. They could consider &quot;NeuroFrame&quot;, &quot;NeuroSeries&quot; or &quot;NeuroPandas&quot; to help users realize this is not an apple product.</p>
</disp-quote>
<p>We thank the referee for this interesting comment. However, we are not willing to make such change at this point. The community of users has been growing in the last year and it seems too late to change the name. To note, it is the ﬁrst time such comment is made to us and it does not seem that users and collaborators are confused with any Apple products.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Abstract</p>
</list-item></list>
<p>The authors mentioned that the Pynapple is &quot;fully open source&quot;. It may be better to simply say it is &quot;open source&quot;.</p>
</disp-quote>
<p>We agree, corrected.</p>
<disp-quote content-type="editor-comment">
<p>Assuming the authors keep the name, it would be helpful if the full meaning of Pynapple - Python Neural Analysis Package was presented as early as possible.</p>
</disp-quote>
<p>Corrected in the abstract.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Highlight</p>
</list-item></list>
<p>An application being lightweight and standalone does not imply nor ensure backward compatibility. In general, it would be useful if the authors identiﬁed a set of desirable code characteristics, deﬁned them clearly in the introduction, and then describe their so&lt;ware in terms of those characteristics.</p>
</disp-quote>
<p>Thank you for your comment. We agree that being lightweight and standalone does not necessarily imply backward compatibility. Our intention was to emphasize that Pynapple is designed to be as simple and ﬂexible as possible, with a focus on providing a consistent interface for users across diﬀerent versions. However, we understand that this may not be enough to ensure long-term stability, which is why we are committed to regular updates and maintenance to ensure that the code remains functional as the underlying code base (Python versions, etc.) changes.</p>
<p>Regarding your suggestion to identify a set of desirable code characteristics, we believe this is an excellent idea. In the introduction, we brieﬂy touch upon some of the core principles that guided our development of Pynapple: a lightweight, stable, and simple package. However, we acknowledge that providing a more detailed discussion of these characteristics and how they relate to the design of our so&lt;ware would be useful for readers. We have added this paragraph in the discussion:</p>
<p>“Pynapple was developed to be lightweight, stable, and simple. As simplicity does not necessarily imply backward compatibility (i.e. long-term stability of the code), Pynapple main objects and their properties will remain the same for the foreseeable future, even if the code in the backend may eventually change (e.g. not relying on Pandas in future version). The small number of external dependencies also decrease the need to adapt the code to new versions of external packages. This approach favors long-term backward compatibility.”</p>
<disp-quote content-type="editor-comment">
<p>Page 2</p>
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;Despite this rapid progress, data analysis o&lt;en relies on custom-made, lab-speciﬁc code, which is susceptible to error and can be diﬃcult to compare across research groups.&quot;</p>
<p>It would be helpful to add that custom-made, lab-speciﬁc code can lead to a violation of FAIR principles (<ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/FAIR_datadata">https://en.wikipedia.org/wiki/FAIR_datadata</ext-link>). More generally, any package can have errors, so it would be helpful to explain any testing regiments or other approach the authors have taken to ensure that their code is error-free.</p>
</disp-quote>
<p>We understand the importance of the FAIR principles for data sharing. However, Pynapple was not designed to handle data through their pre-processing. The only aspect that is somehow covered by the FAIR principles is the interoperability, but again, it is a requirement for the data to interoperate with diﬀerent storage and analysis pipelines, not of the analysis framework itself. Unlike custom-made code, Pynapple will make interoperability easier, as, in theory, once the required data loaders are available, any analysis could be run on any dataset. We have added the following sentence to the discussion:</p>
<p>“Data in neuroscience vary widely in their structure, size, and need for pre-processing. Pynapple is built around the idea that raw data has already been pre-processed (for example, spike sorting and ROI detection). According to the FAIR principles, pre-processed data should interoperate across diﬀerent analysis pipelines. Pynapple makes this interoperability possible as, once the data are loaded in the Pynapple framework, the same code can be used to analyze diﬀerent datasets”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;While several toolboxes are available to perform neuronal data analysis ti‚Äì11,2ti (see ref. 29 for review), most of these programs focus on producing high-level analysis from speciﬁed types of data and do not oﬀer the versatility required for rapidly-changing analytical methods and experimental methods.&quot;</p>
<p>Here it would be helpful if the authors could give a more speciﬁc example or explain why this is problematic enough to be a concern. Users may not see a problem with high-level analysis or using speciﬁc data types.</p>
</disp-quote>
<p>Again, we apologize for not fully elaborating upon our goals here. Our intention was to point out that toolboxes o&lt;en focus on one particular case of high-level analysis. In many cases, such packages lack low level analysis features or the ﬂexibility to derive new analysis pipelines quickly and eﬀortlessly. Users can decide to use low-level packages such as Pandas, but in that case, the learning curve can be steep for users with low, if any, computational background. The simplicity of Pynapple, and the set of examples and notebooks, make it possible for individuals who start coding to be quickly able to analyze their data.</p>
<p>As we do not want to be too speciﬁc at this point of the manuscript (second paragraph of the intro) and as we have clariﬁed many of the aspects of the toolbox in the new revised version, we have only added the following sentence to the paragraph:</p>
<p>“Users can decide to use low-level data manipulation packages such as Pandas, but in that case, the learning curve can be steep for users with low, if any, computational background.”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;To meet these needs, a general toolbox for data analysis must be designed with a few principles in mind&quot;</p>
<p>Toolboxes based on many diﬀerent principles can solve problems. It is likely more accurate to say that the authors designed their toolbox with a particular set of principles in mind. A clear description of those principles (as mentioned in the comment above) would help the reader understand why the speciﬁc choices made are beneﬁcial.</p>
</disp-quote>
<p>We agree that these are not “universal” principles and clearly more the principles we had in mind when we designed the package. We have clariﬁed these principles and made clear that these are personal point of views.</p>
<p>We have rephrased the following paragraph:</p>
<p>“To meet these needs, we designed Pynapple, a general toolbox for data analysis in systems Neuroscience with a few principles in mind.“</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;The ﬁrst property of such a toolbox is that it should be object-oriented, organizing so&lt;ware around data.&quot;</p>
<p>What facts make this true? For example, React is a web development library. A common approach to using this library is to use Hooks (essentially a collection of functions). This is becoming more popular than the previous approach of using Components (a collection of classes). This is an example of how Object-oriented programming is not always the best solution. In some cases, for example, object- oriented coding can cause problems (e.g. it can be hard to ﬁnd the place where a given function is deﬁned and to ﬁgure out which version is being used given complex inheritance structures.)</p>
<p>In general, key selling points of object-oriented programming are extension, inheritance, and encapsulation. If the authors want to retain this text (which would be entirely reasonable), it would be helpful if they explained clearly how an object-oriented approach enables these functions and why they are critical for this application in particular.</p>
</disp-quote>
<p>The referee makes a particularly important point. We are aware of the limits of OOP, especially when these objects become over-complex, and that the inheritance become unclear.</p>
<p>We have clariﬁed our goal here. We believe that in our case, OOP is powerful and, overall, is less error- prone that a collection of functions. The reasons are the following:</p>
<p>An object-oriented approach facilitates better interactions between objects. By encapsulating data and behavior within objects, object-oriented programming promotes clear and well-deﬁned interfaces between objects. This results in more structured and manageable code, as objects communicate with each other through these well-deﬁned interfaces. Such improved interactions lead to increased code reliability.</p>
<p>Inheritance, a key concept in object-oriented programming, allows for the inheritance of properties. One important example of how inheritance is crucial in the Pynapple framework is the time support of Pynapple objects. It determines the valid epoch on which the object is deﬁned. This property needs to be carried over during diﬀerent manipulations of the object. Without OOP, this property could easily be forgotten, resulting in erroneous conclusions for many types of analysis. The simplest case is the average rate of a TS object: the rate must be computed on the time support ( a property of TS objects), not the beginning to the end of the recording (or of a speciﬁc epoch, independent of the TS). Finally, it is easier to access and manipulate the meta information of a Pynapple object than without using objects.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;drastically diminishing the odds of a coding error&quot;</p>
<p>This seems a bit strong here. Perhaps &quot;reducing the odds&quot; would be more accurate.</p>
</disp-quote>
<p>We agree. Now changed.</p>
<disp-quote content-type="editor-comment">
<p>Page 3</p>
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;. Another property of an eﬃcient toolbox is that as much data as possible should be captured by only a small number of objects This ensures that the same code can be used for various datasets
and eliminates the need of adapting the structure&quot;</p>
<p>It may be better to write something like - &quot;Objects have a collection of preset variables/values that are well suited for general use and are very ﬂexible.&quot; Capturing &quot;as much data as possible&quot; may be confusing, because it's not the amount that this helps with but rather the variety.</p>
</disp-quote>
<p>We thank the referee for this remark. We have rephrased this sentence as follows:</p>
<p>“Another property of an eﬃcient toolbox is that a small number of objects could virtually represents all possible data streams in neuroscience, instead of objects made for speciﬁc physiological processes (e.g. spike trains).”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;The properties listed above ensure the long-term stability of a toolbox, a crucial aspect for maintaining the code repository. Toolboxes built around these principles will be maximally ﬂexible and will have the most general
application&quot;</p>
<p>There are two issues with this statement. First, ensuring long-term stability is only possible with a long- term commitment of time and resources to ensure that that code remains functional as the underlying code base (python versions, etc.) changes. If that is something you are commisng to, it would be great to make that clear. If not, these statements need to be less ﬁrm.</p>
<p>Second, it is not clear how these properties were arrived at in the ﬁrst place. There are things like the FAIR Principles which could provide an organizing framework, ideally when combined with good so&lt;ware engineering practices, and if some more systematic discussion of these properties and their justiﬁcation could be added, it would help the ﬁeld think about this issue more clearly.</p>
</disp-quote>
<p>The referee makes a valid point that ensuring long-term stability requires a long-term commitment of time and resources to maintain the code as the underlying technology evolves. While we cannot make guarantees about the future of Pynapple, we believe that one of the best ways to ensure long-term stability is by fostering a strong community of users and contributors who can provide ongoing support and development. By promoting open-source collaboration and encouraging community involvement, we hope to create a sustainable ecosystem around Pynapple that can adapt to changes in technology and scientiﬁc practices over time. Ultimately, the longevity of any scientiﬁc tool depends on its adoption and use by the research community, and we hope that Pynapple can provide value to neuroscience researchers and continue to evolve and improve as the ﬁeld progresses.</p>
<p>It is noteworthy that the ﬁrst author, and main developer of the package, has now been hired as a data scientist at the Center for Computational Neuroscience, Flatiron Institute, to explicitly continue the development of the tool and build a community of users and contributors.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;each with a limited number of methods...&quot;</p>
<p>This may give the impression that the functionality is limited, so rephrasing may be helpful.</p>
</disp-quote>
<p>Indeed! We have now rephrased this sentence:</p>
<p>“The core of Pynapple is ﬁve versatile timeseries objects, whose methods make it possible to intuitively manipulate and analyze the data.”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote that object-oriented coding</p>
</list-item></list>
<p>&quot;limits the chances of coding error&quot;</p>
<p>This is not always the case, but if it is the case here, it would be helpful if the authors explain exactly how it helps to use object-oriented approaches for this package.</p>
</disp-quote>
<p>We agree with the referee that it is not always the case. As we explained above, we believe it is less error-prone that a collection of functions. Quite o&lt;en, it also makes it easier to debug.
We have changed this sentence with the following one:</p>
<p>“Because objects are designed to be self-contained and interact with each other through well-deﬁned methods, users are less likely to make errors when using them. This is because objects can enforce their own internal consistency, reducing the chances of data inconsistencies or unexpected behavior. Overall, OOP is a powerful tool for managing complexity and reducing errors in scientiﬁc programming.”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Fig 1</p>
</list-item></list>
<p>In object-oriented programming, a class is a blueprint for the classes that inherit it. Instantiating that</p>
<p>
class creates an object. An object contains any or all of these - data, methods, and events. The ﬁgure could be improved if it maintained these organizational principles as ﬁgure properties.</p>
</disp-quote>
<p>We agree with the referee’s remark regarding the logic of objects instantiation but how this could be incorporated in Fig. 1 without making it too complex is unclear. Here, objects are instantiated from the ﬁrst to the second column. We have not provided details about the parent objects, as we believe these details are not important for reader comprehension. In its present form, the objects are inherited from Pandas objects, but it is possible that a future version is based on something else. For the users, this will be transparent as the toolbox is designed in such a way that only the methods that are speciﬁc to Pynapple are needed to do most computation, while only expert programmers may be interested in using Pandas functionalities.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote that Pynapple does -</p>
</list-item></list>
<p>&quot;not depend on any external package&quot;</p>
<p>As mentioned above, this is not true. It depends on Numpy and likely other packages, and this should be explained. It is perfectly reasonable to say that it depends on only a few other packages.</p>
</disp-quote>
<p>As said above, we have now clariﬁed this claim.</p>
<disp-quote content-type="editor-comment">
<p>Page 5.</p>
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;represent arrays of Ts and Tsd&quot;</p>
<p>For a knowledgeable reader's reference, it would be helpful to refer to these either as Numpy arrays (at least at ﬁrst when they are deﬁned) or as lists if they are native python objects.</p>
</disp-quote>
<p>Indeed, using the word “arrays” here could be confusing because of Numpy arrays. We have changed this term with “groups”.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;Pynapple is built with objects from the Pandas library ... Pynapple objects inherit the computational stability and ﬂexibility&quot;</p>
<p>Here a deﬁnition of stability would be useful. Is it the case that by stability you mean &quot;does not change o&lt;en&quot;? Or is some other meaning of stability implied?</p>
</disp-quote>
<p>Yes, this is exactly what we meant when referring to the stability of Pandas. We have added the following precision:</p>
<p>“As such, Pynapple objects inherit the long-term consistency of the code and the computational ﬂexibility computational stability and ﬂexibility from this widely used package.”</p>
<disp-quote content-type="editor-comment">
<p>Page 6</p>
<list list-type="bullet">
<list-item><p>Fig 2</p>
</list-item></list>
<p>In Fig 2 A and B, the illustrations are good. It would also be very helpful to use toy code examples to illustrate how Pynapple will be used to carry out on a sample analysis-problem so that potential users can see what would need to be done.</p>
</disp-quote>
<p>We appreciate the kind works. Regarding the toy code, this is what we tried to do in Fig. 4. Instead of including the code directly in the paper, which does not seem a modern way of doing this, we now refer to the online notebooks that reproduce all panels of Figure 4.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;While these objects and methods are relatively few&quot;</p>
<p>In object-oriented programming, objects contain methods. If a method is not in an object, it is not technically a method but a function. It would be helpful if the authors made sure their terminology is accurate, perhaps by saying something like &quot;While there are relatively few objects, and while each object has relatively few methods ... &quot;</p>
</disp-quote>
<p>We agree with the referee, we have changed the sentence accordingly.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;if not implemented correctly, they can be both computationally intensive and highly susceptible to user error&quot;</p>
<p>Here the authors are using &quot;correctly&quot; to refer to two things - &quot;accuracy&quot; - gesng the right answer, and &quot;eﬃciency&quot; - gesng to that answer with relatively less computation. It would be clearer if they split out those two concepts in the phrasing.</p>
</disp-quote>
<p>Indeed, we used the term to cover both aspects of the problem, leading to the two possible issues cited in the second part of the sentence. We have changed the sentence following the referee’s advice:</p>
<p>“While there are relatively few objects, and while each object has relatively few methods, they are the foundation of almost any analysis in systems neuroscience. However, if not implemented eﬃciently, they can be computationally intensive and if not implemented accurately, they are highly susceptible to user error.”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>In the next sentence the authors wrote -</p>
</list-item></list>
<p>&quot;Pynapple addresses this concern.&quot;</p>
<p>This statement would beneﬁt from just additional text explaining how the concern is addressed.</p>
</disp-quote>
<p>We thank the referee for the suggestion. We have changed the sentence to this one:
“The implementation of core features in Pynapple addresses the concerns of eﬃciency and accuracy”</p>
<disp-quote content-type="editor-comment">
<p>Page 9</p>
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>This is implemented via a set of specialized object subclasses of the BaseLoader class. To avoid code redundancy, these I/O classes inherit the properties of the BaseLoader class. &quot;</p>
<p>From a programming perspective, the point of a base class is to avoid redundancy, so it might be better to just mention that this avoids the need to redeﬁne I/O operations in each class.</p>
</disp-quote>
<p>We have rephrased the sentence as follows:</p>
<p>“This is implemented via a set of specialized object subclasses of the BaseLoader class, avoiding the need to redeﬁne I/O operations in each subclass&quot;</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;classes are unique and independent from each other, ensuring stability&quot;</p>
<p>How do classes being unique and independent ensure stability? Perhaps here again the misunderstanding is due to the lack of a deﬁnition of stability.</p>
</disp-quote>
<p>We thank the referee for the remark. We ﬁrst changed “stability” for “long-term backward compatibility”. We further added the following sentence to clarify this claim. “For instance, if the spike sorting tool Phy changes its output in the future, this would not aﬀect the “Neurosuite” IO class as they are independent of each other. This allows each tool to be updated or modiﬁed independently, without requiring changes to the other tool or the overall data format.”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;Using preexisting code to load data in a speciﬁc manner instead of rewriting already existing functions avoids preprocessing errors&quot;</p>
<p>Here it might be helpful to use the lingo of Object-oriented programming. (e.g. inheritance and polymorphism). Deﬁning these terms for a neuroscience audience would be useful as well.</p>
</disp-quote>
<p>We do not think it is necessary to use too much technical term in this manuscript. However, this sentence was indeed confusing. We have now simpliﬁed it:</p>
<p>“[…], users can develop their own custom I/O using available template classes. Pynapple already includes several of such templates and we expect this collection to grow in the future.”</p>
<disp-quote content-type="editor-comment">
<p>Page 10</p>
<list list-type="bullet">
<list-item><p>The authors wrote -</p>
</list-item></list>
<p>&quot;These analyses are powerful because they are able to describe the relationships between time series objects while requiring the fewest number of parameters to be set by the user.&quot;</p>
<p>It is not clear that this makes for a powerful analysis as opposed to an easy-to-use analysis.</p>
</disp-quote>
<p>We have changed “powerful” with “easy to use&quot;.</p>
<disp-quote content-type="editor-comment">
<p>Page 12</p>
<p>&quot;they are built-in and thus do not have any external dependencies&quot;</p>
<p>If the authors want to retain this, it would be helpful to explain (perhaps in the introduction) why having fewer external dependencies is useful. And is it true that these functions use only base python classes?</p>
</disp-quote>
<p>We have rephrased this sentence as follows:</p>
<p>“they are for the most part built-in and only depend on a few common external packages, ensuring that they can be used stand-alone without relying on packages that are at risk of not being maintained or of not being compatible in the near future.”</p>
<disp-quote content-type="editor-comment">
<p>Other comments:</p>
<list list-type="bullet">
<list-item><p>It would be helpful, as mentioned in the public review, to frame this work in the broader context of what is needed to go from data to scientiﬁc results so that people understand what this package does and does not provide.</p>
</list-item></list>
</disp-quote>
<p>We have added the following sentence to the discussion to make sure readers understand:</p>
<p>“The path from data collection to reliable results involves a number of critical steps: exploratory data analysis, development of an analysis pipeline that can involve custom-made developed processing steps, and ideally the use of that pipeline and others to replicate the results. Pynapple provides a platform for these steps.”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>It would also be helpful to describe the Pynapple so&lt;ware ecosystem as something that readers could contribute to. Note here that GNU may not be a good license. Technically, GNU requires any changes users make to Pynapple for their internal needs to be oﬀered back to the Pynapple team. Some labs may ﬁnd that burdensome or unacceptable. A workaround would be to have GNU and MIT licenses.</p>
</list-item></list>
</disp-quote>
<p>The main restriction of the GPL license is that if the code is changed by others and released, a similar license should be used, so that it cannot become proprietary. We therefore stick to this choice of license.</p>
<p>We would be more than happy to receive contributions from the community. To note, several users outside the lab have already contributed. We have added the following sentence in the introduction:</p>
<p>“As all users are also invited to contribute to the Pynapple ecosystem, this framework also provides a foundation upon which novel analyses can be shared and collectively built by the neuroscience community.”</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>This so&lt;ware shares some similarities with the nelpy package, and some mention of that package would be appropriate.</p>
</list-item></list>
</disp-quote>
<p>While we acknowledge the reviewer's observation that Nelpy is a similar package to Pynapple, there are several important diﬀerences between the two.</p>
<p>First, Nelpy includes predeﬁned objects such as SpikeTrain, BinnedSpikeTrain, and AnalogSignal, whereas Pynapple would use only Ts and Tsd for those. This design choice was made to provide greater ﬂexibility and allow users to deﬁne their own data structures as needed.</p>
<p>Second, Nelpy is primarily focused on electrophysiology data, whereas Pynapple is designed to handle a wider range of data types, including calcium imaging and behavioral data. This reﬂects our belief that the NWB format should be able to accommodate diverse experimental paradigms and modalities.</p>
<p>Finally, while Nelpy oﬀers visualization and high-level analysis tools tailored to electrophysiology, Pynapple takes a more general-purpose approach. We believe that users should be free to choose their own visualization and analysis tools based on their speciﬁc needs and preferences.</p>
<p>The package has now been cited.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Pynapple and Pynacollada have the potential to become very valuable and foundational tools for the analysis of neurophysiological data. NWB still has a steep learning curve and Pynapple oﬀers a user- friendly toolset that can also serve as a wrapper for NWB.</p>
<p>The scope of the manuscript is not clear to me, and the authors could help clarify if Pynacollada and other toolsets in the making become a future aspect of this paper (and Pynapple), or are the authors planning on building these as separate publications.</p>
<p>The author writes that Pynapple can be used without the I/O layer, but the author should clarify how or if Pynapple may work outside NWB.</p>
</disp-quote>
<p>Absolutely. Pynapple can be used for generic data analysis, with no requirement of speciﬁc inputs nor NWB data. For example, the lab is currently using it for a computational project in which the data are loaded from simple ﬁles (and not from full I/O functions as provided in the toolbox) for further analysis and ﬁgure generation.</p>
<p>This was already noted in the manuscript, last paragraph of the section “Importing data from common and custom pipelines”</p>
<p>“Third, users can still use Pynapple without using the I/O layer of Pynapple.”.</p>
<p>We have added the following sentence in the discussion</p>
<p>“To note, Pynapple can be used without the I/O layer and independent of NWB for generic, on-the-ﬂy analysis of data.”</p>
<disp-quote content-type="editor-comment">
<p>This brings us to an important fundamental question. What are the advantages of the current approach, where data is imported into the Ts objects, compared to doing the data import into NWB ﬁles directly, and then making Pynapple secondary objects loaded from the NWB ﬁle? Does NWB natively have the ability to store the 5 object types or are they initialized on every load call?</p>
</disp-quote>
<p>NWB and Pynapple are complimentary but not interdependent. NWB is meant to ensure long-term storage of data and as such contains a as much information as possible to describe the experiment. Pynapple does not use NWB to directly store the objects, however it can read from NWB to organize the data in Pynapple objects. Since the original version of this manuscript was submitted, new methods address this. Speciﬁcally, in the current beta version, each object now has a “save” method. Obviously, we are developing functions to load these objects as well. This does not depend on NWB but on npz, a Numpy speciﬁc ﬁle format. However, we believe it is a bit too premature to include these recent developments in the manuscript and prefer not to discuss this for now.</p>
<disp-quote content-type="editor-comment">
<p>Many of these functions and objects have a long history in MATLAB - which documents their usefulness, and I believe it would be ﬁsng to put further stress on this aspect - what aspects already existed in MATLAB and what is completely novel. A widely used MATLAB toolset, the FMA toolbox (the Freely moving animal toolbox) has not been cited, which I believe is a mistake.</p>
</disp-quote>
<p>We agree that the FMA toolbox should have been cited. This ha now been corrected.</p>
<p>Pynapple was ﬁrst developed in Matlab (it was then called TSToolbox). The ﬁrst advantage is of course that Python is more accessible than Matlab. It has also been adopted by a large community of developers in data analysis and signal processing, which has become without a doubt much larger than the Matlab community, making it possible to ﬁnd solutions online for virtually any problem one can have. Furthermore, in our experience, trainees are now unwilling to get training in Matlab.</p>
<p>Yet, Python has drawbacks, which we are fully aware of. Matlab can be very computationally eﬃcient, and old code can usually run without any change, even many years later.</p>
<disp-quote content-type="editor-comment">
<p>A limitation in using NWB ﬁles is its standardization with limited built-in options for derived data and additional metadata. How are derived data stored in the NWB ﬁles?</p>
</disp-quote>
<p>NWB has predetermined a certain number of data containers, which are most common in systems neuroscience. It is theoretically possible to store any kind of data and associated metadata in NWB but this is diﬃcult for a non-expert user. In addition, NWB does not allow data replacement, making is necessary to rewrite a whole new NWB ﬁle each time derived data are changed and stored. Therefore, we are currently addressing this issue as described above. Derived data and metadata will soon be easy to store and read.</p>
<disp-quote content-type="editor-comment">
<p>How is Pynapple handling an existing NWB dataset, where spikes, behavioral traces, and other data types have already been imported?</p>
</disp-quote>
<p>This is an interesting point. In theory, Pynapple should be able to open a NWB ﬁle automatically, without providing much information. In fact, it is challenging to open a NWB ﬁle without knowing what to look for exactly and how the data were preprocessed. This would require adapting a I/O function for a speciﬁc NWB ﬁle. Unfortunately, we do not believe there is a universal solution to this problem. There are solutions being developed by others, for example NWB Widgets (NWB Widgets). We will keep an eye on this and see whether this could be adapted to create a universal NWB loader for Pynapple.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>Other tools and solutions are being developed by the NWB community. How will you make sure that these tools can take advantage of Pynapple and vice versa?</p>
</disp-quote>
<p>We recognize the importance of collaboration within the NWB community and are committed to making sure that our tools can integrate seamlessly with other tools and solutions developed by the community.</p>
<p>Regarding Pynapple speciﬁcally, we are designing it to be modular and ﬂexible, with clear APIs and documentation, so that other tools can easily interface with it. One important thing is that we want to make sure Pynapple is not too dependent of another package or ﬁle format such as NWB. Ideally, Pynapple should be designed so that it is independent of the underlying data storage pipeline.</p>
<p>Most of the tools that have been developed in the NWB community so far were designed for data visualisation and data conversion, something that Pynapple does not currently address. Multiple packages for behavioral analysis and exploration of electro/optophysiological datasets are compatible with the NWB format but do not provide additional solutions per se. They are complementary to Pynapple.</p>
</body>
</sub-article>
</article>