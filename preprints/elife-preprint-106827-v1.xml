<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106827</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106827</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106827.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.6</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>Biologically informed cortical models predict optogenetic perturbations</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0007-0519-1116</contrib-id>
<name>
<surname>Sourmpis</surname>
<given-names>Christos</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3344-4495</contrib-id>
<name>
<surname>Petersen</surname>
<given-names>Carl CH</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4344-2189</contrib-id>
<name>
<surname>Gerstner</surname>
<given-names>Wulfram</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7568-4994</contrib-id>
<name>
<surname>Bellec</surname>
<given-names>Guillaume</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>guillweb@gmail.com</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Laboratory of Computational Neuroscience, Brain Mind Institute, School of Computer and Communication Sciences and School of Life Sciences, École Polytechnique Fédérale de Lausanne (EPFL)</institution></institution-wrap>, <city>Lausanne</city>, <country country="CH">Switzerland</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Laboratory of Sensory Processing, Brain Mind Institute, School of Life Sciences, École Polytechnique Fédérale de Lausanne (EPFL)</institution></institution-wrap>, <city>Lausanne</city>, <country country="CH">Switzerland</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04d836q62</institution-id><institution>Machine Learning Research Unit, Technical University of Vienna (TU Wien)</institution></institution-wrap>, <city>Vienna</city>, <country country="AT">Austria</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country country="FR">France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country country="GR">Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-06-16">
<day>16</day>
<month>06</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106827</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-03-21">
<day>21</day>
<month>03</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-03-15">
<day>15</day>
<month>03</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.27.615361"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Sourmpis et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Sourmpis et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106827-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>A recurrent neural network fitted to large electrophysiological datasets may help us understand the chain of cortical information transmission. In particular, successful network reconstruction methods should enable a model to predict the response to optogenetic perturbations. We test recurrent neural networks (RNNs) fitted to electrophysiological datasets on unseen optogenetic interventions, and measure that generic RNNs used predominantly in the field generalize poorly on these perturbations. Our alternative RNN model adds biologically informed inductive biases like structured connectivity of excitatory and inhibitory neurons and spiking neuron dynamics. We measure that some of the biological inductive biases can improve the model prediction under perturbation in a simulated dataset and a dataset recorded in mice in vivo. Furthermore, we show in simulations that gradients of the fitted RNN can predict the effect of micro-perturbations in the recorded circuits, and discuss potentials for measuring brain gradients or using gradient-targeted stimulation to bias an animal behavior.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We have added funding support and authors affiliations.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>A fundamental question in neuroscience is how cortical circuit mechanisms drive perception and behavior. To tackle this question, the field has been collecting large-scale electrophysiology datasets under reproducible experimental settings (<xref ref-type="bibr" rid="c48">Siegle et al., 2021</xref>; <xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="c56">Urai et al., 2022</xref>; <xref rid="c26" ref-type="bibr">International Brain Laboratory et al., 2023</xref>). However, the field lacks data-grounded modeling approaches to generate and test hypotheses on the causal role of neuronal and circuit-level mechanisms. To leverage the high information density of contemporary recordings, we need both (1) modeling approaches that scale well with data, and (2) metrics to quantify when the models provide a plausible mechanism for the observed phenomena.</p>
<p>Biophysical simulations have been crucial for our understanding of single-cell mechanisms (<xref ref-type="bibr" rid="c25">Hodgkin, 1958</xref>), and have been used to describe interactions across cortical layers, columns, and areas (<xref ref-type="bibr" rid="c33">Markram et al., 2015</xref>; <xref ref-type="bibr" rid="c8">Billeh et al., 2020</xref>; <xref ref-type="bibr" rid="c27">Isbister et al., 2023</xref>; <xref ref-type="bibr" rid="c10">Chen et al., 2022</xref>; <xref ref-type="bibr" rid="c45">Rimehaug et al., 2023</xref>; <xref ref-type="bibr" rid="c20">Fraile et al., 2023</xref>; <xref ref-type="bibr" rid="c50">Spieler et al., 2023</xref>). A promising approach to constrain models to electrophysiological data lies in the optimization of the simulation parameters by gradient descent. These methods were successful in quantitatively classifying functional cell types (<xref ref-type="bibr" rid="c42">Pozzorini et al., 2015</xref>; <xref ref-type="bibr" rid="c54">Teeter et al., 2018</xref>), and modeling micro-circuit interactions (<xref ref-type="bibr" rid="c41">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="c14">Deny et al., 2017</xref>; <xref ref-type="bibr" rid="c32">Mahuas et al., 2020</xref>). To bridge the gap from single neurons or small retinal networks to cortical recordings in vivo, recent studies made substantial progress towards data-constrained recurrent neural network (RNN) models (<xref ref-type="bibr" rid="c40">Perich et al., 2020</xref>; <xref ref-type="bibr" rid="c7">Bellec et al., 2021</xref>; <xref ref-type="bibr" rid="c3">Arthur et al., 2023</xref>; <xref ref-type="bibr" rid="c28">Kim et al., 2023</xref>; <xref ref-type="bibr" rid="c15">Dinc et al., 2023</xref>; <xref ref-type="bibr" rid="c49">Sourmpis et al., 2023</xref>; <xref ref-type="bibr" rid="c37">Pals et al., 2024</xref>). In this line of work, neurons in the RNN are mapped one-to-one to recorded cells and optimized by gradient descent to predict recorded activity at large.</p>
<p>An important question is whether these data-constrained RNNs can reveal a truthful mechanism of neuronal activity and behavior. By construction, the RNNs can generate brain-like network activity, but how can we measure whether the reconstructed network faithfully represents the biophysical mechanism? To answer this question, we submit a range of RNN reconstruction methods to a difficult <italic>perturbation test</italic> : we measure the similarity of the network response to unseen perturbations in the RNN and the recorded biological circuit.</p>
<p>Optogenetics is a powerful tool to induce precise causal perturbations in vivo (<xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="c22">Guo et al., 2014</xref>). It involves the expression of light-sensitive ion channels (<xref ref-type="bibr" rid="c2">Aravanis et al., 2007</xref>), such as channelrhodopsins, in specific populations of neurons (e.g., excitatory/pyramidal or inhibitory/parvalbumin-expressing). In this paper, we use datasets combining dense electrophysiological recordings with optogenetic perturbations to evaluate RNN reconstruction methods. Since the neurons in our RNNs are mapped one-to-one to the recorded cells, we can model optogenetic perturbations targeting the same cell-types and areas as done in vivo. Yet, we observe that the similarity between the simulated and recorded perturbations varies greatly depending on the reconstruction methods.</p>
<p>Most prominently, we study two opposite types of RNN specifications. First, as a control model, we consider a traditional sigmoidal RNN (<italic>σ</italic>RNN) which is arguably the most common choice for contemporary data-constrained RNNs (<xref ref-type="bibr" rid="c40">Perich et al., 2020</xref>; <xref ref-type="bibr" rid="c3">Arthur et al., 2023</xref>; <xref ref-type="bibr" rid="c37">Pals et al., 2024</xref>); and second, we develop a model with biologically informed inductive biases (bioRNN): (1) neuronal dynamics follow a simplified spiking neuron model, and (2) neurons associated with fast-spiking inhibitory cells have short-distance inhibitory projections (other neurons are excitatory with both local and long-range interareal connectivity). Following <xref ref-type="bibr" rid="c35">Neftci et al. (2019)</xref>; <xref ref-type="bibr" rid="c6">Bellec et al. (2018b</xref>, <xref ref-type="bibr" rid="c7">2021</xref>); <xref ref-type="bibr" rid="c49">Sourmpis et al. (2023)</xref>, we adapt gradient descent techniques to optimize the bioRNN parameters of neurons and synapses to explain the recorded neural activity and behavior.</p>
<p>Strikingly, we find that the bioRNN is more robust to perturbations than the <italic>σ</italic>RNN. This is nontrivial because it is in direct contradiction with other metrics often used in the field: the <italic>σ</italic>RNN simulation achieves higher similarity with unseen recorded trials before perturbation, but lower than the bioRNN on perturbed trials. This contradiction is confirmed both on synthetic and in vivo datasets. To analyze this result, we submit a spectrum of intermediate bioRNN models to the same <italic>perturbation tests</italic>, and identify two bioRNN model features that are most important to improve robustness to perturbation: (1) Dale’s law (the cell type constrains the sign of the connections (<xref ref-type="bibr" rid="c17">Eccles, 1976</xref>)), and (2) local-only inhibition (inhibitory neurons do not project to other cortical areas). We also find that other biological inductive biases like spiking neuron dynamics and a sparsity prior improve the robustness to perturbation but to a lesser extent.</p>
<p>Furthermore, a perturbation-robust bioRNN enables the prediction of the causal effect of perturbations in the recorded circuit. This becomes particularly interesting with micro-perturbations (<italic>µ</italic>-perturbation) targeting dozens of neurons in a small time window. We show in silico that the causal effect of <italic>µ</italic>-perturbations can be well approximated by the RNN gradients, which has two important implications for experimental neuroscience: (1) in a close-loop experimental setup, we can use RNN gradients to target a <italic>µ</italic>-perturbation which optimally increase (or decrease) movement in a simulated mouse (this is demonstrated in silico); (2) our RNN reconstruction methodology enables the estimation of gradients of the recorded circuit. So fitting RNNs becomes a tool to measure “brain gradients” and potentially relate contemporary in vivo measurement to decades of theoretical results from machine learning where the gradient is a foundational concept (<xref ref-type="bibr" rid="c30">LeCun et al., 2015</xref>; <xref ref-type="bibr" rid="c44">Richards and Kording, 2023</xref>).</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>Reconstructed Networks: biological inductive biases strengthen robustness to perturbations</title>
<sec id="s2a1">
<title>Synthetic dataset for challenging causal inference</title>
<p>We build a toy synthetic dataset to formalize how we intend to reverse engineer the mechanism of a recorded circuit using optogenetic perturbations and RNN reconstruction methods. It also serves as the first dataset to evaluate our network reconstruction methods. This toy example represents a simplified version of large-scale cortical recordings from multiple brain areas during a low-dimensional instructed behavior (<xref ref-type="bibr" rid="c52">Steinmetz et al., 2019</xref>; <xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>; <xref rid="c26" ref-type="bibr">International Brain Laboratory et al., 2023</xref>), similarly to the vivo dataset of a GO/No-Go task <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref> analyzed in the next section. Let’s consider two areas <italic>A</italic> and <italic>B</italic> which are either transiently active together (“hit trial” occurring with frequency <italic>p</italic>) or quiescent together (“miss trial” occurring with probability 1 − <italic>p</italic>). Since the two areas are active or in-active together, it is hard to infer if they are connected in a feedforward or recurrent fashion. In Method 4.1, we describe a theoretical example where it is impossible to decide between opposing mechanistic hypothesis (feedforward or recurrent) when recording only the macroscopic activations of areas <italic>A</italic> and <italic>B</italic>. In this case, performing optogenetic inactivation of one area is decisive to distinguish between the feedforward or recurrent hypothesis.</p>
<p>To generate artificial spike train recordings that capture this problem, we design two reference circuits (RefCircs) from which we can record the spike trains. Each RefCirc consist of two populations of 250 spiking neurons (80% are excitatory) representing areas <italic>A</italic> and <italic>B</italic>. To highlight the importance of optogenetic perturbations as in the Methods 4.1, the first circuit RefCirc1 is feedforward and the second Refcirc2 is recurrent: RefCirc1 (and not RefCirc2) has strictly zero feedback connections from <italic>B</italic> to <italic>A</italic>. Yet, the two RefCircs are almost identical without optogenetic perturbations: each neuron in RefCirc1 has been constructed to have an almost identical trial-averaged activity as the corresponding neuron in RefCirc2; and in response to a stimulus, the circuits display a similar bi-modal hit-or-miss response with a hit trial frequency <italic>p</italic> ≈ 50%. We consider that a trial is a hit if area <italic>A</italic> is active (averaged firing rate above 8Hz; Defining a “hit” trial based on area <italic>A</italic> is equivalent to saying that both areas need to be active during unperturbed trials with this dataset. But excluding <italic>B</italic> in this definition avoids that the hit rate is trivially impacted when manipulating the activity of <italic>B</italic> with optogenetic perturbations.). To simulate optogenetic inactivations of an area in the RefCircs, we inject a transient current into the inhibitory neurons, modeling the opening of light-sensitive ion channels (Symmetrically, an optogenetic activation is simulated as a positive current injected into excitatory cells). <xref rid="figS1" ref-type="fig">Figure S1</xref> shows that optogenetic perturbations in area B reflect the presence or absence of feedback connections which differs in RefCirc 1 and 2. Methods 4.4 provides more details on the construction of the artificial circuits. Our <italic>perturbation test</italic> will be to compare the effect of optogenetic perturbations in the reconstructed RNN and in the reference circuits RefCirc1 and 2.</p>
</sec>
<sec id="s2a2">
<title>Network reconstruction methodology (synthetic dataset)</title>
<p>To reconstruct the recorded circuits with an RNN, we record activity from the spiking RefCircs, and optimize the parameters of an RNN to generate highly similar network activity. The whole reconstruction method is summarized graphically in panel A of <xref rid="fig1" ref-type="fig">Figure 1</xref>. In the simplest cases, the RNN is specified as a sigmoidal network model (<xref ref-type="bibr" rid="c46">Rosenblatt, 1960</xref>; <xref ref-type="bibr" rid="c18">Elman, 1990</xref>): <italic>σ</italic>RNN1 and <italic>σ</italic>RNN2 are optimized to reproduce the recording from RefCirc1 and RefCirc2 respectively. In this synthetic dataset, the reconstructed <italic>σ</italic>RNNs have the same size as the RefCircs (500 neurons) and sigmoidal neurons are mapped one-to-one with RefCirc neurons (20% are mapped to inhibitory RefCirc neurons). They are initialized with all-to-all connectivity and are therefore blind to the structural difference of the RefCirc1 and 2 (feed-forward or recurrent). From each of the two RefCircs, we store 2,000 trials of simultaneous spike train recordings of all 500 neurons (step 1 in <xref rid="fig1" ref-type="fig">Fig. 1A</xref>). Half of the trials are used as training set and will be the basis for our data-driven RNN optimization. The second half of the recorded trials form the testing set and are used to evaluate the quality of the reconstruction before perturbations.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Network reconstruction and perturbation tests.</title>
<p><bold>A</bold>. The three steps to reconstruct the reference circuit (RefCirc) using a biologically informed RNN (bioRNN) or a simgoidal RNN (<italic>σ</italic>RNN) and evaluate the reconstruction based on perturbation tests. <bold>B</bold>. Summary of the differences between a bioRNN and a <italic>σ</italic>RNN. <bold>C</bold>. Example raster plots for bioRNN1 and <italic>σ</italic>RNN1, neurons fitted to spike trains from inhibitory cells in RefCirc are shown in red. <bold>D</bold>. Trial-averaged activity of area <italic>A</italic> of the two circuits during hit (black-dashed: RefCirc1; blue: bioRNN1; pink: <italic>σ</italic>RNN1) and miss (grey-dashed: RefCirc1; light blue: bioRNN1; light pink: <italic>σ</italic>RNN1) trials. All models display a hit rate of <italic>p</italic> 5 ≈ 0%. <bold>E</bold>. Same as <bold>D</bold> during inactivation of area <italic>B</italic>. <inline-formula><inline-graphic xlink:href="615361v6_inline60.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the recorded change of hit rate for the feedforward circuit RefCirc1, so a successful reconstruction achieves <inline-formula><inline-graphic xlink:href="615361v6_inline61.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p></caption>
<graphic xlink:href="615361v6_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We optimize the synaptic “weights” of the <italic>σ</italic>RNN to minimize the difference between its activity and that of the RefCirc (step 2 in <xref rid="fig1" ref-type="fig">Fig 1A</xref>, see Methods). The optimization combines three loss functions defined mathematically in Methods 4.5: (i) the neuron-specific loss function ℒ<sub>neuron</sub> is the mean-square error of the <italic>trial-averaged</italic> neural activity (e.g. the PSTH) between <italic>σ</italic>RNN and RefCirc neurons. (ii) To account for fluctuations of the single-trial network activity, we use a trial-specific loss function ℒ<sub>trial</sub>, which is the distance between the distribution of single trial population-averaged activity of <italic>σ</italic>RNN and RefCirc (see <xref ref-type="bibr" rid="c49">Sourmpis et al. (2023)</xref>). (iii) Finally, we add a regularization loss function ℒ<sub>reg</sub> to penalize unnecessary large weights.</p>
<p>We also developed a biologically informed RNN model (bioRNN) for which we have designed a successful optimization technique. The main differences between <italic>σ</italic>RNNs and bioRNNs consist in the following biological inductive biases. Firstly, the bioRNN neuron model follows a simplified leaky integrate and fire dynamics (see Methods 4.2) yielding strictly binary spiking activity. Secondly, we constrain the recurrent weight matrix to describe cell-type specific connectivity constraints: following Dale’s law, neurons have either non-negative or non-positive outgoing connections; moreover, since cortical inhibitory neurons rarely project across areas, we assume that inhibitory neurons project only locally within the same area. Thirdly, we add a term to the regularization loss ℒ <sub>reg</sub> to implement the prior knowledge that cross-area connections are more sparse than within an area. Adding these biological features into the model requires an adapted gradient descent algorithm and matrix initialization strategies (Methods 4.5). The reconstruction method with <italic>σ</italic>RNNs and bioRNNs is otherwise identical: the models have the same size, and are optimized on the same data, for the same number of steps and using the same loss functions. The two models bioRNN1 and bioRNN2 are optimized to explain recordings from RefCirc1 and Refcirc2, respectively. Importantly, the structural difference between RefCirc1 (feedforward) and RefCirc2 (feedback) is assumed to be unknown during parameter optimization: at initialization, excitatory neurons in bioRNN1 or bioRNN2 project to any neuron in the network with transmission efficacies (aka as synaptic weights) initialized randomly.</p>
<p>After parameter optimization, we have four models, <italic>σ</italic>RNN1, <italic>σ</italic>RNN2, bioRNN1 and bioRNN2, that we call “reconstructed” models. To validate the reconstructed models, we verify that the network trajectories closely match the data on the test set in terms of (i) the “behavioral” hit-trial frequency, (ii) the peristimulus time histogram (PSTH) mean-square error of single neurons as evaluated by ℒ <sub>neuron</sub>, and (iii) the distance between single-trial network dynamics as evaluated by ℒ <sub>trial</sub> (see <xref rid="figS2" ref-type="fig">Suppl. Fig. S2</xref> and Suppl. Table 1). At first sight, the <italic>σ</italic>RNN displays a better performance when comparing the activity simulated by the RNN and recordings held out in the testing set: ℒ <sub>trial</sub> is for instance lower with <italic>σ</italic>RNN (see Suppl. Table 1). This is expected considering that the optimization of bioRNNs is less flexible and efficient because of the sign-constrained weight matrix and the imperfect surrogate gradient approximation through spiking activity. However, the two bioRNNs are drastically more robust when evaluating the models with <italic>perturbation tests</italic>.</p>
</sec>
<sec id="s2a3">
<title>Perturbation test</title>
<p>To test which of the reconstructed RNNs capture the causal mechanisms of the RefCircs, we simulate optogenetic activations and inactivations of area <italic>B</italic> (step 3 in <xref rid="fig1" ref-type="fig">Fig. 1A</xref>). We first compare the change of hit probability after perturbations in the reconstructed RNN <inline-formula><inline-graphic xlink:href="615361v6_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and recorded in RefCirc <inline-formula><inline-graphic xlink:href="615361v6_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in <xref rid="fig2" ref-type="fig">Figure 2</xref>. For the <italic>σ</italic>RNN the activation or inactivation of area <italic>B</italic> changes drastically the peak firing rate in area <italic>A</italic>: all trials become a hit during inactivation of area <italic>B</italic>. This drastic increase of hit rate is not consistent the reference where the effect of the optogenetic inactivations is mild: the distribution of network responses remains bi-modal (hit versus miss) with only a moderate change of hit frequency for RefCirc2<inline-formula><inline-graphic xlink:href="615361v6_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. For RefCirc1 we even expect <inline-formula><inline-graphic xlink:href="615361v6_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> by design because of the absence of feedback connections from <italic>B</italic> to <italic>A</italic>. In contrast, the bioRNN models capture these changes more accurately (see <xref rid="fig1" ref-type="fig">Fig. 1</xref> and <xref rid="fig2" ref-type="fig">2</xref>). Quantitative results are summarized in <xref rid="fig2" ref-type="fig">Fig. 2C</xref>, the error of hit probability changes <inline-formula><inline-graphic xlink:href="615361v6_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is 7% with bioRNNs when averaged over all conditions (bioRNN1 and bioRNN2, with optogenetic inactivations and activations). The corresponding error is 48.5% on average for <italic>σ</italic>RNNs. In this sense, we argue that the bioRNN provides a better prediction of the perturbed hit frequency than the <italic>σ</italic>RNN. We also performed spike train recordings in the area that is not directly targeted by the light to compare the perturbed network dynamics in the fitted RNNs and the RefCirc. The perturbed dynamics are displayed in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>. The quantity <inline-formula><inline-graphic xlink:href="615361v6_inline5a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is a distance between the network dynamics (RNN versus reference) and is reported in <xref rid="fig2" ref-type="fig">Fig. 2D-E</xref> and Supplementary Table 1. Again, the perturbed dynamics of the bioRNN are more similar to those of the reference circuits <inline-formula><inline-graphic xlink:href="615361v6_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, than with the <italic>σ</italic>RNN <inline-formula><inline-graphic xlink:href="615361v6_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (t-test p-value is 0.0003).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Biologically informed RNNs predict optogenetic perturbations.</title>
<p><bold>A</bold>. Parameters of bioRNN1, bioRNN2, <italic>σ</italic>RNN1 and <italic>σ</italic>RNN2 are optimized to reconstruct RefCirc1 or RefCirc2 from spiking recordings. The bioRNNs and <italic>σ</italic>RNNs are blind to the structural difference of RefCirc1 and 2 and must infer this from the spiking data alone. BioRNN variants are defined by removing one of the biologically inspired features, for instance “No Dale’s law” refers to a bioRNN without sign constraints in the weight matrix, or removing the trial-matching loss function (No TM). <bold>B</bold>. Trial-averaged activity in area <italic>A</italic> under activation/inactivation of area <italic>B</italic>. Dashed black lines indicate the activity of RefCirc1 (thick dashed) and RefCirc2 (thin dashed). All the RNNs are tested with the same reference circuit and training data, each bioRNN model variant is shown with a different color. <bold>C</bold>. Error between the change of hit probability after perturbations in the RNN <inline-formula><inline-graphic xlink:href="615361v6_inline62.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and in the RefCirc<inline-formula><inline-graphic xlink:href="615361v6_inline63.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. <bold>D</bold>. The distance of network dynamics <inline-formula><inline-graphic xlink:href="615361v6_inline64.gif" mimetype="image" mime-subtype="gif"/></inline-formula> between each RNN and RefCirc, as a function of perturbation strength applied to area <italic>B</italic> (horizontal axis: light power in arbitrary units). <bold>E</bold>. Same quantity as <italic>D</italic> but averaged for each RNN under the strongest light power condition of perturbation of area <italic>B</italic> (activation/inactivation across the strongest power level). Statistical significance is computed using the mean over multiple network initializations and compared with the full bioRNN method, significance is indicated with 0 to 4 stars corresponding to p-values thresholds: 0.05, 0.01, 0.001 and 0.0001.</p></caption>
<graphic xlink:href="615361v6_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To analyze which features of bioRNN explain this robustness to perturbation, we then derive a family of models where only one feature of the reconstruction is omitted. Namely, the “No Dale’s law” model does not have excitatory and inhibitory weight constraints, the “Non-local inhibition” model allows inhibitory neurons to project outside of their areas, the “No Spike” model replaces the spiking dynamics with a sigmoidal neuron model, and the “No Sparsity” model omits the cross-area sparsity penalty in ℒ <sub>reg</sub>. Omitting all these features in bioRNN would be equivalent to using a <italic>σ</italic>RNN. The accuracy metrics on the testing sets before perturbation are reported for all RNN variants on the <xref rid="figS2" ref-type="fig">Suppl. Fig. 2C</xref> and <xref rid="figS2" ref-type="fig">F</xref>. For reference, we also include the model “No TM” (trial-matching) which omits the loss function ℒ <sub>trial</sub> during training.</p>
<p>The strongest effect measured with this analysis is that the Dale’s law and local inhibition explain most of the improved robustness of bioRNNs. This is visible in <xref rid="fig2" ref-type="fig">Fig. 2</xref> as the perturbed trajectories of “No Dale’s law” and “Non-local inhibition” are most distant from the reference in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>. This is confirmed numerically where both the hit-rate error and the distance of network dynamics increase the most when lifting these constraints (<xref rid="fig2" ref-type="fig">Fig. 2C-E</xref> and Supplementary Table 1. We explain this result as follows: the mono-synaptic effect of a cell stimulated by the light are always correct in bioRNN (according to Dale’s law, and inhibition locality), but often wrong in the alternative models (see <xref rid="fig2" ref-type="fig">Fig. 2A</xref>). For instance, a simple explanation may justify the failure of the “Non-local inhibition” model: the stimulation of inhibitory neurons in <italic>B</italic> induces (via the erroneous mono-synaptic inhibition) a reduction in the baseline activity in area <italic>A</italic> (see the green trace during inactivation in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>). More generally for <italic>perturbation testing</italic>, we speculate that these features are measured to be important here because they are central to the biophysical nature of the perturbation considered: optogenetic perturbation targets specific cell types, and these features incorporate a biophysical connectivity priors which is hard to infer entirely from the unperturbed data.</p>
<p>In comparison, it is less clear how spiking is relevant to model optogenetic perturbations. Considering that train spiking networks is a long-standing open question (<xref ref-type="bibr" rid="c9">Bohte et al., 2002</xref>; <xref ref-type="bibr" rid="c35">Neftci et al., 2019</xref>; <xref ref-type="bibr" rid="c58">Wunderlich and Pehle, 2021</xref>), it is technologically nontrivial however that the spiking bioRNN matches the performance of the “No Spike” model on both perturbation test metrics (hit-frequency in <xref rid="fig2" ref-type="fig">Figure 2C</xref> and distances of network dynamics ℒ <sub>trial</sub> <xref rid="fig2" ref-type="fig">Figure 2E</xref>). The spiking bioRNN is expected to achieve worse predictions because surrogate gradients <xref ref-type="bibr" rid="c35">Neftci et al. (2019)</xref> through the binarized spiking dynamics is an approximation. Indeed the <italic>σ</italic>RNN models achieves lower loss functions on unperturbed data (see Suppl. Table 1), but the fitting does not generalize to perturbations. On perturbed data, it is the spiking bioRNN which achieves slightly better performance (spiking is better on all metrics, but without significant margins, t-test p-value is 0.31 for<inline-formula><inline-graphic xlink:href="615361v6_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, see Suppl. Table 1 and <xref rid="fig2" ref-type="fig">Fig. 2E</xref>). Also, we speculate that simulating spikes will have a stronger predictive advantage on other perturbation test experiments where timing matters more than long-lasting area-wide activations.</p>
<p>So far, we have identified biological inductive biases of bioRNN that improve its robustness to perturbation. But the prediction error in <xref rid="fig2" ref-type="fig">Fig. 2B</xref> on the perturbed trajectory is still too large for our method to unequivocally reveal the characteristic differences between the feedforward RefCirc1 and the recurrent RefCirc2. To reveal this difference in the optimized bioRNN1 and bioRNN2 models, we find that adding a sparse connectivity prior across areas is necessary. This error of the “No Sparsity” model is visible in <xref rid="fig2" ref-type="fig">Fig. 2B</xref> (red curves): the PSTH traces for both RefCirc1 and 2 (in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>) display the early increase in area <italic>A</italic> characteristic of feedback from area <italic>B</italic> to <italic>A</italic>, it should not exist for RefCirc1. This is corrected with the sparsity prior (bioRNN, blue curves). Altogether on this dataset, the model with constrained weights, spiking activity, and sparsity prior predicted best the perturbed trials. It was also able to capture this characteristic difference between feedforward and recurrent circuits RefCirc1 and RefCirc2. We will now investigate whether the same approach can predict optogenetic perturbations with data recorded in vivo.</p>
</sec>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Predicting perturbations on in vivo electrophysiology data</title>
<p>To test whether our reconstruction method with biological inductive biases can predict optogenetic perturbations in large-scale recordings, we used the dataset from <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref>. In this study, water-deprived mice were trained to perform a whisker tactile detection task. In 50% of the trials (Go trials), a whisker is deflected, followed by a 1-second delay, after which an auditory cue signals the mice can lick a water spout to receive a water reward. In the other 50% of trials (No-Go trials), no whisker deflection occurs, and licking after the auditory cue results in a penalty with an extended time-out period. While the mice performed the task, experimenters recorded 6, 182 units from 12 areas across 18 mice. Using this dataset, we focused on the 6 most relevant areas for executing this task (<xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>). From each area, we randomly selected 250 neurons (200 putative excitatory and 50 putative inhibitory), which correspond to 1500 neurons in total. These areas, all shown to be causally involved in the task (<xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>), include the primary and secondary whisker sensory cortex (wS1, wS2), the primary and secondary whisker motor cortex (wM1, wM2), the anterior lateral motor cortex (ALM), and the primary tongue-jaw motor cortex (tjM1). We fit the neuronal activity using the same reconstruction method as used for the synthetic dataset. In the model, we simulate the jaw movement of the mouse as a linear readout driven by the model’s neural activity. This readout is regressed with the real jaw movement extracted from video footage. The parameter optimization of the behavioral readout is performed jointly with fitting the synaptic weights to the neuronal recordings, see Methods 4.5. After training, our reconstructed model can generate neural activity with firing rate distribution, trial-averaged activity, single-trial network dynamics and behavioral outcome which are all consistent with the recordings (see <xref rid="figS5" ref-type="fig">Suppl. Fig S5</xref>). Before perturbations, we observe again that the <italic>σ</italic>RNN model fits the testing set data better than the bioRNN model (see <xref rid="tbl2" ref-type="table">Table 2</xref> and <xref rid="figS5" ref-type="fig">Fig. S5</xref>).</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>Trial-matching loss test loss ℒ <sub>trial</sub> of the different reconstruction methods with the real recordings from (<xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>) ± indicates the 95% confidence interval.</p></caption>
<graphic xlink:href="615361v6_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>We then submit the reconstructed <italic>σ</italic>RNNs and bioRNNs models to <italic>perturbation tests</italic>. In the dataset, the behavior of the animal is tracked during inactivation of the an area at a given time window (stimulus, delay, or choice periods). For each of the six areas and time windows, we extract the averaged hit frequency under optogenetic inactivation, and attempt to predict this perturbed behavior by inducing the same inactivations to the fitted RNNs. These perturbations are acute spatiotemporal optogenetic inactivations of each area during different time periods (see <xref rid="fig3" ref-type="fig">Figure 3B</xref>). As an example, we show the effect of an inactivation of wS1 during the whisker period in the model in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. In panel C, we display the simulated trial of a fitted bioRNN with and without perturbations side by side. The two trials are simulated with the same random seed, and this example shows that an early perturbation in wS1 can change a lick decision from hit to miss in the model (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Predicting optogenetic perturbations for in vivo electrophysiology data</title>
<p><bold>A</bold>. During a delayed whisker detection task, the mouse reports a whisker stimulation by licking to obtain a water reward. Jaw movements are recorded by a camera. Our model simulates the jaw movements and the neural activity from six areas. <bold>B</bold>. The experimentalists performed optogenetic inactivations of cortical areas (one area at a time) in three temporal windows. <bold>C</bold>. Example hit trial of a reconstructed network (left). Using the same random seed, the trial turns into a miss trial if we inactivate area wS1 (right, light stimulus indicated by blue shading) during the whisker period by stimulation of inhibitory neurons (red dots). <bold>D</bold>. Error of the change in lick frequency caused by the perturbation, <inline-formula><inline-graphic xlink:href="615361v6_inline65.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is predicted by the model, and Δ<italic>p</italic><sup><italic>𝒟</italic></sup> is recorded in mice. Light-shaded circles show individual reconstructed networks with different initializations. The whiskers are the standard error of means. <bold>E</bold>. Examples of <inline-formula><inline-graphic xlink:href="615361v6_inline66.gif" mimetype="image" mime-subtype="gif"/></inline-formula> hit rate changes under perturbation for wS1 (Top) and tjM1 (Bottom). See <xref rid="figS6" ref-type="fig">Suppl. Fig. S6</xref> for the other areas.</p></caption>
<graphic xlink:href="615361v6_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We denote by Δ<italic>p</italic><sup><italic>D</italic></sup> the in vivo change in lick probability across Go trials in response to optogenetic pertubations. The perturbations were performed in different periods for each area in <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref> (stimulation, delay, or choice periods). For all areas and time windows, we measure the corresponding <inline-formula><inline-graphic xlink:href="615361v6_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the model. On average, the error change probability obtained with the <italic>σ</italic>RNN model is <inline-formula><inline-graphic xlink:href="615361v6_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> which is significantly worse than the bioRNN model’s 16% (t-test p-value is 0.014, see <xref rid="fig3" ref-type="fig">Figure 3D</xref>). As in the synthetic dataset, we find this to be consistent over multiple bioRNN model variant, and we find that imposing Dale’s law and local inhibition best explain the improvement in perturbation-robustness. We also measure that the spiking bioRNN predicts the change in lick probability slightly better than the “No Spike” bioRNN model. Conversely, adding the sparsity prior does not seem to improve the perturbed hit-rate prediction on the real data as seen in the recurrent artifical dataset (RefCirc2) and not in the feedforward case (RefCirc1) as shown in <xref rid="figS4" ref-type="fig">Suppl. Fig. S4</xref>.</p>
<p>To further analyze the consistency of the perturbations in the model, we can compare the perturbation map showing changes in lick probability obtained from acute inactivation in the data and the model. The <xref rid="figS6" ref-type="fig">Suppl. Fig. S6</xref> summarizes visually which area has a critical role at specific time points. The change of lick probability in area wS1, ALM and tjM1 are accurately predicted by the bioRNN. In contrast, our model tends to underestimate the causal effect induced by the inactivations of wS2, wM1 and wM2 (<xref rid="figS6" ref-type="fig">Suppl Fig S6</xref>). Overall, our model is consistent with a causal chain of interaction from wS1 to ALM and continuing to tjM1.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Applications for experimental electrophysiology</title>
<p>With future progress in recording technology and reconstruction methods, network reconstruction may soon predict the effect of optogenetic perturbation with even higher accuracy. In this section, we explore possible consequences and applications for experimental electro-physiology. We demonstrate in the following that (1) perturbation-robust bioRNNs enable us to estimate gradients of the recorded circuits, (2) which in turn enable us to target <italic>µ</italic>-perturbations in the recorded circuit and optimally increase (or decrease) induced movements in our simulated mouse. The so-called “recorded circuit” is a bioRNN trained on the in vivo dataset that we use as a proxy experimental preparation. Its mathematical underpinnings enables us for rigorous theoretical considerations and the design of forward-looking in silico experiments.</p>
<sec id="s2c1">
<title><italic>µ</italic>-perturbations measure brain gradients</title>
<p>We first prove a mathematical relationship between gradients in the recorded circuit and <italic>µ</italic>-perturbations. We define the integrated movement as <inline-formula><inline-graphic xlink:href="615361v6_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>y</italic><sub><italic>t</italic></sub> is the movement of the jaw movement at time <italic>t</italic> generated by the model, and we denote Δ<italic>Y</italic> <sup>7</sup> as the change of movement caused by the <italic>µ</italic>-perturbation. If the circuit has well-defined gradients (e.g. say a “No spike” bioRNN model trained on the in vivo recordings in the previous section), using a Taylor expansion, we find that:
<disp-formula id="eqn1">
<graphic xlink:href="615361v6_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where ℐ are the neuron and time indices selected for the optogenetic intervention. The error term <italic>ϵ</italic> is negligible when the current <inline-formula><inline-graphic xlink:href="615361v6_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula> induced by the light is small. We first confirm this approximation with numerical visualization in <xref rid="fig4" ref-type="fig">Fig. 4A</xref>: we display movement perturbations ⟨Δ<italic>Y ↯</italic>⟩ in the circuit with time windows of decreasing sizes (⟨·⟩ indicates a trial average). When the time window is small, and the perturbation is only applied to excitatory or inhibitory cells in <xref rid="fig4" ref-type="fig">Fig. 4A</xref>, one can appreciate visually the similarity with the binned gradient <inline-formula><inline-graphic xlink:href="615361v6_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in <xref rid="fig4" ref-type="fig">Fig. 4B</xref>. Proceeding to a quantitative verification of equation (1), we now compare the effect of small perturbations targeting only 20 neurons on a singletrial. We use the gradient <inline-formula><inline-graphic xlink:href="615361v6_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (see <xref rid="fig4" ref-type="fig">Fig. 4C</xref>) to predict the outcome of <italic>µ</italic>-perturbation as follows: for each trial, and each 100ms time window, we identify 20 neurons in the model with highest (or lowest) gradients<inline-formula><inline-graphic xlink:href="615361v6_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We then re-simulate the exact same trial with identical random seed, but induce a <italic>µ</italic>-perturbation on selected neurons (see rectangles in <xref rid="fig4" ref-type="fig">Figure 4</xref>). If we target neurons with strongly positive gradients, the perturbed jaw movements are strongly amplified Δ<italic>Y ↯ &gt;</italic> 0; conversely, if we target neurons with negative gradients the jaw movements are suppressed Δ<italic>Y ↯ &lt;</italic> 0. Although the equation (1) is only rigorously valid for models with well-defined gradients like the “No Spike” model, we also confirm in <xref rid="fig4" ref-type="fig">Fig. 4D</xref> that this numerical verification also holds in a spiking circuit model where the gradients are replaced with surrogate gradients (<xref ref-type="bibr" rid="c35">Neftci et al., 2019</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Measuring circuit gradients with <italic>µ</italic>-perturbations</title>
<p><bold>A-B</bold>. Numerical verification for equation (1). <bold>A</bold> shows the change of jaw movement Δ<italic>Y</italic> following inactivations in a “No Spike” bioRNN. From left to right, we reduce the size of the spatiotemporal window for the optogenetic stimulation. <bold>B</bold>. Gradients values <inline-formula><inline-graphic xlink:href="615361v6_inline67.gif" mimetype="image" mime-subtype="gif"/></inline-formula> that approximate Δ<italic>Y</italic> from <bold>A</bold> using equation (1). <bold>C-D</bold>. Verification that gradients predict the change of movement on single trials. In <bold>C</bold>, we display the gradients and jaw movement for three different trials, the neurons targeted by the <italic>µ</italic>-perturbation are boxed and the perturbed jaw movement is blue. Results averaged for every 100ms stimulation windows are shown in <bold>D</bold>: positive (resp. negative) modulated means that the 20 neurons with highest (resp. lowest) gradients are targeted, random neurons are selected for the shuffled case.</p></caption>
<graphic xlink:href="615361v6_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>An implication of equation (1) i s that the measurements ⟨Δ<italic>Y ↯</italic>⟩ that can be recorded in vivo are estimates of the gradients <inline-formula><inline-graphic xlink:href="615361v6_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the recorded circuit. Yet, measuring detailed gradient maps (or perturbation maps) as displayed in <xref rid="fig4" ref-type="fig">Fig. 4</xref> would be costly in vivo as it requires to average Δ<italic>Y ↯</italic> over dozens of trials for each spatio-temporal window. Instead, gradient calculation in a bioRNN model (that was fitted to the experimental preparation) is a rapid mathematical exercise. If the extracted model is valid, then the gradients <inline-formula><inline-graphic xlink:href="615361v6_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the bioRNN approximate (1) the effect of <italic>µ</italic>-perturbations Δ<italic>Y</italic> <sup>7</sup> in the experimental preparation; (2) the gradient <inline-formula><inline-graphic xlink:href="615361v6_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the recorded circuit.</p>
</sec>
<sec id="s2c2">
<title>Targeting in vivo <italic>µ</italic>-perturbations with bioRNN gradients</title>
<p>Building on this theoretical finding, we build a speculative experimental setup where the bioRNN gradients are used to target a <italic>µ</italic>-perturbation and increase (or decrease) the movements <italic>Y</italic> in the experimental preparation in real time. We show a schematic of this speculative close-loop experiment in <xref rid="fig5" ref-type="fig">Fig. 5C</xref> extending contemporary read-write elecotrophysiology setups (<xref ref-type="bibr" rid="c36">Packer et al., 2015</xref>; <xref ref-type="bibr" rid="c1">Adesnik and Abdeladim, 2021</xref>; <xref ref-type="bibr" rid="c21">Grosenick et al., 2015</xref>; <xref ref-type="bibr" rid="c39">Papagiakoumou et al., 2020</xref>). We demonstrate in silico in <xref rid="fig5" ref-type="fig">Fig. 5A-B</xref> how this experiment could use bioRNN gradients to bias the simulated mouse movement <italic>Y</italic>. As a preparation step, and before applying perturbations, we assume that the bioRNN is well fitted to the recorded circuit and we collect a large databank ℬ of simulated trials from the fitted bioRNN. Then in real-time, we record the activity from the experimental preparation until the time <italic>t</italic><sup><italic>∗</italic></sup> at which the stimulation will be delivered (Step 1 in <xref rid="fig5" ref-type="fig">Fig. 5A</xref>, <italic>t</italic><sup><italic>∗</italic></sup> is 100ms before the decision period). Rapidly, we find the trial with the closest spike trains in the databank of simulated trials (Step 2) and use the corresponding gradient maps to target neurons with the highest gradient <inline-formula><inline-graphic xlink:href="615361v6_inline19.gif" mimetype="image" mime-subtype="gif"/></inline-formula> in the model (Step 3). The targeted stimulation is then delivered immediately at <italic>t</italic><sup><italic>∗</italic></sup> to the experimental preparation (Step 4). When testing this in silico on our artificial experimental preparation, we show in <xref rid="fig5" ref-type="fig">Fig. 5C</xref> that this approach can bias the quantity of jaw movement <italic>Y</italic> driven by the circuit in a predictable way. The amount of movement is consistently augmented if we target neurons with the highest <inline-formula><inline-graphic xlink:href="615361v6_inline20.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (or diminished if we target neurons with the lowest<inline-formula><inline-graphic xlink:href="615361v6_inline21.gif" mimetype="image" mime-subtype="gif"/></inline-formula>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Gradient targeted <italic>µ</italic>-perturbations could precisely bias an animal behavior</title>
<p><bold>A</bold>. Protocol to deliver an optimal <italic>µ</italic>-perturbation on the experimental preparation based on jaw gradients. (Step 1) The circuit is recorded until stimulation time <italic>t</italic><sup><italic>∗</italic></sup>. (Step 2) The closest bioRNN trial to the ongoing recorded trial is retrieved from the databank. ℬ (Step 3) We select the neurons with the highest (or lowest) gradient value for the <italic>µ</italic>-perturbation. (Step 4) The <italic>µ</italic>-perturbation is delivered at <italic>t</italic><sup><italic>∗</italic></sup>. <bold>B</bold>. Effect of the <italic>µ</italic>-perturbation using the artificial setup <bold>A</bold> under different light protocols. Practically, for “High gradient”, we keep step 3 as it is, for “Low gradient”, we change the sign of the gradient, and for “Zero gradient”, we pick the 40 neurons with lowest gradient norm. <bold>C</bold>. Speculative schematic of a close-up setup implementing the protocol <bold>A</bold> inspired by the all optical “read-write” setup from <xref ref-type="bibr" rid="c36">Packer et al. (2015)</xref>.</p></caption>
<graphic xlink:href="615361v6_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>Finding the right level of detail to model recorded phenomena has sparked intensive debates in computational neuroscience. Our results show that the co-development of <italic>perturbation tests</italic> and biological modeling provides a quantitative measurement to address this problem. <italic>Perturbation tests</italic> can characterize two types of incomplete brain models: (1) physiologically detailed models intended to explain brain mechanisms but do not enable quantitative predictions; (2) machine learning models with high predictive power but capturing a wrong causal mechanism causing erroneous generalizations.</p>
<p>Our reconstruction method uses data-constrained RNNs with informed inductive biases, and is validated on <italic>perturbation tests</italic>. We achieve a reconstruction of the sensory-motor pathway in the mouse cortex during a sensory detection task from electrophysiology data. The model is optimized to explain electrophysiological recordings and generalizes better than standard models to in vivo optogenetic interventions. We found unambiguously that anatomically informed sign and connectivity constraints for dominant excitatory and inhibitiory cell types improve the model robustness to optogenetic perturbations. In hindsight, we conclude that adding biological constraints becomes beneficial when (1) they model the interaction between the circuit and the perturbation mechanism; (2) their implementation should not impair the efficiency of the optimization process. Broadly speaking this hindsight is also supported by other results elsewhere in neuroscience. For instance: biologically inspired topological maps improve convolutional models of the Monkey’s visual system <xref ref-type="bibr" rid="c47">Schrimpf et al. (2024)</xref>, and detailed cell-type distribution and connectome improve vision model in the fly brain <xref ref-type="bibr" rid="c29">Lappalainen et al. (2023)</xref>; <xref ref-type="bibr" rid="c12">Cowley et al. (2024)</xref>. For future work, there is a dense knowledge of unexploited physiological data at the connectivity, laminar or cell-type level that could be added to improve a cortical model like ours (<xref ref-type="bibr" rid="c23">Harris et al., 2019</xref>; <xref ref-type="bibr" rid="c31">Liu et al., 2022</xref>; <xref ref-type="bibr" rid="c55">Udvary et al., 2022</xref>; <xref ref-type="bibr" rid="c51">Staiger and Petersen, 2021</xref>; <xref ref-type="bibr" rid="c45">Rimehaug et al., 2023</xref>). By submitting the extended models to the relevant <italic>perturbation tests</italic>, it becomes possible to measure quantitatively the goodness of their biological mechanism implementations. We do not rule out, that significant improvements on <italic>perturbation tests</italic> can also be achieved with other means (e.g. by training deep learning architectures <xref ref-type="bibr" rid="c4">Azabou et al. (2024)</xref>; <xref ref-type="bibr" rid="c38">Pandarinath et al. (2018)</xref>; <xref ref-type="bibr" rid="c60">Ye et al. (2023)</xref> on larger datasets to enable generalization, or with generic regularization techniques like low-rank connectivity <xref ref-type="bibr" rid="c16">Dubreuil et al. (2022)</xref>; <xref ref-type="bibr" rid="c57">Valente et al. (2022)</xref>). However, in a similar way as the <italic>σ</italic>RNN was apriori an excellent predictor on our initial test-set, any powerful brain model will likely have failure modes that can be well characterized and measured with an appropriate perturbation test. So <italic>perturbation tests</italic> could become a central component of an iterative loop to identify needed data collection or model improvements towards robust brain models.</p>
<p>To highlight the importance of perturbation-robust circuit models, we have discussed possible implications for experimental neuroscience in section 2.3. We have shown that network reconstructions generate detailed perturbation sensitivity maps from non-perturbed recordings only. These sensitivity maps correspond to the RNN gradients and can be used to make targeted micro-stimulation in vivo. As a result we could design a hypothetical close-loop setup combining read-write electrophysiology with a brain model to influence the brain activity or behavior, having potentially important practical and ethical consequences. More conceptually, we have shown theoretically that the gradients of a perturbation robust RNN are also consistent with the gradients of the true recorded circuits. In perspective with the foundational role of gradients in machine learning theory <xref ref-type="bibr" rid="c30">LeCun et al. (2015)</xref>; <xref ref-type="bibr" rid="c44">Richards and Kording (2023)</xref>, it enables the measurement of “brain gradients” and lays a computational link between in vivo experimental research and decades of theoretical results on artificial learning and cognition.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Mathematical toy model of the difficult causal inference between H1 and H2</title>
<p>Let’s consider two simplistic mathematical models that both depend on two binary random variables <italic>A</italic> and <italic>B</italic> which represent that putative area A is active as <italic>A</italic> = 1 and area B as <italic>B</italic> = 1. With this notation, we can construct two hypothetical causal mechanisms <italic>H</italic>1 (“a feedforward hypothesis”) and <italic>H</italic>2 (“a recurrent hypothesis”), which are radically different. The empirical frequency <italic>p</italic>(<italic>A, B</italic>) of the outcome does not allow us to differentiate whether the system was generated by a feedforward mechanism <italic>H</italic>1 or a recurrent mechanism <italic>H</italic>2. Schematically, we can represent the two mechanism hypotheses as follows:
<disp-formula id="eqn2">
<graphic xlink:href="615361v6_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="615361v6_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For hypothesis <italic>H</italic>1: we assume that external inputs are driving the activity of area A such that <italic>A</italic> = 1 is active with probability <italic>p</italic><sub>0</sub>, and there are strong feed-forward connections from <italic>A</italic> to <italic>B</italic> causing systemically <italic>B</italic> = 1 as soon as <italic>A</italic> = 1. Alternatively, in <italic>H</italic>2, we assume that areas A and B receive independent external inputs with probability <inline-formula><inline-graphic xlink:href="615361v6_inline22.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Each of these two inputs is sufficient to cause <italic>A</italic> = 1 or <italic>B</italic> = 1, and the two areas are also strongly connected, so <italic>A</italic> = 1 always causes <italic>B</italic> = 1 and vice versa. Under these hypothetical mechanisms <italic>H</italic>1 and <italic>H</italic>2, one finds that the empirical probability table <italic>p</italic>(<italic>A, B</italic>) is identical: <inline-formula><inline-graphic xlink:href="615361v6_inline22a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (“Hit trial”, both areas are active), <italic>p</italic>(<italic>A</italic> = 0, <italic>B</italic> = 0) = 1 − <italic>p</italic><sub>0</sub> (“Miss trial”, the areas are quiescent) (To prove this: <italic>a</italic> and <italic>b</italic> to denote the binary external inputs into <italic>A</italic> and <italic>B</italic>, so we have: <inline-formula><inline-graphic xlink:href="615361v6_inline58.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where we used that <italic>p</italic>(<italic>A</italic> = 1, <italic>B</italic> = 1|<italic>a, b</italic>) is 0 or 1, then using <italic>p</italic>(<italic>a</italic> = 1) = <italic>p</italic>(<italic>b</italic> = 1) = <italic>p</italic>1 and the independence between <italic>a</italic> and <italic>b</italic> we find: <inline-formula><inline-graphic xlink:href="615361v6_inline59.gif" mimetype="image" mime-subtype="gif"/></inline-formula>). In both cases, the possibility that only one area is active is excluded by construction. So for any <italic>A</italic> and <italic>B p</italic><sub><italic>H</italic>1</sub>(<italic>A, B</italic>) = <italic>p</italic><sub><italic>H</italic>2</sub>(<italic>A, B</italic>) and in other words, even if we observe an infinite number of trials and compute any statistics of the binary activations <italic>A</italic> and <italic>B</italic>, discriminating the two possible causal interactions (H1 versus H2) is impossible.</p>
<p>A solution to discriminate between hypotheses <italic>H</italic>1 and <italic>H</italic>2 is to induce a causal perturbation. We can discriminate between our two hypotheses if we can impose a perturbation that forces the inactivation of area <italic>B</italic> in both mathematical models. In mathematical terms we refer to the <italic>do</italic> operator from causality theory. Under the feedforward mechanism <italic>H</italic>1 and inactivation of <italic>B, A</italic> is not affected <italic>p</italic><sub><italic>H</italic>1</sub> (<italic>A</italic> = 1 | <italic>do</italic> (<italic>B</italic> = 0)) = <italic>p</italic><sub>0</sub>. Under the recurrent hypothesis, <italic>H</italic>2, and inactivation of <italic>B, A</italic> is activated only by its external input such that <italic>p</italic><sub><italic>H</italic>2</sub> (<italic>A</italic> = 1 | <italic>do</italic> (<italic>B</italic> = 0)) = <italic>p</italic>1 ≠ <italic>p</italic><sub>0</sub>. So the measurement of the frequency of activation of area <italic>A</italic> under inactivation of <italic>B</italic> can discriminate between <italic>H</italic>1 and <italic>H</italic>2 which illustrates mathematically how a causal perturbation can be decisive to discriminate between those two hypothetical mechanisms.</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Neuron and jaw movement model</title>
<p>We model neurons as leaky-integrate and fire (LIF) neurons. The output of every neuron <italic>j</italic> at time <italic>t</italic> is a binary outcome <inline-formula><inline-graphic xlink:href="615361v6_inline23.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (spike if<inline-formula><inline-graphic xlink:href="615361v6_inline24.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, no spike if<inline-formula><inline-graphic xlink:href="615361v6_inline25.gif" mimetype="image" mime-subtype="gif"/></inline-formula>) generated from its membrane voltage<inline-formula><inline-graphic xlink:href="615361v6_inline26.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The following equations give the dynamics of the membrane voltage <inline-formula><inline-graphic xlink:href="615361v6_inline27.gif" mimetype="image" mime-subtype="gif"/></inline-formula>:
<disp-formula id="eqn4">
<graphic xlink:href="615361v6_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="615361v6_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="615361v6_inline28.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="615361v6_inline29.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are the recurrent and input weight matrices. The timestep of the simulation <italic>δt</italic> is 2 ms when we simulate the real dataset and 1 ms otherwise. The superscript <italic>d</italic> denotes the synaptic delay; every synapse has one synaptic delay of either 2 or 3 ms. With<inline-formula><inline-graphic xlink:href="615361v6_inline30.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, we define the integration speed of the membrane voltage, where <italic>τ</italic><sub><italic>m</italic></sub> = 30 ms for excitatory and <italic>τ</italic><sub><italic>m</italic></sub> = 10 ms for inhibitory neurons. The noise source <inline-formula><inline-graphic xlink:href="615361v6_inline31.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is a Gaussian random variable with zero mean and standard deviation <inline-formula><inline-graphic xlink:href="615361v6_inline32.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is typically initialized at 0.14). The input <inline-formula><inline-graphic xlink:href="615361v6_inline32a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is a binary pulse signal with a duration of 10 ms. For the real dataset, we have two binary pulse input signals, one for the whisker deflection and one for the auditory cue. The spikes are sampled with a Bernoulli distribution <inline-formula><inline-graphic xlink:href="615361v6_inline33.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>v</italic><sub>0</sub> is the temperature of the exponential function and <italic>v</italic><sub><italic>thr,j</italic></sub> is the effective membrane threshold. After each spike, the neuron receives a reset current with an amplitude of <italic>v</italic><sub><italic>trh,j</italic></sub> and enters an absolute refractory period of 4 ms, during which it cannot fire.</p>
<p>For networks fitted to the real dataset, we also simulate the jaw movement. The jaw movement trace <italic>y</italic> is controlled by a linear readout from the spiking activity of all excitatory neurons. Specifically, <italic>y</italic> is computed as<inline-formula><inline-graphic xlink:href="615361v6_inline34.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>b</italic> is a scaling parameter and <inline-formula><inline-graphic xlink:href="615361v6_inline35.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is given by <inline-formula><inline-graphic xlink:href="615361v6_inline36.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Here, <inline-formula><inline-graphic xlink:href="615361v6_inline37.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the output weight matrix (linear readout) for the jaw, and <italic>τ</italic><sub><italic>jaw</italic></sub> = 5ms defines<inline-formula><inline-graphic xlink:href="615361v6_inline38.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which controls the integration velocity of the jaw trace.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Session-stitching and network structure</title>
<p>As in (<xref ref-type="bibr" rid="c49">Sourmpis et al., 2023</xref>), we simulate multi-area cortical neuronal activity fitted to electrophysiology neural recordings. Before we start the optimization, we define and fix each neuron’s area and cell type in the model by uniquely assigning them to a neuron from the recordings. For the real dataset from <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref>, the cell type is inferred from the cell’s action potential waveform (with fast-spiking neurons classified as inhibitory and regular-spiking neurons as excitatory). Most electrophysiology datasets include recordings from multiple sessions, and our method would typically require simultaneous recordings of all neurons. To address this challenge, similarly to (<xref ref-type="bibr" rid="c49">Sourmpis et al., 2023</xref>) we use the technique called “session-stitching” which allows neighboring modeled neurons to be mapped with neurons recorded across multiple sessions. This effectively creates a “collage” that integrates data from multiple sessions within our model. This approach has practical implications for our optimization process. Specifically, the trial-matching loss includes a term for each session, with the overall loss calculated as the average across all sessions (see 4.5).</p>
<p>For both the real and the synthetic datasets, we simulate each area with 250 LIF neurons and impose that each area has 200 excitatory neurons and 50 inhibitory. Respecting the observation that inhibitory neurons mostly project in the area that they belong to (<xref ref-type="bibr" rid="c53">Tamamaki and Tomioka, 2010</xref>; <xref ref-type="bibr" rid="c34">Markram et al., 2004</xref>), we don’t allow for across-area inhibitory connections. The “thalamic” input is available to every neuron of the circuit, and the “motor” output for the real dataset, i.e., jaw movement, is extracted with a trained linear readout from all the excitatory neurons of the network, see 4.2.</p>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Reference circuits for hypotheses 1 and 2</title>
<p>To build a synthetic dataset that illustrates the difficulty of separating the feedforward (H1) and recurrent hypotheses (H2), we construct two reference spiking circuit models RefCirc1 and RefCirc2. The two networks consists of two areas A and B, and their activity follows the hard causal inference problem from method 4.1, making it hard to distinguish A1 and A2 when recording the co-activation of A and B. Moreover, to make the problem even harder, the two networks are constructed to make it almost impossible to distinguish between H1 and H2 with dense recordings: the two circuits are designed to have the same PSTH and single-trial network dynamics despite their structural difference, one is feedforward and the other is recurrent.</p>
<p>To do so, RefCirc1 and 2 are circuit models that start from random network initializations following the specifications described in Methods 4.2 and 4.3. The only difference is that we do not allow feedback connections from A to B in RefCirc1, the construction below is otherwise identical. The synaptic weights of the two circuits are optimized with the losses described in Methods 4.5 to fit the identical target statistics in all areas: the same PSTH activity for each neuron and the same distribution of single-trial network dynamics. The target statistics are chosen so the activity in RefCirc1 and 2 resemble kinematics and statistics from a primary and a secondary sensory area. The baseline firing rates of the neurons is dictated by the target PSTH distribution and it follows a log-normal distribution, with excitatory neurons having a mean of 2.9 Hz and a standard deviation of 1.25 Hz and inhibitory neurons having a mean of 4.47 Hz and a standard deviation of 1.31 Hz. The distribution of single-trial activity is given by the targeted single-trial dynamics: in RefCirc1 and 2, the areas A and B respond to input 50% of the time with a transient population average response following a double exponential kernel characterized by <italic>τ</italic><sub><italic>rise</italic></sub> = 5 ms and <italic>τ</italic><sub><italic>fall</italic></sub> = 20 ms. Mimicking a short signal propagation between areas, these transients have a 4 ms delay in area A and 12 ms delay in B (relative to the onset time of the stimulus). To impose a “behavioral” hit versus miss distribution that could emerge from a feedforward and recurrent hypothesis (see method 4.1), the targeted population-averaged response of each trial is either a double-exponential transient in both area A and B (”Hit trials”), or remains at a baseline level in both areas (”Miss trials”) in the remaining trials. At the end of the training, we verified that RefCirc1 and RefCirc2 generate very similar network activity in the absence of perturbation (see <xref rid="figS1" ref-type="fig">Figure S1</xref>). The circuits are then frozen and used to generate the synthetic dataset. We generate 2000 trials from these RefCircs, 1000 of which are used for the training set and 1000 for the testing set.</p>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Optimization and loss function</title>
<p>The optimization method we use to fit our models is back-propagation through time (BPTT). To overcome the non-differentiability of the spiking function, we use surrogate gradients (<xref ref-type="bibr" rid="c35">Neftci et al., 2019</xref>). In particular, we use the piece-wise linear surrogate derivative from <xref ref-type="bibr" rid="c6">Bellec et al. (2018b)</xref>. For the derivative calculations, we use <inline-formula><inline-graphic xlink:href="615361v6_inline39.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and not exp<inline-formula><inline-graphic xlink:href="615361v6_inline40.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We use sample-and-measure loss functions that rely on summary statistics, as in (<xref ref-type="bibr" rid="c7">Bellec et al., 2021</xref>; <xref ref-type="bibr" rid="c49">Sourmpis et al., 2023</xref>), to fit the networks to the data. Our loss function has two main terms: one to fit the trial-averaged activity of every neuron (ℒ <sub>neuron</sub>), and one to fit the single trial population average activity (ℒ <sub>trial</sub>), = ℒ <sub>neuron</sub> + ℒ <sub>trial</sub>. The two terms of the loss function are reweighted with a parameter-free multi-task method (<xref ref-type="bibr" rid="c13">Défossez et al., 2023</xref>) that enables the gradients to have comparable scales.</p>
<p>As in <xref ref-type="bibr" rid="c49">Sourmpis et al. (2023)</xref>: (1) To calculate the <bold>trial-averaged loss</bold>, we first filter the trial-averaged spiking activity <inline-formula><inline-graphic xlink:href="615361v6_inline41.gif" mimetype="image" mime-subtype="gif"/></inline-formula> using a rolling average window (<italic>f</italic>) of 8 ms. We then normalize it by the trial-averaged filtered data activity, (<bold><italic>z</italic></bold><sup><italic>D</italic></sup> are recorded spike trains)
<disp-formula id="eqn6">
<graphic xlink:href="615361v6_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where ⟨. <italic>⟩</italic> <sub><italic>t</italic></sub> is the time average, and <italic>σ</italic><sub><italic>t</italic></sub> the standard deviation over time. The trial-averaged loss function is defined as:
<disp-formula id="eqn7">
<graphic xlink:href="615361v6_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>T</italic> is the number of time points in a trial and <italic>N</italic> is the number of neurons. For the real dataset, where we want to fit also the jaw movement, we have an additional term for the trial-averaged filtered and normalized jaw, <inline-formula><inline-graphic xlink:href="615361v6_inline42.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>y</italic> is the simulated jaw movement and <italic>y</italic><sup><italic>D</italic></sup> the recorded jaw movement.</p>
<p>(2)To calculate the <bold>trial-matching loss</bold>, we first filter the population-average activity of each area <inline-formula><inline-graphic xlink:href="615361v6_inline43.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, using a rolling average window of 32 ms. We then normalize it by the population-averaged filtered activity of the same area from the recordings, <inline-formula><inline-graphic xlink:href="615361v6_inline44.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and concatenate all the areas that were simultaneously recorded, <inline-formula><inline-graphic xlink:href="615361v6_inline45.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where ⟨.⟩<sub><italic>k</italic></sub> is the trial average, and <italic>σ</italic><sub><italic>k</italic></sub> the standard deviation over trials. The trial-matching loss is defined as:
<disp-formula id="eqn8">
<graphic xlink:href="615361v6_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>π</italic> is an assignment between pairs of <italic>K</italic> recorded and generated trials <italic>π</italic> : {1, … <italic>K</italic>} → {1, … <italic>K</italic>}. Note that the minimum over <italic>π</italic> is a combinatorial optimization that needs to be calculated for every evaluation of the loss function. For the real dataset, we consider the jaw movement as an additional area, and we concatenate it to the <inline-formula><inline-graphic xlink:href="615361v6_inline46.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
<p>Based on this loss function, we optimize the following parameters: <inline-formula><inline-graphic xlink:href="615361v6_inline47.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and <italic>β</italic> for the RefCircs. For the RNNs, we optimize only the recurrent connectivity <italic>W</italic> <sup><italic>rec,d</italic></sup>, and the rest are fixed from the RefCircs. For the real dataset, additionally to the parameters optimized in the RefCircs, we also optimize the jaw’s linear readout <inline-formula><inline-graphic xlink:href="615361v6_inline48.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and its scaling parameter <italic>b</italic>.</p>
<sec id="s4e1">
<title>Implementing Dale’s law and local inhibition</title>
<p>In our network, the recurrent weights <italic>W</italic> <sup><italic>rec</italic></sup> are computed as the elementwise product of two matrices:<inline-formula><inline-graphic xlink:href="615361v6_inline49.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which encodes the strength of synaptic efficacies and is always positive, and<inline-formula><inline-graphic xlink:href="615361v6_inline50.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which has a fixed sign determined by the neurotransmitter type of the presynaptic neuron and<inline-formula><inline-graphic xlink:href="615361v6_inline51.gif" mimetype="image" mime-subtype="gif"/></inline-formula> :
<disp-formula id="eqn9">
<graphic xlink:href="615361v6_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To enforce Dale’s law during optimization, we set any negative values of <inline-formula><inline-graphic xlink:href="615361v6_inline52.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to zero at each iteration as in <xref ref-type="bibr" rid="c5">Bellec et al. (2018a)</xref>. Similarly, to constrain only local inhibitory connections during optimization, we zero out any changes in the synaptic efficacies of across-areas inhibitory connections at each iteration. In simplified models, Dale’s law or the local inhibition constraint can be disrupted by omitting this correction step.</p>
<p>The success of the network optimization highly depends on the initialization of the recurrent weight matrices. To initialize signed matrices we follow the theoretical <xref ref-type="bibr" rid="c43">Rajan and Abbott (2006)</xref> and practical insights <xref ref-type="bibr" rid="c6">Bellec et al. (2018b)</xref>; <xref ref-type="bibr" rid="c11">Cornford et al. (2020)</xref> developed previously. After defining the constraints on the weight signs<inline-formula><inline-graphic xlink:href="615361v6_inline53.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the initialization amplitude <inline-formula><inline-graphic xlink:href="615361v6_inline54.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for each target neuron is adjusted to a zero-summed input weights (the sum of incoming excitatory inputs is equal to the sum of inhibitory inputs). Then the weight amplitude is re-normalized by the modulus of its largest eigenvalue of <italic>W</italic> <sup><italic>rec</italic></sup>, so all the eigenvalues of this matrix <italic>W</italic> <sup><italic>rec</italic></sup> have modulus 1 or smaller.</p>
</sec>
<sec id="s4e2">
<title>Stopping criterion for the optimization</title>
<p>For the synthetic dataset, we train the models for 4000 gradient descent steps. For the real dataset, due to limited data and a noisy test set, we select the final model based on the optimization step that yields the best trial-type accuracy (closest to the trial-type accuracy from the data), derived from the jaw trace and whisker stimulus, along with the highest trial-matched Pearson correlation between the model and the recordings.</p>
</sec>
<sec id="s4e3">
<title>Sparsity regularization</title>
<p>There is a plethora of ways to enforce sparsity. In this work, we use weight regularization. In particular, we use the <inline-formula><inline-graphic xlink:href="615361v6_inline55.gif" mimetype="image" mime-subtype="gif"/></inline-formula> norm of the recurrent and input weights that promote a high level of sparsity (<xref ref-type="bibr" rid="c59">Xu et al., 2012</xref>). To avoid numerical instabilities, we apply this regularization only for synaptic weights above <italic>α</italic> and prune all synapses below <italic>α</italic>. (we set <italic>α</italic> = 1<italic>e</italic><sup><italic>−</italic>7</sup>). The regularized loss function becomes:
<disp-formula id="eqn10">
<graphic xlink:href="615361v6_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>W</italic> <sup>across,<italic>d</italic></sup> are the connections from one area to the other.</p>
<p>For the synthetic dataset, we choose the level of across-area sparsity by performing a small grid search for <italic>λ</italic><sub>3</sub>. In particular, the sparsity level <italic>λ</italic><sub>3</sub> is the maximum value <italic>λ</italic><sub>3</sub> where the performance remains as good as without sparsity, see <xref rid="figS3" ref-type="fig">Suppl. Fig S3</xref>. For the real dataset, we use the same value <italic>λ</italic><sub>3</sub> as the one we found for the full reconstruction method of bioRNN1.</p>
</sec>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>Perturbation test of in silico optogenetics</title>
<p>In systems neuroscience, a method to test causal interactions between brain regions uses spatially and temporally precise optogenetic activations or inactivations (<xref ref-type="bibr" rid="c19">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="c22">Guo et al., 2014</xref>). Usually, inactivations refer to the strong activation of inhibitory neurons for cortical areas. These inhibitory neurons have strong intra-area connections that effectively “silence” their local-neighborhood (<xref ref-type="bibr" rid="c24">Helmstaedter et al., 2009</xref>).</p>
<p>Our model can simulate these perturbations and allow us to compare the causal mechanisms of two networks based on their responses to optogenetic perturbations. We implement activations and inactivations as a strong input current to all the neurons in one area’s excitatory or inhibitory population. For the RefCircs and reconstructed RNNs, we use a transient current that lasts 40 ms, from 20 ms before to 20 ms after the input stimulus. The strength of the current (light power) varies until there is an effect in the full reconstruction method bioRNN1. For the synthetic dataset in <xref rid="fig2" ref-type="fig">Figure 2</xref> (except for panel D), we inject a current of <inline-formula><inline-graphic xlink:href="615361v6_inline56.gif" mimetype="image" mime-subtype="gif"/></inline-formula> into excitatory neurons for activations and <inline-formula><inline-graphic xlink:href="615361v6_inline57.gif" mimetype="image" mime-subtype="gif"/></inline-formula> into inhibitory neurons for inactivations. For the real dataset, we perform optogenetics inactivations in three different periods. As in <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref>, we silence the cortical circuit during the whisker presentation, the time between the whisker and auditory stimulus, or when the animal was licking for the reward. In particular, we use transient currents to the inhibitory neurons during (i.) 100 ms before and after the whisker presentation, (ii.) 100 ms after the whisker presentation till 100ms before the onset of the auditory cue, and (iii.) after the auditory cue till the end of our simulation. For cases (i.) and (ii.), we linearly decreased the strength of the current to avoid rebound excitation. The light power is chosen so that our model has the best results in reproducing the lick probability of the recordings. It is important to mention that the perturbation data are not used to optimize the network but to test whether the resulting network has the same causal interactions with the recordings.</p>
<p>For the RefCircs and bioRNNs, we evaluate the effect of the perturbations directly from the neural activity. We use the distance of network dynamics ℒ <sub>trial</sub> to compare the two perturbed networks. For the real dataset, we compare the effect of the inactivations on the behavior; as behavior here, we mean whether the mouse/model licked. We classify the licking action using a multilayer perceptron with two hidden layers with 128 neurons each. The classifier is trained with the jaw movement of the real dataset, which was extracted from video filming using custom software, to predict the lick action, which was extracted from a piezo sensor placed in the spout. This classifier predicted lick correctly 94% of the time. We then used the same classifier on the jaw movement from the model to determine whether there was a “lick” or not.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data availability statement</title>
<p>The code for this project is open sourced and published at <ext-link ext-link-type="uri" xlink:href="https://github.com/Sourmpis/BiologicallyInformed">https://github.com/Sourmpis/BiologicallyInformed</ext-link>. The dataset for the artificial dataset can be downloaded/generated on our code repository. The in vivo dataset was published openly for the previous publication <xref ref-type="bibr" rid="c19">Esmaeili et al. (2021)</xref>. The dataset is accessible at: <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/4720013">https://zenodo.org/records/4720013</ext-link>.</p>
</sec>
<sec id="s6">
<title>Supplemental information</title>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>The loss ℒ <sub>trial</sub> measures the distance between the network dynamics in single trials.</title>
<p>Here for the synthetic dataset, we report the distance between activity statistics in area <italic>A</italic> during stimulation of area <italic>B</italic>. The column “light” reports the distance between RNN and RefCirc dynamics during perturbation with the highest light amplitude, the column “no light” reports the same value when no stimulation is delivered. ± indicates the 95% confidence interval.</p></caption>
<graphic xlink:href="615361v6_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1:</label>
<caption><title>Modeling “optogenetic” perturbations.</title>
<p><bold>A</bold>. Two different network hypotheses for implementing a detection task. In RefCirc1, area A projects to area B but not vice versa. In RefCirc2, the areas are recurrently connected. <bold>B</bold>. Raster plots of all neurons in RefCirc1 during a single hit trial under normal conditions (control, left) and under optogenetic perturbation of excitatory (middle) and inhibitory (right) neurons. The duration of the light stimulus is shown with a blue shading. <bold>C</bold>. Same for RefCirc2 <bold>D</bold>. Trial-averaged activity of the two circuits during Hit (blue: RefCirc1; green: RefCirc2) and Miss (yellow: RefCirc 1; red: RefCirc2) trials. A trial is classified as “Hit” if area <italic>A</italic> reaches a transient firing rate above 8Hz; and otherwise as “Miss”. For the control case, the maximal difference between the trial average activity of the two networks is below 0.51 Hz (zoom inset).</p></caption>
<graphic xlink:href="615361v6_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2:</label>
<caption><title>Fitting Reconstructed networks to the synthetic dataset.</title>
<p><bold>A</bold>. Schematic representation of the RefCirc1 and bioRNN1. and probability of hit trials. <bold>B</bold>. Histogram of the firing rate distribution of the RefCirc1 and all the RNN1 versions. We observe that all RNN1 versions fit well with the RefCirc1. <bold>C</bold>. Left: Neuron loss of the different RNN1 variants. Right: Trial-matching loss of the different RNN1 variants. We observe that the model without the trial-matching loss function behaves considerably worse. The whiskers show the 95% confidence interval of the mean across trials. <bold>D-F</bold>. Same as <bold>A-B</bold> for RefCirc2 and RNNs2.</p></caption>
<graphic xlink:href="615361v6_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3:</label>
<caption><title>Picking the sparsity level.</title>
<p><bold>A</bold>. Grid search for the optimal maximum regularization strength (<italic>λ</italic><sub>3</sub>) without a drop in performance. As a performance measure, we used the trial-matching loss, L<sub>trial</sub>.</p></caption>
<graphic xlink:href="615361v6_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4:</label>
<caption><title><bold>A</bold> Hit frequency prediction error <inline-formula><inline-graphic xlink:href="615361v6_inline68.gif" mimetype="image" mime-subtype="gif"/></inline-formula> as in <xref rid="fig2" ref-type="fig">Figure 2C</xref>.</title>
<p>In contrast to <xref rid="fig2" ref-type="fig">Figure 2C</xref>, here we show separately the change of hit probability for RefCirc1 (left) and RefCirc2 (right).</p></caption>
<graphic xlink:href="615361v6_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5:</label>
<caption><title>Reconstruction of the real recordings.</title>
<p><bold>A</bold>. Probability of hit trials of the different variant models. <bold>B</bold>. Histogram of the firing rate distribution from the real recordings and all the variants. <bold>C</bold>. Top: Neuron loss of the different RNN1 variants. All RNN versions have a similar loss value. Bottom: Trial-matching loss of the different model variants. We observe that the model without the trial-matching loss function behaves considerably worse. The whiskers show the 95% confidence interval.</p></caption>
<graphic xlink:href="615361v6_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6:</label>
<caption><title>A Change of lick probability under inactivation of all areas in all the different temporal windows. We show the Δ<italic>p</italic> from the data and reconstruction model variants.</title></caption>
<graphic xlink:href="615361v6_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We thank Alireza Modirshanechi, Shuqi Wang, and Tâm Nguyen for their valuable feedback on the manuscript. We are grateful to Vahid Esmaeili for collecting the dataset and ongoing support throughout this project. This research is supported by the Sinergia project CR-SII5 198612, the Swiss National Science Foundation (SNSF) project 200020 207426 awarded to WG, SNSF projects TMAG-3 209271 and 31003A 182010 awarded to CP, and the Vienna Science and Technology Fund (WWTF) project VRG24-018 awarded to GB.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adesnik</surname>, <given-names>H.</given-names></string-name> and <string-name><surname>Abdeladim</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Probing neural codes with two-photon holographic optogenetics</article-title>. <source>Nature Neuroscience</source>, <volume>24</volume>(<issue>10</issue>):<fpage>1356</fpage>–<lpage>1366</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aravanis</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>L.-P.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Meltzer</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Mogri</surname>, <given-names>M. Z.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>M. B.</given-names></string-name>, and <string-name><surname>Deisseroth</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2007</year>). <article-title>An optical neural interface: in vivo control of rodent motor cortex with integrated fiberoptic and optogenetic technology</article-title>. <source>Journal of Neural Engineering</source>, <volume>4</volume>(<issue>3</issue>):<fpage>S143</fpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arthur</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Preibisch</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Darshan</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2023</year>). <article-title>A scalable implementation of the recursive least-squares algorithm for training spiking neural networks</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>17</volume>:<fpage>1099510</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Azabou</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Arora</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Ganesh</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Mao</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Nachimuthu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Mendelson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Perich</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lajoie</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Dyer</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2024</year>). <article-title>A unified, scalable framework for neural population decoding</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bellec</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kappel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Maass</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Legenstein</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2018a</year>). <article-title>Deep rewiring: Training very sparse deep networks</article-title>. <conf-name>Iclr</conf-name>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bellec</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Salaj</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Subramoney</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Legenstein</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Maass</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2018b</year>). <article-title>Long short-term memory and learning-to-learn in networks of spiking neurons</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>31</volume>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bellec</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Modirshanechi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Brea</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Fitting summary statistics of neural data with a differentiable spiking network simulator</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>34</volume>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Billeh</surname>, <given-names>Y. N.</given-names></string-name>, <string-name><surname>Cai</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Gratiy</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Iyer</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gouwens</surname>, <given-names>N. W.</given-names></string-name>, <string-name><surname>Abbasi-Asl</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Olsen</surname>, <given-names>S. R.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2020</year>). <article-title>Systematic integration of structural and functional data into multi-scale models of mouse primary visual cortex</article-title>. <source>Neuron</source>, <volume>106</volume>(<issue>3</issue>):<fpage>388</fpage>–<lpage>403</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bohte</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Kok</surname>, <given-names>J. N.</given-names></string-name>, and <string-name><surname>La Poutre</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Error-backpropagation in temporally encoded networks of spiking neurons</article-title>. <source>Neurocomputing</source>, <volume>48</volume>(<issue>1-4</issue>):<fpage>17</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Scherr</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Maass</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A data-based large-scale model for primary visual cortex enables brain-like robust and versatile visual processing</article-title>. <source>Science Advances</source>, <volume>8</volume>(<issue>44</issue>):<fpage>eabq7592</fpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Cornford</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kalajdzievski</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Leite</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lamarquette</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kullmann</surname>, <given-names>D. M.</given-names></string-name>, and <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Learning to live with dale’s principle: Anns with separate excitatory and inhibitory units</article-title>. <source>bioRxiv</source>, pages <elocation-id>2020.11.02.364968</elocation-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cowley</surname>, <given-names>B. R.</given-names></string-name>, <string-name><surname>Calhoun</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Rangarajan</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Ireland</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name>, and <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Mapping model units to visual neurons reveals population code for social behaviour</article-title>. <source>Nature</source>, <volume>629</volume>(<issue>8014</issue>):<fpage>1100</fpage>–<lpage>1108</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Défossez</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Copet</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Synnaeve</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Adi</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2023</year>). <article-title>High fidelity neural audio compression</article-title>. <conf-name>Transactions on Machine Learning Research</conf-name>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deny</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ferrari</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Mace</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Yger</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Caplette</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Picaud</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tkačik</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Marre</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Multiplexed computations in retinal ganglion cells of a single type</article-title>. <source>Nature Communications</source>, <volume>8</volume>(<issue>1</issue>):<fpage>1964</fpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dinc</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Shai</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schnitzer</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Tanaka</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Cornn: Convex optimization of recurrent neural networks for rapid inference of neural dynamics</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dubreuil</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Valente</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Beiran</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mastrogiuseppe</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Ostojic</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The role of population structure in computations through neural dynamics</article-title>. <source>Nature Neuroscience</source>, <volume>25</volume>(<issue>6</issue>):<fpage>783</fpage>–<lpage>794</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Eccles</surname>, <given-names>J. C.</given-names></string-name></person-group> (<year>1976</year>). <source>From Electrical to Chemical Transmission in the Central Nervous System: The Closing Address of the Sir Henry Dale Centennial Symposium</source>. <publisher-name>Springer</publisher-name>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elman</surname>, <given-names>J. L.</given-names></string-name></person-group> (<year>1990</year>). <article-title>Finding structure in time</article-title>. <source>Cognitive Science</source>, <volume>14</volume>(<issue>2</issue>):<fpage>179</fpage>–<lpage>211</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esmaeili</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Tamura</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Muscinelli</surname>, <given-names>S. P.</given-names></string-name>, <string-name><surname>Modirshanechi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Boscaglia</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Oryshchuk</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Foustoukos</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Crochet</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2021</year>). <article-title>Rapid suppression and sustained activation of distinct cortical regions for a delayed sensory-triggered motor response</article-title>. <source>Neuron</source>, <volume>109</volume>(<issue>13</issue>):<fpage>2183</fpage>–<lpage>2201</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Fraile</surname>, <given-names>J. G.</given-names></string-name>, <string-name><surname>Scherr</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ramasco</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Arkhipov</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Maass</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Mirasso</surname>, <given-names>C. R.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Competition between bottom-up visual input and internal inhibition generates error neurons in a model of the mouse primary visual cortex</article-title>. <source>bioRxiv</source>, pages <elocation-id>2023.01.27.525984</elocation-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grosenick</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Marshel</surname>, <given-names>J. H.</given-names></string-name>, and <string-name><surname>Deisseroth</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Closed-loop and activity-guided optogenetic control</article-title>. <source>Neuron</source>, <volume>86</volume>(<issue>1</issue>):<fpage>106</fpage>–<lpage>139</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname>, <given-names>Z. V.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Huber</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ophir</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Gutnisky</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ting</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Flow of cortical activity underlying a tactile decision in mice</article-title>. <source>Neuron</source>, <volume>81</volume>(<issue>1</issue>):<fpage>179</fpage>–<lpage>194</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Mihalas</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hirokawa</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Whitesell</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Bernard</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bohn</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Caldejon</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Casal</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Cho</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2019</year>). <article-title>Hierarchical organization of cortical and thalamic connectivity</article-title>. <source>Nature</source>, <volume>575</volume>(<issue>7781</issue>):<fpage>195</fpage>–<lpage>202</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Helmstaedter</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sakmann</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Feldmeyer</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2009</year>). <article-title>L2/3 interneuron groups defined by multiparameter analysis of axonal projection, dendritic geometry, and electrical excitability</article-title>. <source>Cerebral Cortex</source>, <volume>19</volume>(<issue>4</issue>):<fpage>951</fpage>–<lpage>962</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hodgkin</surname>, <given-names>A. L.</given-names></string-name></person-group> (<year>1958</year>). <article-title>The croonian lecture-ionic movements and electrical activity in giant nerve fibres</article-title>. <source>Proceedings of the Royal Society of London. Series B-Biological Sciences</source>, <volume>148</volume>(<issue>930</issue>):<fpage>1</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><collab>International Brain Laboratory</collab>, <string-name><surname>Benson</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Benson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Birman</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bonacchi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Catarino</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Chapuis</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Dan</surname>, <given-names>Y.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2023</year>). <article-title>A brain-wide map of neural activity during complex behaviour</article-title>. <source>bioRxiv</source>, pages <elocation-id>2023.07.04.547681</elocation-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Isbister</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pokorny</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bolaños-Puchet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Santander</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Arnaudon</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Awile</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Barros-Zulaica</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Alonso</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Boci</surname>, <given-names>E.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2023</year>). <article-title>Modeling and simulation of neocortical micro-and mesocircuitry. part ii: Physiology and experimentation</article-title>. <source>bioRxiv</source>, pages <elocation-id>2023.05.17.541168</elocation-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Finkelstein</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chow</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Darshan</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Distributing task-related neural activity across a cortical network through task-independent connections</article-title>. <source>Nature Communications</source>, <volume>14</volume>(<issue>1</issue>):<fpage>2851</fpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lappalainen</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Tschopp</surname>, <given-names>F. D.</given-names></string-name>, <string-name><surname>Prakhya</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>McGill</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nern</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Shinomiya</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Takemura</surname>, <given-names>S.-y.</given-names></string-name>, <string-name><surname>Gruntman</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Macke</surname>, <given-names>J. H.</given-names></string-name>, and <string-name><surname>Turaga</surname>, <given-names>S. C.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Connectome-constrained deep mechanistic networks predict neural responses across the fly visual system at single-neuron resolution</article-title>. <source>bioRxiv</source>, pages <elocation-id>2023.03.11.532232</elocation-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Deep learning</article-title>. <source>Nature</source>, <volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>444</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Foustoukos</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Crochet</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Petersen</surname>, <given-names>C. C.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Axonal and dendritic morphology of excitatory neurons in layer 2/3 mouse barrel cortex imaged through whole-brain two-photon tomography and registered to a digital brain atlas</article-title>. <source>Frontiers in Neuroanatomy</source>, <volume>15</volume>:<fpage>791015</fpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahuas</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Isacchini</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Marre</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Ferrari</surname>, <given-names>U.</given-names></string-name>, and <string-name><surname>Mora</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2020</year>). <article-title>A new inference approach for training shallow and deep generalized linear models of noisy interacting neurons</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>33</volume>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markram</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Muller</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ramaswamy</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Reimann</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Abdellah</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sanchez</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Ailamaki</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Alonso-Nanclares</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Antille</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Arsever</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2015</year>). <article-title>Reconstruction and simulation of neocortical microcircuitry</article-title>. <source>Cell</source>, <volume>163</volume>(<issue>2</issue>):<fpage>456</fpage>–<lpage>492</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markram</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Toledo-Rodriguez</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Gupta</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Silberberg</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Wu</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Interneurons of the neocortical inhibitory system</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>5</volume>(<issue>10</issue>):<fpage>793</fpage>–<lpage>807</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neftci</surname>, <given-names>E. O.</given-names></string-name>, <string-name><surname>Mostafa</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Zenke</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks</article-title>. <source>IEEE Signal Processing Magazine</source>, <volume>36</volume>(<issue>6</issue>):<fpage>51</fpage>–<lpage>63</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Packer</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Russell</surname>, <given-names>L. E.</given-names></string-name>, <string-name><surname>Dalgleish</surname>, <given-names>H. W.</given-names></string-name>, and <string-name><surname>Häusser</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Simultaneous all-optical manipulation and recording of neural circuit activity with cellular resolution in vivo</article-title>. <source>Nature Methods</source>, <volume>12</volume>(<issue>2</issue>):<fpage>140</fpage>–<lpage>146</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Pals</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sağtekin</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Pei</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Gloeckler</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Macke</surname>, <given-names>J. H.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Inferring stochastic low-rank recurrent neural networks from neural data</article-title>. <source>arXiv</source>:<pub-id pub-id-type="arxiv">2406.16749</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pandarinath</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>O’Shea</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Collins</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jozefowicz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Stavisky</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Kao</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Trautmann</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name>, <string-name><surname>Hochberg</surname>, <given-names>L. R.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2018</year>). <article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title>. <source>Nature Methods</source>, <volume>15</volume>(<issue>10</issue>):<fpage>805</fpage>–<lpage>815</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Papagiakoumou</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ronzitti</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Emiliani</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Scanless two-photon excitation with temporal focusing</article-title>. <source>Nature Methods</source>, <volume>17</volume>(<issue>6</issue>):<fpage>571</fpage>–<lpage>581</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Perich</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Arlt</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Soares</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Young</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Mosher</surname>, <given-names>C. P.</given-names></string-name>, <string-name><surname>Minxha</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Rutishauser</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Rudebeck</surname>, <given-names>P. H.</given-names></string-name>, <string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2020</year>). <article-title>Inferring brain-wide interactions using data-constrained recurrent neural network models</article-title>. <source>bioRxiv</source>, pages <elocation-id>2020.12.18.423348</elocation-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Shlens</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Paninski</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sher</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Litke</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Chichilnisky</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source>, <volume>454</volume>(<issue>7207</issue>):<fpage>995</fpage>–<lpage>999</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pozzorini</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Mensi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hagens</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Naud</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Automated high-throughput characterization of single neurons by means of simplified spiking models</article-title>. <source>PLoS Computational Biology</source>, <volume>11</volume>(<issue>6</issue>):<fpage>e1004275</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rajan</surname>, <given-names>K.</given-names></string-name> and <string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Eigenvalue spectra of random matrices for neural networks</article-title>. <source>Physical Review Letters</source>, <volume>97</volume>(<issue>18</issue>):<fpage>188104</fpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richards</surname>, <given-names>B. A.</given-names></string-name> and <string-name><surname>Kording</surname>, <given-names>K. P.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The study of plasticity has always been about gradients</article-title>. <source>The Journal of Physiology</source>, <volume>601</volume>(<issue>15</issue>):<fpage>3141</fpage>–<lpage>3149</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rimehaug</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Stasik</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Hagen</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Billeh</surname>, <given-names>Y. N.</given-names></string-name>, <string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Olsen</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Einevoll</surname>, <given-names>G. T.</given-names></string-name>, and <string-name><surname>Arkhipov</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Uncovering circuit mechanisms of current sinks and sources with biophysical simulations of primary visual cortex</article-title>. <source>eLife</source>, <volume>12</volume>:<elocation-id>e87169</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.87169</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rosenblatt</surname>, <given-names>F.</given-names></string-name></person-group> (<year>1960</year>). <chapter-title>Perceptual generalization over transformation groups</chapter-title>. <source>Self Organizing Systems</source>, <publisher-name>Pergamon Press</publisher-name>, <publisher-loc>NY</publisher-loc>, pages <fpage>63</fpage>–<lpage>96</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Schrimpf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>McGrath</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Margalit</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Do topographic deep ann models of the primate ventral stream predict the perceptual effects of direct it cortical interventions?</article-title> <source>bioRxiv</source>, pages <elocation-id>2024.01.09.572970</elocation-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Durand</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gale</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Graddis</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Heller</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ramirez</surname>, <given-names>T. K.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Luviano</surname>, <given-names>J. A.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2021</year>). <article-title>Survey of spiking in the mouse visual system reveals functional hierarchy</article-title>. <source>Nature</source>, <volume>592</volume>(<issue>7852</issue>):<fpage>86</fpage>–<lpage>92</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sourmpis</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Bellec</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Trial matching: capturing variability with data-constrained spiking neural networks</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Spieler</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rahaman</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Martius</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Schölkopf</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Levina</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The expressive leaky memory neuron: an efficient and expressive phenomenological neuron model can solve long-horizon tasks</article-title>. In <conf-name>The Twelfth International Conference on Learning Representations</conf-name>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Staiger</surname>, <given-names>J. F.</given-names></string-name> and <string-name><surname>Petersen</surname>, <given-names>C. C.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Neuronal circuits in barrel cortex for whisker sensory perception</article-title>. <source>Physiological Reviews</source>, <volume>101</volume>(<issue>1</issue>):<fpage>353</fpage>–<lpage>415</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Zatka-Haas</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Distributed coding of choice, action and engagement across the mouse brain</article-title>. <source>Nature</source>, <volume>576</volume>(<issue>7786</issue>):<fpage>266</fpage>–<lpage>273</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tamamaki</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Tomioka</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Long-range gabaergic connections distributed throughout the neocortex and their possible function</article-title>. <source>Frontiers in Neuroscience</source>, <volume>4</volume>:<fpage>202</fpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teeter</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Iyer</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Menon</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Gouwens</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Szafer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cain</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Zeng</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Hawrylycz</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2018</year>). <article-title>Generalized leaky integrate-and-fire models classify multiple neuron types</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>):<fpage>709</fpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Udvary</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Harth</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Macke</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Hege</surname>, <given-names>H.-C.</given-names></string-name>, <string-name><surname>de Kock</surname>, <given-names>C. P.</given-names></string-name>, <string-name><surname>Sakmann</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Oberlaender</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The impact of neuron morphology on cortical network architecture</article-title>. <source>Cell Reports</source>, <volume>39</volume>(<issue>2</issue>).</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Urai</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Doiron</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Leifer</surname>, <given-names>A. M.</given-names></string-name>, and <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Large-scale neural recordings call for new insights to link brain and behavior</article-title>. <source>Nature Neuroscience</source>, <volume>25</volume>(<issue>1</issue>):<fpage>11</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Valente</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name>, and <string-name><surname>Ostojic</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Extracting computational mechanisms from neural data using low-rank rnns</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>35</volume>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wunderlich</surname>, <given-names>T. C.</given-names></string-name> and <string-name><surname>Pehle</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Event-based backpropagation can compute exact gradients for spiking neural networks</article-title>. <source>Scientific Reports</source>, <volume>11</volume>(<issue>1</issue>):<fpage>12829</fpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2012</year>). <article-title>l {1/2} regularization: A thresholding representation theory and a fast solver</article-title>. <source>IEEE Transactions on neural networks and learning systems</source>, <volume>23</volume>(<issue>7</issue>):<fpage>1013</fpage>–<lpage>1027</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ye</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Collinger</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wehbe</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Gaunt</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Neural data transformer 2: multi-context pretraining for neural spiking activity</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>:<fpage>80352</fpage>–<lpage>80374</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106827.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study demonstrates the significance of incorporating biological constraints in training neural networks to develop models that make accurate predictions under novel conditions. By comparing standard sigmoid recurrent neural networks (RNNs) with biologically constrained RNNs, the manuscript offers <bold>compelling</bold> evidence that biologically grounded inductive biases enhance generalization to perturbed conditions. This manuscript will appeal to a wide audience in systems and computational neuroscience.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106827.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>I congratulate the authors on this beautiful work.</p>
<p>This manuscript introduces a biologically informed RNN (bioRNN) that predicts the effects of optogenetic perturbations in both synthetic and in vivo datasets. By comparing standard sigmoid RNNs (σRNNs) and bioRNNs, the authors make a compelling case that biologically grounded inductive biases improve generalization to perturbed conditions. This work is innovative, technically strong, and grounded in relevant neuroscience, particularly the pressing need for data-constrained models that generalize causally.</p>
<p>I have some suggestions for improvement, which I present in the order of re-reading the paper.</p>
<p>Major</p>
<p>(1) In line 76, the authors make a very powerful statement: 'σRNN simulation achieves higher similarity with unseen recorded trials before perturbation, but lower than the bioRNN on perturbed trials.' I couldn't find a figure showing this. This might be buried somewhere and, in my opinion, deserves some spotlight - maybe a figure or even inclusion in the abstract.</p>
<p>(2) It's mentioned in the introduction (line 84) and elsewhere (e.g., line 259) that spiking has some advantage, but I don't see any figure supporting this claim. In fact, spiking seems not to matter (Figure 2C, E). Please clarify how spiking improves performance, and if it does not, acknowledge that. Relatedly, in line 246, the authors state that 'spiking is a better metric but not significant' when discussing simulations. Either remove this statement and assume spiking is not relevant, or increase the number of simulations.</p>
<p>(3) The authors prefer the metric of predicting hits over MSE, especially when looking at real data (Figure 3). I would bring the supplementary results into the main figures, as both metrics are very nicely complementary. Relatedly, why not add Pearson correlation or R2, and not just focus on MSE Loss?</p>
<p>(4) I really like the 'forward-looking' experiment in closed loop! But I felt that the relevance of micro perturbations is very unclear in the intro and results. This could be better motivated: why should an experimentalist care about this forward-looking experiment? Why exactly do we care about micro perturbation (e.g., in contrast to non-micro perturbation)? Relatedly, I would try to explain this in the intro without resorting to technical jargon like 'gradients'.</p>
<p>Minor</p>
<p>(1) In the intro, the authors refer to 'the field' twice. Personally, I find this term odd. I would opt for something like 'in neuroscience'.</p>
<p>(2) Line 45: When referring to previous work using data-constrained RNN models, Valente et al. is missing (though it is well cited later when discussing regularization through low-rank constraints).</p>
<p>(3) Line 11: Method should be methods (missing an 's').</p>
<p>(4) In line 250, starting with 'So far', is a strange choice of presentation order. After interpreting the results for other biological ingredients, the authors introduce a new one. I would first introduce all ingredients and then interpret. It's telling that the authors jump back to 2B after discussing 2C.</p>
<p>(5) The black dots in Figure 3E are not explained, or at least I couldn't find an explanation.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106827.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Sourmpis et al. present a study in which the importance of including certain inductive biases in the fitting of recurrent networks is evaluated with respect to the generalization ability of the networks when exposed to untrained perturbations.</p>
<p>The work proceeds in three stages:</p>
<p>
(1) a simple illustration of the problem is made. Two reference (ground-truth) networks with qualitatively different connectivity, but similar observable network dynamics, are constructed, and recurrent networks with varying aspects of design similarity to the reference networks are trained to reproduce the reference dynamics. The activity of these trained networks during untrained perturbations is then compared to the activity of the perturbed reference networks. It is shown that, of the design characteristics that were varied, the enforced sign (Dale's law) and locality (spatial extent) of efference were especially important.</p>
<p>
(2) The intuition from the constructed example is then extended to networks that have been trained to reproduce certain aspects of multi-region neural activity recorded from mice during a detection task with a working-memory component. A similar pattern is demonstrated, in which enforcing the sign and locality of efference in the fitted networks has an influence on the ability of the trained networks to predict aspects of neural activity during unseen (untrained) perturbations.</p>
<p>
(3) The authors then illustrate the relationship between the gradient of the motor readout of trained networks with respect to the net inputs to the network units, and the sensitivity of the motor readout to small perturbations of the input currents to the units, which (in vivo) could be controlled optogenetically. The paper is concluded with a proposed use for trained networks, in which the models could be analyzed to determine the most sensitive directions of the network and, during online monitoring, inform a targeted optogenetic perturbation to bias behavior.</p>
<p>The authors do not overstate their claims, and in general, I find that I agree with their conclusions. A couple of points to be made:</p>
<p>(1) Some aspects of the methods are unclear. For comparisons between recurrent networks trained from randomly initialized weights, I would expect that many initializations were made for each model variant to be compared, and that the performance characteristics are constructed by aggregating over networks trained from multiple random initializations. I could not tell from the methods whether this was done or how many models were aggregated.</p>
<p>1. It is possible that including perturbation trials in the training sets would improve model performance across conditions, including held-out (untrained) perturbations (for instance, to units that had not been perturbed during training). It could be noted that if perturbations are available, their use may alleviate some of the design decisions that are evaluated here.</p>
</body>
</sub-article>
</article>