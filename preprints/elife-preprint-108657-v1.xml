<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108657</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108657</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108657.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Developmental prosopagnosics have normal spatial integration in posterior ventral face-selective regions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1673-3260</contrib-id>
<name>
<surname>Stehr</surname>
<given-names>Daniel A</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>Daniel.A.Stehr@dartmouth.edu</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Yiyuan</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Patgiri</surname>
<given-names>Anusha</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kidder</surname>
<given-names>Alexis</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Kay</surname>
<given-names>Kendrick</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="author-notes" rid="n1">‚Ä°</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Duchaine</surname>
<given-names>Bradley</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">‚Ä°</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/049s0rh22</institution-id><institution>Department of Psychological and Brain Sciences, Dartmouth College</institution></institution-wrap>, <city>Hanover</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02n2fzt79</institution-id><institution>Department of Psychology and Neuroscience, Boston College</institution></institution-wrap>, <city>Boston</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/017zqws13</institution-id><institution>Center for Magnetic Resonance Research (CMRR), Department of Radiology, University of Minnesota</institution></institution-wrap>, <city>Minneapolis</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Huxlin</surname>
<given-names>Krystel R</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/022kthw22</institution-id><institution>University of Rochester</institution>
</institution-wrap>
<city>Rochester</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>‚Ä°</label><p>These authors contributed equally to this work</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-11-21">
<day>21</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108657</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-08-29">
<day>29</day>
<month>08</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-07-26">
<day>26</day>
<month>07</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.07.25.666588"/>
</event>
</pub-history>
<permissions>
<copyright-statement>¬© 2025, Stehr et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Stehr et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108657-v1.pdf"/>
<abstract><p>Population receptive field (pRF) mapping is an influential neuroimaging technique used to estimate the region of visual space modulating the response of a neuronal population. While pRF mapping has advanced our understanding of visual cortical organization, evidence linking variation in pRF properties to behavioral performance remains limited. One of the most compelling pRF-to-behavior relationships has emerged from research into developmental prosopagnosia (DP). Individuals with DP have severe deficits in facial identity recognition sometimes linked to diminished holistic processing of faces. This perceptual deficit could be explained at the neural level by abnormally small pRFs in face-selective regions that restrict spatial integration of the face information. This hypothesis is supported by data from a small group of DPs but needs to be rigorously evaluated in a larger sample. Here, we measured pRF properties in 20 individuals with DP and 20 controls using a stimulus designed to robustly activate both low- and high-level visual areas. Consistent with previous studies, DPs exhibited weaker face selectivity in core ventral face-selective areas. Crucially, however, across the visual processing hierarchy ‚Äì from early visual cortex, to intermediate visual areas, and face-selective areas ‚Äì DPs and controls exhibited remarkable similarity in pRF properties, including pRF size, visual field coverage, and the distribution of pRF centers. Using a larger sample and the latest methods for mapping and modeling pRFs, these results challenge theories attributing DP to reduced spatial integration in face-selective regions. This underscores the need to explore alternative neural mechanisms of DP and to re-evaluate links between pRF properties and behavior more broadly.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id>
<institution>NIH</institution>
</institution-wrap>
</funding-source>
<award-id>1R01EY030613-01A1</award-id>
</award-group>
<award-group id="funding-1a">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id>
<institution>NIH</institution>
</institution-wrap>
</funding-source>
<award-id>R01EY034118</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The receptive field (RF) is a fundamental concept in sensory neuroscience. In visual cortex, the RF of a sensory neuron refers to the region of visual space influencing the response of a neuron (<xref ref-type="bibr" rid="c39">Hubel and Wiesel, 1965</xref>). Nearby points in visual space are mapped to nearby points on the cortical surface forming a continuously varying map of the retinal input (i.e. a retinotopic map). Because neurons with similar RFs tend to be grouped together, human neuroimaging techniques have been developed for studying the aggregate RF properties of populations of neurons, termed the population RF or pRF (<xref ref-type="bibr" rid="c24">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="c38">Harvey et al., 2013</xref>; <xref ref-type="bibr" rid="c63">Nasiotis et al., 2017</xref>; <xref ref-type="bibr" rid="c92">Winawer et al., 2013</xref>). Studies of pRFs measured within the volumetric units recorded by functional magnetic resonance imaging (fMRI) have revealed that a visuospatial framework structures much of the cortex ‚Äì from early visual areas (<xref ref-type="bibr" rid="c9">Benson et al., 2018</xref>; <xref ref-type="bibr" rid="c19">Deyoe et al., 1996</xref>; <xref ref-type="bibr" rid="c26">Engel et al., 1997</xref>; <xref ref-type="bibr" rid="c36">Groen et al., 2022</xref>; <xref ref-type="bibr" rid="c73">Sereno et al., 1995</xref>) to high-level visual regions (<xref ref-type="bibr" rid="c12">Brewer et al., 2005</xref>; <xref ref-type="bibr" rid="c29">Finzi et al., 2021</xref>; <xref ref-type="bibr" rid="c35">Grill-Spector et al., 2018</xref>; <xref ref-type="bibr" rid="c36">Groen et al., 2022</xref>; <xref ref-type="bibr" rid="c44">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="c75">Silson et al., 2015</xref>), and even other sensory modalities (<xref ref-type="bibr" rid="c37">Hagler and Sereno, 2006</xref>; <xref ref-type="bibr" rid="c76">Silver and Kastner, 2009</xref>; <xref ref-type="bibr" rid="c78">Steel et al., 2024</xref>; <xref ref-type="bibr" rid="c81">Szinte and Knapen, 2020</xref>; <xref ref-type="bibr" rid="c87">van Es et al., 2019</xref>).</p>
<p>Though a wealth of studies have produced many detailed descriptive accounts of pRF organization across a variety of brain regions, few studies to date have linked individual variation in pRF properties to variation in behavioral performance. One study found a correlation between individual variation in pRF sizes in early visual cortex and perceptual position discrimination thresholds (<xref ref-type="bibr" rid="c77">Song et al., 2015</xref>). Another study examining ventral face-selective regions found that pRFs mapped with upright face stimuli were larger and higher in the visual field than those mapped with inverted face stimuli (<xref ref-type="bibr" rid="c68">Poltoratski et al., 2021</xref>; but see <xref ref-type="bibr" rid="c62">Morsi et al., 2024</xref>), suggesting that large and foveally positioned pRFs are implicated in the common behavioral advantage for recognizing upright versus inverted faces.</p>
<p>The most striking finding linking variations in pRF properties to behavior, however, has emerged from the study of developmental prosopagnosia (DP; also known as <italic>congenital prosopagnosia</italic>). Individuals with DP have great difficulty recognizing facial identity despite having normal low-level vision, normal intelligence, and no history of brain damage (<xref ref-type="bibr" rid="c3">Avidan and Behrmann, 2021</xref>; <xref ref-type="bibr" rid="c5">Bate and Tree, 2017</xref>; <xref ref-type="bibr" rid="c21">Duchaine, 2011</xref>; <xref ref-type="bibr" rid="c58">McConachie, 1976</xref>; <xref ref-type="bibr" rid="c79">Susilo and Duchaine, 2013</xref>). Witthoft and colleagues (<xref ref-type="bibr" rid="c94">Witthoft et al., 2016</xref>) conducted retinotopic mapping in seven DPs and 15 controls and found that the DPs had pRFs that were far smaller and closer to the fovea in ventral face-selective regions. For instance, pRFs in a combined ventral face-selective ROI were approximately three times smaller in DPs compared to controls (control mean¬±SD = 3.4¬∞ ¬± 3.1¬∞, DP mean¬±SD = 1.0¬∞ ¬± 0.37¬∞). Similar differences were also seen in intermediate regions including hV4 and VO1, whereas no group differences were detected in early visual cortex. Furthermore, mean pRF sizes in the face-selective areas were positively correlated with performance on the Cambridge Face Memory Test (<xref ref-type="bibr" rid="c23">Duchaine and Nakayama, 2006</xref>).</p>
<p>These findings provide an appealing and tractable hypothesis about the abnormal nature of the computations underlying DP. A long history of perceptual research has characterized normal face processing as heavily dependent on the ability to form holistic representations consisting of facial features and their spatial relationships (<xref ref-type="bibr" rid="c53">Levine and Calvanio, 1989</xref>; <xref ref-type="bibr" rid="c61">Mckone and Yovel, 2009</xref>; <xref ref-type="bibr" rid="c72">Rossion, 2009</xref>; <xref ref-type="bibr" rid="c83">Tanaka and Farah, 1993</xref>; <xref ref-type="bibr" rid="c95">Yin, 1969</xref>; <xref ref-type="bibr" rid="c97">Young et al., 1987</xref>). Although the term <italic>holistic processing</italic> has been conceptualized in a variety of ways (<xref ref-type="bibr" rid="c57">Maurer et al., 2002</xref>), recent theoretical work has grounded it in an understanding of the receptive field characteristics of face-selective regions (<xref ref-type="bibr" rid="c3">Avidan and Behrmann, 2021</xref>; <xref ref-type="bibr" rid="c35">Grill-Spector et al., 2018</xref>; <xref ref-type="bibr" rid="c94">Witthoft et al., 2016</xref>). When a face is viewed from a typical distance of one meter, an early visual area, such as V1, would not enable holistic face processing, since a representative pRF from V1 only covers an area equivalent in size to the corner of the eye. In contrast, a pRF in a typical observer from a ventral face-selective area, such as one found on the medial fusiform (mFUS) gyrus, covers nearly the entire face, thus enabling the processing of multiple features in parallel as would be required for holistic processing. Therefore, a parsimonious account of the impairment in DP, and one supported by the data from <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref>, is that the scope of the spatial integration window in face-selective regions is too small to permit simultaneous analysis of distal features, a phenomenon leading to the inability to get an overview of the face &quot;as a whole in a single glance&quot; (<xref ref-type="bibr" rid="c53">Levine and Calvanio, 1989</xref>). This interpretation dovetails with anecdotal reports from some DPs who have described relying on slow, compensatory strategies that depend on the piecemeal processing of individual face features (<xref ref-type="bibr" rid="c21">Duchaine, 2011</xref>).</p>
<p>The idea that reduced spatial integration underlies the deficit in DP has gained support (<xref ref-type="bibr" rid="c3">Avidan and Behrmann, 2021</xref>; <xref ref-type="bibr" rid="c41">Jiahui et al., 2018</xref>; <xref ref-type="bibr" rid="c42">Kaiser et al., 2019</xref>; <xref ref-type="bibr" rid="c55">Linka et al., 2022</xref>; <xref ref-type="bibr" rid="c84">Towler et al., 2018</xref>) and serves as one of the best examples linking pRF organization to high-level perception. However, several methodological limitations of the original study highlight the need for additional investigations into pRF characteristics in DP. First, in the years since the study was conducted, pRF results, especially in high-level regions, have been shown to depend critically on the type of carrier stimulus used during mapping. For instance, the ventral occipito-temporal reading circuitry showed less visual field coverage and higher variance explained for a word stimulus compared to a black-and- white checkerboard stimulus (<xref ref-type="bibr" rid="c51">Le et al., 2017</xref>). Across another set of studies, ventral face-selective regions were found to have lower/upper visual field biases when pRFs were mapped with scene images (<xref ref-type="bibr" rid="c75">Silson et al., 2015</xref>) but no bias when mapped with cartoon imagery (<xref ref-type="bibr" rid="c29">Finzi et al., 2021</xref>). Therefore, pRF mapping stimuli consisting of images of faces, objects, bodies, and scenes have been increasingly used in an effort to better match the preferences of a broad range of category-selective regions (<xref ref-type="bibr" rid="c29">Finzi et al., 2021</xref>; <xref ref-type="bibr" rid="c44">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="c75">Silson et al., 2015</xref>). However, <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref>, used a black-and-white checkerboard pattern as the mapping stimulus. While this decision is unlikely to have affected pRF estimates in EVC, results in higher-level areas are likely to have been impacted. In addition, the sample size used by <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref> is small by current standards (ùëÅ = 7 DPs) and the findings have not been published in a peer-reviewed journal. Large-scale studies and meta-analyses in developmental dyslexia, a condition with many similarities to DP, indicate that neuroimaging studies with small samples are prone to false positives (<xref ref-type="bibr" rid="c40">Jednor√≥g et al., 2015</xref>; <xref ref-type="bibr" rid="c70">Ramus et al., 2018</xref>), echoing the concerns about reproducibility in the wider neuroscience community (<xref ref-type="bibr" rid="c11">Boekel et al., 2015</xref>; <xref ref-type="bibr" rid="c13">Button et al., 2013</xref>; <xref ref-type="bibr" rid="c67">Poldrack et al., 2017</xref>; <xref ref-type="bibr" rid="c82">Szucs and Ioannidis, 2017</xref>; <xref ref-type="bibr" rid="c86">Turner et al., 2018</xref>).</p>
<p>To deepen our understanding of the relationship between pRFs in category-selective areas and behavior as well as the role that spatial integration deficits might play in DP, we measured pRF properties in 20 DPs and 20 controls across early, intermediate, and category-selective areas. For mapping pRFs, we used stimuli containing vivid high-level images that effectively stimulate high-level category-selective regions as well as early visual areas. Moreover, we used the compressive spatial summation (CSS) model for estimating pRFs which is a more accurate model for high-level regions where responses to visual stimuli often sum in a subadditive rather than linear manner (<xref ref-type="bibr" rid="c45">Kay et al., 2013a</xref>,<xref ref-type="bibr" rid="c46">b</xref>). If DP stems from a deficit in spatial integration affecting face perception, we would expect to observe reduced pRF sizes and restricted visual field coverage in face-selective regions of DPs compared to neurotypical controls.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>To investigate spatial integration in developmental prosopagnosia (DP), we measured population receptive fields (pRFs) in a sample of 20 DPs and 20 controls across early, intermediate, and category-selective visual regions. During the pRF mapping experiment, participants fixated a central dot while variously-sized apertures (bars, wedges, and a disk) occupied portions of the visual field revealing colorful, high-level images flickering at a rate of 5 Hz (<xref rid="figs5" ref-type="fig">Supplemental Figure 5</xref>). To ensure stable fixation, participants performed a color change-detection task at fixation, and eye movements were recorded for quality control. The compressive spatial summation (CSS) model was used for estimating pRFs (<xref ref-type="bibr" rid="c45">Kay et al., 2013a</xref>,<xref ref-type="bibr" rid="c46">b</xref>), which produces estimates of pRF center position (expressed in either Cartesian or polar coordinates), pRF size (defined as <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="666588v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> ), pRF gain, and the strength of the static nonlinear exponent (ùëõ).</p>
<p>Qualitatively, the pRF mapping procedure yielded the expected pattern of retinotopic polar angle and eccentricity maps in all participants ‚Äì a hemifield representation along the calcarine sulcus flanked by mirror reversals of polar angle, accompanied by foveal representations near the occipital pole that gradually shift to more peripheral eccentricities anteriorly and medially. These maps served as the basis for defining V1, V2, V3, and hV4 with the dorsal and ventral arms of V1 through V3 combined. Delineation of hV4 followed the procedure outlined in <xref ref-type="bibr" rid="c93">Winawer and Witthoft (2017)</xref>. Polar angle maps for each individual in the study are displayed in Supplemental Figures 6 and 7 as they serve to demonstrate that participants maintained fixation and pRF mapping worked appropriately.</p>
<p>Participants underwent functional localizer scans to define category selective regions including face-selective regions on the inferior occipital gyrus (occipital face area or OFA but also referred to as IOG-faces, <xref ref-type="bibr" rid="c66">Pitcher et al., 2011</xref>), posterior fusiform gyrus (pFUS, <xref ref-type="bibr" rid="c91">Weiner and Grill-spector, 2010</xref>) and mid-fusiform gyrus (mFUS, <xref ref-type="bibr" rid="c91">Weiner and Grill-spector, 2010</xref>). Additionally, an region on the parahippocampal gyrus selective for scenes (parahippocampal place area or PPA, <xref ref-type="bibr" rid="c27">Epstein and Kanwisher, 1998</xref>) was included to serve as a high-level control region not involved in the perception of faces.</p>
<p>Compared to controls, DPs had lower face-selectivity in all six face-selective ROIs, with face-selectivity defined as the percent signal change to blocks of faces minus the percent signal change to blocks of objects. These differences reached statistical significance in three areas (right and left OFA, and left pFUS; see <xref rid="tbls3" ref-type="table">Supplemental Table 3</xref>). Inspection of t-maps thresholded at <italic>t</italic> &gt; 3.3 (uncorrected, <italic>p</italic> &lt; 0.001) overlaid on the anatomical masks revealed that, despite slightly weaker face selectivity, DPs had detectable clusters of activity near the expected anatomical landmarks which is in keeping with previous findings (<xref ref-type="bibr" rid="c2">Avidan and Behrmann, 2009</xref>; <xref ref-type="bibr" rid="c20">Dinkelacker et al., 2011</xref>; <xref ref-type="bibr" rid="c31">Fur et al., 2011</xref>; <xref ref-type="bibr" rid="c41">Jiahui et al., 2018</xref>).</p>
<p>PRF parameters were fit individually to each voxel‚Äôs preprocessed time course. Voxels for which the variance explained by the pRF model (coefficient of determination, <italic>R</italic><sup>2</sup>) was less than 20% were excluded from further analysis. ROIs from any participant with fewer than 10 voxels remaining after <italic>R</italic><sup>2</sup> thresholding were omitted from further analysis as the data was deemed insufficient to fully characterize the retinotopic response. The number of surviving ROIs was similar between DPs and controls (<xref rid="fig1" ref-type="fig">Figure 1C</xref>) with the exception of left mFUS, which was present in 18/20 controls versus 14/20 DPs.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>PRF diagnostics and mean parameter values by group (control, DP) and hemisphere (left, right).</title><p>A) Mean coefficient of determination, <italic>R</italic><sup>2</sup>, of all pRF fits included in ROIs B) Mean category selectivity of pRF selections from the functional localizer. Category selectivity was defined as the percent signal change for each ROI‚Äôs preferred category with respect to percent signal change for objects. Voxels were chosen by selecting the top 20% highest <italic>t</italic> values within anatomical masks. C) Counts of surviving ROIs after pRF fitting by group and hemisphere. An ROI was defined as retinotopically driven, and therefore included for analysis, if more than 10 voxels remained after <italic>R</italic><sup>2</sup> thresholding. D) The proportion of voxels that were retained after thresholding on <italic>R</italic><sup>2</sup> &gt; .20. E) Mean pRF size, which was defined as <inline-formula id="inline-eqn-2"><inline-graphic xlink:href="666588v1_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. F) Mean pRF eccentricity in degrees of visual angle. G) Mean pRF exponent representing the static nonlinearity in the CSS model. H) Mean pRF gain value.</p></caption>
<graphic xlink:href="666588v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To test for differences between groups (DPs, controls) by hemisphere (right, left) on various measures of pRF properties, a series of linear mixed-effects models (LMMs) were created. LMMs were selected for their ability to: accommodate missing data (e.g. absent ROIs), model the hierarchical/nested structure of the data (such as the fact that right/left hemisphere is nested within each functional ROI and ROIs were nested within subject (<xref ref-type="bibr" rid="c28">Etzel et al., 2011</xref>)) and provide robust inference without strict assumptions about sphericity or homogeneity of variances (<xref ref-type="bibr" rid="c69">Quen√© and Van Den Bergh, 2004</xref>). For each LMM, the random effects component included, at minimum, a random intercept for participant. The fixed effects portion of the model included group, hemisphere, and the interaction between group and hemisphere. In all models, the intercept was mapped to the control group in the right hemisphere. To evaluate the impact of each fixed effect on the dependent variable, likelihood ratio tests were conducted comparing models including the variable (or interaction) in question to a simpler model without it. All statistical analyses were performed in R using the lme4 <xref ref-type="bibr" rid="c6">Bates et al. (2015)</xref> and lmerTest <xref ref-type="bibr" rid="c50">Kuznetsova et al. (2017)</xref> packages.</p>
<sec id="s2a">
<title>Goodness of pRF model fits</title>
<p>We first compared the goodness-of-fit (<italic>R</italic><sup>2</sup>) of the voxels following pRF fittings. <xref rid="fig1" ref-type="fig">Figure 1A</xref> shows that, consistent with previous studies (<xref ref-type="bibr" rid="c29">Finzi et al., 2021</xref>; <xref ref-type="bibr" rid="c94">Witthoft et al., 2016</xref>), a higher mean <italic>R</italic><sup>2</sup> was observed in the early visual cortex (V1, V2, and V3) and intermediate visual area hV4 than in face-selective areas (OFA, pFUS, mFUS) (<xref ref-type="bibr" rid="c29">Finzi et al., 2021</xref>; <xref ref-type="bibr" rid="c94">Witthoft et al., 2016</xref>). This pattern held for both controls (mean ¬± SE: <italic>EVC</italic> = 53.3¬±1.29; <italic>hV4</italic> = 55.7¬±1.75; <italic>ventral face-selective regions</italic> = 41.6¬±0.81) and DPs (mean ¬± SE: <italic>EVC</italic> = 51.5¬±1.01; <italic>hV4</italic> = 53.7¬±1.72; <italic>ventral face-selective regions</italic> = 42.6¬±1.24). Similar regional trends were found for the proportion of voxels within an ROI retained after thresholding <italic>R</italic><sup>2</sup> at 20% (see <xref rid="fig1" ref-type="fig">Figure 1D</xref>). Despite these regional differences in goodness-of-fit, the face-selective ROIs were still strongly retinotopically modulated by the pRF stimulus. Furthermore, across all ROIs examined, DPs and controls did not differ significantly in terms of <italic>R</italic><sup>2</sup> nor were there any significant interactions between group and hemisphere (LMMs on <italic>R</italic><sup>2</sup> with subject-specific random slopes nested within hemisphere, all <italic>p</italic>ùë† &gt; 0.01, see <xref rid="tbls4" ref-type="table">Supplementary Table 4</xref> for all parameter estimates). The comparable goodness-of-fit values between DPs and controls and strong retinotopic modulation by the pRF stimulus indicate that the data quality was sufficient to support meaningful comparisons of pRF properties in these regions.</p>
</sec>
<sec id="s2b">
<title>Eyetracking analysis</title>
<p>All participants were instructed on the importance of maintaining steady fixation throughout the entirety of the retinotopic mapping runs. Before beginning the experiment, participants were given ample time to practice fixating while inside the scanner, and eye tracking was performed whenever possible. The experimenters monitored real-time plots of eye positions during data collection and provided feedback in between runs, if needed. One control was eliminated from the study due to drowsiness and inability to maintain adequate central fixation. Another control had one of five retinotopic mapping runs dropped from analysis because the spread of eye positions in that run was unusually large. In total, eye tracking data was successfully recorded for all or some runs of the experiment in 14/20 controls and 14/20 DPs. An LMM on the area of the 95% probability contour of the distribution of eye positions (a summary of bivariate scatter) revealed no significant effect of group (ùúí<sup>2</sup>(1) = 0.40, <italic>p</italic> = 0.529), though there was a main effect of run (ùúí<sup>2</sup>(4) = 16.30, <italic>p</italic> = 0.003), with later runs having more dispersed eye positons. Despite this main effect of run, 95% probability contours were small across the majority of runs revealing that participants, regardless of group, were able to maintain central fixation throughout the retinotopic mapping runs. Heatmaps of eye position recordings for individual participants and runs are displayed in <xref rid="figs8" ref-type="fig">Supplemental Figures 8</xref> and 9 and a group summary is displayed in <xref rid="figs10" ref-type="fig">Supplemental Figure 10</xref>.</p>
</sec>
<sec id="s2c">
<title>DPs do not have restricted visual field coverage in face-selective or intermediate visual regions</title>
<p>Spatial representations within functional ROIs are built up from the responses of many pRFs that vary in their position and size within the visual field. To evaluate how pRFs collectively tile the visual field in DPs and controls, we measured the visual field coverage (VFC) within each ROI. To do this, pRFs were conceptualized as circles of radius <inline-formula id="inline-eqn-3"><inline-graphic xlink:href="666588v1_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and VFC was computed as the proportion of these circles covering each point in visual space (<xref rid="fig2" ref-type="fig">Figure 2A</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Measures of visual field coverage (VFC).</title><p>VFC was computed as the proportion of pRFs covering each point in the visual field. A) Group averaged plots of VFC by ROI. Data from the right hemisphere has been reflected across the vertical meridian and combined with data from the left hemisphere. The dotted lines show the convex hull polygon at the 50% density threshold. B) Convex hull polygons, computed at the 50% density threshold, displayed for each individual subject by group and ROI. C) Cuts through the horizontal meridian showing normalized group coverage to the left and right of fixation (at 0 degrees) by hemisphere. D) Total area (in degrees of visual angle squared) for the convex hull polygons in panel B. Datapoints represent individual subjects. E) Proportion of the total convex hull area falling above the horizontal meridian. Values above 0.5 indicate an upper visual field bias and values below 0.5 indicate a lower visual field bias. F) Proportion of the convex hull area falling into the contralateral visual field.</p></caption>
<graphic xlink:href="666588v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To better quantify variation in VFC across individuals and groups, we computed the convex hull area of the VFC density, which is the area (in degrees of visual angle squared) of the smallest convex polygon wrapping around all points at a given density threshold. For the density threshold, we chose 50% of the normalized density as a robust middle ground. The convex hull area was measured separately for each participant and ROI. <xref rid="fig2" ref-type="fig">Figure 2B</xref> displays the convex hull polygons from each participant. In addition, a group-averaged convex hull polygon is superimposed on the visual field coverage density plots in <xref rid="fig2" ref-type="fig">Figure 2A</xref> as a dotted line.</p>
<p>In general, the VFC increased as one ascends from V1 through hV4 to the ventral face-selective regions, in accordance with previous studies (<xref ref-type="bibr" rid="c9">Benson et al., 2018</xref>; <xref ref-type="bibr" rid="c29">Finzi et al., 2021</xref>; <xref ref-type="bibr" rid="c44">Kay et al., 2015</xref>). An LMM with convex hull area as dependent measure and subject-specific random intercepts and fixed effects of ROI (V1, V2, V3, hV4, OFA, pFUS, mFUS, and PPA), group (DP, control), and hemisphere (right, left) revealed a strong main effect of ROI (ùúí<sup>2</sup>(6) = 160.71, <italic>p</italic> &lt; 0.001), indicating that convex hull area was larger for later visual areas. However, no other main effects or interactions were significant (all <italic>p</italic>ùë† &gt; .05).</p>
<p>For all participants, we also computed the proportion of the total convex hull area that fell above the horizontal meridian (upper visual field bias) and the proportion that fell to the contralateral side of the vertical meridian (contralateral visual field bias). An LMM on the proportion of convex hull area above the horizontal meridian revealed a strong main effect of ROI (ùúí<sup>2</sup>(7) = 216.31, <italic>p</italic> &lt; 0.001) but no other significant main effects (including group or hemisphere) and no significant interactions. The regions of V1, V2, and OFA exhibited a lower visual field bias (V1: <italic>t</italic>(39) = -6.56, <italic>p</italic> &lt; 0.001; V2: <italic>t</italic>(39) = -3.53, <italic>p</italic> = 0.001; OFA: <italic>t</italic>(39) = -7.18, <italic>p</italic> &lt; 0.001), and hV4 and PPA exhibited an upper visual field bias (hV4: <italic>t</italic>(39) = 2.94, <italic>p</italic> = 0.006; PPA: <italic>t</italic>(39) = 16.80, <italic>p</italic> &lt; 0.001). Neither pFUS nor mFUS exhibited either an upper or lower visual field bias, which is consistent with several previous reports (<xref ref-type="bibr" rid="c29">Finzi et al., 2021</xref>; <xref ref-type="bibr" rid="c44">Kay et al., 2015</xref>, but see <xref ref-type="bibr" rid="c75">Silson et al., 2015</xref>). An LMM on the proportion of the convex hull area covering the contralateral visual field revealed a strong significant effect of ROI (ùúí<sup>2</sup>(7) = 132.72, <italic>p</italic> &lt; 0.001) but no other significant main effects or interactions.</p>
<p>Together, these results indicate that, across the visual processing hierarchy, DPs and controls have highly similar visual field characteristics and, notably, DPs do not have reduced coverage of the visual field in any of the ROIs studied, including face-selective ones.</p>
</sec>
<sec id="s2d">
<title>DPs have normal scaling between eccentricity and pRF size</title>
<p>Previous studies have found a strong linear relationship between pRF eccentricity and pRF size that spans early, intermediate, and ventral (category-selective) visual areas (<xref ref-type="bibr" rid="c29">Finzi et al., 2021</xref>; <xref ref-type="bibr" rid="c36">Groen et al., 2022</xref>; <xref ref-type="bibr" rid="c44">Kay et al., 2015</xref>). Critically, the slope of the line relating pRF eccentricity to pRF size increases as one ascends through the visual hierarchy. The previous results ‚Äì highly similar mean pRF size between DPs and controls ‚Äì could therefore have hidden atypical scaling of pRF size and eccentricity. Therefore, to compare how pRF sizes scale with eccentricity within DPs and controls, ROI-specific LMMs were created that modeled the linear relationship between pRF size and pRF eccentricity with subject-specific intercepts and slopes as random effects. Then, the effects of group, hemisphere, and their interaction were introduced as fixed effects. Parameter estimates from each ROI‚Äôs model are displayed in <xref rid="tbl1" ref-type="table">Table 1</xref> and subject-specific slopes by group and ROI are displayed graphically in <xref rid="fig3" ref-type="fig">Figure 3</xref>. Since multiple models were constructed, a conservative alpha, ùõº &lt; 0.01, was adopted to guide statistical interpretation. In none of the ROIs did group membership, hemisphere, or the interaction between the two significantly explain additional variance in relating pRF eccentricity to pRF size (all <italic>p</italic>ùë† &gt; 0.01). Hence, DPs and controls show highly similar relationships between pRF size and pRF eccentricity.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Lines of best fit relating pRF eccentricities (in degrees of visual angle) to pRF sizes by group and ROI.</title><p>Gray lines show lines of best fit for individual subjects and colored lines show the group average. Lines were constrained to the central 6 degrees of visual angle because data in far eccentricities were often sparse, especially in more anterior face-selective regions.</p></caption>
<graphic xlink:href="666588v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Fixed effect parameter estimates of pRF size by hemisphere (right, left) for the control and developmental prosopagnosic (DP) groups.</title>
<p>Separate models were created for each region of interest. Note that in each one the intercept has been mapped to the control group in the left hemisphere. Formula (R, lme4 package): size ‚àº group + hemisphere + group:hemisphere + (1+eccentricity|subjectID) * <italic>p</italic> &lt; 0.01; ** <italic>p</italic> &lt; 0.001</p></caption>
<graphic xlink:href="666588v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2e">
<title>DPs have normal distributions of pRF centers</title>
<p>Our results show that DPs and controls have very similar VFC. However, since VFC depends on both pRF sizes and pRF center locations ‚Äì with larger pRFs covering more visual space and pRFs typically increasing in size with eccentricity ‚Äì similar VFC patterns can arise even if the groups differ in the underlying spatial distribution of pRF centers. In other words, if one group had pRFs located more peripherally, the accompanying increase in pRF size could lead to comparable VFC between the groups despite underlying differences in where the pRFs are located. Therefore, in the next analysis, we directly compare groups on the distributions of pRF center locations.</p>
<p>To this end, we computed the area of the 95% probability contour encircling the bivariate distributions of ùë• and ùë¶ coordinates of pRF centers. Larger values indicate greater dispersion of pRF centers. In none of the ROIs (V1-hV4, OFA, pFUS, or mFUS) did the pRF dispersion differ significantly between DPs and controls (two sample <italic>t</italic>-tests, uncorrected, all <italic>p</italic>ùë† &gt; 0.05). The group-wise density of pRF center locations across all ROIs is represented in <xref rid="fig4" ref-type="fig">Figure 4A</xref>, and they show similar patterns between groups. Both groups also showed similar proportions of total pRFs occupying distinct eccentricity bins (<xref rid="fig4" ref-type="fig">Figure 4</xref> B).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Spatial distributions of pRF center coordinates across ROIs and hemispheres.</title><p>A) Contours representing the group density distributions of pRF center coordinates by group and hemisphere. Green colors represent pRFs in the right hemisphere and blue colors represent pRFs in the left hemisphere. B) The proportion of pRFs falling into four distinct eccentricity bins. Datapoints represent individual participants. C) Measures of bivariate dispersion of pRF centers by group and ROI. Datapoints represent individual participants. Bivariate spread was measured by computing the area of the 95% probability contour encircling the distribution of pRF centers.</p></caption>
<graphic xlink:href="666588v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In summary, across multiple measures of retinotopic organization, DPs show no deviation from control participants. DPs and controls have highly similar coverage of the visual field, scaling between pRF eccentricity and size, and distribution of pRF centers.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Population receptive field (pRF) mapping is a valuable technique in visual neuroscience that has provided an understanding of the location, size, and shape of aggregate receptive fields across the cortex (<xref ref-type="bibr" rid="c36">Groen et al., 2022</xref>; <xref ref-type="bibr" rid="c89">Wandell and Winawer, 2015</xref>), however few results have demonstrated strong relationships between individual variation in pRF properties and behavioral measures of perception. One of the most striking studies showing a link between pRF properties and behavior reported that a group of individuals with developmental prosopagnosia (DP) had exceptionally small and foveally-concentrated pRFs in intermediate visual areas and face-selective regions (<xref ref-type="bibr" rid="c94">Witthoft et al., 2016</xref>). Moreover, pRF size correlated with their face recognition performance. These findings presented an appealing neural explanation for the diminished holistic face processing sometimes observed in DPs (<xref ref-type="bibr" rid="c3">Avidan and Behrmann, 2021</xref>; <xref ref-type="bibr" rid="c4">Avidan et al., 2011</xref>; <xref ref-type="bibr" rid="c17">Degutis, 2012</xref>; <xref ref-type="bibr" rid="c65">Palermo et al., 2011</xref>).</p>
<p>To rigorously investigate spatial integration in DPs, we used fMRI to measure pRF properties in DPs and controls (ùëÅ = 20 per group) using high-level mapping stimuli capable of robustly stimulating both early and category-selective visual regions. In contrast to <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref>, our results provide strong empirical evidence that pRF properties are <italic>highly similar</italic> between DPs and controls. Both groups showed comparable: 1) visual field coverage (VFC) maps, providing a measure of how pRFs within an ROI collectively tile the visual field; 2) scaling between pRF eccentricity and pRF size, both within regions of interest (ROIs) and across the visual hierarchy; and 3) dispersion of pRF centers around the visual field. These three patterns of results held for early, intermediate, and ventral face-selective areas, as well as a category-selective control ROI not involved in face processing (the scene-selective parahippocampal place area or PPA; <xref ref-type="bibr" rid="c27">Epstein and Kanwisher (1998)</xref>). Similarly, we found no significant association between participants‚Äô CFMT scores and mean pRF size in OFA, pFUS, or mFUS.</p>
<sec id="s3a">
<title>Methodological strengths of the current study</title>
<p>Our methods incorporated several features that ensure the validity and robustness of the findings. First, we employed a carefully thought-out, ecologically valid stimulus capable of significantly stimulating both early visual and category-selective regions. The stimulus featured vivid, high-contrast content including images of faces, objects, bodies, and places randomly placed on a background of pink noise. Although high-level regions have been mapped with low-level stimuli before (e.g., black- and-white checkerboard patterns), these types of stimuli do not drive category-selective regions as strongly, producing poorer goodness-of-fit (<xref ref-type="bibr" rid="c51">Le et al., 2017</xref>; <xref ref-type="bibr" rid="c75">Silson et al., 2015</xref>) and unstable pRF estimates.</p>
<p>Second, we acquired in-scanner eye-tracking data during the retinotopic mapping sessions whenever it was feasible. Eye movements can severely distort pRF measurements, usually by increasing estimated pRF size, so it was critical to compare fixation behavior in the DPs and controls. Participants with usable eye-tracking data demonstrated steady fixation, with no significant differences in gaze stability between DPs and controls. For participants without usable eye tracking data ‚Äì due to either interference from thick prescription lenses or incompatibility of head anatomy and hardware setup ‚Äì consistent fixation was confirmed by checking for clearly delineated polar angle and eccentricity maps in early visual cortex (<xref rid="figs6" ref-type="fig">Supplemental Figures 6</xref> and 7).</p>
<p>One explanation for the comparable results for the DPs and controls is that the putative DPs may not have possessed a true face recognition deficit. However, we are confident that our selection process effectively categorized DPs for the following reasons. First, all DPs self-reported substantial face recognition difficulties affecting their daily lives, quantified by scores on the PI-20 questionnaire (<xref ref-type="bibr" rid="c74">Shah et al., 2015</xref>). Furthermore, objective inclusion criteria required DPs to score more than 1.8 standard deviations below the mean of a control sample on two independent tests of face recognition ‚Äì the CFMT and a famous faces test ‚Äì a stringent threshold consistent with or exceeding those used in numerous studies of DP (<xref ref-type="bibr" rid="c18">DeGutis et al., 2023</xref>). Furthermore, using not one but two tests for classification substantially minimizes the risk of false identification. Additional testing with the DPs was conducted to rule out low-level visual deficits or autistic-like traits as alternative causes for poor face performance. Univariate measures of face-selectivity provided another opportunity to confirm that our DP participants showed the neural abnormalities that have been found in previous fMRI studies of DP (<xref ref-type="bibr" rid="c31">Fur et al., 2011</xref>; <xref ref-type="bibr" rid="c41">Jiahui et al., 2018</xref>). Univariate functional localizer analyses from our pool of participants revealed that, at the group level, DPs compared to controls exhibited significantly lower face-selective activation in many core ventral face-selective regions (See Supplemental Table 3), replicating previous findings (<xref ref-type="bibr" rid="c20">Dinkelacker et al., 2011</xref>; <xref ref-type="bibr" rid="c31">Fur et al., 2011</xref>; <xref ref-type="bibr" rid="c32">Gerlach et al., 2019</xref>; <xref ref-type="bibr" rid="c41">Jiahui et al., 2018</xref>). Notably, limiting voxels to only those that showed strong retinotopic modulation, still revealed significant reductions in face-selectivity in DPs (<xref rid="fig1" ref-type="fig">Figure 1</xref> B) - a novel finding that provides additional evidence that DPs were classified correctly.</p>
<p>Confidence in our pRF mapping methods was further supported by the fact that our results replicated many key pRF phenomenon from prior studies. Both groups showed significant increases in pRF size across the visual processing hierarchy (<xref ref-type="bibr" rid="c29">Finzi et al., 2021</xref>; <xref ref-type="bibr" rid="c44">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="c75">Silson et al., 2015</xref>) and, within ROIs, there was a consistent increase in pRF size with pRF eccentricity (<xref ref-type="bibr" rid="c9">Benson et al., 2018</xref>; <xref ref-type="bibr" rid="c44">Kay et al., 2015</xref>). Regardless of group, the three ventral face-selective areas exhibited a higher proportion of foveally-positioned pRFs than early visual areas (<xref ref-type="bibr" rid="c35">Grill-Spector et al., 2018</xref>; <xref ref-type="bibr" rid="c44">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="c75">Silson et al., 2015</xref>). Furthermore, these face-selective areas showed a relatively higher degree of subadditive spatial summation (larger static nonlinearity parameter in the CSS model; <xref ref-type="bibr" rid="c44">Kay et al., 2015</xref>, <xref ref-type="bibr" rid="c45">2013a</xref>,b). Also, we replicated previously reported visual field biases ‚Äì namely, a lower visual field bias in OFA (<xref ref-type="bibr" rid="c33">Gomez et al., 2018</xref>; <xref ref-type="bibr" rid="c75">Silson et al., 2015</xref>) and an upper visual field bias in PPA (<xref ref-type="bibr" rid="c75">Silson et al., 2015</xref>). Across both groups, qualitative inspection of polar angle maps near the calcarine sulcus in each hemisphere revealed the canonical retinotopic organization of smooth sweeps from the upper vertical meridian to the lower vertical meridian through the contralateral side of the visual field. Eccentricity maps likewise showed the expected progression from foveal pRFs near the occipital pole to increasingly more peripheral pRFs anteriorly and medially (<xref ref-type="bibr" rid="c9">Benson et al., 2018</xref>). Together, these multiple converging lines of evidence confirm that data quality was high and closely matched between groups.</p>
</sec>
<sec id="s3b">
<title>How do our pRF results fit in with other behavioral data from DPs?</title>
<p>Our results directly challenge the view that DP stems from impaired spatial integration caused by abnormally small pRFs. While this view has garnered substantial interest (<xref ref-type="bibr" rid="c3">Avidan and Behrmann, 2021</xref>; <xref ref-type="bibr" rid="c35">Grill-Spector et al., 2018</xref>; <xref ref-type="bibr" rid="c94">Witthoft et al., 2016</xref>), some behavioral evidence is inconsistent with it. Behaviorally, the most common tasks used to evaluate holistic face processing are the composite (<xref ref-type="bibr" rid="c97">Young et al., 1987</xref>) and part-whole tasks (<xref ref-type="bibr" rid="c83">Tanaka and Farah, 1993</xref>). Comparisons between DPs and controls on these measures have yielded some mixed results: some report reduced holistic processing (<xref ref-type="bibr" rid="c4">Avidan et al., 2011</xref>; <xref ref-type="bibr" rid="c17">Degutis, 2012</xref>; <xref ref-type="bibr" rid="c65">Palermo et al., 2011</xref>), while others find it intact (<xref ref-type="bibr" rid="c10">Biotti et al., 2017</xref>; <xref ref-type="bibr" rid="c52">Le Grand et al., 2006</xref>; <xref ref-type="bibr" rid="c80">Susilo et al., 2010</xref>). This inconsistency highlights the heterogeneous nature of DP and complicates linking DP to a single underlying cause such as poor spatial integration.</p>
<p>Behavioral face studies that experimentally restrict spatial integration further challenge the notion that spatial integration deficits underlie DP. For instance, <xref ref-type="bibr" rid="c85">Tsantani et al. (2020)</xref> compared the ability of DPs and controls to identify famous faces under two viewing conditions: when faces were presented briefly in their entirety versus progressively revealed by a dynamic, narrow aperture. The dynamic aperture viewing condition allowed participants to inspect local features of the face but prevented distant regions from being processed simultaneously, thus simulating the effect of having a small window of integration. In this condition, control participants exhibited substantial decrements in performance likely because it hindered holistic face processing mechanisms. If the DPs‚Äô deficits at face recognition stemmed from deficient sampling of the visual field by pRFs, one would expect the dynamic aperture viewing condition to elicit a smaller decrement in performance. However, contrary to this prediction, both groups showed comparable performance decrements. Similar results were found when DPs were tested with gaze-contingent displays restricting information to a small region around the fovea (<xref ref-type="bibr" rid="c88">Verfaillie et al., 2014</xref>), reinforcing the conclusion that spatial integration deficits alone do not underlie DP.</p>
<p>Lastly, DP is defined by deficits with facial identity processing. However, if DP is a deficit in spatial integration, individuals with DP should have difficulties with <italic>any</italic> aspect of face processing that involves holistic perception. Facial sex judgments (<xref ref-type="bibr" rid="c7">Baudouin and Humphreys, 2006</xref>) and facial expression recognition (<xref ref-type="bibr" rid="c14">Calder et al., 2000</xref>) both elicit composite effects that are similar in size to composite tasks involving facial identity recognition. Most DPs though perform as well as controls on face perception tasks involving judgments not involving identity such as sex classification (<xref ref-type="bibr" rid="c15">Chatterjee and Nakayama, 2012</xref>; <xref ref-type="bibr" rid="c17">Degutis, 2012</xref>; <xref ref-type="bibr" rid="c56">Marsh et al., 2019</xref>) and facial expression recognition (<xref ref-type="bibr" rid="c8">Bell et al., 2023</xref>; <xref ref-type="bibr" rid="c65">Palermo et al., 2011</xref>), so they do not appear to have broad deficits with spatial integration of facial information (<xref ref-type="bibr" rid="c7">Baudouin and Humphreys, 2006</xref>; <xref ref-type="bibr" rid="c14">Calder et al., 2000</xref>; <xref ref-type="bibr" rid="c25">Durand et al., 2007</xref>; <xref ref-type="bibr" rid="c96">Yokoyama et al., 2014</xref>).</p>
</sec>
<sec id="s3c">
<title>What factors may have contributed to the different results for the present study and Witthoft et al. (2016)?</title>
<p>In light of the difference in findings between our study and that of <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref>, several methodological differences between the two studies are worth considering. First, while <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref> mapped pRFs using black-and-white checkerboard patterns as the carrier image, we employed a more ecologically valid stimulus featuring colorful high-level images including faces, scenes, body parts, and objects. Although both types of stimuli would be expected to robustly stimulate early visual cortex, our stimulus better targets higher-level regions as reflected in noticeably higher goodness of fit, <italic>R</italic><sup>2</sup>, in our data compared to that of <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref> (current study mean <italic>R</italic><sup>2</sup>: OFA=.45, pFUS=.42, mFUS=.33; <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref> approximate mean <italic>R</italic><sup>2</sup>: OFA=.21, pFUS=.15, mFUS=.07;). This boost agrees with prior within-subject studies demonstrating that more naturalistic and region-appropriate mapping stimuli outperform black-and-white checkerboard patterns in category-selective areas (<xref ref-type="bibr" rid="c51">Le et al., 2017</xref>; <xref ref-type="bibr" rid="c75">Silson et al., 2015</xref>).</p>
<p>More critically, the choice of stimulus can systematically alter pRF parameters themselves. For instance, <xref ref-type="bibr" rid="c51">Le et al. (2017)</xref> showed that visual field coverage (VFC) in ventral occipito-temporal cortex, a word form-selective region, shrinks toward the central visual field when mapped with word stimuli compared to checkerboard patterns. Thus, the results with DPs - reduced VFC with checker-boards but normal VFC with more naturalistic mapping stimuli - may reflect a stimulus-dependent effect that is elicited solely by low-level, non-preferred stimuli in face-selective regions. Nevertheless, our findings, demonstrating normal spatial integration in face-selective regions when using more appropriate, naturalistic stimuli offer a more definite and convincing proof of neural tuning in these higher-level regions.</p>
<p>To model pRFs, we used the compressive spatial summation (CSS; <xref ref-type="bibr" rid="c45">Kay et al. (2013a</xref>,<xref ref-type="bibr" rid="c46">b</xref>)) model as opposed to a linear pRF model as used in <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref>. The main difference between these two models is that the CSS model applies a compressive static nonlinearity to account for subadditive spatial summation (situations where the sum of the individual responses to apertures forming complementary pairs is &lt; 1). Subadditive spatial summation is present in EVC and grows more pronounced in higher-order regions, making this model more appropriate for studying spatial integration in face-selective regions.</p>
<p>It is also worth noting that <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref> mapped pRFs with stimuli spanning approximately 30¬∞ in diameter, compared to 14.7¬∞ in the present study, a substantial difference in field of view. For us, it was necessary to limit the field of view presented inside the scanner in order to collect high-quality eye-tracking data for verification of stable fixation, which we consider a critical step. Nevertheless, the field of view we used still greatly exceeds the typical visual angle subtended by a face at normal conversational distance (‚àº8¬∞, <xref ref-type="bibr" rid="c59">McKone (2009)</xref>), so this design appears to provide a sufficiently wide enough visual field to detect any spatial integration deficits in DPs, if present.</p>
<p>Finally, there were differences in the aperture design used to map pRFs. <xref ref-type="bibr" rid="c94">Witthoft et al. (2016)</xref> employed blocked runs of rotating wedges and expanding rings (2 runs each, 4 runs total), whereas we used a sequence of apertures including bars (sweeping in 8 different directions), one rotating wedge, and a brief full-field stimulation. While the precise impact of aperture design on pRF estimates is not well understood (for one investigation see <xref ref-type="bibr" rid="c1">Alvarez et al. (2015)</xref>), our aperture design successfully replicated many established pRF properties and it remains unclear why this difference between the designs would have differentially affect DPs and controls. Moreover, our study col-lected more than twice as much data per participant (5 runs of 6.8 mins each versus 4 runs of 3.7 mins each), enhancing the precision and reliability of pRF parameter estimates, especially when combined with a more effective mapping stimulus.</p>
</sec>
<sec id="s4">
<title>Conclusion</title>
<p>Although we do agree that abnormally small pRFs and restricted VFC in face-selective regions <italic>could</italic> cause deficits to spatial integration mechanisms necessary for face recognition, our results ‚Äì using rigorous pRF methods, a large sample, and region-appropriate mapping stimuli ‚Äì indicate that spatial integration, as measured by pRFs, is normal in DP. In line with many previous studies, however, we did discover diminished face selectivity in DPs within the posterior ventral face areas that points to qualitative differences in how face-selective regions respond specifically to face stimuli. Face recognition deficits in DP may therefore arise from atypical neural tuning to facial information rather than from atypical spatial integration, shifting theoretical accounts of DP toward mechanisms involving representational content rather than receptive field properties.</p>
</sec>
</sec>
<sec id="s5">
<title>Materials and methods</title>
<sec id="s5a">
<title>Participants</title>
<p>Twenty individuals with DP (15 females, ùëÄ<sub>ùëéùëîùëí</sub> = 38.18, ùëÜùê∑<sub>ùëéùëîùëí</sub> = 11.13) and 20 typical controls (14 females, ùëÄ<sub>ùëéùëîùëí</sub> = 33.86, ùëÜùê∑<sub>ùëéùëîùëí</sub> = 10.92) participated in the study. The mean age of the groups did not differ significantly (<italic>t</italic>(40.97) = 1.286, <italic>p</italic> = .206, ùëë = 0.398, ùê∂ùêº<sub>95%</sub> = -0.216, 1.006). Written informed consent was obtained from all participants in accordance with the Declaration of Helsinki and a protocol approved by the Dartmouth College Committee for the Protection of Human Subjects (#23282).</p>
</sec>
<sec id="s5b">
<title>Diagnostic testing and participant selection</title>
<p>DP participants were recruited from our prosopagnosia database (<ext-link ext-link-type="uri" xlink:href="https://www.faceblind.org">www.faceblind.org</ext-link>), and interested participants first completed the Twenty-Item Prosopagnosia Index (PI20; <xref ref-type="bibr" rid="c74">Shah et al. (2015)</xref>). The PI20 responses from every DP participant enrolled in the study indicated substantial face recognition difficulties that affected their daily lives (M = 81.90/100, SD = 8.86).</p>
<p>Face recognition ability was assessed with two online tests: The Cambridge Face Memory Test (CFMT; <xref ref-type="bibr" rid="c23">Duchaine and Nakayama (2006)</xref>) and a Famous Faces Test (FFT; <xref ref-type="bibr" rid="c48">Kieseler and Duchaine (2023)</xref>). Individual scores and control summary statistics are presented in <xref rid="tbl2" ref-type="table">Table 2</xref>. The famous faces test was newly created and required participants to identify (via free response) the faces of 40 celebrities familiar to North Americans. Immediately after completing the test, participants were asked about their familiarity with each celebrity. Human raters judged if each response was sufficiently correct, and each participant‚Äôs performance was quantified as the number of correct responses out of the total number of celebrities they reported being familiar with. Potential DPs‚Äô face recognition was deemed sufficiently impaired if they scored more than 1.8 standard deviations below the control sample means on both the CFMT and FFT. Scores on the CFMT were compared against data from 50 typical observers reported by (<xref ref-type="bibr" rid="c23">Duchaine and Nakayama, 2006</xref>), and scores on the FFT were compared against data from N typical observers collected online through <ext-link ext-link-type="uri" xlink:href="https://www.testable.org">www.testable.org</ext-link>.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Scores from each member of the prosopagnosic sample on diagnostic tests consisting of: The Hanover Early Visual Assessment (HEVA), The 20-Item Prosopagnosia Index (PI20), The Cambridge Face Memory Test (CFMT), The Cambridge Face Memory Test with Australian Faces (CFMT Aus), famous faces test (FFT), old/new faces test, and The Subthreshold Autism Trait Questionairre (SATQ).</title>
<p>Asterisks (*) indicates a given score meets the inclusion criterion set in advance of the study.</p></caption>
<graphic xlink:href="666588v1_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>In cases where potential DP participants met the inclusion criteria for only one of the two face recognition tests, participants were given a third test. If a participant‚Äôs CFMT score was in the normal range, they were administered the CFMT-Australian (<xref ref-type="bibr" rid="c60">McKone et al., 2011</xref>); if their Famous Face Test score was in the normal range, they were tested with an old/new test involving female faces (<xref ref-type="bibr" rid="c22">Duchaine and Nakayama, 2005</xref>). The CFMT-Australian follows the same format and scoring as the original CFMT. The old/new test required memorization of ten target faces, followed by 50 test trials (20 targets plus 30 distractors). Performance on the old/new test was evaluated by calculating A prime scores. Participants were included in the study if they scored below the threshold (CFMT-Australian: ùëç &lt; -1.8; Old/new faces: ùëç &lt; -2) on either of the follow-up tests.</p>
<p>To ensure that the DPs‚Äô impairments in face recognition reflected a high-level deficit rather than a deficit to early visual processing, each DP recruit was assessed with the Hanover Early Visual Assessment (<xref ref-type="bibr" rid="c47">Kieseler et al., 2022</xref>). Any potential recruit with a score below 1.8 SDs below the mean of a control sample (ùëÅ = 117) was not given further consideration for the study. This left a final sample of DPs that scored very similarly to controls (Controls: mean¬±SE = 90.0¬±13.3; DPs: mean¬±SE = 95.6¬±7.8).</p>
<p>Because rates of face recognition impairments are elevated in Autism Spectrum condition (<xref ref-type="bibr" rid="c34">Griffin et al., 2021</xref>), participants were also screened for subthreshold autistic traits using the self-report Subthreshold Autism Trait Questionnaire (SATQ; <xref ref-type="bibr" rid="c43">Kanne et al. (2012)</xref>). In our analysis of the SATQ responses, we omitted 5 of the 24 questions that measured behavioral tendencies likely to be influenced by poor face recognition (‚Äú<italic>I enjoy social situations where I can meet new people and chat (i.e. parties, dances, sports, and games)</italic>‚Äù; ‚Äú<italic>I seek out and approach others for social interactions</italic>‚Äù; ‚Äú<italic>Others think I am strange or bizarre</italic>‚Äù; ‚Äú<italic>I have some behaviors that others consider strange or odd</italic>‚Äù; ‚Äú<italic>I make eye contact when talking with others</italic>‚Äù). All DPs were less than 2 SDs above the mean of a control sample (ùëÅ = 133; <xref ref-type="bibr" rid="c8">Bell et al. (2023)</xref>).</p>
</sec>
<sec id="s5c">
<title>Experiment design</title>
<sec id="s5c1">
<title>Session 1: Functional localizer</title>
<p>To localize face-selective regions of interest (ROIs), a category localizer was run that presented six classes of stimuli: faces, bodies, objects, phase-scrambled objects, natural scenes, and words. Stimuli consisted of dynamic video clips featuring various visual elements in motion, all filmed against a uniform black backdrop to maximize contrast and visibility. Videos of the natural scenes filled the entire frame and displayed a slow forward camera movement equivalent to a normal walking pace from a first-person perspective. Video clips subtended ‚àº26¬∞ ùë• 15¬∞ of visual angle in width and height. Participants viewed four runs, each of which lasted 8 minutes and 22 seconds. Within each run, stimuli were grouped into 14-second blocks containing five videos each. Blocks of each category were displayed four times in each run in a quasi-random order, twice in color and twice in grayscale. The trial order was the same for all participants. Throughout the experiment, participants were allowed to freely move their eyes and instructed to identify back-to-back repeats with a button press (1-back task). Each of the four runs presented unique stimuli.</p>
<p>Face-selective ROIs were created by analyzing voxels within masks centered on associated anatomical landmarks and selecting the 20% of the voxels within the mask with the highest <italic>t</italic>-values for the contrast comparing faces versus objects. This method was chosen for its ability to generate individually tailored functional ROIs, while at the same time ensuring equal voxel counts across participants as well as retention of data from participants whose activations might otherwise fall below fixed thresholds (<xref ref-type="bibr" rid="c41">Jiahui et al., 2018</xref>; <xref ref-type="bibr" rid="c64">Norman-Haignere et al., 2013</xref>). Additionally, an ROI on the parahippocampal gyrus selective more for scenes than objects (parahippocampal place area or PPA <xref ref-type="bibr" rid="c27">Epstein and Kanwisher, 1998</xref>) was included to serve as a category-selective control region not involved in face perception.</p>
</sec>
<sec id="s5c2">
<title>Session 2: pRF mapping</title>
<p>Each participant completed five runs of retinotopy to identify the portion of the visual field capable of eliciting a response from the population of neurons within a voxel (i.e., each voxel‚Äôs pRF). In each run, participants fixated a central dot (0.2¬∞ x 0.2¬∞) while apertures of various shapes and sizes gradually swept across the visual field revealing colorful high-level images flickering at a rate of 5 Hz.</p>
<p>To elicit a strong BOLD response across a variety of category-selective areas, we used stimuli introduced by <xref ref-type="bibr" rid="c9">Benson et al. (2018)</xref>. These stimuli were created by taking cutout images from <xref ref-type="bibr" rid="c49">Kriegeskorte et al. (2008)</xref> (containing faces, body parts, foods, objects, street signs, animals, and architectural elements, etc.) and placing them at random positions and scales on achromatic pink-noise backgrounds (1/ùëì amplitude spectrum). As such, we refer to them as <italic>‚Äúimage mashup‚Äù</italic>. Our early pilot studies demonstrated the image mashup stimuli were effective at driving responses in face-selective regions but not scene-selective regions, which were of tangential interest to our study of DP (see <xref ref-type="bibr" rid="c41">Jiahui et al. (2018)</xref>). Therefore, we interleaved the image mashup stimuli with photographs of natural landscapes, and pilot data showed this combined approach generated a strong response in both face-selective <italic>and</italic> scene-selective regions. The image mashup and natural landscape stimuli alternated at a rate of 5 Hz, so each 2 s aperture presentation included an equal number of presentations of each stimulus type. Apertures and images were prepared at a resolution of 1060 x 1060 pixels and were constrained to a central circular region of diameter 14.7¬∞. All stimuli were presented against a medium gray background with a faint semi-transparent polar grid pattern superimposed to facilitate fixation.</p>
<p>Three different types of apertures were presented in every run ‚Äì a full field disk, a sequence of bars, and a rotating wedge (a schematic diagram is included in <xref rid="figs5" ref-type="fig">Supplemental Figure 5</xref>). Apertures progressed the same way in every run: After 8 s of initial blank fixation, a disk-shaped aperture appeared that revealed stimuli within the entire circular extent of the visual field, lasting for a duration of 4 s. Following 6 s of rest, bar-shaped apertures traversed the visual field in sweeps of 18 discrete, evenly spaced steps, each lasting 2 s. Each bar presentation overlapped 50% of the area of the previous position. Eight sweeps occurred, each interleaved with 6 s of rest, in the following order for all runs: Top‚ÄìBottom, Bottom‚ÄìTop, Right‚ÄìLeft, Left‚ÄìRight, Upper Right‚ÄìBottom Left, Bottom Left‚ÄìUpper Right, Upper Left‚ÄìBottom Right, Bottom Right‚ÄìUpper Left. Following another 6 s of rest, a 90¬∞ wedge appeared in the upper left quadrant and made 16 discrete clockwise steps, 2 s each, sweeping a full 360¬∞ around the central circular visual field (22.5¬∞ turn per step). Each run ended with 17 s of final fixation.</p>
<p>Throughout all retinotopy scans, participants performed a color detection task at fixation in which they reported via a button press when the white fixation dot changed to red. The central dot changed color to either red, green, or blue semi-randomly at a rate of approximately 30 per minute.</p>
</sec>
</sec>
<sec id="s5d">
<title>MRI data acquisition</title>
<p>All functional and structural images were acquired using a 3 Tesla Siemens Prisma scanner at the Dartmouth Brain Imaging Center at Dartmouth College. A 32-channel receive-only phased array head coil was used for all data acquisition.</p>
<p><italic>T1 images:</italic> For each participant, a high-resolution whole-brain anatomical volume was collected using a T1-weighted magnetization-prepared rapid acquisition gradient echo (MPRAGE) sequence (1 mm isovoxel resolution; 208 sagittal slices; TR = 2,300 ms; TE = 2.03 ms; flip angle = 9<sup>‚ó¶</sup>; FOV = 256 x 256 mm, bandwidth = 240 Hz/px).</p>
<p><italic>Functional images:</italic> Two types of functional scans were acquired across two sessions, both using a T2*-weighted gradient-recalled echoplanar imaging multiband pulse sequence. Across all scans, slices were oriented approximately in plane with the calcarine sulcus to ensure coverage of most, and often all, of the temporal, parietal, and occipital lobes. At the beginning of each session, a pair of EPI images with phase encoding directions of opposite polarity in the anterior-to-posterior plane were collected for post-hoc correction of EPI spatial distortion. Session 1 consisted of localizer scans designed to identify category-selective regions of interest (nominal spatial resolution = 2 mm x 2 mm x 2mm; 69 oblique slices; TR = 2,000 ms; TE = 30 ms; flip angle = 79<sup>‚ó¶</sup>; matrix size 106 x 106; field of view = 212 mm x 212 mm; phase partial Fourier scheme of 6/8; bandwidth = 1,814 Hz/px; echo spacing = 0.66 ms; excite pulse duration = 8,200 microseconds; multiband acceleration factor = 3; phase encoding direction = AP). Session 2 comprised the event-related retinotopy scans and therefore used a scanning protocol that prioritized a faster TR to sample the hemodynamic response more finely (nominal spatial resolution = 2 mm x 2 mm x 2mm; 69 oblique slices; TR = 1,350 ms; TE = 33.60 ms; flip angle = 68<sup>‚ó¶</sup>; matrix size 106 x 106; field of view = 212 mm x 212 mm; phase partial Fourier scheme off; bandwidth = 2358 Hz/px; echo spacing = 0.54 ms; excite pulse duration = 5,140 microseconds; multiband acceleration factor = 4; phase encoding direction = AP)</p>
</sec>
<sec id="s5e">
<title>Stimulus display and scanner peripherals</title>
<p>Stimuli were presented using a Panasonic PT-DW750BU 7000-Lumen WXGA DLP Projector and an SR Research in-bore back-projection screen positioned just inside the head end of the magnet. Use of an in-bore screen allowed for a stimulus that subtended a greater visual angle. Participants viewed the screen via a mirror mounted atop the head coil. The viewing distance was 127 mm from the eye to the mirror + 1,010 mm from the mirror to the screen = 1,137 mm total. The maximum extent of the image projected on the screen was 297 mm x 297 mm. This resulted in a maximum possible visual angle of 14.97 ¬∞ square.</p>
<p>A Lenovo Thinkpad T480s computer running Linux Ubuntu 20.04.6 (Focal Fossa) controlled the stimulus presentations and recorded button presses using Matlab R2022a and extensions from PsychToolbox version 3.0.18.</p>
<p>To verify accurate and steady eye fixation during the retinotopic mapping runs, eye movements were recorded using an SR Research Eyelink 1000 eyetracker mounted underneath the back projection screen. Eye tracking was performed for the right eye, and samples were obtained at 250 Hz using the Pupil-CR centroid mode with default thresholding. Black gaffer tape was applied to the eye cutouts of the headcoil in an effort to reduce unwanted reflections from the infrared illuminator. Before each run, a calibration, validation, and drift correction was performed, and data was discarded if the results of the calibration were poor. During data collection, the experimenters monitored real-time plots of eye position shown on the Eyelink host computer located inside the MRI control room. Experimenters intervened and restarted data collection if eye fixation was not maintained steadily enough throughout the run. The eyetracking data, however, was low-quality for some participants. Notably, achieving sufficient pupil contrast was more difficult for participants who required MR-compatible corrective lenses. In addition, the nose bridge on the head coil sometimes partially occluded the pupil for participants with narrow inter-pupillary distance. High-quality eye-tracking data was obtained from 14/20 controls and 14/20 DPs. Based on visual inspection of the eye gaze recordings, one control participant was discarded due to insufficiently stable eye fixation.</p>
<p>Behavioral responses from scanning sessions were collected using a Current Designs two-button fiber optic handheld response pad connected to a Current Designs 932 interface.</p>
</sec>
<sec id="s5f">
<title>Pre-processing of MRI data</title>
<p>All DICOM images were converted to NIfTI format using dcm2niix version 1.0.2018.11.25 (<xref ref-type="bibr" rid="c54">Li et al., 2016</xref>). From each participant‚Äôs T1-weighted volume, cortical surface meshes of the pial/gray matter boundaries and gray matter/white matter boundaries were reconstructed using Freesurfer‚Äôs <italic>recon-all</italic> program (<xref ref-type="bibr" rid="c30">Fischl and Dale, 2000</xref>). Functional data from the retinotopy scans were temporally resampled with cubic interpolation to correct for differences in slice time acquisition and to simultaneously upsample the time resolution from 1,350 ms to 1,000 ms with a custom Matlab script.</p>
<p>Subsequent pre-processing was performed using AFNI version 23.3.12 Septimius Severus (<xref ref-type="bibr" rid="c16">Cox, 1996</xref>). AFNI‚Äôs <italic>afni_proc.py</italic> was used to create processing pipelines for both the localizer and retinotopy scans. Slice timing differences were corrected, and non-linear geometric distortion correction was applied using separately acquired volumes in which the phase-encoding direction was reversed (‚Äòblip-up/blip-down‚Äô strategy). EPI-to-anatomical alignment was carried out using the lpc+ZZ cost function while checking for possible left-right flips (<xref ref-type="bibr" rid="c71">Reynolds et al., 2023</xref>). Motion correction was applied, and timepoints were censored when the Euclidean norm (<italic>enorm</italic>) exceeded 0.3 mm or when the outlier fraction exceeded 5%. Spatial blurring of FWHM = 4 mm (twice the EPI voxel dimensions) was applied to the localizer data but omitted from preprocessing of the retinotopy data. Volumetric data from each scan was then projected onto the corresponding hemispheres of the high-density SUMA standard meshes (created from the Freesurfer output with AFNI function @SUMA_Make_Spec_FS). Finally, each time series was scaled to units of local BOLD percent signal change. All pre-processing results were carefully visually inspected to ensure quality control using the QC HTML reports generated by <italic>afni_proc.py</italic> (<xref ref-type="bibr" rid="c71">Reynolds et al., 2023</xref>).</p>
</sec>
<sec id="s5g">
<title>pRF analysis</title>
<p>The population receptive field (pRF) was modeled using the compressive spatial summation (CSS) model (<xref ref-type="bibr" rid="c45">Kay et al., 2013a</xref>), which was fit independently to each voxel‚Äôs preprocessed timecourse. The CSS model fits a 2D isotropic Gaussian to each voxel of the form:
<disp-formula id="disp-eqn-1">
<graphic xlink:href="666588v1_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with parameters ùë•<sup>‚Ä≤</sup> and ùë¶<sup>‚Ä≤</sup> describing the center coordinates of the pRF, and ùúé, the standard deviation of its Gaussian profile. The Gaussian is normalized to the unit area to make the amplitude of the response more interpretable as a percent increase.</p>
<p>The input stimulus to the CSS model consists solely of the spatial extent (not content) of the flickering imagery revealed through the aperture or mask at each timepoint. Therefore, the stimuli for the purpose of model fitting were prepared by converting the aperture masks to binarized (contrast) images with values including 0 (no contrast) and 1 (full contrast). Masks were downsampled to 200 by 200 pixels and then concatenated across time. The predicted response at each timepoint was obtained by computing the overlap (via dot product) between each stimulus mask and the 2D Gaussian, then multiplying it by a non-negative scaling factor, ùëî for gain, and exponentiating the result by a parameter ùëõ constituting a static power-law nonlinearity. Afterward, the resulting time series was convolved with a model of the hemodynamic response function. The predicted BOLD response at each timepoint, ùêµ(<italic>t</italic>), can therefore be expressed formally as:
<disp-formula id="disp-eqn-2">
<graphic xlink:href="666588v1_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where ùëÜ(ùë•, ùë¶, <italic>t</italic>) is the contrast image at timepoint <italic>t</italic>, ùê∫ is the 2D isotropic Gaussian, and ‚Ñé(<italic>t</italic>) is the hemodynamic response function used in SPM. The static power-law nonlinearity, ùëõ, is included because it captures the nonlinear spatial summation and position tolerance of pRFs, particularly in the later stages of the visual hierarchy, in a single parameter that is easy to interpret (<xref ref-type="bibr" rid="c29">Finzi et al., 2021</xref>; <xref ref-type="bibr" rid="c45">Kay et al., 2013a</xref>; <xref ref-type="bibr" rid="c51">Le et al., 2017</xref>). The parameter ùëõ is typically observed to be less than 1 making it a <italic>compressive</italic> exponent.</p>
<p>After fitting the five pRF parameters (ùë•, ùë¶, ùúé, ùëî, ùëõ) in the CSS model, the Cartesian coordinates, (ùë•, ùë¶), of the pRF centers were converted to polar coordinates to produce maps of polar angle, <inline-formula id="inline-eqn-4"><inline-graphic xlink:href="666588v1_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and eccentricity, <inline-formula id="inline-eqn-5"><inline-graphic xlink:href="666588v1_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. pRF size was defined as <inline-formula id="inline-eqn-6"><inline-graphic xlink:href="666588v1_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Lastly, pRF size and center location were converted from pixel units to degrees of visual angle.</p>
<p>Model fitting was performed using the Levenberg-Marquardt approach to nonlinear parameter estimation (MATLAB function <monospace>lsqcurvefit()</monospace> from the Optimization Toolbox). The initial seed for the solver was chosen by pre-computing model predictions for 4,632 grid points, with different combinations of ùë•, ùë¶, ùúé, and ùëõ, and picking the point that produces the highest correlation with the observed data. To avoid local minima, a two-stage refined fitting procedure was implemented. In the first stage, we optimized the ùë•, ùë¶, ùúé, and ùëî parameters with the ùëõ parameter, controlling the compressive nonlinearity, fixed at one of three values (0.5, 0.25, or 0.125, chosen based on the prior grid search). In the second stage, the seed was updated to the solution found in the first stage, and then all five parameters were optimized simultaneously. Lower bounds of 0 and 0.1 were placed on the ùúé and ùëõ parameters, respectively. The maximum allowed iterations was 500, and the step tolerance was 1ùëí ‚àí 6. Data from all five runs was averaged together in time prior to fitting pRFs.</p>
</sec>
<sec id="s5h">
<title>Eye tracking analysis</title>
<p>For participants for whom eye movements were successfully recorded (14/20 controls and 14/20 DPs), the time series of horizontal and vertical eye positions were obtained and parsed by the proprietary Eyelink software . All pre-processing steps were performed separately by run. First, time periods detected by the Eyelink software as blinks were removed along with an additional 300 msecs on either shoulder. Censored datapoints were then interpolated using a method based on discrete cosine transforms (<xref ref-type="bibr" rid="c90">Wang et al., 2012</xref>), and the resultant time series were low-pass zero-phase filtered using a second-order Butterworth filter with cutoff frequency of 15 Hz. Then, the data were detrended using a linear and quadratic polynomial to account for slow drifts in gaze position. Finally, the data were median-centered, the blink periods were recensored, and the time series were downsampled to 50 Hz. Units for all eye fixations were converted from pixel coordinates on the display screen to degrees of visual angle.</p>
<p>Fixation performance for each subject and run was summarized by computing the area of the 95% probability contour (in units of degrees of visual angle squared) encircling the distribution of eye positions.</p>
</sec>
</sec>
</body>
<back>
<sec id="s6" sec-type="data-availability">
<title>Data Availability</title>
<p>The raw fMRI recordings from both the functional localizer and retinotopy experiments are available via OpenNeuro at <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds006472">https://openneuro.org/datasets/ds006472</ext-link>.</p>
</sec>
<sec id="s7">
<title>Code Availability</title>
<p>The code used to preprocess the raw fMRI timeseries and fit pRFs to the resultant timecourses is available via Figshare at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.29582888.v1">https://doi.org/10.6084/m9.figshare.29582888.v1</ext-link>.</p>
</sec>
<sec sec-type="supplementary" id="supplementary26">
<title>Supplementary Figures</title>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 0‚Äîfigure 5.</label>
<caption><title>Schematic diagram of retinotopic mapping runs.</title>
<p>Diagram is for illustration purposes only and not drawn to scale. All original faces have been replaced with AI-generated faces to preserve identifying information of original people. Runs began with a 4 second full-field exposure followed by eight sweeps of bar stimuli in eight different directions and ending with a single clockwise rotation of a wedge. Participants fixated a central dot throughout each run and indicated via button press whenever the dot changed color to red. The carrier images shown inside each aperture changed every 250 msecs (5 Hz), alternating between natural outdoor scenes and collages of various visual objects on backgrounds of pink noise (shown). See Methods for more details.</p></caption>
<graphic xlink:href="666588v1_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 0‚Äîfigure 6.</label>
<caption><title>Polar angle plots displayed on inflated cortical surfaces for all control participants.</title></caption>
<graphic xlink:href="666588v1_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 0‚Äîfigure 7.</label>
<caption><title>Polar angle plots displayed on inflated cortical surfaces for all DP participants.</title></caption>
<graphic xlink:href="666588v1_figs7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 0‚Äîfigure 8.</label>
<caption><title>Two dimensional density plots (heatmaps) of eye position recordings by subject (controls) and run.</title></caption>
<graphic xlink:href="666588v1_figs8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 0‚Äîfigure 9.</label>
<caption><title>Two dimensional density plots (heatmaps) of eye position recordings by subject (DPs) and run.</title></caption>
<graphic xlink:href="666588v1_figs9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs10" position="float" orientation="portrait" fig-type="figure">
<label>Appendix 0‚Äîfigure 10.</label>
<caption><title>Eye fixation performance during the retinotopic mapping experiment for controls and DPs.</title>
<p>Eye fixation performance was summarized by computing the area of the 95% probability contour of eye position samples. Points are averages across all runs obtained. Diamonds are averages for each group (Controls, DPs).</p></caption>
<graphic xlink:href="666588v1_figs10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls3" orientation="portrait" position="float">
<label>Appendix 0‚Äîtable 3.</label>
<caption><title>Face-selectivity was defined as the percent signal change to blocks of face stimuli minus the percent signal change to blocks of object stimuli.</title>
<p>To quantitatively compare face-selectivity in DPs and controls, we used the variable window method (<xref ref-type="bibr" rid="c64">Norman-Haignere et al., 2013</xref>) whereby voxels were selected for the analysis by applying group-defined region of interest (ROI) masks and choosing the top 20% of the voxels with the highest <italic>t</italic> value for the faces-minus-objects contrast. Voxel selection was fully cross-validated in a leave-one-run-out fashion ‚Äì voxels were selected based on three out of four runs and face selectivity was measured from the left out run. For each participant, the final measure of face-selectivity was the average across four cross-validated folds. For each ROI and hemisphere, separate Welche two sample <italic>t</italic>-tests were conducted comparing data from DPs to controls. Due to the number of tests conducted, a more conservative alpha threshold of <italic>p</italic> &lt; 0.01 was used to establish statistical significance. * <italic>p</italic> &lt; 0.01; ** <italic>p</italic> &lt; 0.001</p></caption>
<graphic xlink:href="666588v1_tbls3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls4" orientation="portrait" position="float">
<label>Appendix 0‚Äîtable 4.</label>
<caption><title>Fixed effect parameter estimates for models evaluating pRF model goodness-of-fit, <italic>R</italic><sup>2</sup>, by hemisphere (right, left) and group (control, DP).</title>
<p>Separate models were created for each region of interest. In each model, the intercept has been mapped to the control group in the left hemisphere. Formula (R, lme4 package): <italic>R</italic><sup>2</sup> ‚àº group + hemisphere + group:hemisphere + (1|subjectID/hemisphere) * <italic>p</italic> &lt; 0.01; ** <italic>p</italic> &lt; 0.001</p></caption>
<graphic xlink:href="666588v1_tbls4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We are truly grateful to all our research participants, whose dedication and enthusiasm was essential to this work. We extend our heartfelt thanks to Logan Dowdle for his generous support in developing and testing the fMRI pulse sequences used. We also deeply appreciate the thoughtful guidance and encouragement provided by Kalanit Grill-Spector, Edward Silson, and Ivan Alvarez throughout various stages of the project. This work was made possible by the generous support of the National Institutes of Health (NIH) under grants 1R01EY030613-01A1 to Bradley Duchaine and R01EY034118 to Kendrick Kay.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alvarez</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>de Haas</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Clark</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Rees</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Samuel Schwarzkopf</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Comparing different stimulus configurations for population receptive field mapping in human fMRI</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>9</volume>(<issue>FEB</issue>):<fpage>1</fpage>‚Äì<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Avidan</surname>, <given-names>G.</given-names></string-name> and <string-name><surname>Behrmann</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Functional MRI Reveals Compromised Neural Integrity of the Face Processing Network in Congenital Prosopagnosia</article-title>. <source>Current Biology</source>, <volume>19</volume>(<issue>13</issue>):<fpage>1146</fpage>‚Äì<lpage>1150</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Avidan</surname>, <given-names>G.</given-names></string-name> and <string-name><surname>Behrmann</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Spatial Integration in Normal Face Processing and Its Breakdown in Congenital Prosopagnosia</article-title>. <source>Annual Review of Vision Science, pages</source> <fpage>1</fpage>‚Äì<lpage>21</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Avidan</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Tanzer</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Behrmann</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Impaired holistic processing in congenital prosopagnosia</article-title>. <source>Neuropsychologia</source>, <volume>49</volume>(<issue>9</issue>):<fpage>2541</fpage>‚Äì<lpage>2552</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bate</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Tree</surname>, <given-names>J. J</given-names></string-name></person-group>. (<year>2017</year>). <article-title>The definition and diagnosis of developmental prosopagnosia</article-title>. <source>The Quarterly Journal of Experimental Psychology</source>, <volume>70</volume>(<issue>2</issue>):<fpage>193</fpage>‚Äì<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bates</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Maechler</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bolker</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Walker</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Fitting Linear Mixed-Effects Models using lme4</article-title>. <source>Journal of Statistical Software</source>, <volume>67</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>48</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baudouin</surname>, <given-names>J. Y.</given-names></string-name> and <string-name><surname>Humphreys</surname>, <given-names>G. W</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Configural information in gender categorisation</article-title>. <source>Perception</source>, <volume>35</volume>(<issue>4</issue>):<fpage>531</fpage>‚Äì<lpage>540</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bell</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Duchaine</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Susilo</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Dissociations between face identity and face expression processing in developmental prosopagnosia</article-title>. <source>Cognition</source>, <volume>238</volume>(<issue>April</issue>):<fpage>105469</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benson</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Jamison</surname>, <given-names>K. W.</given-names></string-name>, <string-name><surname>Arcaro</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Vu</surname>, <given-names>A. T.</given-names></string-name>, <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Coalson</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2018</year>). <article-title>The Human Connectome Project 7 Tesla retinotopy dataset: Description and population receptive field analysis</article-title>. <source>Journal of Vision</source>, <volume>18</volume>(<issue>13</issue>):<fpage>1</fpage>‚Äì<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biotti</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Jiahui</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Duchaine</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Cook</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Normal composite face effects in developmental prosopagnosia</article-title>. <source>Cortex</source>, <volume>95</volume>:<fpage>63</fpage>‚Äì<lpage>76</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boekel</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Wagenmakers</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Belay</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Verhagen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Forstmann</surname>, <given-names>B. U</given-names></string-name></person-group>. (<year>2015</year>). <article-title>A purely confirmatory replication study of structural brain-behavior correlations</article-title>. <source>Cortex</source>, <volume>66</volume>:<fpage>115</fpage>‚Äì<lpage>133</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brewer</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wade</surname>, <given-names>A. R.</given-names></string-name>, and <string-name><surname>Wandell</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Visual field maps and stimulus selectivity in human ventral occipital cortex</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>(<issue>8</issue>):<fpage>1102</fpage>‚Äì<lpage>1109</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Button</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>Ioannidis</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Mokrysz</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Nosek</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Flint</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>E. S.</given-names></string-name>, and <string-name><surname>Munaf√≤</surname>, <given-names>M. R</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Power failure: Why small sample size undermines the reliability of neuroscience</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>14</volume>(<issue>5</issue>):<fpage>365</fpage>‚Äì<lpage>376</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calder</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Keane</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Young</surname>, <given-names>A. W.</given-names></string-name>, and <string-name><surname>Dean</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2000</year>). <article-title>Configural information in facial expression perception</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>26</volume>(<issue>2</issue>):<fpage>527</fpage>‚Äì<lpage>551</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chatterjee</surname>, <given-names>G.</given-names></string-name> and <string-name><surname>Nakayama</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Normal facial age and gender perception in developmental prosopagnosia</article-title>. <source>Cognitive Neuropsychology</source>, <volume>29</volume>(<issue>5-6</issue>):<fpage>482</fpage>‚Äì<lpage>502</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cox</surname>, <given-names>R. W</given-names></string-name></person-group>. (<year>1996</year>). <article-title>AFNI : Software for Analysis and Visualization of Functional Magnetic Resonance Neuroimages</article-title>. <source>Computets and Biomedical Research</source>, (<volume>29</volume>):162‚Äì173.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Degutis</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Face gender recognition in developmental prosopagnosia : Evidence for holistic processing and use of configural information</article-title>. <source>Visual Cognition</source>, <volume>20</volume>(February 2013):37‚Äì41.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DeGutis</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bahierathan</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Barahona</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Evans</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Shin</surname>, <given-names>H. M.</given-names></string-name>, <string-name><surname>Mishra</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Likitlersuang</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Wilmer</surname>, <given-names>J. B</given-names></string-name></person-group>. (<year>2023</year>). <article-title>What is the prevalence of developmental prosopagnosia? An empirical assessment of different diagnostic cutoffs</article-title>. <source>Cortex</source>, <volume>161</volume>:<fpage>51</fpage>‚Äì<lpage>64</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deyoe</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Carman</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Bandettini</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Glickman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wieser</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cox</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Neitz</surname>, <given-names>J</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Mapping striate and extrastriate visual areas in human cerebral cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>93</volume>(<issue>6</issue>):<fpage>2382</fpage>‚Äì<lpage>2386</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dinkelacker</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Gr√ºter</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Klaver</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Gr√ºter</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Specht</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Weis</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kennerknecht</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Elger</surname>, <given-names>C. E.</given-names></string-name>, and <string-name><surname>Fernandez</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Congenital prosopagnosia: Multistage anatomical and functional deficits in face processing circuitry</article-title>. <source>Journal of Neurology</source>, <volume>258</volume>(<issue>5</issue>):<fpage>770</fpage>‚Äì<lpage>782</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Duchaine</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2011</year>). <chapter-title>Developmental Prosopagnosia: Cognitive, Neural, and Developmental Investigations</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Rhodes</surname>, <given-names>G.</given-names></string-name> and <string-name><surname>Haxby</surname>, <given-names>J</given-names></string-name></person-group>., editors, <source>Oxford Handbook of Face Perception</source>, chapter 42, pages <fpage>821</fpage>‚Äì<lpage>838</lpage>. <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duchaine</surname>, <given-names>B.</given-names></string-name> and <string-name><surname>Nakayama</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Dissociations of face and object recognition in developmental prosopagnosia</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>17</volume>(<issue>2</issue>):<fpage>249</fpage>‚Äì<lpage>261</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duchaine</surname>, <given-names>B.</given-names></string-name> and <string-name><surname>Nakayama</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2006</year>). <article-title>The Cambridge Face Memory Test: Results for neurologically intact individuals and an investigation of its validity using inverted face stimuli and prosopagnosic participants</article-title>. <source>Neuropsychologia</source>, <volume>44</volume>(<issue>4</issue>):<fpage>576</fpage>‚Äì<lpage>585</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dumoulin</surname>, <given-names>S. O.</given-names></string-name> and <string-name><surname>Wandell</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Population receptive field estimates in human visual cortex</article-title>. <source>NeuroImage</source>, <volume>39</volume>(<issue>2</issue>):<fpage>647</fpage>‚Äì<lpage>660</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Durand</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Gallay</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Seigneuric</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Robichon</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Baudouin</surname>, <given-names>J. Y</given-names></string-name></person-group>. (<year>2007</year>). <article-title>The development of facial emotion recognition: The role of configural information</article-title>. <source>Journal of Experimental Child Psychology</source>, <volume>97</volume>(<issue>1</issue>):<fpage>14</fpage>‚Äì<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engel</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Glover</surname>, <given-names>G. H.</given-names></string-name>, and <string-name><surname>Wandell</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Retinotopic Organization in Human Visual Cortex and the Spatial Precision of Functional MRI</article-title>. <source>Cerebral Cortex</source>, <volume>7</volume>(<issue>2</issue>):<fpage>181</fpage>‚Äì<lpage>192</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Epstein</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Kanwisher</surname>, <given-names>N</given-names></string-name></person-group>. (<year>1998</year>). <article-title>A cortical representation of the local visual environment</article-title>. <source>Nature</source>, <volume>392</volume>:<fpage>598</fpage>‚Äì <lpage>601</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Etzel</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Valchev</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Keysers</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2011</year>). <article-title>The impact of certain methodological choices on multivariate analysis of fMRI data with support vector machines</article-title>. <source>NeuroImage</source>, <volume>54</volume>(<issue>2</issue>):<fpage>1159</fpage>‚Äì<lpage>1167</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Finzi</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Gomez</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Nordt</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rezai</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Poltoratski</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Grill-Spector</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Differential spatial computations in ventral and lateral face-selective regions are scaffolded by structural connections</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name> and <string-name><surname>Dale</surname>, <given-names>A. M</given-names></string-name></person-group>. (<year>2000</year>). <article-title>Measuring the thickness of the human cerebral cortex from magnetic resonance images</article-title>. <source>PNAS</source>, <volume>2000</volume>(<issue>20</issue>):<fpage>11050</fpage>-<lpage>11055</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fur</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Garrido</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Driver</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Duchaine</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Fusiform gyrus face selectivity relates to individual differences in facial recognition ability</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>23</volume>(<issue>7</issue>):<fpage>1723</fpage>‚Äì<lpage>1740</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gerlach</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Klargaard</surname>, <given-names>S. K.</given-names></string-name>, <string-name><surname>Aln√¶s</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kolsk√•r</surname>, <given-names>K. K.</given-names></string-name>, <string-name><surname>Karstoft</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Westlye</surname>, <given-names>L. T.</given-names></string-name>, and <string-name><surname>Starrfelt</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Left hemisphere abnormalities in developmental prosopagnosia when looking at faces but not words</article-title>. <source>Brain Communications</source>, <volume>1</volume>(<issue>1</issue>).</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gomez</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Natu</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Jeska</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Barnett</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Grill-Spector</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Development differentially sculpts receptive fields across early and high-level human visual cortex</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>12</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Griffin</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Bauer</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Scherf</surname>, <given-names>K. S.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Supplemental Material for A Quantitative Meta-Analysis of Face Recognition Deficits in Autism: 40 Years of Research</article-title>. <source>Psychological Bulletin</source>, <volume>147</volume>(<issue>3</issue>):<fpage>268</fpage>‚Äì<lpage>292</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>Gomez</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Stigliani</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Natu</surname>, <given-names>V. S</given-names></string-name></person-group>. (<year>2018</year>). <article-title>The functional neuroanatomy of face perception: From brain measurements to deep neural networks</article-title>. <source>Interface Focus</source>, <volume>8</volume>(<issue>4</issue>).</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Groen</surname>, <given-names>I. I.</given-names></string-name>, <string-name><surname>Dekker</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Knapen</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Silson</surname>, <given-names>E. H</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Visuospatial coding as ubiquitous scaffolding for human cognition</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>26</volume>(<issue>1</issue>):<fpage>81</fpage>‚Äì<lpage>96</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hagler</surname>, <given-names>D. J.</given-names></string-name> and <string-name><surname>Sereno</surname>, <given-names>M. I</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Spatial maps in frontal and prefrontal cortex</article-title>. <source>NeuroImage</source>, <volume>29</volume>(<issue>2</issue>):<fpage>567</fpage>‚Äì<lpage>577</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harvey</surname>, <given-names>B. M.</given-names></string-name>, <string-name><surname>Vansteensel</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Ferrier</surname>, <given-names>C. H.</given-names></string-name>, <string-name><surname>Petridou</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Zuiderbaan</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Aarnoutse</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Bleichner</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Dijkerman</surname>, <given-names>H. C.</given-names></string-name>, <string-name><surname>van Zandvoort</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Leijten</surname>, <given-names>F. S.</given-names></string-name>, <string-name><surname>Ramsey</surname>, <given-names>N. F.</given-names></string-name>, and <string-name><surname>Dumoulin</surname>, <given-names>S. O.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Frequency specific spatial interactions in human electrocorticography: V1 alpha oscillations reflect surround suppression</article-title>. <source>NeuroImage</source>, <volume>65</volume>:<fpage>424</fpage>‚Äì<lpage>432</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hubel</surname>, <given-names>D. H.</given-names></string-name> and <string-name><surname>Wiesel</surname>, <given-names>T. N</given-names></string-name></person-group>. (<year>1965</year>). <article-title>Receptive Fields and Functional Architecture in Two Nonstriate Visual Areas (18 and 19) of the Cat</article-title>. <source>Journal of neurophysiology</source>, <volume>28</volume>(<issue>2</issue>):<fpage>229</fpage>‚Äì<lpage>289</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jednor√≥g</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Marchewka</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Altarelli</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Monzalvo Lopez</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>van Ermingen-Marbach</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Grande</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Grabowska</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Heim</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Ramus</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2015</year>). <article-title>How reliable are gray matter disruptions in specific reading disability across multiple countries and languages? Insights from a large-scale voxel-based morphometry study</article-title>. <source>Human Brain Mapping</source>, <volume>36</volume>(<issue>5</issue>):<fpage>1741</fpage>‚Äì<lpage>1754</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jiahui</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Duchaine</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Developmental prosopagnosics have widespread selectivity reductions across category-selective visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>115</volume>(<issue>28</issue>):<fpage>E6418</fpage>‚Äì<lpage>E6427</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaiser</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Quek</surname>, <given-names>G. L.</given-names></string-name>, <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, and <string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Object Vision in a Structured World</article-title>. <source>Trends Cogn Sci</source>. <volume>23</volume>(<issue>8</issue>):<fpage>672</fpage>-<lpage>685</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kanne</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Christ</surname>, <given-names>S. E</given-names></string-name></person-group>. (<year>2012</year>). <article-title>The Subthreshold Autism Trait Questionnaire (SATQ): Development of a brief self-report measure of subthreshold autism traits</article-title>. <source>Journal of Autism and Developmental Disorders</source>, <volume>42</volume>(<issue>5</issue>):<fpage>769</fpage>‚Äì<lpage>780</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, and <string-name><surname>Weiner</surname>, <given-names>K. S</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Attention Reduces Spatial Uncertainty in Human Ventral Temporal Cortex Attention Reduces Spatial Uncertainty in Human Ventral Temporal Cortex</article-title>. <source>Current Biology</source>, <volume>25</volume>(<issue>5</issue>):<fpage>595</fpage>‚Äì<lpage>600</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mezer</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Wandell</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>2013a</year>). <article-title>Compressive spatial summation in human visual cortex</article-title>. <source>J Neurophysiol</source>, <volume>94305</volume>:<fpage>481</fpage>‚Äì<lpage>494</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rokem</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mezer</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Wandell</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>2013b</year>). <article-title>A Two-Stage Cascade Model of BOLD Responses in Human Visual Cortex</article-title>. <source>PLoS Computational Biology</source>, <volume>9</volume>(<issue>5</issue>).</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kieseler</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dickstein</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Krafian</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Duchaine</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2022</year>). <article-title>HEVA-A new basic visual processing test</article-title>. <source>Journal of Vision</source>, <volume>22</volume>(<issue>14</issue>):<fpage>4109</fpage>‚Äì<lpage>4109</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kieseler</surname>, <given-names>M. L.</given-names></string-name> and <string-name><surname>Duchaine</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Persistent prosopagnosia following COVID-19</article-title>. <source>Cortex</source>, <volume>162</volume>:<fpage>56</fpage>‚Äì<lpage>64</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ruff</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Kiani</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bodurka</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Esteky</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Tanaka</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Bandettini</surname>, <given-names>P. A</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey</article-title>. <source>Neuron</source>, <volume>60</volume>(<issue>6</issue>):<fpage>1126</fpage>‚Äì<lpage>1141</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuznetsova</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Brockhoff</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Christensen</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2017</year>). <article-title>LmerTest Package: Tests in Linear Mixed Effects Models</article-title>. <source>Journal of Statistical Software</source>, <volume>82</volume>(<issue>13</issue>):<fpage>1</fpage>‚Äì<lpage>26</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Le</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Witthoft</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Ben-Shachar</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Wandell</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2017</year>). <article-title>The field of view available to the ventral occipito-temporal reading circuitry</article-title>. <source>Journal of Vision</source>, <volume>17</volume>(<issue>4</issue>):<fpage>1</fpage>‚Äì<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Le Grand</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Cooper</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Mondloch</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Lewis</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Sagiv</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>de Gelder</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Maurer</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2006</year>). <article-title>What aspects of face processing are impaired in developmental prosopagnosia?</article-title> <source>Brain and Cognition</source>, <volume>61</volume>(<issue>2</issue>):<fpage>139</fpage>‚Äì <lpage>158</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levine</surname>, <given-names>D. N.</given-names></string-name> and <string-name><surname>Calvanio</surname>, <given-names>R</given-names></string-name></person-group>. (<year>1989</year>). <article-title>Prosopagnosia: A defect in visual configural processing</article-title>. <source>Brain and Cognition</source>, <volume>10</volume>(<issue>2</issue>):<fpage>149</fpage>‚Äì<lpage>170</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Morgan</surname>, <given-names>P. S.</given-names></string-name>, <string-name><surname>Ashburner</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Rorden</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>The first step for neuroimaging data analysis : DICOM to NIfTI conversion</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>264</volume>:<fpage>47</fpage>‚Äì<lpage>56</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Linka</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Broda</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Alsheimer</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>de Haas</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Ramon</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Characteristic fixation biases in Super-Recognizers</article-title>. <source>Journal of Vision</source>, <volume>22</volume>(<issue>8</issue>).</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marsh</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Biotti</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Cook</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Gray</surname>, <given-names>K. L</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The discrimination of facial sex in developmental prosopagnosia</article-title>. <source>Scientific Reports</source>, <volume>9</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maurer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Grand</surname>, <given-names>R. L.</given-names></string-name>, and <string-name><surname>Mondloch</surname>, <given-names>C. J</given-names></string-name></person-group>. (<year>2002</year>). <article-title>The many faces of configural processing</article-title>. <source>Trends in cognitive sciences</source>, <volume>6</volume>(<issue>6</issue>):<fpage>255</fpage>‚Äì<lpage>260</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McConachie</surname>, <given-names>H. R</given-names></string-name></person-group>. (<year>1976</year>). <article-title>Developmental Prosopagnosia. A Single Case Report</article-title>. <source>Cortex</source>, <volume>12</volume>(<issue>1</issue>):<fpage>76</fpage>‚Äì<lpage>82</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McKone</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Holistic processing for faces operates over a wide range of sizes but is strongest at identification rather than conversational distances</article-title>. <source>Vision Research</source>, <volume>49</volume>(<issue>2</issue>):<fpage>268</fpage>‚Äì<lpage>283</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McKone</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Hall</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pidcock</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Palermo</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Wilkinson</surname>, <given-names>R. B.</given-names></string-name>, <string-name><surname>Rivolta</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Yovel</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Davis</surname>, <given-names>J. M.</given-names></string-name>, and <string-name><surname>O‚ÄôConnor</surname>, <given-names>K. B</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Face ethnicity and measurement reliability affect face recognition performance in developmental prosopagnosia: Evidence from the Cambridge face memory test-Australian</article-title>. <source>Cognitive Neuropsychology</source>, <volume>28</volume>(<issue>2</issue>):<fpage>109</fpage>‚Äì<lpage>146</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mckone</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Yovel</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Why does picture-plane inversion sometimes dissociate perception of features and spacing in faces, and sometimes not? toward a new theory of holistic processing</article-title>. <source>Psychonomic Bulletin and Review</source>, <volume>16</volume>(<issue>5</issue>):<fpage>778</fpage>‚Äì<lpage>797</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Morsi</surname>, <given-names>A. Y.</given-names></string-name>, <string-name><surname>Chow-wing bom</surname>, <given-names>H. T.</given-names></string-name>, <string-name><surname>Samuel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Goffaux</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Dekker</surname>, <given-names>T. M.</given-names></string-name>, and <string-name><surname>John</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Shared spatial selectivity in early visual cortex and face-selective brain regions</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nasiotis</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Clavagnier</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Baillet</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Pack</surname>, <given-names>C. C</given-names></string-name></person-group>. (<year>2017</year>). <article-title>High-resolution retinotopic maps estimated with magnetoencephalography</article-title>. <source>NeuroImage</source>, <volume>145</volume>(October 2016):107‚Äì117.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Norman-Haignere</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>McDermott</surname>, <given-names>J. H</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Cortical pitch regions in humans respond primarily to resolved harmonics and are located in specific tonotopic regions of anterior auditory cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>50</issue>):<fpage>19451</fpage>‚Äì<lpage>19469</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Palermo</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Willis</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Rivolta</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>McKone</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>C. E.</given-names></string-name>, and <string-name><surname>Calder</surname>, <given-names>A. J</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Impaired holistic coding of facial expression and facial identity in congenital prosopagnosia</article-title>. <source>Neuropsychologia</source>, <volume>49</volume>(<issue>5</issue>):<fpage>1226</fpage>‚Äì<lpage>1235</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pitcher</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Walsh</surname>, <given-names>V.</given-names></string-name>, and <string-name><surname>Duchaine</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2011</year>). <article-title>The role of the occipital face area in the cortical face perception network</article-title>. <source>Experimental Brain Research</source>, <volume>209</volume>(<issue>4</issue>):<fpage>481</fpage>‚Äì<lpage>493</lpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name>, <string-name><surname>Durnez</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gorgolewski</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Matthews</surname>, <given-names>P. M.</given-names></string-name>, <string-name><surname>Munaf√≤</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Poline</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Vul</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Yarkoni</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Scanning the horizon: Towards transparent and reproducible neuroimaging research</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>18</volume>(<issue>2</issue>):<fpage>115</fpage>‚Äì<lpage>126</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poltoratski</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Finzi</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Grill-Spector</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Holistic face recognition is an emergent phenomenon of spatial processing in face-selective regions</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>‚Äì<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quen√©</surname>, <given-names>H.</given-names></string-name> and <string-name><surname>Van Den Bergh</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2004</year>). <article-title>On multi-level modeling of data from repeated measures designs: A tutorial</article-title>. <source>Speech Communication</source>, <volume>43</volume>(<issue>1-2</issue>):<fpage>103</fpage>‚Äì<lpage>121</lpage>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramus</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Altarelli</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Jednor√≥g</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Scotto di Covella</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Neuroanatomy of developmental dyslexia: Pitfalls and promise</article-title>. <source>Neuroscience and Biobehavioral Reviews</source>, <volume>84</volume>(July 2017):434‚Äì452.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reynolds</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>P. A.</given-names></string-name>, and <string-name><surname>Glen</surname>, <given-names>D. R</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Quality control practices in FMRI analysis: Philosophy, methods and examples using AFNI</article-title>. <source>Frontiers in Neuroscience</source>, <volume>16</volume>(<issue>January</issue>):<fpage>1</fpage>‚Äì<lpage>36</lpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rossion</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Distinguishing the cause and consequence of face inversion: The perceptual field hypothesis</article-title>. <source>Acta Psychologica</source>, <volume>132</volume>(<issue>3</issue>):<fpage>300</fpage>‚Äì<lpage>312</lpage>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sereno</surname>, <given-names>M. I.</given-names></string-name>, <string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Reppas</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Kwong</surname>, <given-names>K. K.</given-names></string-name>, <string-name><surname>Belliveau</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Rosen</surname>, <given-names>B. R.</given-names></string-name>, and <string-name><surname>Tootell</surname>, <given-names>R. B</given-names></string-name></person-group>. (<year>1995</year>). <article-title>Borders of multiple visual areas in humans revealed by functional magnetic reson2ance imaging</article-title>. <source>Science</source>, <volume>268</volume>(<issue>5212</issue>):<fpage>889</fpage>‚Äì<lpage>893</lpage>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shah</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Gaule</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sowden</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bird</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Cook</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2015</year>). <article-title>The 20-item prosopagnosia index (PI20): A self-report instrument for identifying developmental prosopagnosia</article-title>. <source>Royal Society Open Science</source>, <volume>2</volume>(<issue>6</issue>).</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silson</surname>, <given-names>E. H.</given-names></string-name>, <string-name><surname>Chan</surname>, <given-names>X. A. W.-y.</given-names></string-name>, <string-name><surname>Reynolds</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Kravitz</surname>, <given-names>D. J.</given-names></string-name>, and <string-name><surname>Baker</surname>, <given-names>X. C. I.</given-names></string-name></person-group> (<year>2015</year>). <article-title>A Retinotopic Basis for the Division of High-Level Scene Processing between Lateral and Ventral Human Occipitotemporal Cortex</article-title>. <source>J.Neurosci</source>., <volume>35</volume>(<issue>34</issue>):<fpage>11921</fpage>‚Äì<lpage>11935</lpage>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silver</surname>, <given-names>M. A.</given-names></string-name> and <string-name><surname>Kastner</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Topographic maps in human frontal and parietal cortex</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>13</volume>(<issue>11</issue>):<fpage>488</fpage>‚Äì<lpage>495</lpage>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Samuel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kanai</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Rees</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Schwarzkopf</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Kanai</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Rees</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Neural Population Tuning Links Visual Cortical Anatomy to Human Visual Perception</article-title>. <source>Neuron</source>, <volume>85</volume>(<issue>3</issue>):<fpage>641</fpage>‚Äì<lpage>656</lpage>.</mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Silson</surname>, <given-names>E. H.</given-names></string-name>, <string-name><surname>Garcia</surname>, <given-names>B. D.</given-names></string-name>, and <string-name><surname>Robertson</surname>, <given-names>C. E</given-names></string-name></person-group>. (<year>2024</year>). <article-title>A retinotopic code structures the interaction between perception and memory systems</article-title>. <source>Nature Neuroscience</source>, <volume>27</volume>(<issue>2</issue>):<fpage>339</fpage>‚Äì<lpage>347</lpage>.</mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Susilo</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Duchaine</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Advances in developmental prosopagnosia research</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>23</volume>(<issue>3</issue>):<fpage>423</fpage>‚Äì<lpage>429</lpage>.</mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Susilo</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>McKone</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Dennett</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Darke</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Palermo</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Hall</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pidcock</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dawel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jeffery</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>C. E.</given-names></string-name>, and <string-name><surname>Rhodes</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Face recognition impairments despite normal holistic processing and face space coding: Evidence from a case of developmental prosopagnosia</article-title>. <source>Cognitive Neuropsychology</source>, <volume>27</volume>(<issue>8</issue>):<fpage>636</fpage>‚Äì<lpage>664</lpage>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szinte</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Knapen</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Visual Organization of the Default Network</article-title>. <source>Cerebral Cortex</source>, <volume>30</volume>(<issue>6</issue>):<fpage>3518</fpage>‚Äì<lpage>3527</lpage>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szucs</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>Ioannidis</surname>, <given-names>J. P</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature</article-title>. <source>PLoS Biology</source>, <volume>15</volume>(<issue>3</issue>):<fpage>1</fpage>‚Äì<lpage>18</lpage>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanaka</surname>, <given-names>J. W.</given-names></string-name> and <string-name><surname>Farah</surname>, <given-names>M. J</given-names></string-name></person-group>. (<year>1993</year>). <article-title>Parts and wholes in face recognition</article-title>. <source>The Quarterly Journal of Experimental Psychology Section A</source>, <volume>46</volume>(<issue>2</issue>):<fpage>225</fpage>‚Äì<lpage>245</lpage>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Towler</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fisher</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Eimer</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Holistic face perception is impaired in developmental prosopagnosia</article-title>. <source>Cortex</source>, <volume>108</volume>:<fpage>112</fpage>‚Äì<lpage>126</lpage>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsantani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gray</surname>, <given-names>K. L.</given-names></string-name>, and <string-name><surname>Cook</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Holistic processing of facial identity in developmental prosopagnosia</article-title>. <source>Cortex</source>, <volume>130</volume>:<fpage>318</fpage>‚Äì<lpage>326</lpage>.</mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turner</surname>, <given-names>B. O.</given-names></string-name>, <string-name><surname>Paul</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>M. B.</given-names></string-name>, and <string-name><surname>Barbey</surname>, <given-names>A. K</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Small sample sizes reduce the replicability of task-based fMRI studies</article-title>. <source>Communications Biology</source>, <volume>1</volume>(<issue>1</issue>).</mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Es</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>van der Zwaag</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Knapen</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Topographic Maps of Visual Space in the Human Cerebellum</article-title>. <source>Current Biology</source>, <volume>29</volume>(<issue>10</issue>):<fpage>1689</fpage>‚Äì<lpage>1694</lpage>.</mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Verfaillie</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Huysegems</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>De Graef</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Van Belle</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Impaired holistic and analytic face processing in cong prosop Evidence from eye contingent mask window</article-title>. <source>Visual Cognition</source>, <volume>22</volume>(<issue>3</issue>):<fpage>503</fpage>‚Äì<lpage>521</lpage>.</mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> and <string-name><surname>Winawer</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Computational neuroimaging and population receptive fields</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>19</volume>(<issue>6</issue>):<fpage>349</fpage>‚Äì<lpage>357</lpage>.</mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Garcia</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>de Jeu</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Johannes Dolman</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2012</year>). <article-title>A three-dimensional gap filling method for large geophysical datasets: Application to global satellite soil moisture observations</article-title>. <source>Environmental Modelling and Software</source>, <volume>30</volume>:<fpage>139</fpage>‚Äì<lpage>142</lpage>.</mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name> and <string-name><surname>Grill-spector</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Sparsely-distributed organization of face and limb activations in human ventral temporal cortex</article-title>. <source>NeuroImage</source>, <volume>52</volume>(<issue>4</issue>):<fpage>1559</fpage>‚Äì<lpage>1573</lpage>.</mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Foster</surname>, <given-names>B. L.</given-names></string-name>, <string-name><surname>Rauschecker</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Parvizi</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Wandell</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Asynchronous broadband signals are the principal source of the bold response in human visual cortex</article-title>. <source>Current Biology</source>, <volume>23</volume>(<issue>13</issue>):<fpage>1145</fpage>‚Äì<lpage>1153</lpage>.</mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Witthoft</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Identification of the ventral occipital visual field maps in the human brain</article-title>. <source>F1000Research</source>, 6(0):1‚Äì18.</mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Witthoft</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Poltoratski</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Golarai</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Liberman</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>LaRocque</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Grill-Spector</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Reduced spatial integration in the ventral visual cortex underlies face recognition deficits in developmental prosopagnosia</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yin</surname>, <given-names>R. K</given-names></string-name></person-group>. (<year>1969</year>). <article-title>Looking at Upside-Down Faces</article-title>. <source>Journal of Experimental Psychology</source>, <volume>81</volume>(<issue>1</issue>):<fpage>141</fpage>‚Äì<lpage>145</lpage>.</mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yokoyama</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Noguchi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Tachibana</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mukaida</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Kita</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2014</year>). <article-title>A critical role of holistic processing in face gender perception</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>8</volume>(<issue>JUNE</issue>):<fpage>1</fpage>‚Äì<lpage>10</lpage>.</mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Young</surname>, <given-names>A. W.</given-names></string-name>, <string-name><surname>Hellawell</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Hay</surname>, <given-names>D. C</given-names></string-name></person-group>. (<year>1987</year>). <article-title>Configural information in face perception</article-title>. <source>Perception</source>, <volume>16</volume>:<fpage>747</fpage>‚Äì <lpage>759</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108657.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Huxlin</surname>
<given-names>Krystel R</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/022kthw22</institution-id><institution>University of Rochester</institution>
</institution-wrap>
<city>Rochester</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This functional MRI study critically tests the hypothesis that poor face recognition in developmental prosopagnosia in humans is driven by reduced spatial integration and smaller receptive fields in face-selective brain regions. The evidence provided is <bold>compelling</bold> as it is well-powered, uses state-of-the-art functional brain imaging, eye tracking, and computational analyses. The observed lack of difference in population receptive field sizes between face-selective brain regions of individuals with and without prosopagnosia, though a null result, has <bold>important</bold> implications for the field, and specifically, for theories of face recognition.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108657.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors examine the neural correlates of face recognition deficits in individuals with Developmental Prosopagnosia (DP; 'face blindness'). Contrary to theories that poor face recognition is driven by reduced spatial integration (via smaller receptive fields), here the authors find that the properties of receptive fields in face-selective brain regions are the same in typical individuals vs. those with DP. The main analysis technique is population Receptive Field (pRF) mapping, with a wide range of measures considered. The authors report that there are no differences in goodness-of-fit (R2), the properties of the pRFs (neither size, location, nor the gain and exponent of the Compressive Spatial Summation model), nor their coverage of the visual field. The relationship of these properties to the visual field (notably the increase in pRF size with eccentricity) is also similar between the groups. Eye movements do not differ between the groups.</p>
<p>Strengths:</p>
<p>Although this is a null result, the large number of null results gives confidence that there are unlikely to be differences between the two groups. Together, this makes a compelling case that DP is not driven by differences in the spatial selectivity of face-selective brain regions, an important finding that directly informs theories of face recognition. The paper is well written and enjoyable to read, the studies have clearly been carefully conducted with clear justification for design decisions, and the analyses are thorough.</p>
<p>Weaknesses:</p>
<p>One potential issue relates to the localisation of face-selective regions in the two groups. As in most studies of the neural basis of face recognition, localisers are used to find the face-selective Regions of Interest (ROIs) - OFA, mFus, and pFus, with comparison to the scene-selective PPA. To do so, faces are contrasted against other objects to find these regions (or scenes vs. others for the PPA). The one consistent difference that does emerge between groups in the paper is in the selectivity of these regions, which are less selective for faces in DP than in typical individuals (e.g., Figure 1B), as one might expect. 6/20 prosopagnosic individuals are also missing mFus, relative to only 2/20 typical individuals. This, to me, raises the question of whether the two groups are being compared fairly. If the localised regions were smaller and/or displaced in the DPs, this might select only a subset of the neural populations typically involved in face recognition. Perhaps the difference between groups lies outside this region. In other words, it could be that the differences in prosopagnosic face recognition lie in the neurons that are not able to be localised by this approach. The authors consider in the discussion whether their DPs may not have been 'true DPs', which is convincing (p. 12). The question here is whether the regions selected are truly the 'prosopagnosic brain areas' or whether there is a kind of survivor bias (i.e., the regions selected are normal, but perhaps the difference lies in the nature/extent of the regions. At present, the only consideration given to explain the differences in prosopagnosia is that there may be 'qualitative' differences between the two (which may be true), but I would give more thought to this.</p>
<p>The discussion considers the differences between the current study and an unpublished preprint (Witthoft et al, 2016), where DPs were found to have smaller pRFs than typical individuals. The discussion presents the argument that the current results are likely more robust, given the use of images within the pRF mapping stimuli here (faces, objects, etc) as opposed to checkerboards in the prior work, and the use of the CSS model here as opposed to a linear Gaussian model previously. This is convincing, but fails to address why there is a lack of difference in the control vs. DP group here. If anything, I would have imagined that the use of faces in mapping stimuli would have promoted differences between the groups (given the apparent difference in selectivity in DPs vs. controls seen here), which adds to the reliability of the present result. Greater consideration of why this should have led to a lack of difference would be ideal. The latter point about pRF models (Gaussian vs. CSS) does seem pertinent, for instance - could the 'qualitative' difference lead to changes in the shape of these pRFs in prosopagnosia that are better characterised by the CSS model, perhaps? Perhaps more straightforwardly, and related to the above, could differences in the localisation of face-selective regions have driven the difference in prior work compared to here?</p>
<p>Finally, the lack of variations in the spatial properties of these brain regions is interesting in light of the theories that spatial integration is a key aspect of effective face recognition. In this context, it is interesting to note the marked drop in R2 values in face-selective regions like mFus relative to earlier cortex. The authors note in some sense that this is related to the larger receptive field size, but is there a broader point here that perhaps the receptive field model (even with Compressive Spatial Summation) is simply a poor fit for the function of these areas? Could it be that these areas are simply not spatial at all? A broader link between the null results presented here and their implications for theories of face recognition would be ideal.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108657.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This is a well-conducted and clearly written manuscript addressing the link between population receptive fields (pRFs) and visual behavior. The authors test whether developmental prosopagnosia (DP) involves atypical pRFs in face-selective regions, a hypothesis suggested by prior work with a small DP sample. Using a larger cohort of DPs and controls, robust pRF mapping with appropriate stimuli and CSS modeling, and careful in-scanner eye tracking, the authors report no group differences in pRF properties across the visual processing hierarchy. These results suggest that reduced spatial integration is unlikely to account for holistic face processing deficits in DP.</p>
<p>Strengths:</p>
<p>The dataset quality, sample size, and methodological rigor are notable strengths.</p>
<p>Weaknesses:</p>
<p>The primary concern is the interpretation of the results.</p>
<p>(1) Relationship between pRFs and spatial integration</p>
<p>While atypical pRF properties could contribute to deficits in spatial integration, impairments in holistic processing in DPs are not necessarily caused by pRF abnormalities. The discussion could be strengthened by considering alternative explanations for reduced spatial integration, such as altered structural or functional connectivity in the face network, which has been reported to underlie DP's difficulties in integrating facial features.</p>
<p>(2) Beyond the null hypothesis testing framework</p>
<p>The title claims &quot;normal spatial integration,&quot; yet this conclusion is based on a failure to reject the null hypothesis, which does not justify accepting the alternative hypothesis. To substantiate a claim of &quot;normal,&quot; the authors would need to provide analyses quantifying evidence for the absence of effects, e.g., using a Bayesian framework.</p>
<p>(3) Face-specific or broader visual processing</p>
<p>Prior work from the senior author's lab (Jiahui et al., 2018) reported pronounced reductions in scene selectivity and marginal reductions in body selectivity in DPs, suggesting that visual processing deficits in DPs may extend beyond faces. While the manuscript includes PPA as a high-level control region for scene perception, scene selectivity was not directly reported. The authors could also consider individual differences and potential data-quality confounds (tSNR difference between and within groups, several obvious outliers in the figures, etc). For instance, examining whether reduced tSNR in DPs contributed to lower face selectivity in the DP group in this dataset.</p>
<p>(4) Linking pRF properties to behavior</p>
<p>The manuscript aims to examine the relationship between pRF properties and behavior, but currently reports only one aspect of pRF (size) in relation to a single behavioral measure (CFMT), without full statistical reporting:</p>
<p>&quot;We found no significant association between participants' CFMT scores and mean pRF size in OFA, pFUS, or mFUS.&quot;</p>
<p>For comprehensive reporting, the authors could examine additional pRF properties (e.g., center, eccentricity, scaling between eccentricity and pRF size, shape of visual field coverage, etc), additional ROIs (early, intermediate, and category-selective areas), and relate them to multiple behavioral measures (e.g., HEVA, PI20, FFT). This would provide a full picture of how pRF characteristics relate to behavioral performance in DP.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108657.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Stehr</surname>
<given-names>Daniel A</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1673-3260</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Yiyuan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Patgiri</surname>
<given-names>Anusha</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kidder</surname>
<given-names>Alexis</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kay</surname>
<given-names>Kendrick</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Duchaine</surname>
<given-names>Bradley</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>The authors examine the neural correlates of face recognition deficits in individuals with Developmental Prosopagnosia (DP; 'face blindness'). Contrary to theories that poor face recognition is driven by reduced spatial integration (via smaller receptive fields), here the authors find that the properties of receptive fields in face-selective brain regions are the same in typical individuals vs. those with DP. The main analysis technique is population Receptive Field (pRF) mapping, with a wide range of measures considered. The authors report that there are no differences in goodness-of-fit (R2), the properties of the pRFs (neither size, location, nor the gain and exponent of the Compressive Spatial Summation model), nor their coverage of the visual field. The relationship of these properties to the visual field (notably the increase in pRF size with eccentricity) is also similar between the groups. Eye movements do not differ between the groups.</p>
<p>Strengths:</p>
<p>Although this is a null result, the large number of null results gives confidence that there are unlikely to be differences between the two groups. Together, this makes a compelling case that DP is not driven by differences in the spatial selectivity of face-selective brain regions, an important finding that directly informs theories of face recognition. The paper is well written and enjoyable to read, the studies have clearly been carefully conducted with clear justification for design decisions, and the analyses are thorough.</p>
<p>Weaknesses:</p>
<p>One potential issue relates to the localisation of face-selective regions in the two groups. As in most studies of the neural basis of face recognition, localisers are used to find the face-selective Regions of Interest (ROIs) - OFA, mFus, and pFus, with comparison to the scene-selective PPA. To do so, faces are contrasted against other objects to find these regions (or scenes vs. others for the PPA). The one consistent difference that does emerge between groups in the paper is in the selectivity of these regions, which are less selective for faces in DP than in typical individuals (e.g., Figure 1B), as one might expect. 6/20 prosopagnosic individuals are also missing mFus, relative to only 2/20 typical individuals. This, to me, raises the question of whether the two groups are being compared fairly. If the localised regions were smaller and/or displaced in the DPs, this might select only a subset of the neural populations typically involved in face recognition. Perhaps the difference between groups lies outside this region. In other words, it could be that the differences in prosopagnosic face recognition lie in the neurons that are not able to be localised by this approach. The authors consider in the discussion whether their DPs may not have been 'true DPs', which is convincing (p. 12). The question here is whether the regions selected are truly the 'prosopagnosic brain areas' or whether there is a kind of survivor bias (i.e., the regions selected are normal, but perhaps the difference lies in the nature/extent of the regions. At present, the only consideration given to explain the differences in prosopagnosia is that there may be 'qualitative' differences between the two (which may be true), but I would give more thought to this.</p>
</disp-quote>
<p>We acknowledge that face-selective ROIs in DPs, relative to controls, may be smaller, less selective, or altogether missing when traditional methods of localization with fixed thresholds are used (Furl et al, 2011). For this reason - to circumvent potential survivor bias and ensure ROI voxel counts across participants are equated - we used a method of ROI definition whereby each subject‚Äôs individual statistical map from the localizer was intersected with a generously-sized group mask for each ROI and the top 20% most category-selective voxels were retained for the pRF analysis (Norman-Haignere et al., 2013; Jiahui et al., 2018). This means that the raw number of voxels per ROI was equal across all participants with respect to the common group space, thereby ensuring a fair comparison even in cases where one group shows diminished category-selectivity. The details of the ROI definition are provided in the Methods at the end of the manuscript. To ensure readers understand our approach, we will also make more explicit mention of this in the main body of the manuscript.</p>
<p>With regard to the question of whether face-selective ROIs may be displaced in DPs compared to controls, previous work from the senior author‚Äôs lab (Jiahui et al., 2018) shows that, despite exhibiting weaker activations, the peak coordinates of significant clusters in DPs occupy very similar locations to those of controls. And, even if there were indeed slight displacements of face-selective ROIs for some subjects, the group-defined masks used in the present analysis were large enough to capture the majority of the top voxels. In the supplemental materials section, we will include a diagram of the group masks used in our study.</p>
<p>The reviewer here also points out that more DPs than controls were missing the mFUS region (6/20 DPs vs 2/20 controls; Figure 1C). However, ‚Äòmissing‚Äô in this context was not based on face-selectivity but rather a lack of retinotopic tuning. PRFs were fit to all voxels within each ROI - with all subjects starting out with equal voxel counts - and thereafter, voxels for which the variance explained by the pRF model was below 20% were excluded from subsequent analysis. We decided that any ROI with fewer than 10 voxels remaining after thresholding on the pRF fit should be deemed ‚Äòmissing‚Äô since we considered the amount of data insufficient to reliably characterize the region‚Äôs retinotopic profile. While it may be somewhat interesting that four more DPs than controls were ‚Äòmissing‚Äô left mFUS, using this particular set of decision criteria, it is important to keep in mind that left mFUS was just one of six face-selective regions under study. The other five regions, many of which evinced strong fits by the pRF model, were represented comparably in DPs and controls and showed high similarity in the pRF parameters. Furthermore, across most participants, mFUS exhibited a low proportion of retinotopically modulated voxels (defined as voxels with pRF R squared greater than 20%, see Figure 1D). A follow-up analysis showed that the count of voxels surviving pRF R squared thresholding in left mFUS was not significantly correlated with mean pRF size (r(30)=0.23, t=1.28,  p=0.21) indicating that the greater exclusion of DPs in this region is unlikely to have biased the group‚Äôs average pRF size.</p>
<disp-quote content-type="editor-comment">
<p>The discussion considers the differences between the current study and an unpublished preprint (Witthoft et al, 2016), where DPs were found to have smaller pRFs than typical individuals. The discussion presents the argument that the current results are likely more robust, given the use of images within the pRF mapping stimuli here (faces, objects, etc) as opposed to checkerboards in the prior work, and the use of the CSS model here as opposed to a linear Gaussian model previously. This is convincing, but fails to address why there is a lack of difference in the control vs. DP group here. If anything, I would have imagined that the use of faces in mapping stimuli would have promoted differences between the groups (given the apparent difference in selectivity in DPs vs. controls seen here), which adds to the reliability of the present result. Greater consideration of why this should have led to a lack of difference would be ideal. The latter point about pRF models (Gaussian vs. CSS) does seem pertinent, for instance - could the 'qualitative' difference lead to changes in the shape of these pRFs in prosopagnosia that are better characterised by the CSS model, perhaps? Perhaps more straightforwardly, and related to the above, could differences in the localisation of face-selective regions have driven the difference in prior work compared to here?</p>
</disp-quote>
<p>We agree that the use of high-level mapping stimuli (including faces) adds to the reliability of the present results for DPs and could have further emphasized differences between the groups if true differences did, in fact, exist. We speculate on the extent to which the type of mapping stimuli and various other methodological factors (e.g. stimulus size, aperture design, pRF model) could have explained the divergent findings in our study versus that of Witthoft et al. (2016) in the section of the Discussion titled, ‚ÄúWhat factors may have contributed to the different results for the present study and Witthoft et al. (2016)‚Äù. In brief, our use of more colorful, naturalistic stimuli targeting higher-level visual areas elicited better model fits than the black and white checkerboard pattern used by Witthoft et al. (2016). The CSS model we used is better suited for higher-level regions and makes fewer assumptions than the linear pRF model. The field of view of our stimulus was smaller but still relevant for real-world perception of faces. Finally, our aperture design and longer run length likely also improved reliability. Overall, these methodological improvements, along with our larger sample size, provide stronger evidence for our findings. These are our best attempts to make sense of the divergent findings, but it is not possible to come to a definitive explanation. Examples abound of exaggerated or spurious effects from small-scale studies that ultimately fail to replicate in the related field of dyslexia research (Jednorog et al., 2015; Ramus et al., 2018) and neuroimaging research more generally (Turner et al., 2018; Poldrack et al., 2017). Sometimes there are clear explanations for a lack of replicability (e.g. software bugs, overly flexible preprocessing methods, etc.), but many times the real reason cannot be determined.</p>
<p>Regarding the type of pRF model deployed, our use of a non-linear exponent (versus a linear model as in the Witthoft et al. (2016) preprint) is unlikely to explain the similarity we observed between the groups in terms of pRF size. Specifically, the groups did not show substantial differences in the exponent by ROI, as seen in Figure 1E, so the use of a linear model should, in theory, produce similar outcomes for the two groups. We will mention this point in the main text.</p>
<disp-quote content-type="editor-comment">
<p>Finally, the lack of variations in the spatial properties of these brain regions is interesting in light of the theories that spatial integration is a key aspect of effective face recognition. In this context, it is interesting to note the marked drop in R2 values in face-selective regions like mFus relative to earlier cortex. The authors note in some sense that this is related to the larger receptive field size, but is there a broader point here that perhaps the receptive field model (even with Compressive Spatial Summation) is simply a poor fit for the function of these areas? Could it be that these areas are simply not spatial at all? A broader link between the null results presented here and their implications for theories of face recognition would be ideal.</p>
</disp-quote>
<p>The weaker pRF fits found in mFUS, to us, raise the question of whether there is a more effective pRF stimulus for these more anterior regions. For example, it might be possible to obtain higher and more reliable responses there using single isolated faces (Cf. Kay, Weiner, Grill-Spector, 2015). More broadly, though, we agree that it is important to acknowledge that the receptive field model might ultimately be a coarse and incomplete characterization of neural function in these areas. As the other reviewer suggests, one possibility is that other brain processes (e.g. functional or structural connectivity between ROIs) may give rise to holistic face processing in ways that are not captured by pRF properties.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>This is a well-conducted and clearly written manuscript addressing the link between population receptive fields (pRFs) and visual behavior. The authors test whether developmental prosopagnosia (DP) involves atypical pRFs in face-selective regions, a hypothesis suggested by prior work with a small DP sample. Using a larger cohort of DPs and controls, robust pRF mapping with appropriate stimuli and CSS modeling, and careful in-scanner eye tracking, the authors report no group differences in pRF properties across the visual processing hierarchy. These results suggest that reduced spatial integration is unlikely to account for holistic face processing deficits in DP.</p>
<p>Strengths:</p>
<p>The dataset quality, sample size, and methodological rigor are notable strengths.</p>
<p>Weaknesses:</p>
<p>The primary concern is the interpretation of the results.</p>
<p>(1) Relationship between pRFs and spatial integration</p>
<p>While atypical pRF properties could contribute to deficits in spatial integration, impairments in holistic processing in DPs are not necessarily caused by pRF abnormalities. The discussion could be strengthened by considering alternative explanations for reduced spatial integration, such as altered structural or functional connectivity in the face network, which has been reported to underlie DP's difficulties in integrating facial features.</p>
</disp-quote>
<p>We agree the Discussion section could benefit from mentioning that alterations to other neural mechanisms, besides pRF organization, could produce deficits in holistic processing. This could take the form of altered functional connectivity (Rosenthal et al., 2017; Lohse et al., 2016; Avidan et al., 2014) or altered structural connectivity (Gomez et al., 2015; Song et al., 2015)</p>
<disp-quote content-type="editor-comment">
<p>(2) Beyond the null hypothesis testing framework</p>
<p>The title claims &quot;normal spatial integration,&quot; yet this conclusion is based on a failure to reject the null hypothesis, which does not justify accepting the alternative hypothesis. To substantiate a claim of &quot;normal,&quot; the authors would need to provide analyses quantifying evidence for the absence of effects, e.g., using a Bayesian framework.</p>
</disp-quote>
<p>We acknowledge that, using frequentist statistical methods, failing to reject the null hypothesis is not sufficient to claim equivalence. For the revision, we will look into additional analyses that could quantify evidence for the null hypothesis. And we will adjust the wording of the title in this regard.</p>
<disp-quote content-type="editor-comment">
<p>(3) Face-specific or broader visual processing</p>
<p>Prior work from the senior author's lab (Jiahui et al., 2018) reported pronounced reductions in scene selectivity and marginal reductions in body selectivity in DPs, suggesting that visual processing deficits in DPs may extend beyond faces. While the manuscript includes PPA as a high-level control region for scene perception, scene selectivity was not directly reported. The authors could also consider individual differences and potential data-quality confounds (tSNR difference between and within groups, several obvious outliers in the figures, etc). For instance, examining whether reduced tSNR in DPs contributed to lower face selectivity in the DP group in this dataset.</p>
</disp-quote>
<p>Thank you for this suggestion - we will compare tSNR between the groups as a measure of data quality and we will include these comparisons. A preliminary look indicates that both groups possessed similar distributions of tSNR across many of the face-selective regions investigated here.</p>
<disp-quote content-type="editor-comment">
<p>(4) Linking pRF properties to behavior</p>
<p>The manuscript aims to examine the relationship between pRF properties and behavior, but currently reports only one aspect of pRF (size) in relation to a single behavioral measure (CFMT), without full statistical reporting:</p>
<p>&quot;We found no significant association between participants' CFMT scores and mean pRF size in OFA, pFUS, or mFUS.&quot;</p>
<p>For comprehensive reporting, the authors could examine additional pRF properties (e.g., center, eccentricity, scaling between eccentricity and pRF size, shape of visual field coverage, etc), additional ROIs (early, intermediate, and category-selective areas), and relate them to multiple behavioral measures (e.g., HEVA, PI20, FFT). This would provide a full picture of how pRF characteristics relate to behavioral performance in DP.</p>
</disp-quote>
<p>We will report the full statistical values (r, p) for the (albeit non-significant) relationship between CFMT score and pRF size - thank you for bringing that to our attention. Additionally, we will add other analyses assessing the relationship between a wider array of pRF measures and the other behavioral tests administered to provide a more comprehensive picture of the relation between pRFs and behavior.</p>
<p>References:</p>
<p>Avidan, G., Tanzer, M., Hadj-Bouziane, F., Liu, N., Ungerleider, L. G., &amp; Behrmann, M. (2014). Selective Dissociation Between Core and Extended Regions of the Face Processing Network in Congenital Prosopagnosia. Cerebral Cortex, 24(6), 1565‚Äì1578. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bht007">https://doi.org/10.1093/cercor/bht007</ext-link></p>
<p>Furl, N., Garrido, L., Dolan, R. J., Driver, J., &amp; Duchaine, B. (2011). Fusiform gyrus face selectivity relates to individual differences in facial recognition ability. Journal of Cognitive Neuroscience, 23(7), 1723‚Äì1740. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2010.21545">https://doi.org/10.1162/jocn.2010.21545</ext-link></p>
<p>Gomez, J., Pestilli, F., Witthoft, N., Golarai, G., Liberman, A., Poltoratski, S., Yoon, J., &amp; Grill-Spector, K. (2015). Functionally Defined White Matter Reveals Segregated Pathways in Human Ventral Temporal Cortex Associated with Category-Specific Processing. Neuron, 85(1), 216‚Äì227. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.12.027">https://doi.org/10.1016/j.neuron.2014.12.027</ext-link></p>
<p>Jednor√≥g, K., Marchewka, A., Altarelli, I., Monzalvo Lopez, A. K., van Ermingen-Marbach, M., Grande, M., Grabowska, A., Heim, S., &amp; Ramus, F. (2015). How reliable are gray matter disruptions in specific reading disability across multiple countries and languages? Insights from a large-scale voxel-based morphometry study. Human Brain Mapping, 36(5), 1741‚Äì1754. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hbm.22734">https://doi.org/10.1002/hbm.22734</ext-link></p>
<p>Jiahui, G., Yang, H., &amp; Duchaine, B. (2018). Developmental prosopagnosics have widespread selectivity reductions across category-selective visual cortex. Proceedings of the National Academy of Sciences of the United States of America, 115(28), E6418‚ÄìE6427. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1802246115">https://doi.org/10.1073/pnas.1802246115</ext-link></p>
<p>Kay, K. N., Weiner, K. S., Kay, K. N., &amp; Weiner, K. S. (2015). Attention Reduces Spatial Uncertainty in Human Ventral Temporal Cortex Attention Reduces Spatial Uncertainty in Human Ventral Temporal Cortex. Current Biology, 25(5), 595‚Äì600. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.12.050">https://doi.org/10.1016/j.cub.2014.12.050</ext-link></p>
<p>Lohse, M., Garrido, L., Driver, J., Dolan, R. J., Duchaine, B. C., &amp; Furl, N. (2016). Effective connectivity from early visual cortex to posterior occipitotemporal face areas supports face selectivity and predicts developmental prosopagnosia. Journal of Neuroscience, 36(13), 3821‚Äì3828. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3621-15.2016">https://doi.org/10.1523/JNEUROSCI.3621-15.2016</ext-link></p>
<p>Norman-Haignere, S., Kanwisher, N., &amp; McDermott, J. H. (2013). Cortical pitch regions in humans respond primarily to resolved harmonics and are located in specific tonotopic regions of anterior auditory cortex. Journal of Neuroscience, 33(50), 19451‚Äì19469. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2880-13.2013">https://doi.org/10.1523/JNEUROSCI.2880-13.2013</ext-link></p>
<p>Poldrack, R. A., Baker, C. I., Durnez, J., Gorgolewski, K. J., Matthews, P. M., Munaf√≤, M. R., Nichols, T. E., Poline, J. B., Vul, E., &amp; Yarkoni, T. (2017). Scanning the horizon: Towards transparent and reproducible neuroimaging research. Nature Reviews Neuroscience, 18(2), 115‚Äì126. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn.2016.167">https://doi.org/10.1038/nrn.2016.167</ext-link></p>
<p>Ramus, F., Altarelli, I., Jednor√≥g, K., Zhao, J., &amp; Scotto di Covella, L. (2018). Neuroanatomy of developmental dyslexia: Pitfalls and promise. Neuroscience and Biobehavioral Reviews, 84(July 2017), 434‚Äì452. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2017.08.001">https://doi.org/10.1016/j.neubiorev.2017.08.001</ext-link></p>
<p>Rosenthal, G., Tanzer, M., Simony, E., Hasson, U., Behrmann, M., &amp; Avidan, G. (2017). Altered topology of neural circuits in congenital prosopagnosia. ELife, 6, 1‚Äì20. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.25069">https://doi.org/10.7554/eLife.25069</ext-link></p>
<p>Song, S., Garrido, L., Nagy, Z., Mohammadi, S., Steel, A., Driver, J., Dolan, R. J., Duchaine, B., &amp; Furl, N. (2015). Local but not long-range microstructural differences of the ventral temporal cortex in developmental prosopagnosia. Neuropsychologia, 78, 195‚Äì206. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2015.10.010">https://doi.org/10.1016/j.neuropsychologia.2015.10.010</ext-link></p>
<p>Turner, B. O., Paul, E. J., Miller, M. B., &amp; Barbey, A. K. (2018). Small sample sizes reduce the replicability of task-based fMRI studies. Communications Biology, 1(1). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s42003-018-0073-z">https://doi.org/10.1038/s42003-018-0073-z</ext-link></p>
<p>Witthoft, N., Poltoratski, S., Nguyen, M., Golarai, G., Liberman, A., LaRocque, K., Smith, M., &amp; Grill-Spector, K. (2016). Reduced spatial integration in the ventral visual cortex underlies face recognition deficits in developmental prosopagnosia. BioRxiv, 1‚Äì26.</p>
</body>
</sub-article>
</article>