<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99957</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99957</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99957.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The value of initiating a pursuit in temporal decision-making</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sutlief</surname>
<given-names>Elissa</given-names>
</name>
<xref ref-type="author-notes" rid="n1">*</xref>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Walters</surname>
<given-names>Charlie</given-names>
</name>
<xref ref-type="author-notes" rid="n1">*</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Marton</surname>
<given-names>Tanya</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>tanya.marton@gmail.com</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1927-0970</contrib-id>
<name>
<surname>Shuler</surname>
<given-names>Marshall G Hussain</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<email>shuler@jhmi.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Neuroscience, Johns Hopkins University School of Medicine</institution>, 725 N. Wolfe Street, Baltimore, MD 21205, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Kavli Neuroscience Discovery Institute, Department of Neuroscience, Johns Hopkins University School of Medicine</institution>, 725 N. Wolfe Street, Baltimore, MD 21205, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ahmed</surname>
<given-names>Alaa A</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Colorado Boulder</institution>
</institution-wrap>
<city>Boulder</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Frank</surname>
<given-names>Michael J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Brown University</institution>
</institution-wrap>
<city>Providence</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="others"><label>*</label><p>Co-first Author</p></fn>
<fn fn-type="others"><p><email>esutlie1@jhmi.edu</email></p></fn>
<fn fn-type="others"><p><email>ckwalters@jhmi.edu</email></p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-10-02">
<day>02</day>
<month>10</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99957</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-06-16">
<day>16</day>
<month>06</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-06-16">
<day>16</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.06.16.599189"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Sutlief et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Sutlief et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99957-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Reward rate maximization is a prominent normative principle commonly held in behavioral ecology, neuroscience, economics, and artificial intelligence. Here, we identify and compare equations for evaluating the worth of initiating pursuits that an agent could implement to enable reward-rate maximization. We identify two fundamental temporal decision-making categories requiring the valuation of the initiation of a pursuit—forgo and choice decision-making—over which we generalize and analyze the optimal solution for how to evaluate a pursuit in order to maximize reward rate. From this reward rate maximizing formulation, we derive expressions for the subjective value of a pursuit, i.e. that pursuit’s equivalent immediate reward magnitude, and reveal that time’s cost is composed of an <italic>apportionment</italic>, in addition to, an <italic>opportunity</italic> cost. By re-expressing subjective value as a temporal discounting function, we show precisely how the temporal discounting function of a reward rate optimal agent is sensitive not just to the properties of a considered pursuit, but to the time spent and reward acquired outside of the pursuit for every instance spent within it. In doing so, we demonstrate how the apparent discounting function of a reward-rate optimizing agent depends on the temporal structure of the environment and is a combination of hyperbolic and linear components, whose contributions relate the apportionment and opportunity cost of time, respectively. We further then show how purported signs of suboptimal behavior (hyperbolic discounting, the “Magnitude” effect, the “Sign” effect) are in fact consistent with reward rate maximization. In clarifying what features are, and are not signs of optimal decision-making, we then analyze the impact of misestimation of identified reward rate maximizing parameters to best account for the pattern of errors actually observed in humans and animals. We find that errors in agents’ assessment of the apportionment of time inside versus outside a considered pursuit type is the likely driver of suboptimal temporal decision-making observed behaviorally, which we term the ‘Malapportionment Hypothesis’. By providing a generalized form for reward rate maximization, and by relating it to subjective value and temporal discounting, the true pattern of errors exhibited by humans and animals can now be more deeply understood, identified, and quantified, being key to deducing the learning algorithms and representational architectures actually used by humans and animals to evaluate the worth of pursuits.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Temporal decision-making</kwd>
<kwd>Reward rate maximization</kwd>
<kwd>Subjective Value</kwd>
<kwd>time’s cost</kwd>
<kwd>opportunity cost</kwd>
<kwd>apportionment cost</kwd>
<kwd>equivalent immediate reward</kwd>
<kwd>Discounting Function</kwd>
<kwd>Misestimation</kwd>
<kwd>Normative theory</kwd>
<kwd>Malapportionment Hypothesis</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>What is the worth of a pursuit? At the most universal level, temporal decision-making regards weighing the return of pursuits against their cost in time. The fields of economics, psychology, behavioral ecology, neuroscience, and artificial intelligence have endeavored to understand how animals, humans, and learning agents evaluate the worth of pursuits: how they factor the cost of time in temporal decision-making. A central step in doing so is to identify a normative principle and then to solve for how an agent, abiding by that principle, would best invest time in pursuits that compose a world. A normative principle with broad appeal identified in behavioral ecology is that of reward-rate maximization, as expressed in Optimal Foraging Theory (OFT), where animals seek to maximize reward rate while foraging in an environment (<xref ref-type="bibr" rid="c22">Charnov, 1976a</xref>, <xref ref-type="bibr" rid="c23">1976b</xref>; <xref ref-type="bibr" rid="c55">Krebs et al., 1977</xref>; <xref ref-type="bibr" rid="c85">Pyke et al., 1977</xref>; <xref ref-type="bibr" rid="c84">Pyke, 1984</xref>). Solving for the optimal decision-making behavior under this objective provides the means to examine the curious pattern of adherence and deviation that is exhibited by humans and animals with respect to that ideal behavior. This difference provides clues into the process that animals and humans use to learn the value of, and represent, pursuits. Therefore, it is essential to analyze reward rate maximizing solutions for the worth of initiating a pursuit to clarify what behavioral signs are—and are not—deviations from optimal performance in the identification of the process (and its sources of error) actually used by humans and animals.</p>
<sec id="s1a">
<title>Equivalent immediate reward (subjective value, <italic>sv</italic>)</title>
<p>To ask, ‘what is the value of a pursuit?’ is to quantify by some metric the worth of a future state—the pursuit’s outcome—at the time of a prior one, the pursuit’s initiation. A sensible metric for the worth of a pursuit is the magnitude of immediate reward that would be treated by an agent as equivalent to a policy of investing the requisite time in the pursuit and obtaining its reward. This <italic>equivalent immediate reward</italic>, as judged by the agent, is the pursuit’s “Subjective Value” (sv), in the parlance of the field (<xref ref-type="bibr" rid="c70">Mischel et al., 1969</xref>). It is widely assumed that decisions about what pursuits should be taken are made on the basis of their subjective value (<xref ref-type="bibr" rid="c80">Niv, 2009</xref>). However, a decision-making algorithm needn’t calculate subjective value in its evaluation of the worth of initiating a pursuit. It could, for instance, assess the reward rate of the pursuit over that of the reward rate received in the world as a whole. Indeed, algorithms leading to reward rate optimization can arise from different underlying processes, each with their own controlling variables. Nonetheless, any algorithm’s evaluation can be re-expressed in terms of equivalent immediate reward, providing a ready means to compare evaluation across different learning algorithms and representational architectures as biologically realized in animals and humans or as artificially implemented in silico.</p>
</sec>
<sec id="s1b">
<title>Decisions to initiate pursuits</title>
<p>As decisions occur at branch points between pursuits, the value of initiating a pursuit is of particular importance, as it is on this basis that an agent would decide 1) whether to accept or <italic>forgo</italic> an offered pursuit; or, 2) how to <italic>choose</italic> between mutually exclusive pursuits. Though ‘Forgo’ decisions are regarded as near-optimal, as in prey selection (<xref ref-type="bibr" rid="c55">Krebs et al., 1977</xref>; <xref ref-type="bibr" rid="c101">Stephens and Krebs, 1986</xref>; <xref ref-type="bibr" rid="c12">Blanchard and Hayden, 2014</xref>), ‘Choice’ decisions—as commonly tested in laboratory settings—reveal a suboptimal bias for smaller-sooner rewards when selection of later-larger rewards would maximize global reward rate (<xref ref-type="bibr" rid="c60">Logue et al., 1985</xref>; <xref ref-type="bibr" rid="c13">Blanchard and Hayden, 2015</xref>; <xref ref-type="bibr" rid="c20">Carter and Redish, 2016</xref>; <xref ref-type="bibr" rid="c49">Kane et al., 2019</xref>). This curious pattern of behavior, wherein forgo decisions can present as optimal while choice decisions as suboptimal, poses a challenge to any theory purporting to rationalize temporal decision-making as observed in animals and humans.</p>
</sec>
<sec id="s1c">
<title>Temporal Discounting Functions</title>
<p>Historically, temporal decision-making has been examined using a temporal discounting function to describe how delays in rewards influence their valuation. The “temporal discounting function” describes the magnitude-normalized subjective value of an offered reward as a function of when the offered reward is realized. An understanding of the form of temporal discounting has important implications in life, as steeper temporal discounting has been associated with many negative life outcomes (<xref ref-type="bibr" rid="c15">Bretteville-Jensen, 1999</xref>; <xref ref-type="bibr" rid="c26">Critchfield and Kollins, 2001</xref>; <xref ref-type="bibr" rid="c11">Bickel et al., 2007</xref>, 2012; <xref ref-type="bibr" rid="c103">Story et al., 2014</xref>), most notably the risk of developing an addiction. Psychologists and behavioral scientists have long found that animals’ temporal discounting in intertemporal choice tasks is well-fit by a hyperbolic discounting function (<xref ref-type="bibr" rid="c2">Ainslie, 1974</xref>; <xref ref-type="bibr" rid="c63">Mazur, 1987</xref>; <xref ref-type="bibr" rid="c89">Richards et al., 1997</xref>; <xref ref-type="bibr" rid="c73">Monterosso and Ainslie, 1999</xref>; <xref ref-type="bibr" rid="c34">Green and Myerson, 2004</xref>; <xref ref-type="bibr" rid="c44">Hwang et al., 2009</xref>; <xref ref-type="bibr" rid="c61">Louie and Glimcher, 2010</xref>). Other examples of motivated behavior also show hyperbolic temporal discounting (<xref ref-type="bibr" rid="c37">Haith et al., 2012</xref>).</p>
<p>Often, this perspective assumes that the delay in and of itself devalues a pursuit’s reward, failing to carefully distinguish the impact of its delay from the impact of the time required and reward obtained <italic>outside</italic> the considered pursuit. As a result, the discounting function tends to be treated as a process unto itself rather than the consequence of a process. Consequently, the field has concerned itself with the form of the discounting function—exponential (<xref ref-type="bibr" rid="c31">Glimcher et al., 2007</xref>; <xref ref-type="bibr" rid="c67">McClure et al., 2007</xref>), hyperbolic (<xref ref-type="bibr" rid="c87">Rachlin et al., 1972</xref>; <xref ref-type="bibr" rid="c1">Ainslie, 1975</xref>; <xref ref-type="bibr" rid="c106">Thaler, 1981</xref>; <xref ref-type="bibr" rid="c63">Mazur, 1987</xref>; <xref ref-type="bibr" rid="c7">Benzion et al., 1989</xref>; <xref ref-type="bibr" rid="c33">Green et al., 1994</xref>; <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>; <xref ref-type="bibr" rid="c53">Kobayashi and Schultz, 2008</xref>; <xref ref-type="bibr" rid="c17">Calvert et al., 2010</xref>), pseudo-hyperbolic (<xref ref-type="bibr" rid="c56">Laibson, 1997</xref>; <xref ref-type="bibr" rid="c72">Montague et al., 2006</xref>; <xref ref-type="bibr" rid="c9">Berns et al., 2007</xref>), etc., as either derived from some normative principle, or as fit to behavioral observation. An exponential discounting function, for instance, was derived by Samuelson from the normative principle of time consistency (<xref ref-type="bibr" rid="c91">Samuelson 1937</xref>) and is widely held as rational (<xref ref-type="bibr" rid="c91">Samuelson, 1937</xref>; <xref ref-type="bibr" rid="c54">Koopmans, 1960</xref>; <xref ref-type="bibr" rid="c56">Laibson, 1997</xref>; <xref ref-type="bibr" rid="c71">Montague and Berns, 2002</xref>; <xref ref-type="bibr" rid="c68">McClure et al., 2004</xref>, 2007; <xref ref-type="bibr" rid="c65">Mazur, 2006</xref>; <xref ref-type="bibr" rid="c93">Schweighofer et al., 2006</xref>; <xref ref-type="bibr" rid="c9">Berns et al., 2007</xref>; <xref ref-type="bibr" rid="c75">Nakahara and Kaveri, 2010</xref>; <xref ref-type="bibr" rid="c49">Kane et al., 2019</xref>), and by implication, reward rate maximizing. Observed temporal decision-making behavior, however, routinely exhibits time inconsistencies (<xref ref-type="bibr" rid="c104">Strotz, 1956</xref>; <xref ref-type="bibr" rid="c1">Ainslie, 1975</xref>; <xref ref-type="bibr" rid="c56">Laibson, 1997</xref>; <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>) and is better fit by a hyperbolic discounting function (<xref ref-type="bibr" rid="c1">Ainslie, 1975</xref>; <xref ref-type="bibr" rid="c66">Mazur et al., 1985</xref>; <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>; <xref ref-type="bibr" rid="c34">Green and Myerson, 2004</xref>), and on that contrasting basis, humans and animals have commonly been regarded as <italic>irrational</italic> (<xref ref-type="bibr" rid="c105">Takahashi and Han, 2012</xref>; <xref ref-type="bibr" rid="c49">Kane et al., 2019</xref>). In addition, the case that humans and animals are irrational is, ostensibly, furthered by the observation of the ‘Magnitude Effect’ (<xref ref-type="bibr" rid="c35">Green et al., 1997</xref>; <xref ref-type="bibr" rid="c4">Baker et al., 2003</xref>; <xref ref-type="bibr" rid="c27">Estle et al., 2006</xref>; <xref ref-type="bibr" rid="c110">Yi et al., 2006</xref>; <xref ref-type="bibr" rid="c32">Grace et al., 2012</xref>; <xref ref-type="bibr" rid="c52">Kinloch and White, 2013</xref>) and the ‘Sign Effect’ (<xref ref-type="bibr" rid="c106">Thaler, 1981</xref>; <xref ref-type="bibr" rid="c58">Loewenstein and Thaler, 1989</xref>; Loewenstein and Prelec, 1992; <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>; <xref ref-type="bibr" rid="c4">Baker et al., 2003</xref>; <xref ref-type="bibr" rid="c27">Estle et al., 2006</xref>; <xref ref-type="bibr" rid="c47">Kalenscher and Pennartz, 2008</xref>), where the apparent discounting function is affected by the magnitude and the sign of the offered pursuit’s outcome, respectively.</p>
<p>Here, we aim to identify equations for evaluating the worth of initiating pursuits that an agent could implement to enable reward-rate maximization. We wish to gain deeper insight into how a considered pursuit, with its defining features (its reward and time), relates to the world of pursuits in which it is embedded, in determining the pursuit’s worth. Specifically, we investigate how pursuits and the pursuit-to-pursuit structure of a world interact with policies of investing time in particular pursuits to determine the global reward rate reaped from an environment. We aim to provide greater clarity into what constitutes time’s cost and how it can be understood with respect to the reward and temporal structure of an environment and to counterfactual time investment policies. We propose that, by determining optimal decision-making equations and converting them to their equivalent subjective value and temporal discounting functions, actual (rather than assumed) deviations from optimality exhibited by humans and animals can be truly determined. We speculate that purported anomalies deviating from ostensibly ‘rational’ decision-making may in fact be consistent with reward rate optimization. Further, by identifying parameters enabling reward rate maximization and assessing resulting errors in valuation caused by their misestimation, we aim to gain insight into which parameters humans and animals may (mis)-represent that most parsimoniously explains the pattern of temporal decision-making actually observed.</p>
</sec>
</sec>
<sec id="s2">
<title>Results</title>
<p>To gain insight into the manner by which animals and humans attribute value to pursuits, it is essential to first understand how a reward rate maximizing agent would evaluate the worth of any pursuit within a temporal decision-making world. Here, by considering <bold>Forgo</bold> and <bold>Choice</bold> temporal decisions, we re-conceptualize how an ideal reward rate maximizing agent ought to evaluate the worth of initiating pursuits. We begin by formalizing temporal decision-making worlds as constituted of pursuits, with pursuits described as having reward rates and weights (their relative occupancy). Then, we examine <bold>Forgo</bold> decisions to examine what composes the cost of time and how a policy of taking/forgoing pursuits factors into the global reward rate of an environment and thus the worth of a pursuit. Having done so, we derive two equivalent expressions for the worth of a pursuit and from them re-express the worth of a pursuit as its equivalent immediate reward (its ‘subjective value’) in terms of the global reward rate achieved under policies of 1) accepting or 2) forgoing the considered pursuit type. We next examine <bold>Choice</bold> worlds to investigate the apparent nature of a reward rate optimizing agent’s temporal discounting function. Finally, having identified reward rate maximizing equations, we examine what parameter misestimation leads to suboptimal pursuit evaluation that best explains behavior observed in humans and animals. Together, by considering the temporal structure of a time investment world as one composed of pursuits described by their rates and weights (relative occupancy), we seek to identify equations for how a reward rate maximizing agent could evaluate the worth of any pursuit comprising a world and how those evaluations would be affected by misestimation of enabling parameters.</p>
<sec id="s2a">
<title>Temporal decision worlds are composed of pursuits with reward rates and weights</title>
<p>A temporal decision-making world is one composed of pursuits. A <italic>pursuit</italic> is a defined path over which an agent can traverse by investing time that often (but not necessarily) results in reward but which always leads to a state from which one or more potential other pursuits are discoverable. Pursuits have a <italic>reward magnitude</italic> (<italic>r</italic>) and a <italic>time</italic> (<italic>t</italic>). A pursuit therefore has 1) a <italic>reward rate</italic> (ρ, rho) and 2) a <italic>weight</italic> (<italic>w</italic>), being its relative occupancy with respect to all other pursuits. To refer to the reward, the time, the reward rate, or the weight of a given pursuit, <italic>r, t</italic>, ρ, or <italic>w</italic>, respectively, is prepended to the subscript (or name) of the pursuit (ρ<sub>Pursuit,</sub> <italic>w</italic><sub>Pursuit</sub>). In this way, the pursuit structure of temporal decision-making worlds, and the qualities defining pursuits, can be adequately referenced.</p>
<p>The temporal decision-making worlds considered are recurrent in that an agent traversing a world under a given policy will eventually return back to its current location. As pursuits constitute an environment, the environment itself then has a reward rate, the ‘global reward rate’ <italic>ρ</italic><sub><italic>g</italic></sub>, achieved under a given decision policy, <italic>ρ</italic><sub><italic>g</italic></sub><italic>Policy</italic>. Whereas the global reward rate realized under a given policy of choosing one or another pursuit path may or may not be reward-rate optimal, the global reward rate achieved under a reward-rate maximal policy will be denoted as ρg*.</p>
</sec>
<sec id="s2b">
<title>Forgo and Choice decision topologies</title>
<p>Having established a nomenclature for the properties of a temporal decision-making world, we now identify two fundamental types of decisions regarding whether to initiate a pursuit: “Forgo” decisions, and “Choice” decisions. In a Forgo decision (<xref rid="fig1" ref-type="fig">Figure 1</xref>, left), the agent is presented with one of possibly many pursuits that can either be accepted or rejected. After either the conclusion of the pursuit, if accepted, or immediately after rejection, the agent returns to a pursuit by default (the “default” pursuit), which effectively can be a waiting period, until the next pursuit opportunity becomes available. Rejecting the offered pursuit constitutes a policy of spending less time to make a traversal across that decision-making world, whereas accepting the offered pursuit constitutes a policy of spending more time to make a traversal. In a Choice decision (<xref rid="fig1" ref-type="fig">Figure 1</xref>, right), the agent is presented with a choice between at least two simultaneous and mutually exclusive pursuits, typically differing in their respective rewards’ magnitudes and delays. Under any decision, upon exit from a pursuit, the agent returns to the same environment that it would have entered were the pursuit rejected. In the Forgo case in <xref rid="fig1" ref-type="fig">Figure 1</xref>, a policy of spending less time to traverse the world by rejecting the purple pursuit to return to the gold pursuit—and thus obtaining a smaller amount of reward (left)—must be weighed against a policy of acquiring more reward by accepting the purple pursuit at the expense of spending more time to traverse the world (right). In the Choice case in <xref rid="fig1" ref-type="fig">Figure 1</xref>, a policy of spending less time to traverse the world (left) by taking the smaller-sooner pursuit (aqua) must be weighed against a policy of spending more time to traverse the world (right) by accepting the larger-later pursuit (purple).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Fundamental classes of temporal decision-making regarding initiating a pursuit: “Forgo” and “Choice”. Fundamental classes of temporal decision-making regarding initiating a pursuit: “Forgo” and “Choice”. <underline>1st row- Topologies</underline>. The temporal structure of worlds exemplifying Forgo (left) and Choice (right) decisions mapped as their topologies. <underline>Forgo</underline>: A forgo decision to accept or reject the purple pursuit. When exiting the gold pursuit having obtained its reward (small blue circle), an agent is faced with 1) a path to re-enter gold, or 2) a path to enter the purple pursuit, which, on its completion, re-enters gold. <underline>Choice</underline>: A choice decision between an aqua pursuit, offering a small reward after a short amount of time, or a purple pursuit offering a larger amount of reward after a longer time. When exiting the gold pursuit, an agent is faced with a path to enter 1) the aqua or 2) the purple pursuit, both of which lead back to the gold pursuit upon their completion. <underline>2nd row-Policies</underline>. Decision-making policies chart a course through the pursuit-to-pursuit structure of a world. Policies differ in the reward obtained, and in the time required, to complete a traversal of that world under that policy. Policies of investing less (left) or more (right) time to traverse the world are illustrated for the considered Forgo and Choice worlds. <underline>Forgo</underline>: A policy of rejecting the purple pursuit to re-enter the gold pursuit (left) acquires less reward though it requires less time to make a traversal of the world than a policy of accepting the purple option (right). <underline>Choice</underline>: A policy of choosing the aqua pursuit (left) results in less reward though requires less time to traverse the world than a policy of choosing the purple pursuit (right). <underline>3rd row-Time/reward investment</underline>. The times (solid horizontal lines, colored by pursuit) and rewards (vertical blue lines) of pursuits, and their associated reward rates (dashed lines) acquired under a policy of forgo or accept in the Forgo world, or, of choosing the sooner smaller or later larger pursuit in the Choice world.</p></caption>
<graphic xlink:href="599189v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2c">
<title>Behavioral observations under Forgo and Choice decisions</title>
<p>These classes of temporal decisions have been investigated by ecologists, behavioral scientists, and psychologists for decades. Forgo decisions describe instances likened to prey selection (<xref ref-type="bibr" rid="c55">Krebs et al., 1977</xref>; <xref ref-type="bibr" rid="c101">Stephens and Krebs, 1986</xref>; <xref ref-type="bibr" rid="c12">Blanchard and Hayden, 2014</xref>). Choice decisions have extensively been examined in intertemporal choice experiments (<xref ref-type="bibr" rid="c87">Rachlin et al., 1972</xref>; <xref ref-type="bibr" rid="c2">Ainslie, 1974</xref>; <xref ref-type="bibr" rid="c5">Bateson and Kacelnik, 1996</xref>; <xref ref-type="bibr" rid="c98">Stephens and Anderson, 2001</xref>; <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>; <xref ref-type="bibr" rid="c42">Hayden and Platt, 2007</xref>; <xref ref-type="bibr" rid="c67">McClure et al., 2007</xref>; <xref ref-type="bibr" rid="c19">Carter et al., 2015</xref>; <xref ref-type="bibr" rid="c20">Carter and Redish, 2016</xref>). Experimental observation in temporal decision-making demonstrates that animals are optimal (or virtually so) in Forgo (<xref ref-type="bibr" rid="c55">Krebs et al., 1977</xref>; <xref ref-type="bibr" rid="c101">Stephens and Krebs, 1986</xref>; <xref ref-type="bibr" rid="c12">Blanchard and Hayden, 2014</xref>), taking the offered pursuit when its rate exceeds the “background” reward rate, and are as if sub-optimally impatient in choice, selecting the smaller-sooner (SS) pursuit when the larger-later (LL) pursuit is just as good if not better (<xref ref-type="bibr" rid="c60">Logue et al., 1985</xref>; <xref ref-type="bibr" rid="c13">Blanchard and Hayden, 2015</xref>; <xref ref-type="bibr" rid="c20">Carter and Redish, 2016</xref>; <xref ref-type="bibr" rid="c49">Kane et al., 2019</xref>).</p>
</sec>
<sec id="s2d">
<title>Deriving optimal policy from forgo decision-making worlds</title>
<p>We begin our examination of how to maximize the global reward rate reaped from a landscape of rewarding pursuits by examining forgo decisions. A general formula for the global reward rate of an environment in which agents must invest time in obtaining rewards is needed in order to formally calculate a policy’s ability to accumulate reward. Optimal policies maximize reward accumulation over the time spent foraging in that environment. In a forgo decision, an agent is faced with the decision to take, or to <italic>forgo</italic>, pursuit opportunities. We sought to determine the reward rate an agent would achieve were it to pursue rewards with magnitudes <italic>r</italic><sub>1,</sub> <italic>r</italic><sub>2,</sub> …, <italic>r</italic><sub><italic>n</italic></sub> each requiring an investment of time <italic>t</italic><sub>1,</sub> <italic>t</italic><sub>2,</sub> …, <italic>t</italic><sub><italic>n</italic></sub>. At any particular time, the agent is either 1) investing time in a pursuit of a specific reward and time, or 2) available to encounter and take new pursuits from a pursuit to which it defaults. With the assumption that reward opportunities become randomly encountered by the agent at a frequency of <italic>f</italic><sub>1,</sub> <italic>f</italic><sub>2,</sub> …, <italic>f</italic><sub><italic>n</italic></sub> from the default pursuit, it becomes possible to calculate the total global reward rate of the environment, <italic>ρ</italic><sub><italic>g</italic></sub>, as in <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref> (Ap. 1 - Derivation of global reward rate under multiple pursuits)…
<disp-formula id="eqn1">
<graphic xlink:href="599189v1_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>…where <italic>ρ</italic><sub><italic>d</italic></sub> is the rate of reward attained in the default pursuit. Should rewards not occur while in the default pursuit, <italic>ρ</italic><sub><italic>d</italic></sub>, will be zero. <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref> allows for the calculation of the global reward rate achieved by any policy accepting a particular set of pursuits from the environment. This derivation of global reward rate is akin to those similarly derived for prey selection models (see (<xref ref-type="bibr" rid="c21">Charnov and Orians, 1973</xref>) and (<xref ref-type="bibr" rid="c101">Stephens and Krebs, 1986</xref>).</p>
<sec id="s2d1">
<title>Parceling the world into the considered pursuit type (“in” pursuit) and everything else (“out” of pursuit)</title>
<p>In order to simplify representations of policies governing any given pursuit opportunity, we reformulate the above expression for global reward rate, <italic>ρ</italic><sub><italic>g</italic></sub>, from the perspective of a policy of accepting any given pursuit. The environment may be parcellated into the time spent and rewards achieved <italic>inside</italic> the considered pursuit on average, for every instance that time is spent and rewards achieved <italic>outside</italic> the considered pursuit, on average. We can pull out the inside reward (<italic>r</italic><sub><italic>in</italic></sub>) and inside time (<italic>t</italic><sub><italic>in</italic></sub>) from the equation above, to isolate the inside and outside components of the equation.
<disp-formula id="eqn2">
<graphic xlink:href="599189v1_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>From there, we define <italic>t</italic><sub><italic>out</italic></sub> as the average time spent outside the considered pursuit for each instance that the considered pursuit is experienced.
<disp-formula id="eqn3">
<graphic xlink:href="599189v1_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Similarly, the outside reward, <italic>r</italic><sub><italic>out</italic></sub>, encompasses the average amount of reward collected from all sources outside the considered pursuit.
<disp-formula id="eqn4">
<graphic xlink:href="599189v1_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Parceling a pursuit world into a considered pursuit (all instances “inside” the considered pursuit type) and everything else (i.e., everything “outside” the considered pursuit type), then gives the generalized form for the reward rate of an environment under a given policy as…
<disp-formula id="eqn5">
<graphic xlink:href="599189v1_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>…which depends on the average reward earned and the average time spent between opportunities to make the decision, in addition to the average reward returned and average time spent in the considered pursuit (Ap. 3 &amp; <xref rid="fig3" ref-type="fig">Figure 3</xref>).</p>
<p><xref rid="fig2" ref-type="fig">Figure 2</xref> depicts the global reward rate achieved with respect to the time and reward obtained from a considered pursuit (“Inside”) and the time and reward obtained outside that considered pursuit type, i.e., that pursuit’s (“Outside”). By so parsing the world into “in” and “outside” the considered pursuit, it can also be appreciated from <xref rid="fig2" ref-type="fig">Figure 2</xref> that the fraction of time in the environment invested in the considered option, <italic>in</italic>, can be expressed as <inline-formula><inline-graphic xlink:href="599189v1_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and the fraction of time spent outside the considered pursuit as 1 − <italic>w</italic><sub><italic>in</italic></sub>. A world can thus be understood in terms of its composing pursuits’ reward rates and weights (their relative occupancy), with the global reward rate being a weighted average of the reward rate from the considered pursuit,<inline-formula><inline-graphic xlink:href="599189v1_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and the reward rate outside the considered pursuit, <inline-formula><inline-graphic xlink:href="599189v1_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.
<disp-formula id="eqn6">
<graphic xlink:href="599189v1_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Global reward rate with respect to parceling the world into “in” and “outside” the considered pursuit.</title>
<p>A-C as in <xref ref-type="fig" rid="fig1">Figure 1</xref> “Forgo”. D) The world divided into “Inside” and “Outside” the purple pursuit, as the agent decides whether to forgo or accept. The axes are centered on the position of the agent, just before the purple pursuit, where the upper right quadrant shows the inside (purple) pursuit’s reward rate (ρ<sub>in</sub>), while the bottom left quadrant shows the outside (gold) pursuit reward rate (ρ<sub>out</sub>). The global reward rate (ρ<sub>g</sub>) is shown in magenta, calculated from the equation in the box to the right. The agent may determine the higher reward rate yielding policy by comparing the outside reward rate (ρ<sub>out</sub>) with the resulting global reward rate (ρ<sub>g</sub>) under a policy of accepting the considered pursuit.</p></caption>
<graphic xlink:href="599189v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Therefore, the global reward rate is the sum of the local reward rates of the world’s constituent pursuits under a given policy when weighted by their relative occupancy: the weighted average of the local reward rates of the pursuits constituting the world.</p>
</sec>
<sec id="s2d2">
<title>Reward-rate optimizing forgo policy: compare a pursuit’s local reward rate to its outside reward rate</title>
<p>We can now compare two competing policies to identify the policy that maximizes reward rate, such that it is the maximum possible reward rate, <italic>ρg</italic><sup>*</sup>. A policy of taking or forgoing a given pursuit type may improve the reward rate reaped from the environment as a whole (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Using <xref ref-type="disp-formula" rid="eqn5">equation 5</xref>, the policy achieving the greatest global reward rate can be realized through an iterative process where pursuits with lower reward rates than the reward rate obtained from everything other than the considered pursuit type, are sequentially removed from the policy. The optimal policy for forgoing can therefore be calculated directly from the considered pursuit’s reward rate, <italic>ρ</italic><sub><italic>in</italic></sub>, and the reward rate outside of that pursuit type, <italic>ρ</italic><sub><italic>out</italic></sub>. Global reward rate can be maximized by iteratively forgoing the considered pursuit if its reward rate is less than its outside reward rate, <italic>ρ</italic><sub><italic>in</italic></sub> &lt; <italic>ρ</italic><sub><italic>out</italic></sub>, treating forgoing and taking a considered pursuit as equivalent when <italic>ρ</italic><sub><italic>in</italic></sub> = <italic>ρ</italic><sub><italic>out</italic></sub>, and taking the considered pursuit when <italic>ρ</italic><sub><italic>in</italic></sub> &gt; <italic>ρ</italic><sub><italic>out</italic></sub> (Ap. 5).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Forgo Decision-making.</title>
<p>A) When the reward rate of the considered pursuit exceeds that of its outside rate, the global reward rate will be greater than the outside, and therefore the agent should accept the considered pursuit. B) When the reward rates inside and outside the considered pursuit are equivalent, the global reward rate will be the same when accepting or forgoing: the policies are equivalent. C) When the reward rate of the considered pursuit is less than its outside rate, the resulting global reward rate if accepting the considered pursuit will be less than its outside reward rate and therefore should be forgone.</p></caption>
<graphic xlink:href="599189v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Following this policy would be equivalent to comparing the local reward rate of a pursuit to the global reward rate obtained under the reward rate optimal policy: forgo the pursuit when its local reward rate is less than the global reward under the reward rate optimal policy, <italic>ρ</italic><sub><italic>in</italic></sub> &lt; <italic>ρ</italic><sub><italic>g</italic></sub> <sup>*</sup>, take or forgo pursuit when the reward rate of the pursuit is equal to the global reward rate under the optimal policy <italic>ρ</italic><sub><italic>in</italic></sub> = <italic>ρ</italic><sub><italic>g</italic></sub> <sup>*</sup>, and take pursuit when its local reward rate is more than the global reward rate under the reward rate optimal policy, <italic>ρ</italic><sub><italic>in</italic></sub> &gt; <italic>ρ</italic><sub><italic>g</italic></sub> <sup>*</sup> (Ap. 5). The maximum reward rate reaped from the environment can thus be eventually obtained by comparing the local reward rate of a considered pursuit to its outside reward rate (i.e., the global reward rate of a policy of <italic>not</italic> accepting the considered pursuit type).</p>
</sec>
</sec>
<sec id="s2e">
<title>Equivalent immediate reward: the ‘subjective value’, <italic>sv</italic>, of a pursuit</title>
<p>Having recognized how a world can be decomposed into pursuits described by their rates and weights and identifying optimal policies under forgo decisions, we may now ask anew, “What is the worth of a pursuit?” <xref rid="fig2" ref-type="fig">Figure 2D</xref> illustrates that the global reward rate obtained under a policy of taking a pursuit is not just a function of the time and return of the pursuit itself, but also the time spent and return gained outside of that pursuit type. Therefore, the worth of a pursuit relates to how much the pursuit would add (or detract) from the global reward rate realized in its acquisition.</p>
<sec id="s2e1">
<title>Subjective Value of the considered pursuit with respect to the global reward rate</title>
<p>This relationship between a considered pursuit type, its outside, and the global reward rate can be re-expressed in terms of an immediate reward magnitude requiring no time investment that yields the same global reward rate as that arising from a policy of taking the pursuit (<xref rid="fig4" ref-type="fig">Figure 4</xref>). Thus, for any pursuit in a world, the amount of immediate reward that would be accepted in place of its initiation and attainment could serve, then, as a metric of the pursuit’s worth at the time of its initiation. Given the optimal policy above, an expression for this immediate reward magnitude can be derived (Ap. 6). This <italic>global reward-rate equivalent immediate reward</italic> (see <xref rid="fig4" ref-type="fig">Figure 4</xref>) is the <italic>subjective value</italic> of a pursuit, <italic>sv</italic><sub>Pursuit</sub> (or simply, <italic>sv</italic>, when the referenced pursuit can be inferred).
<disp-formula id="eqn6a">
<graphic xlink:href="599189v1_eqn6a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>The Subjective Value (sv) of a pursuit is the global reward rate-equivalent immediate reward magnitude.</title>
<p>The subjective value of a pursuit is that amount of reward requiring no investment of time that the agent would take as equivalent to accepting and acquiring the considered pursuit. For this amount to be equivalent, the immediate reward magnitude must result in the same global reward rate as that of accepting the pursuit. The global reward rate obtained under a policy of accepting the considered pursuit type is the slope of the line connecting the average times and rewards obtained in and outside the considered pursuit type. Therefore, the global reward rate equivalent immediate reward (i.e., the subjective value of the pursuit) can be depicted graphically as the y-axis intercept of the line representing the global reward rate achieved under a policy of accepting the considered pursuit.</p></caption>
<graphic xlink:href="599189v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p><xref ref-type="disp-formula" rid="eqn8">Equation 8</xref>. <italic>The Subjective Value of a pursuit expressed in terms of the global reward rate achieved under a policy of accepting that pursuit</italic></p>
<p>The subjective value of a pursuit under the reward-rate optimal policy will be denoted as <italic>sv*</italic><sub>Pursuit</sub>.</p>
<p>The calculation of the subjective value of a pursuit, <italic>sv</italic>, quantifies precisely the worth of a pursuit in terms of an immediate reward that would result in the same global reward rate as that pursuant to its attainment. Thus, choosing either an immediate reward of magnitude <italic>sv</italic>, or choosing to pursue the considered pursuit, investing the required time and acquiring its reward, would produce an equivalent global reward rate. An agent pursuing an optimal policy would find immediate rewards of magnitude less than <italic>sv</italic> less preferred than the considered pursuit, and immediate rewards of magnitude greater than <italic>sv</italic> more preferred than the pursuit.</p>
</sec>
<sec id="s2e2">
<title>The forgo decision can also be made from subjective value</title>
<p>With this understanding, in the case that the considered pursuit’s reward rate is greater than its outside reward rate, it will be greater than the optimal global reward rate, and therefore the subjective value under an optimal policy will be greater than zero (<xref rid="fig3" ref-type="fig">Figure 3A</xref>).
<disp-formula id="eqn7">
<graphic xlink:href="599189v1_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Should the considered pursuit’s reward rate be equal to its outside reward rate, it will be equal to the optimal global reward rate, and the subjective value of the considered pursuit will be zero (<xref rid="fig3" ref-type="fig">Figure 3B</xref>).
<disp-formula id="eqn7a">
<graphic xlink:href="599189v1_eqn7a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Finally, if the considered pursuit’s reward rate is less than the outside reward rate, it must also be less than the global optimal reward rate; therefore, the subjective value of the considered pursuit under the optimal policy will be less than zero (<xref rid="fig3" ref-type="fig">Figure 3C</xref>).
<disp-formula id="eqn7b">
<graphic xlink:href="599189v1_eqn7b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>While brains of humans and animals may not in fact calculate subjective value, converting to the equivalent immediate reward, <italic>sv</italic> 1) makes connection to temporal decision-making experiments where such equivalences between delayed and immediate rewards are assessed, 2) serves as a common scale of comparison irrespective of the underlying decision-making process, and 3) deepens an understanding of how the worth of a pursuit is affected by the temporal structure of the environment’s reward-time landscape.</p>
</sec>
<sec id="s2e3">
<title>Subjective value with respect to the pursuit’s outside: insights into the cost of time</title>
<p>To the latter point, <xref ref-type="disp-formula" rid="eqn8">Equation 8</xref> has a (deceptively) simple appeal: the worth of a pursuit ought be its reward magnitude less its cost of time (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). But what is the cost of time? The <bold>cost of time</bold> of a considered pursuit is the global reward rate of the world under a policy of accepting the pursuit, times the time that the pursuit would take, <italic>ρ</italic><sub><italic>g</italic></sub><italic>t</italic><sub><italic>in</italic></sub> (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). Therefore, the equivalent immediate reward of a pursuit, its <bold><italic>subjective value</italic></bold>, corresponds to the subtraction of the cost of time from the pursuit’s reward. The subjective value of a pursuit is how much <italic>extra</italic> reward is earned from the pursuit than would on average be earned by investing that amount of time, in that world, under a policy of accepting the considered pursuit.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Equivalent expressions for subjective value reveal time’s cost comprises an opportunity as well as apportionment cost.</title>
<p>A. The subjective value of a pursuit can be expressed in terms of the global reward rate obtained under a policy of accepting the pursuit. It is how much extra reward is earned from the pursuit over its duration than would on average be earned under a policy of accepting the pursuit. B. The cost of time of a pursuit is the amount of reward earned on average in an environment over the time needed for its obtainment under a policy of accepting the pursuit. The reward rate earned on average is the global reward rate (slope of maroon line). Projecting that global reward over the time of the considered pursuit (dashed maroon line) provides the cost of time for the pursuit (vertical maroon bar). Therefore, the subjective value of a pursuit is equivalent to its reward magnitude less the cost of time of the pursuit. C. Expressing subjective value with respect to the outside reward rate rather than the global reward rate reveals that a portion of a pursuit’s time costs arises from an opportunity cost (orange bar). The opportunity cost of a pursuit is the amount of reward earned over the considered pursuit’s time on average under a policy of not taking the considered pursuit (the outside reward rate (slope of gold line). Projecting the slope of the gold line over the time of the considered pursuit (dashed gold line) provides the opportunity cost of the pursuit (vertical orange bar). The opportunity cost-subtracted reward (cyan bar) can then be scaled to a magnitude of reward requiring no time investment that would be equivalent to investing the time and acquiring the reward of the pursuit, i.e., its subjective value. The equation’s denominator provides this scaling term, which is the proportion that the outside time is to the total time to traverse the world (the equation’s denominator). D. The difference between time’s cost and the opportunity cost of a pursuit is a pursuit’s apportionment cost (brown bar). The apportionment cost is the amount of the opportunity subtracted reward that would occur on average over the pursuit’s time under a policy of accepting the pursuit. E&amp;F. Whether expressed in terms of the global reward rate achieved under a policy of not accepting the considered pursuit (E) or accepting the considered pursuit (F), the subjective value expressions are equivalent.</p></caption>
<graphic xlink:href="599189v1_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>While appealing in its simplicity, the terms on the right-hand side of <xref ref-type="disp-formula" rid="eqn8">Equation 8</xref>, <italic>r</italic><sub><italic>in</italic></sub> and <italic>ρ</italic><sub><italic>g</italic></sub><italic>t</italic><sub><italic>in</italic></sub>, lack independence from one another—the reward of the considered pursuit type contributes to the global reward rate, <italic>ρ</italic><sub><italic>g</italic></sub>. Subjective value can alternatively and more deeply be understood by re-expressing subjective value in terms that are independent of one another. Rather than expressing the worth of a pursuit in terms of the global reward rate obtained when accepting it, as in <xref ref-type="disp-formula" rid="eqn8">Equation 8</xref>, the worth of a pursuit can be expressed in terms of the rate of reward obtained outside the considered pursuit type (<xref rid="fig5" ref-type="fig">Figure 5C</xref>), as in <xref ref-type="disp-formula" rid="eqn9">Equation 9</xref> (and see Ap. 8 for derivation).
<disp-formula id="eqn8">
<graphic xlink:href="599189v1_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><xref ref-type="disp-formula" rid="eqn9">Equation 9</xref>. Subjective value of a pursuit from perspective of the considered pursuit and its outside</p>
<p>These expressions are equivalent to one another (see Ap. 8 and <xref rid="fig5" ref-type="fig">Figure 5</xref>).
<disp-formula id="eqn8a">
<graphic xlink:href="599189v1_eqn8a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>For an interactive exploration of the effects of changing the outside and inside reward and time on subjective value, see <ext-link ext-link-type="uri" xlink:href="https://github.com/HuShuLab/InteractivePlot">Supplemental GUI</ext-link>.</p>
</sec>
<sec id="s2e4">
<title>Time’s cost: opportunity &amp; apportionment costs determine a pursuit’s subjective value</title>
<p>By decomposing the global reward rate into ‘inside’ and ‘outside the considered pursuit, the cost of time is revealed as being determined by an 1) opportunity cost, <italic>and</italic> an 2) apportionment cost (<xref rid="fig5" ref-type="fig">Figure 5</xref>). The <bold><italic>opportunity cost</italic></bold> associated with a considered pursuit, <italic>ρ</italic><sub><italic>out</italic></sub><italic>t</italic><sub><italic>in</italic></sub>, is the reward rate of the world under a policy of <italic>not</italic> accepting the considered pursuit (its outside rate), <italic>ρ</italic><sub><italic>out</italic></sub>, times the time of the considered pursuit, <italic>t</italic><sub><italic>in</italic></sub> (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). In the numerator of <xref ref-type="disp-formula" rid="eqn9">Equation 9</xref> (right hand side), this opportunity cost is subtracted from the reward obtained from accepting the considered pursuit. In addition to this opportunity cost subtraction, the cost of time is also determined by time’s <italic>apportionment cost</italic> (<xref rid="fig5" ref-type="fig">Figure 5D</xref>). The apportionment cost relates to time’s allocation in the world: the time spent within a pursuit type relative to the time spent outside that pursuit type, appearing in the denominator. The denominator uses time’s apportionment to scale the opportunity cost subtracted reward of the pursuit to its global reward rate equivalent magnitude requiring no time investment. The amount of reward by which this downscaling decreases the opportunity cost subtracted reward is the apportionment cost of time. In so downscaling, the subjective value of a considered pursuit (green) is to the time it would take to traverse the world were the pursuit not taken, <italic>t</italic><sub><italic>out</italic></sub>, as its opportunity cost subtracted reward (cyan) is to the time to traverse the world were it to be taken (<italic>t</italic><sub><italic>in</italic></sub>+ <italic>t</italic><sub><italic>out</italic></sub>) (<xref rid="fig5" ref-type="fig">Figure 5E</xref>). Let us now consider the impact that changing the outside reward and/or outside time has on these two determinants of time’s cost— opportunity and apportionment cost—to further our understanding of the subjective value of a pursuit.</p>
</sec>
<sec id="s2e5">
<title>The effect of increasing the outside reward on the subjective value of a pursuit</title>
<p><xref rid="fig6" ref-type="fig">Figure 6</xref> illustrates the impact of changing the reward reaped from outside the pursuit on the pursuit’s subjective value. By holding the time spent outside the considered pursuit constant, changing the outside reward thus changes the outside reward rate. When the considered pursuit’s reward rate is greater than its outside reward rate, the subjective value is positive (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). The subjective value diminishes linearly (<xref rid="fig6" ref-type="fig">Figure 6B, green dots</xref>) to zero as the outside reward rate increases to match the pursuit’s reward rate, and turns negative as the outside reward rate exceeds the pursuit’s reward rate, indicating that a policy of accepting the considered pursuit would result in a lower attained global reward rate than that garnered under a policy of forgoing the pursuit. Under these conditions, the subjective value is shown to decrease linearly as the outside reward increases because the cost of time increases linearly (<xref rid="fig6" ref-type="fig">Figure 6B, shaded region</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>The impact of outside reward on the subjective value of a pursuit.</title>
<p><bold>A)</bold> Increasing the outside reward while holding the outside time constant increases the outside reward rate (slope of gold lines), resulting in increasing the global reward rate (slope of the purple lines), and decreasing the subjective value (green dots) of the pursuit. As the reward rate of the environment outside the considered pursuit type increases from lower than, to higher than that of the considered pursuit, the subjective value of the pursuit decreases, becomes zero when the in/outside rates are equivalent, and goes negative when ρ<sub>out</sub> exceeds ρ<sub>in</sub>. <bold>B)</bold> Plotting the subjective value of the pursuit as a function of increasing the outside reward (while holding t<sub>out</sub> constant) reveals that the subjective value of the pursuit decreases linearly. This linear decrease is due to the linear increase in the cost of time of the pursuit (purple dotted region). <bold>C)</bold> Time’s cost (the area, as in B, between the pursuit’s reward magnitude and its subjective value) is the sum of the opportunity cost of time (orange dotted region) and the apportionment cost of time (plum annuli region). When the outside reward rate is zero, time’s cost is composed entirely of an apportionment cost. As the outside reward increases, opportunity cost increases linearly as apportionment cost decreases linearly, until the reward rates in and outside the pursuit become equivalent, at which point the subjective value of the pursuit is zero. When subjective value is zero, the cost of time is entirely composed of opportunity cost. As the outside rate exceeds the inside rate, opportunity cost continues to increase, while the apportionment cost becomes negative (which is to say, the apportionment cost of time becomes an apportionment gain of time). Adding the positive opportunity cost and the negative apportionment cost (subtracting the purple &amp; orange region of overlap from opportunity cost) yields the subjective value of the pursuit.</p></caption>
<graphic xlink:href="599189v1_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Time’s cost is the sum of the opportunity cost and apportionment cost of time (<xref rid="fig6" ref-type="fig">Figure 6C</xref>). When the outside reward is zero, there is zero opportunity cost of time, with time’s cost being entirely constituted by the apportionment cost of time. Apportionment cost (<xref rid="fig6" ref-type="fig">Figure 6C, left hand y-axis</xref>) decreases as outside reward increases because the difference between the inside and outside reward rate diminishes, thus making how time is apportioned in and outside the pursuit less relevant. At the same time, as outside reward increases, the opportunity cost of time increases (<xref rid="fig6" ref-type="fig">Figure 6C, right hand y-axis</xref>). When inside and outside rates are the same, how the agent apportions its time in or outside the pursuit does not impact the global rate of reward. At this point, the apportionment cost of time has fallen to zero, while the opportunity cost of the pursuit has now come to entirely constitute time’s cost. Further increases in the outside reward now result in the outside rate being increasingly greater than the inside rate making the apportionment of time in/outside the pursuit increasingly relevant. Now, however, though the opportunity cost of time continues to grow positively, the apportionment cost of time grows increasingly negative (which is to say the pursuit has an apportionment <italic>gain</italic>). Subtracting the sum of the opportunity cost of the pursuit and the <italic>negative</italic> apportionment cost (i.e., the apportionment gain), from the pursuit’s reward, yields the subjective value of the pursuit.</p>
</sec>
<sec id="s2e6">
<title>The effect of changing the outside time on the subjective value of the considered pursuit</title>
<p><xref rid="fig7" ref-type="fig">Figure 7</xref> examines the effect of changing the outside time on the subjective value of a pursuit, while holding the outside reward constant at a value of zero. Doing so affords a means to examine the apportionment cost of time in isolation from the opportunity cost of time. Despite there being no opportunity cost, there <italic>is</italic> yet a cost of time (<xref rid="fig7" ref-type="fig">Figure 7B</xref>) composed entirely of the apportionment cost (<xref rid="fig7" ref-type="fig">Figure 7C</xref>). When the portion of time spent outside dominants, time’s apportionment cost of the pursuit is small. As the portion of time spent outside the pursuit decreases and the relative apportionment of time spent in the pursuit increases, the apportionment cost of the pursuit increases purely hyperbolically, resulting in the subjective value of the pursuit decreasing purely hyperbolically (<xref rid="fig7" ref-type="fig">Figure 7</xref>). As time spent outside the considered pursuit becomes diminishingly small, the pursuit comprises more and more of the world, until the apportionment of time is entirely devoted to the pursuit, at which point the apportionment cost of time equals the pursuit’s reward rate * t (i.e., the pursuit’s reward magnitude).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>The impact of the apportionment cost of time on the subjective value of a pursuit.</title>
<p><bold>A)</bold> The apportionment cost of time can best be illustrated dissociated from the contribution of the opportunity cost of time by considering the special instance in which the outside has no reward, and therefore has a reward rate of zero. <bold>B)</bold> In such instances, the pursuit still has a cost of time, however. <bold>C)</bold> Here, the cost of time is entirely composed of apportionment cost, which arises from the fact that the considered pursuit is contributing its proportion to the global reward rate. How much is the pursuit’s time cost is thus determined by the ratio of the time spent in the pursuit versus outside the pursuit: the more time is spent outside the pursuit, the less the apportionment cost of time of the pursuit, and therefore, the greater the subjective value of the pursuit. When apportionment cost solely composes the cost of time, the cost of time decreases hyperbolically as the outside time increases, resulting in the subjective value of a pursuit increasing hyperbolically.</p></caption>
<graphic xlink:href="599189v1_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2e7">
<title>The effect of changing the outside time and the outside reward rate on the subjective value of a pursuit</title>
<p>In having examined the effect of varying outside reward (<xref rid="fig6" ref-type="fig">Figure 6</xref>) and outside time (<xref rid="fig7" ref-type="fig">Figure 7</xref>), let us now consider the impact of varying, jointly, the outside time <italic>and</italic> the outside reward rate (<xref rid="fig8" ref-type="fig">Figure 8</xref>). By changing the outside time while holding the outside reward constant, the reward rate obtained in the outside will be varied while the apportionment of time in &amp; outside the pursuit changes (<xref rid="fig8" ref-type="fig">Figure 8A</xref>), thus impacting the opportunity and apportionment cost of time. Plotting the subjective value-by-outside time function, <xref rid="fig8" ref-type="fig">Figure 8B</xref> then reveals that subjective value increases hyperbolically under these conditions as outside time increases, which is to say, time’s cost decreases hyperbolically. Decomposing time’s cost into its constituent opportunity and apportionment costs (<xref rid="fig8" ref-type="fig">Figure 8C</xref>) illustrates how these components vary when varying outside time. Opportunity cost (orange dots) decreases hyperbolically as the outside time increases. Apportionment cost varies as the difference of two hyperbolas (plum annuli area), initially decreasing to zero as the outside and inside rates become equal, and then increasing (plum annuli area). Taken together, their sum (opportunity and apportionment costs) decreases hyperbolically as outside time increases, resulting in subjective values that hyperbolically increase, spanning from the negative of the outside reward magnitude to the inside reward magnitude.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><title>The effect of changing the outside time and the outside reward rate on the subjective value of a pursuit.</title>
<p><bold>A)</bold> The subjective value (green dots) of the considered pursuit when changing the outside time and outside reward rate. <bold>B)</bold> As outside time increases under these conditions (holding positive outside reward constant), the subjective value of the pursuit increases hyperbolically, from the negative of the outside reward magnitude to, in the limit, the inside reward magnitude. Conversely, time’s cost (purple annuli) decreases hyperbolically. <bold>C)</bold> Opportunity cost decreases hyperbolically as outside time increases. Apportionment cost initially decreases to zero as the outside and inside rates become equal, and then increases as the difference of two hyperbolas (plum annuli area). When the outside reward rate is greater than the inside reward rate, apportionment could be said to have a gain (a negative cost). Summing opportunity cost and apportionment cost yields time’s cost.</p></caption>
<graphic xlink:href="599189v1_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s2f">
<title>The value of initiating pursuits in choice decision-making</title>
<p>Above, we determined how a reward rate maximizing agent would evaluate the worth of a pursuit, identifying the impact of a policy of taking (or forgoing) that pursuit on the realized global reward rate, and expressing that pursuit’s worth as subjective value. We did so by opposing a pursuit with its equivalent offer requiring no time investment—a special and instructive case. In this section we consider what decision should be made when an agent is simultaneously presented with a choice of more than one pursuit of any potential magnitude and time investment. Using the subjective value under these choice decisions, we more thoroughly examine how the duration and magnitude of a pursuit, and the context in which it is embedded (its ‘outside’), impacts reward rate optimal valuation. We then re-express subjective value as a temporal discounting function, revealing the nature of the <italic>apparent</italic> temporal discounting function of a reward rate maximizing agent as one determined wholly by the temporal structure and magnitude of rewards in the environment. We then assess whether hyperbolic discounting and the “Magnitude” and “Sign” effect—purported signs of suboptimal decision-making (<xref ref-type="bibr" rid="c106">Thaler, 1981</xref>; <xref ref-type="bibr" rid="c58">Loewenstein and Thaler, 1989</xref>; <xref ref-type="bibr" rid="c27">Estle et al., 2006</xref>)—are in fact consistent with optimal decision-making.</p>
<sec id="s2f1">
<title>Choice decision-making</title>
<p>Consider a temporal decision in which two or more mutually exclusive options are simultaneously presented following a period that is common to policies of choosing one or another of the considered options (<xref rid="fig9" ref-type="fig">Figure 9</xref>). In such scenarios, subjects choose between outcomes differing in magnitude and the time at which they will be delivered. Of particular interest are choices between a smaller, sooner reward pursuit (“SS” pursuit) and a larger, later reward pursuit (“LL” pursuit) (<xref ref-type="bibr" rid="c74">Myerson and Green, 1995</xref>; <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>; <xref ref-type="bibr" rid="c62">Madden and Bickel, 2010</xref>; <xref ref-type="bibr" rid="c83">Peters and Büchel, 2011</xref>). Such intertemporal decision-making is commonplace in the laboratory setting (<xref ref-type="bibr" rid="c69">McDiarmid and Rilling, 1965</xref>; <xref ref-type="bibr" rid="c87">Rachlin et al., 1972</xref>; <xref ref-type="bibr" rid="c2">Ainslie, 1974</xref>; <xref ref-type="bibr" rid="c96">Snyderman, 1983</xref>; <xref ref-type="bibr" rid="c74">Myerson and Green, 1995</xref>; <xref ref-type="bibr" rid="c5">Bateson and Kacelnik, 1996</xref>; <xref ref-type="bibr" rid="c81">Ostaszewski, 1996</xref>; <xref ref-type="bibr" rid="c98">Stephens and Anderson, 2001</xref>; <xref ref-type="bibr" rid="c24">Cheng et al., 2002</xref>; <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>; <xref ref-type="bibr" rid="c42">Hayden and Platt, 2007</xref>; <xref ref-type="bibr" rid="c40">Hayden et al., 2007</xref>; <xref ref-type="bibr" rid="c67">McClure et al., 2007</xref>; <xref ref-type="bibr" rid="c8">Beran and Evans, 2009</xref>; <xref ref-type="bibr" rid="c83">Peters and Büchel, 2011</xref>; <xref ref-type="bibr" rid="c102">Stevens and Mühlhoff, 2012</xref>; <xref ref-type="bibr" rid="c19">Carter et al., 2015</xref>; <xref ref-type="bibr" rid="c20">Carter and Redish, 2016</xref>).</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9.</label>
<caption><title>Policy options considered during the initiation of pursuits in worlds with a “Choice” topology.</title>
<p><bold>A-C)</bold> Choice topology, and policies of choosing the small-sooner or larger-later pursuit, as in <xref ref-type="fig" rid="fig1">Figure 1</xref> “Choice”. <bold>D)</bold> The world divided into “Inside” and “Outside” the selected pursuit, as the agent decides whether to accept SS (aqua) or LL (purple) pursuit. The global reward rate (ρ<sub>g</sub>) under a policy of choosing the SS or LL (slopes of the magenta lines), calculated from the equation in the box to the right.</p></caption>
<graphic xlink:href="599189v1_fig9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2f2">
<title>Global reward rate equation and Optimal Choice Policy</title>
<p>With the global reward rate equation previously derived, which choice policy (i.e., choosing SS, or LL) would maximize global reward rate can be identified. The optimal choice between the SS and the LL pursuit is as follows…
<disp-formula id="eqn9">
<graphic xlink:href="599189v1_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9a">
<graphic xlink:href="599189v1_eqn9a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9b">
<graphic xlink:href="599189v1_eqn9b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>These policies’ optimality is intuitive. By choosing option LL, the subject earns <italic>r</italic><sub><italic>LL</italic></sub> − <italic>r</italic><sub><italic>SS</italic></sub> more reward than when choosing SS but spends <italic>t</italic><sub><italic>LL</italic></sub> − <italic>t</italic><sub><italic>SS</italic></sub> more time. If the reward rate from that extra time spent exceeds the reward rate of the environment generally, it would be optimal to spend the extra time on the larger-later option. In other words, if the agent were to choose pursuit SS, <italic>t</italic><sub><italic>LL</italic></sub> − <italic>t</italic><sub><italic>SS</italic></sub> time would be spent earning reward at a global reward rate under that policy, <italic>ρ</italic><sub><italic>g</italic>,<italic>choose SS</italic></sub>, with the magnitude <italic>ρ</italic><sub><italic>g</italic></sub>(<italic>t</italic><sub><italic>LL</italic></sub> − <italic>t</italic><sub><italic>SS</italic></sub>). If <italic>ρ</italic><sub><italic>g</italic></sub>(<italic>t</italic><sub><italic>LL</italic></sub> − <italic>t</italic><sub><italic>SS</italic></sub>) exceeds the extra reward <italic>r</italic><sub><italic>LL</italic></sub> − <italic>r</italic><sub><italic>SS</italic></sub> that could be earned with that extra time by investing the LL pursuit, more reward would be earned in the same amount of time by choosing the SS Pursuit.</p>
</sec>
<sec id="s2f3">
<title>Optimal Choice Policies based on Subjective Value</title>
<p>As under forgo decision-making, we can now also identify the global reward rate optimizing choice policies based on subjective value (<xref rid="fig9" ref-type="fig">Figure 9</xref>). The following policies would optimize reward rate when choosing between two options of different magnitude that require different amounts of time invested:
<disp-formula id="eqn10">
<graphic xlink:href="599189v1_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn10a">
<graphic xlink:href="599189v1_eqn10a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn10b">
<graphic xlink:href="599189v1_eqn10b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s2f4">
<title>The impact of opportunity &amp; apportionment costs on choice decision-making</title>
<p>With optimal policies for choice expressed in terms of subjective value, the impact of time’s opportunity and apportionment costs on choice decision-making can now be more deeply appreciated. Keeping the outside time constant, the opportunity cost of time increases as the outside reward (and thus the outside reward rate) increases, decreasing linearly the subjective value of the considered pursuits (<xref rid="fig10" ref-type="fig">Figure 10</xref>). However, as the opportunity cost of the LL pursuit is greater than that of the SS due to its greater time requirement, its slope is greater than that of the SS, resulting in a switch in preference from the LL pursuit to that of the SS pursuit at some critical outside reward rate threshold.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10.</label>
<caption><title>Effect of opportunity cost on subjective value in choice decision-making.</title>
<p>The effect of increasing the outside reward while holding the outside time constant is to linearly increase the opportunity cost of time, thus decreasing the subjective value of pursuits considered in choice decision-making. When the outside reward is sufficiently small, the subjective value of the LL pursuit can exceed the SS pursuit, indicating that selection of the LL pursuit would maximize the global reward rate. As outside reward increases, however, the subjective value of pursuits will decrease linearly as the opportunity cost of time increases. Since a policy of choosing the LL pursuit will have the greater opportunity cost, the slope of its function relating subjective value to outside reward will be greater than that of a policy of choosing the SS pursuit. Thus, outside reward can be increased sufficiently such that the subjective value of the LL and SS pursuits will become equal, past which the agent will switch to choosing the SS pursuit.</p></caption>
<graphic xlink:href="599189v1_fig10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>A switch in preference between the SS and LL pursuits will also occur when the time spent outside the considered pursuit increases past some critical threshold even if the outside reward rate earned remains constant (<xref rid="fig11" ref-type="fig">Figure 11</xref>). As any inside time will constitute a greater fraction of the total time under a LL versus a SS pursuit policy, the apportionment cost of the LL pursuit will be greater. This can result in the subjective value of the SS pursuit being greater, initially, than the LL pursuit. As the outside time increases, however, the ordering of subjective value will switch as apportionment costs becoming diminishingly small.</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure 11.</label>
<caption><title>Effect of apportionment cost on subjective value in choice decision-making.</title>
<p>The effect of increasing the outside time (while maintaining outside rate) is to decrease the apportionment cost of the considered pursuit, thus increasing its subjective value. When the outside time is sufficiently small, the apportionment cost for LL and SS pursuits will be large, but can be greater still for the LL pursuit given its proportionally longer duration to the outside time. As outside reward time increases, however, the subjective value of pursuits increase as the apportionment cost of time of the considered pursuit decreases. As apportionment costs diminish and the magnitudes of pursuits’ rewards become more fully realized, the subjective value of the LL pursuit will eventually exceed that of the SS pursuit at sufficiently long outside times.</p></caption>
<graphic xlink:href="599189v1_fig11.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Finally, the effect of varying opportunity and apportionment costs on subjective value in Choice behavior is considered (<xref rid="fig12" ref-type="fig">Figure 12</xref>). Opportunity and apportionment costs can simultaneously be varied, for instance, by maintaining outside reward but increasing outside time. Doing so decreases the apportionment as well as the opportunity cost of time by changing the proportion of time in and outside the considered pursuit, which, in turn, lowers the outside reward rate. A switch in preference will then occur from the SS to the LL pursuit as they are differentially impacted by both the opportunity as well as the apportionment cost of time.</p>
<fig id="fig12" position="float" fig-type="figure">
<label>Figure 12.</label>
<caption><title>Effect of varying opportunity and apportionment costs on Choice behavior.</title>
<p>The effect of increasing the outside time while maintaining outside reward is to decrease the apportionment as well as the opportunity cost of time, thus increasing pursuit’s subjective value. Increasing outside time, which in turn, also decreases outside reward rate, results in the agent appearing as if to become more patient, being willing to switch from a policy of selecting the SS pursuit to a policy of selecting the LL pursuit past some critical threshold (vertical dashed black line).</p></caption>
<graphic xlink:href="599189v1_fig12.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>A reward rate optimal agent will thus appear as if more patient the longer the time spent outside a considered pursuit, the lower the outside reward rate, or both, switching from a policy of choosing the SS to choosing the LL option at some critical outside reward rate and/or time. Having analyzed the impact of time spent and reward obtained outside a pursuit on a pursuit’s valuation, we now examine the impact time spent within a pursuit has on its valuation.</p>
</sec>
<sec id="s2f5">
<title>The Discounting Function of a reward rate optimal agent</title>
<p>How does the value of a pursuit change as the time required for its obtainment grows? Intertemporal decision-making between pursuits requiring differing time investments resulting in different reward magnitudes has typically been examined using a ‘temporal discounting function’ to describe how delays in reward influence their valuation. This question has been investigated experimentally by pitting smaller-sooner options against later-larger options to experimentally determine the <italic>subjective value</italic> of the delayed reward (<xref ref-type="bibr" rid="c70">Mischel, Grusec, &amp; Masters, 1969</xref>), with the best fit to many such observations across delays determining the subjective value function. After normalizing by the magnitude of reward, the curve of subjective values as a function of delay is the “temporal discounting function” (for review see <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>). While the temporal discounting function has historically been used in many fields, including economics, psychology, ethology, and neuroscience to describe how delays influence rewards’ subjective value, its origins—from a normative perspective—remain unclear (Hayden, 2015). What, then, is the temporal discounting function of a reward-rate optimal agent? And would its determination provide insight into why experimentally derived discounting functions present in the way they do, with their varied forms and curious sensitivity to the context, magnitude, and sign of pursuit outcomes?</p>
</sec>
<sec id="s2f6">
<title>Discounting Function of an Optimal Agent is a Hyperbolic Function</title>
<p>The temporal discounting function of an optimal agent can be expressed by normalizing its subjective value-time function by the considered pursuit’s magnitude.
<disp-formula id="ueqn1">
<graphic xlink:href="599189v1_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><xref ref-type="disp-formula" rid="eqn9">Equation 9</xref>. <italic>The Discounting Function of a Global Reward Rate Optimal Agent</italic>.</p>
<p>To illustrate the discounting function of a reward-rate maximal agent, <xref rid="fig13" ref-type="fig">Figure 13</xref> depicts how the worth of a pursuit’s reward would change as its required time investment increases in three different world contexts: a world in which there is, A) zero outside reward rate &amp; large outside time, B) zero outside reward rate &amp; small outside time, and, C) positive outside reward rate &amp; small outside time. <xref rid="fig13" ref-type="fig">Figure 13</xref> first graphically depicts the subjective values of the pursuit’s reward at increasing temporal delays (the y-intercepts of the lines depicting the resulting global reward rates, green dots) in each of these world contexts (A-C). Then, by replotting these subjective values at their corresponding delays, the subjective value-time function is created for this increasingly delayed reward in each of these worlds (D-F). By normalizing by the reward magnitude, these subjective value-time functions are then converted to their corresponding discounting functions (color coded) and overlaid so that their shapes may be compared (G).</p>
<fig id="fig13" position="float" fig-type="figure">
<label>Figure 13.</label>
<caption><title>The temporal discounting function of a global reward-rate optimal agent is a hyperbolic function relating the apportionment and opportunity cost of time.</title>
<p><bold>A-C)</bold> The effect, as exemplified in three different worlds, of varying the outside time and reward on the subjective value of a pursuit as its reward is displaced into the future. The subjective value, sv, of this pursuit, as its temporal displacement into the future increases, is indicated as the green dots along the y-intercept in these three different contexts: a world in which there is A) zero outside reward rate &amp; large outside time, B) zero outside reward rate &amp; small outside time, and C) positive outside reward rate &amp; the small outside time as in B. <bold>D-F)</bold> Replotting these subjective values at their corresponding temporal displacement yields the subjective value function of the offered reward in each of these contexts. <bold>G:</bold> Normalizing these subjective value functions by the reward magnitude and superimposing the resulting temporal discounting functions reveals how the steepness and curvature of the apparent discounting function of a reward rate maximizing agent changes with respect to the average reward and time spent outside the considered pursuit. When the time spent outside is increased (compare B to A)—thus decreasing the apportionment cost of time—the temporal discounting function becomes less curved, making the agent appear as if more patient. When the outside reward is increased (compare B to C)—thus increasing the opportunity cost of time—the temporal discounting function becomes steeper, making the agent appear as if less patient.</p></caption>
<graphic xlink:href="599189v1_fig13.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Doing so illustrates how the mathematical form of the temporal discount function—as it appears for the optimal agent—is a hyperbolic function. This function’s form depends wholly on the temporal reward structure of the environment and is composed of hyperbolic and linear components which relate to the apportionment and to the opportunity cost of time. To best appreciate the contributions of opportunity and apportionment costs to the discounting function of a reward rate-optimal agent, consider the following instances exemplified in <xref rid="fig13" ref-type="fig">Figure 13</xref>. First, in worlds in which no reward is received outside a considered pursuit, the apparent discounting function is <italic>purely</italic> hyperbolic (<xref rid="fig13" ref-type="fig">Figure 13A</xref>). Purely hyperbolic discounting is therefore optimal when the subjective value function follows the equation sv = rt + ITI (ITI: intertrial interval with no reward), as in many experimental designs. Second, as less time is apportioned outside the considered pursuit type (<xref rid="fig13" ref-type="fig">Figure 13B</xref>), this hyperbolic curve becomes more curved as the pursuit’s time apportionment cost increases. The curvature of the hyperbolic component is thus controlled by how much time the agent spends in versus outside the considered pursuit: with the more time spent outside the pursuit, the gentler the curvature of apparent hyperbolic discounting, and the more patient the agent appears to become for the considered pursuit. Third, in worlds in which reward is received outside a considered pursuit (compare B to C), the apparent discounting function will become more steep the more outside reward is obtained, as the linear component relating the opportunity cost of time increases (while the apportionment cost of time decreases).</p>
<p>Thus, by expressing the worth of a pursuit as would be evaluated by a reward-rate optimal agent in terms of its discounting function, we find that its form is consonant with what is commonly reported experimentally in humans and animals, and will exhibit apparent changes in curvature and steepness that relate directly to the reward acquired and time spent outside the considered pursuit for every time spent within it.</p>
</sec>
<sec id="s2f7">
<title>Magnitude effect and the Sign Effect</title>
<p>With this insight into how opportunity and apportionment costs impact the cost of time, and therefore the subjective value of pursuits in Choice decision-making, reward-rate optimal agents are now understood to exhibit a hyperbolic form of discounting, as commonly exhibited by humans and animals (<xref ref-type="bibr" rid="c87">Rachlin et al., 1972</xref>; <xref ref-type="bibr" rid="c1">Ainslie, 1975</xref>; <xref ref-type="bibr" rid="c106">Thaler, 1981</xref>; <xref ref-type="bibr" rid="c63">Mazur, 1987</xref>; <xref ref-type="bibr" rid="c7">Benzion et al., 1989</xref>; <xref ref-type="bibr" rid="c33">Green et al., 1994</xref>; <xref ref-type="bibr" rid="c86">Rachlin et al., 2000</xref>; <xref ref-type="bibr" rid="c53">Kobayashi and Schultz, 2008</xref>; <xref ref-type="bibr" rid="c17">Calvert et al., 2010</xref>; <xref ref-type="bibr" rid="c28">Fedus et al., 2019</xref>). As hyperbolic discounting is not a sign of suboptimal decision-making, as is widely asserted, are other purported signs of suboptimal decision-making, namely the “Magnitude” and “Sign” effect, also consistent with optimal temporal decisions?</p>
</sec>
<sec id="s2f8">
<title>Magnitude effect</title>
<p>The Magnitude Effect refers to the observation that the temporal discounting function, as experimentally determined, is observed to become less steep the larger the offered reward. If brains apply a discounting function to account for the delay to reward, why, as it is posed, do different magnitudes of reward appear as if discounted with different temporal discounting functions? <xref rid="fig14" ref-type="fig">Figure 14</xref> considers how a reward-rate maximizing agent would appear to discount rewards of two magnitudes (large - top row; small - bottom row), first by determining the subjective value (green dots) of differently sized rewards (<xref rid="fig14" ref-type="fig">Figure 14 A &amp; D</xref>) across a range of delays, and second, by replotting the <italic>sv</italic>’s at their corresponding delays (<xref rid="fig14" ref-type="fig">Figure B &amp; E</xref>), to form their subjective value functions (blue and red curves, respectively). After normalizing these subjective value functions by their corresponding reward magnitudes, the resulting temporal discounting functions that would be fit for a reward-rate maximizing agent are then shown in (<xref rid="fig14" ref-type="fig">Figure 14C</xref>). The pursuit with the larger reward outcome (blue) thus would appear as if discounted by a less steep discounting function than the smaller pursuit (red), under what are otherwise the same circumstances. Therefore, the ‘Magnitude Effect’, as observed in humans and animals, would also be exhibited by a reward-rate maximizing agent.</p>
<fig id="fig14" position="float" fig-type="figure">
<label>Figure 14.</label>
<caption><title>Reward-rate maximizing agents would exhibit the “Magnitude effect”.</title>
<p><bold>A&amp;B)</bold> The global reward rate (the slope of magenta vectors) that would be obtained when acquiring a considered pursuit’s reward of a given size (either relatively large as in A or small as in B) but at varying temporal removes, depicts how a considered pursuit’s subjective value (green dots, y-intercept) would decrease as the time needed for its obtainment increases in environments that are otherwise the same. <bold>C&amp;D)</bold> Replotting the subjective values of the considered pursuit to correspond to their required delay forms the subjective value-time function for the “large” reward case (C), and the “small” reward case (D). <bold>E)</bold> Normalizing the subjective value-time functions by their reward magnitude transforms these functions into their corresponding discounting functions (blue: large reward DF; red: small reward DF), and reveals that a reward-rate maximizing agent would exhibit the “Magnitude Effect” as the steepness of the apparent discounting function would change with the size of the pursuit, and manifest as being less steep the greater the magnitude of reward.</p></caption>
<graphic xlink:href="599189v1_fig14.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2f9">
<title>The Sign Effect</title>
<p>The Sign Effect refers to the observation that the discounting functions for outcomes of the same magnitude but opposite valence (rewards and punishments) appear to discount at different rates, with punishments discounting less steeply than rewards. Should the brain apply a discounting function to outcomes to account for their temporal delays, why does it seemingly use different discount functions for rewards and punishments of the same magnitude? <xref rid="fig15" ref-type="fig">Figure 15</xref> considers how a reward-rate maximizing agent would appear to discount outcomes (reward and punishment) of the same magnitude but opposite valence when spending time outside a pursuit, obtaining a positive reward rate. By determining the subjective value of these oppositely signed outcomes across a range of delays and plotting their normalized subjective values at their corresponding delay, the apparent discounting function for reward and punishment, as expressed by a reward-rate maximizing agent, exhibits the “Sign effect” observed in humans and animals. In addition, we note that the difference in discounting function slopes between rewards and punishments of equal magnitude would diminish as the outside reward approached zero, become identical when zero, and even invert when the outside reward rate is negative (which is to say, reward would appear to discount less steeply than punishments).</p>
<fig id="fig15" position="float" fig-type="figure">
<label>Figure 15.</label>
<caption><title>Reward-rate maximizing agents would exhibit the “Sign effect”.</title>
<p><bold>A&amp;B)</bold> The global reward rate (the slope of magenta lines) that would be obtained when acquiring a considered pursuit’s outcome of a given magnitude but differing in sign (either rewarding as in A, or punishing as in B), depicts how the subjective value (green dots, y-intercept) would decrease as the time of its obtainment increases in environments that are otherwise the same (one in which the agent spends the same amount of time and receives the same amount of reward outside the considered pursuit for every instance within it). <bold>C&amp;D)</bold> Replotting the subjective values of the considered pursuit to correspond to their required delay forms the subjective value-time function for the reward (C) and for the punishment (D). <bold>E)</bold> Normalizing the subjective value-time functions by their outcome transforms these functions into their corresponding discounting functions (blue: reward DF; red: punishment DF). This reveals that a reward-rate maximizing agent would exhibit the “Sign Effect”, as the steepness of the apparent discounting function would change with the sign of the pursuit, manifesting as being less steep for punishing than for rewarding outcomes of the same magnitude.</p></caption>
<graphic xlink:href="599189v1_fig15.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2f10">
<title>Summary</title>
<p>In the above sections, we provide a richer understanding of the origins of time’s cost in evaluating the worth of initiating a pursuit. We demonstrate that the intuitive, if deceptively simple, equation for subjective value (<xref ref-type="disp-formula" rid="eqn8">Equation 8</xref>) that subtracts time’s cost is equivalent to subtracting an opportunity cost <italic>and</italic> an apportionment cost of time (<xref ref-type="disp-formula" rid="eqn9">Equation 9</xref>). Whereas the simple equation’s time cost is calculated from the global reward rate under a policy of accepting the considered pursuit (<xref ref-type="disp-formula" rid="eqn8">Equation 8</xref>), parceling the world into the contribution from in and outside the considered pursuit type (<xref ref-type="disp-formula" rid="eqn9">Equation 9</xref>) reveals that the opportunity cost of time arises from the global reward rate achieved under a policy of <italic>not</italic> accepting the considered pursuit (it’s outside reward rate), and that the apportionment cost of time arises from the allocation of time spent in, versus outside, the considered pursuit. These equivalent expressions for the normatively-defined (reward-rate maximizing) subjective value of a pursuit give rise to an apparent discounting function that is a hyperbolic function of time, who’s hyperbolic component constitutes the apportionment cost, and whose linear component constitutes the opportunity cost of time. By re-expressing reward rate maximization as its apparent temporal discounting function, we demonstrate how fits of hyperbolic discounting, as well as observations of the Magnitude and Sign effect—commonly taken as signs of suboptimal decision-making—are in fact consistent with optimal temporal decision-making.</p>
</sec>
</sec>
<sec id="s2g">
<title>Sources of error and their consequences</title>
<p>While these added insights enrich our understanding of time’s cost and reveal how purported signs of irrationality can in fact be consistent with a reward-rate maximizing agent, it nonetheless remains true that animals and humans <italic>are</italic> suboptimal temporal decision makers—exhibiting an “impatience” by selecting smaller, sooner (SS) options in cases where selecting larger, later (LL) options would maximize global reward rate. However, when decisions to accept or reject pursuits are presented in Forgo situations, they are observed to be optimal. As the equivalent immediate reward equations enabling global reward rate optimization may potentially be instantiated by neural representations of their underlying variables, we conjecture that misrepresentation of one or another variable may best explain the particular ways in which observed behavior deviates, <italic>as well as accords</italic>, with optimality. Therefore, we now ask what errors in temporal decision-making behavior would result from misestimating these variables, with the aim of identifying the nature of misestimation that best accounts for the pattern actually observed in animals and humans regarding whether to initiate a given pursuit.</p>
<p>To understand how systematic error in an agent’s estimation of different time and/or reward variables would affect its behavior, we examine the agent’s pattern of behavior in both Choice and Forgo decisions across different outside reward rates. First, we ask whether the agent would choose a SS or LL pursuit as in a choice task. Then we ask whether the agent would take or forgo the same LL and SS pursuits when either are presented alone in a forgo task. The actions taken by the agent can therefore be described as a triplet of policies referring to the two pursuits (e.g., <bold>choose SS, forgo LL, forgo SS</bold>).</p>
<p>Let us first consider how a reward rate optimal agent would transition from one to another pattern of decision-making as outside reward rate increases for the situation of fundamental interest: where the reward rate of the SS pursuit is greater than that of the LL pursuit (<xref rid="fig16" ref-type="fig">Figure 16</xref>). When the outside reward rate (slope of golden line) is sufficiently low (<xref rid="fig16" ref-type="fig">Figure 16A</xref>), the agent should prefer LL in Choice, be willing to take the LL pursuit in Forgo, and be willing to take the SS pursuit in Forgo (choose LL, take LL, take SS). Here, a “sufficiently low” outside rate is one such that the resulting global reward rate (slope of magenta line) is less than the difference in the reward rates of the SS and LL pursuits. When the outside reward rate increases to greater than this difference in the pursuits’ reward rates but is less than the reward rate of the LL option, the agent should choose SS in Choice and be willing to take either in Forgo (choose SS, take LL, take SS) (<xref rid="fig16" ref-type="fig">Figure 16B</xref>). Further increases in outside rate up to that equaling the reward rate of the SS results in the agent selecting the SS in Choice, forgoing LL in Forgo, and taking SS in Forgo (choose SS, forgo LL, take SS) (<xref rid="fig16" ref-type="fig">Figure 16C</xref>). Finally, any additional increase in outside rate would result in choosing the SS pursuit under Choice, and forgoing both pursuits in Forgo (choose SS, forgo LL, forgo SS) (<xref rid="fig16" ref-type="fig">Figure 16D</xref>). Colored regions thus describe the pattern of decision-making behavior exhibited by a reward rate optimal agent under any combination of outside reward and time.</p>
<fig id="fig16" position="float" fig-type="figure">
<label>Figure 16.</label>
<caption><title>Relationship between outside time and reward with optimal temporal decision-making behavioral transitions.</title>
<p>An agent may be presented with three decisions: the decision to take or forgo a smaller, sooner reward of 2.5 units after 2.5 seconds (SS pursuit), the decision to take or forgo a larger, later reward of 5 units after 8.5 seconds (LL pursuit), and the decision to choose between the SS and LL pursuits. The slope of the purple line indicates the global reward rate (ρ<sub>g</sub>) resulting from a Choice or Take policy, while the slope of “outside” the pursuit (golden line) indicates the outside reward rate (i.e., global reward rate resulting from a Forgo policy). In each panel (A-D), an example outside reward rate is plotted, illustrating the relative ordering of ρ<sub>g</sub> slopes for each policy. Location in the lower left quadrant is thereby shaded according to the combination of global rate-maximizing policies for each of the three decision types.</p></caption>
<graphic xlink:href="599189v1_fig16.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>With this understanding of the optimal thresholds between behavior policies, we can now examine the impact on decision-making behavior of different types of error in the agent’s understanding of the world (<xref rid="fig17" ref-type="fig">Figure 17</xref>). We introduce an error term, ω, such that different parameters impacting the global reward rate of each considered policy are underestimated (ω&lt;1) or overestimated (ω&gt;1) (<xref rid="fig17" ref-type="fig">Figure 17</xref> column 1, see Ap. 11 for formal definitions). Resulting global reward rate mis-estimations are equivalent to introducing error in the considered pursuit’s subjective value, which will result in various deviations from reward-rate maximization (<xref rid="fig17" ref-type="fig">Figure 17</xref>). Conditions wherein overestimation of global reward rate would lead to suboptimal choice behavior are identified formally in Ap. 12.</p>
<fig id="fig17" position="float" fig-type="figure">
<label>Figure 17.</label>
<caption><title>Patterns of suboptimal temporal decision-making behavior resulting from time and/or reward misestimation.</title>
<p>Patterns of temporal decision-making in Choice and Forgo situations deviate from optimal (top row) under various parameter misestimations (subsequent rows). Characterization of the nature of suboptimality is aided by the use of the outside reward rate as the independent variable influencing decision-making (x-axis), plotted against the degree of error (y-axis) of a given parameter (ω&lt;1 underestimation, ω=1 actual, ω&gt;1 overestimation). The leftmost column provides a schematic exemplifying true outside (gold) and inside (blue) pursuit parameters and the nature of parameter error (dashed red) investigated per row (all showing an instance of underestimation). For each error case, the agent’s resulting choice between SS and LL pursuits (2nd column), decision to take or forgo the LL pursuit (3rd column), and decision to take or forgo the SS pursuit (4th column) are indicated by the shaded color (legend, bottom of columns) for a range of outside rates and degrees of error. The rightmost column depicts the triplet of behavior observed, combined across tasks. <underline>Rows</underline>: <bold>A)</bold> “No error” - Optimal choice and forgo behavior. Vertical white lines show outside reward rate thresholds for optimal forgo behavior. B-G) Suboptimal behavior resulting from parameter misestimation. B-D) The impact of outside pursuit parameter misestimation. <bold>B)</bold> “Outside Time”- The impact of misestimating outside time (and thus misestimating outside reward rate). <bold>C)</bold> “Outside Reward”- The impact of misestimating outside reward (and thus misestimating outside reward rate). <bold>D)</bold> “Outside Time &amp; Reward”- The impact of misestimating outside time and reward, but maintaining outside reward rate. <bold>E-G)</bold> The impact of inside pursuit parameter misestimation. <bold>E)</bold> “Pursuit Time”- The impact of misestimating inside pursuit time (and thus misestimating inside pursuit reward rate. <bold>F)</bold> “Pursuit Reward” - The impact of misestimating the pursuit reward (and thus misestimating the pursuit reward rate). <bold>G)</bold> “Pursuit Time and Reward” - The impact of misestimating the pursuit reward and time, but maintaining the pursuit’s reward rate. For this illustration, we determined the policies for a SS pursuit of 2 reward units after 2.5 seconds, a LL pursuit of 4.75 reward units after 8 seconds, and an outside time of 10 seconds. The qualitative shape of each region and resulting conclusions are general for all situations where the SS pursuit has a higher rate than the LL pursuit (and where a region exists where the optimal agent would choose LL at low outside rates).</p></caption>
<graphic xlink:href="599189v1_fig17.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The sources of error considered are mis-estimations of the reward obtained and/or time spent “outside” (rows B-D) and “inside” (rows E-G) the considered pursuit. When both reward and time are misestimated, we examine the case in which the reward rate of that portion of the world is maintained (rows D &amp; G). The agent’s resulting policies in Choice (second column) and both Forgo situations (third and fourth columns) are determined across a range of outside reward rates (x-axes) and degrees of parameter misestimation (y-axes) and color-coded, with the boundary between the colored regions indicating the outside reward rate threshold for transitions in the agent’s behavior. These individual policies are collapsed into the triplet of behavior expressed across the decision types (fifth column). In this way, characterization of the nature of suboptimality is aided by the use of the outside reward rate as the independent variable influencing decision-making, with the outside reward rate thresholds for optimal behavior being compared to the outside reward rate thresholds under any given parameter misestimation (comparing top “optimal” row A, against any subsequent row B-G). Any deviations in this pattern of behavior from that of the optimal agent (row A) are suboptimal, resulting in a failure to maximize reward rate in the environment.</p>
<p>While misestimation of any of these parameters will lead to suboptimal behavior, only specific sources and directions of error may result in behavior that qualitatively matches human and animal behavior observed experimentally. Misestimation of outside time (B), outside reward (C), inside time (E), and inside reward (F) all display Choice behavior that is qualitatively similar to experimentally observed behavior, either via underestimation or overestimation of the key variable. For example, underestimation of the outside time (B, ω&lt;1) leads to selection of the SS pursuit at sub-optimally low outside reward rates. However, agents with these types of error never display optimal Forgo behavior. By contrast, misestimation of either outside time and reward (D) or inside time and reward (G) display suboptimal Choice while maintaining optimal Forgo. Specifically, underestimation of outside time and reward (D, ω&lt;1) and overestimation of inside time and reward (G, ω&gt;1) both result in suboptimal preference for SS at low outside rates. Therefore, and critically, if the rates of both inside and outside are maintained despite misestimating reward and time magnitudes, the resulting errors allow for optimal Forgo behavior while displaying suboptimal “impatience” in Choice, and thus match experimentally observed behavior.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In order to understand why humans and animals factor time the way they do in temporal decision-making, our initial step has been to understand how a reward-rate maximizing agent would evaluate the worth of initiating a pursuit within a temporal decision-making world. We did so in order to identify what are and are not signs of suboptimality and to gain insight into how animals’ and humans’ valuation of pursuits actually deviate from optimality. By analyzing fundamental temporal decisions, we identified equations enabling reward-rate maximization that evaluate the worth of initiating a pursuit. We first considered <bold>Forgo</bold> decisions to appreciate that a world can be parcellated into its constituent pursuits, revealing how pursuits’ rates and relative occupancies (their ‘weights’), along with the decision policy, determine the global reward rate. In doing so, we derived an expression for the worth of a pursuit in terms of the resulting global reward rate, and from it, re-expressed the pursuit’s worth in terms of its global reward rate-equivalent immediate reward, i.e., its ‘subjective value’. We further show that time’s cost, rather than being calculated from the global reward rate under a policy of accepting the considered pursuit, can equally be calculated in terms of the outside reward rate and time (a policy of <italic>not</italic> accepting the considered pursuit type). Expressing subjective value in terms of a pursuit’s outside reward rate and time reveals that time’s cost is constituted by an apportionment cost, as well as an opportunity cost. By then examining <bold>Choice</bold> decisions, we provide a deeper understanding of the nature of apparent temporal discounting in reward rate maximizing agents and establish that hyperbolic discounting, the Magnitude Effect, and the Sign Effect, are <italic>not</italic> signs of suboptimal decision-making, but rather are consistent with reward-rate maximization. While these purported signatures of suboptimality would in fact arise from reward-rate maximization, humans and animals are, nonetheless, suboptimal temporal decision makers, exhibiting apparent discounting functions that are too steep. By examining misestimation of the parameters that enable reward-rate maximization identified here, we implicate overestimation of the relative time spent in versus outside the considered pursuit type as the likely source of error committed by animals and humans in temporal decision-making that underlies their suboptimal pursuit valuation. We term this “The Malapportionment Hypothesis”.</p>
<sec id="s3a">
<title>Temporal decision-making theories and frameworks</title>
<p>Two theories have predominated over the course of theorizing about how animals should invest time when pursuing rewards of a diversity of magnitudes and delays: a theory of exponential discounting (<xref ref-type="bibr" rid="c91">Samuelson, 1937</xref>; <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>; <xref ref-type="bibr" rid="c47">Kalenscher and Pennartz, 2008</xref>), and a theory of optimal foraging (<xref ref-type="bibr" rid="c23">Charnov, 1976b</xref>; <xref ref-type="bibr" rid="c85">Pyke et al., 1977</xref>; <xref ref-type="bibr" rid="c101">Stephens and Krebs, 1986</xref>; <xref ref-type="bibr" rid="c97">Stephens, 2008</xref>). According to the former, exhibiting a permanent preference for one option over another through time was argued to be rational (<xref ref-type="bibr" rid="c71">Montague and Berns, 2002</xref>; <xref ref-type="bibr" rid="c65">Mazur, 2006</xref>; <xref ref-type="bibr" rid="c75">Nakahara and Kaveri, 2010</xref>), as in Discounted Utility Theory (DUT) (<xref ref-type="bibr" rid="c92">Samuelson, 1938</xref>). Discounting functions operating under this principle would then be exponential, with the best fit exponent controlling and embodying the agent’s appreciation of the cost of time. In contrast, Optimal Foraging Theory (OFT) invoked reward rate maximization as the normative principle. Referenced by a wide assortment of ethologists and ecologists (for review see (<xref ref-type="bibr" rid="c84">Pyke, 1984</xref>)), the specific formulation proponents of OFT generally use would result in an apparent discounting function that is hyperbolic. Indeed, in controlled laboratory experiments in which animals make decisions about how to spend time between rewarding options (<xref ref-type="bibr" rid="c38">Hariri et al., 2006</xref>; <xref ref-type="bibr" rid="c41">Hayden et al., 2011</xref>; <xref ref-type="bibr" rid="c108">Wikenheiser et al., 2013</xref>; <xref ref-type="bibr" rid="c12">Blanchard and Hayden, 2014</xref>, <xref ref-type="bibr" rid="c13">2015</xref>; <xref ref-type="bibr" rid="c19">Carter et al., 2015</xref>; <xref ref-type="bibr" rid="c20">Carter and Redish, 2016</xref>), experimental observations have demonstrated that hyperbolic functions are better fits to choice behavior in intertemporal choice tasks than exponential functions (<xref ref-type="bibr" rid="c1">Ainslie, 1975</xref>; <xref ref-type="bibr" rid="c107">Thaler and Shefrin, 1981</xref>; <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>; <xref ref-type="bibr" rid="c34">Green and Myerson, 2004</xref>; <xref ref-type="bibr" rid="c51">Kim et al., 2008</xref>; <xref ref-type="bibr" rid="c13">Blanchard and Hayden, 2015</xref>). Nonetheless, and problematically for OFT, in most intertemporal choice tasks, animal behavior is far from optimal for maximizing reward rate (<xref ref-type="bibr" rid="c88">Reynolds and Schiffbauer, 2004</xref>; <xref ref-type="bibr" rid="c41">Hayden et al., 2011</xref>; <xref ref-type="bibr" rid="c14">Blanchard et al., 2013</xref>; <xref ref-type="bibr" rid="c13">Blanchard and Hayden, 2015</xref>).</p>
<sec id="s3a1">
<title>Hyperbolic Temporal Discounting Functions</title>
<p>Indeed, with respect to global reward rate maximization, animals and humans typically exhibit much too great a preference for smaller-sooner rewards (SS) in apparent discounting of delayed rewards (<xref ref-type="bibr" rid="c25">Chung and Herrnstein, 1967</xref>; <xref ref-type="bibr" rid="c87">Rachlin et al., 1972</xref>; <xref ref-type="bibr" rid="c2">Ainslie, 1974</xref>; <xref ref-type="bibr" rid="c106">Thaler, 1981</xref>; <xref ref-type="bibr" rid="c45">Ito and Asaki, 1982</xref>; <xref ref-type="bibr" rid="c36">Grossbard and Mazur, 1986</xref>; <xref ref-type="bibr" rid="c64">Mazur, 1988</xref>; <xref ref-type="bibr" rid="c7">Benzion et al., 1989</xref>; Loewenstein and Prelec, 1992; <xref ref-type="bibr" rid="c33">Green et al., 1994</xref>; <xref ref-type="bibr" rid="c5">Bateson and Kacelnik, 1996</xref>; <xref ref-type="bibr" rid="c46">Kacelnik and Bateson, 1996</xref>; <xref ref-type="bibr" rid="c18">Cardinal et al., 2001</xref>; <xref ref-type="bibr" rid="c98">Stephens and Anderson, 2001</xref>; <xref ref-type="bibr" rid="c6">Bennett, 2002</xref>; <xref ref-type="bibr" rid="c29">Frederick et al., 2002</xref>; <xref ref-type="bibr" rid="c43">Holt et al., 2003</xref>; <xref ref-type="bibr" rid="c109">Winstanley et al., 2004</xref>; <xref ref-type="bibr" rid="c48">Kalenscher et al., 2005</xref>; <xref ref-type="bibr" rid="c90">Roesch et al., 2007</xref>; <xref ref-type="bibr" rid="c53">Kobayashi and Schultz, 2008</xref>; <xref ref-type="bibr" rid="c61">Louie and Glimcher, 2010</xref>; <xref ref-type="bibr" rid="c82">Pearson et al., 2010</xref>). More precisely, what is meant by this suboptimal bias for SS is that the switch in preference from LL to SS occurs at an outside reward rate that is lower—and/or an outside time that is less than—what an optimal agent would exhibit. To account for this departure from optimality, a free-fit parameter, <italic>k</italic>, controlling the steepness of temporal discounting was introduced, <inline-formula><inline-graphic xlink:href="599189v1_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, accommodating the variability observed across and within subjects, and is commonly interpreted as a psychological trait, such as patience, or willingness to delay gratification (<xref ref-type="bibr" rid="c1">Ainslie, 1975</xref>).</p>
<p>In this way, the Discounting Function framework has often been reified into a function possessed by the brain, an intrinsic property used to reduce, in a manner idiosyncratic to the agent, the value of delayed reward. Indeed, discounting functions have been directly incorporated into numerous models (<xref ref-type="bibr" rid="c75">Nakahara and Kaveri, 2010</xref>; <xref ref-type="bibr" rid="c49">Kane et al., 2019</xref>), motivating the search for its neurophysiological signature (<xref ref-type="bibr" rid="c72">Montague et al., 2006</xref>). In addition to accommodating intra- and inter-subject variability through the use of this free-fit parameter, discounting function formulations must also contend with the fact that best fits differ in steepness 1) when the time spent and reward gained outside the pursuit changes (<xref ref-type="bibr" rid="c57">Lea, 1979</xref>; <xref ref-type="bibr" rid="c99">Stephens and Dunlap, 2009</xref>; <xref ref-type="bibr" rid="c14">Blanchard et al., 2013</xref>; <xref ref-type="bibr" rid="c13">Blanchard and Hayden, 2015</xref>; <xref ref-type="bibr" rid="c19">Carter et al., 2015</xref>; <xref ref-type="bibr" rid="c95">Smethells and Reilly, 2015</xref>; <xref ref-type="bibr" rid="c20">Carter and Redish, 2016</xref>), 2) when the reward magnitude of the pursuit changes (the Magnitude Effect), and 3) when considering the sign of the outcome of the pursuit (the Sign Effect). This sensitivity to conditions and variability across and within subjects has spurred a hunt for the ‘perfect’ discounting function (<xref ref-type="bibr" rid="c76">Namboodiri and Hussain Shuler, 2016</xref>) in an effort to better fit behavioral observations, resulting in formulations of increasing complexity (<xref ref-type="bibr" rid="c56">Laibson, 1997</xref>; <xref ref-type="bibr" rid="c68">McClure et al., 2004</xref>; <xref ref-type="bibr" rid="c3">al-Nowaihi and Dhami, 2008</xref>; <xref ref-type="bibr" rid="c50">Killeen, 2009</xref>). While such accommodations may provide for better fits of data, the uncertain origins of discounting functions (<xref ref-type="bibr" rid="c39">Hayden, 2016</xref>) pose a challenge to the utility of this framework in rationalizing observed behavior.</p>
</sec>
<sec id="s3a2">
<title>The apparent discounting function of global reward-rate optimal agents exhibits purported signs of suboptimality</title>
<p>Of the array of temporal decision-making behaviors commonly observed and viewed through the lens of discounting, what might be better accounted for by a deeper understanding of how a reward rate optimal agent would evaluate the worth of initiating a pursuit? To address this, we derived expressions of reward rate maximization, translated them into subjective value, and then re-expressed subjective value in terms of the apparent discounting function that would be exhibited by a reward-rate maximizing agent. We demonstrate that a simple and intuitive equation subtracting time’s cost is equivalent to a hyperbolic discounting equation. This analysis determines that the form and sensitivity to conditions that temporal discounting is experimentally observed to exhibit would actually be expressed by a reward-rate maximizing agent. In doing so, we emphasize how discounting functions should be considered as descriptions of the result of a process, rather than being the process itself.</p>
<p>Regarding form, our analysis reveals that the apparent discounting function of a reward-rate maximizing agent is a hyperbolic function. The diminishment of the value of a pursuit as its time investment increases is thus due to time’s cost―itself hyperbolic―which is shown to be composed of an apportionment (hyperbolic – linear) as well as an opportunity cost (linear) (<xref rid="fig18" ref-type="fig">Figure 18</xref> &amp; <xref rid="tbl1" ref-type="table">Table 1, right column</xref>).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Opportunity cost, apportionment cost, time cost, and subjective value functions by change in outside and inside reward and time.</title>
<p>Functions assume positive inside and outside rewards and times. <sup>1</sup>If outside reward rate is zero, opportunity cost becomes a constant at zero. <sup>2</sup>If outside reward rate is zero, as outside or inside time is varied, apportionment cost becomes purely hyperbolic.</p></caption>
<graphic xlink:href="599189v1_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<fig id="fig18" position="float" fig-type="figure">
<label>Figure 18.</label>
<caption><title>The cost of time of a pursuit comprises both an opportunity as well as an apportionment cost.</title>
<p>The global reward rate under a policy of accepting the considered pursuit type (slope of magenta time), times the time that that pursuit takes (t<sub>in</sub>), is the pursuit’s <underline>time’s cost</underline> (height of maroon bar). The subjective value of a pursuit (height of green bar) is its reward magnitude (height of the purple bar) less its cost of time. Opportunity and apportionment costs are shown to compose the cost of time of a pursuit. <underline>Opportunity cost</underline> associated with a considered pursuit, ρ<sub>out</sub>*t<sub>in</sub>, (height of orange bar) is the reward rate of the world under a policy of not accepting the considered pursuit (its outside rate), ρ<sub>out</sub>, times the time of the considered pursuit, t<sub>in</sub>. The amount of reward that would be (on average) obtained over the time of accepting the considered pursuit—were there to be no opportunity cost—is the <underline>apportionment cost</underline> of time (height of brown bar).</p></caption>
<graphic xlink:href="599189v1_fig18.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In addition to demonstrating the form of the discounting function of an optimal agent, we can now also rationalize why it would appear to change in relationship to the features of the temporal decision-making world. First, rather than being a free-fit parameter like <italic>k</italic> in hyperbolic discounting models (<xref rid="fig19" ref-type="fig">Figure 19A</xref>), the reciprocal of the time spent outside the considered pursuit type controls the degree of curvature in reward-rate optimizing agents (<xref rid="fig19" ref-type="fig">Figure 19B</xref>, denominator<bold>)</bold>. Therefore, changes in the apparent ‘willingness’ of a reward-rate optimal agent to wait for reward would accompany any change in the amount of time that that agent needs to spend outside the considered pursuit, making the agent act as if more patient the greater the time spent outside a pursuit for every instance it spends within it.</p>
<fig id="fig19" position="float" fig-type="figure">
<label>Figure 19.</label>
<caption><title>Comparison of typical hyperbolic discounting versus apparent discounting of a reward-rate optimal agent.</title>
<p>Whereas <bold>(A)</bold> the curvature of hyperbolic discounting models is typically controlled by the free fit parameter k, <bold>(B)</bold> the curvature and steepness of the apparent discounting function of a reward rate optimal agent is controlled by the time spent and reward rate obtained outside the considered pursuit. Understanding the shape of discounting models from the perspective of a reward-rate optimal agent reveals that k ought relate to the apportionment of time spent in, versus outside, the considered pursuit, underscoring, how typical hyperbolic discounting models fail to account for the opportunity cost of time (and thus cannot yield negative sv’s no matter the temporal displacement of reward). Should k be understood as representing time’s apportionment cost, the failure to account for the opportunity cost of time would lead to aberrantly high values of k.</p></caption>
<graphic xlink:href="599189v1_fig19.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Second, discounting frameworks must also rationalize why the apparent steepness of discounting changes as the reward rate acquired outside the considered pursuit changes, which we show here to be related to the linear opportunity cost of time in a reward rate maximizing agent (<xref rid="fig19" ref-type="fig">Figure 19B</xref>, subtraction of opportunity cost occurring in the numerator). The greater the opportunity cost of time, the steeper the apparent discounting function, and the less patient the agent would appear to be, even forgoing pursuits resulting in reward (when their acceptance would yield rates less than the outside rate, i.e., when <italic>sv</italic> &lt; 0). Hyperbolic discounting functions that lack a proper accounting of the opportunity cost cannot then fit negative subjective values, and thus must compensate by overestimating <italic>k</italic> (which rightfully should only relate to the apportionment cost). In this way, such hyperbolic discounting models are only appropriate in worlds with no “outside” reward, or, where being in a pursuit does not exclude the agent from receiving rewards at the rate that occurs outside of it (Ap. 13).</p>
<p>Third and fourth, discounting frameworks must make an accounting of the Magnitude Effect and Sign Effect, respectively, as they are considered important “anomalous” departures from microeconomic theory (<xref ref-type="bibr" rid="c58">Loewenstein and Thaler, 1989</xref>). To do so, rationalizations from previous work have invoked additional assumptions, such as separate processes for small and large rewards (<xref ref-type="bibr" rid="c106">Thaler, 1981</xref>), or the inclusion of a utility function (Loewenstein and Prelec, 1992b; <xref ref-type="bibr" rid="c50">Killeen, 2009</xref>). We demonstrate here how the ‘Magnitude Effect’ would be a natural consequence of a process that <italic>would</italic> maximize reward rate, without invoking specialized processes or additional functions. This analysis predicts that the size of the Magnitude Effect would be observed, experimentally, to diminish the greater the outside time and/or the smaller the outside reward rate. Whereas discounting frameworks need invoke separate discounting functions to contend with different discounting rates for positive (rewarding) and negative (punishing) outcomes of the same magnitude (the Sign Effect), here too, we demonstrate how this is consistent with a reward-rate maximizing process, wherein the asymmetry in the steepness of apparent discounting to rewards and punishments results from the average time and magnitude of rewards (or punishments) received outside the considered pursuit. The average of rewards and punishments experienced outside the considered pursuit type thus forms a bias in evaluating equivalently sized outcomes of opposite sign. From the global reward-rate maximizing perspective, we then also predict that the size of the Sign effect would diminish as the outside reward rate decreases (and as the outside time increases), and in fact would invert should the outside reward rate turn negative (become net punishing), such that punishments would appear to discount more steeply than rewards.</p>
<p>Collectively, our analysis of discounting functions reveals that features typically taken as signs of suboptimal/irrational decision-making are, in fact, consistent with reward-rate maximization. In this way, the general form and sensitivity to conditions of discounting functions, as observed experimentally, can be better understood from the perspective of a reward-rate optimal agent (<xref rid="tbl1" ref-type="table">Table 1</xref>), providing a more parsimonious accounting of a confusing array of temporal decision-making behaviors reported.</p>
</sec>
</sec>
<sec id="s3b">
<title>Humans and animals are nonetheless suboptimal. What is the nature of this suboptimality?</title>
<p>These insights into the behavior of a reward-rate maximizing agent inform on the meaning of the concept “patience”. Patience oughtn’t imply a willingness to wait a longer time, as it is not correct to say that an agent that chooses a pursuit requiring a long time investment is more patient that one that does not, for the amount of time a reward-rate maximizing agent is willing to invest isn’t an intrinsic property of the agent itself. Rather, it is a consequence of the temporal decision-making world’s reward-time structure. So, if patience is to mean investing the ‘correct’ amount of time (i.e., the reward-rate maximizing time), then a reward-rate optimal agent doesn’t <italic>become</italic> more or less patient as the context of what is otherwise the same pursuit changes; rather, it is <italic>precisely</italic> patient, under all circumstances. Impatience and over-patience then are terms to describe the behavior of a global reward-rate <italic>suboptimal</italic> agent that invests either too little, or too much time into a pursuit policy than one that would maximize global reward rate.</p>
<p>Having clarified what behaviors are and are not signs of suboptimality, actual differences to optimal performance exhibited by humans and animals can now be identified and quantified. So, what then are the decision-making behaviors of humans and animals when tasked with valuing the initiation of a pursuit, as in forgo and choice decisions? In controlled experimental situations, forgo decision-making is observed to be near optimal, consistent with observations from the field of behavioral ecology (<xref ref-type="bibr" rid="c55">Krebs et al., 1977</xref>; <xref ref-type="bibr" rid="c101">Stephens and Krebs, 1986</xref>; <xref ref-type="bibr" rid="c12">Blanchard and Hayden, 2014</xref>). In contrast, a suboptimal bias for smaller-sooner rewards is widely reported in Choice decision-making in situations where selection of later-larger rewards would maximize global reward rate (<xref ref-type="bibr" rid="c60">Logue et al., 1985</xref>; <xref ref-type="bibr" rid="c13">Blanchard and Hayden, 2015</xref>; <xref ref-type="bibr" rid="c20">Carter and Redish, 2016</xref>; <xref ref-type="bibr" rid="c49">Kane et al., 2019</xref>). Collectively, the pattern of temporal decision-making behavior observed under forgo and choice decisions shows that humans and animals act as if sub-optimally impatience under choice, while exhibiting near-optimal decision-making under forgo decisions.</p>
</sec>
<sec id="s3c">
<title>The Malapportionment Hypothesis</title>
<p>How can animals and humans be sub-optimally impatient in choice, but optimal in forgo decisions? We postulated that previous behavioral findings of suboptimality can be understood from the perspective of overestimating the global reward rate. While misestimation of any variable underlying global reward rate calculation will lead to errors, not all misestimations will lead to errors that match the behavioral pattern of decisions observed experimentally. Having identified equations and their variables enabling reward-rate maximization, we sought to identify the likely source of error committed by animals and humans by analyzing the pattern of behavior consequent to misestimating one or another parameter. To do so, we identified the reward rate obtained outside a considered pursuit type as a useful variable to characterize departure from optimal decision-making behavior. Sweeping over a range of these values as the independent variable, we determined change points in decision-making behavior that would arise from misestimation (over- and under-estimations) of given reward-rate maximizing parameters.</p>
<p>Our analysis shows how, precisely, misestimation of the inside and outside time or reward will lead to suboptimal temporal decision-making behavior. What errors, however, result in decisions that best accord with what is observed experimentally (i.e., result in suboptimal impatience in choice and optimal forgo decision-making)? Overestimating outside time, underestimating outside reward, underestimating inside time, or overestimating inside reward would fail to match suboptimal ‘impatience’ in Choice <italic>and</italic> would result in suboptimal Forgo. Underestimating outside time, overestimating outside reward, overestimating inside time, or underestimating inside reward would match experimentally observed ‘impatience’ in Choice, but fail to match experimentally observed optimal Forgo behavior. To exhibit optimal forgo behavior, the inside and outside reward rates must be accurately appreciated. Therefore, misestimations of reward <italic>and</italic> time that preserve the true reward rates in and outside the pursuit would permit optimal forgo decisions while still misestimating the global reward rate. Overestimation of the outside time or underestimation of the inside time―while maintaining reward rates―fails to match experimentally observed ‘impatience’ in choice tasks while achieving optimal forgo decisions. However, underestimation of the outside time or overestimation of the inside time―while maintaining true inside and outside reward rates―<italic>would</italic> allow optimal forgo decision-making behavior while resulting in impatient choice behavior, as experimentally observed.</p>
<p>Previous experimental observations are consistent with, and have been interpreted as, an agent underestimating the time spent outside the considered pursuit (<xref ref-type="bibr" rid="c99">Stephens and Dunlap, 2009</xref>; <xref ref-type="bibr" rid="c14">Blanchard et al., 2013</xref>; <xref ref-type="bibr" rid="c95">Smethells and Reilly, 2015</xref>), as would occur with underestimation of post-reward delays (<xref ref-type="bibr" rid="c99">Stephens and Dunlap, 2009</xref>; <xref ref-type="bibr" rid="c95">Smethells and Reilly, 2015</xref>; <xref ref-type="bibr" rid="c39">Hayden, 2016</xref>). Therefore, observed behavioral errors point to misestimating time apportionment in/outside the pursuit, either by 1) overestimating the occupancy of the considered choice or 2) underestimating the time spent outside the considered pursuit type, but not by 3) an misestimation of either the inside or outside reward rate. Only errors in time apportionment that underweight the outside time, (or, equivalently, overweight the inside time)―while maintaining the true inside and outside reward rates―will accord with experimentally observed temporal decision-making regarding whether to initiate a pursuit.</p>
<p>Thus, when a temporal decision world can effectively be bisected into two components, as often the case in experimental situations, only the reward rates, <italic>but not the weights</italic> of those portions need be accurately appreciated for the agent to optimally perform forgo decisions. Therefore, when tested in such situations, even agents that misestimate the apportionment of time can yet make optimal forgo decisions based solely from a comparison of the reward rate in versus outside the pursuit. However, when faced with a choice between two or more pursuits when emerging from a path in common to any choice policy, optimal pursuit selection based on relative rate comparisons is no longer guaranteed, as <italic>not only</italic> the reward rates of pursuits, but their weights as well must then be accurately appreciated. Misestimation of the weights of pursuits comprising a world then results in errors in valuation regarding the initiation of a pursuit under choice instances. We term this reckoning of the source of error committed by animals and humans the <bold><italic>Malapportionment Hypothesis</italic></bold>, which identifies the underweighting of the time spent outside versus inside, a considered pursuit (but <italic>not</italic> the misestimation of pursuit rates) as the source of error committed by animals and humans (<xref rid="fig20" ref-type="fig">Figure 20</xref>). This hypothesis therefore captures previously published behavioral observations showing that animals can make decisions to take or forgo reward options that optimize reward accumulation (<xref ref-type="bibr" rid="c55">Krebs et al., 1977</xref>; <xref ref-type="bibr" rid="c101">Stephens and Krebs, 1986</xref>; <xref ref-type="bibr" rid="c12">Blanchard and Hayden, 2014</xref>), but make suboptimal decisions when presented with simultaneous and mutually exclusive choices between rewards of different delays (<xref ref-type="bibr" rid="c13">Blanchard and Hayden, 2015</xref>; <xref ref-type="bibr" rid="c16">Calhoun and Hayden, 2015</xref>; <xref ref-type="bibr" rid="c20">Carter and Redish, 2016</xref>).</p>
<fig id="fig20" position="float" fig-type="figure">
<label>Figure 20.</label>
<caption><title>The Malapportionment Hypothesis.</title>
<p>The Malapportionment Hypothesis holds that suboptimal decision-making, as revealed under Choice decision-making, arises in humans and animals as a consequence of the valuation process underweighting the contribution of accurately assessed reward rates outside versus inside the considered pursuit type. A) An example Choice situation where the global reward rate is maximized by choosing a larger later reward over a smaller sooner reward. B) An agent that underweights the outside time but accurately appreciates the outside and inside reward rates, overestimates the global reward rate resulting from each policy, and thus exhibits suboptimal impatience by selecting the smaller sooner reward. C) Similarly, an agent that overweights the time inside the considered pursuits but accurately appreciates the outside and inside reward rates also overestimates the global reward rate and selects the smaller sooner reward. As inside and outside reward rates are accurately assessed, forgo decisions can correctly be made despite any misappreciation of the relative time spent in/outside the considered pursuit.</p></caption>
<graphic xlink:href="599189v1_fig20.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3d">
<title>Comparisons to prior models</title>
<p>As our description of global reward rate-optimizing valuation is motivated by the same normative principle, how is our formalism unique from OFT, and, more generally, from other models proposing some form of reward-rate maximization? Firstly, the specific formulation proponents of OFT have used fails to adequately recognize how outside rewards influence the value of considered pursuits. Additionally, the relationship between time’s cost and apparent temporal discounting has not been explicitly identified in prior OFT explanations. By contrast, our formulation, because of its specificity, can potentially align with neural representations of the variables we propose, and their misestimations may explain the ways in which observed animal behavior may deviate from optimality. Models inspired by OFT’s objective of global reward rate maximization but that seek to make a better accounting of observed deviations make the concession that, while global reward rate maximization is sought, it is not achieved. Rather, some <italic>non</italic>-global reward rate maximization is obtained by the agent (<xref ref-type="bibr" rid="c5">Bateson and Kacelnik, 1996</xref>; <xref ref-type="bibr" rid="c14">Blanchard et al., 2013</xref>; <xref ref-type="bibr" rid="c78">Namboodiri et al., 2014b</xref>; <xref ref-type="bibr" rid="c30">Fung et al., 2021</xref>). Of particular interest, the TIMERR model (<xref ref-type="bibr" rid="c79">Namboodiri et al., 2014c</xref>) and the Heuristic model (<xref ref-type="bibr" rid="c14">Blanchard et al., 2013</xref>) both assume non-global reward-rate maximization.</p>
<sec id="s3d1">
<title>TIMERR Model</title>
<p>The essential feature of the TIMERR model (<xref ref-type="bibr" rid="c78">Namboodiri et al., 2014b</xref>) is that the agent looks back into its near past to estimate the reward rate of the environment, with this ‘look-back’ time, T<sub>ime</sub>, being the model’s free-fit parameter. In contrast to the reward rate optimal agent, this look-back time, then, is not a basic feature of the external world, but rather is related to how the animal uses its experience. TIMERR’s policy is then determined by the reward rate obtained across this interval and that of the considered pursuit. In this way, TIMERR includes sources outside of the considered pursuit type in its evaluation, and because of this, exhibits many of the behaviors that the reward rate optimal agent is demonstrated here to express (<xref ref-type="bibr" rid="c77">Namboodiri et al., 2014a</xref>, <xref ref-type="bibr" rid="c78">2014b</xref>, <xref ref-type="bibr" rid="c79">2014c</xref>; <xref ref-type="bibr" rid="c94">Shuler and Namboodiri, 2018</xref>). Indeed, the TIMERR model and the optimal agent share the same mathematical form, though, critically, the meaning of their terms differ. An important additional difference is that TIMERR is specific in the manner in which reward obtained outside the current instance of the considered pursuit is used: as recently experienced rewards from the past contribute to the estimation of the average reward rate of the environment, this ‘look-back’ time can include rewards from the pursuit type currently under consideration. Therefore, TIMERR commits an overestimation of the outside reward rate, and thus, an overestimation of global reward rate, manifesting as suboptimal impatience in choice <italic>and</italic> forgo decisions. In this way, while TIMERR is appealing in assuming that the recent past is used to estimate the global reward rate, and reproduces a number of sensitivities to conditions observed behaviorally, it is not in accordance with the Malapportionment Hypothesis as it mistakes pursuits’ rates as well as their weights.</p>
</sec>
<sec id="s3d2">
<title>Heuristic Model</title>
<p>In the “Heuristic” model (<xref ref-type="bibr" rid="c14">Blanchard et al., 2013</xref>), as in Ecological Rationality Theory, ERT (<xref ref-type="bibr" rid="c100">Stephens et al., 2004</xref>), it is thought that animals prioritize the local reward rate of considered pursuits, rather than the global reward rate. In the Heuristic model, however, suboptimal “impatience” is rationalized as being the consequence of the animal’s inability to fully appreciate post-reward delays (time subsequent to reward until re-entry into states/pursuits common to one or another policy). Indeed, while animals are demonstrated to be sensitive to post-reward delays, they act as if they significantly underestimate post-reward delays incurred, exhibiting a suboptimal bias for SS pursuits when LL pursuits would maximize global reward rate (<xref ref-type="bibr" rid="c14">Blanchard et al., 2013</xref>). Through a parameter, ω, which adjusts the degree in which post-reinforcer delays are underestimated, the Heuristic model can be sufficient to capture observed animal behavior in intertemporal choice tasks (<xref ref-type="bibr" rid="c14">Blanchard et al., 2013</xref>). However, as the Heuristic model is quite specific as to the source of error—the underestimation post-reward delays—it would well fit observed behavior only in certain experimental conditions. Should appreciable 1) reward be obtained or 2) time be spent outside of a considered pursuit type and its post-reward interval, then the Heuristic model would fail to make a good accounting of observed behavior.</p>
<p>The Heuristic model can be modified to specify the uniform downscaling of <italic>all</italic> non-pursuit intervals (rather than just post-reward delays), as in the implementation by Carter and Redish (<xref ref-type="bibr" rid="c20">Carter and Redish, 2016</xref>). This modification would bring the Heuristic model closer into alignment with the Malapportionment Hypothesis. But, as temporal underestimation would not apply to pursuits occurring outside the currently considered one, fits to observed behavior would be strained in worlds composed predominantly of pursuits with little non-pursuit time. Further, by underestimating the time spent outside the considered pursuit without a corresponding underestimation of reward earned outside the considered pursuit, the Heuristic model ought to overestimate the outside reward rate and thus the global reward rate.</p>
<p>So, while impatience under Choice could be fit under some experimental circumstances, behavior under Forgo instances would then be expected to also be sub-optimally impatient. Therefore, to bring the Heuristic model fully into alignment with the Malapportionment Hypothesis, it must be further assumed that the reward rate from the considered pursuit can be compared to the true outside or true global reward rate of the environment (<xref ref-type="bibr" rid="c20">Carter and Redish, 2016</xref>), as well as expanding the model to incorporate all intervals of time occurring outside a considered pursuit.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<title>Conclusion</title>
<p>An enriched understanding of how a reward-rate optimal agent evaluates temporal decision-making empowers insight into the nature of human and animal valuation. It does so not by advancing the claim that we are optimal, but rather by clarifying what are and are not signs of optimality, which then permits quantification of the intriguing pattern of adherence and deviation from this normative expectation. Therein lies clues for deducing the learning algorithm and representational architecture used by brains to attribute value to representations of the temporal structure of the world. Here we have conceptualized and generalized temporal decision-making worlds as composed of pursuits, described by their rates and weights, and in so doing, come to better appreciate the cost of time, how policies impact the reward rates reaped from those worlds, and how processes that fail to accurately appreciate those features would misvalue the worth of initiating pursuits. We propose the Malapportionment Hypothesis, which identifies a failure to accurately appreciate the weights rather than the rates of pursuits, as the root cause of errors made, to reckon with the curious pattern of behavior observed regarding whether to initiate a pursuit. We postulate that the value learning algorithm and representational architecture selected for by evolution has favored the ability to appreciate the reward rates of pursuits over that of their weights.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1">
<title>Appendices</title>
<sec id="s5">
<label>Ap 1.</label>
<title>Derivation of equation for global reward rate given a menu of options</title>
<p><italic>E</italic>(<italic>r</italic>): the expected reward magnitude for each reward opportunity</p>
<p><italic>E</italic>(<italic>t</italic>): the expected time between the initiation of reward pursuits</p>
<p><inline-formula><inline-graphic xlink:href="599189v1_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> global reward rate: the average reward per pursuit divided by the average time per pursuit.</p>
<p><italic>ρ</italic><sub><italic>d</italic></sub>: the average rate of collecting rewards while in the default pursuit</p>
<p><italic>p</italic><sub><italic>i</italic></sub> : reward opportunities <italic>i</italic> as a proportion of total pursued rewards
<disp-formula id="eqn1_1">
<graphic xlink:href="599189v1_eqn1_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><inline-formula><inline-graphic xlink:href="599189v1_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the average reward received per reward opportunity
<disp-formula id="eqn1_2">
<graphic xlink:href="599189v1_eqn1_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><inline-formula><inline-graphic xlink:href="599189v1_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the average time invested per reward opportunity
<disp-formula id="eqn1_3">
<graphic xlink:href="599189v1_eqn1_3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><inline-formula><inline-graphic xlink:href="599189v1_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula>: the average time spent in the default pursuit between reward opportunities</p>
<p><inline-formula><inline-graphic xlink:href="599189v1_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula>: the average reward received in the default pursuit between reward opportunities</p>
<p><inline-formula><inline-graphic xlink:href="599189v1_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the global reward rate of the reward opportunity landscape
<disp-formula id="eqn1_4">
<graphic xlink:href="599189v1_eqn1_4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s6">
<label>Ap 2.</label>
<title>Average time spent outside <italic>t</italic><sub><italic>out</italic></sub> the considered pursuit type, <italic>in</italic>, and the average reward rate earned outside that pursuit type, <italic>ρ</italic><sub><italic>out</italic></sub></title>
<disp-formula id="eqn2_1">
<graphic xlink:href="599189v1_eqn2_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn2_2">
<graphic xlink:href="599189v1_eqn2_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn2_3">
<graphic xlink:href="599189v1_eqn2_3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn2_4">
<graphic xlink:href="599189v1_eqn2_4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn2_5">
<graphic xlink:href="599189v1_eqn2_5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<p><italic>ρ</italic><sub><italic>out</italic></sub> is the reward rate achieved from all the time spent outside the considered pursuit, <italic>in</italic>, which is also the reward rate achieved if the considered pursuit, <italic>in</italic>, is never pursued.</p>
</sec>
<sec id="s7">
<label>Ap 3.</label>
<title>Reformulation of global reward rate in terms of <italic>ρ</italic><sub><italic>out</italic></sub> and <italic>t</italic><sub><italic>out</italic></sub></title>
<disp-formula id="eqn3_1">
<graphic xlink:href="599189v1_eqn3_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn3_2">
<graphic xlink:href="599189v1_eqn3_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn2_4a">
<graphic xlink:href="599189v1_eqn2_4a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn3_3">
<graphic xlink:href="599189v1_eqn3_3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</sec>
<sec id="s8">
<label>Ap 4.</label>
<title>Global reward rate is a weighted average of an option’s reward rate and its outside reward rate</title>
<disp-formula id="eqn4_1">
<graphic xlink:href="599189v1_eqn4_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn4_2">
<graphic xlink:href="599189v1_eqn4_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</sec>
<sec id="s9">
<label>Ap 5.</label>
<title>Derivation of reward-rate maximizing forgo policies</title>
<p>Forgo the considered pursuit <italic>in</italic> if <italic>ρ</italic><sub>∀<italic>i</italic></sub> &lt; <italic>ρ</italic><sub>∀<italic>i</italic>≠<italic>in</italic></sub></p>
<p>
<disp-formula id="eqn5_1">
<graphic xlink:href="599189v1_eqn5_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_2">
<graphic xlink:href="599189v1_eqn5_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_3">
<graphic xlink:href="599189v1_eqn5_3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_4">
<graphic xlink:href="599189v1_eqn5_4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_5">
<graphic xlink:href="599189v1_eqn5_5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_6">
<graphic xlink:href="599189v1_eqn5_6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_7">
<graphic xlink:href="599189v1_eqn5_7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Choose considered pursuit <italic>in</italic> if <italic>ρ</italic><sub>∀<italic>i</italic></sub> &gt; <italic>ρ</italic><sub>∀<italic>i</italic>≠<italic>in</italic></sub></p>
<p>
<disp-formula id="eqn5_8">
<graphic xlink:href="599189v1_eqn5_8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_9">
<graphic xlink:href="599189v1_eqn5_9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_10">
<graphic xlink:href="599189v1_eqn5_10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_10a">
<graphic xlink:href="599189v1_eqn5_10a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_11">
<graphic xlink:href="599189v1_eqn5_11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_12">
<graphic xlink:href="599189v1_eqn5_12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_13">
<graphic xlink:href="599189v1_eqn5_13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Choosing and forgoing the considered option <italic>in</italic> are equivalent if
<disp-formula id="eqn5_14">
<graphic xlink:href="599189v1_eqn5_14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_15">
<graphic xlink:href="599189v1_eqn5_15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_16">
<graphic xlink:href="599189v1_eqn5_16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5_17">
<graphic xlink:href="599189v1_eqn5_17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s10">
<label>Ap 6.</label>
<title>Derivation of the equivalent immediate reward (i.e. the subjective value) for optimal global reward rate</title>
<p>Pursuit <italic>in</italic>1 and pursuit <italic>in</italic>2 produce the equivalent global reward rate if <inline-formula><inline-graphic xlink:href="599189v1_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula></p>
<p>By definition, if <italic>t</italic><sub><italic>in</italic>2</sub> = 0, pursuit <italic>in</italic>2 is an immediate reward. Finding <italic>r</italic><sub><italic>in</italic>2</sub> such that <inline-formula><inline-graphic xlink:href="599189v1_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> describes the equivalent immediate subjective value of pursuit <italic>in</italic>1.
<disp-formula id="eqn6_1">
<graphic xlink:href="599189v1_eqn6_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn6_2">
<graphic xlink:href="599189v1_eqn6_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Therefore, for a considered pursuit, <italic>in</italic>,…
<disp-formula id="eqn6_3">
<graphic xlink:href="599189v1_eqn6_3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s11">
<label>Ap 7.</label>
<title>Equivalent immediate subjective value need not be calculated from option-specific estimations of global reward rate</title>
<p>If <italic>sv</italic><sub><italic>in</italic>1</sub> &lt; <italic>sv</italic><sub><italic>in</italic>2</sub></p>
<p>
<disp-formula id="eqn7_1">
<graphic xlink:href="599189v1_eqn7_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7_2">
<graphic xlink:href="599189v1_eqn7_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>If <italic>sv</italic><sub><italic>in</italic>1</sub> = <italic>sv</italic><sub><italic>in</italic>2</sub></p>
<p>
<disp-formula id="eqn7_3">
<graphic xlink:href="599189v1_eqn7_3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7_4">
<graphic xlink:href="599189v1_eqn7_4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>If <italic>sv</italic><sub><italic>in</italic>1</sub> &gt; <italic>sv</italic><sub><italic>in</italic>2</sub>
<disp-formula id="eqn7_5">
<graphic xlink:href="599189v1_eqn7_5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7_6">
<graphic xlink:href="599189v1_eqn7_6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>If <italic>ρ</italic><sub><italic>in</italic></sub> &lt; <italic>ρ</italic><sub><italic>out</italic></sub>
<disp-formula id="eqn7_7">
<graphic xlink:href="599189v1_eqn7_7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7_8">
<graphic xlink:href="599189v1_eqn7_8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>If <italic>ρ</italic><sub><italic>in</italic></sub> = <italic>ρ</italic><sub><italic>out</italic></sub>
<disp-formula id="eqn7_9">
<graphic xlink:href="599189v1_eqn7_9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7_9a">
<graphic xlink:href="599189v1_eqn7_9a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7_10">
<graphic xlink:href="599189v1_eqn7_10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>If <italic>ρ</italic><sub><italic>in</italic></sub> &gt; <italic>ρ</italic><sub><italic>out</italic></sub>
<disp-formula id="eqn7_11">
<graphic xlink:href="599189v1_eqn7_11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7_12">
<graphic xlink:href="599189v1_eqn7_12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s12">
<label>Ap 8.</label>
<title>Reformulation of equivalent immediate subjective value in terms of outside parameters</title>
<disp-formula id="eqn8_1">
<graphic xlink:href="599189v1_eqn8_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</sec>
<sec id="s13">
<label>Ap 9.</label>
<title>Derivation of choice policies that optimize global reward rate</title>
<p>Let <italic>t</italic><sub><italic>in</italic>1</sub> &gt; <italic>t</italic><sub><italic>in</italic>2</sub></p>
<p>Choose option <italic>in</italic>1 if <italic>ρ</italic><sub><italic>in</italic>1+∀<italic>i</italic>≠<italic>in</italic>1,<italic>in</italic>2</sub> &gt; <italic>ρ</italic><sub><italic>in</italic>2<italic>j</italic>+∀<italic>i</italic>≠<italic>in</italic>1,<italic>in</italic>2</sub>
<disp-formula id="eqn9_1">
<graphic xlink:href="599189v1_eqn9_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9_2">
<graphic xlink:href="599189v1_eqn9_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><italic>f</italic><sub><italic>in</italic>1,<italic>in</italic>2</sub>: the frequency at which the choice between option <italic>in</italic>1 and <italic>in</italic>2 are presented.</p>
<p><italic>ρ</italic><sub><italic>out</italic></sub>: the reward rate earned outside of the <italic>in</italic>1 v. <italic>in</italic>2 choice</p>
<p><italic>t</italic><sub><italic>out</italic></sub>: the average time per choice spent outside of <italic>in</italic>1 or <italic>in</italic>2.
<disp-formula id="eqn9_3">
<graphic xlink:href="599189v1_eqn9_3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9_4">
<graphic xlink:href="599189v1_eqn9_4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
relationship between <italic>ρ</italic><sub><italic>in</italic>1+∀<italic>i</italic>≠<italic>in</italic>1,<italic>in</italic>2</sub>, <italic>ρ</italic><sub><italic>in</italic>1+∀<italic>i</italic>≠<italic>in</italic>1,<italic>in</italic>2</sub>
<disp-formula id="eqn9_5">
<graphic xlink:href="599189v1_eqn9_5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9_5a">
<graphic xlink:href="599189v1_eqn9_5a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn10_5">
<graphic xlink:href="599189v1_eqn10_5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9_6">
<graphic xlink:href="599189v1_eqn9_6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9_7">
<graphic xlink:href="599189v1_eqn9_7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><italic>ρ</italic><sub><italic>g</italic></sub> <sup>*</sup> : the maximum reward rate
<disp-formula id="eqn9_8">
<graphic xlink:href="599189v1_eqn9_8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Choose option <italic>in</italic>2 if <italic>ρ</italic><sub><italic>in</italic>1+∀<italic>i</italic>≠<italic>in</italic>1,<italic>in</italic>2</sub> &lt; <italic>ρ</italic><sub><italic>in</italic>2+∀<italic>i</italic>≠<italic>in</italic>1,<italic>in</italic>2</sub>
<disp-formula id="eqn9_4a">
<graphic xlink:href="599189v1_eqn9_4a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9_9">
<graphic xlink:href="599189v1_eqn9_9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Option <italic>in</italic>2 and option <italic>in</italic>1 are equivalent if <italic>ρ</italic><sub><italic>in</italic>1+∀<italic>i</italic>≠<italic>in</italic>1,<italic>in</italic>2</sub> = <italic>ρ</italic><sub><italic>in</italic>2+∀<italic>i</italic>≠<italic>in</italic>1,<italic>in</italic>2</sub>
<disp-formula id="eqn10_4">
<graphic xlink:href="599189v1_eqn10_4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9_10">
<graphic xlink:href="599189v1_eqn9_10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s14">
<label>Ap 10.</label>
<title>Equivalent immediate subjective value policies that optimize global reward rate</title>
<p>Choose option <italic>in</italic>1 over pursuit <italic>in</italic>2 if <italic>ρ</italic><sub><italic>g</italic></sub>(<italic>in</italic>1) &gt; <italic>ρ</italic><sub><italic>g</italic></sub>(<italic>in</italic>2)
<disp-formula id="eqn10_1">
<graphic xlink:href="599189v1_eqn10_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn10_2">
<graphic xlink:href="599189v1_eqn10_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Choose option <italic>in</italic>2 over option <italic>in</italic>1 if <italic>ρ</italic><sub><italic>g</italic></sub>(<italic>in</italic>2) &gt; <italic>ρ</italic><sub><italic>g</italic></sub>(<italic>in</italic>1)
<disp-formula id="eqn10_3">
<graphic xlink:href="599189v1_eqn10_3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Option <italic>in</italic>2 and option <italic>in</italic>1 are equivalent if <italic>ρ</italic><sub><italic>g</italic></sub>(<italic>in</italic>1) = <italic>ρ</italic><sub><italic>g</italic></sub>(<italic>in</italic>2)
<disp-formula id="eqn10_4a">
<graphic xlink:href="599189v1_eqn10_4a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s15">
<label>Ap 11.</label>
<title>Definitions for misestimating global reward rate-enabling parameters</title>
<p>Each misestimated variable (column 1) is multiplied by an error term, ω, to give <inline-formula><inline-graphic xlink:href="599189v1_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the misestimated global reward rate (column 2). When ω = (0,1) the variable is underestimated, when ω = (1,2) the variable is overestimated, and when ω = 1 the variable is correctly estimated and <inline-formula><inline-graphic xlink:href="599189v1_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<table-wrap id="utbl1" orientation="portrait" position="float">
<graphic xlink:href="599189v1_utbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s16">
<label>Ap 12.</label>
<title>Conditions wherein overestimation of global reward rate leads to suboptimal choice behavior</title>
<p>If <italic>t</italic><sub><italic>LL</italic></sub> &gt; <italic>t</italic><sub><italic>ss</italic></sub> and <inline-formula><inline-graphic xlink:href="599189v1_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula> <italic>r</italic><sub><italic>LL</italic></sub> &gt; <italic>r</italic><sub><italic>ss</italic></sub>
<disp-formula id="eqn12_1">
<graphic xlink:href="599189v1_eqn12_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn12_2">
<graphic xlink:href="599189v1_eqn12_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn12_3">
<graphic xlink:href="599189v1_eqn12_3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn12_4">
<graphic xlink:href="599189v1_eqn12_4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>pursuit LL is optimal if <inline-formula><inline-graphic xlink:href="599189v1_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <italic>sv</italic><sub><italic>LL</italic></sub> &gt; <italic>sv</italic><sub><italic>SS</italic></sub></p>
<p>Policy from global reward rate overestimation</p>
<p><inline-formula><inline-graphic xlink:href="599189v1_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the animal will choose pursuit LL
<disp-formula id="ueqn2">
<graphic xlink:href="599189v1_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="ueqn2a">
<graphic xlink:href="599189v1_ueqn2a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><inline-formula><inline-graphic xlink:href="599189v1_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the animal will choose pursuit SS
<disp-formula id="ueqn3">
<graphic xlink:href="599189v1_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>pursuit <italic>LL</italic> is optimal if <inline-formula><inline-graphic xlink:href="599189v1_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and pursuit <italic>LL</italic> is chosen if <inline-formula><inline-graphic xlink:href="599189v1_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula></p>
<p>pursuit <italic>LL</italic> is optimal if <inline-formula><inline-graphic xlink:href="599189v1_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> but pursuit SS is chosen if <inline-formula><inline-graphic xlink:href="599189v1_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula></p>
<p>The policy from overestimation is suboptimal if <inline-formula><inline-graphic xlink:href="599189v1_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula></p>
<p>The policy from overestimation is suboptimal if <inline-formula><inline-graphic xlink:href="599189v1_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula> but <italic>sv</italic><sub><italic>LL</italic></sub> &gt; <italic>sv</italic><sub><italic>SS</italic></sub>
<disp-formula id="ueqn4">
<graphic xlink:href="599189v1_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn12_5">
<graphic xlink:href="599189v1_eqn12_5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s17">
<label>Ap 13.</label>
<title>Situations in which the rewarding option does not exclude the animal from receiving outside reward</title>
<disp-formula id="ueqn5">
<graphic xlink:href="599189v1_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</sec>
</app>
</app-group>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ainslie</surname> <given-names>G</given-names></string-name></person-group> (<year>1975</year>) <article-title>Specious reward: A behavioral theory of impulsiveness and impulse control</article-title>. <source>Psychol Bull</source> <volume>59</volume>:<fpage>257</fpage>–<lpage>272</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ainslie</surname> <given-names>GW</given-names></string-name></person-group> (<year>1974</year>) <article-title>Impulse control in pigeons</article-title>. <source>J Exp Anal Behav</source> <volume>21</volume>:<fpage>485</fpage>–<lpage>489</lpage> <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Citation&amp;list_uids=16811760">http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Citation&amp;list_uids=16811760</ext-link>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>al-Nowaihi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Dhami</surname> <given-names>S</given-names></string-name></person-group> (<year>2008</year>) <article-title>A general theory of time discounting : The reference-time theory of intertemporal choice</article-title>. <source>Discussion Papers in Economics</source></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baker</surname> <given-names>F</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Bickel</surname> <given-names>WK</given-names></string-name></person-group> (<year>2003</year>) <article-title>Delay discounting in current and never-before cigarette smokers: similarities and differences across commodity, sign, and magnitude</article-title>. <source>J Abnorm Psychol</source> <volume>112</volume>:<fpage>382</fpage>–<lpage>392</lpage> <pub-id pub-id-type="doi">10.1037/0021-843x.112.3.382</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bateson</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kacelnik</surname> <given-names>A</given-names></string-name></person-group> (<year>1996</year>) <article-title>Rate currencies and the foraging starling: the fallacy of the averages revisited</article-title>. <source>Behav Ecol</source> <volume>7</volume>:<fpage>341</fpage>–<lpage>352</lpage> <pub-id pub-id-type="doi">10.1093/beheco/7.3.341</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="thesis"><person-group person-group-type="author"><string-name><surname>Bennett</surname> <given-names>SM</given-names></string-name></person-group> (<year>2002</year>) <article-title>Preference reversal and the estimation of indifference points using a fast-adjusting delay procedure with rats</article-title>. <publisher-name>University of Florida</publisher-name></mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benzion</surname> <given-names>U</given-names></string-name>, <string-name><surname>Rapoport</surname> <given-names>A</given-names></string-name>, <string-name><surname>Yagil</surname> <given-names>J</given-names></string-name>, <collab>Science, Source Management</collab>, <string-name><surname>Mar</surname> <given-names>N</given-names></string-name></person-group> (<year>1989</year>) <article-title>Discount Rates Inferred from Decisions : An Experimental Study</article-title>. <source>Manage Sci</source> <volume>35</volume>:<fpage>270</fpage>–<lpage>284</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beran</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Evans</surname> <given-names>TA</given-names></string-name></person-group> (<year>2009</year>) <article-title>Delay of gratification by chimpanzees (Pan troglodytes) in working and waiting situations</article-title>. <source>Behav Processes</source> <volume>80</volume>:<fpage>177</fpage>–<lpage>181</lpage> <pub-id pub-id-type="doi">10.1016/j.beproc.2008.11.008</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berns</surname> <given-names>GS</given-names></string-name>, <string-name><surname>Laibson</surname> <given-names>D</given-names></string-name>, <string-name><surname>Loewenstein</surname> <given-names>G</given-names></string-name></person-group> (<year>2007</year>) <article-title>Intertemporal choice--toward an integrative framework</article-title>. <source>Trends Cogn Sci</source> <volume>11</volume>:<fpage>482</fpage>–<lpage>488</lpage> <pub-id pub-id-type="doi">10.1016/j.tics.2007.08.011</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bickel</surname> <given-names>WK</given-names></string-name>, <string-name><surname>Jarmolowicz</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Mueller</surname> <given-names>ET</given-names></string-name>, <string-name><surname>Koffarnus</surname> <given-names>MN</given-names></string-name>, <string-name><surname>Gatchalian</surname> <given-names>KM</given-names></string-name></person-group> (<year>2012</year>) <article-title>Excessive discounting of delayed reinforcers as a trans-disease process contributing to addiction and other disease-related vulnerabilities: emerging evidence</article-title>. <source>Pharmacol Ther</source> <volume>134</volume>:<fpage>287</fpage>–<lpage>297</lpage> <pub-id pub-id-type="doi">10.1016/j.pharmthera.2012.02.004</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bickel</surname> <given-names>WK</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Yi</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kowal</surname> <given-names>BP</given-names></string-name>, <string-name><surname>Diana</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pitcock</surname> <given-names>JA</given-names></string-name></person-group> (<year>2007</year>) <article-title>Behavioral and Neuroeconomics of Drug Addiction: Competing Neural Systems and Temporal Discounting Processes</article-title>. <source>Drug Alcohol Depend</source> <volume>90</volume>:<fpage>S85</fpage>–<lpage>S91</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blanchard</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name></person-group> (<year>2014</year>) <article-title>Neurons in dorsal anterior cingulate cortex signal postdecisional variables in a foraging task</article-title>. <source>J Neurosci</source> <volume>34</volume>:<fpage>646</fpage>–<lpage>655</lpage> <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3151-13.2014</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blanchard</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name></person-group> (<year>2015</year>) <article-title>Monkeys are more patient in a foraging task than in a standard intertemporal choice task</article-title>. <source>PLoS One</source> <volume>10</volume>:<fpage>e0117057</fpage> <pub-id pub-id-type="doi">10.1371/journal.pone.0117057</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blanchard</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Pearson</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name></person-group> (<year>2013</year>) <article-title>Postreward delays and systematic biases in measures of animal temporal discounting</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>110</volume>:<fpage>15491</fpage>–<lpage>15496</lpage> <pub-id pub-id-type="doi">10.1073/pnas.1310446110</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bretteville-Jensen</surname> <given-names>AL</given-names></string-name></person-group> (<year>1999</year>) <article-title>Addiction and discounting</article-title>. <source>J Health Econ</source> <volume>18</volume>:<fpage>393</fpage>–<lpage>407</lpage> <pub-id pub-id-type="doi">10.1016/s0167-6296(98)00057-5</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calhoun</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name></person-group> (<year>2015</year>) <article-title>The foraging brain</article-title>. <source>Current Opinion in Behavioral Sciences</source> <volume>5</volume>:<fpage>24</fpage>–<lpage>31</lpage> <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S235215461500090X">https://www.sciencedirect.com/science/article/pii/S235215461500090X</ext-link>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calvert</surname> <given-names>AL</given-names></string-name>, <string-name><surname>Green</surname> <given-names>L</given-names></string-name>, <string-name><surname>Myerson</surname> <given-names>J</given-names></string-name></person-group> (<year>2010</year>) <article-title>Delay discounting of qualitatively different reinforcers in rats</article-title>. <source>J Exp Anal Behav</source> <volume>93</volume>:<fpage>171</fpage>–<lpage>184</lpage> <pub-id pub-id-type="doi">10.1901/jeab.2010.93-171</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cardinal</surname> <given-names>RN</given-names></string-name>, <string-name><surname>Pennicott</surname> <given-names>DR</given-names></string-name>, <string-name><surname>Sugathapala</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Robbins</surname> <given-names>TW</given-names></string-name>, <string-name><surname>Everitt</surname> <given-names>BJ</given-names></string-name></person-group> (<year>2001</year>) <article-title>Impulsive choice induced in rats by lesions of the nucleus accumbens core</article-title>. <source>Science</source> <volume>292</volume>:<fpage>2499</fpage>–<lpage>2501</lpage> <pub-id pub-id-type="doi">10.1126/science.1060818</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carter</surname> <given-names>EC</given-names></string-name>, <string-name><surname>Pedersen</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>McCullough</surname> <given-names>ME</given-names></string-name></person-group> (<year>2015</year>) <article-title>Reassessing intertemporal choice: human decision-making is more optimal in a foraging task than in a self-control task</article-title>. <source>Front Psychol</source> <volume>6</volume>:<fpage>95</fpage> <pub-id pub-id-type="doi">10.3389/fpsyg.2015.00095</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carter</surname> <given-names>EC</given-names></string-name>, <string-name><surname>Redish</surname> <given-names>AD</given-names></string-name></person-group> (<year>2016</year>) <article-title>Rats value time differently on equivalent foraging and delay-discounting tasks</article-title>. <source>J Exp Psychol Gen</source> <volume>145</volume>:<fpage>1093</fpage>–<lpage>1101</lpage> <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/27359127">https://www.ncbi.nlm.nih.gov/pubmed/27359127</ext-link>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Charnov</surname> <given-names>E</given-names></string-name>, <string-name><surname>Orians</surname> <given-names>GH</given-names></string-name></person-group> (<year>1973</year>) <source>Optimal Foraging: Some Theoretical Explorations</source>. <ext-link ext-link-type="uri" xlink:href="https://digitalrepository.unm.edu/biol_fsp/45/?sequence">https://digitalrepository.unm.edu/biol_fsp/45/?sequence</ext-link> x[Accessed <date-in-citation content-type="access-date">July 20, 2022</date-in-citation>].</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Charnov</surname> <given-names>EL</given-names></string-name></person-group> (<year>1976a</year>) <article-title>Optimal Foraging: Attack Strategy of a Mantid</article-title>. <source>Am Nat</source> <volume>110</volume>:<fpage>141</fpage>–<lpage>151</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Charnov</surname> <given-names>EL</given-names></string-name></person-group> (<year>1976b</year>) <article-title>Optimal Foraging, the Marginal Value Theorem</article-title>. <source>Theor Popul Biol</source> <volume>9</volume>:<fpage>129</fpage>– <lpage>136</lpage> <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/1273796">https://www.ncbi.nlm.nih.gov/pubmed/1273796</ext-link>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheng</surname> <given-names>K</given-names></string-name>, <string-name><surname>Peña</surname> <given-names>J</given-names></string-name>, <string-name><surname>Porter</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Irwin</surname> <given-names>JD</given-names></string-name></person-group> (<year>2002</year>) <article-title>Self-control in honeybees</article-title>. <source>Psychon Bull Rev</source> <volume>9</volume>:<fpage>259</fpage>–<lpage>263</lpage> <pub-id pub-id-type="doi">10.3758/bf03196280</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chung</surname> <given-names>S-H</given-names></string-name>, <string-name><surname>Herrnstein</surname> <given-names>RJ</given-names></string-name></person-group> (<year>1967</year>) <article-title>CHOICE AND DELAY OF REINFORCEMENT</article-title>. <source>J Exp Anal Behav</source> <volume>10</volume>:<fpage>67</fpage>–<lpage>74</lpage> <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Citation&amp;list_uids=16811307">http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Citation&amp;list_uids=16811307</ext-link>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Critchfield</surname> <given-names>TS</given-names></string-name>, <string-name><surname>Kollins</surname> <given-names>SH</given-names></string-name></person-group> (<year>2001</year>) <article-title>Temporal discounting: basic research and the analysis of socially important behavior</article-title>. <source>J Appl Behav Anal</source> <volume>34</volume>:<fpage>101</fpage>–<lpage>122</lpage> <pub-id pub-id-type="doi">10.1901/jaba.2001.34-101</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Estle</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Green</surname> <given-names>L</given-names></string-name>, <string-name><surname>Myerson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Holt</surname> <given-names>DD</given-names></string-name></person-group> (<year>2006</year>) <article-title>Differential effects of amount on temporal and probability discounting of gains and losses</article-title>. <source>Mem Cognit</source> <volume>34</volume>:<fpage>914</fpage>–<lpage>928</lpage> <pub-id pub-id-type="doi">10.3758/bf03193437</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Fedus</surname> <given-names>W</given-names></string-name>, <string-name><surname>Gelada</surname> <given-names>C</given-names></string-name>, <string-name><surname>Bengio</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Bellemare</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Larochelle</surname> <given-names>H</given-names></string-name></person-group> (<year>2019</year>) <article-title>Hyperbolic Discounting and Learning over Multiple Horizons</article-title>. <source>arXiv [statML]</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1902.06865">http://arxiv.org/abs/1902.06865</ext-link>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frederick</surname> <given-names>S</given-names></string-name>, <string-name><surname>Loewenstein</surname> <given-names>G</given-names></string-name>, <string-name><surname>Donoghue</surname> <given-names>TO</given-names></string-name>, <string-name><surname>Donoghue</surname> <given-names>TEDO</given-names></string-name></person-group> (<year>2002</year>) <article-title>Time Discounting and Time Preference : A Critical Review</article-title>. <source>J Econ Lit</source> <volume>40</volume>:<fpage>351</fpage>–<lpage>401</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fung</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Sutlief</surname> <given-names>E</given-names></string-name>, <string-name><surname>Hussain Shuler</surname> <given-names>MG</given-names></string-name></person-group> (<year>2021</year>) <article-title>Dopamine and the interdependency of time perception and reward</article-title>. <source>Neurosci Biobehav Rev</source> <volume>125</volume>:<fpage>380</fpage>–<lpage>391</lpage> <pub-id pub-id-type="doi">10.1016/j.neubiorev.2021.02.030</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glimcher</surname> <given-names>PW</given-names></string-name>, <string-name><surname>Kable</surname> <given-names>J</given-names></string-name>, <string-name><surname>Louie</surname> <given-names>K</given-names></string-name></person-group> (<year>2007</year>) <article-title>Neuroeconomic Studies of Impulsivity: Now or Just as Soon as Possible?</article-title> <source>Am Econ Rev</source> <volume>97</volume>:<fpage>142</fpage>–<lpage>147</lpage> <pub-id pub-id-type="doi">10.1257/aer.97.2.142</pub-id> [Accessed <date-in-citation content-type="access-date">July 21, 2022</date-in-citation>].</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grace</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Sargisson</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>White</surname> <given-names>KG</given-names></string-name></person-group> (<year>2012</year>) <article-title>Evidence for a magnitude effect in temporal discounting with pigeons</article-title>. <source>J Exp Psychol Anim Behav Process</source> <volume>38</volume>:<fpage>102</fpage>–<lpage>108</lpage> <pub-id pub-id-type="doi">10.1037/a0026345</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Green</surname> <given-names>L</given-names></string-name>, <string-name><surname>Fristoe</surname> <given-names>N</given-names></string-name>, <string-name><surname>Myerson</surname> <given-names>J</given-names></string-name></person-group> (<year>1994</year>) <article-title>Temporal discounting and preference reversals in choice between delayed outcomes</article-title>. <source>Psychon Bull Rev</source> <volume>1</volume>:<fpage>383</fpage>–<lpage>389</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Green</surname> <given-names>L</given-names></string-name>, <string-name><surname>Myerson</surname> <given-names>J</given-names></string-name></person-group> (<year>2004</year>) <article-title>A discounting framework for choice with delayed and probabilistic rewards</article-title>. <source>Psychol Bull</source> <volume>130</volume>:<fpage>769</fpage>–<lpage>792</lpage> <pub-id pub-id-type="doi">10.1037/0033-2909.130.5.769</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Green</surname> <given-names>L</given-names></string-name>, <string-name><surname>Myerson</surname> <given-names>J</given-names></string-name>, <string-name><surname>McFadden</surname> <given-names>E</given-names></string-name></person-group> (<year>1997</year>) <article-title>Rate of temporal discounting decreases with amount of reward</article-title>. <source>Mem Cognit</source> <volume>25</volume>:<fpage>715</fpage>–<lpage>723</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grossbard</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Mazur</surname> <given-names>JE</given-names></string-name></person-group> (<year>1986</year>) <article-title>A comparison of delays and ration requirements in self-control choice</article-title>. <source>J Exp Anal Behav</source> <volume>45</volume>:<fpage>305</fpage>–<lpage>315</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haith</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Reppert</surname> <given-names>TR</given-names></string-name>, <string-name><surname>Shadmehr</surname> <given-names>R</given-names></string-name></person-group> (<year>2012</year>) <article-title>Evidence for hyperbolic temporal discounting of reward in control of movements</article-title>. <source>J Neurosci</source> <volume>32</volume>:<fpage>11727</fpage>–<lpage>11736</lpage> <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0424-12.2012</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hariri</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Brown</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Williamson</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Flory</surname> <given-names>JD</given-names></string-name>, <string-name><surname>de Wit</surname> <given-names>H</given-names></string-name>, <string-name><surname>Manuck</surname> <given-names>SB</given-names></string-name></person-group> (<year>2006</year>) <article-title>Preference for immediate over delayed rewards is associated with magnitude of ventral striatal activity</article-title>. <source>J Neurosci</source> <volume>26</volume>:<fpage>13213</fpage>–<lpage>13217</lpage> <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3446-06.2006</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name></person-group> (<year>2016</year>) <article-title>Time discounting and time preference in animals: A critical review</article-title>. <source>Psychon Bull Rev</source> <volume>23</volume>:<fpage>39</fpage>–<lpage>53</lpage> <pub-id pub-id-type="doi">10.3758/s13423-015-0879-3</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name>, <string-name><surname>Parikh</surname> <given-names>PC</given-names></string-name>, <string-name><surname>Deaner</surname> <given-names>RO</given-names></string-name>, <string-name><surname>Platt</surname> <given-names>ML</given-names></string-name></person-group> (<year>2007</year>) <article-title>Economic principles motivating social attention in humans</article-title>. <source>Proc Biol Sci</source> <volume>274</volume>:<fpage>1751</fpage>–<lpage>1756</lpage> <pub-id pub-id-type="doi">10.1098/rspb.2007.0368</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name>, <string-name><surname>Pearson</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Platt</surname> <given-names>ML</given-names></string-name></person-group> (<year>2011</year>) <article-title>Neuronal basis of sequential foraging decisions in a patchy environment</article-title>. <source>Nat Neurosci</source> <volume>14</volume>:<fpage>933</fpage>–<lpage>939</lpage> <pub-id pub-id-type="doi">10.1038/nn.2856</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name>, <string-name><surname>Platt</surname> <given-names>ML</given-names></string-name></person-group> (<year>2007</year>) <article-title>Temporal discounting predicts risk sensitivity in rhesus macaques</article-title>. <source>Curr Biol</source> <volume>17</volume>:<fpage>49</fpage>–<lpage>53</lpage> <pub-id pub-id-type="doi">10.1016/j.cub.2006.10.055</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holt</surname> <given-names>DD</given-names></string-name>, <string-name><surname>Green</surname> <given-names>L</given-names></string-name>, <string-name><surname>Myerson</surname> <given-names>J</given-names></string-name></person-group> (<year>2003</year>) <article-title>Is discounting impulsive?</article-title> <source>Behav Processes</source> <volume>64</volume>:<fpage>355</fpage>–<lpage>367</lpage> <pub-id pub-id-type="doi">10.1016/S0376-6357(03)00141-4</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hwang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>D</given-names></string-name></person-group> (<year>2009</year>) <article-title>Temporal discounting and inter-temporal choice in rhesus monkeys</article-title>. <source>Front Behav Neurosci</source> <volume>3</volume>:<fpage>9</fpage> <pub-id pub-id-type="doi">10.3389/neuro.08.009.2009</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ito</surname> <given-names>M</given-names></string-name>, <string-name><surname>Asaki</surname> <given-names>K</given-names></string-name></person-group> (<year>1982</year>) <article-title>CHOICE BEHAVIOR OF RATS IN A CONCURRENT-CHAINS SCHEDULE: AMOUNT AND DELAY OF REINFORCEMENT</article-title>. <source>J Exp Anal Behav</source> <volume>37</volume>:<fpage>383</fpage>– <lpage>392</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kacelnik</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bateson</surname> <given-names>M</given-names></string-name></person-group> (<year>1996</year>) <article-title>Risky Theories—The Effects of Variance on Foraging Decisions</article-title>. <source>Integr Comp Biol</source> <volume>36</volume>:<fpage>402</fpage>–<lpage>434</lpage> <pub-id pub-id-type="doi">10.1093/icb/36.4.402</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kalenscher</surname> <given-names>T</given-names></string-name>, <string-name><surname>Pennartz</surname> <given-names>CMA</given-names></string-name></person-group> (<year>2008</year>) <article-title>Is a bird in the hand worth two in the future? The neuroeconomics of intertemporal decision-making</article-title>. <source>Prog Neurobiol</source> <volume>84</volume>:<fpage>284</fpage>–<lpage>315</lpage> <pub-id pub-id-type="doi">10.1016/j.pneurobio.2007.11.004</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kalenscher</surname> <given-names>T</given-names></string-name>, <string-name><surname>Windmann</surname> <given-names>S</given-names></string-name>, <string-name><surname>Diekamp</surname> <given-names>B</given-names></string-name>, <string-name><surname>Rose</surname> <given-names>J</given-names></string-name>, <string-name><surname>Güntürkün</surname> <given-names>O</given-names></string-name>, <string-name><surname>Colombo</surname> <given-names>M</given-names></string-name></person-group> (<year>2005</year>) <article-title>Single units in the pigeon brain integrate reward amount and time-to-reward in an impulsive choice task</article-title>. <source>Curr Biol</source> <volume>15</volume>:<fpage>594</fpage>–<lpage>602</lpage> <pub-id pub-id-type="doi">10.1016/j.cub.2005.02.052</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kane</surname> <given-names>GA</given-names></string-name>, <string-name><surname>Bornstein</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Shenhav</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>JD</given-names></string-name></person-group> (<year>2019</year>) <article-title>Rats exhibit similar biases in foraging and intertemporal choice tasks</article-title>. <source>Elife</source> <volume>8</volume> <pub-id pub-id-type="doi">10.7554/eLife.48429</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Killeen</surname> <given-names>PR</given-names></string-name></person-group> (<year>2009</year>) <article-title>An additive-utility model of delay discounting</article-title>. <source>Psychol Rev</source> <volume>116</volume>:<fpage>602</fpage>–<lpage>619</lpage> <pub-id pub-id-type="doi">10.1037/a0016414</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hwang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>D</given-names></string-name></person-group> (<year>2008</year>) <article-title>Prefrontal coding of temporally discounted values during intertemporal choice</article-title>. <source>Neuron</source> <volume>59</volume>:<fpage>161</fpage>–<lpage>172</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2008.05.010</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kinloch</surname> <given-names>JM</given-names></string-name>, <string-name><surname>White</surname> <given-names>KG</given-names></string-name></person-group> (<year>2013</year>) <article-title>A concurrent-choice analysis of amount-dependent temporal discounting</article-title>. <source>Behav Processes</source> <volume>97</volume>:<fpage>1</fpage>–<lpage>5</lpage> <pub-id pub-id-type="doi">10.1016/j.beproc.2013.03.007</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kobayashi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schultz</surname> <given-names>W</given-names></string-name></person-group> (<year>2008</year>) <article-title>Influence of reward delays on responses of dopamine neurons</article-title>. <source>J Neurosci</source> <volume>28</volume>:<fpage>7837</fpage>–<lpage>7846</lpage> <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1600-08.2008</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koopmans</surname> <given-names>TC</given-names></string-name></person-group> (<year>1960</year>) <article-title>Stationary Ordinal Utility and Impatience</article-title>. <source>Econometrica</source> <volume>28</volume>:<fpage>287</fpage>–<lpage>309</lpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krebs</surname> <given-names>BYJR</given-names></string-name>, <string-name><surname>Erichsen</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Webber</surname> <given-names>MI</given-names></string-name></person-group> (<year>1977</year>) <article-title>OPTIMAL PREY SELECTION IN THE GREAT TIT (PARUS MAJOR)</article-title>. <source>Anim Behav</source> <volume>25</volume>:<fpage>30</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laibson</surname> <given-names>D</given-names></string-name></person-group> (<year>1997</year>) <article-title>Golden eggs and hyperbolic discounting</article-title>. <source>Q J Econ</source> <volume>112</volume>:<fpage>443</fpage>–<lpage>477</lpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lea</surname> <given-names>SEG</given-names></string-name></person-group> (<year>1979</year>) <article-title>Foraging and reinforcement schedules in the pigeon: Optimal and non-optimal aspects of choice</article-title>. <source>Anim Behav</source> <volume>27</volume>:<fpage>875</fpage>–<lpage>886</lpage> <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/0003347279900253">https://www.sciencedirect.com/science/article/pii/0003347279900253</ext-link>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Loewenstein</surname> <given-names>G</given-names></string-name>, <string-name><surname>Thaler</surname> <given-names>RH</given-names></string-name></person-group> (<year>1989</year>) <article-title>Anomalies: Intertemporal Choice</article-title>. <source>J Econ Perspect</source> <volume>3</volume>:<fpage>181</fpage>–<lpage>193</lpage> <ext-link ext-link-type="uri" xlink:href="https://www.aeaweb.org/articles?id=10.1257/jep.3.4.181">https://www.aeaweb.org/articles?id=10.1257/jep.3.4.181</ext-link> [Accessed <date-in-citation content-type="access-date">March 13, 2024</date-in-citation>].</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Loewenstein</surname>, <given-names>Prelec</given-names></string-name></person-group> (<year>1992</year>) <article-title>Anomalies in intertemporal choice: Evidence and an interpretation</article-title>. <source>Q J Econ</source> <ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/qje/article-abstract/107/2/573/1838331">https://academic.oup.com/qje/article-abstract/107/2/573/1838331</ext-link>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Logue</surname> <given-names>AW</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>ME</given-names></string-name>, <string-name><surname>Rachlin</surname> <given-names>H</given-names></string-name></person-group> (<year>1985</year>) <article-title>Sensitivity of pigeons to prereinforcer and postreinforcer delay</article-title>. <source>Anim Learn Behav</source> <volume>13</volume>:<fpage>181</fpage>–<lpage>186</lpage> <pub-id pub-id-type="doi">10.3758/bf03199271</pub-id>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Louie</surname> <given-names>K</given-names></string-name>, <string-name><surname>Glimcher</surname> <given-names>PW</given-names></string-name></person-group> (<year>2010</year>) <article-title>Separating value from choice: delay discounting activity in the lateral intraparietal area</article-title>. <source>J Neurosci</source> <volume>30</volume>:<fpage>5498</fpage>–<lpage>5507</lpage> <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5742-09.2010</pub-id>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Madden</surname> <given-names>GF</given-names></string-name>, <string-name><surname>Bickel</surname> <given-names>WK</given-names></string-name></person-group> eds. (<year>2010</year>) <source>Impulsivity: The behavioral and neurological science of discounting</source>. <publisher-name>American Psychological Association</publisher-name></mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mazur</surname> <given-names>JE</given-names></string-name></person-group> (<year>1987</year>) <chapter-title>An adjusting procedure for studying delayed reinforcement. In: The effect of delay and of intervening events on reinforcement value</chapter-title>., pp <fpage>55</fpage>–<lpage>73</lpage> <source>Quantitative analyses of behavior</source>, Vol. <volume>5</volume>. <publisher-loc>Hillsdale, NJ, US</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates, Inc</publisher-name>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mazur</surname> <given-names>JE</given-names></string-name></person-group> (<year>1988</year>) <article-title>Estimation of indifference points with an adjusting-delay procedure</article-title>. <source>J Exp Anal Behav</source> <volume>49</volume>:<fpage>37</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mazur</surname> <given-names>JE</given-names></string-name></person-group> (<year>2006</year>) <article-title>Mathematical models and the experimental analysis of behavior</article-title>. <source>J Exp Anal Behav</source> <volume>85</volume>:<fpage>275</fpage>–<lpage>291</lpage> <pub-id pub-id-type="doi">10.1901/jeab.2006.65-05</pub-id>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mazur</surname> <given-names>JE</given-names></string-name>, <string-name><surname>Snyderman</surname> <given-names>M</given-names></string-name>, <string-name><surname>Coe</surname> <given-names>D</given-names></string-name></person-group> (<year>1985</year>) <article-title>Influences of delay and rate of reinforcement on discrete-trial choice</article-title>. <source>J Exp Psychol Anim Behav Process</source> <volume>11</volume>:<fpage>565</fpage>–<lpage>575</lpage> <pub-id pub-id-type="doi">10.1037//0097-7403.11.4.565</pub-id>.</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McClure</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Ericson</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Laibson</surname> <given-names>DI</given-names></string-name>, <string-name><surname>Loewenstein</surname> <given-names>G</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>JD</given-names></string-name></person-group> (<year>2007</year>) <article-title>Time discounting for primary rewards</article-title>. <source>J Neurosci</source> <volume>27</volume>:<fpage>5796</fpage>–<lpage>5804</lpage> <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4246-06.2007</pub-id>.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McClure</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Laibson</surname> <given-names>DI</given-names></string-name>, <string-name><surname>Loewenstein</surname> <given-names>G</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>JD</given-names></string-name></person-group> (<year>2004</year>) <article-title>Separate neural systems value immediate and delayed monetary rewards</article-title>. <source>Science</source> <volume>306</volume>:<fpage>503</fpage>–<lpage>507</lpage> <pub-id pub-id-type="doi">10.1126/science.1100907</pub-id>.</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDiarmid</surname> <given-names>CG</given-names></string-name>, <string-name><surname>Rilling</surname> <given-names>ME</given-names></string-name></person-group> (<year>1965</year>) <article-title>Reinforcement delay and reinforcement rate as determinants of schedule preference</article-title>. <source>Psychon Sci</source> <volume>2</volume>:<fpage>195</fpage>–<lpage>196</lpage> <pub-id pub-id-type="doi">10.3758/BF03343402</pub-id>.</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mischel</surname> <given-names>W</given-names></string-name>, <string-name><surname>Grusec</surname> <given-names>J</given-names></string-name>, <string-name><surname>Masters</surname> <given-names>JC</given-names></string-name></person-group> (<year>1969</year>) <article-title>Effects of Expected Delay Time on Subjective Value of Rewards and Punishments</article-title>. <source>J Pers Soc Psychol</source> <volume>11</volume>:<fpage>363</fpage>–<lpage>373</lpage>.</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Montague</surname> <given-names>PR</given-names></string-name>, <string-name><surname>Berns</surname> <given-names>GS</given-names></string-name></person-group> (<year>2002</year>) <article-title>Neural economics and the biological substrates of valuation</article-title>. <source>Neuron</source> <volume>36</volume>:<fpage>265</fpage>–<lpage>284</lpage> <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Citation&amp;list_uids=12383781">http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Citation&amp;list_uids=12383781</ext-link>.</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Montague</surname> <given-names>PR</given-names></string-name>, <string-name><surname>King-Casas</surname> <given-names>B</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>JD</given-names></string-name></person-group> (<year>2006</year>) <article-title>Imaging valuation models in human choice</article-title>. <source>Annu Rev Neurosci</source> <volume>29</volume>:<fpage>417</fpage>–<lpage>448</lpage> <pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.112903</pub-id>.</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Monterosso</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ainslie</surname> <given-names>G</given-names></string-name></person-group> (<year>1999</year>) <article-title>Beyond discounting: possible experimental models of impulse control</article-title>. <source>Psychopharmacology</source> <volume>146</volume>:<fpage>339</fpage>–<lpage>347</lpage> <pub-id pub-id-type="doi">10.1007/pl00005480</pub-id>.</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Myerson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Green</surname> <given-names>L</given-names></string-name></person-group> (<year>1995</year>) <article-title>Discounting of delayed rewards: models of individual choice</article-title>. <source>J Exp Anal Behav</source> <volume>64</volume>:<fpage>263</fpage>–<lpage>276</lpage>.</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nakahara</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kaveri</surname> <given-names>S</given-names></string-name></person-group> (<year>2010</year>) <article-title>Internal-time temporal difference model for neural value-based decision making</article-title>. <source>Neural Comput</source> <volume>22</volume>:<fpage>3062</fpage>–<lpage>3106</lpage> <pub-id pub-id-type="doi">10.1162/NECO_a_00049</pub-id>.</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Namboodiri</surname> <given-names>VM</given-names></string-name>, <string-name><surname>Hussain Shuler</surname> <given-names>MG</given-names></string-name></person-group> (<year>2016</year>) <article-title>The hunt for the perfect discounting function and a reckoning of time perception</article-title>. <source>Curr Opin Neurobiol</source> <volume>40</volume>:<fpage>135</fpage>–<lpage>141</lpage> <pub-id pub-id-type="doi">10.1016/j.conb.2016.06.019</pub-id>.</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Namboodiri</surname> <given-names>VMK</given-names></string-name>, <string-name><surname>Mihalas</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hussain Shuler</surname> <given-names>MG</given-names></string-name></person-group> (<year>2014a</year>) <article-title>Rationalizing decision-making: understanding the cost and perception of time</article-title>. <source>Timing and Time Perception Reviews</source> <volume>1</volume>:<fpage>1</fpage>–<lpage>40</lpage> <ext-link ext-link-type="uri" xlink:href="https://ugp.rug.nl/ttpr/article/view/15503">https://ugp.rug.nl/ttpr/article/view/15503</ext-link>.</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Namboodiri</surname> <given-names>VMK</given-names></string-name>, <string-name><surname>Mihalas</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hussain Shuler</surname> <given-names>MG</given-names></string-name></person-group> (<year>2014b</year>) <article-title>A temporal basis for the origin of Weber’s law in value perception</article-title>. <source>Front Integr Neurosci</source> <volume>8</volume>:<fpage>1</fpage>–<lpage>11</lpage> <pub-id pub-id-type="doi">10.3389/fnint.2014.00079</pub-id>.</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Namboodiri</surname> <given-names>VMK</given-names></string-name>, <string-name><surname>Mihalas</surname> <given-names>S</given-names></string-name>, <string-name><surname>Marton</surname> <given-names>T</given-names></string-name>, <string-name><surname>Hussain Shuler</surname> <given-names>MG</given-names></string-name></person-group> (<year>2014c</year>) <article-title>A general theory of intertemporal decision-making and the perception of time</article-title>. <source>Front Behav Neurosci</source> <volume>8</volume>:<fpage>61</fpage> <pub-id pub-id-type="doi">10.3389/fnbeh.2014.00061</pub-id>.</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niv</surname> <given-names>Y</given-names></string-name></person-group> (<year>2009</year>) <article-title>Reinforcement learning in the brain</article-title>. <source>J Math Psychol</source> <volume>53</volume>:<fpage>139</fpage>–<lpage>154</lpage> <pub-id pub-id-type="doi">10.1016/j.jmp.2008.12.005</pub-id>.</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ostaszewski</surname> <given-names>P</given-names></string-name></person-group> (<year>1996</year>) <article-title>The relation between temperament and rate of temporal discounting</article-title>. <source>Eur J Pers</source> <volume>10</volume>:<fpage>161</fpage>–<lpage>172</lpage> <ext-link ext-link-type="uri" xlink:href="https://journals.sagepub.com/doi/abs/10.1002/%28SICI%291099-0984%28199609%2910%3A3%3C161%3A%3AAID-PER259%3E3.0.CO%3B2-R">https://journals.sagepub.com/doi/abs/10.1002/%28SICI%291099-0984%28199609%2910%3A3%3C161%3A%3AAID-PER259%3E3.0.CO%3B2-R</ext-link>.</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pearson</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name>, <string-name><surname>Platt</surname> <given-names>ML</given-names></string-name></person-group> (<year>2010</year>) <article-title>Explicit information reduces discounting behavior in monkeys</article-title>. <source>Front Psychol</source> <volume>1</volume>:<fpage>237</fpage> <pub-id pub-id-type="doi">10.3389/fpsyg.2010.00237</pub-id>.</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peters</surname> <given-names>J</given-names></string-name>, <string-name><surname>Büchel</surname> <given-names>C</given-names></string-name></person-group> (<year>2011</year>) <article-title>The neural mechanisms of inter-temporal decision-making: understanding variability</article-title>. <source>Trends Cogn Sci</source> <volume>15</volume>:<fpage>227</fpage>–<lpage>239</lpage> <pub-id pub-id-type="doi">10.1016/j.tics.2011.03.002</pub-id>.</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pyke</surname> <given-names>GH</given-names></string-name></person-group> (<year>1984</year>) <article-title>OPTIMAL FORAGING THEORY : A CRITICAL REVIEW</article-title>. <source>Annu Rev Ecol Syst</source> <volume>15</volume>:<fpage>523</fpage>–<lpage>575</lpage>.</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pyke</surname> <given-names>GH</given-names></string-name>, <string-name><surname>Pulliam</surname> <given-names>HR</given-names></string-name>, <string-name><surname>Charnov</surname> <given-names>EL</given-names></string-name></person-group> (<year>1977</year>) <article-title>Optimal Foraging: A selective review of theory and tests</article-title>. <source>Q Rev Biol</source> <volume>52</volume>.</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rachlin</surname> <given-names>H</given-names></string-name>, <string-name><surname>Brown</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cross</surname> <given-names>D</given-names></string-name></person-group> (<year>2000</year>) <article-title>Discounting in judgments of delay and probability</article-title>. <source>J Behav Decis Mak</source> <volume>13</volume>:<fpage>145</fpage>–<lpage>159</lpage> <pub-id pub-id-type="doi">10.1002/(SICI)1099-0771(200004/06)13:2&lt;145::AID-BDM320&gt;3.0.CO;2-4</pub-id>.</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rachlin</surname> <given-names>H</given-names></string-name>, <string-name><surname>Green</surname> <given-names>L</given-names></string-name>, <string-name><surname>Vi</surname> <given-names>AD</given-names></string-name></person-group> (<year>1972</year>) <article-title>Commitment, choice and self-control</article-title>. <source>J Exp Anal Behav</source> <volume>17</volume>:<fpage>15</fpage>–<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reynolds</surname> <given-names>B</given-names></string-name>, <string-name><surname>Schiffbauer</surname> <given-names>R</given-names></string-name></person-group> (<year>2004</year>) <article-title>Measuring state changes in human delay discounting: an experiential discounting task</article-title>. <source>Behav Processes</source> <volume>67</volume>:<fpage>343</fpage>–<lpage>356</lpage> <pub-id pub-id-type="doi">10.1016/j.beproc.2004.06.003</pub-id>.</mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richards</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Mitchell</surname> <given-names>SH</given-names></string-name>, <string-name><surname>de Wit</surname> <given-names>H</given-names></string-name>, <string-name><surname>Seiden</surname> <given-names>LS</given-names></string-name></person-group> (<year>1997</year>) <article-title>Determination of discount functions in rats with an adjusting-amount procedure</article-title>. <source>J Exp Anal Behav</source> <volume>67</volume>:<fpage>353</fpage>–<lpage>366</lpage> <pub-id pub-id-type="doi">10.1901/jeab.1997.67-353</pub-id>.</mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roesch</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Calu</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Schoenbaum</surname> <given-names>G</given-names></string-name></person-group> (<year>2007</year>) <article-title>Dopamine neurons encode the better option in rats deciding between differently delayed or sized rewards</article-title>. <source>Nat Neurosci</source> <volume>10</volume>:<fpage>1615</fpage>–<lpage>1624</lpage> <pub-id pub-id-type="doi">10.1038/nn2013</pub-id>.</mixed-citation></ref>
<ref id="c91"><label>91.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Samuelson</surname> <given-names>PA</given-names></string-name></person-group> (<year>1937</year>) <article-title>A Note on Measurement of Utility</article-title>. <source>Rev Econ Stud</source> <volume>4</volume>:<fpage>155</fpage>–<lpage>161</lpage> <pub-id pub-id-type="doi">10.2307/2967612</pub-id>.</mixed-citation></ref>
<ref id="c92"><label>92.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Samuelson</surname> <given-names>PA</given-names></string-name></person-group> (<year>1938</year>) <article-title>A Note on the Pure Theory of Consumer’s Behaviour</article-title>. <source>Economica</source> <volume>5</volume>:<fpage>61</fpage>–<lpage>71</lpage> <ext-link ext-link-type="uri" xlink:href="http://www.jstor.org/stable/2548836">http://www.jstor.org/stable/2548836</ext-link>.</mixed-citation></ref>
<ref id="c93"><label>93.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schweighofer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Shishida</surname> <given-names>K</given-names></string-name>, <string-name><surname>Han</surname> <given-names>CE</given-names></string-name>, <string-name><surname>Okamoto</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Tanaka</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Yamawaki</surname> <given-names>S</given-names></string-name>, <string-name><surname>Doya</surname> <given-names>K</given-names></string-name></person-group> (<year>2006</year>) <article-title>Humans can adopt optimal discounting strategy under real-time constraints</article-title>. <source>PLoS Comput Biol</source> <volume>2</volume>:<fpage>e152</fpage> <pub-id pub-id-type="doi">10.1371/journal.pcbi.0020152</pub-id>.</mixed-citation></ref>
<ref id="c94"><label>94.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Shuler</surname> <given-names>M</given-names></string-name>, <string-name><surname>Namboodiri</surname> <given-names>V</given-names></string-name></person-group> (<year>2018</year>) <chapter-title>Think Tank: Forty Neuroscientists Explore the Biological Roots of Human Experience</chapter-title>. <source>In: Time’s weird in the brain-that’s a good thing, and here’s why</source> (<person-group person-group-type="editor"><string-name><surname>Linden</surname> <given-names>D</given-names></string-name></person-group>, ed), pp <fpage>135</fpage>–<lpage>144</lpage>. <publisher-name>Yale University Press</publisher-name>.</mixed-citation></ref>
<ref id="c95"><label>95.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smethells</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Reilly</surname> <given-names>MP</given-names></string-name></person-group> (<year>2015</year>) <article-title>Intertrial interval duration and impulsive choice</article-title>. <source>J Exp Anal Behav</source> <volume>103</volume>:<fpage>153</fpage>–<lpage>165</lpage> <pub-id pub-id-type="doi">10.1002/jeab.131</pub-id>.</mixed-citation></ref>
<ref id="c96"><label>96.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Snyderman</surname> <given-names>M</given-names></string-name></person-group> (<year>1983</year>) <article-title>Delay and amount of reward in a concurrent chain</article-title>. <source>J Exp Anal Behav</source> <volume>39</volume>:<fpage>437</fpage>–<lpage>447</lpage> <pub-id pub-id-type="doi">10.1901/jeab.1983.39-437</pub-id>.</mixed-citation></ref>
<ref id="c97"><label>97.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stephens</surname> <given-names>DW</given-names></string-name></person-group> (<year>2008</year>) <article-title>Decision ecology: foraging and the ecology of animal decision making</article-title>. <source>Cogn Affect Behav Neurosci</source> <volume>8</volume>:<fpage>475</fpage>–<lpage>484</lpage> <pub-id pub-id-type="doi">10.3758/CABN.8.4.475</pub-id>.</mixed-citation></ref>
<ref id="c98"><label>98.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stephens</surname> <given-names>DW</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>D</given-names></string-name></person-group> (<year>2001</year>) <article-title>The adaptive value of preference for immediacy : when shortsighted rules have farsighted consequences</article-title>. <source>Behav Ecol</source> <volume>12</volume>:<fpage>330</fpage>–<lpage>339</lpage>.</mixed-citation></ref>
<ref id="c99"><label>99.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stephens</surname> <given-names>DW</given-names></string-name>, <string-name><surname>Dunlap</surname> <given-names>AS</given-names></string-name></person-group> (<year>2009</year>) <article-title>Why do animals make better choices in patch-leaving problems?</article-title> <source>Behav Processes</source> <volume>80</volume>:<fpage>252</fpage>–<lpage>260</lpage> <pub-id pub-id-type="doi">10.1016/j.beproc.2008.11.014</pub-id>.</mixed-citation></ref>
<ref id="c100"><label>100.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stephens</surname> <given-names>DW</given-names></string-name>, <string-name><surname>Kerr</surname> <given-names>B</given-names></string-name>, <string-name><surname>Fernández-Juricic</surname> <given-names>E</given-names></string-name></person-group> (<year>2004</year>) <article-title>Impulsiveness without discounting: the ecological rationality hypothesis</article-title>. <source>Proc Biol Sci</source> <volume>271</volume>:<fpage>2459</fpage>–<lpage>2465</lpage> <pub-id pub-id-type="doi">10.1098/rspb.2004.2871</pub-id>.</mixed-citation></ref>
<ref id="c101"><label>101.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Stephens</surname> <given-names>DW</given-names></string-name>, <string-name><surname>Krebs</surname> <given-names>JR</given-names></string-name></person-group> (<year>1986</year>) <source>Foraging Theory</source>. <publisher-name>Princeton University Press</publisher-name></mixed-citation></ref>
<ref id="c102"><label>102.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stevens</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Mühlhoff</surname> <given-names>N</given-names></string-name></person-group> (<year>2012</year>) <article-title>Intertemporal choice in lemurs</article-title>. <source>Behav Processes</source> <volume>89</volume>:<fpage>121</fpage>–<lpage>127</lpage> <pub-id pub-id-type="doi">10.1016/j.beproc.2011.10.002</pub-id>.</mixed-citation></ref>
<ref id="c103"><label>103.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Story</surname> <given-names>GW</given-names></string-name>, <string-name><surname>Vlaev</surname> <given-names>I</given-names></string-name>, <string-name><surname>Seymour</surname> <given-names>B</given-names></string-name>, <string-name><surname>Darzi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ</given-names></string-name></person-group> (<year>2014</year>) <article-title>Does temporal discounting explain unhealthy behavior? A systematic review and reinforcement learning perspective</article-title>. <source>Front Behav Neurosci</source> <volume>8</volume>:<fpage>76</fpage> <pub-id pub-id-type="doi">10.3389/fnbeh.2014.00076</pub-id>.</mixed-citation></ref>
<ref id="c104"><label>104.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Strotz</surname> <given-names>RH</given-names></string-name></person-group> (<year>1956</year>) <article-title>Myopia and Inconsistency in Dynamic Utility Maximization</article-title>. <source>Rev Econ Stud</source> <volume>23</volume>:<fpage>165</fpage>–<lpage>180</lpage>.</mixed-citation></ref>
<ref id="c105"><label>105.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takahashi</surname> <given-names>T</given-names></string-name>, <string-name><surname>Han</surname> <given-names>R</given-names></string-name></person-group> (<year>2012</year>) <article-title>Tempospect theory of intertemporal choice</article-title>. <source>Psychology</source> <volume>3</volume>:<fpage>555</fpage>–<lpage>557</lpage> <pub-id pub-id-type="doi">10.4236/psych.2012.38082</pub-id>.</mixed-citation></ref>
<ref id="c106"><label>106.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thaler</surname> <given-names>R</given-names></string-name></person-group> (<year>1981</year>) <article-title>Some empirical evidence on dynamic inconsistency</article-title>. <source>Econ Lett</source> <volume>8</volume>:<fpage>201</fpage>–<lpage>207</lpage>.</mixed-citation></ref>
<ref id="c107"><label>107.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thaler</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Shefrin</surname> <given-names>HM</given-names></string-name></person-group> (<year>1981</year>) <article-title>An Economic Theory of Self-Control</article-title>. <source>J Polit Econ</source> <volume>89</volume>:<fpage>392</fpage>–<lpage>406</lpage> <ext-link ext-link-type="uri" xlink:href="http://www.jstor.org/stable/1833317">http://www.jstor.org/stable/1833317</ext-link>.</mixed-citation></ref>
<ref id="c108"><label>108.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wikenheiser</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Stephens</surname> <given-names>DW</given-names></string-name>, <string-name><surname>Redish</surname> <given-names>AD</given-names></string-name></person-group> (<year>2013</year>) <article-title>Subjective costs drive overly patient foraging strategies in rats on an intertemporal foraging task</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>110</volume>:<fpage>8308</fpage>–<lpage>8313</lpage> <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/23630289">https://www.ncbi.nlm.nih.gov/pubmed/23630289</ext-link>.</mixed-citation></ref>
<ref id="c109"><label>109.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Winstanley</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Theobald</surname> <given-names>DEH</given-names></string-name>, <string-name><surname>Cardinal</surname> <given-names>RN</given-names></string-name>, <string-name><surname>Robbins</surname> <given-names>TW</given-names></string-name></person-group> (<year>2004</year>) <article-title>Contrasting roles of basolateral amygdala and orbitofrontal cortex in impulsive choice</article-title>. <source>J Neurosci</source> <volume>24</volume>:<fpage>4718</fpage>–<lpage>4722</lpage> <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5606-03.2004</pub-id>.</mixed-citation></ref>
<ref id="c110"><label>110.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yi</surname> <given-names>R</given-names></string-name>, <string-name><surname>de la Piedad</surname> <given-names>X</given-names></string-name>, <string-name><surname>Bickel</surname> <given-names>WK</given-names></string-name></person-group> (<year>2006</year>) <article-title>The combined effects of delay and probability in discounting</article-title>. <source>Behav Processes</source> <volume>73</volume>:<fpage>149</fpage>–<lpage>155</lpage> <pub-id pub-id-type="doi">10.1016/j.beproc.2006.05.001</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99957.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ahmed</surname>
<given-names>Alaa A</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Colorado Boulder</institution>
</institution-wrap>
<city>Boulder</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This paper undertakes a <bold>valuable</bold> theoretical treatment of the potential role of foraging-related concepts in several forms of intertemporal choice. While the computational evidence and methodologies employed are novel, some issues with clarity and generality result in <bold>incomplete</bold> support for the paper's claims.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99957.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This theoretical paper addresses how to optimize reward-rate-maximizing decisions in certain foraging-style environments. It presents a series of equations and graphical illustrations for quantities such as reward rates and time-related costs that a decision maker could estimate as a basis for such decisions. One of the main takeaways is that if the hypothetical agent underweights the time spent outside a focal reward pursuit relative to the time spent within it, this can predict a broadly realistic pattern of impatience in two alternative intertemporal choices paired with well-calibrated take-it-or-leave-it decisions. Another takeaway is that if the optimally estimated subjective value of a reward pursuit is plotted as a function of a range of temporal durations, the result resembles a hyperbolic discounting function and is affected in empirically realistic ways by the magnitude and sign of the reward. Thus, the rate-maximization framework might lead to a hypothesis about the basis for the magnitude and sign effects in discounting.</p>
<p>Strengths:</p>
<p>The paper makes a useful contribution by broadening the application of reward-rate maximization to time-related decision scenarios. The paper's breadth of scope includes applying the same framework to accept/reject decisions and multi-alternative discounting decisions. The figures take a creative approach to illustrating the internal quantities in the model. It's particularly useful that the paper gives consideration to internal distortions that could give rise to documented anomalies in decision behavior.</p>
<p>Weaknesses:</p>
<p>(1) Although there are many citations acknowledging relevant previous work, there often isn't a very granular attribution of individual previous findings to their sources. In the results section, it's sometimes ambiguous when the paper is recapping established background and when it is breaking new ground. For example, around equation 8 in the results (sv = r - rho*t), it would be good to refer to previous places where versions of this equation have been presented. Offhand, McNamara 1982 (Theoretical Population Biology) is one early instance and Fawcett et al. 2012 (Behavioural Processes) is a later one. Line 922 of the discussion seems to imply this formulation is novel here.</p>
<p>(2) The choice environments that are considered in detail in the paper are very simple. The simplicity facilitates concrete examples and visualizations, but it would be worth further consideration of whether and how the conclusions generalize to more complex environments. The paper considers &quot;forgo&quot; scenario in which the agent can choose between sequences of pursuits like A-B-A-B (engaging with option B at all opportunities, which are interleaved with a default pursuit A) and A-A-A-A (forgoing option B). It considers &quot;choice&quot; scenarios where the agent can choose between sequences like A-B-A-B and A-C-A-C (where B and C are larger-later and smaller-sooner rewards, either of which can be interleaved with the default pursuit). Several forms of additional complexity would be valuable to consider. One would be a greater number of unique pursuits, not repeated identically in a predictable sequence, akin to a prey-selection paradigm. It seems to me this would cause t_out and r_out (the time and reward outside of the focal prospect) to be policy-dependent, making the 'apportionment cost' more challenging to ascertain. Another relevant form of complexity would be if there were variance or uncertainty in reward magnitudes or temporal durations or if the agent had the ability to discontinue a pursuit such as in patch-departure scenarios.</p>
<p>(3) I had a hard time arriving at a solid conceptual understanding of the 'apportionment cost' around Figure 5. I understand the arithmetic, but it would help if it were possible to formulate a more succinct verbal description of what makes the apportionment cost a useful and meaningful quality to focus on. I think Figure 6C relates to this, but I had difficulty relating the axis labels to the points, lines, and patterned regions in the plot. I also was a bit confused by how the mathematical formulation was presented. As I understood it, the apportionment cost essentially involves scaling the rest of the SV expression by t_out/(t_in + t_out). The way this scaling factor is written in Figure 5C, as 1/(1 + (1/t_out)t_in), seems less clear than it could be. Also, the apportionment cost is described in the text as being subtracted from SV rather than as a multiplicative scaling factor. It could be written as a subtraction, by subtracting a second copy of the rest of the SV expression scaled by t_in/(t_in + t_out). But that shows the apportionment cost to depend on the opportunity cost, which is odd because the original motivation on line 404 was to resolve the lack of independence between terms in the SV expression.</p>
<p>(4) In the analysis of discounting functions (line 664 and beyond), the paper doesn't say much about the fact that many discounting studies take specific measures to distinguish true time preferences from opportunity costs and reward-rate maximization. In many of the human studies, delay time doesn't preclude other activities. In animal studies, rate maximization can serve as a baseline against which to measure additional effects of temporal discounting. This is an important caveat to claims about discounting anomalies being rational under rate maximization (e.g., line 1024).</p>
<p>(5) The paper doesn't feature any very concrete engagement with empirical data sets. This is ok for a theoretical paper, but some of the characterizations of empirical results that the model aims to match seem oversimplified. An example is the contention that real decision-makers are optimal in accept/reject decisions (line 816 and elsewhere). This isn't always true; sometimes there is evidence of overharvesting, for example.</p>
<p>(6) Related to the point above, it would be helpful to discuss more concretely how some of this paper's theoretical proposals could be empirically evaluated in the future. Regarding the magnitude and sign effects of discounting, there is not a very thorough overview of the several other explanations that have been proposed in the literature. It would be helpful to engage more deeply with previous proposals and consider how the present hypothesis might make unique predictions and could be evaluated against them. A similar point applies to the 'malapportionment hypothesis' although in this case there is a very helpful section on comparisons to prior models (line 1163). The idea being proposed here seems to have a lot in common conceptually with Blanchard et al. 2013, so it would be worth saying more about how data could be used to test or reconcile these proposals.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99957.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper from Sutlief et al. focuses on an apparent contradiction observed in experimental data from two related types of pursuit-based decision tasks. In &quot;forgo&quot; decisions, where the subject is asked to choose whether or not to accept a presented pursuit, after which they are placed into a common inter-trial interval, subjects have been shown to be nearly optimal in maximizing their overall rate of reward. However, in &quot;choice&quot; decisions, where the subject is asked which of two mutually-exclusive pursuits they will take, before again entering a common inter-trial interval, subjects exhibit behavior that is believed to be sub-optimal. To investigate this contradiction, the authors derive a consistent reward-maximizing strategy for both tasks using a novel and intuitive geometric approach that treats every phase of a decision (pursuit choice and inter-trial interval) as vectors. From this approach, the authors are able to show that previously reported examples of sub-optimal behavior in choice decisions are in fact consistent with a reward-maximizing strategy. Additionally, the authors are able to use their framework to deconstruct the different ways the passage of time impacts decisions, demonstrating that time cost contains both an opportunity cost and an apportionment cost, as well as examining how a subject's misestimation of task parameters impacts behavior.</p>
<p>Strengths:</p>
<p>The main strength of the paper lies in the authors' geometric approach to studying the problem. The authors chose to simplify the decision process by removing the highly technical and often cumbersome details of evidence accumulation that are common in most of the decision-making literature. In doing so, the authors were able to utilize a highly accessible approach that is still able to provide interesting insights into decision behavior and the different components of optimal decision strategies.</p>
<p>Weaknesses:</p>
<p>While the details of the paper are compelling, the authors' presentation of their results is often unclear or incomplete:</p>
<p>(1) The mathematical details of the paper are correct but contain numerous notation errors and are presented as a solid block of subtle equation manipulations. This makes the details of the authors' approach (the main contribution of the paper to the field) highly difficult to understand.</p>
<p>(2) One of the main contributions of the paper is the notion that time cost in decision-making contains an apportionment cost that reflects the allocation of decision time relative to the world. The authors use this cost to pose a hypothesis as to why subjects exhibit sub-optimal behavior in choice decisions. However, the equation for the apportionment cost is never clearly defined in the paper, which is a significant oversight that hampers the effectiveness of the authors' claims.</p>
<p>(3) Many of the paper's figures are visually busy and not clearly detailed in the captions (for example, Figures 6-8). Because of the geometric nature of the authors' approach, the figures should be as clean and intuitive as possible, as in their current state, they undercut the utility of a geometric argument.</p>
<p>(4) The authors motivate their work by focusing on previously-observed behavior in decision experiments and tell the reader that their model is able to qualitatively replicate this data. This claim would be significantly strengthened by the inclusion of experimental data to directly compare to their model's behavior. Given the computational focus of the paper, I do not believe the authors need to conduct their own experiments to obtain this data; reproducing previously accepted data from the papers the authors' reference would be sufficient.</p>
<p>(5) While the authors reference a good portion of the decision-making literature in their paper, they largely ignore the evidence-accumulation portion of the literature, which has been discussing time-based discounting functions for some years. Several papers that are both experimentally-(Cisek et al. 2009, Thurs et al. 2012, Holmes et al. 2016) and theoretically-(Drugowitsch et al. 2012, Tajima et al. 2019, Barendregt et al. 22) driven exist, and I would encourage the authors to discuss how their results relate to those in different areas of the field.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99957.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The goal of the paper is to examine the objective function of total reward rate in an environment to understand the behavior of humans and animals in two types of decision-making tasks: (1) stay/forgo decisions and (2) simultaneous choice decisions. The main aims are to reframe the equation of optimizing this normative objective into forms that are used by other models in the literature like subjective value and temporally discounted reward. One important contribution of the paper is the use of this theoretical analysis to explain apparent behavioral inconsistencies between forgo and choice decisions observed in the literature.</p>
<p>Strengths:</p>
<p>The paper provides a nice way to mathematically derive different theories of human and animal behavior from a normative objective of global reward rate optimization. As such, this work has value in trying to provide a unifying framework for seemingly contradictory empirical observations in literature, such as differentially optimal behaviors in stay-forgo v/s choice decision tasks. The section about temporal discounting is particularly well motivated as it serves as another plank in the bridge between ecological and economic theories of decision-making.</p>
<p>Weaknesses:</p>
<p>One broad issue with the paper is readability. Admittedly, this is a complicated analysis involving many equations that are important to grasp to follow the analyses that subsequently build on top of previous analyses.</p>
<p>But, what's missing is intuitive interpretations behind some of the terms introduced, especially the apportionment cost without referencing the equations in the definition so the reader gets a sense of how the decision-maker thinks of this time cost in contrast with the opportunity cost of time.</p>
<p>Re-analysis of some existing empirical data through the lens of their presented objective functions, especially later when they describe sources of error in behavior.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99957.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sutlief</surname>
<given-names>Elissa</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Walters</surname>
<given-names>Charlie</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Marton</surname>
<given-names>Tanya</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shuler</surname>
<given-names>Marshall G Hussain</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1927-0970</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the reviewers for their thoughtful criticisms.  This provisional response addresses what we consider the central critiques, with a full, point-by-point reply to follow with the revised manuscript.  Central critiques concern 1) providing further clarity about the apportionment cost of time, 2) generality &amp; scope, and 3) clarifying the meaning of key equations.</p>
<p>(1) Apportionment cost</p>
<p>Reviewers commonly identified a need to provide a concise and intuitive definition of apportionment cost, and to explicitly solve and provide for its mathematical expression.</p>
<p>We will add the following definition of apportionment cost to the manuscript: “Apportionment cost is the difference in reward that can be expected, on average, between a policy of <italic>taking</italic> versus a policy of <italic>not taking</italic> the considered pursuit, over a time equal to its duration.”  While this difference is the <italic>apportionment cost</italic> of time, the amount that would be expected over a time equal to the considered pursuit under a policy of <italic>not</italic> taking the considered pursuit is the <italic>opportunity cost</italic> of time.  Together, they sum to Time’s Cost.  The above definition of apportionment cost adds to other stated relationships of apportionment cost found throughout the paper (Lines 434,435,447,450).</p>
<p>As suggested, we will also add equations of apportionment cost, as below.</p>
<disp-formula id="sa4equ1">
<graphic mime-subtype="jpg" xlink:href="elife-99957-sa4-equ1.jpg" mimetype="image"/>
</disp-formula>
<p>(2) Generality &amp; Scope</p>
<p><italic>Generality</italic>. We will add further examples in support of the generality of these equations for assessing and thinking about the value of initiating a pursuit.  Specifically, this will include 1) illustrating forgo decision making in a world composed of multiple pursuits, as in prey selection, 2) demonstrating and examining worlds in which a sequence of pursuits compose a considered pursuit’s ‘outside’, and 3) clarifying how our framework does contend with variance and uncertainty in reward magnitude and occurrence.</p>
<p><italic>Scope</italic>. In this manuscript, we consider the worth of <italic>initiating</italic> one or another pursuit having completed a prior one, and not the issue of <italic>continuing</italic> within a pursuit having already engaged in it.  The worth of continuing a pursuit, as in patch-foraging/give-up tasks, constitutes a third fundamental time decision-making topology which is outside the scope of the current work.  It engages a large and important literature, encompassing evidence accumulation, and requires a paper in its own right that can use the concepts and framework developed here.  We will further consider applying this framework to extant datasets.</p>
<p>(3) Correction of typographical errors and further explanation of equations.</p>
<p>We would like to redress the two typographical errors identified by the reviewers that appeared in the equations on line 277 and on line 306, and provide further explanation to equations that gave pause to the reviewers.</p>
<p>Typographical errors:</p>
<p>The first typographical error in the main text regards equation 2 and will be corrected so that equation 2 appears correctly as…</p>
<p>Line 277: <inline-formula id="sa4equ2"><inline-graphic xlink:href="elife-99957-sa4-equ2.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula></p>
<p>The second typo regards the definition of the considered pursuit’s reward rate, and will be corrected to appear as…</p>
<p>Line 306: <inline-formula id="sa4equ3"><inline-graphic xlink:href="elife-99957-sa4-equ3.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula></p>
<p>Regarding equations:</p>
<p>Cross-reference to equations in the main text refer to equations as they appear in the main text.  Where needed, the appendix in which they are derived is also given.   Equation numbering within the appendices refer to equations as they appear in the appendices.  In the revision, we will refer to all equations that appear in the appendices as Ap.#.#. so as to avoid confusion between referencing equations as they appear in the main text and equations as they appear in the appendices.</p>
<p>We would also like to clarify that equation 8, <inline-formula id="sa4equ4"><inline-graphic xlink:href="elife-99957-sa4-equ4.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>, as we derive, is not new, as it is similarly derived and expressed in prior foundational work by McNamara (1982), which is now so properly attributed.</p>
<p>Equation 1 and Appendix 1</p>
<p>Equation 1 is formulated to calculate the average reward received and average time spent per unit time spent in the default pursuit. So, <italic>fi</italic> is the encounter rate of pursuit  for one unit of time spent in the default pursuit (lines 259-262). Added to the summation in the numerator, we have the average reward obtained in the default pursuit per unit time <inline-formula id="sa4equ5"><inline-graphic xlink:href="elife-99957-sa4-equ5.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> and in the denominator we have the time spent in the default pursuit per unit time (1).</p>
<disp-formula id="sa4equ6">
<graphic mime-subtype="jpg" xlink:href="elife-99957-sa4-equ6.jpg" mimetype="image"/>
</disp-formula>
<p><bold>Equation 2 and Appendix 2</bold></p>
<p>Eq. 2.4 in Appendix 2 calculates the average time spent outside of the considered pursuit, per encounter with the considered pursuit. Breaking down eq. 2.4, the first term in the numerator,</p>
<disp-formula id="sa4equ7">
<graphic mime-subtype="jpg" xlink:href="elife-99957-sa4-equ7.jpg" mimetype="image"/>
</disp-formula>
<p>gives the expected time spent in other pursuits, per unit time spent in the default pursuit, where <italic>fi</italic> is the encounter rate of pursuit  per unit time spent in the default pursuit, and  is the time required by pursuit <italic>i</italic>. The second term in the numerator, (1, added outside the summation) simply represents the unit of time spent in the default pursuit, over which the encounter rate of each pursuit is calculated. Together, these represent the total time spent outside the considered pursuit, per unit time spent in the default pursuit. The denominator,</p>
<disp-formula id="sa4equ8">
<graphic mime-subtype="jpg" xlink:href="elife-99957-sa4-equ8.jpg" mimetype="image"/>
</disp-formula>
<p>is the frequency with which the considered pursuit is encountered per unit time spent in the default pursuit, so</p>
<disp-formula id="sa4equ9">
<graphic mime-subtype="jpg" xlink:href="elife-99957-sa4-equ9.jpg" mimetype="image"/>
</disp-formula>
<p>is the average time spent within the default pursuit, per encounter with the considered pursuit. By multiplying the average time spent outside of the considered pursuit per unit time spent in the default pursuit by the average time spent within the default pursuit per encounter with the considered pursuit, we get eq. 2.4, the average time spent outside of the considered pursuit, per encounter with the considered pursuit, which is equal to <italic>tout</italic>.</p>
<p><inline-formula id="sa4equ10"><inline-graphic xlink:href="elife-99957-sa4-equ10.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>                       (eq. 2.4)</p>
</body>
</sub-article>
</article>