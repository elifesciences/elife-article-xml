<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">98047</article-id>
<article-id pub-id-type="doi">10.7554/eLife.98047</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98047.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Reconstructing Voice Identity from Noninvasive Auditory Cortex Recordings</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9918-8258</contrib-id>
<name>
<surname>Lamothe</surname>
<given-names>Charly</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8214-6278</contrib-id>
<name>
<surname>Thoret</surname>
<given-names>Etienne</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1137-8669</contrib-id>
<name>
<surname>Trapeau</surname>
<given-names>Régis</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7002-0486</contrib-id>
<name>
<surname>Giordano</surname>
<given-names>Bruno L</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1767-5330</contrib-id>
<name>
<surname>Sein</surname>
<given-names>Julien</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8410-0962</contrib-id>
<name>
<surname>Takerkart</surname>
<given-names>Sylvain</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2982-7127</contrib-id>
<name>
<surname>Ayache</surname>
<given-names>Stéphane</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3696-0321</contrib-id>
<name>
<surname>Artières</surname>
<given-names>Thierry</given-names>
</name>
<email>thierry.artieres@lis-lab.fr</email>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="author-notes" rid="n1">7</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7578-6365</contrib-id>
<name>
<surname>Belin</surname>
<given-names>Pascal</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="author-notes" rid="n1">7</xref>
</contrib>
<aff id="a1"><label>1</label><institution>La Timone Neuroscience Institute UMR 7289</institution>, CNRS, <institution>Aix-Marseille University</institution>, Marseille, <country>France</country></aff>
<aff id="a2"><label>2</label><institution>Laboratoire d’Informatique et Systèmes UMR 7020</institution>, CNRS, <institution>Aix-Marseille University</institution>, Marseille, <country>France</country></aff>
<aff id="a3"><label>3</label><institution>Perception, Representation, Image, Sound, Music UMR 7061</institution>, CNRS, Marseille, <country>France</country></aff>
<aff id="a4"><label>4</label><institution>Institute of Language Communication &amp; the Brain</institution>, Marseille</aff>
<aff id="a5"><label>5</label><institution>Centre IRM-INT@CERIMED</institution>, Marseille, <country>France</country></aff>
<aff id="a6"><label>6</label><institution>École Centrale de Marseille</institution>, Marseille, <country>France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1"><label>7</label><p>These authors jointly supervised this work: Thierry Artières, Pascal Belin</p></fn>
<corresp id="cor1"><label>*</label>E-mail: <email>charlylmth@gmail.com</email>, <email>thierry.artieres@lis-lab.fr</email>, <email>pascal.belin@univ-amu.fr</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-07-15">
<day>15</day>
<month>07</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP98047</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-16">
<day>16</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-19">
<day>19</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.27.582302"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Lamothe et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Lamothe et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-98047-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>The cerebral processing of voice information is known to engage, in human as well as non-human primates, “temporal voice areas” (TVAs) that respond preferentially to conspecific vocalizations. However, how voice information is represented by neuronal populations in these areas, particularly speaker identity information, remains poorly understood. Here, we used a deep neural network (DNN) to generate a high-level, small-dimension representational space for voice identity—the ‘voice latent space’ (VLS)—and examined its linear relation with cerebral activity via encoding, representational similarity, and decoding analyses. We find that the VLS maps onto fMRI measures of cerebral activity in response to tens of thousands of voice stimuli from hundreds of different speaker identities and better accounts for the representational geometry for speaker identity in the TVAs than in A1. Moreover, the VLS allowed TVA-based reconstructions of voice stimuli that preserved essential aspects of speaker identity as assessed by both machine classifiers and human listeners. These results indicate that the DNN-derived VLS provides high-level representations of voice identity information in the TVAs.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Correct spelling mistakes; add details on the model training; justify the approach better; add details on the data acquisition; deepen the discussion part; add missing references; fix wrong references.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The human voice carries speech, but is also an “auditory face” that carries much valuable information on the stable physical characteristics of the speaker (hereafter, ‘identity-related’; <xref ref-type="bibr" rid="c9">Belin et al., 2004</xref>, <xref ref-type="bibr" rid="c7">2011</xref>). The ability of listeners to extract identity-related information in voice such as gender, age, or unique identity even in brief stimuli plays a crucial role in our social interactions, yet its neural bases remain poorly understood compared to those of speech processing. Studies over the past two decades have clearly established via complementary neuroimaging techniques that the cerebral processing of voice information involves a set of temporal voice areas (TVAs) in secondary auditory cortical regions of the human (fMRI: <xref ref-type="bibr" rid="c11">Belin et al., 2000</xref>, von Kriegstein et al., 2004, <xref ref-type="bibr" rid="c67">Pernet et al., 2015</xref>; EEG, MEG: <xref ref-type="bibr" rid="c22">Charest et al., 2009</xref>, <xref ref-type="bibr" rid="c16">Capilla et al., 2013</xref>, <xref ref-type="bibr" rid="c6">Barbero et al., 2021</xref>; Electrophysiology: <xref ref-type="bibr" rid="c70">Rupp et al., 2022</xref>, <xref ref-type="bibr" rid="c90">Zhang et al., 2021</xref>) as well as macaque brain (<xref ref-type="bibr" rid="c68">Petkov et al., 2008</xref>; <xref ref-type="bibr" rid="c14">Bodin et al., 2021</xref>). The TVAs respond more strongly to sounds of voice – with or without speech (<xref ref-type="bibr" rid="c67">Pernet et al., 2015</xref>; <xref ref-type="bibr" rid="c70">Rupp et al., 2022</xref>; <xref ref-type="bibr" rid="c78">Trapeau et al., 2022</xref>)—and categorize voice apart from other sounds (<xref ref-type="bibr" rid="c14">Bodin et al., 2021</xref>) but the nature of the information encoded at these stages of cortical processing, especially with respect to speaker identity-related information, remains largely unknown (<xref ref-type="bibr" rid="c13">Blank et al., 2014</xref>; <xref ref-type="bibr" rid="c8">Belin et al., 2018</xref>).</p>
<p>In recent years, deep neural networks (DNNs) have emerged as a powerful tool for representing complex visual data, such as images (<xref ref-type="bibr" rid="c53">LeCun et al., 2015</xref>) or videos (<xref ref-type="bibr" rid="c54">Liu et al., 2020</xref>). In the auditory domain, DNNs have been shown to provide valuable representations—so-called feature or latent spaces—for modeling the cerebral processing of sound (brain encoding) (speech: <xref ref-type="bibr" rid="c44">Kell et al., 2018</xref>; <xref ref-type="bibr" rid="c59">Millet et al., 2022</xref>; Tuckute &amp; Feather, 2023; semantic content: <xref ref-type="bibr" rid="c17">Caucheteux et al., 2022</xref>; <xref ref-type="bibr" rid="c19">Caucheteux &amp; King, 2022</xref>; <xref ref-type="bibr" rid="c18">Caucheteux et al., 2023</xref>; <xref ref-type="bibr" rid="c32">Giordano et al., 2023</xref>; music: <xref ref-type="bibr" rid="c36">Güçlü et al., 2016</xref>), or reconstructing the stimuli listened by a participant (brain decoding) (<xref ref-type="bibr" rid="c3">Akbari et al., 2019</xref>). They have not yet been used to explain cerebral representations of identity-related information due in part to the focus on speech information (<xref ref-type="bibr" rid="c50">von Kriegstein et al., 2003</xref>).</p>
<p>Here, we addressed this challenge by training a ‘Variational autoencoder’ (VAE; Kingma et Welling, 2014) DNN to reconstruct voice spectrograms from 182,000 250-ms voice samples from 405 different speaker identities in 8 different languages from the CommonVoice database (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>). Brief (250 ms) samples were used to emphasize speaker identity-related information in voice, already available after a few hundred milliseconds (<xref ref-type="bibr" rid="c75">Schweinberger et al., 1997</xref>; <xref ref-type="bibr" rid="c51">Lavan, 2023</xref>), over linguistic information unfolding over longer periods (word, &gt;350 ms; <xref ref-type="bibr" rid="c58">Mcallister et al., 1994</xref>). While a quarter of a second is admittedly short compared to standards of, e.g., computational speaker identification that typically uses 2-3 s samples, this short duration is sufficient to allow near-perfect gender classification and performance levels well above chance for speaker discrimination (<xref rid="fig5" ref-type="fig">Fig. 5d</xref>, red dotted line). This brief duration allowed the presentation of many more stimuli to our participants in the scanner while preserving acceptable behavioral and classifier performance levels.</p>
<p>State-of-the-art studies have primarily relied on task-optimized neural networks (i.e., DNN trained using supervised learning to classify a category from the input) to study sensory cortex processes (<xref ref-type="bibr" rid="c88">Yamins &amp; DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="c73">Schrimpf et al., 2018</xref>). They can reach high accuracies in brain encoding (<xref ref-type="bibr" rid="c45">Khaligh-Razavi &amp; Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="c73">Schrimpf et al., 2018</xref>; <xref ref-type="bibr" rid="c38">Han et al., 2019</xref>). However, there is increasing evidence that unsupervised learning, such as that used for the VAE, also provides plausible computational models for investigating brain processing (<xref ref-type="bibr" rid="c40">Higgins et al., 2021</xref>; Zhuang et al., 2021; <xref ref-type="bibr" rid="c59">Millet et al., 2022</xref>; Orhan et al., 2022). Thus, the VAE-derived VLS, exploited within encoding, representational similarity, and decoding frameworks, offers a potentially promising tool for investigating the representations of voice stimuli in the secondary auditory cortex (<xref ref-type="bibr" rid="c62">Naselaris et al., 2011</xref>). Autoencoders learn to compress stimuli with high dimensionality into a lower-dimensional space that nonetheless allows reconstruction of the original stimuli via an inverse transformation learned by the second part of the network called the decoder. <xref rid="fig1" ref-type="fig">Fig. 1a</xref> shows the architecture of the VAE, with its encoder that reduces an input spectrogram to a highly compressed, 128-dimension <italic>voice latent space</italic> (VLS) representation and its decoder that reconstructs the spectrogram from this VLS representation. We selected this latent space size as it was the first value that produced satisfactory reconstructions. Points in the VLS correspond to voice samples with different identities and phonetic content. A line segment in the VLS contains points corresponding to perceptual interpolations between its two extremities (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>; Supplementary Audio 1). VLS coordinates of samples presented to the participants averaged by speaker identity suggest that a major organizational dimension of the latent space is voice gender (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>) (colored by age or language in Supplementary Figure 1).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>DNN-derived Voice Latent Space (VLS).</title>
<p><bold>a,</bold> Variational autoencoder (VAE) Architecture. Two networks learned complementary tasks. An encoder was trained using 182K voice samples to compress their spectrogram into a 128-dimension representation, the voice latent space (VLS), while a decoder learned the reverse mapping. The network was trained end-to-end by minimizing the difference between the original and reconstructed spectrograms. <bold>b,</bold> Distribution of the 405 speaker identities along the first 2 principal components of the VLS coordinates from all sounds, averaged by speaker identity. Each disk represents a speaker’s identity colored by gender. PC2 largely maps onto voice gender (ANOVAs on the first two components: PC1: F(1, 405)=0.10, p=.74; PC2: F(1, 405)=11.00, p&lt;.001). Large disks represent the average of all male (black) or female (gray) speaker coordinates, with their associated reconstructed spectrograms (note the flat fundamental frequency (f0) and formant frequencies contours caused by averaging). The bottom of the spectrograms illustrates an interpolation between stimuli of two different speaker identities: spectrograms at the extremes correspond to two original stimuli (A, B) and their VLS-reconstructed spectrograms (A’, B’). Intermediary spectrograms were reconstructed from linearly interpolated coordinates between those two points in the VLS (red line) (cf. Supplementary Audio 1). <bold>c,d e,</bold> Performance of linear classifiers at categorizing speaker gender (chance level: 50%), age (young/adult, chance level: 50%), or identity (119 identities, chance level: 0.84%) based on VLS or LIN coordinates. Error bars indicate the standard error of the mean (s.e.m) across 100 random classifier initializations. All ps&lt;1e-10. The horizontal black dashed lines indicate chance levels. ****: p&lt;0.0001.</p></caption>
<graphic xlink:href="582302v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In order to test whether VLS accounts well for cerebral activity in response to voice stimuli, we scanned three healthy volunteers using fMRI to measure an indirect index of their cerebral activity across 10+ hours of scanning each in response to ∼12,000 of the voice samples, denoted <italic>BrainVoice</italic> in the following (different from the ones used to train the DNN). The small number of participants does not allow for generalization at the general population level as in standard fMRI studies. However, it allows testing for replicability as in comparable studies involving 10+ hours of scanning per participant (<xref ref-type="bibr" rid="c81">VanRullen &amp; Reddy, 2019</xref>). Different stimulus sets were used across participants to provide a stringent test of replicability based on subject-level analyses. Stimuli consisted of randomly spliced 250-ms excerpts of speech samples from the CommonVoice database (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>) by 119 speakers in 8 languages. For assessing generalization performances of decoding models and brain-based reconstruction, six test stimuli were repeated more often (60 times) for each participant to provide robust estimates of their induced cerebral activity (see Methods). We first modeled these responses to voice using a general linear model (GLM) (<xref ref-type="bibr" rid="c30">Friston et al., 1994</xref>) with several nuisance regressors as an initial denoising step (Supplementary Figure 3), then used a second GLM modeling cerebral responses to the different speaker identities (Supplementary Figure 2a), resulting in one voxel activity map per speaker (Supplementary Figure 2b). We independently localized in each participant several regions of interest (ROIs) on which subsequent analyses were focused: the anterior, middle and posterior TVAs in each hemisphere (individually localized via an independent ‘voice localizer scan’ and MNI coordinates provided in <xref ref-type="bibr" rid="c67">Pernet et al., 2015</xref>; Supplementary Figure 2c) as well as primary auditory cortex (A1) (using a probabilistic map in MNI space (<xref ref-type="bibr" rid="c66">Penhune et al., 1996</xref>; Supplementary Figure 2d).</p>
<p>We first asked how the VLS could account for the brain responses to speaker identities (encoding) measured in A1 and the TVAs, in comparison with a linear autoencoder’s latent space (LIN). This approach was chosen to compare a representation learned linearly under similar conditions (same input data, learning algorithm, reconstruction objective and latent space size) with the VLS, which has non-linear transformations and a regularized latent space. For this, we used a general linear model (GLM) of fMRI responses to the speaker identities, resulting in one voxel activity map per speaker (Supplementary Figure 2). Then, we computed the average VLS coordinates of the fMRI voice stimuli for each speaker identity, which may be seen as a speaker representation in the VLS (see <italic>Identity-based and stimulus-based representations</italic> section). Next we trained a linear voxel-based encoding model to predict the speaker voxel activity maps from the speaker VLS coordinates. As VAE achieves compression through a series of nonlinear transformations (<xref ref-type="bibr" rid="c84">Wetzel, 2017</xref>), we choose to contrast its results with a linear autoencoder’s latent space. This method has previously been applied to fMRI-based image reconstructions (<xref ref-type="bibr" rid="c24">Cowen et al., 2014</xref>; <xref ref-type="bibr" rid="c81">VanRullen &amp; Reddy, 2019</xref>; <xref ref-type="bibr" rid="c60">Mozafari et al., 2020</xref>).</p>
<p>The extent to which the VLS allows linearly predicting the fMRI recordings does not provide insight into the representational geometries, i.e., the differences between the patterns of cerebral activity for speaker identity. We addressed this question by using representational similarity analysis (RSA; Kriegeskorte et al., 2008) to test which model better accounts for the representational geometry for voice identities in the auditory cortex. Using RSA as a model comparison framework is relevant to examining the brain-model relationship from complementary angles (<xref ref-type="bibr" rid="c27">Diedrichsen &amp; Kriegeskorte, 2017</xref>; <xref ref-type="bibr" rid="c32">Giordano et al., 2023</xref>; Tuckute &amp; Feather, 2023). We built speaker x speaker representational dissimilarity matrices (RDMs) capturing pairwise differences in cerebral activity or model predictions between all pairs of speakers; then, we examined how well the LIN and VLS-derived RDMs correlated with the cerebral RDMs from A1 and the TVAs.</p>
<p>A robust test of the adequacy of models of brain activity, and a long-standing goal in computational neurosciences, is the reconstruction of a stimulus presented to a participant from the evoked brain responses. While reconstruction of visual stimuli (images, videos) from cerebral activity has been performed by a number of groups (<xref ref-type="bibr" rid="c81">VanRullen &amp; Reddy, 2019</xref>; <xref ref-type="bibr" rid="c60">Mozafari et al., 2020</xref>; <xref ref-type="bibr" rid="c52">Le et al., 2022</xref>; <xref ref-type="bibr" rid="c31">Gaziv et al., 2022</xref>; <xref ref-type="bibr" rid="c25">Dado et al., 2022</xref>; <xref ref-type="bibr" rid="c23">Chen et al., 2023</xref>), validating the DNN-derived representational spaces, comparable work in the auditory domain is scarce, almost exclusively concentrated on linguistic information (<xref ref-type="bibr" rid="c71">Santoro et al., 2017</xref>). <xref ref-type="bibr" rid="c3">Akbari et al. (2019)</xref> used a DNN to reconstruct speech stimuli based on ECoG recording of auditory cortex activity, an invasive method compared to techniques like fMRI. They obtained a good phonetic recognition rate but chance-level gender categorization performance from reconstructed spectrograms and no evaluation of speaker identity discrimination.</p>
<p>Here, we built on the linear relationship uncovered in our encoding analysis between the VLS and the fMRI recordings to invert it and try to predict VLS coordinates from the recorded fMRI data; then, using the decoder, we reconstructed the spectrograms of stimuli presented to the participants (<xref ref-type="bibr" rid="c86">Wu et al., 2006</xref>; <xref ref-type="bibr" rid="c62">Naselaris et al., 2011</xref>). The voice identity information available in the reconstructed stimuli was finally assessed by human listeners using both machine learning classifiers and behavioral tasks (<xref rid="fig4" ref-type="fig">Fig. 4</xref>).</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title><bold>Voice Information in the Voice Latent Space (VLS)</bold></title>
<p>In order to probe the informational content of the VLS, linear classifiers were trained to categorize the voice stimuli from 405 speakers by gender (2 classes), age (2 classes) or identity (119 classes, cf Methods) based on VLS coordinates, or their LIN features as control (<xref rid="fig1" ref-type="fig">Fig. 1c,d,e</xref>; we aggregated the stimuli from the 3 participants; for each model computed the latent space of each stimulus and averaged the latent spaces by speaker identity, leading to 405 128-dimensional vectors. We then trained linear classifiers using a 5-fold cross-validation scheme, see <italic>Characterization of the autoencoder latent space</italic>). The mean of the distribution of accuracies obtained for 100 random classifier initializations (as to account for variance; <xref ref-type="bibr" rid="c15">Bouthillier et al., 2021</xref>) was significantly above chance level (all ps &lt; 1e-10) for all classifications (LIN: gender (mean accuracy ± s.d.) = 97.64±1.77%, t(99)=266.94; age: 64.39±4.54%, t(99)=31.53; identity: 40.52±9.14%, t(99)=39.37; VLS: gender: 98.59±1.19%, t(99)=406.47; age: 67.31±4.86%, t(99)=35.41; identity: 38.40±8.75%, t(99)=38.73). We then evaluated the difference in performance at preserving identity-related information between the VLS and LIN via one-way ANOVAs. Results showed a significant effect of Feature (LIN/VLS) in categories (all Fs(1, 198) &gt; 225.15, all ps&lt;.0001) but not in identity. Post-hoc paired t-tests showed that the VLS was better than the LIN at encoding information related to voice identity, as evidenced by a significant difference in means for gender (t(99)=-6.11, p&lt;.0001), age (t(99)=-6.10, p&lt;.0001) but not for identity classifications (t(99)=1.71).</p>
<p>Thus, despite its low number of dimensions (each input spectrogram has 401×21=8421 parameters and is summarized in the VLS by a mere 128 dimensions), the VLS appears to meaningfully represent the different sources of voice information perceptually available in the vocal stimuli. This representational space, therefore, constitutes a relevant candidate for linearly modeling voice stimulus representations by the brain.</p>
</sec>
<sec id="s2b">
<title><bold>Brain Encoding</bold></title>
<p>We used a linear voxel-based encoding model to test whether VLS linearly maps onto cerebral responses to speaker identities measured with fMRI in the different ROIs. A regularized linear regression model (cf. Methods) was trained on a subset of the data (5-fold cross-validation scheme) to predict the voxel maps for each speaker identity. For each fold, the trained model was tested on the held-out speaker identities (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). The model’s performance was assessed for each ROI using the Pearson correlation score between each voxel’s actual and predicted responses (<xref ref-type="bibr" rid="c72">Schrimpf et al., 2021</xref>). Similar predictions were tested with features derived from LIN (cf. Methods). <xref rid="fig2" ref-type="fig">Fig. 2b</xref> shows the distribution of correlation coefficients obtained for each of the ROIs for the 2 sets of features across voxels, hemispheres, and participants.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2.</label>
<caption><title>Predicting brain activity from the VLS.</title>
<p><bold>a,</bold> Linear brain activity prediction from VLS for ∼135 speaker identities in the different ROIs. We first fit a GLM to predict the BOLD responses to each voice speaker identity. Then, using the trained encoder, we computed the average VLS coordinates of the voice stimuli presented to the participants based on speaker identity. Finally, we trained a linear voxel-based encoding model to predict the speaker voxel activity maps from the speaker VLS coordinates. The cube illustrates the linear relationship between the fMRI responses to speaker identity and the VLS coordinates. The left face of the cube represents the activity of the voxels for each speaker’s identity, with each line corresponding to one speaker. The right face displays the VLS coordinates for each speaker’s identity. The cube’s top face shows the encoding model’s weight vectors. <bold>b,</bold> Encoding results. For each region of interest, the model’s performance was assessed using the Pearson correlation score between the true and the predicted responses of each voxel on the held-out speaker identities. Pearson’s correlation coefficients were computed for each voxel on the speakers’ axis and then averaged across hemispheres and participants. Similar predictions were tested with the LIN features. Error bars indicate the standard error of the mean (s.e.m) across voxels. *p &lt; 0.05; **p &lt; 0.01; **p &lt; 0.001; ****p &lt; 0.0001. <bold>c,</bold> Venn diagrams of the number of voxels in each ROI with the LIN, the VLS, or both models. For each ROI and each voxel, we checked whether the test correlation was higher than the median of all participant correlations (intersection circle), and if not, which model (LIN or VLS) yielded the highest correlation (left or right circles).</p></caption>
<graphic xlink:href="582302v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>One-sample t-tests showed that the means of Fisher z-transformed coefficients for both LIN features and VLS were significantly higher than zero (LIN: A1 t(197)=7.25, p&lt;.0001, pTVA t(175)=4.49, p&lt;.0001, mTVA t(164)=9.12, p&lt;.0001 and aTVA t(147)=6.81, p&lt;.0001; VLS: A1 t(197)=4.76, p&lt;.0001, mTVA t(164)=10.12, p&lt;.0001 and aTVA t(147)=5.52, p&lt;.0001 but not pTVA t(175)=-1.60) (Supplementary Tables 2-3).</p>
<p>A mixed ANOVA performed on the Fisher z-transformed coefficients with Feature (VLS, LIN) and ROI (A1, pTVA, mTVA, aTVA) as factors showed a significant effect of Feature (F(3, 683)=56.65, p&lt;.0001), a significant effect of ROI (F(3, 683)=18.50, p&lt;.0001), and a moderate interaction Feature x ROI (F(3, 683)=5.25, p&lt;.01). Post-hoc comparisons revealed that the mean of correlation coefficients was higher for LIN than for VLS in A1 (t(197)=4.02, p&lt;.0001), pTVA (t(175)=6.64, p&lt;.0001), aTVA (t(147)=3.78, p&lt;.001) but not in mTVA (t(164)=0.58) (Supplementary Table 4); and that the voxel patterns are better predicted in mTVA than in A1 for both models (LIN: t(361)=2.36, p&lt;.05); VLS: t(361)=4.91, p&lt;.0001) (Supplementary Table 5). However, inspecting the distribution of model-voxel correlations, we found that both models account for different parts of the voice identity responses and differ across ROIs (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>).</p>
</sec>
<sec id="s2c">
<title><bold>Representational Similarity Analysis</bold></title>
<p>For RSA, we built speaker x speaker representational dissimilarity matrices (RDMs), capturing for each ROI the dissimilarity in voxel space between each pair of speaker voxel maps (‘brain RDMs’; cf. Methods) using Pearson’s correlation (<xref ref-type="bibr" rid="c82">Walther et al., 2016</xref>). We compared these four bilateral brain RDMs (A1, aTVA, mTVA, pTVA) to two ‘model RDMs’ capturing speaker pairwise feature differences predicted by LIN and the VLS (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>) built using cosine distance (<xref ref-type="bibr" rid="c87">Xing et al., 2015</xref>; <xref ref-type="bibr" rid="c12">Bhattacharya et al., 2017</xref>; <xref ref-type="bibr" rid="c83">Wang et al., 2018</xref>). <xref rid="fig3" ref-type="fig">Fig. 3b</xref> shows for each ROI the Spearman correlation coefficients between the brain RDMs and the two model RDMs for each participant and hemisphere (Kriegeskorte et al., 2008; <xref rid="fig3" ref-type="fig">Fig. 3c</xref> for an example of brain-model correlation).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>The VLS better explains representational geometry for voice identities in the TVAs than the linear model.</title>
<p><bold>a,</bold> Representational dissimilarity matrices (RDMs) of pairwise speaker dissimilarities for ∼135 identities (arranged by gender, cf. sidebars), according to LIN and VLS. <bold>b,</bold> Spearman correlation coefficients between the brain RDMs for A1, the 3 TVAs, and the 2 model RDMs. Error bars indicate the standard error of the mean (s.e.m) across brain-model correlations. <bold>c,</bold> Example of brain-model RDM correlation in the TVAs. The VLS RDM and the brain RDM yielding one of the highest correlations (LaTVA) are shown in the insert.</p></caption>
<graphic xlink:href="582302v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>These brain-model correlation coefficients were compared to zero using a ‘maximum statistics’ approach based on random permutations of the model RDMs’ rows and columns (<xref ref-type="bibr" rid="c56">Maris &amp; Oostenveld, 2007</xref>; cf. Methods; <xref rid="fig3" ref-type="fig">Fig. 3b</xref>). For the LIN model, only one brain-model RDM correlation was significantly different from zero (one-tailed test): in mTVA, right hemisphere in S3 (p=.0500). For the VLS model, in contrast, 5 significant brain-model RDM correlations were observed in all four ROIs: in A1, right hemisphere in S3 (p=.0142); pTVA: right hemisphere in S3 (p=.0160); mTVA: left hemisphere in S3 (p=.007); aTVA: left hemispheres in S1 (p=.0417) and S3 (p=.0001) (Supplementary Table 6).</p>
<p>A two-way repeated-measures ANOVA with Feature (VLS, LIN) and ROI (A1, pTVA, mTVA, aTVA) as factors performed on the Fisher z-transformed correlation coefficients showed a tendency towards a significant effect of Feature (F(1, 2)=22.53, p=.04), and no ROI (F(3, 6)=1.79, p=.30) or interaction effects (F(3, 6)=1.94, p=.22). We compared the correlation coefficients between the VLS and LIN models within participants and hemispheres using one-tailed tests, based on the a priori hypothesis that the VLS models would exhibit greater brain-model correlations than the LIN models (cf. Methods). The results revealed two significant differences in one of the three participants, both favoring the VLS model (S3: right pTVA, p=.0366; left aTVA, p=.00175) (Supplementary Table 7).</p>
</sec>
<sec id="s2d">
<title><bold>Decoding and Reconstruction</bold></title>
<p>We finally inverted the brain-VLS relationship to predict linearly VLS coordinates based on fMRI measurements (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>; see ‘Brain decoding’ in Methods) and reconstructed via the trained decoder the spectrograms of 18 Test Stimuli (3 participants x 6 stimuli per participant; see <xref rid="fig4" ref-type="fig">Fig. 4b</xref>, and Supplementary Audio 2; audio estimated from spectrogram through phase reconstruction).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 4.</label>
<caption><title>Reconstructing voice identity from brain recordings.</title>
<p><bold>a,</bold> A linear voxel-based decoding model was used to predict the VLS coordinates of 18 Test Stimuli based on fMRI responses to ∼12,000 Train stimuli in the different ROIs. To reconstruct the audio stimuli from the brain recordings, the predicted VLS coordinates were then fed to the trained decoder to yield reconstructed spectrograms, synthesized into sound waveforms using the Griffin-Lim phase reconstruction algorithm (<xref ref-type="bibr" rid="c35">Griffin &amp; Lim, 1983</xref>). <bold>b,</bold> Reconstructed spectrograms of the stimuli presented to the participants. The left panels show the spectrogram of example original stimuli reconstructed from the VLS, and the right panels show brain-reconstructed spectrograms via LIN and the VLS (cf. Supplementary Audio 2).</p></caption>
<graphic xlink:href="582302v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We first assessed the nature of the reconstructed stimuli by using a DNN trained to categorize natural audio events (<xref ref-type="bibr" rid="c42">Howard et al., 2017</xref>): all reconstructed versions of the 18 Test Stimuli were categorized as ‘speech’ (1 class out of 521 - no ‘voice’ classes). To evaluate the preservation of voice identity information in the reconstructed voices, pre-trained linear classifiers were used to classify the speaker gender (2 classes), age (2 classes), and identity (17 classes; one identity was shared across participants) of the 18 reconstructed Test Stimuli. The mean of the accuracy distribution obtained across random classifier initializations (20 per ROI) used on the stimuli reconstructed from the induced brain activity was significantly above chance level for gender (LIN: pTVA (mean accuracy ± s.d.): 72.08±5.48, t(39)=25.15; VLS: A1: 61.11±2.15, t(39)=32.25; pTVA: 63.89±2.78, t(39)=31.22), age (LIN: pTVA: 54.58±4.14, t(39)=6.90; aTVA: 63.96±12.55, t(39)=6.94; VLS: pTVA: 65.00±7.26, t(39)=12.89; aTVA: 60.42±5.19, t(39)=12.54) and identity (LIN: A1: 9.20±9.23, t(39)=2.24; pTVA: 9.48±4.90, t(39)=4.59; aTVA: 9.41±6.28, t(39)=3.51; VLS: pTVA: 16.18±7.05, t(39)=9.11; aTVA: 8.23±4.70, t(39)=3.12) (<xref rid="fig5" ref-type="fig">Fig. 5a-c</xref>; Supplementary Tables 8-10).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 5.</label>
<caption><title>Behavioural and machine classification of the reconstructed stimuli.</title>
<p><bold>a,b,c,</bold> Decoding voice identity information in brain-reconstructed spectrograms. Performance of linear classifiers at categorizing speaker gender (chance level: 50%), age (chance level: 50%), and identity (17 identities, chance level: 5.88%). Error bars indicate s.e.m across 40 random classifier initializations per ROI (instance of classifiers; 2 hemispheres x 20 seeds). The horizontal black dashed line indicates the chance level. The blue and yellow dashed lines indicate the LIN and VLS ceiling levels, respectively. *p &lt; .05; **p &lt; .001, ***p &lt; .001; ****p &lt; .0001. <bold>d,e,f,</bold> Listener performance at categorizing speaker gender (chance level: 50%) and age (chance level: 50%), and at identity discrimination (2 forced choice task, chance level: 50%) in the brain-reconstructed stimuli. Error bars indicate s.e.m across participant scores. The horizontal black dashed line indicates the chance level, while the red, blue, and yellow dashed lines indicate the ceiling levels for the original stimuli, the LIN-reconstructed and the VLS-reconstructed, respectively. *p &lt; .05; **p &lt; .01; ***p &lt; .001, ***p &lt; .0001. <bold>g,</bold> Perceptual ratings of voice naturalness in the brain-reconstructed stimuli’ as assessed by human listeners, between 0 and 100 (zoomed between 5-80). *p &lt; .05, ****p &lt; .0001.</p></caption>
<graphic xlink:href="582302v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Two-way ANOVAs with Feature (VLS, LIN) and ROI (A1, pTVA, mTVA, aTVA) as factors performed on classification accuracy scores (gender, age, identity) revealed for gender classifications significant effects of Feature F(1, 312)=12.82, p&lt;.0005) and ROI (gender: F(3, 312)=245.06, p&lt;.0001; age: F(3, 312)=64.49, p&lt;.0001; identity: F(3, 312)=14.49, p&lt;.0001), as well as Feature x ROI interactions (gender: F(3, 312)=56.74, p&lt;.0001; age: F(3, 312)=4.31, p&lt;.001; identity: F(3, 312)=8.82, p&lt;.0001). Post-hoc paired t-tests indicated that the VLS was better than LIN in preserving gender, age and identity information in at least one TVA compared with A1 (gender: aTVA: t(39)=5.13, p&lt;.0001; age: pTVA: t(39)=9.78, p&lt;.0001; identity: pTVA: t(39)=4.01, p&lt;.0005) (all tests in Supplementary Table 11). Post-hoc two sample t-tests comparing ROIs revealed significant differences in all classifications, in particular with pTVA outperforming other ROIs in gender (LIN: pTVA vs A1: t(78)=22.40, p&lt;.0001; pTVA vs mTVA: t(78)=10.92, p&lt;.0001; pTVA vs aTVA: t(78)=31.47, p&lt;.0001; VLS: pTVA vs A1: t(78)=4.94, p&lt;.0001; pTVA vs mTVA: t(78)=13.96, p&lt;.0001; pTVA vs aTVA: t(78)=22.06, p&lt;.0001), age (LIN: pTVA vs A1: t(78)=7.26, p&lt;.0001; pTVA vs mTVA: t(78)=10.11, p&lt;.0001; VLS: pTVA vs A1: t(78)=5.71, p&lt;.0001; pTVA vs mTVA: t(78)=10.11, p&lt;.0001; pTVA vs aTVA: t(78)=3.21, p&lt;.005) and identity (LIN: pTVA vs mTVA: t(78)=2.27, p&lt;.05; VLS: pTVA vs A1: t(78)=6.45, p&lt;.0001; pTVA vs mTVA: t(78)=6.62, p&lt;.0001; pTVA vs aTVA: t(78)=5.85, p&lt;.0001) (Supplementary Table 12).</p>
<p>We further evaluated voice identity information in the reconstructed stimuli by testing human participants (n=13) in a series of 4 online experiments assessing the reconstructed stimuli on (i) naturalness judgment, (ii) gender categorization, (iii) age categorization, and (iv) speaker categorization (cf. Methods). The naturalness rating task showed that the VLS-reconstructed stimuli sounded more natural compared to LIN-reconstructed ones, as revealed by a two-way repeated-measures ANOVA (factors: Feature and ROI) with a strong effect of Feature (F(1, 12)=53.72, p&lt;.0001) and a small ROI x Feature interaction (F(3, 36)=5.36, p&lt;.005). Post-hoc paired t-tests confirmed the greater naturalness of VLS-reconstructed stimuli in both A1 and the TVAs (all ps&lt;.0001) (<xref rid="fig5" ref-type="fig">Fig. 5g</xref>). For the gender task, one-sample t-tests showed that categorization of the reconstructed stimuli was only significantly above chance level for the VLS (A1: (mean accuracy ± s.d.) 55.77±10.84, t(25)=2.66, p&lt;.01; pTVA: 61.75±7.11, t(25)=8.26, p&lt;.0001; aTVA: 55.13±9.23, t(25)=2.78, p&lt;.01). Regarding the age and speaker categorizations, results also indicated that both the LIN- and VLS-reconstructed stimuli yielded above-chance performance in the TVAs (age: LIN: aTVA, 55.77±14.95, t(25)=1.93, p&lt;.05; VLS: aTVA, 63.14±11.82, t(25)=5.56, p&lt;.0001; identity: LIN: pTVA: 54.38±9.34, t(17)=1.93, p&lt;.05; VLS: pTVA: 63.33±6.75, t(17)=8.14, p&lt;.0001) (Supplementary Tables 13-15). Two-way repeated-measures ANOVAs revealed a significant effect of ROI for all categories (gender: F(3, 27)=5.90, p&lt;.05; age: F(3, 36)=14.25, p&lt;.0001; identity: F(3, 24)=38.85, p&lt;.0001), and a Feature effect for gender (F(1, 9)=43.61, p&lt;.0001) and identity (F(1, 8)=14.07, p&lt;.001), but not for age (F(1, 12)=4.01, p=0.07), as well as a ROI x Feature interaction for identity discrimination (F(3, 24)=3.52, p&lt;.05) (Supplementary Tables 16-17 for the model and ROI comparisons).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study we examined to what extent the cerebral activity elicited by brief voice stimuli can be explained by machine-learned representational spaces, specifically focusing on identity-related information. We trained a linear model and a DNN model to reconstruct 100,000s of short voice samples from 100+ speakers, providing low-dimensional spaces (LIN and VLS), which we related to fMRI measures of cerebral response to thousands of these stimuli. We find: (i) that 128 dimensions are sufficient to explain a sizeable portion of the brain activity elicited by the voice samples and yield brain-based voice reconstructions that preserve identity-related information; (ii) that the DNN-derived VLS outperforms the LIN space, particularly in yielding more brain-like representational spaces and more naturalistic voice reconstructions; (iii) that different ROIs have different degrees of brain-model relationship, with marked differences between A1 and the a, m, and pTVAs.</p>
<p>Low-dimensional spaces generated by machine learning have been used to approximate cerebral face representations and reconstruct recognizable faces based on fMRI (<xref ref-type="bibr" rid="c81">VanRullen &amp; Reddy, 2019</xref>; <xref ref-type="bibr" rid="c25">Dado et al., 2022</xref>). In the auditory domain, however, they have mainly been used with a focus on linguistic (speech) information, ignoring identity-related information (but see <xref ref-type="bibr" rid="c3">Akbari et al., 2019</xref>). Here, we applied them to brief voice stimuli–with minimal linguistic content but already rich identity-related information–and found that as little as 128 dimensions account reasonably well for the complexity of cerebral responses to thousands of these voice samples as measured by fMRI (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). LIN and VLS both showed brain-like representational geometries, particularly the VLS in the aTVAs (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). They made possible what is, to our knowledge, the first fMRI-based voice reconstructions to preserve voice-related identity information such as gender, age, or even individual identity, as indicated by above-chance categorization or discrimination performance by both machine classifiers (<xref rid="fig5" ref-type="fig">Fig. 5a-c</xref>) and human listeners (<xref rid="fig5" ref-type="fig">Fig. 5d-f</xref>).</p>
<p>Estimation of fMRI responses (encoding) by LIN yielded correlations largely comparable to those by VLS (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>), although many voxels were only explained by one or the other space (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). However, in the RSA, VLS yielded higher overall correlations with brain RDMs (<xref rid="fig3" ref-type="fig">Fig. 3</xref>), suggesting a representational geometry closer to that instantiated in the brain than LIN. Further, VLS-reconstructed stimuli sounded more natural than the LIN-reconstructed ones (<xref rid="fig5" ref-type="fig">Fig. 5g</xref>) and yielded both the best speaker discrimination by listeners (<xref rid="fig5" ref-type="fig">Fig. 5f</xref>) and speaker classification by machine classifiers (<xref rid="fig5" ref-type="fig">Fig. 5c</xref>). Unlike LIN, which was generated via linear transforms, VLS was obtained through a series of nonlinear transformations (<xref ref-type="bibr" rid="c84">Wetzel, 2017</xref>). The fact that the VLS outperforms LIN in decoding performance indicates that nonlinear transformation is required to better account for the brain representation of voices (<xref ref-type="bibr" rid="c62">Naselaris et al., 2011</xref>; <xref ref-type="bibr" rid="c24">Cowen et al., 2014</xref>; <xref ref-type="bibr" rid="c38">Han et al., 2019</xref>).</p>
<p>Comparisons between ROIs revealed important differences between A1 and the a, m, and pTVAs. For both LIN and VLS, fMRI signal (encoding) predictions were more accurate for the mTVAs than for A1, and for A1 than for the pTVAs (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>). The aTVAs yielded the highest correlations with the models in the RSA (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). Stimulus reconstructions (<xref rid="fig4" ref-type="fig">Fig. 4</xref>) based on the TVAs also yielded better gender, age, and identity classification than those based on A1, with gender and identity best preserved in the pTVA-, and to a lesser extent, in the aTVA-based reconstructions (<xref rid="fig5" ref-type="fig">Fig. 5</xref>). These results show that the a and pTVAs not only respond more strongly to vocal sounds than A1, but they also represent identity-related information in voice better than mTVA, which was previously anticipated in some neuroimaging studies (Gender: <xref ref-type="bibr" rid="c21">Charest et al., 2013</xref>; Identity: <xref ref-type="bibr" rid="c10">Belin &amp; Zatorre, 2003</xref>; <xref ref-type="bibr" rid="c55">Maguinness et al., 2018</xref>; <xref ref-type="bibr" rid="c69">Roswandowitz et al., 2018</xref>; <xref ref-type="bibr" rid="c2">Aglieri et al., 2021</xref>). Moreover, several recent studies, using intracranial recordings, either through ECoG electrode grids (<xref ref-type="bibr" rid="c90">Zhang et al., 2021</xref>) or sEEG recordings (<xref ref-type="bibr" rid="c70">Rupp et al., 2022</xref>), found evidence that supports the idea of a hierarchical organization of voice patches in the temporal lobe, where the information flow starts from the mTVA patches and moves in two directions: one from mTVA to the anterior TVA (aTVA) and the other one from mTVA to posterior TVA (pTVA).</p>
<p>Overall, we show that a DNN-derived representational space provides an interesting approximation of the cerebral representations of brief voice stimuli that can preserve identity-related information. We find it remarkable that such results could be obtained to explain sound representations despite the poor temporal resolution of fMRI. Future work combining more complex architectures to time-resolved measures of cerebral activity, such as magneto-encephalography (<xref ref-type="bibr" rid="c26">Défossez et al., 2023</xref>) or ECoG (<xref ref-type="bibr" rid="c63">Pasley et al., 2012</xref>), will likely yield better models of the cerebral representations of voice information.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Experimental procedure overview</title>
<p>Three participants attended 13 MRI sessions each. The first session was dedicated to acquire high-resolution structural data, as well as to identify the voice-selective areas of each participant using a ‘voice localizer’ based on different stimuli than those in the same experiment (<xref ref-type="bibr" rid="c67">Pernet et al., 2015</xref>; see below).</p>
<p>Functional scanning was done using a rapid event-related design with a jittered inter-stimulus-interval (2.8-3.2 s). The next 12 sessions began with the acquisition of two fast structural scans for inter-session realignment purposes, followed by six functional runs, during which the main stimulus set of the experiment was presented. Each functional run lasted approximately 12 minutes. Participants 1 and 2 attended all scanning sessions (72 functional runs in total); due to technical issues, Participant 3 only performed 24 runs.</p>
<p>Participants were instructed to stay in the scanner while listening to the stimuli. To maintain participants’ awareness during functional scanning, they were asked to press an MRI-compatible button each time they heard the same stimulus two times in a row, a rare event occurring 3% of the time (correct button hits (median accuracy ± s.d.): S1=96.67±7.10, S2=100.00±0.89, S3=95.00±3.68).</p>
<p>Scanning sessions were spaced by at least two days to avoid possible auditory fatigue due to the exposure to scanner noise. To ensure that participants’ hearing abilities did not vary across scanning sessions, hearing thresholds were measured before each session using a standard audiometric procedure (<xref ref-type="bibr" rid="c57">Martin &amp; Champlin, 2000</xref>; ISO 2004) and compared with the thresholds obtained prior the first session.</p>
</sec>
<sec id="s4b">
<title>Participants</title>
<p>This study was part of the project ‘Réseaux du Langage’ and was promoted by the National Center for Scientific Research (CNRS). It has been given favorable approval by the local ethics committee (Comité de Protection des Personnes Sud-Méditerranée) on the date of 13th February 2019. The National Agency for Medicines (ANSM) has been informed of this study, which is registered under the number 2017-A03614-49. Three native French human speakers were scanned (all females; 26-33 years old). Participants gave written informed consent and received a compensation of 40€ per hour for their participation. All were right-handed and no one had hearing disorder or neurological disease. All participants had normal hearing thresholds of 15 dB HL, for octave frequencies between 0.125 and 8 kHz.</p>
</sec>
<sec id="s4c">
<title>Stimuli</title>
<p>The auditory stimuli were divided into two sequences. One ‘voice localizer’ sequence to identify the voice-selective areas of each participant (<xref ref-type="bibr" rid="c67">Pernet et al., 2015</xref>) and a main voice stimuli.</p>
<sec id="s4c1">
<title>Voice localizer stimuli</title>
<p>The voice localizer stimuli consisted of 96 complex sounds of 500ms grouped in four categories of human voice, macaque vocalizations, marmoset vocalizations, and complex non-vocal sounds (more details in <xref ref-type="bibr" rid="c14">Bodin et al., 2021</xref>).</p>
</sec>
<sec id="s4c2">
<title>Main voice stimuli</title>
<p>The main stimulus set consisted of brief human voice sounds sampled from the Common Voice dataset (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>). Stimuli were organized into four main category levels: language (English, French, Spanish, Deutch, Polish, Portuguese, Russian, Chinese), gender (female/male), age (young/adult; young: teenagers and twenties; adult: thirties to sixties included) and identity (S1: 135 identities; S2: 142 identities; S3: 128 identities; ∼44 samples per identity). Throughout the manuscript, the term ’gender’ rather than ‘sex’ was utilized in reference to the demographic information obtained from the participants of the Common Voice dataset (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>), as it was the terminology employed in the survey (‘male/female/other’). Stimulus sets were different for each participant and the number of stimuli per set also varied slightly (number of unique stimuli: Participant 1, N=6150; Participant 2, N=6148; Participant 3, N=5123). For each participant, six stimuli were selected randomly among the sounds having a high energy (as measured with the amplitude envelope) from their stimulus set and were repeated extensively (60 times), to improve the performance of the brain decoding (<xref ref-type="bibr" rid="c81">VanRullen &amp; Reddy, 2019</xref>; <xref ref-type="bibr" rid="c41">Horikawa &amp; Kamitani, 2017</xref>; <xref ref-type="bibr" rid="c20">Chang et al., 2019</xref>); these will be called the “repeated” stimuli hereafter, the remaining stimuli were presented twice. The third participant attended 5 BrainVoice sessions instead of 12, one BrainVoice session corresponding to 1030 stimuli (1024 unique stimuli and 6 ‘test’ stimuli). Specifically, 5270 stimuli were presented to the third participant instead of ∼12,000 for the two others. Among these 5270 stimuli, 5120 unique stimuli were presented once, as for the two other participants, 6 ‘test’ stimuli were presented 25 times (150 trials). The stimuli were balanced within each run according to language, gender, age, and identity, as to avoid any potential adaptation effect. In addition, identity was balanced across sessions.</p>
<p>All stimuli of the main set were resampled at 24414 Hz and adjusted in duration (250 ms). For each stimulus, a fade-in and a fade-out were applied with a 15 ms cosine ramp to their onset and offset, and were normalized by dividing the root mean square amplitude. During fMRI sessions, stimulus presentations were controlled using custom Matlab scripts (Mathworks, Natick, MA, USA) interfaced with an RM1 Mobile Processor (Tucker-David Technologies, Alachua, USA). The auditory stimuli were delivered pseudo-randomly through MRI-compatible earphones (S14, SensiMetrics, USA) at a comfortable sound pressure level that allowed for clear and intelligible listening.</p>
</sec>
</sec>
<sec id="s4d">
<title>Computational models</title>
<p>We used two computational models to learn representational space for voice signals, Linear Autoencoder (LIN) and Deep Variational Autoencoder (VAE; <xref ref-type="bibr" rid="c46">Kingma &amp; Welling., 2014</xref>). Both are encoder-decoder models that are learnt to reproduce at their output their input while going through a low dimensional representation space usually called latent space (that we will call <italic>voice latent space</italic> since they are learnt on voice data). The autoencoders were trained on a dataset of 182K sounds from the Common Voice dataset (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>), balanced in gender, language and identity to reduce the bias in the synthesis (Gutierrez et al., 2021). Both models operate on sounds which were represented as spectrograms that we describe below. These representations were tested in all the encoding/decoding and RSA analyses.</p>
</sec>
<sec id="s4e">
<title>Spectrograms</title>
<p>We used amplitude spectrograms as input of the models that we describe below. Short term Fourier transforms of the waveform were computed using a sliding window of length 50 ms with a hop size of 12.5 ms (hence an overlap of 37.5 ms) and applying a Hamming window of size 800 samples before computing the Fourier transform of each slice. Only the magnitude of the spectrogram was kept and the phase of the complex representation was removed. At the end, a 250 ms sound is represented by a 21×401 matrix with 21 time steps and 401 frequency bins.</p>
<p>We used a custom code based on <italic>numpy</italic>. <italic>fft</italic> package (<xref ref-type="bibr" rid="c39">Harris et al., 2020</xref>). The size and the overlap between the sliding windows of the spectrogram were chosen to conform with the uncertainty principle between time and frequency resolution. The main constraint was to find a trade-off between accurate phase reconstruction with the Griffin &amp; Lim algorithm (1983) and a reasonable size of the spectrogram.</p>
<p>We standardized each of the 401 frequency bands separately, by centering all the data corresponding to each frequency band at every time step in all spectrograms, which involved removing their mean, and dividing by their standard deviation. This separate standardization of frequency bands resulted in a smaller reconstruction error compared to standardizing across all the bands.</p>
</sec>
<sec id="s4f">
<title>Deep neural network</title>
<p>We designed a deep variational autoencoder (VAE; <xref ref-type="bibr" rid="c46">Kingma &amp; Welling, 2014</xref>) of 15 layers with an intermediate hidden representation of 128 neurons that we refer to as the <italic>voice latent space</italic> (VLS). In an autoencoder model, the two sub-network components, the <italic>Encoder</italic> and the <italic>Decoder</italic>, are jointly learned on complementary tasks (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). The Encoder network (noted <italic>Enc</italic> hereafter; 7 layers) learns to map an input, <italic>s</italic> (a spectrogram of a sound), onto a (128-dimensional) <italic>voice latent space</italic> representation (<italic>z</italic>; in blue in the middle of <xref rid="fig1" ref-type="fig">Fig. 1a</xref>), while the Decoder (noted <italic>Dec</italic> hereafter; 7 layers) aims at reconstructing the spectrogram <italic>s</italic> from <italic>z</italic>. The learning objective of the full model is to make the output spectrogram <italic>Dec</italic>(<italic>Enc</italic>(<italic>s</italic>)) as close as possible to the original one <italic>s</italic>. This reconstruction objective is defined as the L2 loss, ||<italic>Dec</italic>(<italic>Enc</italic>(<italic>s</italic>)) − <italic>s</italic>||². The parameters of the Encoder and of the Decoder are jointly learned using gradient descent to optimize the average L2 loss computed on the training set ∑<sub><italic>s</italic> ∈<italic>Training Set</italic></sub> ||<italic>Dec</italic>(<italic>Enc</italic>(<italic>s</italic>)) − <italic>s</italic>||². We trained this DNN on the Common Voice dataset (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>) according to VAE learning procedure (as explained in <xref ref-type="bibr" rid="c47">Kingma &amp; Welling., 2019</xref>) until convergence (network architecture and particularities of the training procedure are provided in Supplementary Table 1), using the PyTorch python package (<xref ref-type="bibr" rid="c64">Paszke et al., 2019</xref>).</p>
</sec>
<sec id="s4g">
<title>Linear autoencoder</title>
<p>We trained a linear autoencoder on the same dataset (described above) to serve as a linear baseline. Both the <italic>Encoder</italic> and the <italic>Decoder</italic> networks consisted of a single fully-connected layer, without any activation functions. Similar to the VAE, the latent space obtained from the <italic>Encoder</italic> was a 128-dimensional vector. The parameters of both the <italic>Encoder</italic> and of the <italic>Decoder</italic> were jointly learned using gradient descent to optimize the average L2 loss computed on the training set.</p>
</sec>
<sec id="s4h">
<title>Neuroimaging data acquisition</title>
<p>Participants were scanned using a 3 Tesla Prisma scanner (Siemens Healthcare, Erlangen, Germany) equipped with a 64-channel receiver head-coil. Their movements were monitored during the acquisition using the software FIRMM (<xref ref-type="bibr" rid="c28">Dosenbach et al., 2017</xref>). The whole-head high-resolution structural scan acquired during the first session was a T1-weighted multi-echo MPRAGE (MEMPRAGE) (TR = 2.5 s, TE = 2.53, 4.28, 6.07, 7.86 ms, TI=1000 ms flip angle: 8°, matrix size = 208 × 300 × 320; resolution 0.8 × 0.8 × 0.8 mm<sup>3</sup>, acquisition time: 8min22s). Lower resolution scans acquired during all other sessions were T1-weighted MPRAGE scans (TR = 2.3 s, TE = 2.88 ms, TI=900ms, flip angle: 9°, matrix size = 192 × 240 × 256; resolution 1 × 1 × 1 mm<sup>3</sup>, sparse sampling with 2.8 times undersampling and compressed sensing reconstruction, acquisition time: 2min37). Functional imaging was performed using an EPI sequence (multiband factor = 5, TR = 462 ms, TE = 31.2 ms, flip angle: 45°, matrix size = 84 × 84 × 35, resolution 2.5 × 2.5 × 2.5 mm<sup>3</sup>). Functional slices were oriented parallel to the lateral sulci with a z-axis coverage of 87.5 mm, allowing it to fully cover both the TVAs (<xref ref-type="bibr" rid="c67">Pernet et al., 2015</xref>) and the FVAs (Aglieri et al., 2018). The physiological signals (heart rate and respiration) were measured with the external sensors of Siemens.</p>
</sec>
<sec id="s4i">
<title>Pre-processing of neuroimaging data and general linear modeling</title>
<p>Tissue segmentation and brain extraction was performed on the structural scans using the default segmentation procedure of SPM 12 (Ashburner et al., 2012). The preprocessing of the BOLD responses involved correcting motion, registering inter-runs, detrending and smoothing the data. Each functional volume was realigned to a reference volume taken from a steady period in the session that was spatially the closest to the average of all sessions.</p>
<p>Transformation matrices between anatomical and functional data were computed using boundary-based registration (FSL; <xref ref-type="bibr" rid="c76">Smith et al., 2004</xref>). The data were respectively detrended and smoothed using the <italic>nilearn</italic> functions <italic>clean</italic>_<italic>img</italic> and <italic>smooth</italic>_<italic>img</italic> (kernel size of 3mm) (<xref ref-type="bibr" rid="c1">Abraham et al., 2014</xref>), resulting in the matrix <italic>Y</italic> ∈ <italic>R</italic><sup><italic>S</italic></sup> <sup>×</sup> <sup><italic>V</italic></sup>, with <italic>S</italic> the number of scans and <italic>V</italic> the number of voxels.</p>
<p>A first general linear model (GLM) was fit to regress out the noise by predicting <italic>Y</italic> from a “denoised” design matrix, composed of <italic>R</italic> = 38 regressors of nuisance (Supplementary Figure 3). These regressors of nuisance, also called covariates of no interest, included: 6 head motion parameters (3 variable for the translations, 3 variables for the rotations); 18 ‘RETROICOR’ regressors (<xref ref-type="bibr" rid="c33">Glover et al., 2000</xref>) using the <italic>TAPAS PhysIO</italic> package (<xref ref-type="bibr" rid="c43">Kasper et al., 2017</xref>) (with the hyperparameters set as specified in Snoek et al.) were computed from the physiological signals; 13 regressors modeling slow artifactual trends (sines and cosines, cut frequency of the high-pass filter = 0.01 Hz); and a confound-mean predictor. The design matrix was convolved with an hemodynamic response function (HRF) with a peak at 6 s and an undershoot at 16 s (Glover et al., 1999), we note the convolved design matrix as <italic>X</italic><sub><italic>d</italic></sub> ∈ <italic>R</italic><sup><italic>S</italic></sup> <sup>×</sup> <sup><italic>R</italic></sup>. The “denoise” GLM’s parameters <italic>β</italic><sub><italic>d</italic></sub> ∈ <italic>R</italic><sup><italic>R</italic></sup> <sup>×<italic>V</italic></sup> were optimized to minimize the amplitude of the residual <italic>β</italic><sub><italic>d</italic></sub> = <italic>argmin</italic><sub><italic>β</italic>∈<italic>R</italic></sub><sup><italic>R</italic> ×<italic>V</italic></sup> || <italic>Y</italic> − <italic>X</italic><sub><italic>d</italic></sub> <italic>β</italic> ||<sup>2</sup>. We used a lag-1 autoregressive model (ar(1)) to model the temporal structure of the noise (<xref ref-type="bibr" rid="c29">Friston et al., 2002</xref>). The <italic>denoised</italic> BOLD signal <italic>Y</italic><sub><italic>d</italic></sub> was then obtained from the original one according to <italic>Y</italic><sub><italic>d</italic></sub> = <italic>Y</italic> − (<italic>X</italic><sub><italic>d</italic></sub> <italic>β</italic><sub><italic>d</italic></sub>) ∈ <italic>R</italic><sup><italic>S</italic></sup> <sup>×<italic>V</italic></sup>.</p>
<p>A second “stimulus” GLM model was used to predict the denoised BOLD responses for each stimulus using a design matrix <italic>X</italic><sub><italic>s</italic></sub> ∈ <italic>R</italic> <sup><italic>S</italic>×(<italic>N</italic><sub><italic>S</italic></sub>+1)</sup> (which was convolved with an hemodynamic response function, HRF as above) and a parameters matrix <italic>β</italic><sub><italic>s</italic></sub> ∈ <italic>R</italic> <sup>(<italic>N</italic></sup><sub><italic>s</italic></sub><sup>+1)×<italic>V</italic></sup> where <italic>N</italic><sub><italic>S</italic></sub> stands for the number of stimuli. The last row (resp. column) of <italic>β</italic><sub><italic>s</italic></sub> (resp. <italic>X</italic><sub><italic>s</italic></sub>) stands for a silence condition. Again, <italic>β</italic><sub><italic>s</italic></sub> was learned to minimize the residual <italic>β</italic><sub><italic>s</italic></sub> = <italic>argmin</italic><sub><italic>β</italic>∈<italic>R</italic></sub> (<italic>Ns</italic>+1)×<italic>V</italic> || <italic>Y</italic><sub><italic>d</italic></sub> − <italic>X</italic><sub><italic>s</italic></sub> <italic>β</italic> ||<sup>2</sup>. Once learned, each of the first <italic>N</italic><sub><italic>s</italic></sub> line of <italic>β</italic><sub><italic>s</italic></sub> was corrected by subtracting the <italic>(N</italic><sub><italic>s</italic></sub><italic>+1)<sup>th</sup></italic> line, yielding the contrast maps for stimuli <inline-formula><inline-graphic xlink:href="582302v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We note hereafter <inline-formula><inline-graphic xlink:href="582302v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> <italic>R</italic> <sup><italic>V</italic></sup> the contrast map for a given stimulus, it is the <italic>i <sup>th</sup></italic> line of <inline-formula><inline-graphic xlink:href="582302v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
<p>A third “identity” GLM was fit to predict the BOLD responses of each voice speaker identity, using a design matrix <italic>β</italic><sub><italic>i</italic></sub> ∈ <italic>R</italic> <sup>(<italic>N</italic></sup><sub><italic>i</italic></sub><sup>+1)×<italic>V</italic></sup> and a design matrix <italic>X</italic><sub><italic>i</italic></sub> ∈ <italic>R</italic> <sup><italic>S</italic>×(<italic>Ni</italic> +1)</sup> (which was again convolved with an hemodynamic response function, HRF) where <italic>N</italic><sub><italic>s</italic></sub> stands for the number of unique speakers. Again the last row/column in <italic>β</italic><sub><italic>i</italic></sub> and <italic>X</italic><sub><italic>i</italic></sub> stands for the silent condition. <italic>β</italic><sub><italic>i</italic></sub> is learned to minimize the residual <italic>β</italic><sub><italic>i</italic></sub> = <italic>argmin</italic><sub><italic>β</italic>∈<italic>R</italic></sub> <sup>(<italic>Ni</italic>+1)×<italic>V</italic></sup> ||<italic>Y</italic><sub><italic>d</italic></sub> − <italic>X</italic><sub><italic>i</italic></sub> <italic>β</italic> ||<sup>2</sup> (Supplementary Figure 2a). Again, the final speaker contrast maps were obtained by contrasting (i.e., subtracting) the regression coefficients in a row of <italic>β</italic><sub><italic>i</italic></sub> with the silence condition (last row; Supplementary Figure 2a), yielding <inline-formula><inline-graphic xlink:href="582302v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Here the <italic>j<sup>th</sup></italic> row of <inline-formula><inline-graphic xlink:href="582302v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, represents the amplitude of the BOLD response of the contrast map for speaker <italic>j</italic> (i.e. to all the stimuli from this speaker).</p>
<p>A fourth “localizer” GLM model was used to predict the denoised BOLD responses of each sound category from the <italic>Voice localizer stimuli</italic> presented above. The procedure was similar as described for the two previous GLM models. Once the GLM was learned, we contrasted the human voice category with the other sound categories in order to localize for each participant the posterior Temporal Voice Area (pTVA), medial Temporal Voice Area (mTVA) and anterior Temporal Voice Area (aTVA) in each hemisphere. The center of each TVA corresponded to the local maximum of the voice &gt; non voice t-map whose coordinates were the closest to the TVAs reported in (<xref ref-type="bibr" rid="c67">Pernet et al., 2015</xref>). The analyses were carried on for each region of interest (ROI) of each hemisphere.</p>
<p>Additionally, we defined for each participant the primary auditory cortex (A1) as the maximum value of the probabilistic map (non-linearly registered to each participant functional space) of Heschl’s gyri provided with the MNI152 template (<xref ref-type="bibr" rid="c66">Penhune et al., 1996</xref>), intersected with the sound vs silence contrast map.</p>
</sec>
<sec id="s4j">
<title>Identity-based and stimulus-based representations</title>
<p>We performed analyses either at the stimulus level, e.g. predicting the neural activity of a participant listening to a given <italic>stimulus</italic> (<inline-formula><inline-graphic xlink:href="582302v2_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> lines) from the <italic>voice latent space</italic> representation of this stimuli, or at the speaker identity level, e.g. predicting the average neural activity in response to stimuli of a given speaker identity (<inline-formula><inline-graphic xlink:href="582302v2_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> lines) from this speaker’s <italic>voice latent space</italic> representation. The identity-based analyses were used for the characterization of the <italic>voice latent space</italic> (<xref rid="fig1" ref-type="fig">Fig. 1</xref>), the brain encoding (<xref rid="fig2" ref-type="fig">Fig. 2</xref>), and the representational similarity analysis (<xref rid="fig3" ref-type="fig">Fig. 3</xref>), while the stimulus-based analyses were used for the brain decoding analyses (<xref rid="fig4" ref-type="fig">Fig. 4</xref>, 5).</p>
<p>We conducted stimulus-based analyses to examine the relationship between stimulus contrast maps in neural activity <inline-formula><inline-graphic xlink:href="582302v2_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and the encodings of individual stimulus spectrograms computed by the encoder of an autoencoder model (either linear or deep variational autoencoder) on the computational side. We will note <italic>z</italic><sub><italic>s</italic></sub><sup><italic>lin</italic></sup> ∈ <italic>R</italic><sup><italic>N</italic></sup><sub><italic>s</italic></sub><sup>×128</sup> encodings of stimuli by the LIN model and <italic>z</italic><sub><italic>s</italic></sub><sup><italic>vae</italic></sup> ∈ <italic>R</italic><sup><italic>N</italic></sup><sub><italic>s</italic></sub><sup>×128</sup> the encodings of stimuli computed by the VAE model. The encoding of the k<sup>th</sup> stimuli by one of these models is the k<sup>th</sup> row of the corresponding matrix and it is noted as <italic>z</italic><sub><italic>s</italic></sub><sup><italic>model</italic></sup>[<italic>k</italic>,:].</p>
<p>For identity-based analyses we studied relationships between identity contrast maps in <inline-formula><inline-graphic xlink:href="582302v2_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> on the neural activity side, and an encoding of speaker identity in the VLS implemented by an autoencoder model (LIN or VAE) on the computational side, e.g. we note <italic>z</italic><sub><italic>i</italic></sub><sup><italic>vae</italic></sup>[<italic>j</italic>] the representation of speaker <italic>j</italic> as computed by the <italic>vae</italic> model. We chose to define a speaker identity-based representation as the average of a set of sample-based representations for stimuli from this speaker, e.g. <italic>z</italic><sub><italic>i</italic></sub><sup><italic>model</italic></sup>[<italic>j</italic>] = 1/|<italic>S</italic><sub><italic>j</italic></sub>| ∑<sub><italic>k</italic> ∈<italic>S</italic></sub> <italic>z</italic><sub><italic>s</italic></sub><sup><italic>model</italic></sup>[<italic>k</italic>,:] where <italic>S</italic><sub><italic>j</italic></sub> stands for the set of stimuli by speaker <italic>j</italic> and <italic>model</italic> stands for <italic>vae</italic> or <italic>lin</italic>. Averaging in the <italic>voice latent space</italic> is expected to be much more powerful and relevant than averaging in the input space spectrograms (<xref ref-type="bibr" rid="c81">VanRullen &amp; Reddy, 2019</xref>).</p>
</sec>
<sec id="s4k">
<title>Characterization of the autoencoder latent space</title>
<p>We characterized the organization of the <italic>voice latent space</italic> (VLS) and of the features computed by the linear autoencoder (LIN) by measuring through classification experiments the presence of information about speaker’s gender, age, and identity in the representations learned by these models.</p>
<p>We first computed the speaker’s identity <italic>voice latent space</italic> representations for each of the 405 speakers in the main voice dataset (135+142+128 see <italic>Stimuli</italic> section) as explained above.</p>
<p>Next we used these speakers’ <italic>voice latent space</italic> representation to investigate if the gender, age, identity were encoded in the VLS. To do so we divided the data in separate train and test sets and learned classifiers to predict gender, age, or identity from the train set. The balanced (to avoid the small effects associated with unbalanced folds) accuracy of the classifiers were then evaluated on the test set. The higher the performance on the test set the more we are confident that the information is encoded in the VLS. More specifically for each task (gender, age, identity), we trained a Logistic Regression classifier (linear regularized logistic regression; L2 penalty, tol=0.0001, fit_intercept=True, intercept_scaling=1, max_iter=100) using the scikit-learn python package (<xref ref-type="bibr" rid="c65">Pedregosa et al., 2018</xref>).</p>
<p>In order to statistically evaluate the significance of the results and to avoid a potential overfitting, the classifications were repeated 20 times with 20 different initializations (<italic>seed</italic>) and the metrics were then averaged for each voice category (gender, age). More specifically, we repeated the following experiment 20 times with 20 different random seeds. For each seed, we performed 5 train-test splits with 80% of the data in the training and 20% in the test set. For each split we used 5-fold cross validation on the training set to select the optimal value for the regularization hyperparameter C (searching between 10 values logarithmically spaced on the interval [-3, +3]). We then computed the generalization performance on the test set of the model trained on the full training set with the best hyperparameter value. Reported results were then averaged over 20 experiments. Note that data were systematically normalized with a scaler fitted on the training set. We used a robust scaling strategy for these experiments (removing the median, then scaling to the quantile range; 25<sup>th</sup> quantile and 75<sup>th</sup> quantile) which occurs to be more relevant with a small training set.</p>
<p>To investigate how speaker identity information is encoded in the latent space representations of speakers’ voices, we computed speaker identity <italic>voice latent space</italic> representations by averaging 20 stimulus-based representations, in order to obtain a limited amount of data per identity that could be distributed across training and test datasets.</p>
<p>We first tested whether the mean of the distribution of accuracy scores obtained for 20 seeds was significantly above chance level using one-sample t-tests. We then evaluated the difference in classification accuracy between the VLS and LIN via one-way ANOVAs (dependent variable: test balance accuracy; between factor: Feature), for each category (speaker gender, age, identity). We performed post-hoc planned paired t-tests between the models to test the significance of the VLS-LIN difference.</p>
</sec>
<sec id="s4l">
<title>Brain encoding</title>
<p>We performed encoding experiments on identity-based representations for each of the three participants (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). For each participant we explored the ability to learn a regularized linear regression that predicts a speaker-based neural activity, e.g. the <italic>j<sup>th</sup></italic>speaker’s contrast map <inline-formula><inline-graphic xlink:href="582302v2_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, from this speaker’s voice latent space representation, that we note <italic>z</italic><sub><italic>i</italic></sub><sup><italic>model</italic></sup>[<italic>j</italic>] ∈ <italic>R</italic><sup>128</sup> (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). We carried out these regression analyses for each ROI (A1, pTVA, mTVA, aTVA) in each hemisphere and participant, independently.</p>
<p>The regression model parameters <inline-formula><inline-graphic xlink:href="582302v2_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> were learned according to:
<disp-formula>
<graphic xlink:href="582302v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>λ</italic> is a hyperparameter tuning the optimal tradeoff between the data fit and the penalization terms above. We used the ridge regression with built-in cross-validation as implemented as <italic>RidgeCV</italic> in the scikit-learn library (<xref ref-type="bibr" rid="c65">Pedregosa et al., 2018</xref>).</p>
<p>The statistical significance of each result was assessed with the following procedure. We repeated the following experiment 20 times with different random seeds. Each time, we performed 5 train-test splits with 80% of the data in the training and 20% in the test set. For each split we used RidgeCV (relying on leave-one-out) on the training set to select the optimal value for the hyperparameter <italic>λ</italic> (searching between 10 values logarithmically spaced on the interval [10<sup>−1</sup>; 10<sup>8</sup>]). Following standard practice in machine learning, we then computed the generalization performance on the test set of the model trained on the full training set with the best hyperparameter value. Reported results are then averaged over 20 experiments. Note that here again with small training sets data were systematically normalized in each experiment using robust scaling.</p>
<p>Evaluation relied on the ‘brain score’-inspired procedure (<xref ref-type="bibr" rid="c73">Schrimpf et al., 2018</xref>) which evaluates the performance of the ridge regression with a Pearson’s correlation score. Correlations between measured neural activities <inline-formula><inline-graphic xlink:href="582302v2_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and predicted ones <inline-formula><inline-graphic xlink:href="582302v2_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> were computed for each voxel and averaged over repeated experiments (folds and seeds) yielding one correlation value for every voxel and for every setting. The significance of the results was assessed with one-sample t-tests for the Fisher z-transformed correlation scores (3 x participants x 2 hemispheres x V voxels). For each region of interest, the scores are reported across participants and hemispheres (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>). The exact same procedure was followed for the LIN modeling.</p>
<p>In order to determine which of the two feature spaces (VLS, LIN) and which of the two ROI (A1, TVAs) yielded the best prediction of neural activity, we compared the means of distributions of correlations coefficients using a mixed ANOVA performed on the Fisher z-transformed coefficients (dependent variable: correlation; between factor: ROI; repeated measurements: Feature; between-participant identifier: voxel).</p>
<p>For each ROI, we then used t-tests to perform post-hoc contrasts for the VLS-LIN difference in brain encoding performance (comparison tests in <xref rid="fig2" ref-type="fig">Fig. 2b</xref>; Supplementary Table 4). We finally conducted two-sample t-tests between the brain encoding model’s scores trained to predict A1 and those trained to predict temporal voice areas to test the significance of the A1-TVAs difference (Supplementary Table 5).</p>
<p>The statistical tests were all performed using the <italic>pingouin</italic> python package (<xref ref-type="bibr" rid="c80">Vallat, 2018</xref>).</p>
</sec>
<sec id="s4m">
<title>Representational similarity analysis</title>
<p>The RSA analyses were carried out using the package <italic>rsatoolbox</italic> (<xref ref-type="bibr" rid="c74">Schütt et al., 2021</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/rsagroup/rsatoolbox">https://github.com/rsagroup/rsatoolbox</ext-link>). For each participant, region of interest and hemisphere, we computed the cerebral Representational Dissimilarity Matrix (RDM) using the Pearson’s correlation between the speaker identity-specific response patterns of the GLM estimates <inline-formula><inline-graphic xlink:href="582302v2_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (<xref ref-type="bibr" rid="c82">Walther et al., 2016</xref>) (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). The model RDMs were built using cosine distance (<xref ref-type="bibr" rid="c87">Xing et al., 2015</xref>; <xref ref-type="bibr" rid="c12">Bhattacharya et al., 2017</xref>; <xref ref-type="bibr" rid="c83">Wang et al., 2018</xref>), capturing speaker pairwise feature differences predicted by the computational models LIN and the VLS (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). For greater comparability with the rest of the analyses described here, the GLM estimates and the computational models’ features were first normalized using robust scaling. We computed the Spearman correlations coefficients between the brain RDMs for each ROI, and the two model’s RDMs (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>). We assessed the significance of these brain-model correlation coefficients within a permutation-based ‘maximum statistics’ framework for multiple comparison correction (one-tailed inference; N permutations = 10,000 for each test; permutation of rows and columns of distance matrices, see <xref ref-type="bibr" rid="c32">Giordano et al., 2023</xref> and Maris &amp; Oostenveld, 2007; see <xref rid="fig3" ref-type="fig">Fig. 3b</xref>). We evaluated the VLS-LIN difference using a two-way repeated-measures ANOVA on the Fisher z-transformed Spearman correlation coefficients (dependent variable: correlation; within factors: ROI and Feature; participant identifier: participant hemisphere pair). The same permutation framework was also used to assess the significance of the difference between the RSA correlation for the VLS and LIN models.</p>
</sec>
<sec id="s4n">
<title>Brain decoding</title>
<p>Brain decoding was investigated at the stimulus level. The stimuli’s voice latent space representations <italic>z</italic><sub><italic>s</italic></sub><sup><italic>model</italic></sup> ∈ <italic>R</italic><sup><italic>N</italic></sup> <sup>×</sup> <sup>128</sup> and voice samples’ contrast maps <inline-formula><inline-graphic xlink:href="582302v2_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula> were divided into train and test splits, normalized across voice samples using robust scaling, then fit to the training set. For every participant and each ROI, we trained a <italic>L</italic><sub>2</sub>-regularized linear model <italic>W</italic> ∈ <italic>R</italic><sup><italic>V</italic></sup> <sup>×</sup> <sup>128</sup> model to predict the voice samples’ latent vectors from the voice samples’ contrast maps (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). The hyperparameter selection and optimization was done similarly as in the Brain encoding scheme. Training was performed on non repeated stimuli (see Stimuli section). We then used the trained models to predict for each participant the 6 repeated stimuli that were the most presented. Waveforms were estimated starting from the reconstructed spectrograms using the Griffin-Lim phase reconstruction algorithm (<xref ref-type="bibr" rid="c35">Griffin &amp; Lim, 1983</xref>).</p>
<p>We then used classifier analyses to assess the presence of voice information (gender, age, speaker identity) in the reconstructed latent representations (i.e., the latent representation predicted from the brain activity of a participant listening to a specific stimulus) (<xref rid="fig5" ref-type="fig">Fig. 5a, b, c</xref>). To this purpose, we first trained linear classifiers to categorize the training voice stimuli (participant 1, N = 6144; participant 2, N = 6142; participant 3, N = 5117; total, N = 17403) by gender (2 classes), age (2 classes) or identity (17 classes) based on VLS coordinates.</p>
<p>Secondly, we used the previously trained classifiers to predict the identity information based on the VLS derived from the brain responses of the 18 Test voice stimuli (3 participants x 6 stimuli). We first tested using one-sample t-tests that the mean of the distribution of accuracy scores obtained across random classifier initializations of classifiers (2 hemispheres x 20 seeds = 40) was significantly above chance level, for each category, ROI and model. We then evaluated the difference in performance at preserving identity-related information depending on the model or ROI via two-way ANOVAs (dependent variable: accuracy; between factors: Feature and ROI). We performed post-hoc planned paired t-tests between each model pair to test the significance of the VLS-LIN difference. Two-sample t-tests were finally used to test the significance of the A1-TVAs difference.</p>
</sec>
<sec id="s4o">
<title>Listening tests</title>
<p>We recruited 13 participants through the online platform Prolific (<ext-link ext-link-type="uri" xlink:href="http://www.prolific.co">www.prolific.co</ext-link>) for a series of online behavioral experiments. All participants reported having normal hearing. The purpose of these experiments was to evaluate how well voice identity information and naturalness are preserved in fMRI-based reconstructed voice excerpts. In the main session, participants carried out 4 tasks, in the following order: ‘speaker discrimination’ (∼120 min), ‘perceived naturalness’ (∼30 min), ‘gender categorization’ (∼30 min), ‘age categorization’ (∼30 min). The experiment lasted 3 hours and 35 minutes, and each participant was paid £48. 12 participants performed the speaker discrimination task, and all participants performed the other tasks.</p>
<p>Prior to the main experiment session, participants carried out a short loudness-change detection task to ensure that they wore headphones, and that they were attentive and properly set up for the main experiment (<xref ref-type="bibr" rid="c85">Woods et al., 2017</xref>). On each of 12 trials, participants heard 3 tones and were asked to identify which tone was the least loud by clicking on one of 3 response buttons: ‘First’, ‘Second’, or ‘Third’. Participants were admitted to the main experiment only if they achieved perfect performance in this task. We additionally refined the participant pool by excluding those who performed badly on the original stimuli, by retaining only the subjects whose performance was above the 25th percentile of accuracy. (gender and age categorizations: as all participants performed well (<xref rid="fig5" ref-type="fig">Fig. 5d, e, red</xref> dotted lines); speaker discrimination: 9/12 participants performed above the threshold of 64%).</p>
<p>The next three tasks were each carried out on the same set of 342 experimental stimuli, each presented on a different trial: 18 original stimuli, 36 stimuli reconstructed directly from the LIN and the VLS models, and 18 stimuli x 2 models x 4 regions of interest x 2 hemispheres= 288 brain-reconstructed stimuli.</p>
<p>In the ‘perceived naturalness’ task, participants were asked to rate how natural the voice sounded on a scale ranging from ‘Not at all natural’ to ‘Highly natural’ (i.e., similar to a real recording), and were instructed to use the full range of the scale.</p>
<p>During the ‘gender categorization’ task, participants categorized the gender by clicking on a ‘Female’ or ‘Male’ button.</p>
<p>Finally, in the ‘age categorization’ task, participants categorized the age of the speaker by clicking on a ‘Younger’ or ‘Older’ button.</p>
<p>In the ‘speaker discrimination’ task, participants carried out 684 trials (342 experimental stimuli x 2) with short breaks in between. On each trial, they were presented with 2 short sound stimuli, one after the other, and participants had to indicate whether they were from the same speaker or not. The speech material was selected randomly and was different between two stimuli.</p>
<p>To evaluate the performance of the participants, we firstly conducted one-sample t-tests to examine whether the mean accuracy score calculated from their responses was significantly higher than the chance level for each model and ROI. Next, we used two-way repeated-measures ANOVAs to assess the variation in participants’ performances in identifying identity-related information (dependent variable: accuracy; between-participant factors: Feature and ROI). To determine the statistical significance of the VLS-LIN difference, we carried out post-hoc planned paired t-tests between each model pair. Finally, we employed two-sample t-tests to evaluate the statistical significance of the A1-TVAs difference.</p>
</sec>
</sec>
<sec id="s5">
<title>Data and code availability</title>
<p>All data and codes will be made publicly available upon the article publication.</p>
</sec>
<sec id="d1e1939" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e2049">
<label>Supplementary Material</label>
<media xlink:href="supplements/582302_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Bruno Nazarian for the design of an MRI-compatible button. We thank Jean-Luc Anton and Kepkee Loh for useful discussions. This work was funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement no. 788240). This work was performed in the Center IRM-INT@CERIMED (UMR 7289, AMU-CNRS), platform member of France Life Imaging network (grant ANR-11-INBS-0 0 06). This work, carried out within the Institute of Convergence ILCB (ANR-16-CONV-0002), has benefited from support from the French government (<italic>France 2030</italic>), managed by the French National Agency for Research (ANR) and the Excellence Initiative of Aix-Marseille University (A*MIDEX).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Abraham</surname>, <given-names>Alexandre</given-names></string-name>, <string-name><given-names>Fabian</given-names> <surname>Pedregosa</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Eickenberg</surname></string-name>, <string-name><given-names>Philippe</given-names> <surname>Gervais</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Mueller</surname></string-name>, <string-name><given-names>Jean</given-names> <surname>Kossaifi</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>Bertrand</given-names> <surname>Thirion</surname></string-name>, and <string-name><given-names>Gael</given-names> <surname>Varoquaux</surname></string-name>. <year>2014</year>. “<article-title>Machine Learning for Neuroimaging with Scikit-Learn</article-title>.” <source>Frontiers in Neuroinformatics</source> <volume>8</volume>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Aglieri</surname>, <given-names>Virginia</given-names></string-name>, <string-name><given-names>Bastien</given-names> <surname>Cagna</surname></string-name>, <string-name><given-names>Lionel</given-names> <surname>Velly</surname></string-name>, <string-name><given-names>Sylvain</given-names> <surname>Takerkart</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Belin</surname></string-name>. <year>2021</year>. “<article-title>FMRI-Based Identity Classification Accuracy in Left Temporal and Frontal Regions Predicts Speaker Recognition Performance</article-title>.” <source>Scientific Reports</source> <volume>11</volume>(<issue>1</issue>):<fpage>489</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-020-79922-7</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Akbari</surname>, <given-names>Hassan</given-names></string-name>, <string-name><given-names>Bahar</given-names> <surname>Khalighinejad</surname></string-name>, <string-name><given-names>Jose L.</given-names> <surname>Herrero</surname></string-name>, <string-name><given-names>Ashesh D.</given-names> <surname>Mehta</surname></string-name>, and <string-name><given-names>Nima</given-names> <surname>Mesgarani</surname></string-name>. <year>2019</year>. “<article-title>Towards Reconstructing Intelligible Speech from the Human Auditory Cortex</article-title>.” <source>Scientific Reports</source> <volume>9</volume>(<issue>1</issue>):<fpage>874</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-018-37359-z</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="preprint"><string-name><surname>Ardila</surname>, <given-names>Rosana</given-names></string-name>, <string-name><given-names>Megan</given-names> <surname>Branson</surname></string-name>, <string-name><given-names>Kelly</given-names> <surname>Davis</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Henretty</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Kohler</surname></string-name>, <string-name><given-names>Josh</given-names> <surname>Meyer</surname></string-name>, <string-name><given-names>Reuben</given-names> <surname>Morais</surname></string-name>, <string-name><given-names>Lindsay</given-names> <surname>Saunders</surname></string-name>, <string-name><given-names>Francis M.</given-names> <surname>Tyers</surname></string-name>, and <string-name><given-names>Gregor</given-names> <surname>Weber</surname></string-name>. <year>2020</year>. “<article-title>Common Voice: A Massively-Multilingual Speech Corpus</article-title>.” <source>arXiv</source></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Ashburner</surname>, <given-names>John</given-names></string-name>. <year>2012</year>. “<article-title>SPM: A History</article-title>.” <source>NeuroImage</source> <volume>62</volume>(<issue>2</issue>):<fpage>791</fpage>–<lpage>800</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.025</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Barbero</surname>, <given-names>Francesca M.</given-names></string-name>, <string-name><given-names>Roberta P.</given-names> <surname>Calce</surname></string-name>, <string-name><given-names>Siddharth</given-names> <surname>Talwar</surname></string-name>, <string-name><given-names>Bruno</given-names> <surname>Rossion</surname></string-name>, and <string-name><given-names>Olivier</given-names> <surname>Collignon</surname></string-name>. <year>2021</year>. “<article-title>Fast Periodic Auditory Stimulation Reveals a Robust Categorical Response to Voices in the Human Brain</article-title>.” <source>eNeuro</source> <volume>8</volume>(<fpage>3</fpage>):<collab>ENEURO.0471-20.2021</collab>. doi: <pub-id pub-id-type="doi">10.1523/ENEURO.0471-20.2021</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Belin</surname>, <given-names>Pascal</given-names></string-name>, <string-name><given-names>Patricia E. G.</given-names> <surname>Bestelmeyer</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Latinus</surname></string-name>, and <string-name><given-names>Rebecca</given-names> <surname>Watson</surname></string-name>. <year>2011</year>. “<article-title>Understanding Voice Perception: Understanding Voice Perception</article-title>.” <source>British Journal of Psychology</source> <volume>102</volume>(<issue>4</issue>):<fpage>711</fpage>–<lpage>25</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.2044-8295.2011.02041.x</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Belin</surname>, <given-names>Pascal</given-names></string-name>, <string-name><given-names>Clémentine</given-names> <surname>Bodin</surname></string-name>, and <string-name><given-names>Virginia</given-names> <surname>Aglieri</surname></string-name>. <year>2018</year>. “<article-title>A ‘Voice Patch’ System in the Primate Brain for Processing Vocal Information?</article-title>” <source>Hearing Research</source> <volume>366</volume>:<fpage>65</fpage>–<lpage>74</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.heares.2018.04.010</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Belin</surname>, <given-names>Pascal</given-names></string-name>, <string-name><given-names>Shirley</given-names> <surname>Fecteau</surname></string-name>, and <string-name><given-names>Catherine</given-names> <surname>Bédard</surname></string-name>. <year>2004</year>. “<article-title>Thinking the Voice: Neural Correlates of Voice Perception</article-title>.” <source>Trends in Cognitive Sciences</source> <volume>8</volume>(<issue>3</issue>):<fpage>129</fpage>–<lpage>35</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2004.01.008</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Belin</surname>, <given-names>Pascal</given-names></string-name>, and <string-name><given-names>Robert J.</given-names> <surname>Zatorre</surname></string-name>. <year>2003</year>. <article-title>Adaptation to Speaker’s Voice in Right Anterior Temporal Lobe</article-title>: <source>NeuroReport</source> <volume>14</volume>(<issue>16</issue>):<fpage>2105</fpage>–<lpage>9</lpage>. doi: <pub-id pub-id-type="doi">10.1097/00001756-200311140-00019</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Belin</surname>, <given-names>Pascal</given-names></string-name>, <string-name><given-names>Robert J.</given-names> <surname>Zatorre</surname></string-name>, <string-name><given-names>Philippe</given-names> <surname>Lafaille</surname></string-name>, <string-name><given-names>Pierre</given-names> <surname>Ahad</surname></string-name>, and <string-name><given-names>Bruce</given-names> <surname>Pike</surname></string-name>. <year>2000</year>. “<article-title>Voice-Selective Areas in Human Auditory Cortex</article-title>.” <source>Nature</source> <volume>403</volume>(<issue>6767</issue>):<fpage>309</fpage>–<lpage>12</lpage>. doi: <pub-id pub-id-type="doi">10.1038/35002078</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Bhattacharya</surname>, <given-names>Gautam</given-names></string-name>, <string-name><given-names>Jahangir</given-names> <surname>Alam</surname></string-name>, and <string-name><given-names>Patrick</given-names> <surname>Kenny</surname></string-name>. <year>2017</year>. “<article-title>Deep Speaker Embeddings for Short-Duration Speaker Verification</article-title>.” Pp. <fpage>1517</fpage>–<lpage>21</lpage> in <source>Interspeech</source>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Blank</surname>, <given-names>Helen</given-names></string-name>, <string-name><given-names>Nuri</given-names> <surname>Wieland</surname></string-name>, and <string-name><given-names>Katharina</given-names> <surname>von Kriegstein</surname></string-name>. <year>2014</year>. “<article-title>Person Recognition and the Brain: Merging Evidence from Patients and Healthy Individuals</article-title>.” <source>Neuroscience &amp; Biobehavioral Reviews</source> <volume>47</volume>:<fpage>717</fpage>–<lpage>34</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2014.10.022</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Bodin</surname>, <given-names>Clémentine</given-names></string-name>, <string-name><given-names>Régis</given-names> <surname>Trapeau</surname></string-name>, <string-name><given-names>Bruno</given-names> <surname>Nazarian</surname></string-name>, <string-name><given-names>Julien</given-names> <surname>Sein</surname></string-name>, <string-name><given-names>Xavier</given-names> <surname>Degiovanni</surname></string-name>, <string-name><given-names>Joël</given-names> <surname>Baurberg</surname></string-name>, <string-name><given-names>Emilie</given-names> <surname>Rapha</surname></string-name>, <string-name><given-names>Luc</given-names> <surname>Renaud</surname></string-name>, <string-name><given-names>Bruno L.</given-names> <surname>Giordano</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Belin</surname></string-name>. <year>2021</year>. “<article-title>Functionally Homologous Representation of Vocalizations in the Auditory Cortex of Humans and Macaques</article-title>.” <source>Current Biology S</source><volume>31</volume>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2021.08.043</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="confproc"><string-name><surname>Bouthillier</surname>, <given-names>Xavier</given-names></string-name>, <string-name><given-names>Pierre</given-names> <surname>Delaunay</surname></string-name>, <string-name><given-names>Mirko</given-names> <surname>Bronzi</surname></string-name>, <string-name><given-names>Assya</given-names> <surname>Trofimov</surname></string-name>, <string-name><given-names>Brennan</given-names> <surname>Nichyporuk</surname></string-name>, <string-name><given-names>Justin</given-names> <surname>Szeto</surname></string-name>, <string-name><given-names>Naz</given-names> <surname>Sepah</surname></string-name>, <string-name><given-names>Edward</given-names> <surname>Raff</surname></string-name>, <string-name><given-names>Kanika</given-names> <surname>Madan</surname></string-name>, <string-name><given-names>Vikram</given-names> <surname>Voleti</surname></string-name>, <string-name><given-names>Samira Ebrahimi</given-names> <surname>Kahou</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Michalski</surname></string-name>, <string-name><given-names>Dmitriy</given-names> <surname>Serdyuk</surname></string-name>, <string-name><given-names>Tal</given-names> <surname>Arbel</surname></string-name>, <string-name><given-names>Chris</given-names> <surname>Pal</surname></string-name>, <string-name><given-names>Gaël</given-names> <surname>Varoquaux</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Vincent</surname></string-name>. <year>2021</year>. “<article-title>Accounting for Variance in Machine Learning Benchmarks</article-title>.” <source>Proceedings of Machine Learning and Systems 3 (MLSys 2021)</source></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Capilla</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>P.</given-names> <surname>Belin</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Gross</surname></string-name>. <year>2013</year>. “<article-title>The Early Spatio-Temporal Correlates and Task Independence of Cerebral Voice Processing Studied with MEG</article-title>.” <source>Cerebral Cortex</source> <volume>23</volume>(<issue>6</issue>):<fpage>1388</fpage>–<lpage>95</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/bhs119</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Caucheteux</surname>, <given-names>Charlotte</given-names></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, and <string-name><given-names>Jean-Rémi</given-names> <surname>King</surname></string-name>. <year>2022</year>. “<article-title>Deep Language Algorithms Predict Semantic Comprehension from Brain Activity</article-title>.” <source>Scientific Reports</source> <volume>12</volume>(<issue>1</issue>):<fpage>16327</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-022-20460-9</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Caucheteux</surname>, <given-names>Charlotte</given-names></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, and <string-name><given-names>Jean-Rémi</given-names> <surname>King</surname></string-name>. <year>2023</year>. “<article-title>Evidence of a Predictive Coding Hierarchy in the Human Brain Listening to Speech</article-title>.” <source>Nature Human Behaviour</source> <fpage>1</fpage>–<lpage>12</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41562-022-01516-2</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Caucheteux</surname>, <given-names>Charlotte</given-names></string-name>, and <string-name><given-names>Jean-Rémi</given-names> <surname>King</surname></string-name>. <year>2022</year>. “<article-title>Brains and Algorithms Partially Converge in Natural Language Processing</article-title>.” <source>Communications Biology</source> <volume>5</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>10</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s42003-022-03036-1</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Chang</surname>, <given-names>Nadine</given-names></string-name>, <string-name><given-names>John A.</given-names> <surname>Pyles</surname></string-name>, <string-name><given-names>Austin</given-names> <surname>Marcus</surname></string-name>, <string-name><given-names>Abhinav</given-names> <surname>Gupta</surname></string-name>, <string-name><given-names>Michael J.</given-names> <surname>Tarr</surname></string-name>, and <string-name><given-names>Elissa M.</given-names> <surname>Aminoff</surname></string-name>. <year>2019</year>. “<article-title>BOLD5000, a Public fMRI Dataset While Viewing 5000 Visual Images</article-title>.” <source>Scientific Data</source> <volume>6</volume>(<issue>1</issue>):<fpage>49</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-019-0052-3</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Charest</surname>, <given-names>I.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Pernet</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Latinus</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Crabbe</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Belin</surname></string-name>. <year>2013</year>. “<article-title>Cerebral Processing of Voice Gender Studied Using a Continuous Carryover fMRI Design</article-title>.” <source>Cerebral Cortex</source> <volume>23</volume>(<issue>4</issue>):<fpage>958</fpage>–<lpage>66</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/bhs090</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Charest</surname>, <given-names>Ian</given-names></string-name>, <string-name><given-names>Cyril R.</given-names> <surname>Pernet</surname></string-name>, <string-name><given-names>Guillaume A.</given-names> <surname>Rousselet</surname></string-name>, <string-name><given-names>Ileana</given-names> <surname>Quiñones</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Latinus</surname></string-name>, <string-name><given-names>Sarah</given-names> <surname>Fillion-Bilodeau</surname></string-name>, <string-name><given-names>Jean-Pierre</given-names> <surname>Chartrand</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Belin</surname></string-name>. <year>2009</year>. “<article-title>Electrophysiological Evidence for an Early Processing of Human Voices</article-title>.” <source>BMC Neuroscience</source> <volume>10</volume>(<issue>1</issue>):<fpage>127</fpage>. doi: <pub-id pub-id-type="doi">10.1186/1471-2202-10-127</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="confproc"><string-name><surname>Chen</surname>, <given-names>Zijiao</given-names></string-name>, <string-name><given-names>Jiaxin</given-names> <surname>Qing</surname></string-name>, <string-name><given-names>Tiange</given-names> <surname>Xiang</surname></string-name>, <string-name><given-names>Wan Lin</given-names> <surname>Yue</surname></string-name>, and <string-name><given-names>Juan Helen</given-names> <surname>Zhou</surname></string-name>. <year>2023</year>. “<article-title>Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding</article-title>.” Pp. <fpage>22710</fpage>–<lpage>20</lpage> <collab>in 2023</collab> <source>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Vancouver, BC, Canada: IEEE</source>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Cowen</surname>, <given-names>Alan S.</given-names></string-name>, <string-name><given-names>Marvin M.</given-names> <surname>Chun</surname></string-name>, and <string-name><given-names>Brice A.</given-names> <surname>Kuhl</surname></string-name>. <year>2014</year>. “<article-title>Neural Portraits of Perception: Reconstructing Face Images from Evoked Brain Activity</article-title>.” <source>NeuroImage</source> <volume>94</volume>:<fpage>12</fpage>–<lpage>22</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.03.018</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Dado</surname>, <given-names>Thirza</given-names></string-name>, <string-name><given-names>Yağmur</given-names> <surname>Güçlütürk</surname></string-name>, <string-name><given-names>Luca</given-names> <surname>Ambrogioni</surname></string-name>, <string-name><given-names>Gabriëlle</given-names> <surname>Ras</surname></string-name>, <string-name><given-names>Sander</given-names> <surname>Bosch</surname></string-name>, <string-name><given-names>Marcel</given-names> <surname>van Gerven</surname></string-name>, and <string-name><given-names>Umut</given-names> <surname>Güçlü</surname></string-name>. <year>2022</year>. “<article-title>Hyperrealistic Neural Decoding for Reconstructing Faces from fMRI Activations via the GAN Latent Space</article-title>.” <source>Scientific Reports</source> <volume>12</volume>(<issue>1</issue>):<fpage>141</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-021-03938-w</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Défossez</surname>, <given-names>Alexandre</given-names></string-name>, <string-name><given-names>Charlotte</given-names> <surname>Caucheteux</surname></string-name>, <string-name><given-names>Jérémy</given-names> <surname>Rapin</surname></string-name>, <string-name><given-names>Ori</given-names> <surname>Kabeli</surname></string-name>, and <string-name><given-names>Jean-Rémi</given-names> <surname>King</surname></string-name>. <year>2023</year>. “<article-title>Decoding Speech Perception from Non-Invasive Brain Recordings</article-title>.” <source>Nature Machine Intelligence</source> <volume>5</volume>(<issue>10</issue>):<fpage>1097</fpage>–<lpage>1107</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s42256-023-00714-5</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Diedrichsen</surname>, <given-names>Jörn</given-names></string-name>, and <string-name><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>. <year>2017</year>. “<article-title>Representational Models: A Common Framework for Understanding Encoding, Pattern-Component, and Representational-Similarity Analysis</article-title>.” <source>PLOS Computational Biology</source> <volume>13</volume>(<issue>4</issue>):<fpage>e1005508</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005508</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Dosenbach</surname>, <given-names>Nico U. F.</given-names></string-name>, <string-name><given-names>Jonathan M.</given-names> <surname>Koller</surname></string-name>, <string-name><given-names>Eric A.</given-names> <surname>Earl</surname></string-name>, <string-name><given-names>Oscar</given-names> <surname>Miranda-Dominguez</surname></string-name>, <string-name><given-names>Rachel L.</given-names> <surname>Klein</surname></string-name>, <string-name><given-names>Andrew N.</given-names> <surname>Van</surname></string-name>, <string-name><given-names>Abraham Z.</given-names> <surname>Snyder</surname></string-name>, <string-name><given-names>Bonnie J.</given-names> <surname>Nagel</surname></string-name>, <string-name><given-names>Joel T.</given-names> <surname>Nigg</surname></string-name>, <string-name><given-names>Annie L.</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>Victoria</given-names> <surname>Wesevich</surname></string-name>, <string-name><given-names>Deanna J.</given-names> <surname>Greene</surname></string-name>, and <string-name><given-names>Damien A.</given-names> <surname>Fair</surname></string-name>. <year>2017</year>. “<article-title>Real-Time Motion Analytics during Brain MRI Improve Data Quality and Reduce Costs</article-title>.” <source>NeuroImage</source> <volume>161</volume>:<fpage>80</fpage>–<lpage>93</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.025</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><given-names>D. E.</given-names> <surname>Glaser</surname></string-name>, <string-name><given-names>R. N. A.</given-names> <surname>Henson</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kiebel</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Phillips</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Ashburner</surname></string-name>. <year>2002</year>. “<article-title>Classical and Bayesian Inference in Neuroimaging: Applications</article-title>.” <source>NeuroImage</source> <volume>16</volume>(<issue>2</issue>):<fpage>484</fpage>–<lpage>512</lpage>. doi: <pub-id pub-id-type="doi">10.1006/nimg.2002.1091</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><given-names>A. P.</given-names> <surname>Holmes</surname></string-name>, <string-name><given-names>K. J.</given-names> <surname>Worsley</surname></string-name>, <string-name><given-names>J. P.</given-names> <surname>Poline</surname></string-name>, <string-name><given-names>C. D.</given-names> <surname>Frith</surname></string-name>, and <string-name><given-names>R. S. J.</given-names> <surname>Frackowiak</surname></string-name>. <year>1994</year>. “<article-title>Statistical Parametric Maps in Functional Imaging: A General Linear Approach</article-title>.” <source>Human Brain Mapping</source> <volume>2</volume>(<issue>4</issue>):<fpage>189</fpage>–<lpage>210</lpage>. doi: <pub-id pub-id-type="doi">10.1002/hbm.460020402</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Gaziv</surname>, <given-names>Guy</given-names></string-name>, <string-name><given-names>Roman</given-names> <surname>Beliy</surname></string-name>, <string-name><given-names>Niv</given-names> <surname>Granot</surname></string-name>, <string-name><given-names>Assaf</given-names> <surname>Hoogi</surname></string-name>, <string-name><given-names>Francesca</given-names> <surname>Strappini</surname></string-name>, <string-name><given-names>Tal</given-names> <surname>Golan</surname></string-name>, and <string-name><given-names>Michal</given-names> <surname>Irani</surname></string-name>. <year>2022</year>. “<article-title>Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity</article-title>.” <source>NeuroImage</source> <volume>254</volume>:<fpage>119121</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119121</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Giordano</surname>, <given-names>Bruno L.</given-names></string-name>, <string-name><given-names>Michele</given-names> <surname>Esposito</surname></string-name>, <string-name><given-names>Giancarlo</given-names> <surname>Valente</surname></string-name>, and <string-name><given-names>Elia</given-names> <surname>Formisano</surname></string-name>. <year>2023</year>. “<article-title>Intermediate Acoustic-to-Semantic Representations Link Behavioral and Neural Responses to Natural Sounds</article-title>.” <source>Nature Neuroscience</source> <fpage>1</fpage>–<lpage>9</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41593-023-01285-9</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Glover</surname>, <given-names>G. H.</given-names></string-name>, <string-name><given-names>T. Q.</given-names> <surname>Li</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Ress</surname></string-name>. <year>2000</year>. “<article-title>Image-Based Method for Retrospective Correction of Physiological Motion Effects in fMRI: RETROICOR</article-title>.” <source>Magnetic Resonance in Medicine</source> <volume>44</volume>(<issue>1</issue>):<fpage>162</fpage>–<lpage>67</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Glover</surname>, <given-names>Gary H</given-names></string-name>. <year>1999</year>. “<article-title>Deconvolution of Impulse Response in Event-Related BOLD fMRI1</article-title>.” <source>NeuroImage</source> <volume>9</volume>(<issue>4</issue>):<fpage>416</fpage>–<lpage>29</lpage>. doi: <pub-id pub-id-type="doi">10.1006/nimg.1998.0419</pub-id>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="confproc"><string-name><surname>Griffin</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>Jae</given-names> <surname>Lim</surname></string-name>. <year>1983</year>. “<article-title>Signal Estimation from Modified Short-Time Fourier Transform</article-title>.” Pp. <fpage>804</fpage>–<lpage>7</lpage> <source>ICASSP’83. IEEE International Conference on Acoustics, Speech, and Signal Processing. Vol. 8. Boston, MASS, USA: Institute of Electrical and Electronics Engineers</source>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="confproc"><string-name><surname>Güçlü</surname>, <given-names>Umut</given-names></string-name>, <string-name><given-names>Jordy</given-names> <surname>Thielen</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Hanke</surname></string-name>, <string-name><given-names>M. A. J.</given-names> <surname>van Gerven</surname></string-name>, and <string-name><given-names>Marcel A. J.</given-names> <surname>van Gerven</surname></string-name>. <year>2016</year>. “<article-title>Brains on Beats</article-title>.” <source>Proceedings of the International Conference on Neural Information Processing Systems</source> <fpage>2101</fpage>–<lpage>9</lpage>. doi: null.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Gutierrez</surname>, <given-names>Miren</given-names></string-name>. <year>2021</year>. “<article-title>Algorithmic Gender Bias and Audiovisual Data: A Research Agenda</article-title>.” <source>International Journal of Communication</source> <volume>15</volume>:<fpage>439</fpage>–<lpage>61</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Han</surname>, <given-names>Kuan</given-names></string-name>, <string-name><given-names>Haiguang</given-names> <surname>Wen</surname></string-name>, <string-name><given-names>Junxing</given-names> <surname>Shi</surname></string-name>, <string-name><given-names>Kun-Han</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>Yizhen</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Di</given-names> <surname>Fu</surname></string-name>, and <string-name><given-names>Zhongming</given-names> <surname>Liu</surname></string-name>. <year>2019</year>. “<article-title>Variational Autoencoder: An Unsupervised Model for Encoding and Decoding fMRI Activity in Visual Cortex</article-title>.” <source>NeuroImage</source> <volume>198</volume>:<fpage>125</fpage>–<lpage>36</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.039</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Harris</surname>, <given-names>Charles R.</given-names></string-name>, <string-name><given-names>K.</given-names> <surname>Jarrod Millman</surname></string-name>, <string-name><given-names>Stéfan J.</given-names> <surname>van der Walt</surname></string-name>, <string-name><given-names>Ralf</given-names> <surname>Gommers</surname></string-name>, <string-name><given-names>Pauli</given-names> <surname>Virtanen</surname></string-name>, <string-name><given-names>David</given-names> <surname>Cournapeau</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Wieser</surname></string-name>, <string-name><given-names>Julian</given-names> <surname>Taylor</surname></string-name>, <string-name><given-names>Sebastian</given-names> <surname>Berg</surname></string-name>, <string-name><given-names>Nathaniel J.</given-names> <surname>Smith</surname></string-name>, <string-name><given-names>Robert</given-names> <surname>Kern</surname></string-name>, <string-name><given-names>Matti</given-names> <surname>Picus</surname></string-name>, <string-name><given-names>Stephan</given-names> <surname>Hoyer</surname></string-name>, <string-name><given-names>Marten H.</given-names> <surname>van Kerkwijk</surname></string-name>, <string-name><given-names>Matthew</given-names> <surname>Brett</surname></string-name>, <string-name><given-names>Allan</given-names> <surname>Haldane</surname></string-name>, <string-name><given-names>Jaime</given-names> <surname>Fernández del Río</surname></string-name>, <string-name><given-names>Mark</given-names> <surname>Wiebe</surname></string-name>, <string-name><given-names>Pearu</given-names> <surname>Peterson</surname></string-name>, <string-name><given-names>Pierre</given-names> <surname>Gérard-Marchant</surname></string-name>, <string-name><given-names>Kevin</given-names> <surname>Sheppard</surname></string-name>, <string-name><given-names>Tyler</given-names> <surname>Reddy</surname></string-name>, <string-name><given-names>Warren</given-names> <surname>Weckesser</surname></string-name>, <string-name><given-names>Hameer</given-names> <surname>Abbasi</surname></string-name>, <string-name><given-names>Christoph</given-names> <surname>Gohlke</surname></string-name>, and <string-name><given-names>Travis E.</given-names> <surname>Oliphant</surname></string-name>. <year>2020</year>. “<article-title>Array Programming with NumPy</article-title>.” <source>Nature</source> <volume>585</volume>(<issue>7825</issue>):<fpage>357</fpage>–<lpage>62</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Higgins</surname>, <given-names>Irina</given-names></string-name>, <string-name><given-names>Le</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>Victoria</given-names> <surname>Langston</surname></string-name>, <string-name><given-names>Demis</given-names> <surname>Hassabis</surname></string-name>, <string-name><given-names>Christopher</given-names> <surname>Summerfield</surname></string-name>, <string-name><given-names>Doris</given-names> <surname>Tsao</surname></string-name>, and <string-name><given-names>Matthew</given-names> <surname>Botvinick</surname></string-name>. <year>2021</year>. “<article-title>Unsupervised Deep Learning Identifies Semantic Disentanglement in Single Inferotemporal Face Patch Neurons</article-title>.” <source>Nature Communications</source> <volume>12</volume>(<issue>1</issue>):<fpage>6456</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41467-021-26751-5</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Horikawa</surname>, <given-names>Tomoyasu</given-names></string-name>, and <string-name><given-names>Yukiyasu</given-names> <surname>Kamitani</surname></string-name>. <year>2017</year>. “<article-title>Generic Decoding of Seen and Imagined Objects Using Hierarchical Visual Features</article-title>.” <source>Nature Communications</source> <volume>8</volume>(<issue>1</issue>):<fpage>15037</fpage>. doi: <pub-id pub-id-type="doi">10.1038/ncomms15037</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="preprint"><string-name><surname>Howard</surname>, <given-names>Andrew G.</given-names></string-name>, <string-name><given-names>Menglong</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>Bo</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Dmitry</given-names> <surname>Kalenichenko</surname></string-name>, <string-name><given-names>Weijun</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Tobias</given-names> <surname>Weyand</surname></string-name>, <string-name><given-names>Marco</given-names> <surname>Andreetto</surname></string-name>, and <string-name><given-names>Hartwig</given-names> <surname>Adam</surname></string-name>. <year>2017</year>. “<article-title>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</article-title>.” <source>arXiv</source>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Kasper</surname>, <given-names>Lars</given-names></string-name>, <string-name><given-names>Steffen</given-names> <surname>Bollmann</surname></string-name>, <string-name><given-names>Andreea O.</given-names> <surname>Diaconescu</surname></string-name>, <string-name><given-names>Chloe</given-names> <surname>Hutton</surname></string-name>, <string-name><given-names>Jakob</given-names> <surname>Heinzle</surname></string-name>, <string-name><given-names>Sandra</given-names> <surname>Iglesias</surname></string-name>, <string-name><given-names>Tobias U.</given-names> <surname>Hauser</surname></string-name>, <string-name><given-names>Miriam</given-names> <surname>Sebold</surname></string-name>, <string-name><given-names>Zina-Mary</given-names> <surname>Manjaly</surname></string-name>, <string-name><given-names>Klaas P.</given-names> <surname>Pruessmann</surname></string-name>, and <string-name><given-names>Klaas E.</given-names> <surname>Stephan</surname></string-name>. <year>2017</year>. “<article-title>The PhysIO Toolbox for Modeling Physiological Noise in fMRI Data</article-title>.” <source>Journal of Neuroscience Methods</source> <volume>276</volume>:<fpage>56</fpage>–<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.10.019</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Kell</surname>, <given-names>Alexander J. E.</given-names></string-name>, <string-name><given-names>Daniel L. K.</given-names> <surname>Yamins</surname></string-name>, <string-name><given-names>Erica N.</given-names> <surname>Shook</surname></string-name>, <string-name><given-names>Sam V.</given-names> <surname>Norman-Haignere</surname></string-name>, and <string-name><given-names>Josh H.</given-names> <surname>McDermott</surname></string-name>. <year>2018</year>. “<article-title>A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy</article-title>.” <source>Neuron</source> <volume>98</volume>(<issue>3</issue>):<fpage>630</fpage>–<lpage>644.e16</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Khaligh-Razavi</surname>, <given-names>Seyed-Mahdi</given-names></string-name>, and <string-name><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>. <year>2014</year>. <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLoS Computational Biology</source> <volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="preprint"><string-name><surname>Kingma</surname>, <given-names>Diederik P.</given-names></string-name>, and <string-name><given-names>Max</given-names> <surname>Welling</surname></string-name>. <year>2014</year>. “<article-title>Auto-Encoding Variational Bayes</article-title>.” <source>arXiv</source>:<volume>1312</volume>.<fpage>6114</fpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Kingma</surname>, <given-names>Diederik P.</given-names></string-name>, and <string-name><given-names>Max</given-names> <surname>Welling</surname></string-name>. <year>2019</year>. “<article-title>An Introduction to Variational Autoencoders</article-title>.” <source>Foundations and Trends® in Machine Learning</source> <volume>12</volume>(<issue>4</issue>):<fpage>307</fpage>–<lpage>92</lpage>. doi: <pub-id pub-id-type="doi">10.1561/2200000056</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname>, <given-names>Nikolaus</given-names></string-name>. <year>2008</year>. “<article-title>Representational Similarity Analysis – Connecting the Branches of Systems Neuroscience</article-title>.” <source>Frontiers in Systems Neuroscience</source>. doi: <pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Kriegstein</surname>, <given-names>Katharina V.</given-names></string-name>, and <string-name><given-names>Anne-Lise</given-names> <surname>Giraud</surname></string-name>. <year>2004</year>. “<article-title>Distinct Functional Substrates along the Right Superior Temporal Sulcus for the Processing of Voices</article-title>.” <source>NeuroImage</source> <volume>22</volume>(<issue>2</issue>):<fpage>948</fpage>–<lpage>55</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.02.020</pub-id>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>von Kriegstein</surname>, <given-names>Katharina</given-names></string-name>, <string-name><given-names>Evelyn</given-names> <surname>Eger</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Kleinschmidt</surname></string-name>, and <string-name><given-names>Anne Lise</given-names> <surname>Giraud</surname></string-name>. <year>2003</year>. “<article-title>Modulation of Neural Responses to Speech by Directing Attention to Voices or Verbal Content</article-title>.” <source>Cognitive Brain Research</source> <volume>17</volume>(<issue>1</issue>):<fpage>48</fpage>–<lpage>55</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0926-6410(03)00079-X</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Lavan</surname>, <given-names>Nadine</given-names></string-name>. <year>2023</year>. “<article-title>The Time Course of Person Perception From Voices: A Behavioral Study</article-title>.” <source>Psychological Science</source> <volume>34</volume>(<issue>7</issue>):<fpage>771</fpage>–<lpage>83</lpage>. doi: <pub-id pub-id-type="doi">10.1177/09567976231161565</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Le</surname>, <given-names>Lynn</given-names></string-name>, <string-name><given-names>Luca</given-names> <surname>Ambrogioni</surname></string-name>, <string-name><given-names>Katja</given-names> <surname>Seeliger</surname></string-name>, <string-name><given-names>Yağmur</given-names> <surname>Güçlütürk</surname></string-name>, <string-name><given-names>Marcel</given-names> <surname>van Gerven</surname></string-name>, and <string-name><given-names>Umut</given-names> <surname>Güçlü</surname></string-name>. <year>2022</year>. “<article-title>Brain2Pix: Fully Convolutional Naturalistic Video Frame Reconstruction from Brain Activity</article-title>.” <source>Frontiers in Neuroscience</source> <volume>16</volume>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>LeCun</surname>, <given-names>Yann</given-names></string-name>, <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>, and <string-name><given-names>Geoffrey</given-names> <surname>Hinton</surname></string-name>. <year>2015</year>. “<article-title>Deep Learning</article-title>.” <source>Nature</source> <volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>44</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>Dong</given-names></string-name>, <string-name><given-names>Yue</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Jianping</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>Houqiang</given-names> <surname>Li</surname></string-name>, and <string-name><given-names>Feng</given-names> <surname>Wu</surname></string-name>. <year>2020</year>. “<article-title>Deep Learning-Based Video Coding: A Review and a Case Study</article-title>.” <source>ACM Computing Surveys</source> <volume>53</volume>(<issue>1</issue>):<fpage>11:1</fpage>-<lpage>11:35</lpage>. doi: <pub-id pub-id-type="doi">10.1145/3368405</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Maguinness</surname>, <given-names>Corrina</given-names></string-name>, <string-name><given-names>Claudia</given-names> <surname>Roswandowitz</surname></string-name>, and <string-name><given-names>Katharina</given-names> <surname>von Kriegstein</surname></string-name>. <year>2018</year>. “<article-title>Understanding the Mechanisms of Familiar Voice-Identity Recognition in the Human Brain</article-title>.” <source>Neuropsychologia</source> <volume>116</volume>:<fpage>179</fpage>–<lpage>93</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.03.039</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Maris</surname>, <given-names>Eric</given-names></string-name>, and <string-name><given-names>Robert</given-names> <surname>Oostenveld</surname></string-name>. <year>2007</year>. “<article-title>Nonparametric Statistical Testing of EEG-and MEG-Data</article-title>.” <source>Journal of Neuroscience Methods</source> <volume>164</volume>(<issue>1</issue>):<fpage>177</fpage>–<lpage>90</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Martin</surname>, <given-names>F. N.</given-names></string-name>, and <string-name><given-names>C. A.</given-names> <surname>Champlin</surname></string-name>. <year>2000</year>. “<article-title>Reconsidering the Limits of Normal Hearing</article-title>.” <source>Journal of the American Academy of Audiology</source> <volume>11</volume>(<issue>2</issue>):<fpage>64</fpage>–<lpage>66</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Mcallister</surname>, <given-names>Jan</given-names></string-name>, <string-name><given-names>Anne</given-names> <surname>Potts</surname></string-name>, <string-name><given-names>Kathryn</given-names> <surname>Mason</surname></string-name>, and <string-name><given-names>Geoffrey</given-names> <surname>Marchant</surname></string-name>. <year>1994</year>. “<article-title>Word Duration in Monologue and Dialogue Speech</article-title>.” <source>Language and Speech</source> <volume>37</volume>(<issue>4</issue>):<fpage>393</fpage>–<lpage>405</lpage>. doi: <pub-id pub-id-type="doi">10.1177/002383099403700404</pub-id>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="preprint"><string-name><surname>Millet</surname>, <given-names>Juliette</given-names></string-name>, <string-name><given-names>Charlotte</given-names> <surname>Caucheteux</surname></string-name>, <string-name><given-names>Pierre</given-names> <surname>Orhan</surname></string-name>, <string-name><given-names>Yves</given-names> <surname>Boubenec</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>Ewan</given-names> <surname>Dunbar</surname></string-name>, <string-name><given-names>Christophe</given-names> <surname>Pallier</surname></string-name>, and <string-name><given-names>Jean-Remi</given-names> <surname>King</surname></string-name>. <year>2022</year>. <article-title>Toward a Realistic Model of Speech Processing in the Brain with Self-Supervised Learning</article-title>. <source>arXiv</source>:<volume>2206</volume>.<fpage>01685</fpage>. arXiv. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2206.01685</pub-id>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="confproc"><string-name><surname>Mozafari</surname>, <given-names>Milad</given-names></string-name>, <string-name><given-names>Leila</given-names> <surname>Reddy</surname></string-name>, and <string-name><given-names>Rufin</given-names> <surname>VanRullen</surname></string-name>. <year>2020</year>. “<article-title>Reconstructing Natural Scenes from fMRI Patterns Using BigBiGAN</article-title>.” <source>2020 International Joint Conference on Neural Networks (IJCNN)</source> <fpage>1</fpage>–<lpage>8</lpage>. doi: <pub-id pub-id-type="doi">10.1109/IJCNN48605.2020.9206960</pub-id>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Nagrani</surname>, <given-names>Arsha</given-names></string-name>, <string-name><given-names>Joon Son</given-names> <surname>Chung</surname></string-name>, and <string-name><given-names>Andrew</given-names> <surname>Zisserman</surname></string-name>. <year>2017</year>. <article-title>VoxCeleb: A Large-Scale Speaker Identification Dataset</article-title>. Pp. <fpage>2616</fpage>–<lpage>20</lpage> <source>Interspeech</source>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Naselaris</surname>, <given-names>Thomas</given-names></string-name>, <string-name><given-names>Kendrick N.</given-names> <surname>Kay</surname></string-name>, <string-name><given-names>Shinji</given-names> <surname>Nishimoto</surname></string-name>, and <string-name><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name>. <year>2011</year>. “<article-title>Encoding and Decoding in fMRI</article-title>.” <source>NeuroImage</source> <volume>56</volume>(<issue>2</issue>):<fpage>400</fpage>–<lpage>410</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.073</pub-id>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Pasley</surname>, <given-names>Brian N.</given-names></string-name>, <string-name><given-names>Stephen V.</given-names> <surname>David</surname></string-name>, <string-name><given-names>Nima</given-names> <surname>Mesgarani</surname></string-name>, <string-name><given-names>Adeen</given-names> <surname>Flinker</surname></string-name>, <string-name><given-names>Shihab A.</given-names> <surname>Shamma</surname></string-name>, <string-name><given-names>Nathan E.</given-names> <surname>Crone</surname></string-name>, <string-name><given-names>Robert T.</given-names> <surname>Knight</surname></string-name>, and <string-name><given-names>Edward F.</given-names> <surname>Chang</surname></string-name>. <year>2012</year>. “<article-title>Reconstructing Speech from Human Auditory Cortex</article-title>.” <source>PLOS Biology</source> <volume>10</volume>(<issue>1</issue>):<fpage>e1001251</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.1001251</pub-id>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="confproc"><string-name><surname>Paszke</surname>, <given-names>Adam</given-names></string-name>, <string-name><given-names>Sam</given-names> <surname>Gross</surname></string-name>, <string-name><given-names>Francisco</given-names> <surname>Massa</surname></string-name>, <string-name><given-names>Adam</given-names> <surname>Lerer</surname></string-name>, <string-name><given-names>James</given-names> <surname>Bradbury</surname></string-name>, <string-name><given-names>Gregory</given-names> <surname>Chanan</surname></string-name>, <string-name><given-names>Trevor</given-names> <surname>Killeen</surname></string-name>, <string-name><given-names>Zeming</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>Natalia</given-names> <surname>Gimelshein</surname></string-name>, <string-name><given-names>Luca</given-names> <surname>Antiga</surname></string-name>, <string-name><given-names>Alban</given-names> <surname>Desmaison</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Köpf</surname></string-name>, <string-name><given-names>Edward</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Zach</given-names> <surname>DeVito</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Raison</surname></string-name>, <string-name><given-names>Alykhan</given-names> <surname>Tejani</surname></string-name>, <string-name><given-names>Sasank</given-names> <surname>Chilamkurthy</surname></string-name>, <string-name><given-names>Benoit</given-names> <surname>Steiner</surname></string-name>, <string-name><given-names>Lu</given-names> <surname>Fang</surname></string-name>, <string-name><given-names>Junjie</given-names> <surname>Bai</surname></string-name>, and <string-name><given-names>Soumith</given-names> <surname>Chintala</surname></string-name>. <year>2019</year>. <article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title>. <source>Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</source></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Pedregosa</surname>, <given-names>Fabian</given-names></string-name>, <string-name><given-names>Gaël</given-names> <surname>Varoquaux</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Michel</surname></string-name>, <string-name><given-names>Bertrand</given-names> <surname>Thirion</surname></string-name>, <string-name><given-names>Olivier</given-names> <surname>Grisel</surname></string-name>, <string-name><given-names>Mathieu</given-names> <surname>Blondel</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Müller</surname></string-name>, <string-name><given-names>Joel</given-names> <surname>Nothman</surname></string-name>, <string-name><given-names>Gilles</given-names> <surname>Louppe</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Prettenhofer</surname></string-name>, <string-name><given-names>Ron</given-names> <surname>Weiss</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Dubourg</surname></string-name>, <string-name><given-names>Jake</given-names> <surname>Vanderplas</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Passos</surname></string-name>, <string-name><given-names>David</given-names> <surname>Cournapeau</surname></string-name>, <string-name><given-names>Matthieu</given-names> <surname>Brucher</surname></string-name>, <string-name><given-names>Matthieu</given-names> <surname>Perrot</surname></string-name>, and <string-name><given-names>Édouard</given-names> <surname>Duchesnay</surname></string-name>. <year>2018</year>. <article-title>Scikit-Learn: Machine Learning in Python</article-title>.<source>Journal of Machine Learning Research</source></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><surname>Penhune</surname>, <given-names>V. B.</given-names></string-name>, <string-name><given-names>R. J.</given-names> <surname>Zatorre</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>MacDonald</surname></string-name>, and <string-name><given-names>A. C.</given-names> <surname>Evans</surname></string-name>. <year>1996</year>. <article-title>Interhemispheric Anatomical Differences in Human Primary Auditory Cortex: Probabilistic Mapping and Volume Measurement from Magnetic Resonance Scans</article-title>. <source>Cerebral Cortex</source> <volume>6</volume>(<issue>5</issue>):<fpage>661</fpage>–<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/6.5.661</pub-id>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><string-name><surname>Pernet</surname>, <given-names>Cyril R.</given-names></string-name>, <string-name><given-names>Phil</given-names> <surname>McAleer</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Latinus</surname></string-name>, <string-name><given-names>Krzysztof J.</given-names> <surname>Gorgolewski</surname></string-name>, <string-name><given-names>Ian</given-names> <surname>Charest</surname></string-name>, <string-name><given-names>Patricia E. G.</given-names> <surname>Bestelmeyer</surname></string-name>, <string-name><given-names>Rebecca H.</given-names> <surname>Watson</surname></string-name>, <string-name><given-names>David</given-names> <surname>Fleming</surname></string-name>, <string-name><given-names>Frances</given-names> <surname>Crabbe</surname></string-name>, <string-name><given-names>Mitchell</given-names> <surname>Valdes-Sosa</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Belin</surname></string-name>. <year>2015</year>. “<article-title>The Human Voice Areas: Spatial Organization and Inter-Individual Variability in Temporal and Extra-Temporal Cortices</article-title>.” <source>NeuroImage</source> <volume>119</volume>:<fpage>164</fpage>–<lpage>74</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.06.050</pub-id>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>Petkov</surname>, <given-names>Christopher I.</given-names></string-name>, <string-name><given-names>Christoph</given-names> <surname>Kayser</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Steudel</surname></string-name>, <string-name><given-names>Kevin</given-names> <surname>Whittingstall</surname></string-name>, <string-name><given-names>Mark</given-names> <surname>Augath</surname></string-name>, and <string-name><given-names>Nikos K.</given-names> <surname>Logothetis</surname></string-name>. <year>2008</year>. “<article-title>A Voice Region in the Monkey Brain</article-title>.” <source>Nature Neuroscience</source> <volume>11</volume>(<issue>3</issue>):<fpage>367</fpage>–<lpage>74</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn2043</pub-id>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Roswandowitz</surname>, <given-names>Claudia</given-names></string-name>, <string-name><given-names>Claudia</given-names> <surname>Kappes</surname></string-name>, <string-name><given-names>Hellmuth</given-names> <surname>Obrig</surname></string-name>, and <string-name><given-names>Katharina</given-names> <surname>von Kriegstein</surname></string-name>. <year>2018</year>. “<article-title>Obligatory and Facultative Brain Regions for Voice-Identity Recognition</article-title>.” <source>Brain</source> <volume>141</volume>(<issue>1</issue>):<fpage>234</fpage>–<lpage>47</lpage>. doi: <pub-id pub-id-type="doi">10.1093/brain/awx313</pub-id>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><string-name><surname>Rupp</surname>, <given-names>Kyle</given-names></string-name>, <string-name><given-names>Jasmine L.</given-names> <surname>Hect</surname></string-name>, <string-name><given-names>Madison</given-names> <surname>Remick</surname></string-name>, <string-name><given-names>Avniel</given-names> <surname>Ghuman</surname></string-name>, <string-name><given-names>Bharath</given-names> <surname>Chandrasekaran</surname></string-name>, <string-name><given-names>Lori L.</given-names> <surname>Holt</surname></string-name>, and <string-name><given-names>Taylor J.</given-names> <surname>Abel</surname></string-name>. <year>2022</year>. “<article-title>Neural Responses in Human Superior Temporal Cortex Support Coding of Voice Representations</article-title>.” <source>PLOS Biology</source> <volume>20</volume>(<issue>7</issue>):<fpage>e3001675</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.3001675</pub-id>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><string-name><surname>Santoro</surname>, <given-names>Roberta</given-names></string-name>, <string-name><given-names>Michelle</given-names> <surname>Moerel</surname></string-name>, <string-name><given-names>Federico</given-names> <surname>De Martino</surname></string-name>, <string-name><given-names>Giancarlo</given-names> <surname>Valente</surname></string-name>, <string-name><given-names>Kamil</given-names> <surname>Ugurbil</surname></string-name>, <string-name><given-names>Essa</given-names> <surname>Yacoub</surname></string-name>, and <string-name><given-names>Elia</given-names> <surname>Formisano</surname></string-name>. <year>2017</year>. “<article-title>Reconstructing the Spectrotemporal Modulations of Real-Life Sounds from fMRI Response Patterns</article-title>.” <source>Proceedings of the National Academy of Sciences</source> <volume>114</volume>(<issue>18</issue>):<fpage>4799</fpage>–<lpage>4804</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1617622114</pub-id>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><string-name><surname>Schrimpf</surname>, <given-names>Martin</given-names></string-name>, <string-name><given-names>Idan Asher</given-names> <surname>Blank</surname></string-name>, <string-name><given-names>Greta</given-names> <surname>Tuckute</surname></string-name>, <string-name><given-names>Carina</given-names> <surname>Kauf</surname></string-name>, <string-name><given-names>Eghbal A.</given-names> <surname>Hosseini</surname></string-name>, <string-name><given-names>Nancy</given-names> <surname>Kanwisher</surname></string-name>, <string-name><given-names>Joshua B.</given-names> <surname>Tenenbaum</surname></string-name>, and <string-name><given-names>Evelina</given-names> <surname>Fedorenko</surname></string-name>. <year>2021</year>. “<article-title>The Neural Architecture of Language: Integrative Modeling Converges on Predictive Processing</article-title>.” <source>Proceedings of the National Academy of Sciences</source> <volume>118</volume>(<issue>45</issue>):<fpage>e2105646118</fpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.2105646118</pub-id>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="preprint"><string-name><surname>Schrimpf</surname>, <given-names>Martin</given-names></string-name>, <string-name><given-names>Jonas</given-names> <surname>Kubilius</surname></string-name>, <string-name><given-names>Ha</given-names> <surname>Hong</surname></string-name>, <string-name><given-names>Najib J.</given-names> <surname>Majaj</surname></string-name>, <string-name><given-names>Rishi</given-names> <surname>Rajalingham</surname></string-name>, <string-name><given-names>Elias B.</given-names> <surname>Issa</surname></string-name>, <string-name><given-names>Kohitij</given-names> <surname>Kar</surname></string-name>, <string-name><given-names>Pouya</given-names> <surname>Bashivan</surname></string-name>, <string-name><given-names>Jonathan</given-names> <surname>Prescott-Roy</surname></string-name>, <string-name><given-names>Franziska</given-names> <surname>Geiger</surname></string-name>, <string-name><given-names>Kailyn</given-names> <surname>Schmidt</surname></string-name>, <string-name><given-names>Daniel L. K.</given-names> <surname>Yamins</surname></string-name>, and <string-name><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name>. <year>2018</year>. <article-title>Brain-Score: Which Artificial Neural Network for Object Recognition Is Most Brain-Like?</article-title>. <source>bioRxiv</source>. doi: <pub-id pub-id-type="doi">10.1101/407007</pub-id>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="preprint"><string-name><surname>Schütt</surname>, <given-names>Heiko H.</given-names></string-name>, <string-name><given-names>Alexander D.</given-names> <surname>Kipnis</surname></string-name>, <string-name><given-names>Jörn</given-names> <surname>Diedrichsen</surname></string-name>, and <string-name><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>. <year>2021</year>. <article-title>Statistical Inference on Representational Geometries</article-title>. <source>arXiv</source></mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><string-name><surname>Schweinberger</surname>, <given-names>S. R.</given-names></string-name>, <string-name><given-names>A.</given-names> <surname>Herholz</surname></string-name>, and <string-name><given-names>W.</given-names> <surname>Sommer</surname></string-name>. <year>1997</year>. “<article-title>Recognizing Famous Voices: Influence of Stimulus Duration and Different Types of Retrieval Cues</article-title>.” <source>Journal of Speech, Language, and Hearing Research: JSLHR</source> <volume>40</volume>(<issue>2</issue>):<fpage>453</fpage>–<lpage>63</lpage>. doi: <pub-id pub-id-type="doi">10.1044/jslhr.4002.453</pub-id>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>Stephen M.</given-names></string-name>, <string-name><given-names>Mark</given-names> <surname>Jenkinson</surname></string-name>, <string-name><given-names>Mark W.</given-names> <surname>Woolrich</surname></string-name>, <string-name><given-names>Christian F.</given-names> <surname>Beckmann</surname></string-name>, <string-name><given-names>Timothy E. J.</given-names> <surname>Behrens</surname></string-name>, <string-name><given-names>Heidi</given-names> <surname>Johansen-Berg</surname></string-name>, <string-name><given-names>Peter R.</given-names> <surname>Bannister</surname></string-name>, <string-name><given-names>Marilena</given-names> <surname>De Luca</surname></string-name>, <string-name><given-names>Ivana</given-names> <surname>Drobnjak</surname></string-name>, <string-name><given-names>David E.</given-names> <surname>Flitney</surname></string-name>, <string-name><given-names>Rami K.</given-names> <surname>Niazy</surname></string-name>, <string-name><given-names>James</given-names> <surname>Saunders</surname></string-name>, <string-name><given-names>John</given-names> <surname>Vickers</surname></string-name>, <string-name><given-names>Yongyue</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Nicola</given-names> <surname>De Stefano</surname></string-name>, <collab>J.</collab> <string-name><given-names>Michael</given-names> <surname>Brady</surname></string-name>, and <string-name><given-names>Paul M.</given-names> <surname>Matthews</surname></string-name>. <year>2004</year>. “<article-title>Advances in Functional and Structural MR Image Analysis and Implementation as FSL</article-title>.” <source>NeuroImage</source> <volume>23</volume>:<fpage>S208</fpage>–<lpage>19</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><string-name><surname>Snoek</surname>, <given-names>Lukas</given-names></string-name>, <string-name><given-names>Maite M.</given-names> <surname>van der Miesen</surname></string-name>, <string-name><given-names>Tinka</given-names> <surname>Beemsterboer</surname></string-name>, <string-name><given-names>Andries</given-names> <surname>van der Leij</surname></string-name>, <string-name><given-names>Annemarie</given-names> <surname>Eigenhuis</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Steven Scholte</surname></string-name>. <year>2021</year>. “<article-title>The Amsterdam Open MRI Collection, a Set of Multimodal MRI Datasets for Individual Difference Analyses</article-title>.” <source>Scientific Data</source> <volume>8</volume>(<issue>1</issue>):<fpage>85</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-021-00870-6</pub-id>.</mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><string-name><surname>Trapeau</surname>, <given-names>Régis</given-names></string-name>, <string-name><given-names>Etienne</given-names> <surname>Thoret</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Belin</surname></string-name>. <year>2022</year>. “<article-title>The Temporal Voice Areas Are Not ‘Just’ Speech Areas</article-title>.” <source>Frontiers in Neuroscience</source> <volume>16</volume>:<fpage>1075288</fpage>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2022.1075288</pub-id>.</mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><string-name><surname>Tuckute</surname>, <given-names>Greta</given-names></string-name>, <string-name><given-names>Jenelle</given-names> <surname>Feather</surname></string-name>, <string-name><given-names>Dana</given-names> <surname>Boebinger</surname></string-name>, and <string-name><given-names>Josh H.</given-names> <surname>McDermott</surname></string-name>. <year>2023</year>. “<article-title>Many but Not All Deep Neural Network Audio Models Capture Brain Responses and Exhibit Correspondence between Model Stages and Brain Regions</article-title>.” <source>PLOS Biology</source> <volume>21</volume>(<issue>12</issue>):<fpage>e3002366</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.3002366</pub-id>.</mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><string-name><surname>Vallat</surname>, <given-names>Raphael</given-names></string-name>. <year>2018</year>. “<article-title>Pingouin: Statistics in Python</article-title>.” <source>Journal of Open Source Software</source> <volume>3</volume>(<issue>31</issue>):<fpage>1026</fpage>. doi: <pub-id pub-id-type="doi">10.21105/joss.01026</pub-id>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><string-name><surname>VanRullen</surname>, <given-names>Rufin</given-names></string-name>, and <string-name><given-names>Leila</given-names> <surname>Reddy</surname></string-name>. <year>2019</year>. “<article-title>Reconstructing Faces from fMRI Patterns Using Deep Generative Neural Networks</article-title>.” <source>Communications Biology</source> <volume>2</volume>(<issue>1</issue>):<fpage>193</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s42003-019-0438-y</pub-id>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><string-name><surname>Walther</surname>, <given-names>Alexander</given-names></string-name>, <string-name><given-names>Hamed</given-names> <surname>Nili</surname></string-name>, <string-name><given-names>Naveed</given-names> <surname>Ejaz</surname></string-name>, <string-name><given-names>Arjen</given-names> <surname>Alink</surname></string-name>, <string-name><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>, and <string-name><given-names>Jörn</given-names> <surname>Diedrichsen</surname></string-name>. <year>2016</year>. “<article-title>Reliability of Dissimilarity Measures for Multi-Voxel Pattern Analysis</article-title>.” <source>NeuroImage</source> <volume>137</volume>:<fpage>188</fpage>–<lpage>200</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>Xiaosha</given-names></string-name>, <string-name><given-names>Yangwen</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>Yuwei</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Zeng</surname></string-name>, <string-name><given-names>Jiacai</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Zhenhua</given-names> <surname>Ling</surname></string-name>, and <string-name><given-names>Yanchao</given-names> <surname>Bi</surname></string-name>. <year>2018</year>. “<article-title>Representational Similarity Analysis Reveals Task-Dependent Semantic Influence of the Visual Word Form Area</article-title>.” <source>Scientific Reports</source> <volume>8</volume>(<issue>1</issue>):<fpage>3047</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-018-21062-0</pub-id>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><string-name><surname>Wetzel</surname>, <given-names>Sebastian J</given-names></string-name>. <year>2017</year>. “<article-title>Unsupervised Learning of Phase Transitions: From Principal Component Analysis to Variational Autoencoders</article-title>.” <source>Physical Review E</source> <volume>96</volume>(<issue>2</issue>):<fpage>022140</fpage>. doi: <pub-id pub-id-type="doi">10.1103/PhysRevE.96.022140</pub-id>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><string-name><surname>Woods</surname>, <given-names>Kevin J. P.</given-names></string-name>, <string-name><given-names>Max H.</given-names> <surname>Siegel</surname></string-name>, <string-name><given-names>James</given-names> <surname>Traer</surname></string-name>, and <string-name><given-names>Josh H.</given-names> <surname>McDermott</surname></string-name>. <year>2017</year>. <article-title>Headphone Screening to Facilitate Web-Based Auditory Experiments</article-title> <source>Attention, Perception, &amp; Psychophysics</source> <volume>79</volume>(<issue>7</issue>):<fpage>2064</fpage>–<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13414-017-1361-2</pub-id>.</mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>Michael C. K.</given-names></string-name>, <string-name><given-names>Stephen V.</given-names> <surname>David</surname></string-name>, and <string-name><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name>. <year>2006</year>. “<article-title>COMPLETE FUNCTIONAL CHARACTERIZATION OF SENSORY NEURONS BY SYSTEM IDENTIFICATION</article-title>.” <source>Annual Review of Neuroscience</source> <volume>29</volume>(<issue>1</issue>):<fpage>477</fpage>–<lpage>505</lpage>. doi: <pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113024</pub-id>.</mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="confproc"><string-name><surname>Xing</surname>, <given-names>Chao</given-names></string-name>, <string-name><given-names>Dong</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Chao</given-names> <surname>Liu</surname></string-name>, and <string-name><given-names>Yiye</given-names> <surname>Lin</surname></string-name>. <year>2015</year>. “<article-title>Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation</article-title>.” Pp. <fpage>1006</fpage>–<lpage>11</lpage> <source>Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Denver, Colorado: Association for Computational Linguistics</source>.</mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><string-name><surname>Yamins</surname>, <given-names>Daniel L. K.</given-names></string-name>, and <string-name><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name>. <year>2016</year>. “<article-title>Using Goal-Driven Deep Learning Models to Understand Sensory Cortex</article-title>.” <source>Nature Neuroscience</source> <volume>19</volume>(<issue>3</issue>):<fpage>356</fpage>–<lpage>65</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn.4244</pub-id>.</mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><string-name><surname>Zäske</surname>, <given-names>Romi</given-names></string-name>, <string-name><given-names>Marie-Christin</given-names> <surname>Perlich</surname></string-name>, and <string-name><given-names>Stefan R.</given-names> <surname>Schweinberger</surname></string-name>. <year>2016</year>. <article-title>To Hear or Not to Hear: Voice Processing under Visual Load</article-title> <source>Attention, Perception, &amp; Psychophysics</source> <volume>78</volume>(<issue>5</issue>):<fpage>1488</fpage>–<lpage>95</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13414-016-1119-2</pub-id>.</mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname>, <given-names>Yang</given-names></string-name>, <string-name><given-names>Yue</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Juan</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Wenjing</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>Zhipei</given-names> <surname>Ling</surname></string-name>, <string-name><given-names>Bo</given-names> <surname>Hong</surname></string-name>, and <string-name><given-names>Xiaoqin</given-names> <surname>Wang</surname></string-name>. <year>2021</year>. “<article-title>Hierarchical Cortical Networks of ‘Voice Patches’ for Processing Voices in Human Brain</article-title>.” <source>Proceedings of the National Academy of Sciences</source> <volume>118</volume>(<issue>52</issue>):<fpage>e2113887118</fpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.2113887118</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98047.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study used deep neural networks (DNN) to reconstruct voice information (viz., speaker identity), from fMRI responses in the auditory cortex and temporal voice areas, and assessed the representational content in these areas with decoding. A DNN-derived feature space approximated the neural representation of speaker identity-related information. While some of the neural decoding results are <bold>valuable</bold>, the overall evidence for general representational and computational principles is <bold>incomplete</bold> as the results rely on a very specific model architecture.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98047.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this study, the authors trained a variational autoencoder (VAE) to create a high-dimensional &quot;voice latent space&quot; (VLS) using extensive voice samples, and analyzed how this space corresponds to brain activity through fMRI studies focusing on the temporal voice areas (TVAs). Their analyses included encoding and decoding techniques, as well as representational similarity analysis (RSA), which showed that the VLS could effectively map onto and predict brain activity patterns, allowing for the reconstruction of voice stimuli that preserve key aspects of speaker identity.</p>
<p>Strengths:</p>
<p>This paper is well-written and easy to follow. Most of the methods and results were clearly described. The authors combined a variety of analytical methods in neuroimaging studies, including encoding, decoding, and RSA. In addition to commonly used DNN encoding analysis, the authors performed DNN decoding and resynthesized the stimuli using VAE decoders. Furthermore, in addition to machine learning classifiers, the authors also included human behavioral tests to evaluate the reconstruction performance.</p>
<p>Weaknesses:</p>
<p>This manuscript presents a variational autoencoder (VAE) to evaluate voice identity representations from brain recordings. However, the study's scope is limited by testing only one model, leaving unclear how generalizable or impactful the findings are. The preservation of identity-related information in the voice latent space (VLS) is expected, given the VAE model's design to reconstruct original vocal stimuli. Nonetheless, the study lacks a deeper investigation into what specific aspects of auditory coding these latent dimensions represent. The results in Figure 1c-e merely tested a very limited set of speech features. Moreover, there is no analysis of how these features and the whole VAE model perform in standard speech tasks like speech recognition or phoneme recognition. It is not clear what kind of computations the VAE model presented in this work is capable of. Inclusion of comparisons with state-of-the-art unsupervised or self-supervised speech models known for their alignment with auditory cortical responses, such as Wav2Vec2, HuBERT, and Whisper, would strengthen the validation of the VAE model and provide insights into its relative capabilities and limitations.</p>
<p>The claim that the VLS outperforms a linear model (LIN) in decoding tasks does not significantly advance our understanding of the underlying brain representations. Given the complexity of auditory processing, it is unsurprising that a nonlinear model would outperform a simpler linear counterpart. The study could be improved by incorporating a comparative analysis with alternative models that differ in architecture, computational strategies, or training methods. Such comparisons could elucidate specific features or capabilities of the VLS, offering a more nuanced understanding of its effectiveness and the computational principles it embodies. This approach would allow the authors to test specific hypotheses about how different aspects of the model contribute to its performance, providing a clearer picture of the shared coding in VLS and the brain.</p>
<p>The manuscript overlooks some crucial alternative explanations for the discriminant representation of vocal identity. For instance, the discriminant representation of vocal identity can be either a higher-level abstract representation or a lower-level coding of pitch height. Prior studies using fMRI and ECoG have identified both types of representation within the superior temporal gyrus (STG) (e.g., Tang et al., Science 2017; Feng et al., NeuroImage 2021). Additionally, the methodology does not clarify whether the stimuli from different speakers contained identical speech content. If the speech content varied across speakers, the approach of averaging trials to obtain a mean vector for each speaker-the &quot;identity-based analysis&quot;-may not adequately control for confounding acoustic-phonetic features. Notably, the principal component 2 (PC2) in Figure 1b appears to correlate with absolute pitch height, suggesting that some aspects of the model's effectiveness might be attributed to simpler acoustic properties rather than complex identity-specific information.</p>
<p>Methodologically, there are issues that warrant attention. In characterizing the autoencoder latent space, the authors initialized logistic regression classifiers 100 times and calculated the t-statistics using degrees of freedom (df) of 99. Given that logistic regression is a convex optimization problem typically converging to a global optimum, these multiple initializations of the classifier were likely not entirely independent. Consequently, the reported degrees of freedom and the effect size estimates might not accurately reflect the true variability and independence of the classifier outcomes. A more careful evaluation of these aspects is necessary to ensure the statistical robustness of the results.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98047.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Lamothe et al. collected fMRI responses to many voice stimuli in 3 subjects. The authors trained two different autoencoders on voice audio samples and predicted latent space embeddings from the fMRI responses, allowing the voice spectrograms to be reconstructed. The degree to which reconstructions from different auditory ROIs correctly represented speaker identity, gender, or age was assessed by machine classification and human listener evaluations. Complementing this, the representational content was also assessed using representational similarity analysis. The results broadly concur with the notion that temporal voice areas are sensitive to different types of categorical voice information.</p>
<p>Strengths:</p>
<p>The single-subject approach that allows thousands of responses to unique stimuli to be recorded and analyzed is powerful. The idea of using this approach to probe cortical voice representations is strong and the experiment is technically solid.</p>
<p>Weaknesses:</p>
<p>The paper could benefit from more discussion of the assumptions behind the reconstruction analyses and the conclusions it allows. The authors write that reconstruction of a stimulus from brain responses represents 'a robust test of the adequacy of models of brain activity' (L138). I concur that stimulus reconstruction is useful for evaluating the nature of representations, but the notion that they can test the adequacy of the specific autoencoder presented here as a model of brain activity should be discussed at more length. Natural sounds are correlated in many feature dimensions and can therefore be summarized in several ways, and similar information can be read out from different model representations. Models trained to reconstruct natural stimuli can exploit many correlated features and it is quite possible that very different models based on different features can be used for similar reconstructions. Reconstructability does not by itself imply that the model is an accurate brain model. Non-linear networks trained on natural stimuli are arguably not tested in the same rigorous manner as models built to explicitly account for computations (they can generate predictions and experiments can be designed to test those predictions). While it is true that there is increasing evidence that neural network embeddings can predict brain data well, it is still a matter of debate whether good predictability by itself qualifies DNNs as 'plausible computational models for investigating brain processes' (L72). This concern is amplified in the context of decoding and naturalistic stimuli where many correlated features can be represented in many ways. It is unclear how much the results hinge on the specificities of the specific autoencoder architectures used. For instance, it would be useful to know the motivations for why the specific VAE used here should constitute a good model for probing neural voice representations.</p>
<p>Relatedly, it is not clear how VAEs as generative models are motivated as computational models of voice representations in the brain. The task of voice areas in the brain is not to generate voice stimuli but to discriminate and extract information. The task of reconstructing an input spectrogram is perhaps useful for probing information content, but discriminative models, e.g., trained on the task of discriminating voices, would seem more obvious candidates. Why not include discriminatively trained models for comparison?</p>
<p>The autoencoder learns a mapping from latent space to well-formed voice spectrograms. Regularized regression then learns a mapping between this latent space and activity space. All reconstructions might sound 'natural', which simply means that the autoencoder works. It would be good to have a stronger test of how close the reconstructions are to the original stimulus. For instance, is the reconstruction the closest stimulus to the original in latent space coordinates out of using the experimental stimuli, or where does it rank? How do small changes in beta amplitudes impact the reconstruction? The effective dimensionality of the activity space could be estimated, e.g. by PCA of the voice samples' contrast maps, and it could then be estimated how the main directions in the activity space map to differences in latent space. It would be good to get a better grasp of the granularity of information that can be decoded/ reconstructed.</p>
<p>What can we make of the apparent trend that LIN is higher than VLS for identity classification (at least VLS does not outperform LIN)? A general argument of the paper seems to be that VLS is a better model of voice representations compared to LIN as a 'control' model. Then we would expect VLS to perform better on identity classification. The age and gender of a voice can likely be classified from many acoustic features that may not require dedicated voice processing.</p>
<p>The RDM results reported are significant only for some subjects and in some ROIs. This presumably means that results are not significant in the other subjects. Yet, the authors assert general conclusions (e.g. the VLS better explains RDM in TVA than LIN). An assumption typically made in single-subject studies (with large amounts of data in individual subjects) is that the effects observed and reported in papers are robust in individual subjects. More than one subject is usually included to hint that this is the case. This is an intriguing approach. However, reports of effects that are statistically significant in some subjects and some ROIs are difficult to interpret. This, in my view, runs contrary to the logic and leverage of the single-subject approach. Reporting results that are only significant in 1 out of 3 subjects and inferring general conclusions from this seems less convincing.</p>
<p>The first main finding is stated as being that '128 dimensions are sufficient to explain a sizeable portion of the brain activity' (L379). What qualifies this? From my understanding, only models of that dimensionality were tested. They explain a sizeable portion of brain activity, but it is difficult to follow what 'sizable' is without baseline models that estimate a prediction floor and ceiling. For instance, would autoencoders that reconstruct any spectrogram (not just voice) also predict a sizable portion of the measured activity? What happens to reconstruction results as the dimensionality is varied?</p>
<p>A second main finding is stated as being that the 'VLS outperforms the LIN space' (L381). It seems correct that the VAE yields more natural-sounding reconstructions, but this is a technical feature of the chosen autoencoding approach. That the VLS yields a 'more brain-like representational space' I assume refers to the RDM results where the RDM correlations were mainly significant in one subject. For classification, the performance of features from the reconstructions (age/ gender/ identity) gives results that seem more mixed, and it seems difficult to draw a general conclusion about the VLS being better. It is not clear that this general claim is well supported.</p>
<p>It is not clear why the RDM was not formed based on the 'stimulus GLM' betas. The 'identity GLM' is already biased towards identity and it would be stronger to show associations at the stimulus level.</p>
<p>Multiple comparisons were performed across ROIs, models, subjects, and features in the classification analyses, but it is not clear how correction for these multiple comparisons was implemented in the statistical tests on classification accuracies.</p>
<p>Risks of overfitting and bias are a recurrent challenge in stimulus reconstruction with fMRI. It would be good with more control analyses to ensure that this was not the case. For instance, how were the repeated test stimuli presented? Were they intermingled with the other stimuli used for training or presented in separate runs? If intermingled, then the training and test data would have been preprocessed together, which could compromise the test set. The reconstructions could be performed on responses from independent runs, preprocessed separately, as a control. This should include all preprocessing, for instance, estimating stimulus/identity GLMs on separately processed run pairs rather than across all runs. Also, it would be good to avoid detrending before GLM denoising (or at least testing its effects) as these can interact.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98047.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this manuscript, Lamothe et al. sought to identify the neural substrates of voice identity in the human brain by correlating fMRI recordings with the latent space of a variational autoencoder (VAE) trained on voice spectrograms. They used encoding and decoding models, and showed that the &quot;voice&quot; latent space (VLS) of the VAE performs, in general, (slightly) better than a linear autoencoder's latent space. Additionally, they showed dissociations in the encoding of voice identity across the temporal voice areas.</p>
<p>Strengths:</p>
<p>- The geometry of the neural representations of voice identity has not been studied so far. Previous studies on the content of speech and faces in vision suggest that such geometry could exist. This study demonstrates this point systematically, leveraging a specifically trained variational autoencoder.</p>
<p>- The size of the voice dataset and the length of the fMRI recordings ensure that the findings are robust.</p>
<p>Weaknesses:</p>
<p>- Overall, the VLS is often only marginally better than the linear model across analysis, raising the question of whether the observed performance improvements are due to the higher number of parameters trained in the VAE, rather than the non-linearity itself. A fair comparison would necessitate that the number of parameters be maintained consistently across both models, at least as an additional verification step.</p>
<p>- The encoding and RSM results are quite different. This is unexpected, as similar embedding geometries between the VLS and the brain activations should be reflected by higher correlation values of the encoding model.</p>
<p>- The consistency across participants is not particularly high, for instance, S1 seemed to have demonstrated excellent performances, while S2 showed poor performance.</p>
<p>- An important control analysis would be to compare the decoding results with those obtained by a decoder operating directly on the latent spaces, in order to further highlight the interest of the non-linear transformations of the decoder model. Currently, it is unclear whether the non-linearity of the decoder improves the decoding performance, considering the poor resemblance between the VLS and brain-reconstructed spectrograms.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98047.1.sa4</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Lamothe</surname>
<given-names>Charly</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9918-8258</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Thoret</surname>
<given-names>Etienne</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8214-6278</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Trapeau</surname>
<given-names>Régis</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1137-8669</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Giordano</surname>
<given-names>Bruno L</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7002-0486</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Sein</surname>
<given-names>Julien</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1767-5330</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Takerkart</surname>
<given-names>Sylvain</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8410-0962</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Ayache</surname>
<given-names>Stéphane</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2982-7127</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Artières</surname>
<given-names>Thierry</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3696-0321</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Belin</surname>
<given-names>Pascal</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7578-6365</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>Please find below our provisional author response, outlining the revisions we plan to undertake to address the Recommendations received:</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>(1) A set of recent advances have shown that embeddings of unsupervised/self-supervised speech models aligned to auditory responses to speech in the temporal cortex (e.g. Wav2Vec2: Millet et al NeurIPS 2022; HuBERT: Li et al. Nat Neurosci 2023; Whisper: Goldstein et al. bioRxiv 2023). These models are known to preserve a variety of speech information (phonetics, linguistic information, emotions, speaker identity, etc) and perform well in a variety of downstream tasks. These other models should be evaluated or at least discussed in the study.</p>
</disp-quote>
<p>We plan to evaluate two of these other models, Wav2Vec2 and HuBERT, in the brain encoding and RSA parts.</p>
<disp-quote content-type="editor-comment">
<p>(2) The test statistics of the results in Fig 1c-e need to be revised. Given that logistic regression is a convex optimization problem typically converging to a global optimum, these multiple initializations of the classifier were likely not entirely independent. Consequently, the reported degrees of freedom and the effect size estimates might not accurately reflect the true variability and independence of the classifier outcomes. A more careful evaluation of these aspects is necessary to ensure the statistical robustness of the results.</p>
</disp-quote>
<p>We plan to address this point to ensure the statistical robustness of our results.</p>
<disp-quote content-type="editor-comment">
<p>(3) In Line 198, the authors discuss the number of dimensions used in their models. To provide a comprehensive comparison, it would be informative to include direct decoding results from the original spectrograms alongside those from the VLS and LIN models. Given the vast diversity in vocal speech characteristics, it is plausible that the speaker identities might correlate with specific speech-related features also represented in both the auditory cortex and the VLS. Therefore, a clearer understanding of the original distribution of voice identities in the untransformed auditory space would be beneficial. This addition would help ascertain the extent to which transformations applied by the VLS or LIN models might be capturing or obscuring relevant auditory information.</p>
</disp-quote>
<p>We plan to include direct decoding results from the original spectrograms in addition from the VLS and LIN models.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
</disp-quote>
<p>We plan to address the following points raised by Reviewer #2:</p>
<disp-quote content-type="editor-comment">
<p>(1) English mistakes, rewordings:</p>
<p>a. L31: 'in voice' &gt; consider rewording (from a voice?).</p>
<p>b. L33: consider splitting sentence (after interactions).</p>
<p>c. L39: 'brain' after parentheses.</p>
<p>d. L45-: certainly DNNs 'as a powerful tool' extend to audio (not just image and video) beyond their use in brain models.</p>
<p>e. L52: listened to / heard.</p>
<p>f. L63: use second/s consistently.</p>
<p>g. L64: the reference to Figure 5D is maybe a bit confusing here in the introduction.</p>
<p>h. L79-88: this section is formulated in a way that is too detailed for the introduction text (confusing to read). Consider a more general introduction to the VLS concept here and the details of this study later.</p>
<p>i. L99-: again, I think the experimental details are best saved for later. It's good to provide a feel for the analysis pipeline here, but some of the details provided (number of averages, denoising, preprocessing), are anyway too unspecific to allow the reader to fully follow the analysis.</p>
</disp-quote>
<p>We will correct the mistakes, apply the suggested rewordings, and clarify the points raised.</p>
<disp-quote content-type="editor-comment">
<p>(2) Clarification.</p>
<list list-type="bullet">
<list-item><p>L159: what was the motivation for classifying age as a 2-class classification problem? Rather than more classes or continuous prediction? How did you choose the age split?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L263: Is the test of RDM correlation&gt;0 corrected for multiple comparisons across ROIs, subjects, and models?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L379: 'these stimuli' - weren't the experimental stimuli different from those used to train the V/AE?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L443: what are 'technical issues' that prevented subject 3 from participating in 48 runs??</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L444: participants were instructed to 'stay in the scanner'!? Do you mean 'stay still', or something?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L463: Hearing thresholds of 15 dB: do you mean that all had thresholds lower than 15 dB at all frequencies and at all repeated audiogram measurements?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L472: were the 4 category levels balanced across the dataset (in number of occurrences of each category combination)?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L482: the test stimuli were selected as having high energy by the amplitude envelope. It is unclear what this means (how is the envelope extracted, what feature of it is used to measure 'high energy'?)</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L500 was the audio filtered to account for the transfer function of the Sensimetrics headphones?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L500: what does 'comfortable level' correspond to and was it set per session (i.e. did it vary across sessions)?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L526- does the normalization imply that the reconstructed spectrograms are normalized? Were the reconstructions then scaled to undo the normalization before inversion?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L606: does the identity GLM model the denoised betas from the first GLM or simply the BOLD data? The text indicates the latter, but I suspect the former.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L704: could you unpack this a bit more? It is not easy to see why you specify the summing in the objective. Shouldn't this just be the ridge objective for a given voxel/ROI? Then you could just state it in matrix notation.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L716: you used robust scaling for the classifications in latent space but haven't mentioned scaling here. Are we to assume that the same applies?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>L720: Pearson correlation as a performance metric and its variance will depend on the choice of test/train split sizes. Can you show that the results generalize beyond your specific choices? Maybe the report explained variance as well to get a better idea of performance.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Could you specify (somewhere) the stimulus timing in a run? ISI and stimulus duration are mentioned in different places, but it would be nice to have a summary of the temporal structure of runs.</p>
</list-item></list>
</disp-quote>
<p>We will clarify the points raised.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
</disp-quote>
<p>We plan to address the following points raised by Reviewer #3:</p>
<disp-quote content-type="editor-comment">
<p>Comments:</p>
<list list-type="bullet">
<list-item><p>Code and data are not currently available.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>In the supplementary material, it would be beneficial to present the different analyses as boxplots, as in the main text, but with the ROIs in the left and right hemispheres separated, to better show potential hemispheric effect. Although this information is available in the Supplementary Tables, it is currently quite tedious to access it.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>In Figure 3a, it might be beneficial to order the identities by age for each gender in order to more clearly illustrate the structure of the RDMs,</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>In Figure 3b, the variance for the correlations for the aTVA is higher than in other regions, why?</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Please make sure that all acronyms are defined, and that they are redefined in the figure legends.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Gender and age are primarily encoded by different brain regions (Figure 5, pTVA vs aTVA). How does this finding compare with existing literature?</p>
</list-item></list>
</disp-quote>
<p>We will upload the code and the preprocessed data; improve the supplementary material figures; Fix Figure 3 according to the Reviewer’s suggestion, and clarify the points raised.</p>
</body>
</sub-article>
</article>