<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108748</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108748</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108748.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Medicine</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Peer reviewers altered their recommendation based on whether they were cited or wanted to be cited. A matched study of open peer review at four journals</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6339-0374</contrib-id>
<name>
<surname>Barnett</surname>
<given-names>Adrian</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<email>a.barnett@qut.edu.au</email>
</contrib>
<aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03pnv4752</institution-id><institution>School of Public Health &amp; Social Work, Queensland University of Technology</institution></institution-wrap>, <city>Brisbane</city>, <country country="AU">Australia</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Rodgers</surname>
<given-names>Peter</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>eLife</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Rodgers</surname>
<given-names>Peter</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>eLife</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<pub-date pub-type="epub">
<day>15</day>
<month>08</month>
<year>2025</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2025-09-22">
<day>22</day>
<month>09</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108748</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2025-08-14">
<day>14</day>
<month>08</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-08-19">
<day>19</day>
<month>08</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.31219/osf.io/wdvr9"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Barnett</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Barnett</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108748-v1.pdf"/>
<self-uri xlink:href="wdvr9.pdf" content-type="docx" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Peer reviewers judge the validity and quality of new research. These judgements would ideally be impartial, but some reviewers may give a more favourable review if they are cited in the article because the authors have recognised their work and because citations are a valuable academic currency. Reviewers sometimes request self-citations to their own work, which may be justified as reviewers should be relevant experts. However, some self-citation requests may be unethical, with reviewers exploiting the authors’ need to publish. We examined whether citations to a reviewer and self-citations influenced their peer review. We used a matched design at four journals that use open peer review and make all article versions available. Our sample included more than 37,000 peer reviews, with 13% where the reviewer was cited in the article and 6% where the reviewer included a self-citation to their work in their review. Reviewers who were cited were more likely to approve the article, with an odds ratio of 1.61 compared with reviewers who were not cited (adjusted 99.4% CI: 1.16 to 2.23). Reviewers who suggested a self-citation were much less likely to approve the article, with an odds ratio of 0.15 compared with reviewers with no self-citations (adjusted 99.4% CI: 0.08 to 0.30). Reviewers who requested and received a citation were much more likely to approve the article compared to reviewers whose request was disregarded (odds ratio of 3.5, 95% CI: 2.0 to 6.1). Some reviewers’ recommendations are dependent on whether they are cited or want to be cited. Self-citation requests can turn peer review into a transaction rather than an objective critique of the article.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p>In 2024, a published peer reviewed article included this remarkable sentence: “As strongly requested by the reviewers, here we cite some references [<xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c47">47</xref>] although they are completely irrelevant to the present work” [<xref ref-type="bibr" rid="c1">1</xref>]. This was a rare public example of coerced citations, where a reviewer exploits the peer review process to increase their citation counts and hence further their own career [<xref ref-type="bibr" rid="c2">2</xref>–<xref ref-type="bibr" rid="c4">4</xref>]. Reviewers should be relevant experts, so some suggestions to cite their work will be appropriate. However, excessive requests for self-citations or requests to cite unrelated work are unethical [<xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c9">9</xref>]. Coerced citations can also come from editors trying to boost their journal’s ranking [<xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c12">12</xref>].</p>
<p>Coerced citations are reported as a common problem in peer review. In author surveys, two-thirds reported pressure from peer reviewers to cite unrelated articles [<xref ref-type="bibr" rid="c13">13</xref>] and 23% had experienced a reviewer that “required them to include unnecessary references to their publication(s)” [<xref ref-type="bibr" rid="c14">14</xref>]. Publishers have investigated whether “hundreds of researchers” have manipulated the peer review process to increase their own citations [<xref ref-type="bibr" rid="c15">15</xref>]. Some reviewers may be exploiting their power over authors who “have a strong incentive to […] accept all ‘suggestions’ by the referees even if one knows that they are misleading or even incorrect” [<xref ref-type="bibr" rid="c16">16</xref>].</p>
<p>As reviewers are often in the same field as the authors, they may already be cited in the article without the need for coerced citations. Reviewers who are cited may give a more favourable peer review and be more willing to overlook flaws [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c18">18</xref>]. Some authors may try to exploit this using “referee baiting” [<xref ref-type="bibr" rid="c3">3</xref>] or “flattery citations” [<xref ref-type="bibr" rid="c19">19</xref>] by favourably citing a reviewer’s work.</p>
<p>The interactions during peer review between authors and reviewers can determine whether an article is accepted [<xref ref-type="bibr" rid="c20">20</xref>] and what results are included in the published version [<xref ref-type="bibr" rid="c21">21</xref>]. Given the importance of peer review for science, studies that examine how peer review works in practice are needed [<xref ref-type="bibr" rid="c22">22</xref>–<xref ref-type="bibr" rid="c26">26</xref>]. Here, we examine interactions between peer reviewers and authors using four journals that publish all article versions and all peer reviews. We had two research questions:
<list list-type="order">
<list-item><p>Do peer reviewers give a more or less favourable recommendation when they are cited in the article?</p></list-item>
<list-item><p>Do peer reviewers give a more or less favourable recommendation when their review includes a self-citation?</p></list-item>
</list>
</p>
</sec>
<sec id="s2" sec-type="methods">
<title>Methods</title>
<sec id="s2-1">
<title>Journal selection</title>
<p>We studied journals from the publisher <italic>F1000</italic> as their journals use open peer review with signed reviewers. <italic>F1000</italic> journals use a publish–review–curate model [<xref ref-type="bibr" rid="c27">27</xref>], meaning all versions of the article are publicly available, including versions updated after peer review. This allowed us to examine the interactions between authors and reviewers throughout the peer review process. We selected four <italic>F1000</italic> journals that each had over 100 articles. Some characteristics of the four journals are given in <xref ref-type="table" rid="tbl1">Table 1</xref>. Three journals were created to support funders.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1</label>
<caption><title>Brief information about the four included journals from the publisher <italic>F1000</italic>.</title></caption>
<alternatives>
<graphic xlink:href="wdvr9v2_tbl1.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="cols">
<thead>
<tr>
<th align="left" valign="top">Journal title</th>
<th align="left" valign="top">Year started</th>
<th align="left" valign="top">Field(s) of research</th>
<th align="left" valign="top">Articles must concern research funded by</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">F1000Research</td>
<td align="left" valign="top">2012</td>
<td align="left" valign="top">All disciplines</td>
<td align="left" valign="top"><italic>No restriction</italic></td>
</tr>
<tr>
<td align="left" valign="top">Wellcome Open Research</td>
<td align="left" valign="top">2016</td>
<td align="left" valign="top">Medicine, Genomics</td>
<td align="left" valign="top">Wellcome</td>
</tr>
<tr>
<td align="left" valign="top">Gates Open Research</td>
<td align="left" valign="top">2017</td>
<td align="left" valign="top">Medicine</td>
<td align="left" valign="top">The Gates Foundation</td>
</tr>
<tr>
<td align="left" valign="top">Open Research Europe</td>
<td align="left" valign="top">2021</td>
<td align="left" valign="top">All disciplines</td>
<td align="left" valign="top">European Commission</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The peer review process used by <italic>F1000</italic> journals differs from most standard journals. The journals do not use academic editors, but do have in-house editors who manage articles but do not make editorial decisions. This means that most interactions during peer review are between authors and reviewers directly. In-house editors perform checks prior to the first version of the article being published and at <italic>F1000Research</italic> this results in 40 to 50% submissions being rejected (personal communication, <italic>F1000</italic> staff). Up to mid-2024, the authors were asked to identify potential reviewers who were qualified experts with no competing interests [<xref ref-type="bibr" rid="c28">28</xref>]. Since mid-2024, reviewer identification is made in-house, although authors can suggest reviewers.</p>
<p>Reviewers are asked to recommend one of three categories: Approved, Approved with reservations, and Not approved. For brevity, we refer to ‘Approved with Reservations’ as ‘Reservations’. An article is indexed once it receives two ‘Approved’ or two ‘Reservations’ and one ‘Approved’. The guidelines for recommending Approved are: “the aims and research methods are adequate; results are presented accurately, and the conclusions are justified and supported by the presented data” [<xref ref-type="bibr" rid="c29">29</xref>]. Peer reviewers are asked to assess the validity of an article’s content, rather than novelty or interest levels, an approach designed to combat publication bias [<xref ref-type="bibr" rid="c30">30</xref>].</p>
<p>All four journals have a peer reviewer code of conduct and state that reviewers should familiarise themselves with the ethical guidelines for peer reviewers by the <italic>Committee On Publication Ethics</italic> [<xref ref-type="bibr" rid="c31">31</xref>]. The journals’ guidelines for reviewers include the following: “reviewers should explicitly state their reasoning when asking authors to cite their own work”.</p>
</sec>
<sec id="s2-2">
<title>Data extraction</title>
<p>We extracted data on authors and articles from the <italic>OpenAlex</italic> database (<ext-link ext-link-type="uri" xlink:href="https://openalex.org/">https://openalex.org/</ext-link>) and directly from the four journals. <italic>OpenAlex</italic> combines scholarly data from multiple sources, including <italic>ORCID</italic> – a unique identifier for researchers, <italic>Microsoft Academic, Crossref</italic> and <italic>PubMed.</italic> A recent study compared <italic>OpenAlex</italic> with the two commonly used proprietary bibliometric databases of <italic>Web of Science</italic> and <italic>Scopus</italic> for the years 2015 to 2022 [<xref ref-type="bibr" rid="c32">32</xref>]. The results were mixed, but <italic>OpenAlex</italic> had better <italic>ORCID</italic> coverage and covered more Digital Object Identifiers (DOIs) – the unique identifier for publications. We accessed <italic>OpenAlex</italic> using the <italic>openalexR</italic> package [<xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c34">34</xref>]. We used each journal’s application programming interface (API) to extract data on the articles and peer reviews. The data were extracted in four stages:</p>
<list list-type="order">
<list-item><p>Searches were made using the APIs of the four journals to find all articles published between 1 Jan 2012 and 28 May 2025, with the start date to capture all potential articles.</p></list-item>
<list-item><p>For each article, the following data were downloaded in XML format:
<list list-type="bullet">
<list-item><p>The article’s publication date and version number</p></list-item>
<list-item><p>The reviewers’ names and <italic>ORCIDs</italic> (if available)</p></list-item>
<list-item><p>The text of all reviews and the reviewers’ recommendations</p></list-item>
<list-item><p>The DOIs and PMIDs <italic>(PubMed</italic> IDs) from the article’s reference list</p></list-item>
<list-item><p>The DOIs and PMIDs of any articles cited by the reviewers. The online peer review system at <italic>F1000</italic> journals includes the DOI of any article cited in the review, which facilitates the identification of self-citations.</p></list-item>
</list>
</p></list-item>
<list-item><p>Articles were excluded if:
<list list-type="bullet">
<list-item><p>They were not peer reviewed or had yet to receive any reviews</p></list-item>
<list-item><p>The reference list was empty</p></list-item>
</list>
</p></list-item>
<list-item><p>The reviewers’ publication histories were collected from <italic>OpenAlex</italic> using their name, institution and <italic>ORCID</italic> (if available). Reviews were excluded if there was no record for the reviewer in <italic>OpenAlex</italic>, or if the reviewer had no published articles as there was no potential for them to be cited or request a self-citation.</p></list-item>
</list>
</sec>
<sec id="s2-3">
<title>Study design</title>
<p>We used two predictor variables about the reviewer:
<list list-type="bullet">
<list-item><p>The number of times they were cited in the article (0,1, 2,…).</p></list-item>
<list-item><p>The number of times they included self-citations to their own work in their review (0,1, 2,…).</p></list-item>
</list>
</p>
<p>We fitted both predictors as linear, but reviewers may behave differently with any citation rather than a linear change, and hence we also fitted both predictors as a binary “none versus any” (0 vs 1, 2, …). We compared the linear and binary alternatives using the Akaike Information Criterion (AIC) to find the parameterisation that best fitted the data [<xref ref-type="bibr" rid="c35">35</xref>].</p>
<p>We matched on article and version to control for confounding by any characteristics of the article [<xref ref-type="bibr" rid="c36">36</xref>]; for example, the article’s topic or writing style. Hence, we compared two or more independent reviewers who considered the same article.</p>
<p>All analyses were stratified by article version, using the first version only or the second and subsequent versions. This is because the reviewers are unknown to the authors for the first version, but from the second version onwards, the authors will know the reviewers as the journals use signed peer reviews. This knowledge could alter the behaviour of authors and reviewers.</p>
<p>Any confounding by the characteristics of the articles was controlled by the matched design, but confounding by the characteristics of the reviewers remains possible [<xref ref-type="bibr" rid="c18">18</xref>]. We considered the potential confounders of the reviewer’s experience and reviewer’s country. More experienced reviewers will likely be cited more often (on average) and could be more or less strict in their recommendations. The reviewer’s country is a potential confounder due to large differences in citation counts by country [<xref ref-type="bibr" rid="c37">37</xref>] and potential differences in recommendations by country [<xref ref-type="bibr" rid="c38">38</xref>].</p>
<p>Some reviews were performed by reviewers together with co-reviewers, who were usually less experienced. Our primary analysis excluded the co-reviewers, but we included them in a sensitivity analysis where we created combined versions of the two independent variables using the sum of citations to reviewers and co-reviewers, and the sum of self-citations from the reviewers and co-reviewers.</p>
<p>The study design is summarised in <xref ref-type="fig" rid="fig1">Figure 1</xref></p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><title>Graphical summary of the study design for research question 1 showing a dummy article and two reviews.</title>
<p>In the first version of the article, the reviewer Smith (blue) is cited whilst Jones (purple) is not. For the second version of the article, the authors are now aware that Jones is a reviewer and Jones has been cited. The reviewers’ recommendations are the outcome and are colour-coded as Not approved (red), Reservations (orange) and Approved (green). We tested whether citations to the reviewer in the article influenced their recommendation. The matched design means that only reviewers of the same article are compared (here, Smith and Jones) and the overall effect is estimated by aggregating over multiple matched comparisons. Research question 2 used the same design but examined self-citations in the reviews.</p></caption>
<graphic xlink:href="wdvr9v2_fig1.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s2-4">
<title>Statistical methods</title>
<p>We used conditional logistic regression to examine the associations between the citations to the reviewer and their ordinal recommendation (Approved → Reservations → Not approved) while matching the article and the version [<xref ref-type="bibr" rid="c39">39</xref>]. Conditional logistic regression requires a binary dependent variable; hence, we fitted two related models that examined the odds of:</p>
<list list-type="order">
<list-item><p>“Approved” compared with “Reservations” or “Not approved”.</p></list-item>
<list-item><p>“Approved” or “Reservations” compared with “Not approved”,</p></list-item>
</list>
<p>These two models tested the same hypothesis, hence we adjusted for multiple testing. We also used repeated testing because of the stratification by article version and the two formulations of the predictors (linear or none versus any). Since we used 8 (2 × 2 × 2) tests, we displayed all the results using 99.4% confidence intervals instead of 95.0% intervals, which is a 5% type I error divided by eight tests.</p>
<p>In an unplanned analysis, we examined the association between the reviewer’s recommendation and whether they included citations to work other than self-citations. This was added to examine whether the citations were used to highlight important errors and/or missing context in the article.</p>
<p>In sensitivity analyses, we controlled for potential confounding due to the reviewer. We used the reviewer’s number of published articles as a proxy for their experience. This association could be non-linear; for example, a diminishing effect for more experienced reviewers, so we examined six fractional polynomials of the reviewers’ number of articles and used the AIC to select the best association [<xref ref-type="bibr" rid="c40">40</xref>].</p>
<p>In a second sensitivity analysis, we planned to use a frailty model for the reviewers’ countries [<xref ref-type="bibr" rid="c41">41</xref>]. However, this model often failed to converge, potentially because there were many countries and some countries had relatively small numbers of reviewers. Hence, we instead used a leave-one-out analysis for each of the top ten most common countries and determined if the results were noticeably different.</p>
<p>Outliers were not excluded. No data were missing in the analysis data set. We tested for potential bias from reviewers who were excluded because they had no data in <italic>OpenAlex</italic> using an elastic net with potential predictors of article version, article date, reviewer’s country, and role (reviewer or co-reviewer) and dependent variable of reviewer excluded (yes/no) [<xref ref-type="bibr" rid="c42">42</xref>].</p>
</sec>
<sec id="s2-5">
<title>Text analysis</title>
<p>We examined how self-citations were justified in the reviews and whether the text differed according to the reviewer’s recommendation. For an initial view of self-citations, we randomly selected 20 reviews that included a self-citation and extracted the most relevant sentence.</p>
<p>To analyse the review text, we first extracted the 100 most commonly used words across all reviews. To standardise the text, all words were transformed into tokens, with stop-words removed and then stemmed. We then tested which of the 100 words were associated with recommending Approved versus Reservations or Not approved amongst those reviewers who included a self-citation. We chose the set of words using an elastic net with 10-fold cross-validation and selected a parsimonious model by using the lambda within one standard error of the minimum cross-validated error [<xref ref-type="bibr" rid="c43">43</xref>]. To get uncertainty intervals for the estimates, we fitted a Bayesian model with the set of words selected by the elastic net and using a sceptical Normal prior centred on zero to create shrinkage.</p>
</sec>
<sec id="s2-6">
<title>Sample size</title>
<p>We aimed for a sample size of approximately 5,000 articles and assumed that half would be the first version, giving a sample size of 2,500 articles for the analysis using the first version only [<xref ref-type="bibr" rid="c44">44</xref>]. In 1,000 simulations, this gave an 89.1% power to detect an odds ratio of 1.5 using conditional logistic regression for a reviewer who recommended a higher category (Approved → Reservations → Not approved) when they were cited. We assumed that 15% of articles would include a citation to the reviewer. Eighty percent of the simulated articles had two reviews, and the remaining 20% had three reviews. Based on preliminary data from two journals, we assumed that the reviewers’ recommendations would have a ratio for Approved:Reservations:Not approved of 70:24:6.</p>
</sec>
<sec id="s2-7">
<title>Reproducibility</title>
<p>Research question 1 was pre-registered using <italic>As Predicted</italic> on 20 May 2024 [<xref ref-type="bibr" rid="c44">44</xref>]. Research question 2 was formulated during data collection but before any data analysis and used the same study design and statistical methods as question 1.</p>
<p>All data extraction and analyses were conducted using <italic>R</italic> version 4.4.1 [<xref ref-type="bibr" rid="c45">45</xref>]. The data and <italic>R</italic> code are available on <italic>GitHub</italic> [<xref ref-type="bibr" rid="c46">46</xref>].</p>
</sec>
</sec>
<sec id="s3" sec-type="results">
<title>Results</title>
<p>A flow chart of the included reviews is shown in <xref ref-type="sec" rid="s5-1">Supplement S.1</xref>. The final sample size was over 37,000 reviews. There were more than 2,000 articles that were not included because they had not yet been peer reviewed, especially recent articles. More than 900 reviewers did not have a record in <italic>OpenAlex</italic> and so could not be included. These missing reviewers were more likely to be from older articles and more likely to be co-reviewers.</p>
<p>Descriptive statistics on the included reviews are in <xref ref-type="table" rid="tbl2">Table 2</xref>. The reviewers were cited at least once in 13% of the articles and 6% of the reviews included a self-citation. Most reviews recommended “Approved” (54%), with only 8% recommending “Not approved” which is low compared with many journals; however, 40 to 50% of submissions are rejected before articles are sent for peer review (personal communication, <italic>F1000</italic> staff). The reviewers were relatively experienced, with a median number of papers of 55.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2</label>
<caption><title>Descriptive statistics for the articles and peer reviews.</title>
<p>Q1 = first quartile, Q3 = third quartile.</p></caption>
<alternatives>
<graphic xlink:href="wdvr9v2_tbl2.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Variable</th>
<th align="left" valign="top">Level / Statistics</th>
<th align="left" valign="top">Result</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Number of reviews</td>
<td align="left" valign="top">n</td>
<td align="left" valign="top">37,332</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top">Year</td>
<td align="left" valign="top">Median [Q1, Q3]</td>
<td align="left" valign="top">2022 [2019, 2024]</td>
</tr>
<tr>
<td align="left" valign="top" rowspan="4">Journal, n (%)</td>
<td align="left" valign="top">F1000Research</td>
<td align="left" valign="top">24,132 (65)</td>
</tr>
<tr>
<td align="left" valign="top">Wellcome Open Research</td>
<td align="left" valign="top">8697 (23)</td>
</tr>
<tr>
<td align="left" valign="top">Open Research Europe</td>
<td align="left" valign="top">2789 (7)</td>
</tr>
<tr>
<td align="left" valign="top">Gates Open Research</td>
<td align="left" valign="top">1714 (5)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top" rowspan="2">Role, n (%)</td>
<td align="left" valign="top">Reviewer</td>
<td align="left" valign="top">34,904 (93)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top">Co-reviewer</td>
<td align="left" valign="top">2428 (7)</td>
</tr>
<tr>
<td align="left" valign="top" rowspan="3">Reviewer’s recommendation, n (%)</td>
<td align="left" valign="top">Approved</td>
<td align="left" valign="top">19,984 (54)</td>
</tr>
<tr>
<td align="left" valign="top">Reservations</td>
<td align="left" valign="top">14,379 (38)</td>
</tr>
<tr>
<td align="left" valign="top">Not approved</td>
<td align="left" valign="top">2969 (8)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top" rowspan="3">Article version, n (%)</td>
<td align="left" valign="top">1</td>
<td align="left" valign="top">26,474 (71)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top">2</td>
<td align="left" valign="top">8995 (24)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top">3+</td>
<td align="left" valign="top">1863 (5)</td>
</tr>
<tr>
<td align="left" valign="top">Number of papers cited in article</td>
<td align="left" valign="top">Median [Q1, Q3]</td>
<td align="left" valign="top">24 [14, 38]</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top" rowspan="2">Any citations to reviewer, n (%)</td>
<td align="left" valign="top">No</td>
<td align="left" valign="top">32,375 (87)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">4957 (13)</td>
</tr>
<tr>
<td align="left" valign="top" rowspan="2">Any papers cited by reviewer, n (%)</td>
<td align="left" valign="top">No</td>
<td align="left" valign="top">31,546 (84)</td>
</tr>
<tr>
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">5786 (16)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top" rowspan="2">Any self-citations from reviewer</td>
<td align="left" valign="top">No</td>
<td align="left" valign="top">35,023 (94)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">2309 (6)</td>
</tr>
<tr>
<td align="left" valign="top">Reviewer’s publication count</td>
<td align="left" valign="top">Median [Q1, Q3]</td>
<td align="left" valign="top">55 [24, 118]</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top" rowspan="5">Reviewer’s country (top five only)</td>
<td align="left" valign="top">USA</td>
<td align="left" valign="top">7655 (21%)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top">United Kingdom</td>
<td align="left" valign="top">4137 (11%)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top">India</td>
<td align="left" valign="top">2472 (7%)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top">Italy</td>
<td align="left" valign="top">1368 (4%)</td>
</tr>
<tr style="background-color:#DFDFDF">
<td align="left" valign="top">Australia</td>
<td align="left" valign="top">1349 (4%)</td>
</tr>
<tr>
<td align="left" valign="top">Number of words in the review</td>
<td align="left" valign="top">Median [Q1, Q3]</td>
<td align="left" valign="top">202 [67, 411]</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The binary predictor for citations of “any versus none” had a generally better fit to the data compared to the linear predictor (<xref ref-type="sec" rid="s5-2">Supplement S.2</xref>). This indicates that for most reviewers receiving any citation is important, and there is no linear increase for two or more citations. The following results are for the binary predictor “any versus none”, with the results using a linear predictor in <xref ref-type="sec" rid="s5-3">Supplement S.3</xref>.</p>
<p>Reviewers who were cited were more likely to approve the article, but only after version 1 (<xref ref-type="fig" rid="fig2">Fig 2</xref> and <xref ref-type="table" rid="tbl3">Table 3</xref>). If a reviewer was cited in any versions after version 1, the odds ratio for recommending Approved versus Reservations or Not approved was 1.61 (adjusted 99.4% CI 1.16 to 2.23).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><title>Odds ratios and probabilities for reviewers giving a more or less favourable recommendation depending on whether they were cited in the article.</title>
<p>The top row examines Approved vs Reservations or Not approved, and the bottom row examines Approved or Reservations vs Not approved. The figures show the mean (dot) and adjusted 99.4% confidence intervals (horizontal lines). All models were split by article version. The odds ratios and probabilities show the same results but on different scales.</p></caption>
<graphic xlink:href="wdvr9v2_fig2.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3</label>
<caption><title>Odds ratios for reviewers giving a more (OR &gt; 1) or less (OR &lt; 1) favourable recommendation depending on whether they were cited in the article (question 1) or included self-citations to their own research (question 2). All models were split by article version.</title></caption>
<alternatives>
<graphic xlink:href="wdvr9v2_tbl3.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="cols">
<thead>
<tr>
<th align="left" valign="top">Research question</th>
<th align="left" valign="top">Article version</th>
<th align="left" valign="top">Outcome</th>
<th align="left" valign="top">OR (Adjusted 99.4% CI)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="middle" rowspan="4">1. Reviewer cited</td>
<td align="center" valign="top">Version = 1</td>
<td align="left" valign="top">Approved vs Reservations/Not approved</td>
<td align="center" valign="top">0.84 (0.69, 1.03)</td>
</tr>
<tr>
<td align="center" valign="top">Version = 1</td>
<td align="left" valign="top">Approved/Reservations vs Not approved</td>
<td align="center" valign="top">0.84 (0.57, 1.23)</td>
</tr>
<tr>
<td align="center" valign="top">Versions = 2+</td>
<td align="left" valign="top">Approved vs Reservations/Not approved</td>
<td align="center" valign="top">1.61 (1.16, 2.23)</td>
</tr>
<tr>
<td align="center" valign="top">Versions = 2+</td>
<td align="left" valign="top">Approved/Reservations vs Not approved</td>
<td align="center" valign="top">1.12 (0.59, 2.13)</td>
</tr>
<tr>
<td align="left" valign="middle" rowspan="4">2. Self-cited</td>
<td align="center" valign="top">Version = 1</td>
<td align="left" valign="top">Approved vs Reservations/Not approved</td>
<td align="center" valign="top">0.57 (0.44, 0.73)</td>
</tr>
<tr>
<td align="center" valign="top">Version = 1</td>
<td align="left" valign="top">Approve/Reservations vs Not approved</td>
<td align="center" valign="top">1.11 (0.77, 1.60)</td>
</tr>
<tr>
<td align="center" valign="top">Versions = 2+</td>
<td align="left" valign="top">Approved vs Reservations/Not approved</td>
<td align="center" valign="top">0.15 (0.08, 0.30)</td>
</tr>
<tr>
<td align="center" valign="top">Versions = 2+</td>
<td align="left" valign="top">Approved/Reservations vs Not approved</td>
<td align="center" valign="top">0.80 (0.37, 1.74)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Reviewers who requested a self-citation were much less likely to approve the article for all versions (<xref ref-type="fig" rid="fig3">Fig 3</xref> and <xref ref-type="table" rid="tbl3">Table 3</xref>). The odds ratio for recommending Approved versus Reservations or Not approved was 0.57 (99.4% CI 0.44 to 0.73) for version 1 and strengthened to 0.15 (99.4% CI 0.08 to 0.30) for versions 2+. The less favourable recommendation was only for the approval of the article and the odds ratios for Approved or Reservations versus Not approved were much closer to 1.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
<caption><title>Odds ratios and probabilities for reviewers giving a more or less favourable recommendation if they included citations to their own research in their review.</title>
<p>The top row examines Approved vs Reservations or Not approved, and the bottom row examines Approved or Reservations vs Not approved. The figures show the mean (dot) and adjusted 99.4% confidence intervals (horizontal lines). All models were split by article version. The odds ratios and probabilities show the same results but on different scales.</p></caption>
<graphic xlink:href="wdvr9v2_fig3.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>In an unplanned analysis, we examined the behaviour of reviewers in the first two versions of the article. We examined the 441 reviews where the reviewer was not cited in version 1 of the article and requested a self-citation in their first review. The reviewers who were then cited in version 2 recommended approval for 92%, compared to 76% for reviewers who were not cited (odds ratio = 3.5, 95% CI: 2.0 to 6.1). This analysis did not use matching.</p>
<p>In an unplanned analysis, we examined whether the reviewers’ recommendations depended on citations to research other than their own by excluding self-citations. Reviewers who included citations in their review were much more likely not to approve the article (<xref ref-type="fig" rid="fig4">Figure 4</xref>), which was similar to the association with self-citations (<xref ref-type="fig" rid="fig3">Figure 3</xref>). However, reviewers who included citations other than self-citations were also much more likely to recommend “Not approved”, as shown by the lower odds of “Not approved” or “Reservations” versus “Not approved”. This association was not seen using self-citations (<xref ref-type="fig" rid="fig3">Figure 3</xref>)</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4</label>
<caption><title>Odds ratios and probabilities for reviewers giving a more or less favourable recommendation depending on if they included citations to other research in their review.</title>
<p>The top row examines Approved vs Reservations or Not approved, and the bottom row examines Approved or Reservations vs Not approved. The figures show the mean (dot) and adjusted 99.4% confidence intervals (horizontal lines). All models were split by article version. The odds ratios and probabilities show the same results but on different scales.</p></caption>
<graphic xlink:href="wdvr9v2_fig4.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<sec id="s3-1">
<title>Sensitivity analyses</title>
<p>The odds ratios when including co-reviewers with reviewers were similar to the odds ratios when using reviewers only (<xref ref-type="sec" rid="s5-4">Supplement S.4</xref>).</p>
<p>We found no evidence that the reviewers’ publication numbers or country confounded the associations between citations and recommendations (<xref ref-type="sec" rid="s5-5">Supplement S.5</xref>).</p>
</sec>
<sec id="s3-2">
<title>Text analyses of reviewers’ comments</title>
<p>A random sample of how reviewers requested self-citations found some vague justifications (<xref ref-type="sec" rid="s5-8">Supplement S.8</xref>); for example, “Here are some additional publications you might consider referencing”. Other sentences adhered to the publisher’s guidelines for reviewers, as specific reasoning was provided for self-citation [<xref ref-type="bibr" rid="c7">7</xref>]. One reviewer thanked the authors for a previous citation. Three reviews did not have a relevant sentence. One reviewer almost certainly used AI to write their review as it included the phrase “Certainly! Here are some potential review questions for the manuscript” [<xref ref-type="bibr" rid="c47">47</xref>]; this review included six self-citations with no justifications.</p>
<p>Reviewers who included a self-citation were more likely to use the words “need” and “please” when not approving the article (<xref ref-type="fig" rid="fig5">Figure 5</xref>). In contrast, the words “genome” and “might” were the most strongly associated with the reviewers’ approval.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><title>Words in the reviewers’ comments that were associated with approving the article or not for reviewers who included a self-citation (<italic>n</italic> = 2, 710).</title>
<p>The words were selected using an elastic net that started with the 100 most commonly used review words with 28 retained. The estimates from the elastic net are shown as empty circles and the mean estimates and 95% credible intervals from a Bayesian model as shown as a solid circle and horizontal line. The axis label shows the stemmed word and most common whole word in brackets.</p></caption>
<graphic xlink:href="wdvr9v2_fig5.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>To examine how often open peer reviews were viewed, we took a random sample of 200 reviews from the four journals and found that, on average, they were viewed just 1.2 times per year (<xref ref-type="sec" rid="s5-7">Supplement S.7</xref>).</p>
</sec>
</sec>
<sec id="s4" sec-type="discussion">
<title>Discussion</title>
<p>Our results show strong evidence that some reviewers have a transactional view of peer review, with their final approval dependent on citations to their work. These reviewers are exploiting the pressure on authors to “publish or perish”. Under this pressure, many authors may oblige and add the suggested citations, especially since adding another citation may only require a minor edit to their article [<xref ref-type="bibr" rid="c48">48</xref>]. Both sides gain from this transaction, as the authors get an indexed publication and the reviewer gets a citation.</p>
<p>Some reviewers who withhold their approval and request a self-citation may be justified, as they may be highlighting important errors or missing context in the article. Self-citations can be justified when the authors have made a “large scholarly oversight” [<xref ref-type="bibr" rid="c8">8</xref>]. However, our matched design shows that other reviewers who evaluated the same article and who did not self-cite were often willing to approve the article, suggesting that the concerns of the self-citing reviewer were specific to them and not serious enough to be noticed by other reviewers.</p>
<p>Reviewers including citations to other research (excluding self-citations) were also less likely to approve the article, supporting the idea that citations (self or otherwise) are highlighting important errors or missing context. However, reviewers citing other research were also much more likely to recommend “Not approved” whereas this association was not observed for self-citations. This indicates that missing citations that were not self-citations were considered more serious than missing self-citations.</p>
<p>Examining the context of the self-citation, we found vague or non-existent justifications (<xref ref-type="sec" rid="s5-8">Supplement S.8</xref>), showing that some reviewers ignored the journals’ guidelines to state their reasoning when including self-citations. Reviewers who included self-citations and withheld their approval were more likely to use the words “need” and “please”, indicating use of coercive language. We also found a large increase in reviewers’ approval after they suggested and then received a citation. Some reviewers are not adhering to good scientific practice and instead are treating peer review as a chance to boost their own career.</p>
<p>For both research questions, the effects were stronger for the second and later versions of the article than for the first version. Reviewers may understand that authors may be more willing to compromise on later versions when they are closer to obtaining an indexed publication. Most researchers understand that the peer review system is imperfect and that they must sometimes make compromises to be successful [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c49">49</xref>].</p>
<sec id="s4-1">
<title>Potential improvements to peer review</title>
<p>Journals could give stronger guidance to reviewers and authors on coercive citations [<xref ref-type="bibr" rid="c4">4</xref>]. However, given the limited time for peer review and the many differences in guidelines between journals [<xref ref-type="bibr" rid="c50">50</xref>], most authors may not read peer review instructions. Hence, guidance alone may have limited impact.</p>
<p>One suggestion is that reviewers declare to editors when they have recommended citations to their own work [<xref ref-type="bibr" rid="c51">51</xref>]. A useful innovation would be for all reviews that contain self-citations to be automatically flagged to the editors who could check if the self-citations are justified. We are aware of one journal where this is already happening (personal communication, Benno Torgler). <italic>F1000</italic> have recently introduced checks to prevent reviewers from publishing a review with three or more self-citations. If the reviewers continue to request more than three, then the review is examined, and if the self-citations are deemed inappropriate and the reviewer declines to remove them, then the review is declined.</p>
<p>Open peer review has been suggested as a way to reduce coercive citations [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c51">51</xref>]. However, our results from four journals that use open review show that it is not a perfect antidote, although the problem could be worse in journals using blinded peer review. The transparency of open peer review should prevent reviewers from leaving self-serving comments; however, we found some dubious justifications for self-citations and blatant use of AI (<xref ref-type="sec" rid="s5-6">Supplement S.6</xref>). These reviewers may have rationalised that although their words are public, they are rarely scrutinised (<xref ref-type="sec" rid="s5-7">Supplement S.7</xref>); hence, it was worth the risk. The assumed additional quality assurance from open peer review [<xref ref-type="bibr" rid="c52">52</xref>] may often be absent.</p>
<p>A more radical change to peer review is that the reviewers initially see a version of the article with all references blinded and no reference list; for example, “A strong association between increased cleaning and reduced hospital infection is well established [x]”. Reviewers are asked to give an initial recommendation and comments, and then are shown the version with the full references and asked if they need to update their recommendation or provide additional comments. However, this involves more administrative work and demands more from peer reviewers. This approach could be used for particularly consequential or controversial articles. Some journals already require authors to partially blind their articles to maintain anonymous peer review; for example, the instructions from <italic>Taylor &amp; Francis</italic> include blinding the authors’ names in the reference list [<xref ref-type="bibr" rid="c53">53</xref>].</p>
<p>An argument could be made for using AI to provide peer review that is unmoved by citation flattery. However, peer review is an inherently human task by peers, and instead we need to improve peer review rather than abdicating this often difficult and time-consuming task to machines [<xref ref-type="bibr" rid="c54">54</xref>].</p>
</sec>
<sec id="s4-2">
<title>Related research</title>
<p>Previous cross-sectional studies of self-citations in reviews found at least one self-citation in 3% at a journal that used blinded peer review [<xref ref-type="bibr" rid="c17">17</xref>], 12% at a journal that used blinded peer review [<xref ref-type="bibr" rid="c55">55</xref>], and 12% at a journal that used open peer review [<xref ref-type="bibr" rid="c56">56</xref>]. A related study found that 15% of reviews included a self-citation and that the self-citations were highest when the reviewer recommended “major revisions” [<xref ref-type="bibr" rid="c57">57</xref>]. These figures are comparable with the 6% found here and indicate that most reviews do not include self-citations.</p>
<p>Previous surveys estimated that 14% of authors had experienced a coercive citation request from an editor [<xref ref-type="bibr" rid="c58">58</xref>], and 7% and 23% had experienced coercive citation pressure from a reviewer [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c59">59</xref>]. The frequency with which researchers interact with peer review means that many will encounter coercive citations at some point in their career.</p>
<p>A study of conference submissions estimated that reviewers who were cited gave submissions much higher scores [<xref ref-type="bibr" rid="c18">18</xref>]. A study of journal peer review estimated that cited reviewers scored the article higher, but with potential confounding by the quality of the article [<xref ref-type="bibr" rid="c17">17</xref>].</p>
<p>A survey of authors concluded that accepting an editor’s request for citations improved the chances of being accepted [<xref ref-type="bibr" rid="c12">12</xref>]. Requests in later versions were more strongly associated with acquiescence, and we found a related pattern in our analysis, with reviewers who requested self-citations being much less likely to recommend approval for later versions (<xref ref-type="fig" rid="fig3">Figure 3</xref>). A study examining open peer review found that self-citation requests were more likely to be included than other suggested citations, indicating that many authors wanted to please the reviewer or felt pressure to do so [<xref ref-type="bibr" rid="c56">56</xref>].</p>
<p>A survey of journal editors found that only 5% objected to reviewers self-citing, and that this should be expected as reviewers are likely to have done related work [<xref ref-type="bibr" rid="c8">8</xref>].</p>
<p>A cross-sectional study found that self-citations were more likely to have no rationale compared to other citations, suggesting that they are more likely to be unwarranted [<xref ref-type="bibr" rid="c55">55</xref>].</p>
</sec>
<sec id="s4-3">
<title>Strengths and limitations</title>
<p>To the best of our knowledge, this is the first analysis to use a matched design when examining reviewer citations, and hence strongly control for any confounding by the characteristics of the authors or articles. We compared reviewers who examined an identical article; hence, the differences we found should be due to the reviewers.</p>
<p>Our models include measurement error, as some citations to the reviewers’ work will be missed by our data collection, and some captured citations will be inaccurate [<xref ref-type="bibr" rid="c60">60</xref>]. We performed random data checks that showed a good accuracy (<xref ref-type="sec" rid="s5-6">Supplement S.6</xref>); however, we also found valid citations that were not captured by our data extraction for conference proceedings and technical reports, which are less likely to have a DOI. This measurement error would most likely underestimate a true association, as it reduced the variance in citation counts and created a regression dilution [<xref ref-type="bibr" rid="c61">61</xref>]. Our estimates will be biased if the associations between citations and reviewers’ recommendations are different for publications that do not have a DOI. Reviewers should be equally happy with any citation to their work; however, some reviewers may prefer citations to indexed articles, as these are more likely to count toward their h-indices [<xref ref-type="bibr" rid="c62">62</xref>].</p>
<p>We examined whether citing a reviewer altered their recommendation, but did not examine the sentiment of the citation [<xref ref-type="bibr" rid="c63">63</xref>]. Some citations would likely have been critical of the reviewer’s work and we would expect these to reduce the chances of a favourable recommendation. An analysis that included the sentiment of the citation would be useful, although previous research found that most citations are neutral or positive [<xref ref-type="bibr" rid="c63">63</xref>].</p>
<p>Our results may not be generalisable to journals that use blinded peer review or journals that use the traditional peer review model rather than the publish–review–curate model studied here. A previous study found that asking reviewers to consent to an open review had no important effect on the quality of the review or the reviewers’ recommendation [<xref ref-type="bibr" rid="c64">64</xref>]. Another potential difference is that the journals in our sample often asked the authors to suggest peer reviewers; however, this is relatively common in other journals [<xref ref-type="bibr" rid="c8">8</xref>].</p>
<p>We found a bias in our sample, as co-reviewers and reviewers from older articles were more likely to be excluded due to not having an <italic>OpenAlex</italic> record (<xref ref-type="sec" rid="s5-5">Supplement S.5</xref>). We therefore lost more junior reviewers who were less likely to be cited. The percentage of reviews lost was 5% (2026 out of 39,113), which is hopefully small enough to avoid a large bias.</p>
</sec>
</sec>
</body>
<back>
<app-group>
<app id="s5">
<title>Supplementary material</title>
<sec id="s5-1">
<title>S.1 Included and excluded reviews</title>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure S.1</label>
<caption><title>Flow chart of included reviews. ‘N’ is the number of articles and ‘n’ is the number of reviews.</title></caption>
<graphic xlink:href="wdvr9v2_fig6.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>The flow chart shows the loss of articles and reviews during the data collection process. More than 3,500 articles did not have reviewers as they had yet to be peer reviewed or were Faculty Reviews that are commissioned and use a different peer review model.</p>
<p>More than 2,000 reviewers did not have an <italic>OpenAlex</italic> record and therefore were excluded from the analyses. We examined the potential bias in the lost reviews by comparing their characteristics with those of the retained reviews. We used a multiple regression model with reviewer lost (yes/no) as the binary dependent variable and predictors of article version, article date, referee or co-referee, and reviewer’s country. We expected many of these predictors to have little effect; therefore, we used an elastic net to reduce the number of predictors [<xref ref-type="bibr" rid="c42">42</xref>]. We used the ‘glmnet’ package in <italic>R</italic> [<xref ref-type="bibr" rid="c43">43</xref>]. For the binary dependent variable, 39,455 reviews were retained and 2082 (5%) were lost.</p>
<p>The elastic net retained two predictors. The date of the article had an odds ratio of 1.09 per year increase, which means that more recent articles were more likely to be retained, likely because the reviewer’s information was more current. Referees were more likely to be retained compared to co-referees with an odds ratio of 1.79, likely because co-referees were often relatively junior and some may not have any publications.</p>
</sec>
<sec id="s5-2">
<title>S.2 Model fit</title>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table S.1</label>
<caption><title>Comparing the two alternatives for the citation predictor variables using either a linear variable or a binary “any versus none” variable.</title>
<p>A vs R/N = Approved vs Reservations/Not approved, A/R vs N = Approve/Reservations vs Not approved.</p></caption>
<alternatives>
<graphic xlink:href="wdvr9v2_tbl4.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top" rowspan="2">Co-reviewers included</th>
<th align="left" valign="top" rowspan="2">Version</th>
<th align="left" valign="top" rowspan="2">Outcome</th>
<th align="center" valign="top" colspan="3">AIC</th>
</tr>
<tr>
<th align="left" valign="top">Linear</th>
<th align="left" valign="top">Binary</th>
<th align="left" valign="top">Difference</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">No</td>
<td align="left" valign="top">1</td>
<td align="center" valign="top">A vs R/N</td>
<td align="center" valign="top">5940.9</td>
<td align="center" valign="top">5937.4</td>
<td align="center" valign="top">3.6</td>
</tr>
<tr>
<td align="left" valign="top">No</td>
<td align="left" valign="top">1</td>
<td align="center" valign="top">A/R vs N</td>
<td align="center" valign="top">1930.3</td>
<td align="center" valign="top">1929.8</td>
<td align="center" valign="top">0.5</td>
</tr>
<tr>
<td align="left" valign="top">No</td>
<td align="left" valign="top">2+</td>
<td align="center" valign="top">A vs R/N</td>
<td align="center" valign="top">1952.4</td>
<td align="center" valign="top">1941.9</td>
<td align="center" valign="top">10.5</td>
</tr>
<tr>
<td align="left" valign="top">No</td>
<td align="left" valign="top">2+</td>
<td align="center" valign="top">A/R vs N</td>
<td align="center" valign="top">572.1</td>
<td align="center" valign="top">571.9</td>
<td align="center" valign="top">0.2</td>
</tr>
<tr>
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">1</td>
<td align="center" valign="top">A vs R/N</td>
<td align="center" valign="top">5978.4</td>
<td align="center" valign="top">5975.4</td>
<td align="center" valign="top">3.0</td>
</tr>
<tr>
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">1</td>
<td align="center" valign="top">A/R vs N</td>
<td align="center" valign="top">1941.1</td>
<td align="center" valign="top">1940.8</td>
<td align="center" valign="top">0.3</td>
</tr>
<tr>
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">2+</td>
<td align="center" valign="top">A vs R/N</td>
<td align="center" valign="top">1963.1</td>
<td align="center" valign="top">1951.4</td>
<td align="center" valign="top">11.7</td>
</tr>
<tr>
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">2+</td>
<td align="center" valign="top">A/R vs N</td>
<td align="center" valign="top">572.6</td>
<td align="center" valign="top">572.8</td>
<td align="center" valign="top">−0.2</td>
</tr>
<tr>
<td align="left" valign="top">No</td>
<td align="left" valign="top">1</td>
<td align="center" valign="top">A vs R/N</td>
<td align="center" valign="top">5932.3</td>
<td align="center" valign="top">5911.1</td>
<td align="center" valign="top">21.2</td>
</tr>
<tr>
<td align="left" valign="top">No</td>
<td align="left" valign="top">1</td>
<td align="center" valign="top">A/R vs N</td>
<td align="center" valign="top">1934.1</td>
<td align="center" valign="top">1935.6</td>
<td align="center" valign="top">−1.5</td>
</tr>
<tr>
<td align="left" valign="top">No</td>
<td align="left" valign="top">2+</td>
<td align="center" valign="top">A vs R/N</td>
<td align="center" valign="top">1881.4</td>
<td align="center" valign="top">1876.0</td>
<td align="center" valign="top">5.4</td>
</tr>
<tr>
<td align="left" valign="top">No</td>
<td align="left" valign="top">2+</td>
<td align="center" valign="top">A/R vs N</td>
<td align="center" valign="top">573.4</td>
<td align="center" valign="top">572.8</td>
<td align="center" valign="top">0.6</td>
</tr>
<tr>
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">1</td>
<td align="center" valign="top">A vs R/N</td>
<td align="center" valign="top">5967.9</td>
<td align="center" valign="top">5944.9</td>
<td align="center" valign="top">23.0</td>
</tr>
<tr>
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">1</td>
<td align="center" valign="top">A/R vs N</td>
<td align="center" valign="top">1945.4</td>
<td align="center" valign="top">1946.6</td>
<td align="center" valign="top">−1.2</td>
</tr>
<tr>
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">2+</td>
<td align="center" valign="top">A vs R/N</td>
<td align="center" valign="top">1917.3</td>
<td align="center" valign="top">1904.1</td>
<td align="center" valign="top">13.2</td>
</tr>
<tr>
<td align="left" valign="top">Yes</td>
<td align="left" valign="top">2+</td>
<td align="center" valign="top">A/R vs N</td>
<td align="center" valign="top">573.1</td>
<td align="center" valign="top">573.5</td>
<td align="center" valign="top">−0.5</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The AIC (Akaike Information Criterion) is a trade-off of model fit and complexity. The smaller the AIC, the better the fit. Differences of 10 are considered large [<xref ref-type="bibr" rid="c35">35</xref>].</p>
<p>In most cases, the difference between the linear and binary variables was small (under 5). There were four comparisons out of 16 in which the linear variable had a smaller AIC than the binary variable and all differences were small (under 2). There were four comparisons where the AIC for the binary variable was over 10 units smaller than the linear variable, indicating a large difference in model fit. In summary, using a binary predictor variable is a generally better fit to the data than using a linear variable.</p>
</sec>
<sec id="s5-3">
<title>S.3 Linear results</title>
<p>The figure shows the estimates for the two research questions using a linear dose-response for citation counts instead of the binary predictor of any citation versus none. The strongest effect was a greatly reduced odds of “Approvep” for increasing self-citations. However, these estimates should be viewed with caution, as the binary predictor generally better fits the data (<xref ref-type="sec" rid="s5-2">Supplement S.2</xref>).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure S.2</label>
<caption><title>Estimated odds ratios for using linear citations as the predictor.</title></caption>
<graphic xlink:href="wdvr9v2_fig7.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s5-4">
<title>S.4 Including co-reviewers</title>
<p>In a sensitivity analysis we included co-reviewers with reviewers. The results examining whether the reviewers gave a more favourable recommendation when cited (research question 1) were very similar.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure S.3</label>
<caption><title>Results with or without co-reviewers for research question 1.</title>
<p>Odds ratios and adjusted 99.4% confidence intervals for whether the reviewer gave a more or less favourable recommendation if they were cited. The results are shown for the combinations of predictor variables (linear or any vs none), outcome (Approved → Reservations → Not approved) and article version. The plot is designed to directly compare paired odds ratios with or without co-reviewers.</p></caption>
<graphic xlink:href="wdvr9v2_fig8.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>The results examining whether the reviewers gave a more favourable recommendation when they included self-citations (research question 2) were mostly very similar. Two noticeable differences were two odds ratios where including co-reviewers somewhat reduced the strength of the association. This was for article versions 2+ and examining Approved vs Reservations or Not approved. Despite the noticeable change in the odds ratio, the interpretation remains similar in that there was a strong reduction in the odds of a favourable recommendation when the reviewers included self-citations.</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure S.4</label>
<caption><title>Results with or without co-reviewers for research question 2.</title>
<p>Odds ratios and adjusted 99.4% confidence intervals for whether the reviewer gave a more or less favourable recommendation when they included a self-citation. The results are shown for the combinations of predictor variables (linear or any vs none), outcome (Approved → Reservations → Not approved) and article version. The plot is designed to directly compare paired odds ratios with or without co-reviewers.</p></caption>
<graphic xlink:href="wdvr9v2_fig9.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s5-5">
<title>S.5 Potential confounding by the reviewers’ characteristics</title>
<p>We examined two potential reviewer-level confounders: their country and their number of publications as a proxy of their seniority.</p>
<p>To examine confounding by the reviewers’ publication counts, we used fractional polynomials to test potentially non-linear associations [<xref ref-type="bibr" rid="c40">40</xref>]. For most models, the best fit (according to the AIC) was achieved using a log-transformation. There was little evidence of any confounding by the reviewers’ publication counts as the odds ratios were similar for both research questions (<xref ref-type="fig" rid="fig10">Figures S.5</xref> and <xref ref-type="fig" rid="fig11">S.6</xref>). A fractional polynomial of −2 tended to show the largest difference compared to the odds ratios with no confounders; however, this transformation was not the best fit and the differences were relatively small.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure S.5</label>
<caption><title>Examining potential confounding by reviewers’ publication counts for research question 1.</title>
<p>Odds ratios and adjusted 99.4% confidence intervals for whether the reviewer gave a more or less favourable recommendation when they were cited. We used fractional polynomials to examine a potentially non-linear association between reviewers’ publication counts and recommendation. The results for “None” are the results without the potential confounder. The results are shown for the combinations of predictor variables (linear or any vs none), outcome (Approved → Reservations → Not approved) and article version. Results are missing when the model did not converge.</p></caption>
<graphic xlink:href="wdvr9v2_fig10.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>For the leave-one-country-out sensitivity analyses, the results were generally similar regardless of which country was left-out. Leaving out the USA, which was the largest country, had a relatively large effect on the odds of recommending approved or reservation vs not approved for versions 2+ when using the “none vs any citations” predictor (<xref ref-type="fig" rid="fig12">Figure S.7</xref>) and on the odds of recommending approved or reservation vs not approved for versions 2+ when using the “none vs any citations” predictor (<xref ref-type="fig" rid="fig13">Figure S.8</xref>). However, neither change was substantively different from the results including all countries.</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure S.6</label>
<caption><title>Examining potential confounding by reviewers’ publication counts for research question 2.</title>
<p>Odds ratios and adjusted 99.4% confidence intervals for whether the reviewer gave a more or less favourable recommendation when they included a self-citation. We used fractional polynomials to examine a potentially non-linear association between reviewers’ publication counts and recommendation. The results for “None” are the results without the potential confounder. The results are shown for the combinations of predictor variables (linear or any vs none), outcome (Approved → Reservations → Not approved) and article version. Results are missing when the model did not converge.</p></caption>
<graphic xlink:href="wdvr9v2_fig11.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig12" position="float" fig-type="figure">
<label>Figure S.7</label>
<caption><title>Leave-one-country-out sensitivity analyses for research question 1.</title>
<p>Odds ratios and adjusted 99.4% confidence intervals for whether the reviewer gave a more or less favourable recommendation when they were cited. The results are shown for the combinations of predictor variables (linear or any vs none), outcome (Approved → Reservations → Not approved) and article version.</p></caption>
<graphic xlink:href="wdvr9v2_fig12.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig13" position="float" fig-type="figure">
<label>Figure S.8</label>
<caption><title>Leave-one-country-out sensitivity analyses for research question 2.</title>
<p>Odds ratios and adjusted 99.4% confidence intervals for whether the reviewer gave a more or less favourable recommendation when they included a self-citation. The results are shown for the combinations of predictor variables (linear or any vs none), outcome (Approved → Reservations → Not approved) and article version.</p></caption>
<graphic xlink:href="wdvr9v2_fig13.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s5-6">
<title>S.6 Data validation</title>
<p>We randomly selected reviews from our analysis data and manually verified the accuracy of our automated data extraction. We checked the accuracy of:</p>
<list list-type="bullet">
<list-item><p>Reviewers that were cited</p></list-item>
<list-item><p>Reviewers that were not cited</p></list-item>
<list-item><p>Reviewers that included self-citations in their review</p></list-item>
</list>
<p>We used a Bayesian calculation to estimate the error rates of our data extraction. We started with a vaguely informative Beta(1, 3.32) prior, which had a 90% probability that the error rate was under 0.5. This vague prior was used to exclude high error rates which were unlikely given our testing of the code during the construction of the data extraction. We created posterior estimates for the error rates using the observed counts of errors from manual checks. We calculated the 90% limits for the posterior distributions as an upper estimate of the error rates.</p>
<p>The distributions are plotted in <xref ref-type="fig" rid="fig14">Figure S.9</xref> and the error rates are shown in <xref ref-type="table" rid="tbl5">Table S.2</xref>. The errors are proportions, with 0 for no errors and 1 for all errors. The highest error rate was for self-citations.</p>
<fig id="fig14" position="float" fig-type="figure">
<label>Figure S.9</label>
<caption><title>Distributions of the error rates. Vaguely informative prior and posteriors for errors for not cited reviewers, cited reviewers and self-citations.</title>
<p>The dashed vertical lines are at Pr(error ≤ x) = 90%.</p></caption>
<graphic xlink:href="wdvr9v2_fig14.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table S.2</label>
<caption><title>Number of errors found in our data extraction algorithm from manual checks and the estimated 90% limit for the error rate</title></caption>
<alternatives>
<graphic xlink:href="wdvr9v2_tbl5.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top">Check</th>
<th align="center" valign="top">Number checked</th>
<th align="center" valign="top">Errors found</th>
<th align="center" valign="top">Pr(Error rate ≤ <italic>x</italic>) = 90%</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Reviewer not cited</td>
<td align="center" valign="top">100</td>
<td align="center" valign="top">2</td>
<td align="center" valign="top">0.051</td>
</tr>
<tr>
<td align="left" valign="top">Reviewer cited</td>
<td align="center" valign="top">100</td>
<td align="center" valign="top">1</td>
<td align="center" valign="top">0.037</td>
</tr>
<tr>
<td align="left" valign="top">Reviewer’s self-citations</td>
<td align="center" valign="top">80</td>
<td align="center" valign="top">4</td>
<td align="center" valign="top">0.094</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The two errors for reviewers not being cited were for citations to a book and a conference paper that did not have a DOI. All four errors in capturing self-citations were where the number captured was fewer than the true number, for example, we extracted 1 self-citation when the true number was 3.</p>
</sec>
<sec id="s5-7">
<title>S.7 Views of reviews</title>
<p>We randomly sampled 200 reviews from our sample and collected the number of times the review had been viewed online. A histogram of view counts is shown in <xref ref-type="fig" rid="fig15">Figure S.10</xref>, which had a strong positive skew with most reviews having 10 or fewer views. We used a Poisson model to estimate the annual number of views per year, accounting for the reviews’ publication dates. The mean number of views per year was 1.24 with a 95% credible interval of 1.20 to 1.28.</p>
<fig id="fig15" position="float" fig-type="figure">
<label>Figure S.10</label>
<caption><title>Histogram of online view counts of published reviews. The bins are in tens starting at [0, 10).</title></caption>
<graphic xlink:href="wdvr9v2_fig15.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s5-8">
<title>S.8 Self-citations examples</title>
<table-wrap id="tbl6" orientation="portrait" position="float">
<label>Table S.3</label>
<caption><title>Example sentences that reviewers used when suggesting self-citations, using a random sample of 20 reviews.</title>
<p>The first column shows the number of self-citations suggested. We have removed any references to names using [xxxx]. The results are ordered by sentence length.</p></caption>
<alternatives>
<graphic xlink:href="wdvr9v2_tbl6.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top">Self-citations</th>
<th align="left" valign="top">Reviewer’s text</th>
</tr>
</thead>
<tbody>
<tr style="background-color:#ECECEC">
<td align="left" valign="top">3</td>
<td align="left" valign="top">Also, the introduction, main discussion, and conclusion must be redrawn to highlight NO as a treatment option, the clinical trials discussion, the use of several NORMS (NORM-1, NORM-2, etc), the effect of NO-carriage system, Natural NO-sources, synthetic NO-sources with limitations, Inorganic versus organic forms, etc (eg., in the review publications as given</td>
</tr>
<tr>
<td align="left" valign="top">1</td>
<td align="left" valign="top">The term ‘true bugs’ applies to the monophyletic Heteroptera, which does include the species presented here (Acanthosoma haemorrhoidale), while aphids and mealybugs belong to the distinct lineage Sternorrhyncha, sometimes (formerly) regarded as a part of the paraphyletic Homoptera (see, for example, <xref ref-type="fig" rid="fig2">Figure 2</xref> in: [xxxx]).</td>
</tr>
<tr style="background-color:#ECECEC">
<td align="left" valign="top">1</td>
<td align="left" valign="top">On a side note: There is already published work on population genomics of the European plaice showing that two large chromosomal rearrangements (two putative inversions) segregate in northern plaice populations (North Sea, Baltic Sea, Barents Sea, and Iceland) and distinguish different plaice populations.</td>
</tr>
<tr>
<td align="left" valign="top">1</td>
<td align="left" valign="top">There is some observational clinical data on how the detrusor compensates for the growing prostate and the-by consequence-increase in bladder outflow obstruction, in addition to the animal studies referred to in the commentary, to explain the pathophysiology.</td>
</tr>
<tr style="background-color:#ECECEC">
<td align="left" valign="top">1</td>
<td align="left" valign="top">I would like to thank the authors for including references to the work done in the [xxxx] project; I would recommend to remove the reference ([xxxx] et al., 2019b) and replace it with a reference to a much more recent and related article ([xxxx] et al.):</td>
</tr>
<tr>
<td align="left" valign="top">2</td>
<td align="left" valign="top">The cited literature is incomplete; it does not include all reports of studies on the presence of the snail in Colombia, and studies with relevant findings of nematodes with or without pathogenic potential in animals are omitted e.g:</td>
</tr>
<tr style="background-color:#ECECEC">
<td align="left" valign="top">1</td>
<td align="left" valign="top">In the last 2 decades, our group has developed a brief instrument to assess the presence and the severity of sensory phenomena (the University of [xxxx]) to investigate OCD phenotypic subtypes and its relationship with TS/CTD.</td>
</tr>
<tr>
<td align="left" valign="top">5</td>
<td align="left" valign="top">The authors can find the following relevant articles to enhance their Materials and Methods section and incorporate citations to support their revised manuscript.</td>
</tr>
<tr style="background-color:#ECECEC">
<td align="left" valign="top">3</td>
<td align="left" valign="top">The authors should consider references from high impact journal publications on crop yield prediction. For example, the following articles by this reviewer</td>
</tr>
<tr>
<td align="left" valign="top">1</td>
<td align="left" valign="top">No mention to more rigorous rankings such as the Leiden Ranking are made nor to what exactly rankings are portraying. See for instance [xxxx].</td>
</tr>
<tr style="background-color:#ECECEC">
<td align="left" valign="top">1</td>
<td align="left" valign="top">Maybe ‘manifest’ and ‘not manifest’ would work better [for example we used this terminology in [xxxx].</td>
</tr>
<tr>
<td align="left" valign="top">1</td>
<td align="left" valign="top">This is especially useful if you have multiple data sets - see, for example, the [xxxx] package.</td>
</tr>
<tr style="background-color:#ECECEC">
<td align="left" valign="top">3</td>
<td align="left" valign="top">I would suggest the authors include some of the results of a large-scale project in Europe.</td>
</tr>
<tr>
<td align="left" valign="top">2</td>
<td align="left" valign="top">Please refer to some further references to revise the relevant description:</td>
</tr>
<tr style="background-color:#ECECEC">
<td align="left" valign="top">1</td>
<td align="left" valign="top">Here are a few additional publications you might consider referencing.</td>
</tr>
<tr>
<td align="left" valign="top">1</td>
<td align="left" valign="top">However, genomics resources are limited, except for parasitoid wasps.</td>
</tr>
<tr style="background-color:#ECECEC">
<td align="left" valign="top">1</td>
<td align="left" valign="top">Refer to this recent literature review.</td>
</tr>
<tr>
<td align="left" valign="top">4</td>
<td align="left" valign="top"><italic>No relevant sentence</italic></td>
</tr>
<tr style="background-color:#ECECEC">
<td align="left" valign="top">1</td>
<td align="left" valign="top"><italic>No relevant sentence</italic></td>
</tr>
<tr>
<td align="left" valign="top">6</td>
<td align="left" valign="top"><italic>No relevant sentence</italic></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
</app>
</app-group>
<ack>
<title>Acknowledgements</title>
<p>Thanks to all four journals for making all their data openly available and easily accessible. Thanks to Robin Blythe and staff from <italic>F1000</italic> for providing feedback on a draft of this paper.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname>, <given-names>F.-X.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>RETRACTION: Origin of the distinct site occupations of H atom in hcp Ti and Zr/Hf</article-title>”. <source>International Journal of Hydrogen Energy</source> <volume>91</volume> (<month>Nov</month>. <year>2024</year>), pp. <fpage>933</fpage>–<lpage>941</lpage>. DOI: <pub-id pub-id-type="doi">10.1016/j.ijhydene.2024.10.197</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seeber</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Self-citations as strategic response to the use of metrics for career decisions</article-title>”. <source>Research Policy</source> <volume>48</volume>.<issue>2</issue> (<year>2019</year>), pp. <fpage>478</fpage>–<lpage>491</lpage>. DOI: <pub-id pub-id-type="doi">10.1016/j.respol.2017.12.004</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cranford</surname>, <given-names>S.</given-names></string-name></person-group> “<article-title>C.R.E.A.M: Citations Rule Everything Around Me</article-title>”. <source>Matter</source> <volume>2</volume>.<issue>6</issue> (<month>June</month> <year>2020</year>), pp. <fpage>1343</fpage>–<lpage>1347</lpage>. DOI: <pub-id pub-id-type="doi">10.1016/j.matt.2020.04.025</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burton</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Cite me! Perspectives on coercive citation in reviewing</article-title>”. <source>Journal of Services Marketing</source> <volume>38</volume>.<issue>7</issue> (<month>Sept</month>. <year>2024</year>), pp. <fpage>809</fpage>–<lpage>815</lpage>. DOI: <pub-id pub-id-type="doi">10.1108/jsm-08-2024-0387</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teixeira da Silva</surname>, <given-names>J. A.</given-names></string-name></person-group> “<article-title>The ethics of peer and editorial requests for self-citation of their work and journal</article-title>”. <source>Medical Journal Armed Forces India</source> <volume>73</volume>.<issue>2</issue> (<year>2017</year>), pp. <fpage>181</fpage>–<lpage>183</lpage>. DOI: <pub-id pub-id-type="doi">10.1016/j.mjafi.2016.11.008</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="data"><person-group person-group-type="author"><collab>Committee on Publication Ethics</collab></person-group>. <source>Citation manipulation</source>. <month>Jan</month>. <year>2019</year>. DOI: <pub-id pub-id-type="doi">10.24318/cope.2019.3.1</pub-id>. <pub-id pub-id-type="doi">10.24318/cope.2019.3.1</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wren</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Valencia</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Kelso</surname>, <given-names>J.</given-names></string-name></person-group> “<article-title>Reviewer-coerced citation: case report, update on journal policy and suggestions for future prevention</article-title>”. <source>Bioinformatics</source> <volume>35</volume>.<issue>18</issue> (<month>Jan</month>. <year>2019</year>), pp. <fpage>3217</fpage>–<lpage>3218</lpage>. DOI: <pub-id pub-id-type="doi">10.1093/bioinformatics/btz071</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamilton</surname>, <given-names>D. G.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Meta-Research: Journal policies and editors’ opinions on peer review</article-title>”. <source>eLife</source> <volume>9</volume> (<month>Nov</month>. <year>2020</year>), <elocation-id>e62529</elocation-id>. DOI: <pub-id pub-id-type="doi">10.7554/eLife.62529</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mehregan</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Moghiman</surname>, <given-names>M.</given-names></string-name></person-group> “<article-title>The Unnoticed Issue of Coercive Citation Behavior for Authors</article-title>”. <source>Publishing Research Quarterly</source> <volume>40</volume>.<issue>2</issue> (<month>June</month> <year>2024</year>), pp. <fpage>164</fpage>–<lpage>168</lpage>. DOI: <pub-id pub-id-type="doi">10.1007/s12109-024-09994-0</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname>, <given-names>B. R.</given-names></string-name></person-group> “<article-title>Whither research integrity? Plagiarism, self-plagiarism and coercive citation in an age of research assessment</article-title>”. <source>Research Policy</source> <volume>42</volume>.<issue>5</issue> (<month>June</month> <year>2013</year>), pp. <fpage>1005</fpage>–<lpage>1014</lpage>. DOI: <pub-id pub-id-type="doi">10.1016/j.respol.2013.03.011</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heneberg</surname>, <given-names>P.</given-names></string-name></person-group> “<article-title>From Excessive Journal Self-Cites to Citation Stacking: Analysis of Journal Self-Citation Kinetics in Search for Journals, Which Boost Their Scientometric Indicators</article-title>”. <source>PLOS One</source> <volume>11</volume>.<issue>4</issue> (<month>Apr</month>. <year>2016</year>), <elocation-id>e0153730</elocation-id>. DOI: <pub-id pub-id-type="doi">10.1371/journal.pone.0153730</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fong</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Patnayakuni</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Wilhite</surname>, <given-names>A. W.</given-names></string-name></person-group> “<article-title>Accommodating coercion: Authors, editors, and citations</article-title>”. <source>Research Policy</source> <volume>52</volume>.<issue>5</issue> (<month>June</month> <year>2023</year>), p. <fpage>104754</fpage>. DOI: <pub-id pub-id-type="doi">10.1016/j.respol.2023.104754</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Singh Chawla</surname>, <given-names>D.</given-names></string-name></person-group> “<article-title>Two-thirds of researchers report ‘pressure to cite’ in Nature poll</article-title>”. <source>Nature</source> (<month>Oct</month>. <year>2019</year>). DOI: <pub-id pub-id-type="doi">10.1038/d41586-019-02922-9</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Resnik</surname>, <given-names>D. B.</given-names></string-name>, <string-name><surname>Gutierrez-Ford</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Peddada</surname>, <given-names>S.</given-names></string-name></person-group> “<article-title>Perceptions of Ethical Problems with Scientific Journal Peer Review: An Exploratory Study</article-title>”. <source>Science and Engineering Ethics</source> <volume>14</volume>.<issue>3</issue> (<month>Mar</month>. <year>2008</year>), pp. <fpage>305</fpage>–<lpage>310</lpage>. DOI: <pub-id pub-id-type="doi">10.1007/s11948-008-9059-4</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Singh Chawla</surname>, <given-names>D.</given-names></string-name></person-group> “<article-title>Elsevier investigates hundreds of peer reviewers for manipulating citations</article-title>”. <source>Nature</source> <volume>573</volume>.<issue>7773</issue> (<month>Sept</month>. <year>2019</year>), p. <fpage>174</fpage>. DOI: <pub-id pub-id-type="doi">10.1038/d41586-019-02639-9</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frey</surname>, <given-names>B. S.</given-names></string-name>, <string-name><surname>Eichenberger</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Frey</surname>, <given-names>R. L.</given-names></string-name></person-group> “<article-title>Editorial Ruminations: Publishing Kyklos</article-title>”. <source>Kyklos</source> <volume>62</volume>.<issue>2</issue> (<month>Apr</month>. <year>2009</year>), pp. <fpage>151</fpage>–<lpage>160</lpage>. DOI: <pub-id pub-id-type="doi">10.1111/j.1467-6435.2009.00428.x</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schriger</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>Kadera</surname>, <given-names>S. P.</given-names></string-name>, and <string-name><surname>von Elm</surname>, <given-names>E.</given-names></string-name></person-group> “<article-title>Are Reviewers’ Scores Influenced by Citations to Their Own Work? An Analysis of Submitted Manuscripts and Peer Reviewer Reports</article-title>”. <source>Annals of Emergency Medicine</source> <volume>67</volume>.<issue>3</issue> (<year>2016</year>), pp. <fpage>401</fpage>–<lpage>406</lpage>. DOI: <pub-id pub-id-type="doi">10.1016/j.annemergmed.2015.09.003</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stelmakh</surname>, <given-names>I.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Cite-seeing and reviewing: A study on citation bias in peer review</article-title>”. <source>PLOS One</source> <volume>18</volume>.<issue>7</issue> (<month>July</month> <year>2023</year>), pp. <fpage>1</fpage>–<lpage>16</lpage>. DOI: <pub-id pub-id-type="doi">10.1371/journal.pone.0283980</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frandsen</surname>, <given-names>T. F.</given-names></string-name> and <string-name><surname>Nicolaisen</surname>, <given-names>J.</given-names></string-name></person-group> “<article-title>Praise the bridge that carries you over: Testing the flattery citation hypothesis</article-title>”. <source>Journal of the American Society for Information Science and Technology</source> <volume>62</volume>.<issue>5</issue> (<month>Mar</month>. <year>2011</year>), pp. <fpage>807</fpage>–<lpage>818</lpage>. DOI: <pub-id pub-id-type="doi">10.1002/asi.21503</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>R.</given-names></string-name></person-group> “<article-title>Peer Review: A Flawed Process at the Heart of Science and Journals</article-title>”. <source>Journal of the Royal Society of Medicine</source> <volume>99</volume>.<issue>4</issue> (<year>2006</year>), pp. <fpage>178</fpage>–<lpage>182</lpage>. DOI: <pub-id pub-id-type="doi">10.1177/014107680609900414</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Bohorquez</surname>, <given-names>N. G.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Health and medical researchers are willing to trade their results for journal prestige: results from a discrete choice experiment</article-title>”. <source>Prometheus</source> (<year>2025</year>).</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>C. J.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Bias in peer review</article-title>”. <source>Journal of the American Society for Information Science and Technology</source> <volume>64</volume>.<issue>1</issue> (<year>2013</year>), pp. <fpage>2</fpage>–<lpage>17</lpage>. DOI: <pub-id pub-id-type="doi">10.1002/asi.22784</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmidt</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Ten considerations for open peer review</article-title>”. <source>FlOOOResearch</source> <volume>7</volume> (<month>June</month> <year>2018</year>), p. <fpage>969</fpage>. DOI: <pub-id pub-id-type="doi">10.12688/f1000research.15334.1</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tennant</surname>, <given-names>J. P.</given-names></string-name> and <string-name><surname>Ross-Hellauer</surname>, <given-names>T.</given-names></string-name></person-group> “<article-title>The limitations to our understanding of peer review</article-title>”. <source>Research Integrity and Peer Review</source> <volume>5</volume>.<issue>1</issue> (<month>Apr</month>. <year>2020</year>). DOI: <pub-id pub-id-type="doi">10.1186/s41073-020-00092-1</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aczel</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>The present and future of peer review: Ideas, interventions, and evidence</article-title>”. <source>Proceedings of the National Academy of Sciences</source> <volume>122</volume>.<issue>5</issue> (<month>Jan</month>. <year>2025</year>). DOI: <pub-id pub-id-type="doi">10.1073/pnas.2401232121</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vendé</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Barberousse</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Ruphy</surname>, <given-names>S.</given-names></string-name></person-group> “<article-title>From 2015 to 2023, eight years of empirical research on research integrity: a scoping review</article-title>”. <source>Research Integrity and Peer Review</source> <volume>10</volume>.<issue>1</issue> (<month>Apr</month>. <year>2025</year>). DOI: <pub-id pub-id-type="doi">10.1186/s41073-025-00163-1</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Currie</surname>, <given-names>G.</given-names></string-name></person-group> <source>Open Science: What is publish, review, curate?</source> <year>2024</year>. <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/inside-elife/dc24a9cd/open-science-what-is-publish-review-curate">https://elifesciences.org/inside-elife/dc24a9cd/open-science-what-is-publish-review-curate</ext-link>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>F1000Research</collab></person-group>. <source>Finding Article Reviewers</source>. <year>2025</year>. <ext-link ext-link-type="uri" xlink:href="https://f1000research.com/for-authors/tips-for-finding-referees">https://f1000research.com/for-authors/tips-for-finding-referees</ext-link>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>F1000Research</collab></person-group>. <source>Guidelines For Article Reviewers</source>. <year>2025</year>. <ext-link ext-link-type="uri" xlink:href="https://f1000research.com/for-referees/guidelines">https://f1000research.com/for-referees/guidelines</ext-link>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Begg</surname>, <given-names>C. B.</given-names></string-name> and <string-name><surname>Berlin</surname>, <given-names>J. A.</given-names></string-name></person-group> “<article-title>Publication Bias: A Problem in Interpreting Medical Data</article-title>”. <source>Journal of the Royal Statistical Society</source>. <series>Series A (Statistics in Society)</series> <volume>151</volume>.<issue>3</issue> (<year>1988</year>), p. <fpage>419</fpage>. DOI: <pub-id pub-id-type="doi">10.2307/2982993</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="data"><person-group person-group-type="author"><collab>Committee on Publication Ethics</collab></person-group>. <source>Ethical Guidelines for Peer Reviewers</source>. <month>Sept</month>. <year>2017</year>. DOI: <pub-id pub-id-type="doi">10.24318/cope.2019.1.9</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Culbert</surname>, <given-names>J. H.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Reference coverage analysis of OpenAlex compared to Web of Science and Scopus</article-title>”. <source>Scientometrics</source> <volume>130</volume>.<issue>4</issue> (<month>Apr</month>. <year>2025</year>), pp. <fpage>2475</fpage>–<lpage>2492</lpage>. DOI: <pub-id pub-id-type="doi">10.1007/s11192-025-05293-3</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Priem</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Piwowar</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Orr</surname>, <given-names>R.</given-names></string-name></person-group> <source>OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts</source>. <year>2022</year>. <series>arXiv: 2205.01833 [cs.DL]</series>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Massimo</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>openalexR: An R-Tool for Collecting Bibliometric Data from OpenAlex</article-title>”. <source>The R Journal</source> <volume>15</volume> (<month>4</month> <year>2024</year>), pp. <fpage>167</fpage>–<lpage>180</lpage>. DOI: <pub-id pub-id-type="doi">10.32614/RJ-2023-089</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Burnham</surname>, <given-names>K.</given-names></string-name> and <string-name><surname>Anderson</surname>, <given-names>D.</given-names></string-name></person-group> <source>Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach</source>. <publisher-loc>Springer New York</publisher-loc>, <year>2002</year>. <isbn>9780387953649</isbn>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bland</surname>, <given-names>J. M.</given-names></string-name> and <string-name><surname>Altman</surname>, <given-names>D. G.</given-names></string-name></person-group> “<article-title>Statistics notes: Matching</article-title>”. <source>BMJ</source> <volume>309</volume>.<issue>6962</issue> (<month>Oct</month>. <year>1994</year>), p. <fpage>1128</fpage>. DOI: <pub-id pub-id-type="doi">10.1136/bmj.309.6962.1128</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gomez</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Herman</surname>, <given-names>A. C.</given-names></string-name>, and <string-name><surname>Parigi</surname>, <given-names>P.</given-names></string-name></person-group> “<article-title>Leading countries in global science increasingly receive more citations than other countries doing similar research</article-title>”. <source>Nature Human Behaviour</source> <volume>6</volume>.<issue>7</issue> (<month>May</month> <year>2022</year>), pp. <fpage>919</fpage>–<lpage>929</lpage>. DOI: <pub-id pub-id-type="doi">10.1038/s41562-022-01351-5</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Campos-Arceiz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Primack</surname>, <given-names>R. B.</given-names></string-name>, and <string-name><surname>Koh</surname>, <given-names>L. P.</given-names></string-name></person-group> “<article-title>Reviewer recommendations and editors’ decisions for a conservation journal: Is it just a crapshoot? And do Chinese authors get a fair shot?</article-title>” <source>Biological Conservation</source> <volume>186</volume> (<year>2015</year>), pp. <fpage>22</fpage>–<lpage>27</lpage>. DOI: <pub-id pub-id-type="doi">10.1016/j.biocon.2015.02.025</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hosmer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lemeshow</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Sturdivant</surname>, <given-names>R.</given-names></string-name></person-group> <source>Applied Logistic Regression</source>. <publisher-name>Wiley Series in Probability and Statistics</publisher-name>. <publisher-loc>Wiley</publisher-loc>, <year>2013</year>. <isbn>9780470582473</isbn>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Royston</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ambler</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Sauerbrei</surname>, <given-names>W.</given-names></string-name></person-group> “<article-title>The use of fractional polynomials to model continuous risk variables in epidemiology</article-title>”. <source>International Journal of Epidemiology</source> <volume>28</volume>.<issue>5</issue> (<month>Oct</month>. <year>1999</year>), pp. <fpage>964</fpage>–<lpage>974</lpage>. DOI: <pub-id pub-id-type="doi">10.1093/ije/28.5.964</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Therneau</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Grambsch</surname>, <given-names>P.</given-names></string-name></person-group> <source>Modeling Survival Data: Extending the Cox Model</source>. <publisher-name>Statistics for Biology and Health</publisher-name>. <publisher-loc>Springer New York</publisher-loc>, <year>2013</year>. <isbn>9781475732948</isbn>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zou</surname>, <given-names>H.</given-names></string-name> and <string-name><surname>Hastie</surname>, <given-names>T.</given-names></string-name></person-group> “<article-title>Regularization and Variable Selection Via the Elastic Net</article-title>”. <source>Journal of the Royal Statistical Society Series B: Statistical Methodology</source> <volume>67</volume>.<issue>2</issue> (<month>Mar</month>. <year>2005</year>), pp. <fpage>301</fpage>–<lpage>320</lpage>. DOI: <pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00503.x</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tay</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Narasimhan</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Hastie</surname>, <given-names>T.</given-names></string-name></person-group> “<article-title>Elastic Net Regularization Paths for All Generalized Linear Models</article-title>”. <source>Journal of Statistical Software</source> <volume>106</volume>.<issue>1</issue> (<year>2023</year>), pp. <fpage>1</fpage>–<lpage>31</lpage>. DOI: <pub-id pub-id-type="doi">10.18637/jss.v106.i01</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Barnett</surname>, <given-names>A.</given-names></string-name></person-group> <source>F1000Research – reviewer citation study</source>. <month>May</month> <year>2024</year>. <ext-link ext-link-type="uri" xlink:href="https://aspredicted.org/rn8vg.pdf">https://aspredicted.org/rn8vg.pdf</ext-link>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>R Core Team</collab></person-group>. <source>R: A Language and Environment for Statistical Computing</source>. <publisher-name>R Foundation for Statistical Computing</publisher-name>. <publisher-loc>Vienna, Austria</publisher-loc>, <year>2024</year>. <ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Barnett</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Code and data for the analysis of the association between citations and peer review recommendations</article-title>. <source>Zenodo</source> <year>2025</year>. DOI: <pub-id pub-id-type="doi">10.5281/zenodo.16551814</pub-id>. <ext-link ext-link-type="uri" xlink:href="https://github.com/agbarnett/cited_reviewers">https://github.com/agbarnett/cited_reviewers</ext-link>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="data"><person-group person-group-type="author"><collab>S D. Peer Review Report</collab></person-group> <source>For: COVID-19 Vaccine: Predicting Vaccine Types and Assessing Mortality Risk Through Ensemble Learning Algorithms [version 2; peer review: 2 approved, 2 approved with reservations]</source>. <year>2024</year>. DOI: <pub-id pub-id-type="doi">10.5256/f1000research.153740.r257039</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oviedo-García</surname>, <given-names>M. Á.</given-names></string-name></person-group> “<article-title>The review mills, not just (self-)plagiarism in review reports, but a step further</article-title>”. <source>Scientometrics</source> <volume>129</volume>.<issue>9</issue> (<month>Aug</month>. <year>2024</year>), pp. <fpage>5805</fpage>–<lpage>5813</lpage>. DOI: <pub-id pub-id-type="doi">10.1007/s11192-024-05125-w</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>M. S.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>The Perverse Effects of Competition on Scientists’ Work and Relationships</article-title>”. <source>Science and Engineering Ethics</source> <volume>13</volume>.<issue>4</issue> (<month>Nov</month>. <year>2007</year>), pp. <fpage>437</fpage>–<lpage>461</lpage>. DOI: <pub-id pub-id-type="doi">10.1007/s11948-007-9042-5</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seeber</surname>, <given-names>M.</given-names></string-name></person-group> “<article-title>How do journals of different rank instruct peer reviewers? Reviewer guidelines in the field of management</article-title>”. <source>Scientometrics</source> <volume>122</volume>.<issue>3</issue> (<month>Jan</month>. <year>2020</year>), pp. <fpage>1387</fpage>–<lpage>1405</lpage>. DOI: <pub-id pub-id-type="doi">10.1007/s11192-019-03343-1</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thombs</surname>, <given-names>B. D.</given-names></string-name> and <string-name><surname>Razykov</surname>, <given-names>I.</given-names></string-name></person-group> “<article-title>A solution to inappropriate self-citation via peer review</article-title>”. <source>Canadian Medical Association Journal</source> <volume>184</volume>.<issue>16</issue> (<month>Oct</month>. <year>2012</year>), p. <fpage>1864</fpage>. DOI: <pub-id pub-id-type="doi">10.1503/cmaj.120597</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ross-Hellauer</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Deppe</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Schmidt</surname>, <given-names>B.</given-names></string-name></person-group> “<article-title>Survey on open peer review: Attitudes and experience amongst editors, authors and reviewers</article-title>”. <source>Plos’ One</source> <volume>12</volume>.<issue>12</issue> (<month>Dec</month>. <year>2017</year>), pp. <fpage>1</fpage>–<lpage>28</lpage>. DOI: <pub-id pub-id-type="doi">10.1371/journal.pone.0189311</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Taylor</surname></string-name> and <string-name><surname>Francis</surname></string-name></person-group>. <source>Anonymous peer review: How to make your article ready for double-anonymous peer review</source>. <year>2025</year>. <ext-link ext-link-type="uri" xlink:href="https://authorservices.taylorandfrancis.com/publishing-your-research/peer-review/anonymous-peer-review/">https://authorservices.taylorandfrancis.com/publishing-your-research/peer-review/anonymous-peer-review/</ext-link>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Bergstrom</surname>, <given-names>C. T.</given-names></string-name> and <string-name><surname>Bak-Coleman</surname>, <given-names>J.</given-names></string-name></person-group> “<article-title>AI, peer review and the human activity of science</article-title>”. <source>Nature</source> (<month>June</month> <year>2025</year>). DOI: <pub-id pub-id-type="doi">10.1038/d41586-025-01839-w</pub-id>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thombs</surname>, <given-names>B. D.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Potentially coercive self-citation by peer reviewers: A cross-sectional study</article-title>”. <source>Journal of Psychosomatic Research</source> <volume>78</volume>.<issue>1</issue> (<year>2015</year>), pp. <fpage>1</fpage>–<lpage>6</lpage>. DOI: <pub-id pub-id-type="doi">10.1016/j.jpsychores.2014.09.015</pub-id>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peebles</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Scandlyn</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Hesp</surname>, <given-names>B. R.</given-names></string-name></person-group> “<article-title>A retrospective study investigating requests for self-citation during open peer review in a general medicine journal</article-title>”. <source>Plos’ One</source> <volume>15</volume>.<issue>8</issue> (<month>Aug</month>. <year>2020</year>), pp. <fpage>1</fpage>–<lpage>9</lpage>. DOI: <pub-id pub-id-type="doi">10.1371/journal.pone.0237804</pub-id>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sugimoto</surname>, <given-names>C. R.</given-names></string-name> and <string-name><surname>Cronin</surname>, <given-names>B.</given-names></string-name></person-group> “<article-title>Citation gamesmanship: testing for evidence of ego bias in peer review</article-title>”. <source>Scientometrics</source> <volume>95</volume>.<issue>3</issue> (<month>Sept</month>. <year>2012</year>), pp. <fpage>851</fpage>–<lpage>862</lpage>. DOI: <pub-id pub-id-type="doi">10.1007/s11192-012-0845-z</pub-id>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fong</surname>, <given-names>E. A.</given-names></string-name> and <string-name><surname>Wilhite</surname>, <given-names>A. W.</given-names></string-name></person-group> “<article-title>Authorship and citation manipulation in academic research</article-title>”. <source>PLOS One</source> <volume>12</volume>.<issue>12</issue> (<month>Dec</month>. <year>2017</year>), pp. <fpage>1</fpage>–<lpage>34</lpage>. DOI: <pub-id pub-id-type="doi">10.1371/journal.pone.0187394</pub-id>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ho</surname>, <given-names>R. C.-M.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Views on the peer review system of biomedical journals: an online survey of academics from high-ranking universities</article-title>”. <source>BMC Medical Research Methodology</source> <volume>13</volume>.<issue>1</issue> (<month>June</month> <year>2013</year>). DOI: <pub-id pub-id-type="doi">10.1186/1471-2288-13-74</pub-id>.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pavlovic</surname>, <given-names>V.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>How accurate are citations of frequently cited papers in biomedical literature?</article-title>” <source>Clinical Science</source> <volume>135</volume>.<issue>5</issue> (<month>Mar</month>. <year>2021</year>), pp. <fpage>671</fpage>–<lpage>681</lpage>. DOI: <pub-id pub-id-type="doi">10.1042/cs20201573</pub-id>.</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clarke</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Underestimation of Risk Associations Due to Regression Dilution in Long-term Follow-up of Prospective Studies</article-title>”. <source>American Journal of Epidemiology</source> <volume>150</volume>.<issue>4</issue> (<month>Aug</month>. <year>1999</year>), pp. <fpage>341</fpage>–<lpage>353</lpage>. DOI: <pub-id pub-id-type="doi">10.1093/oxfordjournals.aje.a010013</pub-id>.</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fire</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Guestrin</surname>, <given-names>C.</given-names></string-name></person-group> “<article-title>Over-optimization of academic publishing metrics: observing Goodhart’s Law in action</article-title>”. <source>GigaScience</source> <volume>8</volume>.<issue>6</issue> (<month>May</month> <year>2019</year>). <issn>2047-217X</issn>. DOI: <pub-id pub-id-type="doi">10.1093/gigascience/giz053</pub-id>. <pub-id pub-id-type="doi">10.1093/gigascience/giz053</pub-id>.</mixed-citation></ref>
<ref id="c63"><label>[63]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tahamtan</surname>, <given-names>I.</given-names></string-name> and <string-name><surname>Bornmann</surname>, <given-names>L.</given-names></string-name></person-group> “<article-title>What do citation counts measure? An updated review of studies on citations in scientific documents published between 2006 and 2018</article-title>”. <source>Scientometrics</source> <volume>121</volume>.<issue>3</issue> (<month>Sept</month>. <year>2019</year>), pp. <fpage>1635</fpage>–<lpage>1684</lpage>. DOI: <pub-id pub-id-type="doi">10.1007/s11192-019-03243-4</pub-id>.</mixed-citation></ref>
<ref id="c64"><label>[64]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rooyen</surname>, <given-names>S. van</given-names></string-name> <etal>et al.</etal></person-group> “<article-title>Effect of open peer review on quality of reviews and on reviewers’ recommendations: a randomised trial</article-title>”. <source>BMJ</source> <volume>318</volume>.<issue>7175</issue> (<month>Jan</month>. <year>1999</year>), pp. <fpage>23</fpage>–<lpage>27</lpage>. DOI: <pub-id pub-id-type="doi">10.1136/bmj.318.7175.23</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108748.1.sa5</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Rodgers</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>eLife</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study explored a number of issues related to citations in the peer review process. An analysis of more than 37000 peer reviews at four journals found that: i) during the first round of review, reviewers were less likely to recommend acceptance if the article under review cited the reviewer's own articles; ii) during the second and subsequent rounds of review, reviewers were more likely to recommend acceptance if the article cited the reviewer's own articles; iii) during all rounds of review, reviewers who asked authors to cite the reviewer's own articles (a practice known as 'coercive citation') were less likely to recommend acceptance. However, when an author agreed to cite work by the reviewer, the reviewer was more likely to recommend acceptance of the revised article. The evidence is <bold>convincing</bold>, but article would benefit from a clearer presentation of the results and a more nuanced discussion of the motivations of reviewers.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108748.1.sa4</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The work used open peer reviews and followed them through a succession of reviews and author revisions. It assessed whether a reviewer had requested the author include additional citations and references to the reviewers' work. It then assessed whether the author had followed these suggestions and what the probability of acceptance was based on the authors decision.</p>
<p>Strengths and weaknesses:</p>
<p>The work's strengths are the in-depth and thorough statistical analysis it contains and the very large dataset it uses. The methods are robust and reported in detail. However, this is also a weakness of the work. Such thorough analysis makes it very hard to read! It's a very interesting paper with some excellent and thought provoking references but it needs to be careful not to overstate the results and improve the readability so it can be disseminated widely. It should also discuss more alternative explanations for the findings and, where possible, dismiss them.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108748.1.sa3</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This article examines reviewer coercion in the form of requesting citations to the reviewer's own work as a possible trade for acceptance and shows that, under certain conditions, this happens.</p>
<p>Strengths:</p>
<p>The methods are well done and the results support the conclusions that some reviewers &quot;request&quot; self-citations and may be making acceptance decisions based on whether an author fulfills that request.</p>
<p>Weaknesses:</p>
<p>The author needs to be more clear on the fact that, in some instances, requests for self-citations by reviewers is important and valuable.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108748.1.sa2</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this article, Barnett examines a pressing question regarding citing behavior of authors during the peer review process. In particular, the author studies the interaction between reviewers and authors, focusing on the odds of acceptance, and how this may be affected by whether or not the authors cited the reviewers' prior work, whether the reviewer requested such citations be added, and whether the authors complied/how that affected the reviewer decision-making.</p>
<p>Strengths:</p>
<p>The author uses a clever analytical design, examining four journals that use the same open peer review system, in which the identities of the authors and reviewers are both available and linkable to structured data. Categorical information about the approval is also available as structured data. This design allows a large scale investigation of this question.</p>
<p>Weaknesses:</p>
<p>My concerns pertain to the interpretability of the data as presented and the overly terse writing style.</p>
<p>Regarding interpretability, it is often unclear what subset of the data are being used both in the prose and figures. For example, the descriptive statistics show many more Version 1 articles than Version 2+. How are the data subset among the different possible methods?</p>
<p>Likewise, the methods indicate that a matching procedure was used comparing two reviewers for the same manuscript in order to control for potential confounds. However, the number of reviews is less than double the number of Version 1 articles, making it unclear which data were used in the final analysis. The methods also state that data were stratified by version. This raises a question about which articles/reviews were included in each of the analyses. I suggest spending more space describing how the data are subset and stratified. This should include any conditional subsetting as in the analysis on the 441 reviews where the reviewer was not cited in Version 1 but requested a citation for Version 2. Each of the figures and tables, as well as statistics provided in the text should provide this information, which would make this paper much more accessible to the reader.
[Note from editor: Please see &quot;Editorial feedback&quot; for more on this]</p>
<p>Finally, I would caution against imputing motivations to the reviewers, despite the important findings provided here. This is because the data as presented suggest a more nuanced interpretation is warranted. First, the author observes similar patterns of accept/reject decisions whether the suggested citation is a citation to the reviewer or not (Figs 3 and 4). Second, much of the observed reviewer behavior disappears or has much lower effect sizes depending on whether &quot;Accept with Reservations&quot; is considered an Accept or a Reject. This is acknowledged in the results text, but largely left out of the discussion. The conditional analysis on the 441 reviews mentioned above does support a more cautious version of the conclusion drawn here, especially when considered alongside the specific comments left by reviewers that were mentioned in the results and information in Table S.3. However, I recommend toning the language down to match the strength of the data.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108748.1.sa1</article-id>
<title-group>
<article-title>Reviewer #4 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work investigates whether a citation to a referee made by a paper is associated with a more positive evaluation by that referee for that paper. It provides evidence supporting this hypothesis. The work also investigates the role of self citations by referees where the referee would ask authors to cite the referee's paper.</p>
<p>Strengths:</p>
<p>This is an important problem: referees for scientific papers must provide their impartial opinions rooted in core scientific principles. Any undue influence due to the role of citations breaks this requirement. This work studies the possible presence and extent of this.</p>
<p>Barring a few issues discussed below, the methods are solid and well done. The work uses a matched pair design which controls for article-level confounding and further investigates robustness to other potential confounds.</p>
<p>It is surprising that even in these investigated journals where referee names are public, there is prevalence of such citation-related behaviors.</p>
<p>Weaknesses:</p>
<p>Some overall claims are questionable:</p>
<p>&quot;Reviewers who were cited were more likely to approve the article, but only after version 1&quot; It also appears that referees who were cited were less likely to approve the article in version 1. This null or slightly negative effect undermines the broad claim of citations swaying referees. The paper highlights only the positive results while not including the absence (and even reversal) of the effect in version 1 in its narrative.</p>
<p>&quot;To the best of our knowledge, this is the first analysis to use a matched design when examining reviewer citations&quot; Does not appear to be a valid claim based on the literature reference [18]</p>
<p>It will be useful to have a control group in the analysis associated to Figure 5 where the control group comprises matched reviews that did not ask for a self citation. This will help demarcate words associated with approval under self citation (as compared to when there is no self citation). The current narrative appears to suggest an association of the use of these words with self citations but without any control.</p>
<p>More discussion on the recommendations will help:
For the suggestion that &quot;the reviewers initially see a version of the article with all references blinded and no reference list&quot; the paper says &quot;this involves more administrative work and demands more from peer reviewers&quot;. I am afraid this can also degrade the quality of peer review, given that the research cannot be contextualized properly by referees. Referees may not revert back to all their thoughts and evaluations when references are released afterwards.</p>
</body>
</sub-article>
<sub-article id="sa5" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108748.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Barnett</surname>
<given-names>Adrian</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6339-0374</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>There was a common theme across the reviews to provide a more cautious interpretation and to consider the key question of whether peer reviewers who include citations are being purely self-serving or are highlighting important missing context. I will include a suggested new text analysis to cover this and will expand the discussion on this key question. Reviewers highlighted some confusion around the sample sizes for the different analyses, and I will clarify all sample sizes in the next version.</p>
</body>
</sub-article>
</article>