<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">101262</article-id>
<article-id pub-id-type="doi">10.7554/eLife.101262</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101262.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Prediction tendency, eye movements, and attention in a unified framework of neural speech tracking</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2536-6522</contrib-id>
<name>
<surname>Schubert</surname>
<given-names>Juliane</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<email>juliane.schubert@plus.ac.at</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9407-2763</contrib-id>
<name>
<surname>Gehmacher</surname>
<given-names>Quirin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9839-1614</contrib-id>
<name>
<surname>Schmidt</surname>
<given-names>Fabian</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8298-8125</contrib-id>
<name>
<surname>Hartmann</surname>
<given-names>Thomas</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7816-0037</contrib-id>
<name>
<surname>Weisz</surname>
<given-names>Nathan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gs8cd61</institution-id><institution>Paris-Lodron-University of Salzburg, Department of Psychology, Centre for Cognitive Neuroscience</institution></institution-wrap>, <city>Salzburg</city>, <country country="AT">Austria</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03z3mg085</institution-id><institution>Neuroscience Institute, Christian Doppler University Hospital, Paracelsus Medical University</institution></institution-wrap>, <city>Salzburg</city>, <country country="AT">Austria</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Department of Experimental Psychology, University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02704qw51</institution-id><institution>Wellcome Centre for Human Neuroimaging, University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bottini</surname>
<given-names>Roberto</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Trento</institution>
</institution-wrap>
<city>Trento</city>
<country>Italy</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>contributed equally</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-10-14">
<day>14</day>
<month>10</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-05-13">
<day>13</day>
<month>05</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP101262</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-08-13">
<day>13</day>
<month>08</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-08-27">
<day>27</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.06.27.546746"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-10-14">
<day>14</day>
<month>10</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101262.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.101262.1.sa4">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101262.1.sa3">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101262.1.sa2">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101262.1.sa1">Reviewer #3 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.101262.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Schubert et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Schubert et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-101262-v2.pdf"/>
<abstract>
<title>Abstract</title><p>Auditory speech comprehension is a multi-faceted process in which attention, prediction, and sensorimotor integration (via active sensing) interact with or complement each other. Although different conceptual models that focus on one of these aspects exist, we still lack a unified understanding of their role in speech processing. Here, we first replicated two recently published studies from our lab, confirming 1) a positive relationship between individual prediction tendencies and neural speech tracking, and 2) the phenomenon of ocular speech tracking – the tracking of attended speech by eye movements – and its shared contribution with neural activity to speech processing. In addition, we extended these findings with complementary analyses and investigated these phenomena in relation to each other in a multi-speaker paradigm with continuous, narrative speech. Importantly, prediction tendency and ocular speech tracking seem to be unrelated. In contrast to the shared contributions of oculomotor and neural activity to speech processing over a distributed set of brain regions that are critical for attention, individual prediction tendency and its relation to neural speech tracking seem to be largely independent of attention. Based on these findings, we propose a framework that aims to bridge the gaps between attention, prediction, and active (ocular) sensing in order to contribute to a holistic understanding of neural speech processing. In this speculative framework for listening, auditory inflow is, on a basic level, temporally modulated via active ocular sensing, and incoming information is interpreted based on probabilistic assumptions.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>active auditory sensing</kwd>
<kwd>speech processing</kwd>
<kwd>predictive processing</kwd>
<kwd>selective attention</kwd>
<kwd>eye movements</kwd>
<kwd>magnetoencephalography</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Following reviewer's feedback we wanted to provide a revised version of our manuscript. In the revised version we provide interim findings and missing data, updated figures that include an intuitive illustration of the metrics (such as TRFs), and a thoroughly revised discussion section where we focus on the relationship between our observed quantities and theoretical entities. We now offer operationalized definitions of the relevant concepts (prediction tendency, active ocular sensing and selective attention) and suggest how these entities might be related in the context of speech processing, based on the current findings.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Listening challenges the brain to infer meaning from vastly overlapping spectrotemporal information. For example, in complex, everyday environments, we need to select and segregate streams of speech from different speakers for further processing. To accomplish this task, (mainly independent) research lines suggested contributions of predictive and attentional processes to speech perception.</p>
<p>Predictive brain accounts (K. <xref ref-type="bibr" rid="c15">Friston, 2010</xref>; K. J. <xref ref-type="bibr" rid="c16">Friston et al., 2021</xref>; <xref ref-type="bibr" rid="c26">Knill &amp; Pouget, 2004</xref>; <xref ref-type="bibr" rid="c48">Yon et al., 2019</xref>) suggest an active engagement in speech perception. In this view, experience-based internal models constantly generate and continuously compare top-down predictions with bottom-up input, thus inferring sound sources from neural activity patterns. This idea is supported by the influential role of speech predictability (e.g. semantic context and word surprisal) on speech processing in naturalistic contexts (<xref ref-type="bibr" rid="c4">Broderick et al., 2019</xref>; <xref ref-type="bibr" rid="c12">Donhauser &amp; Baillet, 2020</xref>; <xref ref-type="bibr" rid="c46">Weissbart et al., 2020</xref>).</p>
<p>Selective attention describes the process by which the brain prioritises information to focus our limited cognitive capacities on relevant inputs while ignoring distracting, irrelevant ones. Its beneficial role in the processing of complex acoustic scenes, like the cocktail-party scenario (<xref ref-type="bibr" rid="c49">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="c50">Zion-Golumbic &amp; Schroeder, 2012</xref>), supports the key role of attentional and predictive processes in natural listening. It is important to note that even “normal hearing” individuals vary greatly in everyday speech comprehension and communication (<xref ref-type="bibr" rid="c35">Ruggles et al., 2011</xref>), which consequently promotes interindividual variability in predictive processes (<xref ref-type="bibr" rid="c40">Siegelman &amp; Frost, 2015</xref>) or selective attention (<xref ref-type="bibr" rid="c33">Oberfeld &amp; Klöckner-Nowotny, 2016</xref>) as underlying modulators.</p>
<p>In a recent study (<xref ref-type="bibr" rid="c39">Schubert et al., 2023</xref>), we quantified individual differences in predictive processes as the tendency to anticipate low-level acoustic features (i.e. prediction tendency). We established a positive relationship of this “trait” with speech processing, demonstrating that “neural speech tracking” is enhanced in individuals with stronger prediction tendency, independent of situative demands on selective attention. Furthermore, we found an increased tracking of words of high surprisal, demonstrating the importance of predictive processes in speech perception. Here (and in the following) “neural speech tracking” refers to a correlation coefficient between actual brain responses and responses predicted from an encoding model based solely on the speech envelope.</p>
<p>Another important aspect with regard to speech processing – that has been vastly overlooked so far in neuroscience – is active auditory sensing. The potential benefit of active sensing – the engagement of (covert) motor routines in acquiring sensory inputs – was outlined for other sensory modalities (<xref ref-type="bibr" rid="c38">Schroeder et al., 2010</xref>). In the auditory domain, and especially for speech perception, motor contributions to an increased weighting of temporal precision were suggested to modulate auditory processing gain in support of (speech) segmentation (<xref ref-type="bibr" rid="c31">Morillon et al., 2015</xref>). Along these lines, it has been shown that covert, mostly blink related eye activity aligns with higher-order syntactic structures of temporally predictable, artificial speech (i.e. monosyllabic words; <xref ref-type="bibr" rid="c23">Jin et al, 2018</xref>). In support of ideas that the motor system is actively engaged in speech perception (Limerman et al., 19985; <xref ref-type="bibr" rid="c17">Galantucci et al., 2006</xref>), the authors suggest a global entrainment across sensory and (oculo)motor areas which implements temporal attention.</p>
<p>In another recent study from our lab (<xref ref-type="bibr" rid="c18">Gehmacher et al., 2024</xref>), we showed that eye movements continuously track intensity fluctuations of attended natural speech, a phenomenon we termed ocular speech tracking. In the present study, we focused on gaze patterns rather than blink-related activity, both to replicate findings from <xref ref-type="bibr" rid="c18">Gehmacher et al. (2024)</xref> and because blink activity is unsuitable for TRF analysis due to its discrete and artifact-prone nature. Hence, “Ocular speech tracking” (similarly to “neural speech tracking” refers to the correlation coefficient between actual eye movements and movements predicted from an encoding model based on the speech envelope.</p>
<p>We further established links to increased intelligibility with stronger ocular tracking, and provided evidence that eye movements and neural activity share contributions to speech tracking. Taking into account the aforementioned study by <xref ref-type="bibr" rid="c39">Schubert and colleagues (2023)</xref>, the two recently uncovered predictors of neural tracking (individual prediction tendency and ocular tracking) raise several empirical questions regarding the relationship between predictive processes, selective attention, and active ocular sensing in speech processing:
<list list-type="order">
<list-item><p>Are predictive processes related to active ocular sensing in the same way they are to neural speech tracking? Specifically, do individuals with a stronger tendency to anticipate predictable auditory features, as quantified through pre-stimulus neural representations in an independent tone paradigm, show increased or even decreased ocular speech tracking, measured as the correlation between predicted and actual eye movements? Or is there no relationship at all?</p></list-item>
<list-item><p>To what extent does selective attention influence the relationship between prediction tendency, neural speech tracking, and ocular speech tracking? For example, does the effect of prediction tendency or ocular speech tracking on neural tracking differ between a single-speaker and multi-speaker listening condition?</p></list-item>
<list-item><p>Are individual prediction tendency and ocular speech tracking related to behavioral outcomes, such as comprehension and perceived task difficulty? Speech comprehension is assessed through accuracy on comprehension questions, and task difficulty is measured through subjective ratings.</p></list-item></list></p>
<p>Although predictive processes, selective attention, and active sensing have been shown to contribute to successful listening, their potential interactions and specific roles in naturalistic speech perception remain unclear. Addressing these questions will help disentangle their contributions and establish an integrated framework for understanding how neural and ocular speech tracking support speech processing.</p>
<p>Here, we set out to answer these questions by a) replicating aforementioned key findings in a single experiment, and b) operationalising prediction tendency, active ocular sensing, and attention in an integrated framework of neural speech tracking. The purpose of this conceptual framework will be to organise these entities according to their assumed function for speech processing and to describe their relationship with each other. We therefore repeated the study protocol of <xref ref-type="bibr" rid="c39">Schubert et al. (2023)</xref> with slight modifications: Again, participants performed a separate, passive listening paradigm (also see <xref ref-type="bibr" rid="c11">Demarchi et al., 2019</xref>) that allowed us to quantify individual prediction tendency. Afterward, they listened to sequences of audiobooks (using the same stimulus material), either in a clear speech (0 distractors) or a multi-speaker (1 distractor) condition. Simultaneously recorded magnetoencephalographic (MEG) and Eye Tracking data confirmed the previous findings of <xref ref-type="bibr" rid="c39">Schubert et al. (2023)</xref> and Gehmacher et al. (2024): 1) Individuals with a stronger prediction tendency showed an increased neural speech tracking over left frontal areas, 2) eye movements track acoustic speech in selective attention, and 3) further mediate neural speech tracking effects over widespread, but mostly auditory regions. Additionally, we found an increased neural tracking of semantic violations (compared to their lexically identical controls), indicating that surprisal evoked responses indeed encode information about the stimulus. Interestingly, we could not find this difference in semantic processing for ocular speech tracking. Finally, we behaviorally assessed speech comprehension by probing participants on story content. Responses indicate that weaker performance in comprehension was related to increased ocular speech tracking while we did not find a significant relation to neural speech tracking. The findings suggest a differential role of prediction tendency, eye movements, and attention in speech processing. Behavioural responses further indicate substantial differences in ocular and neural engagement and perceptual outcomes. Based on these findings, we propose a framework of neural speech tracking where anticipatory predictions support the interpretation of auditory input along the perceptual hierarchy while active ocular sensing increases the temporal precision of peripheral auditory responses to facilitate bottom-up processing of selectively attended input.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Subjects</title>
<p>In total, 39 subjects were recruited to participate in the experiment. For 3 participants, eye tracker calibration failed and they had to abort the experiment. We further controlled for excessive blinking (&gt; 50% of data samples) and eye movements away from fixation cross (&gt; 50% of data samples exceeded ⅓ of screen size on the horizontal or vertical plane) that suggest a lack of commitment to the experiment instructions. 7 subjects had to be excluded according to these criteria. Thus, a final sample size of 29 subjects (12 female, 17 male; mean age = 25.70, range = 19 – 43) was used for the analysis of brain and gaze data.</p>
<p>All participants reported normal hearing and had normal, or corrected to normal, vision. They gave written, informed consent and reported that they had no previous neurological or psychiatric disorders. The experimental procedure was approved by the ethics committee of the University of Salzburg and was carried out in accordance with the declaration of Helsinki. All participants received either a reimbursement of 10 € per hour or course credits.</p>
</sec>
<sec id="s2b">
<title>Experimental Procedure</title>
<p>The current study is a conceptual replication of a previous experiment (for details see <xref ref-type="bibr" rid="c39">Schubert et al., 2023</xref>), using the same experimental structure and the same auditory stimuli. The current design only differs in that one condition was dropped and the whole experiment was shortened (participants spent approximately 2 hours in the MEG including preparation time). In addition to the previous study, this time we recorded ocular movements (also see Data acquisition and Preprocessing).</p>
<p>Before the start of the experiment, participants’ head shapes were assessed using cardinal head points (nasion and pre-auricular points), digitised with a Polhemus Fastrak Digitiser (Polhemus), and around 300 points on the scalp. For every participant, MEG sessions started with a 5-minute resting-state recording, after which the individual hearing threshold was determined using a pure tone of 1043 Hz. This was followed by 2 blocks of passive listening to tone sequences of varying entropy levels to quantify individual prediction tendencies (see Quantification of individual prediction tendency and <xref rid="fig1" ref-type="fig">Figure 1A-C</xref>). Participants were instructed to look at a black fixation cross at the centre of a grey screen. In the main task, 4 different stories were presented in separate blocks in random order and with randomly balanced selection of the target speaker (male vs. female voice). Each block consisted of 2 trials with a continuous storyline, with each trial corresponding to one of 2 experimental conditions: a single speaker and a multi-speaker condition (see also <xref rid="fig1" ref-type="fig">Figure 1D</xref>). The distractor speaker was always of the opposite sex of the target speaker (and was identical to the target speaker in a different run). Distracting speech was presented exactly 20 s after target speaker onset, and all stimuli were presented binaurally at equal volume (40db above individual hearing threshold) for the left and right ear (i.e. at phantom centre). Participants were instructed to attend to the first speaker and their understanding was tested using comprehension questions (true vs. false statements) at the end of each trial (e.g.: “Das Haus, in dem Sofie lebt, ist rot” <italic>(The house Sofie lives in is red)</italic>, “Ein gutes Beispiel für unterschiedliche Dialekte sind die Inuit aus Alaska und Grönland” <italic>(A good example of different dialects are the Inuit from Alaska and Greenland)</italic>…). Furthermore, participants indicated their task engagement and their perceived task-difficulty on a 5-point Likert scale at the end of every trial. During the audiobook presentation, participants were again instructed to look at the fixation-cross on the screen and to blink as little as (still comfortably) possible. The experiment was coded and conducted with the Psychtoolbox-3 (<xref ref-type="bibr" rid="c1">Brainard, 1997</xref>; <xref ref-type="bibr" rid="c25">Kleiner et al., 2007</xref>), with an additional class-based library (‘Objective Psychophysics Toolbox’, o_ptb) on top of it (<xref ref-type="bibr" rid="c20">Hartmann &amp; Weisz, 2020</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Quantification of individual prediction tendency and the multi-speaker paradigm.</title><p><bold>A)</bold> Participants passively listened to sequences of pure tones in different conditions of entropy (ordered vs. random). Four tones of different fundamental frequencies were presented with a fixed stimulation rate of 3 Hz, their transitional probabilities varied according to respective conditions. <bold>B)</bold> Expected classifier decision values contrasting the brains’ prestimulus tendency to predict a forward transition (ordered vs. random). The purple shaded area represents values that were considered as prediction tendency <bold>C)</bold> Exemplary excerpt of a tone sequence in the ordered condition. An LDA classifier was trained on forward transition trials of the ordered condition (75% probability) and tested on all repetition trials to decode sound frequency from brain activity across time. <bold>D)</bold> Participants either attended to a story in clear speech, i.e. 0 distractor condition, or to a target speaker with a simultaneously presented distractor (blue), i.e. 1 distractor condition. <bold>E)</bold> The speech envelope was used to estimate neural and ocular speech tracking in respective conditions with temporal response functions (TRF). <bold>F)</bold> The last noun of some sentences was replaced randomly with an improbable candidate to measure the effect of envelope encoding on the processing of semantic violations. Adapted from <xref ref-type="bibr" rid="c39">Schubert et al., 2023</xref>.</p></caption>
<graphic xlink:href="546746v4_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2c">
<title>Stimuli</title>
<p>We used the same stimulus material as in <xref ref-type="bibr" rid="c39">Schubert et al. (2023)</xref>, recorded with a t.bone SC 400 studio microphone at a sampling rate of 44100 Hz. In total, we used material from 4 different, consistent stories (see <xref rid="tbl1" ref-type="table">Table 1</xref>). These stories were split into 3 separate trials of approximately 3 – 4 min. The first two parts of each story were always narrated by a target speaker, whereas the last part served as distractor material. Additionally, we randomly selected half of the nouns that ended a sentence (<italic>N</italic> = 79) and replaced them with the other half to induce unexpected semantic violations. The swap of nouns happened in the written script before the audio material was recorded in order to avoid any effects of audio clipping. Narrators were aware of the semantic violations and had been instructed to read out the words as normal. Consequently, all target words occurred twice in the text, once in a natural context (serving as “lexical controls”) and once in a mismatched context (serving as “semantic violations”) within each trial, resulting in two sets of lexically identical words that differed greatly in their contextual probabilities (see <xref rid="fig1" ref-type="fig">Figure 1F</xref> for an example). Participants were unaware of these semantic violations. All trials were recorded twice, narrated by a different speaker (male vs. female). Stimuli were presented in 4 blocks containing 2 trials each (a single and a multi-speaker trial), resulting in 2 male and 2 female target speaker blocks for every participant.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>List of all ebooks and short stories that were used as a basis for audio material.</title></caption>
<graphic xlink:href="546746v4_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s2d">
<title>Data Acquisition and Preprocessing</title>
<p>Brain activity was recorded using a whole head MEG system (Elekta Neuromag Triux, Elekta Oy, Finland), placed within a standard passive magnetically shielded room (AK3b, Vacuumschmelze, Germany). We used a sampling frequency of 1 kHz (hardware filters: 0.1 – 330 Hz). The signal was recorded with 102 magnetometers and 204 orthogonally placed planar gradiometers at 102 different positions. In a first step, a signal space separation algorithm, implemented in the Maxfilter program (version 2.2.15) provided by the MEG manufacturer, was used to clean the data from external noise and realign data from different blocks to a common standard head position. Data preprocessing was performed using Matlab R2020b (The MathWorks, Natick, Massachusetts, USA) and the FieldTrip Toolbox (<xref ref-type="bibr" rid="c34">Oostenveld et al., 2011</xref>). All data was filtered between 0.1 Hz and 30 Hz (Kaiser windowed finite impulse response filter) and downsampled to 100 Hz. To identify eye-blinks and heart rate artefacts, 50 independent components were identified from filtered (0.1 – 100 Hz), downsampled (1000 Hz) continuous data of the recordings from the entropy modulation paradigm, and on average 3 components were removed for every subject. All data was filtered between 0.1 Hz and 30 Hz (Kaiser windowed finite impulse response filter) and downsampled to 100 Hz. Data of the entropy modulation paradigm was epoched into segments of 1200 ms (from 400 ms before sound onset to 800 ms after onset). Multivariate pattern analysis (see quantification of individual prediction tendency) was carried out using the MVPA-Light package (<xref ref-type="bibr" rid="c44">Treder, 2020</xref>). Data of the listening task was temporally aligned with the corresponding speech envelope, which was extracted from the audio files using the Chimera toolbox (<xref ref-type="bibr" rid="c41">Smith et al., 2002</xref>) over a broadband frequency range of 100 Hz – 10 kHz (in 9 steps, equidistant on the tonotopic map of auditory cortex; see also <xref rid="fig1" ref-type="fig">Figure 1E</xref>).</p>
<p>Eye tracking data from both eyes were acquired using a Trackpixx3 binocular tracking system (Vpixx Technologies, Canada) at a sampling rate of 2 kHz with a 50 mm lens. Participants were seated in the MEG at a distance of 82 cm from the screen. Their chin rested on a chinrest to reduce head movements. Each experimental block started with a 13-point calibration / validation procedure that was used throughout the block. Blinks were automatically detected by the Trackpixx3 system and excluded from horizontal and vertical eye movement data. We additionally excluded 100 ms of data around blinks to control for additional blink artefacts that were not automatically detected by the eye tracker. We then averaged gaze position data from the left and right eye to increase the accuracy of gaze estimation (<xref ref-type="bibr" rid="c9">Cui &amp; Hondzinski, 2006</xref>). Missing data due to blink removal were then interpolated with a piecewise cubic Hermite interpolation. Afterwards, data was imported into the FieldTrip Toolbox, bandpass filtered between 0.1 – 40 Hz (zero-phase finite impulse response (FIR) filter, order: 33000, hamming window), and cut to the length of respective speech segments of a block. Data were then downsampled to 1000 Hz to match the sampling frequency of neural and speech envelope data and further corrected for a 16 ms delay between trigger onset and actual stimulation. Finally, gaze data was temporally aligned with the corresponding speech envelope and downsampled to 100 Hz for TRF analysis after an antialiasing low-pass filter at 20 Hz was applied (zero-phase FIR filter, order: 662, hamming window).</p>
</sec>
<sec id="s2e">
<title>Quantification of Prediction Tendency</title>
<p>The quantification of individual prediction tendency was the same as in <xref ref-type="bibr" rid="c39">Schubert et al. (2023)</xref>: We used an entropy modulation paradigm where participants passively listened to sequences of 4 different pure tones (f1: 440 Hz, f2: 587 Hz, f3: 782 Hz, f4: 1043 Hz, each lasting 100 ms) during two separate blocks, each consisting of 1500 tones presented with a temporally predictable rate of 3 Hz. Entropy levels (ordered / random) changed pseudorandomly every 500 trials within each block, always resulting in a total of 1500 trials per entropy condition. While in an “ordered” context certain transitions (hereinafter referred to as forward transitions, i.e. f1→f2, f2→f3, f3→f4, f4→f1) were to be expected with a high probability of 75%, self repetitions (e.g., f1→f1, f2→f2,…) were rather unlikely with a probability of 25%. However, in a “random” context all possible transitions (including forward transitions and self repetitions) were equally likely with a probability of 25% (see <xref rid="fig1" ref-type="fig">Figure 1A</xref>). This design gives us the opportunity to directly compare the processing of “predictable” and “unpredictable” sounds of the same frequency in a time-resolved manner.</p>
<p>To estimate the extent to which individuals anticipate auditory features (i.e. sound frequencies) according to their contextual probabilities, we used a multiclass linear discriminant analyser (LDA) to decode sound frequency (f1 – f4) from brain activity (using data from the 102 magnetometers) between –0.3 and 0 s. We specifically looked at a prestimulus window in order to capture top-down expectations driven by contextual regularity and temporal predictability in the design. The classification approach enables us to infer on “anticipatory predictions” (i.e. the representation of sound frequencies of high probabilities before their observation). Based on the resulting classifier decision values (i.e. d1 – d4 for every test-trial and time-point), we calculated “individual prediction tendency”. We define “individual prediction tendency” as the tendency to pre-activate sound frequencies of high probability (i.e. a forward transition from one stimulus to another: f1→f2, f2→f3, f3→f4, f4→f1). In order to capture any prediction-related neural activity, we trained the classifier exclusively on ordered forward trials (see <xref rid="fig1" ref-type="fig">Figure 1B</xref>). Afterwards, the classifier was tested on all self-repetition trials, providing classifier decision values for every possible sound frequency, which were then transformed into corresponding transitions (e.g. d1(t) | f1(t-1) “dval for 1 at trial t, given that 1 was presented at trial t-1” → repetition, d2(t) | f1(t-1) → forward,…). The tendency to represent a forward vs. repetition transition was contrasted for both ordered and random trials (see <xref rid="fig1" ref-type="fig">Figure 1C</xref>). Using self-repetition trials for testing, we ensured a fair comparison between the ordered and random contexts (with an equal number of trials and the same preceding bottom-up input). Thus, we quantified “prediction tendency” as the classifier’s pre-stimulus tendency to a forward transition in an ordered context exceeding the same tendency in a random context (which can be attributed to carry-over processing of the preceding stimulus). Then, using the summed difference across pre-stimulus times, one value can be extracted per subject (also see <xref rid="fig1" ref-type="fig">Figure 1B</xref>).</p>
</sec>
<sec id="s2f">
<title>Encoding Models</title>
<p>To quantify the neural representations corresponding to the acoustic envelope, we calculated a multivariate temporal response function (TRF) using the Eelbrain toolkit (<xref ref-type="bibr" rid="c2">Brodbeck et al., 2021</xref>). A deconvolution algorithm (boosting; <xref ref-type="bibr" rid="c10">David et al., 2007</xref>) was applied to the concatenated trials to estimate the optimal TRF to predict the brain response from the speech envelope, separately for each condition (single vs. multi-speaker). Before model fitting, MEG data of 102 magnetometers were normalised by subtracting the mean and dividing by the standard deviation (i.e. z-scoring) across all channels (as recommended by <xref ref-type="bibr" rid="c8">Crosse et al., 2016</xref>) Similarly, the speech envelope was also z-scored, however, after the transformation the negative of the minimum value (which naturally would be zero) was added to the time-series to retain zero values (z’ = z+(min(z)*-1). The defined time-lags to train the model were from – 0.4 s to 0.8 s. To evaluate the model, the data was split into 4 folds, and a cross-validation approach was used to avoid overfitting (<xref ref-type="bibr" rid="c47">Ying, 2019</xref>). The resulting predicted channel responses (for all 102 magnetometers) were then correlated with the true channel responses to quantify the model fit and the degree of speech envelope tracking at a particular sensor location.</p>
<p>To investigate the effect of semantic violations, we used the same TRFs trained on the whole dataset (with 4-fold cross validation) since single word epochs would have been too short to derive meaningful TRFs. Instead, the true as well as the predicted data was segmented into single word epochs of 2 seconds starting at word onset (using a forced-aligner; <xref ref-type="bibr" rid="c24">Kisler et al., 2017</xref>; <xref ref-type="bibr" rid="c37">Schiel &amp; Ohala, 1999</xref>). We selected semantic violations as well as their lexically identical controls and correlated true with predicted responses for every word (thus, we conducted the same analysis as for the overall encoding effect, focusing on only part of the data). We then averaged the result within each condition (i.e. single vs. multi-speaker) and word type (i.e. high vs. low suprisal).</p>
<p>The same TRF approach was also used to estimate ocular speech tracking, separately predicting eye movements in the horizontal and vertical direction using the same time-lags (from –0.4 s to 0.8 s). The same z-scoring was applied to the speech envelope. However, horizontal and vertical eye channel responses were normalised within channels.</p>
</sec>
<sec id="s2g">
<title>Mediation Analysis</title>
<p>To investigate the contribution of eye movements to neural speech tracking, we approached a mediation analysis similar to <xref ref-type="bibr" rid="c18">Gehmacher et al. (2024)</xref>. The TRFs that we obtained from these encoding models can be interpreted as time-resolved weights for a predictor variable that aims to explain a dependent variable (very similar to beta-coefficients in classic regression analyses). We simply compared the plain effect of the speech envelope on neural activity to its direct (residual) effect by including an indirect effect via eye movements into our model. In order to account for a reduction in speech envelope weights simply due to the inclusion of this additional (eye-movement) predictor, we obtained a control model by including a time-shuffled version of the eye-movement predictor in addition to the unchanged speech envelope. Thus, the plain effect (i.e. speech envelope predicting neural responses) is represented in the absolute weights (i.e. TRFs) obtained from this control model with the speech envelope and shuffled eye movement data as the predictor of neural activity. The direct (residual) effect (not mediated by eye movements) is obtained from the model including the speech envelope as well as true eye movements and is represented in the exclusive weights (c’) of the former predictor (i.e. speech envelope). If model weights are significantly reduced by the inclusion of true eye movements into the model in comparison to a model with a time-shuffled version of the same predictor (i.e. c’ &lt; c), this indicates that a meaningful part of the relationship between the speech envelope and neural responses was mediated by eye movements (for further details see also <xref ref-type="bibr" rid="c18">Gehmacher et al., 2024</xref>).</p>
</sec>
<sec id="s2h">
<title>Source and Principal Component Analysis</title>
<p>In order to estimate the location along with the temporal profile of this mediation effect and at the same time minimise the number of comparisons, we computed the main components of the effect, projected into source space. For this, we used all 306 MEG channels for our models as described in the previous section (note that here MEG responses were z-scored within channel type, i.e. within magnetometers and gradiometers separately). The resulting envelope model weights were then projected into source-space using an LCMV beamforming approach (Van Veen et. al., 1997). Spatial filters were computed by warping anatomical template images to the individual head shape and further brought into a common space by co-registering them based on the three anatomical landmarks (nasion, left and right preauricular points) with a standard brain from the Montreal Neurological Institute (MNI, Montreal, Canada; <xref ref-type="bibr" rid="c30">Mattout et al., 2007</xref>). For each participant, a single-shell head model (<xref ref-type="bibr" rid="c32">Nolte, 2003</xref>) was computed. Finally, as a source model, a grid with 1 cm resolution and 2982 voxels based on an MNI template brain was morphed into the brain volume of each participant. This allowed group-level averaging and statistical analysis as all the grid points in the warped grid belong to the same region across subjects.</p>
<p>Afterwards, envelope TRF edges of both plain and direct models were cropped to time-lags from –0.3 – 0.7 s to exclude potential regression artefacts. We then subtracted the absolute direct from the absolute plain TRF to obtain the ‘abs’ indirect (mediation) effect. We transformed the 2982 voxel space into an orthogonal component space with a principal component analysis (PCA) based on the grand average mediation effect. The number of components for further analysis was visually determined by plotting the ranked cumulative explained variance and estimating the ‘elbow’. Based on this inspection, we extracted weight matrices of the first three components and multiplied individual ‘abs’ plain TRFs (from the control model with time-shuffled eye movements) and ‘abs’ direct TRFs (from the test model with true eye-movements) with this weight matrix to obtain individual source location and temporal profile of the mediation components. The projected, single subject data was then used for statistical analysis.</p>
</sec>
<sec id="s2i">
<title>Statistical Analysis and Bayesian Models</title>
<p>To calculate the statistics, we used Bayesian multilevel regression models with Bambi (<xref ref-type="bibr" rid="c5">Capretto et al., 2020</xref>), a python package built on top of the PyMC3 package (<xref ref-type="bibr" rid="c36">Salvatier et al., 2016</xref>), for probabilistic programming. First, as a conceptual replication of <xref ref-type="bibr" rid="c39">Schubert et al. (2023)</xref>, we investigated the effect of neural speech tracking and its relation to individual prediction tendency as well as the influence of increasing noise (i.e. adding a distractor speaker). Separate models were calculated for all 102 magnetometers using the following model formula:
<disp-formula>
<graphic xlink:href="546746v4_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
“Neural speech envelope tracking” refers to the correlation coefficients between predicted and true brain responses from the aforementioned encoding model, trained and tested on the whole audio material within condition (single vs. multi-speaker). (Note that prediction tendency was always z-scored before entering the models).</p>
<p>Similarly, we investigated the effect of ocular speech tracking under different conditions of attention (i.e. attended single speaker, attended multi-speaker and unattended multi-speaker). In addition to replicating the findings from <xref ref-type="bibr" rid="c18">Gehmacher et al. (2024)</xref>, we extended this analysis for a detailed investigation of horizontal and vertical ocular speech envelope tracking and further included prediction tendency as a predictor:
<disp-formula>
<graphic xlink:href="546746v4_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
“Ocular speech envelope tracking” refers to the correlation coefficients between predicted and true eye movements from the encoding model (again trained and tested on the whole audio material within condition.</p>
<p>To investigate the effect of semantic violations, we compared envelope tracking between target words (high surprisal) and lexically matched controls (low surprisal), both for neural as well as ocular speech tracking:
<disp-formula>
<graphic xlink:href="546746v4_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Here “Speech envelope tracking” refers to the correlation coefficients between predicted and true responses (either neural or ocular) from the encoding model trained on the whole audio material but tested solely on the word-segments.</p>
<p>For the mediation analysis, we compared weights (TRFs) for the speech envelope between a plain (control) model that included shuffled eye-movements as second predictor and a residual (test) model that included true eye movements as a second predictor (i.e. c’ &lt; c). In order to investigate the temporal dynamics of the mediation, we included time-lags (–0.3 – 0.7 s) as a fixed effect into the model. To investigate potential null effects (neural speech tracking that is definitely independent of eye movements), we used a region of practical equivalence (ROPE) approach (see for example <xref ref-type="bibr" rid="c27">Kruschke, 2018</xref>). The dependent variable (absolute weights) was z-scored across time-lags and models to get standardised betas for the mediation effect (i.e. c’ &lt; c):
<disp-formula>
<graphic xlink:href="546746v4_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
As suggested by <xref ref-type="bibr" rid="c27">Kruschke (2018)</xref>, a null effect was considered for betas ranging between – 0.1 and 0.1. Accordingly, it was considered a significant mediation effect if betas were above 0.1 and at minimum two neighbouring time-points also showed a significant result.</p>
<p>Finally, we investigated the relationship between neural as well as ocular speech tracking and behavioural data using the averaged accuracy from the questions on story content that were asked at the end of each trial (in the following: “comprehension”) and averaged subjective ratings of difficulty. In order to avoid calculating separate models for all magnetometers again, we selected 10% of the channels that showed the strongest speech encoding effect and used the averaged speech tracking (z-scored within condition before entering the model) as predictor:
<disp-formula>
<graphic xlink:href="546746v4_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To investigate the relation between ocular speech tracking and behavioural performance, we used the following model (again speech tracking was z-scored within condition) separately for horizontal and vertical gaze direction:
<disp-formula>
<graphic xlink:href="546746v4_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For all models (as in <xref ref-type="bibr" rid="c18">Gehmacher et al, 2024</xref> and Schubert et al., 2023), we used the weakly-or non-informative default priors of Bambi (<xref ref-type="bibr" rid="c5">Capretto et al., 2020</xref>) and specified a more robust Student-T response distributions instead of the default gaussian distribution. To summarise model parameters, we report regression coefficients and the 94% high density intervals (HDI) of the posterior distribution (the default HDI in Bambi). Given the evidence provided by the data, the prior and the model assumptions, we can conclude from the HDIs that there is a 94% probability that a respective parameter falls within this interval. We considered effects as significantly different from zero if the 94%HDI did not include zero (with the exception of the mediation analysis where the 94%HDI had to fall above the ROPE). Furthermore, we ensured the absence of divergent transitions (r^ &lt; 1.05 for all relevant parameters) and an effective sample size &gt; 400 for all models (an exhaustive summary of Bayesian model diagnostics can be found in Vehtari et al., 2021). Finally, when we estimated an effect on brain sensor level (using all 102 magnetometers), we defined clusters for which an effect was only considered as significant if at minimum two neighbouring channels also showed a significant result.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Individual prediction tendency is related to neural speech tracking</title>
<p>In the first step, we wanted to investigate the relationship between individual prediction tendency and neural speech tracking under different conditions of noise. We quantified prediction tendency as the individual tendency to represent auditory features (i.e. pure tone frequency) of high probability in advance of an event (i.e. pure tone onset). Thus, this measure is a single value per subject, which comprises a) differences between two contextual probabilities (i.e. ordered vs. random) in b) feature-specific tone representations c) in advance of their observation (summed over a time-window of –0.3 – 0 s). Importantly, this prediction tendency was assessed in an independent entropy modulation paradigm (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>). On a group level we found an increased tendency to pre-activate a stimulus of high probability (i.e. forward transition) in an ordered context compared to a random context (see Fig, 2A). This effect replicates results from our previous work (<xref ref-type="bibr" rid="c39">Schubert et al., 2023</xref>, 2024). Using the summed difference between entropy levels (ordered – random) across pre-stimulus time, one value was extracted per subject (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>). This value was used as a proxy for “individual prediction tendency” and correlated with encoding of clear speech across different MEG sensors.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Individual prediction tendency.</title><p><bold>A)</bold> Time-resolved contrasted classifier decision: forward &gt; repetition for ordered and random repetition trials. Classifier tendencies showing frequency-specific prediction for tones with the highest probability (forward transitions) can be found even before stimulus onset but only in an ordered context (shaded areas always indicate 95% confidence intervals). Using the summed difference across pre-stimulus time, one prediction value was extracted per individual subject. <bold>B)</bold> Distribution of prediction tendency values across subjects (<italic>N</italic> = 29).</p></caption>
<graphic xlink:href="546746v4_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Neural speech tracking, quantified as the correlation coefficients between predicted and observed MEG responses to the speech envelope, was used as the dependent variable in Bayesian regression models (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>; see <xref rid="fig3" ref-type="fig">Fig. 3B</xref> for TRF weights). These models included condition (single vs. multi-speaker) as a fixed effect, with either prediction tendency or word surprisal as an additional predictor, and random effects for participants. Replicating previous findings (<xref ref-type="bibr" rid="c39">Schubert et al., 2023</xref>), we found widespread encoding of clear speech (average over cluster: <italic>β</italic> = 0.035, 94%HDI = [0.024, 0.046]), predominantly over auditory processing regions (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>), that was decreased (<italic>β</italic> = –0.018, 94%HDI = [-0.029, –0.006]) in a multi-speaker condition (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>). Furthermore, a stronger prediction tendency was associated with increased neural speech tracking (<italic>β</italic> = 0.014, 94%HDI = [0.004, 0.025]) over left frontal sensors (see <xref rid="fig3" ref-type="fig">Fig. 3E</xref>). We found no interaction between prediction tendency and condition (see <xref rid="fig3" ref-type="fig">Fig. 3F</xref>). These findings indicate that the relationship between individual prediction tendency and neural speech tracking is largely unaffected by demands on selective attention. Additionally, we wanted to investigate how semantic violations affect neural speech tracking. For this reason, we introduced rare words of high surprisal into the story by randomly replacing half of the nouns at the end of a sentence with the other half. In a direct comparison with lexically identical controls, we found an increased neural tracking of semantic violations (<italic>β</italic> = 0.039, 94%HDI = [0.007, 0.071]) over left temporal areas (see <xref rid="fig3" ref-type="fig">Fig. 3G</xref>). Furthermore, we found no interaction between word surprisal and speaker condition (see <xref rid="fig3" ref-type="fig">Fig. 3H</xref>). These findings indicate an increased representation of surprising words independent of background noise. In sum, we found that individual prediction tendency as well as semantic predictability affect neural speech tracking.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Neural speech tracking is related to prediction tendency and word surprisal, independent of selective attention.</title><p><bold>A</bold>) Envelope (x) – response (y) relationships are estimated using deconvolution (Boosting). The TRF (filter kernel, h) models how the brain processes the envelope over time. This filter is used to predict neural responses via convolution. Predicted responses are correlated with actual neural activity to evaluate model fit and the TRF’s ability to capture response dynamics. Correlation coefficients from these models are then used as dependent variables in Bayesian regression models. (Panel adapted from <xref ref-type="bibr" rid="c19">Gehmacher et al., 2024b</xref>). <bold>B)</bold> Temporal response functions (TRFs) depict the time-resolved neural tracking of the speech envelope for the single speaker and multi speaker target condition, shown here as absolute values averaged across channels. Solid lines represent the group average. Shaded areas represent 95% Confidence Intervals. <bold>C–H)</bold> The beta weights shown in the sensor plots are derived from Bayesian regression models in <bold>A)</bold>. For Panel C, this statistical model is based on correlation coefficients computed from the TRF models (further details can be found in the Methods Section). <bold>C)</bold> In a single speaker condition, neural tracking of the speech envelope was significant for widespread areas, most pronounced over auditory processing regions. <bold>D)</bold> The condition effect indicates a decrease in neural speech tracking with increasing noise (1 distractor). <bold>E)</bold> Stronger prediction tendency was associated with increased neural speech tracking over left frontal areas. <bold>F)</bold> However, there was no interaction between prediction tendency and conditions of selective attention. <bold>G)</bold> Increased neural tracking of semantic violations was observed over left temporal areas. H) There was no interaction between word surprisal and speaker condition, suggesting a representation of surprising words independent of background noise. Marked sensors indicate ‘significant’ clusters, defined as at least two neighboring channels showing a significant result. <italic>N</italic> = 29.</p></caption>
<graphic xlink:href="546746v4_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3b">
<title>Eye movements track acoustic speech in selective attention</title>
<p>In a second step, we aimed to replicate previous findings from <xref ref-type="bibr" rid="c18">Gehmacher and colleagues (2024)</xref>, showing that eye movements track the acoustic features of speech in absence of visual information. For this, we separately predicted horizontal and vertical eye movements from the acoustic speech envelope using temporal response functions (TRFs). The resulting model fit (i.e. correlation between true and predicted eye movements) is commonly referred to as “speech tracking”. Bayesian regression models were applied to evaluate tracking effects under different conditions of selective attention (single speaker, attended multi-speaker, unattended multi-speaker). Furthermore, we assessed whether individual prediction tendency or semantic word surprisal influenced ocular speech tracking. For vertical eye movements, we found evidence for attended speech tracking in a single speaker condition (<italic>β</italic> = 0.012, 94%HDI = [0.001, 0.0023]) but not in a multi-speaker condition (<italic>β</italic> = 0.006, 94%HDI = [-0.005, 0.016]; see <xref rid="fig4" ref-type="fig">Figure 4A</xref>). There was no evidence for tracking of the distracting speech stream (<italic>β</italic> = – 0.008, 94%HDI = [-0.020, 0.003]). On the contrary, horizontally directed eye movements selectively track attended (<italic>β</italic> = 0.014, 94%HDI = [0.005, 0.024]), but not unattended (<italic>β</italic> = – 0.002, 94%HDI = [-0.011, 0.007]) acoustic speech in a multi-speaker condition (see <xref rid="fig4" ref-type="fig">Figure 4B</xref>). Speech tracking in a single speaker condition did not reach significance (<italic>β</italic> = 0.009, 94%HDI = [-0.001, 0.017]) for horizontal eye movements. These findings indicate that eye movements selectively track attended, but not unattended acoustic speech. Furthermore, there seems to be a dissociation between horizontal and vertical ocular speech tracking, indicating that horizontal movements track attended speech in a multi-speaker condition, whereas vertical movements track attended speech in a single speaker condition (see <xref rid="tbl2" ref-type="table">Table 2</xref> for a summary of ocular speech tracking effects).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Ocular speech tracking is dependent on selective attention.</title><p><bold>A)</bold> Vertical eye movements ‘significantly’ track attended clear speech, but not in a multi-speaker condition. Temporal profiles of this effect show a downward pattern (negative TRF weights). <bold>B)</bold> Horizontal eye movements ‘significantly’ track attended speech in a multi-speaker condition. Temporal profiles of this effect show a left-rightwards (negative to positive TRF weights) pattern. Statistics were performed using Bayesian regression models. A ‘*’ within posterior distributions depicts a significant difference from zero (i.e. the 94%HDI does not include zero). Shaded areas in TRF weights represent 95% confidence intervals. <italic>N</italic> = 29.</p></caption>
<graphic xlink:href="546746v4_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><title>Model summary statistics for ocular speech tracking depending on condition and prediction tendency.</title></caption>
<graphic xlink:href="546746v4_tbl2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>Additionally, we wanted to investigate if predictive processes are related to ocular speech tracking using the same approach as for neural speech tracking (see previous section). Crucially, we found no evidence for a relationship between individual prediction tendency and ocular speech tracking on a vertical (<italic>β</italic> = 0.001, 94%HDI = [-0.005, 0.008]) or horizontal (<italic>β</italic> = – 0.001, 94%HDI = [-0.007, 0.004]) plane. Similarly, we found no difference in ocular speech tracking between words of high surprisal and their lexically matched controls (see <xref rid="tbl3" ref-type="table">Table 3</xref>). These findings indicate that individuals engage in ocular speech tracking independent of their individual prediction tendency or overall semantic probability.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><title>Model summary statistics for ocular speech tracking depending on word type and condition.</title></caption>
<graphic xlink:href="546746v4_tbl3.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s3c">
<title>Neural speech tracking is mediated by eye movements</title>
<p>Additionally, we performed a similar, but more detailed mediation analysis compared to <xref ref-type="bibr" rid="c18">Gehmacher and colleagues (2024)</xref> in order to separately investigate the contribution of horizontal and vertical eye movements to neural speech tracking across different time lags. Following mediation analysis requirements, only significant ocular speech tracking effects were further considered, i.e. vertical eye movements in the clear speech condition and horizontal eye movements in response to a target in the multi-speaker condition. We compared the plain effect (c) of neural speech tracking (using a simple stimulus model with speech envelope and a time-shuffled version of the respective eye movements as the predictors for neural responses) to its direct (residual) effect (c’) by including true horizontal or vertical eye movements as a second predictor into the stimulus model. The decrease in predictor weights from the plain to the residual stimulus model indicates the extent of the mediation effect. This model evaluates to what extent gaze behaviour functions as a mediator between acoustic speech input and brain activity. To establish a time-resolved mediation analysis in source-space, we computed the main components of the effect (via PCA, see <xref rid="fig5" ref-type="fig">Figure 5</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Ocular speech tracking and selective attention to speech share underlying neural computations.</title><p><bold>A)</bold> Vertical eye movements significantly mediate neural clear speech tracking throughout the time-lags from –0.3 – 0.7 s for principal component 1 (PC1) over right-lateralized auditory regions.This mediation effect propagates to more leftwards lateralized auditory areas over later time-lags for PC2 and PC3. <bold>B)</bold> Horizontal eye movements similarly contribute to neural speech tracking of a target in a multi-speaker condition over right-lateralized auditory processing regions for PC1, also with significant anticipatory contributions and a clear peak at ∼ 0.18 s. PC2 shows a clear left-lateralization, however not only over auditory, but also parietal areas almost entirely throughout the time-window of interest with a clear anticipatory effect starting at –0.3 s. For PC3, there still remained a small anticipatory cluster ∼ –0.2 s again over mostly left-lateralized auditory regions. Colour bars represent PCA weights for the group-averaged mediation effect. Shaded areas on time-resolved model-weights represent regions of practical equivalence (ROPE) according to <xref ref-type="bibr" rid="c27">Kruschke (2018)</xref>. Solid lines show ‘significant’ clusters where at minimum two neighbouring time-points showed a significant mediation effect. Statistics were performed using Bayesian regression models. <italic>N</italic> = 29.</p></caption>
<graphic xlink:href="546746v4_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We found significant mediation effects for all three principal components (PC) for both vertical eye movements in the clear speech condition (see <xref rid="fig5" ref-type="fig">Figure 5A</xref>) and horizontal eye movements in response to a target in the multi-speaker condition (see <xref rid="fig5" ref-type="fig">Figure 5B</xref>). PC1 reached significance (nearly) across the whole time-window from –0.3 – 0.7 s over widespread auditory regions in both conditions with a right-hemispheric dominance, peaking at ∼ 0.18 s. Interestingly, PC2 instead loaded mostly on left auditory regions, with a slight anticipation effect for both vertical and horizontal eye movements. Additionally, PC2 loaded over left parietal areas for the mediation via horizontal eye movements in the multi-speaker condition. In both conditions, PC2 showed another early peak at ∼80 ms. The mediation effect remained significant almost entirely over positive time-lags for both conditions. We found less contributions of vertical eye movements to neural clear speech tracking for PC3 with significant clustering ∼ 0.2 – 0.4 s over scattered cortical regions. For horizontal eye movements, PC3 still showed an effect in left auditory areas at several short peaks (∼ –0.2 s, ∼ 0.05 s, and ∼ 0.4 s). Taken together, this suggests that eye movements contribute considerably to neural speech tracking over widespread cortical areas that are commonly related to speech processing and attention.</p>
</sec>
<sec id="s3d">
<title>Neural and ocular speech tracking are differently related to comprehension</title>
<p>In a final step, we addressed the behavioural relevance of neural as well as ocular speech tracking respectively. At the end of every trial, participants were asked to evaluate 4 different true or false statements about the target story. The accuracy of these responses was averaged within condition (single vs. multi-speaker) and served as an approximation for semantic speech comprehension. Additionally, we evaluated the averaged subjective ratings of difficulty (which were given on a 5-point likert scale).</p>
<p>To avoid calculating separate models for all 102 magnetometers, neural encoding was averaged over selected channels (10% showing the strongest encoding effect). Bayesian regression models were used to investigate relationships between neural/ocular speech tracking and comprehension or difficulty. Ocular speech tracking was analysed separately for horizontal and vertical eye movements. We found no significant relationship between neural speech tracking and comprehension (<italic>β</italic> = 0.138, 94%HDI = [-0.050, 0.330]), no interaction between neural speech tracking and condition (<italic>β</italic> = –0.088, 94%HDI = [-0.390, 0.201]), but a significant effect for condition (<italic>β</italic> = –0.438, 94%HDI = [-0.714, –0.178]), indicating that comprehension was decreased in the multi-speaker condition (see <xref rid="fig6" ref-type="fig">Figure 6A</xref>). Similarly, we found no effect of prediction tendency on comprehension (<italic>β</italic> = 0.050, 94%HDI = [-0.092, 0.197]). When investigating subjective ratings of task difficulty, we also found no effect for neural speech tracking (<italic>β</italic> = 0.156, 94%HDI = [-0.079, 0.382]), no interaction between neural speech tracking and condition (<italic>β</italic> = 0.041, 94%HDI = [-0.250, 0.320]), nor individual prediction tendency (<italic>β</italic> = –0.058, 94%HDI = [-0.236, 0.127]). There was, however, a significant difference between conditions (<italic>β</italic> = 1.365, 94%HDI = [1.128, 1.612]), indicating that the multi-speaker condition was rated more difficult than the single speaker condition.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Ocular, but not neural speech tracking is related to semantic speech comprehension.</title><p><bold>A)</bold> There was no significant relationship between neural speech tracking (10% sensors with strongest encoding effect) and comprehension, however, a condition effect indicated that comprehension was generally decreased in the multi-speaker condition. <bold>B &amp; C)</bold> A ‘significant’ negative relationship between comprehension and vertical as well as horizontal ocular speech tracking shows that participants with weaker comprehension increasingly engaged in ocular speech tracking. Statistics were performed using Bayesian regression models. Shaded areas represent 94% HDIs. <italic>N</italic> = 29.</p></caption>
<graphic xlink:href="546746v4_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In contrast to neural findings, we found a negative relationship between ocular speech tracking and comprehension for vertical as well as horizontal eye movements irrespective of condition (see <xref rid="fig6" ref-type="fig">Figure 6B-C</xref> and <xref rid="tbl4" ref-type="table">Table 4</xref>). This suggests that participants with weaker performance in semantic comprehension increasingly engaged in ocular speech tracking. There was, however, no significant relationship between subjectively rated difficulty and ocular speech tracking (see <xref rid="tbl5" ref-type="table">Table 5</xref>). Presumably, subjective ratings are less comparable between participants, which might be one reason why we did find an effect for objective but not subjective measures.</p>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4:</label>
<caption><title>Model summary statistics for comprehension depending on ocular speech tracking and condition.</title></caption>
<graphic xlink:href="546746v4_tbl4.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5:</label>
<caption><title>Model summary statistics for rated difficulty depending on ocular speech tracking and condition.</title></caption>
<graphic xlink:href="546746v4_tbl5.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>In sum, the current findings show that ocular speech tracking is differently related to comprehension than neural speech tracking, suggesting that they might not refer to the same underlying concept (e.g. improved representations vs. increased attention). A mediation analysis, however, suggests that they are related and that ocular speech tracking contributes to neural speech tracking (or vice versa).</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>In the current study, we aimed to replicate and extend findings from two recent studies from our lab in order to integrate them into a comprehensive view of speech processing. In <xref ref-type="bibr" rid="c39">Schubert and colleagues (2023)</xref>, we found that individual prediction tendencies are related to cortical speech tracking. In <xref ref-type="bibr" rid="c18">Gehmacher and colleagues (2024)</xref>, we found that eye movements track acoustic speech in selective attention (a phenomenon that we termed “ocular speech tracking”). In the present study, we were able to replicate both findings, providing further details and insight into these phenomena.</p>
<p>In a first step, we confirmed that individuals with a stronger prediction tendency (which was inferred from anticipatory probabilistic sound representations in an independent paradigm) showed an increased neural speech tracking over left frontal areas. Thus, the finding that prediction tendencies generalise across different listening situations seems to be robust. It is assumed that predictions are directly linked to the interpretation of sensory information. This interpretation a) is likely to occur simultaneously at different places in the cognitive (and anatomical) hierarchy and b) describes a bidirectional process in which internal models influence and are influenced by sensory input (<xref ref-type="bibr" rid="c26">Knill &amp; Pouget, 2004</xref>). It should be noted that our measure of individual prediction tendency makes no claim on the relative weighting of top-down or bottom-up processing; it does, however, infer an absolute tendency to generate anticipatory (top-down) predictions. This type of prediction is relevant for acoustic processing such as speech and music, whose predictability unfolds over time.</p>
<p>The replicable association between individual differences in this prediction tendency and speech processing stresses the importance of research focusing on the beneficial aspects of individual “traits” in predictive processing. As suggested in a comparative study (Schubert et al., 2024), these predictive tendencies or traits do not generalise across modalities but seem to be reliable within the auditory modality. We suggest that they could serve as an independent predictor of linguistic abilities, complementing previous research on statistical learning (e.g. <xref ref-type="bibr" rid="c40">Siegelman &amp; Frost, 2015</xref>).</p>
<p>In a second step, we were able to support the assumption that eye movements track prioritised acoustic modulations in a continuous speech design without visual input. With the current (extended) conceptual replication, we were able to address an important consideration from Gemacher and colleagues (2024): Ocular speech tracking is not restricted to a simplistic 5-word sentence structure but can also be found for continuous, narrative speech. Importantly, in contrast to <xref ref-type="bibr" rid="c18">Gehmacher et al. (2024)</xref>, we did not observe ocular tracking of the multi-speaker distractor in this study. This difference is likely attributable to the simplistic single-trial, 5-word task structure in Gehmacher et al., which resulted in high temporal overlap between the target and distractor speech streams and likely drove the significant distractor-tracking effects observed in that study. The absence of such an effect during continuous listening in our study suggests that ocular tracking is indeed more specific to selective attention. However, in line with previous results, we found that horizontal ocular speech tracking is increased in a multi-speaker (compared to a single speaker) condition in response to an attended target but not a distractor speaker. In contrast, we found that vertical eye movements solely track auditory information in a single speaker condition. The differential contribution of vertical vs. horizontal eye movements to auditory processing is not a widely studied topic in neuroscience and further replications are necessary to establish the robustness of this dissociation. One possible explanation for these findings is, that in the multi-speaker condition, the demand for spatial segregation was increased compared to the single speaker condition. Although speech was always presented at phantom centre even for both speakers during the multi-speaker condition, multiple speakers are distributed mostly horizontally in natural human environments (e.g. the classic cocktailparty situation; <xref ref-type="bibr" rid="c7">Cherry, 1953</xref>). The observed effect might resemble a residual of this learned association between a spatially segregated target speaker’s acoustic output and a lateral shift of gaze (and attention) on the horizontal plane towards the target’s source location, maximising information and thereby minimising uncertainty about the targeted auditory object. Relatedly, it remains an open question whether microsaccades are a key feature driving ocular speech tracking. However, our current study does not analyze microsaccades due to methodological constraints: microsaccades are binary response vectors, which are incompatible with TRF analyses used here. Addressing this would require adapting models to handle time-continuous binary response data or potentially exploring alternative approaches, such as regression-based ERFs (e.g., as in <xref ref-type="bibr" rid="c21">Heilbron et al., 2022</xref>). While these limitations preclude microsaccade analysis in the current study, we hypothesize that they could enhance temporal precision and selectively amplify relevant sensory input, supporting auditory perception. Future studies should explore this possibility to uncover the specific contributions of microsaccades to speech tracking.</p>
<p>Irrespective of the potential differences in gaze direction and condition, we found that ocular movements contribute to neural speech tracking over widespread auditory and parietal areas, replicating the sensor-level findings of <xref ref-type="bibr" rid="c18">Gehmacher et al. (2024)</xref>. In addition, time-resolved analyses of this mediation effect further extend these findings and suggest even anticipatory contributions. It is important to note that our current findings do not allow for inference on directionality. Our choice of ocular movements as a mediator was motivated by the fact that the relationship between acoustic speech and neural activity is well established, as well as previous results indicating that oculomotor activity contributes to cognitive effects in auditory attention (Popov et al., 2022). However, an alternative model may suggest that neural activity mediates the effect of ocular speech tracking. Hence, it is possible that ocular mediation of speech tracking may reflect a) active (ocular) sensing for information driven by (top-down) selective attention or b) improved neural representations as a consequence of temporally aligned increase of sensory gain or c) (not unlikely) both. In fact, when rejecting the notion of a single bottom-up flow of information and replacing it with a model of distributed parallel and dynamic processing, it seems only reasonable to assume that the direction of communication (between our eyes and our brain) will depend on where (within the brain) as well as when we look at the effect. Thus, the regions and time-windows reported here should be taken as an illustration of oculo-neural communication during speech processing rather than an attempt to “explain” neural speech processing by ocular movements.</p>
<p>Despite the finding that eye movements mediate neural speech tracking, the behavioural relevance for semantic comprehension appears to differ between ocular and neural speech tracking. Specifically, we found a negative association between ocular speech tracking and comprehension, indicating that participants with lower comprehension performance exhibited increased ocular speech tracking. Interestingly, no significant relationship was observed between neural tracking and comprehension.</p>
<p>In this context, the negative association between ocular tracking and comprehension might reflect individual differences in how participants allocate cognitive resources. Participants with lower comprehension may rely more heavily on attentional mechanisms to process acoustic features, as evidenced by increased ocular tracking. This reliance could represent a compensatory strategy when higher-order processes, such as semantic integration or memory retrieval, are less effective. Importantly, our comprehension questions (see Experimental Procedure) targeted a broad range of processes, including intelligibility and memory, suggesting that this relationship reflects a trade-off in resource allocation between low-level acoustic focus and integrative cognitive tasks.</p>
<p>Rather than separating eye and brain responses conceptually, our analysis highlights their complementary contributions. Eye movements may enhance neural processing by increasing sensitivity to acoustic properties of speech, while neural activity builds on this foundation to integrate information and support comprehension. Together, these systems form an interdependent mechanism, with eye and brain responses working in tandem to facilitate different aspects of speech processing.</p>
<p>This interpretation is consistent with the absence of a difference in ocular tracking for semantic violations (e.g., words with high surprisal versus lexically matched controls), reinforcing the view that ocular tracking primarily reflects attentional engagement with acoustic features rather than direct involvement in semantic processing. This aligns with previous findings that attention modulates auditory responses to acoustic features (e.g., <xref ref-type="bibr" rid="c14">Forte et al., 2017</xref>), further supporting the idea that ocular tracking reflects mechanisms of selective attention rather than representations of linguistic content.</p>
<p>Future research should investigate how these systems interact and explore how ocular tracking mediates neural responses to linguistic features, such as lexical or semantic processing, to better understand their joint contributions to comprehension.</p>
<p>Interestingly, we were not able to relate individual prediction tendency to ocular speech tracking. This raises the question whether ocular speech tracking reflects attentional mechanisms rather than predictive processes. Indeed, the current findings suggest that prediction tendencies and active ocular sensing are related to different aspects of neural speech processing. We propose a perspective in which active sensing is motivated by selective attention, whereas anticipatory prediction tendency is an independent correlate of neural speech tracking. Even though predictive processing as well as attention might be considered as the two pillars on which perception rests, research investigating their selective contributions and interactions is rare. <xref ref-type="bibr" rid="c43">Summerfield and de Lange (2014)</xref> have argued that predictions and attention are distinctive mechanisms as the former refers to the probability and the latter to the relevance of an event, arguing that events can be conditionally probable but irrelevant for current goals and vice versa. Similarly, it has been proposed that attention can be integrated into the predictive coding framework in reference to the optimization of sensory precision (<xref ref-type="bibr" rid="c13">Feldman &amp; Friston, 2010</xref>). In this view, predictions encode probabilistic representations of a feature and “prediction tendency” refers to the individual reliance on these representations, whereas selective attention determines the precision of sensory inflow that leads to internal model updating. This interpretation is in line with our finding that a) anticipatory feature-specific predictions can be found in a passive listening task, b) prediction tendencies are not increasingly linked to speech tracking with increasing demands on selective attention, whereas on the other hand c) ocular speech tracking seems to increase with selective attention (at least on a horizontal plane), d) remains unaffected by semantic probability and e) contributes to neural speech processing over widespread auditory and parietal areas with a potential overlap with attentional networks. For this reason, we refer to ocular speech tracking as an “active sensing” mechanism that implements the attentional optimization of sensory precision. Instead of passively transducing any input into neural activity, we actively engage with our environment to maximise information, i.e. reduce uncertainty. It has been suggested that motor routines contribute to temporal precision of selective attention, largely determining sensory inflow and hence perception (<xref ref-type="bibr" rid="c31">Morillon et al., 2015</xref>; <xref ref-type="bibr" rid="c38">Schroeder et al., 2010</xref>). In the current framework, we refer to active sensing via eye movements as shifts of gaze at exact points in time aligned with intensity (information content) fluctuations. In particular, active ocular sensing may even help to increase sensory gain already in the auditory periphery at specific intervals, synchronised with the temporal modulation of attended (but not unattended) speech. As a consequence of this “sensory gating” already at early stages, eye movements potentially affect auditory processing from the ear to the cortex (also see <xref ref-type="bibr" rid="c28">Leszczynski et al., 2023</xref> for saccadic modulations of cortical excitability in auditory areas), contributing to neural speech representations rather than encoding them. Again, this idea is supported by our finding that ocular speech tracking seems to be unaffected by semantic violations.</p>
<p>Indeed, similar active ocular sensing mechanisms have already been suggested to facilitate sound localization (<xref ref-type="bibr" rid="c42">Lovich et al., 2022</xref>). Since the current paradigm did not allow for spatial segregation (as competing speech streams have all been presented at phantom centre), it required a different strategy such as (spectro-)temporal differentiation. We argue that active ocular sensing increases temporal precision of complex acoustic input (in order to parse speech into its components of rhythmic, temporally predictable patterns within a particular speaker). Based on the joint findings of the present as well as its preceding studies (<xref ref-type="bibr" rid="c18">Gehmacher et al., 2024</xref>, <xref ref-type="bibr" rid="c39">Schubert et al., 2023</xref>), we propose a unified working model in which anticipatory predictions as well as active ocular sensing work (independently) together to support auditory speech perception (see <xref rid="fig7" ref-type="fig">Figure 7</xref> for a schematic illustration). We suggest that anticipatory predictions about a feature help to interpret auditory information by prioritizing representational content of high probability at different levels along the perceptual hierarchy. Accordingly, these predictions are formed in parallel and carry high feature-specificity but low temporal precision (as they are anticipatory in nature). This idea is supported by our finding that pure-tone anticipation is visible over a widespread prestimulus interval, instead of being locked to sound onset. It should be noted that the representational content is likely to be different at different levels of the perceptual hierarchy (e.g. encoding of phonemes, words, semantics or even more abstract speaker intentions). Even though the terminology is suggestive of a fixed sequence (similar to a multi storey building) with levels that must be traversed one after each other (and even the more spurious idea of a rooftop, where the final perceptual experience is formed and stored into memory), we distance ourselves from these (possibly unwarranted) ideas. Our usage of “higher” or “lower” simply refers to the observation that the probability of a feature at a higher (as in more associative) level affects the interpretation (and thus the representation and prediction) of a feature at lower (as in more segregated) levels (<xref ref-type="bibr" rid="c6">Caucheteux et al., 2023</xref>). Our own findings suggest that individuals differ in their general tendency to create such anticipatory predictions, which leads to differences in neural speech tracking.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><title>A schematic illustration of the framework:</title><p>A speech percept is processed in representational (feature-) spacetime (X and Y-axes) at different levels of the cognitive hierarchy (Z-axis), which ranges from highly selective to more associative representations (without the notion of an “apex” in this hierarchy). The temporal and spatial characteristics of a representation depends on where and when (in the brain) it is probed. Anticipatory predictions (purple) help to interpret auditory information at different levels in parallel with high feature-specificity but low temporal precision. These anticipatory predictions reflect to some extent individual tendencies (and differences) that generalise across listening situations. In contrast, active ocular sensing (green) increases the temporal precision already at lower stages of the auditory system to facilitate bottom–up processing at specific timescales (similar to neural oscillations). It does not necessarily convey feature-specific information, but is more likely used to boost (or filter) information around relevant time windows. Our results suggest that this mechanism is motivated by selective attention (blue) rather than predictive assumptions.</p></caption>
<graphic xlink:href="546746v4_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Active ocular sensing, on the other hand, increases temporal precision – potentially already at the early stages of sound transduction – to facilitate bottom-up processing of selectively attended input. Crucially, we suggest that active (ocular) sensing does not necessarily convey feature-or content-specific information, it is merely used to boost (and conversely filter) sensory input at specific timescales (similar to neural oscillations). This assumption is supported by our finding that semantic violations are not differentially encoded in gaze behaviour than lexical controls. Our research suggests that this active ocular sensing mechanism or “filter” is driven by internal (attentional) goals rather than prior beliefs (stressing the distinction to other active sensing mechanisms).</p>
<p>With this speculative framework we attempt to describe and relate three important phenomena with respect to their relevance for speech processing: 1) “Anticipatory predictions” that are created in absence of attentional demands and contain probabilistic information about stimulus features (here, inferred from frequency-specific pre-activations during passive listening to sound sequences). 2) “Selective attention” that allocates resources towards relevant (whilst suppressing distracting) information (which was manipulated by the presence or absence of a distractor speaker). And finally 3) “active ocular sensing”, which refers to gaze behaviour that is temporally aligned to attended (but not unattended) acoustic speech input (inferred from the discovered phenomenon of ocular speech tracking). We propose that auditory inflow is, at a basic level, temporally modulated via active ocular sensing, which “opens the gates” in the sensory periphery at relevant timepoints. How exactly this mechanism is guided (for example where the information about crucial timepoints comes from, if not from prediction, and whether it requires habituation to a speech stream etc.) is yet unclear. Unlike predictive tendencies, active ocular sensing appears to reflect selective attention, manifesting as a mechanism that optimizes sensory precision. Individual differences with respect to anticipatory predictions on the other hand, seem to be independent from the other two entities, but nevertheless relevant for speech processing. We therefore support the notion that representational content is interpreted based on prior probabilistic assumptions. If we consider the idea that “a percept” of an (auditory) object is actually temporally and spatially distributed (across representational spacetime – see <xref rid="fig7" ref-type="fig">Fig. 7</xref>), the content of information depends on where and when it is probed (see for example Dennett, 1991 for similar ideas on consciousness). Having to select from multiple interpretations across space and time requires a careful balance between the weighting of internal models and the allocation of resources based on current goals. We suggest that in the case of speech processing, this challenge results in an independent adaptation of feature-based precision-weighting by predictions on the one hand and temporal precision-weighting by selective attention on the other.</p>
<p>We suggest that future research on auditory perception should integrate conceptual considerations on predictive processing, active (crossmodal) sensing, and selective attention. In particular, it would be interesting whether ocular speech tracking can be observed for unfamiliar languages with unpredictable prosodic rate. Furthermore, the relationship between neural oscillations in selective attention and active sensing should be further investigated using experimental modulations to address the important, pending question of causality. Brain stimulation (such as tACS; transcranial alternating current stimulation) could be used in an attempt to alter temporal processing frames and / or ocular speech tracking. With regards to the latter, future studies should focus on the potential consequences of inhibited active sensing (e.g. actively disrupting natural gaze behaviour) for neural speech tracking. Our interpretation suggests that increased tracking of unexpected input (i.e. semantic violations) should be affected by active ocular sensing if sensory gain in the periphery is indeed dependent on this mechanism. Currently, the findings propose that active ocular sensing, with its substantial contribution to neural speech tracking, is driven by selective attention, and not by individual differences in prediction tendency.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>J.S. and Q.G. are supported by the Austrian Science Fund (FWF; Doctoral College “Imaging the Mind”; W 1233-B). Q.G. is also supported by the Austrian Research Promotion Agency (FFG; BRIDGE 1 project “SmartCIs”; 871232) and F.S. is supported by WS Audiology. Thanks to the whole research team. Special thanks to Manfred Seifter for his support in conducting the MEG measurements.</p>
</ack>
<sec id="d1e1299" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Author contributions</title>
<p>J.S. and Q.G. designed the experiment, analysed the data, generated the figures, and wrote the manuscript. T.H. recruited participants and supported the data analysis. F.S. supported the data analysis and edited the manuscript. N.W. acquired the funding, supervised the project, and edited the manuscript.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname>, <given-names>D. H</given-names></string-name></person-group>. (<year>1997</year>). <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), <fpage>433</fpage>–<lpage>436</lpage>. <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Das</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kulasingham</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Bhattasali</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gaston</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Resnik</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Simon</surname>, <given-names>J. Z</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Eelbrain: A Python toolkit for time-continuous analysis with temporal response functions</article-title>. <source>BioRxiv</source>, <fpage>2021.08.01.454687</fpage>. <pub-id pub-id-type="doi">10.1101/2021.08.01.454687</pub-id></mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Simon</surname>, <given-names>J. Z</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Continuous speech processing</article-title>. <source>Current Opinion in Physiology</source>, <volume>18</volume>, <fpage>25</fpage>–<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1016/j.cophys.2020.07.014</pub-id></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Broderick</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Lalor</surname>, <given-names>E. C</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Semantic Context Enhances the Early Auditory Encoding of Natural Speech</article-title>. <source>Journal of Neuroscience</source>, <volume>39</volume>(<issue>38</issue>), <fpage>7564</fpage>–<lpage>7575</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0584-19.2019</pub-id></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Capretto</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Piho</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kumar</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Westfall</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yarkoni</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Martin</surname>, <given-names>O. A</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Bambi: A simple interface for fitting Bayesian linear models in Python</article-title>. <source>arXiv</source> Preprint ArXiv:<pub-id pub-id-type="arxiv">2012.10754</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caucheteux</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>King</surname>, <given-names>J. R</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Evidence of a predictive coding hierarchy in the human brain listening to speech</article-title>. <source>Nature human behaviour</source>, <volume>7</volume>(<issue>3</issue>), <fpage>430</fpage>–<lpage>441</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cherry</surname>, <given-names>E. C</given-names></string-name></person-group>. (<year>1953</year>). <article-title>Some experiments on the recognition of speech, with one and with two ears</article-title>. <source>Journal of the acoustical society of America</source>, <volume>25</volume>, <fpage>975</fpage>–<lpage>979</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crosse</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Di Liberto</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Bednar</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Lalor</surname>, <given-names>E. C.</given-names></string-name></person-group> (<year>2016</year>). <article-title>The multivariate temporal response function (mTRF) toolbox: a MATLAB toolbox for relating neural signals to continuous stimuli</article-title>. <source>Frontiers in human neuroscience</source>, <volume>10</volume>, <fpage>604</fpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cui</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Hondzinski</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Gaze tracking accuracy in humans: Two eyes are better than one</article-title>. <source>Neuroscience Letters</source>, <volume>396</volume>(<issue>3</issue>), <fpage>257</fpage>–<lpage>262</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>David</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Shamma</surname>, <given-names>S. A</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Estimating sparse spectro-temporal receptive fields with natural stimuli</article-title>. <source>Network (Bristol, England)</source>, <volume>18</volume>(<issue>3</issue>), <fpage>191</fpage>–<lpage>212</lpage>. <pub-id pub-id-type="doi">10.1080/09548980701609235</pub-id></mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Demarchi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sanchez</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Weisz</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Automatic and feature-specific prediction-related neural activity in the human auditory system</article-title>. <source>Nature Communications</source>, <volume>10</volume>(<issue>1</issue>), Article 1. <pub-id pub-id-type="doi">10.1038/s41467-019-11440-1</pub-id></mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Donhauser</surname>, <given-names>P. W.</given-names></string-name>, &amp; <string-name><surname>Baillet</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Two Distinct Neural Timescales for Predictive Speech Processing</article-title>. <source>Neuron</source>, <volume>105</volume>(<issue>2</issue>), <fpage>385</fpage>–<lpage>393.e9.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.019</pub-id></mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feldman</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Friston</surname>, <given-names>K. J</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Attention, uncertainty, and free-energy</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>4</volume>, <fpage>215</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2010.00215</pub-id></mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Forte</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Etard</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Reichenbach</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2017</year>). <article-title>The human auditory brainstem response to running speech reveals a subcortical mechanism for selective attention</article-title>. <source>eLife</source>, <volume>6</volume>, <elocation-id>e27203</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.27203</pub-id></mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2010</year>). <article-title>The free-energy principle: A unified brain theory?</article-title> <source>Nature Reviews Neuroscience</source>, <volume>11</volume>(<issue>2</issue>), Article 2. <pub-id pub-id-type="doi">10.1038/nrn2787</pub-id></mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Sajid</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Quiroga-Martinez</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Parr</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Price</surname>, <given-names>C. J.</given-names></string-name>, &amp; <string-name><surname>Holmes</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Active listening</article-title>. <source>Hearing Research</source>, <volume>399</volume>, <fpage>107998</fpage>. <pub-id pub-id-type="doi">10.1016/j.heares.2020.107998</pub-id></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galantucci</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Fowler</surname>, <given-names>C. A.</given-names></string-name>, &amp; <string-name><surname>Turvey</surname>, <given-names>M. T</given-names></string-name></person-group>. (<year>2006</year>). <article-title>The motor theory of speech perception reviewed</article-title>. <source>Psychonomic bulletin &amp; review</source>, <volume>13</volume>(<issue>3</issue>), <fpage>361</fpage>–<lpage>377</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gehmacher</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Schubert</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Hartmann</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Reisinger</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rösch</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schwarz</surname> <given-names>K.</given-names></string-name>, <string-name><surname>Popov</surname> <given-names>T.</given-names></string-name>, <string-name><surname>Chait</surname> <given-names>M.</given-names></string-name>, <string-name><surname>Weisz</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Eye movements track prioritized auditory features in selective attention to natural speech</article-title>. <source>Nature Communications</source>, <volume>15</volume>(<issue>1</issue>), <fpage>3692</fpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Gehmacher</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Schubert</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kaltenmaier</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Weisz</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Press</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2024b</year>). <article-title>The “Ocular Response Function” for encoding and decoding oculomotor related neural activity</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hartmann</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Weisz</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2020</year>). <article-title>An Introduction to the Objective Psychophysics Toolbox</article-title>. <source>Frontiers in Psychology</source>, <volume>11</volume>, <fpage>585437</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2020.585437</pub-id></mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heilbron</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Armeni</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schoffelen</surname>, <given-names>J.-M.</given-names></string-name>, <string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A hierarchy of linguistic predictions during natural language comprehension</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>119</volume>(<issue>32</issue>), <fpage>e2201968119</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2201968119</pub-id></mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heilbron</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Armeni</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schoffelen</surname>, <given-names>J.-M.</given-names></string-name>, <string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A hierarchy of linguistic predictions during natural language comprehension</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>119</volume>(<issue>32</issue>), <fpage>e2201968119</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2201968119</pub-id></mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jin</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Zou</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Ding</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Eye activity tracks task-relevant structures during speech and auditory sequence perception</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>), Article 1. <pub-id pub-id-type="doi">10.1038/s41467-018-07773-y</pub-id></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kisler</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Reichel</surname>, <given-names>U.</given-names></string-name>, &amp; <string-name><surname>Schiel</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Multilingual processing of speech via web services</article-title>. <source>Computer Speech &amp; Language</source>, <volume>45</volume>, <fpage>326</fpage>–<lpage>347</lpage>. <pub-id pub-id-type="doi">10.1016/j.csl.2017.01.005</pub-id></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kleiner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Pelli</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ingling</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Murray</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Broussard</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2007</year>). <article-title>What’s new in Psychtoolbox-3</article-title>. <source>Perception</source>, <volume>36</volume>, <fpage>1</fpage>–<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1068/v070821</pub-id></mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knill</surname>, <given-names>D. C.</given-names></string-name>, &amp; <string-name><surname>Pouget</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2004</year>). <article-title>The Bayesian brain: The role of uncertainty in neural coding and computation</article-title>. <source>Trends in Neurosciences</source>, <volume>27</volume>(<issue>12</issue>), <fpage>712</fpage>–<lpage>719</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id></mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kruschke</surname>, <given-names>J. K</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Rejecting or Accepting Parameter Values in Bayesian Estimation</article-title>. <source>Advances in Methods and Practices in Psychological Science</source>, <volume>1</volume>(<issue>2</issue>), <fpage>270</fpage>–<lpage>280</lpage>. <pub-id pub-id-type="doi">10.1177/2515245918771304</pub-id></mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leszczynski</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bickel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nentwich</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Russ</surname>, <given-names>B. E.</given-names></string-name>, <string-name><surname>Parra</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Schroeder</surname>, <given-names>C. E</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Saccadic modulation of neural excitability in auditory areas of the neocortex</article-title>. <source>Current Biology</source>, <volume>33</volume>(<issue>7</issue>), <fpage>1185</fpage>–<lpage>1195.e6.</lpage> <pub-id pub-id-type="doi">10.1016/j.cub.2023.02.018</pub-id></mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liberman</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Mattingly</surname>, <given-names>I. G</given-names></string-name></person-group>. (<year>1985</year>). <article-title>The motor theory of speech perception revised</article-title>. <source>Cognition</source>, <volume>21</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>36</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mattout</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Henson</surname>, <given-names>R. N.</given-names></string-name>, &amp; <string-name><surname>Friston</surname>, <given-names>K. J</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Canonical source reconstruction for MEG</article-title>. <source>Computational Intelligence and Neuroscience</source>, <volume>2007</volume>(<issue>1</issue>), <fpage>067613</fpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morillon</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Hackett</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Kajikawa</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Schroeder</surname>, <given-names>C. E</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Predictive motor control of sensory dynamics in auditory active sensing</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>31</volume>, <fpage>230</fpage>–<lpage>238</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2014.12.005</pub-id></mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nolte</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2003</year>). <article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title>. <source>Physics in Medicine &amp; Biology</source>, <volume>48</volume>(<issue>22</issue>), <fpage>3637</fpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oberfeld</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Klöckner-Nowotny</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Individual differences in selective attention predict speech identification at a cocktail party</article-title>. <source>eLife</source>, <volume>5</volume>, <elocation-id>e16747</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.16747</pub-id></mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Schoffelen</surname>, <given-names>J.-M</given-names></string-name></person-group>. (<year>2011</year>). <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Computational Intelligence and Neuroscience</source>, <volume>2011</volume>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruggles</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bharadwaj</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Shinn-Cunningham</surname>, <given-names>B. G</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Normal hearing is not enough to guarantee robust encoding of suprathreshold features important in everyday communication</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>108</volume>(<issue>37</issue>), <fpage>15516</fpage>–<lpage>15521</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1108912108</pub-id></mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salvatier</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wiecki</surname>, <given-names>T. V.</given-names></string-name>, &amp; <string-name><surname>Fonnesbeck</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Probabilistic programming in Python using PyMC3</article-title>. <source>PeerJ Computer Science</source>, <volume>2</volume>, <fpage>e55</fpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Schiel</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Ohala</surname>, <given-names>J. J</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Automatic Phonetic Transcription of Non-Prompted Speech. 14th International Congress of Phonetic Sciences, San Francisco, USA, 1. - 7. August 1999. Ohala, John J. (ed.)</article-title>, <conf-name>Proceedings of the XIVth International Congress of Phonetic Sciences : ICPhS 99</conf-name>. <fpage>607</fpage>-<lpage>610</lpage> <pub-id pub-id-type="doi">10.5282/ubm/epub.13682</pub-id></mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Radman</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Scharfman</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Dynamics of Active Sensing and Perceptual Selection</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>20</volume>(<issue>2</issue>), <fpage>172</fpage>–<lpage>176</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2010.02.010</pub-id></mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schubert</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Gehmacher</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Bresgen</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Weisz</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Cortical speech tracking is related to individual prediction tendencies. <italic>Cerebral Cortex</italic></article-title>, <source>bhac</source><volume>528</volume>. <pub-id pub-id-type="doi">10.1093/cercor/bhac528</pub-id></mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Siegelman</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Frost</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Statistical learning as an individual ability: Theoretical perspectives and empirical evidence</article-title>. <source>Journal of Memory and Language</source>, <volume>81</volume>, <fpage>105</fpage>–<lpage>120</lpage>. <pub-id pub-id-type="doi">10.1016/j.jml.2015.02.001</pub-id></mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>Z. M.</given-names></string-name>, <string-name><surname>Delgutte</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Oxenham</surname>, <given-names>A. J</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title>. <source>Nature</source>, <volume>416</volume>(<issue>6876</issue>), Article <elocation-id>6876</elocation-id>. <pub-id pub-id-type="doi">10.1038/416087a</pub-id></mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Stephanie N</given-names> <surname>Lovich</surname></string-name>, <string-name><given-names>Cynthia D</given-names> <surname>King</surname></string-name>, <string-name><given-names>David LK</given-names> <surname>Murphy</surname></string-name>, <string-name><given-names>Rachel</given-names> <surname>Landrum</surname></string-name>, <string-name><given-names>Christopher A</given-names> <surname>Shera</surname></string-name>, &amp; <string-name><given-names>Jennifer M</given-names> <surname>Groh</surname></string-name></person-group>. (<year>2022</year>). <article-title>Parametric information about eye movements is sent to the ears</article-title>. <source>BioRxiv</source>, <fpage>2022.11.27.518089</fpage>. <pub-id pub-id-type="doi">10.1101/2022.11.27.518089</pub-id></mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Expectation in perceptual decision making: Neural and computational mechanisms</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>15</volume>(<issue>11</issue>), Article 11. <pub-id pub-id-type="doi">10.1038/nrn3838</pub-id></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Treder</surname>, <given-names>M. S</given-names></string-name></person-group>. (<year>2020</year>). <article-title>MVPA-Light: A Classification and Regression Toolbox for Multi-Dimensional Data</article-title>. <source>Frontiers in Neuroscience</source>, <volume>14</volume>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fnins.2020.00289">https://www.frontiersin.org/articles/10.3389/fnins.2020.00289</ext-link></mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Veen</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>Van Drongelen</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Yuchtman</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Suzuki</surname>, <given-names>A.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title>. <source>IEEE Transactions on biomedical engineering</source>, <volume>44</volume>(<issue>9</issue>), <fpage>867</fpage>–<lpage>880</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weissbart</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kandylaki</surname>, <given-names>K. D.</given-names></string-name>, &amp; <string-name><surname>Reichenbach</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Cortical Tracking of Surprisal during Continuous Speech Comprehension</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>32</volume>(<issue>1</issue>), <fpage>155</fpage>–<lpage>166</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01467</pub-id></mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ying</surname>, <given-names>X</given-names></string-name></person-group>. (<year>2019</year>). <article-title>An Overview of Overfitting and its Solutions</article-title>. <source>Journal of Physics: Conference Series</source>, <volume>1168</volume>, <fpage>022022</fpage>. <pub-id pub-id-type="doi">10.1088/1742-6596/1168/2/022022</pub-id></mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yon</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lange</surname>, <given-names>F. P. de</given-names></string-name>, &amp; <string-name><surname>Press</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2019</year>). <article-title>The Predictive Brain as a Stubborn Scientist</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>23</volume>(<issue>1</issue>), <fpage>6</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2018.10.003</pub-id></mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zion Golumbic</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Bickel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Schevon</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>McKhann</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Goodman</surname>, <given-names>R. R.</given-names></string-name>, <string-name><surname>Emerson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name>, <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Mechanisms Underlying Selective Neuronal Tracking of Attended Speech at a “Cocktail Party.”</article-title> <source>Neuron</source>, <volume>77</volume>(<issue>5</issue>), <fpage>980</fpage>–<lpage>991</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.037</pub-id></mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zion-Golumbic</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Schroeder</surname>, <given-names>C. E</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Attention modulates ‘speech-tracking’ at a cocktail party</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>16</volume>(<issue>7</issue>), <fpage>363</fpage>–<lpage>364</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2012.05.004</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101262.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bottini</surname>
<given-names>Roberto</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Trento</institution>
</institution-wrap>
<city>Trento</city>
<country>Italy</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>These are <bold>valuable</bold> findings for those interested in how neural signals reflect auditory speech streams, and in understanding the roles of prediction, attention, and eye movements in this tracking. However, the evidence as it stands is <bold>incomplete</bold>. Further analyses are needed to clarify how the observed results relate to the relevant theoretical claims.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101262.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study aimed at replicating two previous findings that showed (1) a link between prediction tendencies and neural speech tracking, and (2) that eye movements track speech. The main findings were replicated which supports the robustness of these results. The authors also investigated interactions between prediction tendencies and ocular speech tracking, but the data did not reveal clear relationships. The authors propose a framework that integrates the findings of the study and proposes how eye movements and prediction tendencies shape perception.</p>
<p>Strengths:</p>
<p>This is a well-written paper that addresses interesting research questions, bringing together two subfields that are usually studied in separation: auditory speech and eye movements. The authors aimed at replicating findings from two of their previous studies, which was overall successful and speaks for the robustness of the findings. The overall approach is convincing, methods and analyses appear to be thorough, and results are compelling.</p>
<p>Weaknesses:</p>
<p>Eye movement behavior could have presented in more detail and the authors could have attempted to understand whether there is a particular component in eye movement behavior (e.g., blinks, microsaccades) that drives the observed effects.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101262.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>Schubert et al. recorded MEG and eye tracking activity while participants were listening to stories in single-speaker or multi-speaker speech. In a separate task, MEG was recorded while the same participants were listening to four types of pure tones in either structured (75% predictable) or random (25%) sequences. The MEG data from this task was used to quantify individual 'prediction tendency': the amount by which the neural signal is modulated by whether or not a repeated tone was (un)predictable, given the context. In a replication of earlier work, this prediction tendency was found to correlate with 'neural speech tracking' during the main task. Neural speech tracking is quantified as the multivariate relationship between MEG activity and speech amplitude envelope. Prediction tendency did not correlate with 'ocular speech tracking' during the main task. Neural speech tracking was further modulated by local semantic violations in the speech material and by whether or not a distracting speaker was present. The authors suggest that part of the neural speech tracking is mediated by ocular speech tracking. Story comprehension was negatively related with ocular speech tracking.</p>
<p>Strengths</p>
<p>This is an ambitious study, and the authors' attempt to integrate the many reported findings related to prediction and attention in one framework is laudable. The data acquisition and analyses appear to be done with great attention to methodological detail. Furthermore, the experimental paradigm used is more naturalistic than was previously done in similar setups (i.e.: stories instead of sentences).</p>
<p>Weaknesses</p>
<p>While the analysis pipeline is outlined in much detail, some analysis choices appear ad-hoc and could have been more uniform and/or better motivated (other than: this is what was done before).</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101262.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>I thank the authors for their extensive revision of this paper, and I found some elements greatly improved.</p>
<p>
In particular, the authors do embrace a somewhat more speculative tone in the current version, which I think is fitting for this work, as the data seem (to me) to be not fully conclusive. The data set collected here is clearly valuable and unique (and I would encourage the authors to make it publicly available!), however, my overall impression is that the specific analyses reported here might not fully</p>
<p>Despite the revised description of methods, results and figures, I still have trouble understanding many of the results and the authors conclusive interpretation of them. These are my main reservations:</p>
<p>(1) Regarding &quot;individual prediction tendency&quot; - thank you for adding clarifying methodological details and showing the data in a new Figure (#2). Honestly, however, I still can't say that I fully understand the result. For example, why is there also a significant response in the random condition as well? And how do you interpret the interesting time-course (with a peak ~200ms prior to the stimulus, and a reduction overtime from there?</p>
<p>
Also (I may have missed this, but..) what neural data was used to train the classifier and derive the &quot;prediction tendency&quot; index? Was it just the broadband neural response? Is there a way to know which sensors contributed to this metric (e.g., are they predominantly auditory? Frontal?)? And is there a way to establish the statistical significance of this metric (e.g., how good the decoder actually was in predicting behavioral sensitivity?). I don't see any statistics in the results section describing the individual prediction tendency.</p>
<p>(2) Regarding the TRF analysis - Thanks for clarifying the approach used to obtain 2-second long &quot;segments&quot; of speech tracking. This is an interesting approach, however I think quite new(?) , and for me it raises a whole new set of questions, as well as additional controls and data that I would have liked to see, to be convinced that results are significant. I will elaborate:</p>
<p>- Do I understand correctly that you segment the real and predicted neural response into 2-second long segments and then calculate the Pearsons' correlation between them to assess the goodness of the model? This is very unclear, since in the methods section you state only that &quot;the same&quot; analysis was performed as for the full data - but what exactly? Clearly, values will be very different when using such short segments. I feel that additional details are still required (and perhaps data shown) to fully understand the &quot;semantic violation&quot; analysis of TRFs.</p>
<p>- I would like to reiterate my previous comment regarding the use of permutation tests to verify the validity of TRF-based measures derived. This would be especially important when using new approaches (such as the segmentation used here). The authors argue that this is not needed since this was not done in their previously published study. However, this sounds a bit like &quot;two wrongs make a right&quot; argument... why not just do it, and let us know that this 2-second segmentation approach allows estimating reliable speech tracking?</p>
<p>- Following up on my previous comment that defining &quot;clusters&quot; as at least two neighboring channels (Figure 3) - the fact that this is a default in Fieldtrip is by no means sufficient justification!. This seems quite liberal to me, especially given the many comparisons performed. Here too, permutations can help to determine the necessary data-driven threshold for corrections. This is of course critical for interpreting the result shown in Figures 3E&amp;G that are critical &quot;take home messages&quot; of the paper - i.e., that the prediction-index from the first part of the experiment is related to speech tracking in the second part of the experiment. To my eyes, this does not look extremely convincing, but perhaps the authors can show more conclusive data to support this (e.g., scatter plots of the betas across participant?).</p>
<p>
- A similar point can be made for the effect of semantic violations (though here the scalp-level result is somewhat more clustered). The authors point out that the semantic effect is a &quot;replication&quot; of their result reported in Schubert et al. 2023, but if I am not mistaken the results there were somewhat different (as was the manipulation). It would be nice to explicitly discuss the similarity/difference between these effects.</p>
<p>(3) Regarding the ocular-TRFs -</p>
<p>- Maybe this is just me, but I believe that effects that are robust should be clearly visible in the data, without the need for fancy &quot;black-box&quot; statistical models. In the case of the ocular TRFs, it is hard for me to see how these time-courses are not just noise (and, again, a permutation test would have helped to convince me..). The inconsistent results for horizontal and vertical eye-movements vis a vis the experimental conditions (single vs. multi-speaker conditions) don't help either, despite the authors argument that these are &quot;independent&quot; - but why should this be the case, especially if there is nothing really to look at in this task?</p>
<p>
- I remain with this scepticism for the mediation-portion of the analysis as well... But perhaps replications from other groups or making the data public will help shed further light on this in the future.</p>
<p>Minor</p>
<p>
- Thanks for adding information about the creation of semantic-violation stimuli. Since the violations and lexical-controls were taken from different audio recordings, it would have been nice to verify that differences between neural responses cannot be attributed to differences in articulations (e.g., by comparing their spectro-temporal properties).</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101262.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Schubert</surname>
<given-names>Juliane</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2536-6522</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Gehmacher</surname>
<given-names>Quirin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9407-2763</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Schmidt</surname>
<given-names>Fabian</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9839-1614</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Hartmann</surname>
<given-names>Thomas</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8298-8125</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Weisz</surname>
<given-names>Nathan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7816-0037</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>This study aimed at replicating two previous findings that showed (1) a link between prediction tendencies and neural speech tracking, and (2) that eye movements track speech. The main findings were replicated which supports the robustness of these results. The authors also investigated interactions between prediction tendencies and ocular speech tracking, but the data did not reveal clear relationships. The authors propose a framework that integrates the findings of the study and proposes how eye movements and prediction tendencies shape perception.</p>
<p>Strengths:</p>
<p>This is a well-written paper that addresses interesting research questions, bringing together two subfields that are usually studied in separation: auditory speech and eye movements. The authors aimed at replicating findings from two of their previous studies, which was overall successful and speaks for the robustness of the findings. The overall approach is convincing, methods and analyses appear to be thorough, and results are compelling.</p>
<p>Weaknesses:</p>
<p>Linking the new to the previous studies could have been done in more detail, and the extent to which results were replicated could have been discussed more thoroughly.</p>
<p>Eye movement behavior could have been presented in more detail and the authors could have attempted to understand whether there is a particular component in eye movement behavior (e.g., microsaccades) that drives the observed effects.</p>
</disp-quote>
<p>We would like to thank you for your time and effort in reviewing our work and we appreciate the positive comments!</p>
<p>We extended our manuscript, now providing intermediate results on individual prediction tendency, which can be compared to our results from Schubert et al., (2023).</p>
<p>Furthermore, we expanded our discussion now detailing the extent to which our results (do not) replicate the previous findings (e.g. differences in horizontal vs. vertical ocular speech tracking, lack of distractor tracking, link between ocular speech tracking and behavioral outcomes).</p>
<p>While we agree with the reviewer that it is an important and most interesting question, to what extent individual features of gaze behavior (such as microsaccades, blinks etc.) contribute to the ocular speech tracking effect, it is beyond the scope of the current manuscript. It will be methodologically and conceptually challenging to distinguish these features from one another and to relate them to diverse cognitive processes. We believe that a separate manuscript is needed to give these difficult questions sufficient space for new methodological approaches and control analyses. The primary goal of this manuscript was to replicate the findings of Gehmacher et al. (2024) using similar methods and to relate them to prediction tendencies, attention, and neural speech tracking.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary</p>
<p>Schubert et al. recorded MEG and eye-tracking activity while participants were listening to stories in single-speaker or multi-speaker speech. In a separate task, MEG was recorded while the same participants were listening to four types of pure tones in either structured (75% predictable) or random (25%) sequences. The MEG data from this task was used to quantify individual 'prediction tendency': the amount by which the neural signal is modulated by whether or not a repeated tone was (un)predictable, given the context. In a replication of earlier work, this prediction tendency was found to correlate with 'neural speech tracking' during the main task. Neural speech tracking is quantified as the multivariate relationship between MEG activity and speech amplitude envelope. Prediction tendency did not correlate with 'ocular speech tracking' during the main task. Neural speech tracking was further modulated by local semantic violations in the speech material, and by whether or not a distracting speaker was present. The authors suggest that part of the neural speech tracking is mediated by ocular speech tracking. Story comprehension was negatively related to ocular speech tracking.</p>
<p>Strengths</p>
<p>This is an ambitious study, and the authors' attempt to integrate the many reported findings related to prediction and attention in one framework is laudable. The data acquisition and analyses appear to be done with great attention to methodological detail (perhaps even with too much focus on detail-see below). Furthermore, the experimental paradigm used is more naturalistic than was previously done in similar setups (i.e. stories instead of sentences).</p>
<p>Weaknesses</p>
<p>For many of the key variables and analysis choices (e.g. neural/ocular speech tracking, prediction tendency, mediation) it is not directly clear how these relate to the theoretical entities under study, and why they were quantified in this particular way. Relatedly, while the analysis pipeline is outlined in much detail, an overarching rationale and important intermediate results are often missing, which makes it difficult to judge the strength of the evidence presented. Furthermore, some analysis choices appear rather ad-hoc and should be made uniform and/or better motivated.</p>
</disp-quote>
<p>We would like to thank you very much for supporting our paper and your thoughtful feedback!</p>
<p>To address your concerns, that our theoretical entities as well as some of our analytical choices lack transparency, we expanded our manuscript in several ways:</p>
<p>(1) We now provide the intermediate results of our prediction tendency analysis (see new Figure 2 of our manuscript). These results are comparable to our findings from Schubert et al. (2023), demonstrating that on a group level there is a tendency to pre-activate auditory stimuli of high probability and illustrating the distribution of this tendency value in our subject population.</p>
<p>(2) We expanded our methods section in order to explain our analytical choices (e.g. why this particular entropy modulation paradigm was used to measure individual prediction tendency).</p>
<p>(3) We now provide an operationalisation of the terms “neural speech tracking” and “ocular speech tracking” at their first mention, to make these metrics more transparent to the reader.</p>
<p>(4) We are summarizing important methodological information ahead of each results section, in order to provide the reader with a comprehensible background, without the necessity to read through the detailed methods section.</p>
<p>(5) We expanded our discussion section, with a special emphasis on relating the key variables of the current investigation to theoretical entities.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>Summary:</p>
<p>In this paper, the authors measured neural activity (using MEG) and eye gaze while individuals listened to speech from either one or two speakers, which sometimes contained semantic incongruencies.</p>
<p>The stated aim is to replicate two previous findings by this group: (1) that there is &quot;ocular speech tracking&quot; (that eye-movements track the audio of the speech), (2) that individual differences in neural response to tones that are predictable vs. not-predictable in their pitch is linked to neural response to speech. In addition, here they try to link the above two effects to each other, and to link &quot;attention, prediction, and active sensing&quot;.</p>
<p>Strengths:</p>
<p>This is an ambitious project, that tackles an important issue and combines different sources of data (neural data, eye-movements, individual differences in another task) in order to obtain a comprehensive &quot;model&quot; of the involvement of eye-movements in sensory processing.</p>
<p>The authors use many adequate methods and sophisticated data-analysis tools (including MEG source analysis and multivariate statistical models) in order to achieve this.</p>
<p>Weaknesses:</p>
<p>Although I sympathize with the goal of the paper and agree that this is an interesting and important theoretical avenue to pursue, I am unfortunately not convinced by the results and find that many of the claims are very weakly substantiated in the actual data.</p>
<p>Since most of the analyses presented here are derivations of statistical models and very little actual data is presented, I found it very difficult to assess the reliability and validity of the results, as they currently stand. I would be happy to see a thoroughly revised version, where much more of the data is presented, as well as control analyses and rigorous and well-documented statistical testing (including addressing multiple comparisons).</p>
</disp-quote>
<p>We thank you for your thoughtful feedback. We appreciate your concerns and will address them below in greater detail.</p>
<disp-quote content-type="editor-comment">
<p>These are the main points of concern that I have regarding the paper, in its current format.</p>
<p>(1) Prediction tendencies - assessed by listening to sequences of rhythmic tones, where the pitch was either &quot;predictable&quot; (i.e., followed a fixed pattern, with 25% repetition) or &quot;unpredictable&quot; (no particular order to the sounds). This is a very specific type of prediction, which is a general term that can operate along many different dimensions. Why was this specific design selected? Is there theoretical reason to believe that this type of prediction is also relevant to &quot;semantic&quot; predictions or other predictive aspects of speech processing?</p>
</disp-quote>
<p>Theoretical assumptions and limitations of our quantification of individual prediction tendency are now shortly summarized in the first paragraph of our discussion section. With this paradigm we focus on anticipatory “top-down” predictions, whilst controlling for possibly confounding “bottom-up” processes. Since this study aimed to replicated our previous work we chose the same entropy-modulation paradigm as in other studies from our group (e.g. Demarchi et al. 2019, Schubert et al. 2023;2024, Reisinger et al. 2024), which has proven to give reproducible findings of feature-specific preactivations of sounds in a context of low entropy. One advantage of this design is that it gives us the opportunity to directly compare the processing of “predictable” and “unpredictable” sounds of the same frequency in a time-resolved manner (this argument is now also included in the Methods section).</p>
<p>Regarding the question to what extent this type of prediction might also be relevant to “semantic” predictions we would like to refer to our previous study (Schubert et al., 2023), where we explicitly looked at the interaction between individual prediction tendency and encoding of semantic violations in the cortex. (In short, there we found a spatially dissociable interaction effect, indicating an increased encoding of semantic violations that scales with prediction tendency in the left hemisphere, as well as a disrupted encoding of semantic violations for individuals with stronger prediction tendency in the right hemisphere.) We did not aim to replicate all our findings in the current study, but instead we focused on merging the most important results from two independent phenomena in the domain of speech processing and bringing them into a common framework. However, as now stated in our discussion, we believe that “predictions are directly linked to the interpretation of sensory information. This interpretation is likely to occur at different levels along the cognitive (and anatomical) hierarchy…” and that “this type of prediction is relevant for acoustic processing such as speech and music, whose predictability unfolds over time.”</p>
<disp-quote content-type="editor-comment">
<p>(2) On the same point - I was disappointed that the results of &quot;prediction tendencies&quot; were not reported in full, but only used later on to assess correlations with other metrics. Even though this is a &quot;replication&quot; of previous work, one would like to fully understand the results from this independent study. On that note, I would also appreciate a more detailed explanation of the method used to derive the &quot;prediction tendency&quot; metric (e.g, what portion of the MEG signal is used? Why use a pre-stimulus and not a post-stimulus time window? How is the response affected by the 3Hz steady-state response that it is riding on? How are signals integrated across channels? Can we get a sense of what this &quot;tendency&quot; looks like in the actual neural signal, rather than just a single number derived per participant (an illustration is provided in Figure 1, but it would be nice to see the actual data)? How is this measure verified statistically? What is its distribution across the sample? Ideally, we would want enough information for others to be able to replicate this finding).</p>
</disp-quote>
<p>We now included a new figure (similar to Schubert et al. 2023) showing the interim results of the “prediction tendency” effect as well as individual prediction tendency values of all subjects.</p>
<p>Furthermore we expanded the description of the “prediction tendency” metric in the Methods section, where we explain our analytical choices in more detail. In particular we used a pre-stimulus time window in order to capture “anticipatory predictions”. The temporally predictably design gives us the opportunity to capture this type of predictions. The integration across channels is handled by the multivariate pattern analysis (MVPA), which inherently integrates multidimensional data (as mentioned in the methods section we used data from 102 magnetometers) and links it to (in this case) categorical information.</p>
<disp-quote content-type="editor-comment">
<p>(3) Semantic violations - half the nouns ending sentences were replaced to create incongruent endings. Can you provide more detail about this - e.g., how were the words selected? How were the recordings matched (e.g., could they be detected due to audio editing?)? What are the &quot;lexically identical controls that are mentioned&quot;? Also, is there any behavioral data to know how this affected listeners? Having so many incongruent sentences might be annoying/change the nature of listening. Were they told in advance about these?</p>
</disp-quote>
<p>We expanded the Methods section and included the missing information:</p>
<p>“We randomly selected half of the nouns that ended a sentence (N = 79) and replaced them with the other half to induce unexpected semantic violations. The swap of nouns happened in the written script before the audio material was recorded in order to avoid any effects of audio clipping. Narrators were aware of the semantic violations and had been instructed to read out the words as normal. Consequently all target words occurred twice in the text, once in a natural context (serving as lexical controls) and once in a mismatched context (serving as semantic violations) within each trial, resulting in two sets of lexically identical words that differed greatly in their contextual probabilities (see Figure 1F for an example). Participants were unaware of these semantic violations.” Since we only replaced 79 words with semantic violations in a total of ~ 24 minutes of audio material we believe that natural listening was not impaired. In fact none of the participants mentioned to have noticed the semantic violations during debriefing (even though they had an effect on speech tracking in the brain).</p>
<disp-quote content-type="editor-comment">
<p>(4) TRF in multi-speaker condition: was a univariate or multivariate model used? Since the single-speaker condition only contains one speech stimulus - can we know if univariate and multivariate models are directly comparable (in terms of variance explained)? Was any comparison to permutations done for this analysis to assess noise/chance levels?</p>
</disp-quote>
<p>For mTRF models it depends on the direction (“encoding” vs. “decoding”) whether or not the model is comparable to a univariate model. In our case of an encoding model the TRFs are fitted to each MEG channel independently. This gives us the possibility to explore the effect over different areas (whereas a multivariate “decoding” model would result in only one speech reconstruction value).</p>
<p>In both conditions (single and multi speaker) a single input feature (the envelope of the attended speech stream) was used. Of course it would be possible to fit the model to use a multivariate encoding model, predicting the brain’s response to the total input of sounds. This would, however, target a slightly different question than ours as we aimed to investigate how much of the attended speech is tracked.</p>
<p>Regarding your suggestion of a comparison to permutations to assess noise levels we would like to point out that we chose the same methodological approach as in our previous studies, that we aimed to replicate here. Indeed in these original studies no permuted versions (with exception of the mediation analysis where comparing a model with an additional input predictor to a single predictor model would not result in a fair comparison) have been used. We conducted the mTRF approach considering the guidelines of Crosse et al. (2016) to the best of our knowledge and in accordance with similar studies in this field.</p>
<p>Crosse, M. J., Di Liberto, G. M., Bednar, A., &amp; Lalor, E. C. (2016). The multivariate temporal response function (mTRF) toolbox: a MATLAB toolbox for relating neural signals to continuous stimuli. Frontiers in human neuroscience, 10, 604.</p>
<disp-quote content-type="editor-comment">
<p>(5) TRF analysis at the word level: from my experience, 2-second segments are insufficient for deriving meaningful TRFs (see for example the recent work by Mesik &amp; Wojtczak). Can you please give further details about how the analysis of the response to semantic violations was conducted? What was the model trained on (the full speech or just the 2-second long segments?) Is there a particular advantage to TRFs here, relative - say - to ERPs (one would expect a relatively nice N400 response, not)? In general, it would be nice to see the TRF results on their own (and not just the modulation effects).</p>
</disp-quote>
<p>We fully agree with the reviewers statement that 2-second segments would have been too short to derive meaningful TRFs. To investigate the effect of semantic violations, we used the same TRFs trained on the whole dataset (with 4-fold cross validation). The resulting true as well as the predicted data was segmented into single word epochs of 2 seconds. We selected semantic violations as well as their lexically identical controls and correlated true with predicted responses for every word. Thus, we conducted the same analysis as for the overall encoding effect, focusing on only part of the data. We have reformulated the Methods section accordingly to clear up this misunderstanding. Since the TRFs are identical to the standard TRFs from the overall neural speech tracking, they are not informative to the semantic violation effect. However, since the mTRF approach is the key method throughout the manuscript (and our main focus is not on the investigations of brain responses to semantic violations) we have favoured this approach over the classical ERF analysis.</p>
<disp-quote content-type="editor-comment">
<p>(6) Another related point that I did not quite understand - is the dependent measure used for the regression model &quot;neural speech envelope tracking&quot; the r-value derived just from the 2sec-long epochs? Or from the entire speech stimulus? The text mentions the &quot;effect of neural speech tracking&quot; - but it's not clear if this refers to the single-speaker vs. twospeaker conditions or to the prediction manipulation. Or is it different in the different analyses? Please spell out exactly what metric was used in each analysis.</p>
</disp-quote>
<p>As suggested we now provide a clear definition of each dependent metric for each analysis.</p>
<p>“Neural speech tracking” refers to the correlation coefficients between predicted and true brain responses from the aforementioned encoding model, trained and tested on the whole audio material within condition (single vs. multi-speaker).</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewing Editor Comments:</bold></p>
<p>The reviewers have provided a number of recommendations to improve the manuscript, particularly requesting that more data be reported, with an emphasis on the measurements themselves (eye movements and TRFs) rather than just the numerical outputs of mathematical models.</p>
</disp-quote>
<p>We appreciate all the reviewers' and editor’s comments and effort to improve our manuscript. In the revised version we provide interim findings and missing data, updated figures that include an intuitive illustration of the metrics (such as TRFs), and a thoroughly revised discussion section where we focus on the relationship between our observed quantities and theoretical entities. We now offer operationalized definitions of the relevant concepts (“prediction tendency”, “active ocular sensing” and “selective attention”) and suggest how these entities might be related in the context of speech processing, based on the current findings. We are confident that this revision has improved the quality of our paper a lot and we are grateful for all the feedback and suggestions.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations for the authors):</bold></p>
<p>(1) Participants had to fixate throughout the tasks. How did the authors deal with large eye movements that violated the instructed fixation?</p>
</disp-quote>
<p>As described in the Methods section: “Participants were instructed to look at a black fixation cross at the center of a grey screen.” This instruction was not intended to enforce strict fixation but rather to provide a general reference point, encouraging participants to keep their gaze on the grey screen and avoid freely scanning the room or closing their eyes. Unlike trial-based designs, where strict fixation is feasible due to shorter trial durations, this approach did not impose rigid fixation requirements. Consequently, the threshold for &quot;instruction violation&quot; was inherently more flexible, and no additional preprocessing was applied to the gaze vectors.</p>
<disp-quote content-type="editor-comment">
<p>Fixating for such an extended period of time (1.5 hours?) is hard. Did fixation behavior change over time? Could (fixation) fatigue affect the correlations between eye movements and speech tracking? For example, fatigued participants had to correct their fixation more often and this drives, in part, the negative correlation with comprehension?</p>
</disp-quote>
<p>Yes, participants spent approximately 2 hours in the MEG, including preparation time (~30 minutes). However, participants were given opportunities to rest their eyes between different parts and blocks of the experiment (e.g., resting state, passive listening, and audiobook blocks), which should help mitigate fatigue to some extent.</p>
<p>That said, we agree that it is an intriguing idea that fatigue could drive the ocular speech tracking effect, with participants potentially needing to correct their gaze more as the experiment progresses. However, our analysis suggests this is unlikely for several reasons:</p>
<p>(1) Cross-validation in encoding models: Ocular speech tracking effects were calculated using a 4-fold cross-validation approach (this detail has now been added to the Methods section; please see our response to public review #3). This approach reduces the influence of potential increases in gaze corrections over time, as the models are trained and validated on independent data splits.  Moreover, if there were substantial differences in underlying response magnitudes between folds - for instance, between the first and fourth fold - this would likely compromise the TRF's ability to produce valid response functions for predicting the left-out data. Such a scenario would not result in significant tracking, further supporting the robustness of the observed effects.</p>
<p>(2) TRF time-course stability: If fatigue were driving increased gaze corrections, we would expect this to be reflected in a general offset (capturing the mean difference between folds) in the TRF time-courses shown in Figure 4 (right panel). However, no such trend / offset is evident.</p>
<p>(3) Comparison of eye movement data: To directly investigate this possibility, we compared the amount of total eye movements between the first and last blocks for both the single and multi-speaker conditions. Total movement was calculated by first calculating the differences in pixel values between consecutive eye positions on both the x- and y-axes. The Euclidean distance was then computed for each difference, providing a measure of movement between successive time points. Summing these distances yielded the total movement for each block. Statistical analysis was performed separately for the single speaker (ASS) and multi-speaker (AMS) conditions. For each condition, paired comparisons were made between the first and last blocks (we resorted to non-parametric tests, if assumptions of normality were violated):</p>
<p>For the single speaker condition (ASS), the normality assumption was not satisfied (p≤0.05p, Kolmogorov-Smirnov test). Consequently, a Wilcoxon signedrank test was conducted, which revealed no significant difference in total movements between the first and last blocks (z=−1.330, p=0.184). For the multi-speaker condition (AMS), the data met the normality assumption (p&gt;0.05), allowing the use of a paired t-test. The results showed no significant difference in total movements between the first and last blocks (t=−0.184, p=0.855).</p>
<p>The results are visualized in a bar plot (see below), where individual data points are displayed alongside the mean and standard error for each block. Statistical annotations indicate that neither condition demonstrated significant differences between the blocks. These findings suggest that total eye movements remained stable across the experimental conditions, regardless of whether participants were exposed to a single or multiple speakers.</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-101262-sa4-fig1.jpg" mimetype="image"/>
</fig>
<p>(4) Behavioral responses: Participants’ behavioral responses did not indicate any decrease in comprehensibility for later blocks compared to earlier ones. Specifically, a comparison of comprehension scores between the first and last blocks revealed no significant difference in either the single-speaker condition (ASS; Wilcoxon signed-rank test Z=−0.5911, p=0.5545) or the multi-speaker condition (AMS; Wilcoxon signed-rank test: Z=0.5018, p=0.6158). These findings suggest that participants maintained consistent levels of comprehension throughout the experiment, regardless of the condition or block order. The results are visualized in a bar plot (see below), where individual data points are displayed alongside the mean and standard error for each block. Statistical annotations indicate that neither condition demonstrated significant differences between the blocks.</p>
<fig id="sa4fig2">
<label>Author response image 2.</label>
<graphic mime-subtype="jpg" xlink:href="elife-101262-sa4-fig2.jpg" mimetype="image"/>
</fig>
<p>Together, these factors suggest that fatigue is unlikely to be a significant driver of the ocular speech tracking effects observed in this study.</p>
<disp-quote content-type="editor-comment">
<p>(2) The authors should provide descriptive statistics of fixation behavior /fixational eye movements. What was the frequency and mean direction of microsaccades, do they follow the main sequence, etc., quantify drift and tremor?</p>
</disp-quote>
<p>Thank you for their suggestion regarding descriptive statistics. To address this, we computed the rates of microsaccades (which were extracted using the microsaccade detection algorithm as proposed in Liu, B., Nobre, A. C. &amp; van Ede, F. Functional but not obligatory link between microsaccades and neural modulation by covert spatial attention. Nat. Commun. 13, 3503 (2022)) and fixations as these metrics are directly relevant to our study and the requests above.</p>
<p>Microsaccade Rates:</p>
<p>- Single speaker Condition: Mean = 2.306 Hz, SD = 0.363 Hz. ○ Multi speaker: Mean = 2.268 Hz, SD = 0.355 Hz.</p>
<p>Fixation Rates:</p>
<p>- Single speaker Condition: Mean = 2.858 Hz, SD = 1.617 Hz. ○ Multi speaker Condition: Mean = 2.897 Hz, SD = 1.542 Hz.</p>
<p>These values fall within the expected ranges reported in the literature (fixation rates: 2– 4 Hz, microsaccade rates: ~0.5–2.5 Hz) and serve as a sanity check, confirming the plausibility of our eye-tracking data. Regarding the reviewer’s request for additional metrics (e.g., microsaccade direction, main sequence analysis, drift, and tremor), extracting these features would require advanced algorithms and analyses not supported by our current preprocessing pipeline or dataset. We hope that the provided metrics, which were the main focus of this study, serve as a sufficient sanity check and highlight the robustness of our data.</p>
<disp-quote content-type="editor-comment">
<p>Related to this, I am wondering whether microsaccades are the feature that drives speech tracking.</p>
</disp-quote>
<p>This is an important and pressing question that we aim to address in future publications. Currently, our understanding - and the reason microsaccades and blinks are not analysed in this manuscript - is limited by methodological constraints. Specifically, microsaccades are binary response vectors, which are not compatible with TRF analyses. Addressing this would require adapting future models to handle timecontinuous binary response data or exploring alternative approaches, such as regression-based ERFs (for example as in Heilbron et al. 2022). As the primary goal of this manuscript was to replicate the findings of Gehmacher et al. (2024) using similar methods and to integrate these findings into an initial unified framework, we did not investigate additional eye movement features here. However, we agree that microsaccades (and also blinks, see below) likely contribute, at least in part, to the observed ocular speech tracking effects, and we now suggest this in the Discussion:</p>
<p>“Relatedly, it remains an open question whether microsaccades are a key feature driving ocular speech tracking. However, our current study does not analyze microsaccades due to methodological constraints: microsaccades are binary response vectors, which are incompatible with TRF analyses used here. Addressing this would require adapting models to handle time-continuous binary response data or potentially exploring alternative approaches, such as regression-based ERFs (e.g., as in Heilbron et al., 2022). While these limitations preclude microsaccade analysis in the current study, we hypothesize that they could enhance temporal precision and selectively amplify relevant sensory input, supporting auditory perception. Future studies should explore this possibility to uncover the specific contributions of microsaccades to speech tracking.”</p>
<disp-quote content-type="editor-comment">
<p>(3) Can the authors make sure that interpolated blinks did not drive any of the effects? Can interpolated blink trials be excluded?</p>
</disp-quote>
<p>Using continuous audiobooks as stimuli meant that we could not exclude blink periods from the analysis without introducing substantial continuation artifacts in the TRF analysis. Importantly, the concept of covert motor routines and active sensing suggests that participants engage more strongly in motor routines - including ocular behaviors such as microsaccades and blinks - during tasks like speech tracking. These motor routines are inherently tied to individual gaze patterns, making microsaccades and blinks correlated with other ocular behaviors. This complicates efforts to disentangle their individual contributions to the observed ocular speech tracking effects.</p>
<p>Engagement in these motor routines, as posited by active sensing, would naturally load onto various viewing behaviors, further intertwining their roles.</p>
<p>Even if we were to examine correlations, such as the amount of blinks with the ocular speech tracking effect, it is unlikely to provide a clearer understanding due to these inherent overlaps. The methodological and conceptual challenge lies in distinguishing these features from one another and understanding their respective roles in driving the observed effects.</p>
<p>However, the aim of this manuscript was not to dissect the ocular speech tracking effect in greater detail, but rather to relate it - based on similar analytical choices as in Gehmacher et al - to prediction tendencies, attention, and neural speech tracking. While it will be crucial in future work to differentiate these patterns and their connections to diverse cognitive processes, it is beyond the scope of this study to address all these questions comprehensively.</p>
<p>We acknowledge that eye movements, including microsaccades and blinks (however, see challenges for this in response 2), remain underexplored in many experimental paradigms. Their interplay with cognitive processes - such as attention, prediction, and sensory integration - will undoubtedly be an important focus for future studies.</p>
<disp-quote content-type="editor-comment">
<p>(4) Could the authors provide more details on how time shuffling was done for the eyemovement predictor, and include a circularly shifted version (or a version that does not destroy temporal contiguity) in their model comparisons? Some types of shuffling can result in unrealistic time series, which would end up in an unfair comparison with the model that has the real eye movement traces as predictors.</p>
</disp-quote>
<p>We thank the reviewer for their insightful question regarding the time-shuffling procedure for the eye-movement predictor and for suggesting the inclusion of a circularly shifted version in our model comparisons. Below, we provide further details about our approach and the rationale behind it:</p>
<p>(1) Random Shuffling: In our analysis, the eye-movement predictor was randomly shuffled over time, meaning that individual samples were randomly replaced. This method completely disrupts the temporal structure of the signal, providing a null model that directly tests whether the temporal mediation observed is due to the specific temporal relationship between ocular movements and envelope tracking.</p>
<p>(2) Circular Shifting: While circular shifting maintains temporal contiguity, it introduces certain challenges in the context of TRF analysis. Specifically:</p>
<p>- Adaptation to Shifts: The TRF model could adapt to the introduced shift, potentially reducing the validity of the null comparison.</p>
<p>- Similarity due to Repetition: The broadband envelope exhibits strong repetitive patterns over time, such as rhythms inherent to speech. Circular shifting can therefore produce predictors that are very similar to the original signal. As a result, this similarity may lead to null distributions that do not adequately disrupt the temporal mediation we aim to test, making it less robust as a control.</p>
<p>(3) Rationale for Random Shuffling: The primary goal of our mediation analysis is to determine whether there is a temporal mediation of envelope tracking by ocular movements. By deliberately destroying the temporal structure through random shuffling, we ensure that the null model tests for the specific temporal relationship that is central to our hypothesis. Circularly shifted predictors, on the other hand, may partially preserve temporal dependencies, making them less suitable for this purpose.</p>
<p>In summary, while circular shifting is a valuable approach in other contexts, it is less appropriate for the specific goals of this study. We hope this explanation clarifies our methodological choices and demonstrates their alignment with the aims of our analysis.</p>
<disp-quote content-type="editor-comment">
<p>(5) Replication: I want to point out that it is great that the previous findings were in principle replicated. However, I would like to suggest a more nuanced evaluation of the replication:</p>
<p>a) Instead of a (direct) replication, the present study should be called a 'conceptual replication', since modifications in design and procedure were made.</p>
</disp-quote>
<p>Thank you very much for this suggestion! We now use the term ‘conceptual replication’ throughout the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>b) Not all the findings from the Gehmacher et al., 2024 study were replicated to a full extent:</p>
<p>Did the authors find indications of a vertical vs. horizontal tracking difference in the Gehmacher 2024 data? Could they check this in the Gehmacher 2024 data?</p>
</disp-quote>
<p>The findings for horizontal and vertical gaze tracking in Gehmacher et al. (2024) are detailed in the supplementary material of that publication. Both single-speaker and multi-speaker target conditions showed significant speech tracking effects in both horizontal and vertical directions. However, there was a slightly stronger tracking effect for the single-speaker condition in the vertical direction. Due to the highly predictable structure of words in Gehmacher et al. effects here were probably overall boosted as compared to continuous audiobook listening, likely leading to the differentiation of horizontal and vertical gaze. See figures in Gehmacher et al. supplementary file for reference.</p>
<disp-quote content-type="editor-comment">
<p>c) Another difference between their previous and this study is the non-existent tracking of the multi-speaker distractor in this study. The authors should point this out clearly in the discussion and potentially provide an explanation.</p>
</disp-quote>
<p>Thank you for highlighting this point! We now address this in the discussion:</p>
<p>“Importantly, in contrast to Gehmacher et al. (2024), we did not observe ocular tracking of the multi-speaker distractor in this study. This difference is likely attributable to the simplistic single-trial, 5-word task structure in Gehmacher et al., which resulted in high temporal overlap between the target and distractor speech streams and likely drove the significant distractor-tracking effects observed in that study. The absence of such an effect during continuous listening in our study suggests that ocular tracking is indeed more specific to selective attention.”</p>
<disp-quote content-type="editor-comment">
<p>Minor:</p>
<p>(1) I was a little surprised to not see an indication of eyes/eye movements in Figure 6. The intention of the authors might have been to create a general schematic illustration, but I find this a bit misleading. This paper provides nice evidence for a specific ocular effect in speech tracking. There is, to my knowledge, no indication that speech would be influenced by different kinds of active sensing (if there are, please include them in the discussion). Given that the visuomotor system is quite dominant in humans, it might actually be the case that the speech tracking the authors describe is specifically ocular.</p>
</disp-quote>
<p>Taking into account all the reviewers' remarks on the findings and interpretations, we have updated this figure (now Fig. 7) in the manuscript to make it more specific and aligned with the revised discussion section. Throughout the manuscript, we now explicitly refer to active ocular sensing in relation to speech processing and have avoided the broader term 'active sensing' in this context. We hope these revisions address the concerns raised.</p>
<disp-quote content-type="editor-comment">
<p>(2) I find the part in the discussion (page 2, last paragraph) on cognitive processes hard to follow. I don't agree that 'cognitive processes' are easily separable from any of the measured responses (eye and brain). Referring to the example they provide, there is evidence that eye movements are correlated with brain activity that is correlated with memory performance. How, and more importantly, why would one separate those?</p>
</disp-quote>
<p>Thank you for raising this important point. We have carefully considered your comments, particularly regarding the interplay between cognitive processes and measured responses (eye and brain), as well as the challenge of conceptually separating them. Additionally, we have incorporated Reviewer #2's query (13) into a unified and complementary reasoning. In response, we have rewritten the relevant paragraph in the discussion to provide a clearer and more detailed explanation of how ocular and neural responses contribute to speech processing in an interdependent manner. We hope this revision addresses your concerns and offers a more precise and coherent discussion on this topic:</p>
<p>“Despite the finding that eye movements mediate neural speech tracking, the behavioural relevance for semantic comprehension appears to differ between ocular and neural speech tracking. Specifically, we found a negative association between ocular speech tracking and comprehension, indicating that participants with lower comprehension performance exhibited increased ocular speech tracking. Interestingly, no significant relationship was observed between neural tracking and comprehension.</p>
<p>In this context, the negative association between ocular tracking and comprehension might reflect individual differences in how participants allocate cognitive resources. Participants with lower comprehension may rely more heavily on attentional mechanisms to process acoustic features, as evidenced by increased ocular tracking. This reliance could represent a compensatory strategy when higher-order processes, such as semantic integration or memory retrieval, are less effective. Importantly, our comprehension questions (see Experimental Procedure) targeted a broad range of processes, including intelligibility and memory, suggesting that this relationship reflects a trade-off in resource allocation between low-level acoustic focus and integrative cognitive tasks.</p>
<p>Rather than separating eye and brain responses conceptually, our analysis highlights their complementary contributions. Eye movements may enhance neural processing by increasing sensitivity to acoustic properties of speech, while neural activity builds on this foundation to integrate information and support comprehension. Together, these systems form an interdependent mechanism, with eye and brain responses working in tandem to facilitate different aspects of speech processing.</p>
<p>This interpretation is consistent with the absence of a difference in ocular tracking for semantic violations (e.g., words with high surprisal versus lexically matched controls), reinforcing the view that ocular tracking primarily reflects attentional engagement with acoustic features rather than direct involvement in semantic processing. This aligns with previous findings that attention modulates auditory responses to acoustic features (e.g., Forte et al., 2017), further supporting the idea that ocular tracking reflects mechanisms of selective attention rather than representations of linguistic content.</p>
<p>Future research should investigate how these systems interact and explore how ocular tracking mediates neural responses to linguistic features, such as lexical or semantic processing, to better understand their joint contributions to comprehension.”.</p>
<disp-quote content-type="editor-comment">
<p>(3) Attention vs. predictive coding. I think the authors end up with an elegant description of the observed effects, &quot;as an &quot;active sensing&quot; mechanism that implements the attentional optimization of sensory precision.&quot; However, I feel the paragraph starts with the ill-posed question &quot;whether ocular speech tracking is modulated not by predictive, but other (for example attentional) processes&quot;. If ocular tracking is the implementation of a process (optimization of sensory precision, aka attention), how could it be at the same time modulated by that process? In my opinion, adding the notion that there is a modulation by a vague cognitive concept like attention on top of what the paper shows does not improve our understanding of how speech tracking in humans works.</p>
</disp-quote>
<p>Thank you for raising this point. We agree that it is critical to clarify the relationship between ocular speech tracking, attention, and predictive processes, and we appreciate the opportunity to refine this discussion.</p>
<p>To avoid the potential confusion that active ocular sensing represents on the one hand an implementation of selective attention on the other it seems to be modulated by it, we now use  the formulation “ocular speech tracking reflects attentional mechanisms rather than predictive processes.”</p>
<p>To address your concern that the conceptualization of attention seems rather vague, we have revised the whole paragraph in order to redefine the theoretical entities in question (including selective attention) and to provide a clearer and more precise picture (see also our revised version of Fig. 6, now Fig. 7). We now focus on highlighting the distinct yet interdependent roles of selective attention and individual prediction tendencies for speech tracking.:</p>
<p>“With this speculative framework we attempt to describe and relate three important phenomena with respect to their relevance for speech processing: 1) “Anticipatory predictions” that are created in absence of attentional demands and contain probabilistic information about stimulus features (here, inferred from frequency-specific pre-activations during passive listening to sound sequences). 2) “Selective attention” that allocates resources towards relevant (whilst suppressing distracting) information (which was manipulated by the presence or absence of a distractor speaker). And finally 3) “active ocular sensing”, which refers to gaze behavior that is temporally aligned to attended (but not unattended) acoustic speech input (inferred from the discovered phenomenon of ocular speech tracking). We propose that auditory inflow is, at a basic level, temporally modulated via active ocular sensing, which “opens the gates” in the sensory periphery at relevant timepoints. How exactly this mechanism is guided (for example where the information about crucial timepoints comes from, if not from prediction, and whether it requires habituation to a speechstream etc.) is yet unclear. Unlike predictive tendencies, active ocular sensing appears to reflect selective attention, manifesting as a mechanism that optimizes sensory precision. Individual differences with respect to anticipatory predictions on the other hand, seem to be independent from the other two entities, but nevertheless relevant for speech processing. We therefore support the notion that representational content is interpreted based on prior probabilistic assumptions. If we consider the idea that “a percept” of an (auditory) object is actually temporally and spatially distributed (across representational spacetime - see Fig. 7), the content of information depends on where and when it is probed (see for example Dennett, 1991 for similar ideas on consciousness). Having to select from multiple interpretations across space and time requires a careful balance between the weighting of internal models and the allocation of resources based on current goals. We suggest that in the case of speech processing, this challenge results in an independent adaptation of feature-based precision-weighting by predictions on the one hand and temporal precision-weighting by selective attention on the other.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>My main recommendation is outlined in the Weaknesses above: the overarching rationale for many analysis choices should be made explicit, and intermediate results should be shown where appropriate, so the reader can follow what is being quantified and what the results truly mean. Specifically, I recommend to pay attention to the following (in no particular order):</p>
<p>(1) Define 'neural speech tracking' early on. (e.g.: 'The amount of information in the MEG signal that can multivariately be explained by the speech amplitude envelope.' (is that correct?))</p>
</disp-quote>
<p>Thank you for pointing out that this important definition is missing. It is now defined at the first mention in the Introduction as follows: “Here (and in the following) “neural speech tracking” refers to a correlation coefficient between actual brain responses and responses predicted from an encoding model based solely on the speech envelope”.</p>
<disp-quote content-type="editor-comment">
<p>(2) Same for 'ocular speech tracking'. Here even reading the Methods does not make it unambiguous how this term is used.</p>
</disp-quote>
<p>It is now defined at the first mention in the Introduction as follows: “Ocular speech tracking” (similarly to “neural speech tracking” refers to the correlation coefficient between actual eye movements and movements predicted from an encoding model based on the speech envelope”.</p>
<p>In addition also define both (neural and ocular speech tracking) metrics in the Methods Section.</p>
<disp-quote content-type="editor-comment">
<p>(3) Related to this: for ocular speech tracking, are simply the horizontal and vertical eye traces compared to the speech envelope? If so, this appears somewhat strange: why should the eyes move more rightward/upward with a larger envelope? And the direction here depends on the (arbitrary) sign of right = positive, etc. (It would make more sense to quantify 'amount of movement' in some way, but if this is done, I missed it in Methods.)</p>
</disp-quote>
<p>Thank you for your insightful comments. You are correct that the horizontal and vertical traces were used for ocular speech tracking, and no additional details were included in the Methods. While we agree that the observed rightward/upward movement may seem unusual, this pattern is consistent with previous findings, including those reported in Gehmacher et al. (2024). In that study, we discussed how ocular speech tracking could reflect a broader engagement of the motor system during speech perception. For example, we observed a general right-lateralized gaze bias when participants attended to auditory speech, which we hypothesized might resemble eye movements during text reading, with a similar temporal alignment (~200 ms). We also speculated that this pattern might differ in cultures that read text from right to left.</p>
<p>We appreciate your suggestion to explore alternative methods for quantifying gaze patterns, such as the &quot;amount of movement&quot; or microsaccades. While these approaches hold promise for future studies, our primary aim here was to replicate previous findings using the same signal and analysis methods to establish a basis for further exploration.</p>
<disp-quote content-type="editor-comment">
<p>(4) In the Introduction, specifically blink-related ocular activity is mentioned as being related to speech tracking (for which a reference is, incidentally, missing), while here, any blink-related activity is excluded from the analysis. This should be motivated, as it appears in direct contradiction.</p>
</disp-quote>
<p>Thank you for pointing this out. The mention of blink-related ocular activity in the Introduction refers to findings by Jin et al. (2018), where such activity was shown to align with higher-order syntactic structures in artificial speech. We have now included the appropriate reference for clarity.</p>
<p>While Jin et al. focused on blink-related activity, in the present study, we focused on gaze patterns to investigate ocular speech tracking, replicating findings from</p>
<p>Gehmacher et al. (2024). This approach was motivated by our goal to validate previous results using the same methodology. Importantly to this point, the exclusion of blinks in our analysis was due to methodological constraints of TRF analysis, which requires a continuous response signal; blinks, being discrete and artifact-prone, are incompatible with this approach.</p>
<p>To address your concern, we revised the Introduction to clarify this distinction and provide explicit motivation for focusing on gaze patterns. It now reads:</p>
<p>“Along these lines, It has been shown that covert, mostly blink related eye activity aligns with higher-order syntactic structures of temporally predictable, artificial speech (i.e. monosyllabic words; Jin et al, 2018). In support of ideas that the motor system is actively engaged in speech perception (Galantucci et al., 2006; Liberman &amp; Mattingly, 1985), the authors suggest a global entrainment across sensory and (oculo)motor areas which implements temporal attention.</p>
<p>In another recent study from our lab (Gehmacher et al., 2024), we showed that eye movements continuously track intensity fluctuations of attended natural speech, a phenomenon we termed ocular speech tracking. In the present study, we focused on gaze patterns rather than blink-related activity, both to replicate findings from</p>
<p>Gehmacher et al. (2024) and because blink activity is unsuitable for TRF analysis due to its discrete and artifact-prone nature. Hence, “Ocular speech tracking” (similarly to “neural speech tracking” refers to the correlation coefficient between actual eye movements and movements predicted from an encoding model based on the speech envelope.”</p>
<p>Jin, P., Zou, J., Zhou, T., &amp; Ding, N. (2018). Eye activity tracks task-relevant structures during speech and auditory sequence perception. Nature communications, 9(1), 5374.</p>
<disp-quote content-type="editor-comment">
<p>(5) The rationale for the mediation analysis is questionable. Let speech envelope = A, brain activity = B, eye movements = C. The authors wish to claim that A -&gt; C -&gt; B. But it is equally possible that A -&gt; B -&gt; C. They reflect on this somewhat in Discussion, but throughout the rest of the paper, the mediation analysis is presented as specifically testing whether A -&gt; B is mediated by C, which is potentially misleading.</p>
</disp-quote>
<p>Indeed we share your concern regarding the directionality of the relationships in the mediation analysis. Our choice of ocular movements as a mediator was motivated by the fact that the relationship between acoustic speech and neural activity is well established, as well as previous results indicating that oculomotor activity contributes to cognitive effects in auditory attention (Popov et al., 2022).</p>
<p>Indeed, here we treat both interpretations (“ocular movements contribute to neural speech tracking” versus “neural activity contributes to ocular speech tracking”) as equal.  We now emphasise this point in our discussion quite thoroughly:</p>
<p>“It is important to note that our current findings do not allow for inference on directionality. Our choice of ocular movements as a mediator was motivated by the fact that the relationship between acoustic speech and neural activity is well established, as well as previous results indicating that oculomotor activity contributes to cognitive effects in auditory attention (Popov et al., 2022). However, an alternative model may suggest that neural activity mediates the effect of ocular speech tracking. Hence, it is possible that ocular mediation of speech tracking may reflect a) active (ocular) sensing for information driven by (top-down) selective attention or b) improved neural representations as a consequence of temporally aligned increase of sensory gain or c) (not unlikely) both. In fact, when rejecting the notion of a single bottom-up flow of information and replacing it with a model of distributed parallel and dynamic processing, it seems only reasonable to assume that the direction of communication (between our eyes and our brain) will depend on where (within the brain) as well as when we look at the effect. Thus, the regions and time-windows reported here should be taken as an illustration of oculo-neural communication during speech processing rather than an attempt to &quot;explain&quot; neural speech processing by ocular movements.”</p>
<disp-quote content-type="editor-comment">
<p>(6) The mediation analysis can be improved by a proper quantification of the effect (sizes or variance explained). E.g. how much % of B is explained by A total, and how much of that can in turn be explained by C being involved? For drawing directional conclusions perhaps Granger causality could be used.</p>
</disp-quote>
<p>In Figure 4 (now Figure 5) of our manuscript we use standardized betas (which correspond to effect sizes) to illustrate the mediation effect. With the current mTRF approach it is however not possible (or insightful) to compare the variance explained. It is reasonable to assume that variance in neural activity will be explained better when including oculomotor behavior as a second predictor along with acoustic simulation. However this increase gives no indication to what extent this oculomotor behavior was task relevant or irrelevant (since all kinds of “arbitrary” movements will be captured with brain activity and therefore lead to an increase in variance explained). For this reason we chose to pursue the widely accepted framework of mediation (Baron &amp; Kenny, 1986). This (correlational) approach is indeed limited in its interpretations (see prev. response), however the goal of the current study was to replicate and illustrate the triad relationship of acoustic speech input, neural activity and ocular movements with no particular hypotheses on directionality.</p>
<disp-quote content-type="editor-comment">
<p>(7) Both prediction tendency and neural speech tracking depend on MEG data, and thus on MEG signal-to-noise ratio (SNR). It is possible some participants may have higher SNR recordings in both tasks, which may result in both higher (estimated) prediction tendency and higher (estimated) speech tracking. This would result in a positive correlation, as the authors observe. This trivial explanation should be ruled out, by quantifying the relative SNR and testing for the absence of a mediation here.</p>
</disp-quote>
<p>We agree that for both approaches (MVPA and mTRF models) individual MEG SNR plays an important role. This concern has been raised previously and addressed in our previous manuscript (Schubert et al., 2023). First, it should be noted that our prediction tendency value is the result of a condition contrast (rather than simple decoding accuracy) which compensates for the influence of subject specific signal-to-noise ratio (as no vacuous difference in SNR is to be expected between conditions). Second, in our previous study we also used frequency decoding accuracy as a control variable to correlate with speech tracking variables of interest and found no significant effect.</p>
<disp-quote content-type="editor-comment">
<p>(8) Much of the analysis pipeline features temporal response functions (TRFs). These should be shown in a time-resolved manner as a key intermediate step.</p>
</disp-quote>
<p>We now included the Neural Speech tracking TRFs into the Figure (now Figure 3).</p>
<disp-quote content-type="editor-comment">
<p>(9) Figure 2 shows much-condensed results from different steps in the pipeline. If I understand correctly, 2A shows raw TRF weights (averaged over some time window?), while 2B-F shows standardized mean posterior regressor weights after Bayesian stats? It would be very helpful to make much more explicit what is being shown here, in addition to showing the related TRFs.</p>
</disp-quote>
<p>Thank you for pointing this out! The figure description so far has been indeed not very insightful on this issue. We now adapted the caption and hope this clarifies the confusion: “ Neural speech tracking is related to prediction tendency and word surprisal, independent of selective attention. A) Envelope (x) - response (y) relationships are estimated using deconvolution (Boosting). The TRF (filter kernel, h) models how the brain processes the envelope over time. This filter is used to predict neural responses via convolution. Predicted responses are correlated with  actual neural activity to evaluate model fit and the TRF's ability to capture response dynamics. Correlation coefficients from these models are then used as dependent variables in Bayesian regression models. (Panel adapted from Gehmacher et al., 2024b). B) Temporal response functions (TRFs) depict the time-resolved neural tracking of the speech envelope for the single speaker and multi speaker target condition, shown here as absolute values averaged across channels. Solid lines represent the group average. Shaded areas represent 95% Confidence Intervals. C–H) The beta weights shown in the sensor plots are derived from Bayesian regression models in A). For Panel C, this statistical model is based on correlation coefficients computed from the TRF models (further details can be found in the Methods Section). C) In a single speaker condition, neural tracking of the speech envelope was significant for widespread areas, most pronounced over auditory processing regions. D) The condition effect indicates a decrease in neural speech tracking with increasing noise (1 distractor). E) Stronger prediction tendency was associated with increased neural speech tracking over left frontal areas. F) However, there was no interaction between prediction tendency and conditions of selective attention. G) Increased neural tracking of semantic violations was observed over left temporal areas. H) There was no interaction between word surprisal and speaker condition, suggesting a representation of surprising words independent of background noise. Marked sensors indicate ‘significant’ clusters, defined as at least two neighboring channels showing a significant result. N = 29.”</p>
<p>Gehmacher, Q., Schubert, J., Kaltenmaier, A., Weisz, N., &amp; Press, C. (2024b). The &quot;Ocular Response Function&quot; for encoding and decoding oculomotor related neural activity. bioRxiv, 2024-11.</p>
<disp-quote content-type="editor-comment">
<p>(10) Bayesian hypothesis testing is not done consistently. Some parts test for inclusion of 0 in 94% HDI, while some parts adopt a ROPE approach. The same approach should be taken throughout. Additionally, Bayes factors would be very helpful (I appreciate these depend on the choice of priors, but the default Bambi priors should be fine).</p>
</disp-quote>
<p>Our primary aim in this study was to replicate two recent findings: (1) the relationship between individual prediction tendencies and neural speech tracking, and (2) the tracking of the speech envelope by eye movements. To maintain methodological consistency with the original studies, we did not apply a ROPE approach when analyzing these replication effects. Instead, we followed the same procedures as the original work, focusing on the inclusion of 0 in the HDI for the neural effects and using the same methods for the ocular effects. Additionally, we were not specifically interested in potential null effects in these replication analyses, as our primary goal was to test whether we could reproduce the previously reported findings.</p>
<p>For the mediation analysis, however, we chose to extend the original approach by not only performing the analysis in a time-resolved manner but also applying a ROPE approach. This decision was motivated by our interest in gaining more comprehensive insights — beyond the replication goals — by also testing for potential null effects, which can provide valuable information about the presence or absence of mediation effects.</p>
<p>We appreciate your thoughtful feedback and hope this clarifies our rationale for the differing approaches in our Bayesian hypothesis testing.</p>
<p>Regarding Bayes Factors,</p>
<p>We understand that some researchers find Bayes Factors appealing, as they offer a seemingly simple and straightforward way to evaluate the evidence in favor of/ or against H0 in relation to H1 (e.g. BF10 &gt; 102 =  Decisive; according to the Jeffreys Scale). However, in practice Bayes Factors are often misunderstood e.g. by interpreting Bayes Factor as posterior odds or not acknowledging the notion of relative evidence in the Bayes Factor (see Wong et al. 2022). Instead of using Bayes Factors, we prefer to rely on estimating and reporting the posterior distribution of parameters given the data, prior and model assumptions (in form of the 94% HDI). This allows for a continuous evaluation of evidence for a given hypothesis that is in our eyes easier to interpret as a Bayes Factor.</p>
<p>Jeffreys, Harold (1998) [1961]. The Theory of Probability (3rd ed.). Oxford, England. p. 432. ISBN 9780191589676.</p>
<p>Wong, T. K., Kiers, H., &amp; Tendeiro, J. (2022). On the Potential Mismatch Between the Function of the Bayes Factor and Researchers’ Expectations. Collabra: Psychology, 8(1), 36357. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1525/collabra.36357">https://doi.org/10.1525/collabra.36357</ext-link></p>
<disp-quote content-type="editor-comment">
<p>(11) It would be helpful if Results could be appreciated without a detailed read of Methods. I would recommend a recap of each key methodological step before introducing the relevant Result. (This may also help in making the rationale explicit.)</p>
</disp-quote>
<p>In addition to the short recaps of methods that were already present, and information on quantifications of neural and ocular tracking and bayes statistics (see responses 1, 2, 9), we now added the following parts below to the results sections. Please refer to them in the context of the manuscript where they should now complement a key recap of methodological steps necessary to readily understand each analysis and rational that led to the results:</p>
<p>Individual prediction tendency is related to neural speech tracking:</p>
<p>“Thus, this measure is a single value per subject, which comprises a) differences between two contextual probabilities (i.e. ordered vs. random) in b) feature-specific tone representations c) in advance of their observation (summed over a time-window of -0.3 - 0 s). Importantly, this prediction tendency was assessed in an independent entropy modulation paradigm (see Fig. 1). On a group level we found an increased tendency to pre-activate a stimulus of high probability (i.e. forward transition) in an ordered context compared to a random context (see Fig, 2A). This effect replicates results from our previous work (Schubert et al., 2023, 2024). Using the summed difference between entropy levels (ordered - random) across pre-stimulus time, one value was extracted per subject (Fig. 2B). This value was used as a proxy for “individual prediction tendency” and correlated with encoding of clear speech across different MEG sensors. [...]</p>
<p>Neural speech tracking, quantified as the correlation coefficients between predicted and observed MEG responses to the speech envelope, was used as the dependent variable in Bayesian regression models. These models included condition (single vs. multi-speaker) as a fixed effect, with either prediction tendency or word surprisal as an additional predictor, and random effects for participants.”</p>
<p>Eye movements track acoustic speech in selective attention:</p>
<p>“For this, we separately predicted horizontal and vertical eye movements from the acoustic speech envelope using temporal response functions (TRFs). The resulting model fit (i.e. correlation between true and predicted eye movements) is commonly referred to as “speech tracking”. Bayesian regression models were applied to evaluate tracking effects under different conditions of selective attention (single speaker, attended multi-speaker, unattended multi-speaker). Furthermore, we assessed whether individual prediction tendency or semantic word surprisal influenced ocular speech tracking.”</p>
<p>Neural speech tracking is mediated by eye movements:</p>
<p>“This model evaluates to what extent gaze behaviour functions as a mediator between acoustic speech input and brain activity.”</p>
<p>Neural and ocular speech tracking are differently related to comprehension: “Bayesian regression models were used to investigate relationships between neural/ocular speech tracking and comprehension or difficulty. Ocular speech tracking was analyzed separately for horizontal and vertical eye movements.”</p>
<disp-quote content-type="editor-comment">
<p>(12) The research questions in the Introduction should be sharpened up, to make explicit when a question concerns a theoretical entity, and when it concerns something concretely measured/measurable.</p>
</disp-quote>
<p>We sharpened them up:</p>
<p>“Taking into account the aforementioned study by Schubert and colleagues (2023), the two recently uncovered predictors of neural tracking (individual prediction tendency and ocular tracking) raise several empirical questions regarding the relationship between predictive processes, selective attention, and active ocular sensing in speech processing:</p>
<p>(1) Are predictive processes related to active ocular sensing in the same way they are to neural speech tracking? Specifically, do individuals with a stronger tendency to anticipate predictable auditory features, as quantified through prestimulus neural representations in an independent tone paradigm, show increased or even decreased ocular speech tracking, measured as the correlation between predicted and actual eye movements? Or is there no relationship at all?</p>
<p>(2) To what extent does selective attention influence the relationship between prediction tendency, neural speech tracking, and ocular speech tracking? For example, does the effect of prediction tendency or ocular speech tracking on neural tracking differ between a single-speaker and multi-speaker listening condition?</p>
<p>(3) Are individual prediction tendency and ocular speech tracking related to behavioral outcomes, such as comprehension and perceived task difficulty? Speech comprehension is assessed through accuracy on comprehension questions, and task difficulty is measured through subjective ratings.</p>
<p>Although predictive processes, selective attention, and active sensing have been shown to contribute to successful listening, their potential interactions and specific roles in naturalistic speech perception remain unclear. Addressing these questions will help disentangle their contributions and establish an integrated framework for understanding how neural and ocular speech tracking support speech processing.”</p>
<disp-quote content-type="editor-comment">
<p>(13) The negative relationship between story comprehension and ocular speech tracking appears to go against the authors' preferred interpretation, but the reflection on this in the Discussion is very brief and somewhat vague.</p>
</disp-quote>
<p>Thank you for pointing this out. We have taken your comments into careful consideration and also incorporated Reviewer #1's query (Minor point 2) into a unified and complementary reasoning. We have rewritten the relevant paragraph in the discussion to provide a clearer and more detailed explanation. We hope this revision offers a more precise and less vague discussion on this important point.</p>
<p>“Despite the finding that eye movements mediate neural speech tracking, the behavioural relevance for semantic comprehension appears to differ between ocular and neural speech tracking. Specifically, we found a negative association between ocular speech tracking and comprehension, indicating that participants with lower comprehension performance exhibited increased ocular speech tracking. Interestingly, no significant relationship was observed between neural tracking and comprehension.</p>
<p>In this context, the negative association between ocular tracking and comprehension might reflect individual differences in how participants allocate cognitive resources. Participants with lower comprehension may rely more heavily on attentional mechanisms to process acoustic features, as evidenced by increased ocular tracking. This reliance could represent a compensatory strategy when higher-order processes, such as semantic integration or memory retrieval, are less effective. Importantly, our comprehension questions (see Experimental Procedure) targeted a broad range of processes, including intelligibility and memory, suggesting that this relationship reflects a trade-off in resource allocation between low-level acoustic focus and integrative cognitive tasks.</p>
<p>Rather than separating eye and brain responses conceptually, our analysis highlights their complementary contributions. Eye movements may enhance neural processing by increasing sensitivity to acoustic properties of speech, while neural activity builds on this foundation to integrate information and support comprehension. Together, these systems form an interdependent mechanism, with eye and brain responses working in tandem to facilitate different aspects of speech processing.</p>
<p>This interpretation is consistent with the absence of a difference in ocular tracking for semantic violations (e.g., words with high surprisal versus lexically matched controls), reinforcing the view that ocular tracking primarily reflects attentional engagement with acoustic features rather than direct involvement in semantic processing. This aligns with previous findings that attention modulates auditory responses to acoustic features (e.g., Forte et al., 2017), further supporting the idea that ocular tracking reflects mechanisms of selective attention rather than representations of linguistic content.</p>
<p>Future research should investigate how these systems interact and explore how ocular tracking mediates neural responses to linguistic features, such as lexical or semantic processing, to better understand their joint contributions to comprehension.”.</p>
<disp-quote content-type="editor-comment">
<p>(14) Page numbers would be helpful.</p>
</disp-quote>
<p>We added the page numbers.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations for the authors):</bold></p>
<p>Results</p>
<p>(1) Figure 2 - statistical results are reported in this figure, but they are not fully explained in the text, nor are statistical values provided for any of the analyses (as far as I can tell).</p>
<p>Also, how were multiple comparisons dealt with (the choice of two neighboring channels seems quite arbitrary)? Perhaps for this reason, the main result - namely the effect of &quot;prediction tendency&quot; and &quot;semantic violations&quot; - is quite sparse and might not survive more a rigorous statistical criterion. I would feel more comfortable with these results if the reporting of the statistical analysis had been more thorough (ideally, including comparison to control models).</p>
</disp-quote>
<p>We would like to thank you again for your detailed queries, comments, and questions on our work. We first of all adapted this figure (now Figure 3 in the manuscript, please see responses 8 and 9 to Reviewer #2) to help readers understand the metrics and values within each statistical analysis. In addition, we indeed did not include the detailed statistics in the text! We now added the missing statistic reports calculated as averages over ‘clusters’:</p>
<p>“Replicating previous findings (Schubert et al., 2023), we found widespread encoding of clear speech (average over cluster: β = 0.035, 94%HDI = [0.024, 0.046]), predominantly over auditory processing regions (Fig. 3C), that was decreased (β = -0.018, 94%HDI = [0.029, -0.006]) in a multi-speaker condition (Fig. 3D). Furthermore, a stronger prediction tendency was associated with increased neural speech tracking (β = 0.014, 94%HDI = [0.004, 0.025]) over left frontal sensors (see Fig. 3E). We found no interaction between prediction tendency and condition (see Fig. 3F).” [...] “In a direct comparison with lexically identical controls, we found an increased neural tracking of semantic violations (β = 0.039, 94%HDI = [0.007, 0.071]) over left temporal areas (see Fig. 3G). Furthermore, we found no interaction between word surprisal and speaker condition (see Fig. 3H).”</p>
<p>Regarding the &quot;prediction tendency&quot; effect, it is important to note that this finding replicates a result from Schubert et al. (2023). The left frontal location of this effect is also consistent over studies, which convinces us of the robustness of the finding. Furthermore, testing this relationship properly requires a mixed-effects model in order to account for the variability across subjects that is not explained by fixed effects and the repeated measures design. For this reason a random Intercept had to be fitted for each subject (1|subject in the respective model formula). This statistical requirement motivated our decision to use bayesian statistics as (at least to our knowledge) there is no implementation of a cluster-based permutation mixed effects model (yet). In order to provide a more conservative criterion (as bayesian statistics don’t require a multiple comparison correction) we chose to impose in addition the requirement of a “clustered” effect.</p>
<p>The choice of using two neighboring channels is consistent with the default parameter settings in FieldTrip’s cluster-based permutation testing (cfg.minnbchan = 2). This parameter specifies the minimum number of neighboring channels required for a sample to be included in the clustering algorithm, ensuring spatial consistency in the identified clusters. This alignment ensures that our methodology is comparable to numerous prior studies in the field, where such thresholds are standard. While it is true that all statistical analyses involve some degree of arbitrariness in parameter selection (e.g., alpha levels or clustering thresholds), our approach reflects established conventions and ensures comparability with previous findings.</p>
<p>While the original study utilized source space analyses, we replicated this effect using only 102 magnetometers. This choice was made for computational simplicity, demonstrating that the effect is robust even without source-level modeling. Similarly, the &quot;semantic violation&quot; effect, while perceived as sparse, is based solely on magnetometer data and - in our opinion - should not be viewed as overly sparse given the methods employed. This effect aligns with the two-neighbor clustering approach, ensuring spatial consistency across magnetometers. The results reflect the robustness of the effects within the constraints of magnetometer-level analyses.</p>
<p>Overall, the methodological choices, including the choice of a bayesian linear mixed effects model, the use of two neighboring channels and the reliance on magnetometers, are grounded in established practices and methodological considerations. While stricter thresholds or alternative approaches might yield different results, our methods align with best practices in the field and ensure the robustness, comparability, and replicability of our findings.</p>
<disp-quote content-type="editor-comment">
<p>(2) Figure 3 - the difference between horizontal and vertical eye-movements. This result is quite confusing and although the authors do suggest a possible interpretation for this in the discussion, I do wonder how robust this difference is or whether the ocular signal (in either direction) is simply too noisy or the effect too small to be detected consistently across conditions. Also, the ocular-TRFs themselves are not entirely convincing in suggesting reliable response/tracking of the audio - despite the small-but-significant increase in prediction accuracy.</p>
</disp-quote>
<p>The horizontal versus vertical comparison was conducted to explore potential differences in how these dimensions contribute to ocular tracking of auditory stimuli (please also see our response to Reviewer #1, Response 5b, that includes the vertical vs. horizontal effects of Gehmacher at al. 2024). It would indeed be interesting to develop a measure that combines the two directions into a more natural representation of 'viewing,' such as a combined vector. However, this approach would require the use of complex numbers to represent both magnitude and direction simultaneously, hence the development of novel TRF algorithms capable of modeling this multidimensional signal. While beyond the scope of the current study, this presents an exciting avenue for future research and would allow us to move closer to understanding ocular speech tracking and the robustness of these effects, above and beyond the already successful replication.</p>
<p>It is also important to emphasize that ocular-TRFs are derived from (viewing) behavioral data rather than neural signals, and are thus inherently subject to greater variability across participants and time. This higher variability does not necessarily indicate a small or unreliable effect but reflects the dynamic and task-dependent nature of eye movement behavior. The TRFs with shaded error margins represent this variability, highlighting how eye movements are influenced by both individual differences and moment-to-moment changes in task engagement.</p>
<p>Despite this inherent variability, the significant prediction accuracy improvements confirm that ocular-TRFs reliably capture meaningful relationships between eye movements and auditory stimuli. The observed differences between horizontal and vertical TRFs further support the hypothesis that these dimensions are differentially involved in the task, possibly driven by the specific roles they play in sensorimotor coupling.</p>
<disp-quote content-type="editor-comment">
<p>(3) Figure 4 - this figure shows source distribution of 3 PCA components, derived from the results of the mediation effect of eye movements on the speech-tracking. Here too I am having difficulty in interpreting what the results actually are. For one, all three components are quite widespread and somewhat overlapping, so although they are statistically &quot;independent&quot; it is hard to learn much from them about the brain regions involved and whether they truly represent separable contributions. Similarly difficult to interpret are the time courses, which share some similarities with the known TRFs to speech (especially PC3). I would have expected to find a cleaner &quot;auditory&quot; response, and clearer separation between sensory regions and regions involved in the control of eye movements. I also wonder why the authors chose not to show the sourcelocalization of the neural and ocular speech-tracking responses alone - this could have helped us between understand what &quot;mediation&quot; of the neural response might look like.</p>
</disp-quote>
<p>We appreciate the reviewer’s interest in better understanding the source distribution and time courses of the PCA components. While we acknowledge that the widespread and overlapping nature of the components may make a more fine grained interpretation challenging, it is important to emphasize that our analysis simply reflects the data, hence we can only present and interpret what the analysis revealed.</p>
<p>Regarding your suggestion to show the source localization of ocular speech tracking and neural speech tracking alone, we would like to point out that ocular tracking is represented by only one channel for vertical and one channel for horizontal eye movements. Thus, in this case the estimated source of the effect are the eyes themselves. We believe that the source localization of neural speech tracking has been a thoroughly studied topic in research so far (locating it to perisylvian, auditory areas with a stronger preference for the left hemisphere) and can also be seen in Schubert et al., (2023). Nevertheless, we believe the observed PCA components still provide valuable, and most importantly novel insights into the interplay between eye movements and neural responses in speech tracking.</p>
<disp-quote content-type="editor-comment">
<p>Discussion/interpretation</p>
<p>(1) Although I appreciate the authors' attempt to propose a &quot;unified&quot; theoretical model linking predictions about low-level features to higher features, and the potential involvement of eye movements in 'active sensing' I honestly think that this model is overambitious, given the data presented in the current study. Moreover, there is very little discussion of past literature and existing models of active sensing and hierarchical processing of speech, that could have helped ground the discussion in a broader theoretical context. The entire discussion contains fewer than 20 citations (some of which are by these authors) and needs to be substantially enriched in order to provide context for the authors' claims.</p>
</disp-quote>
<p>Thank you very much for your thoughtful feedback and for appreciating our approach. We hope that the revised manuscript addresses your concerns. Specifically, we now emphasize that our proposal is a conceptual framework, with the main goal to operationale “prediction tendency”, “active ocular sensing”, and “selective attention” and to “organise these entities according to their assumed function for speech processing and to describe their relationship with each other.” We did this by thoroughly revising our discussion section with a clear emphasis on the definition of terms, for example:</p>
<p>“With this speculative framework we attempt to describe and relate three important phenomena with respect to their relevance for speech processing: 1) “Anticipatory predictions” that are created in absence of attentional demands and contain probabilistic information about stimulus features (here, inferred from frequency-specific pre-activations during passive listening to sound sequences). 2) “Selective attention” that allocates resources towards relevant (whilst suppressing distracting) information (which was manipulated by the presence or absence of a distractor speaker). And finally 3) “active ocular sensing”, which refers to gaze behavior that is temporally aligned to attended (but not unattended) acoustic speech input (inferred from the discovered phenomenon of ocular speech tracking).”</p>
<p>Our theoretical proposals are now followed by a recap of our results that support the respective idea, for example:</p>
<p>“...these predictions are formed in parallel and carry high feature-specificity but low temporal precision (as they are anticipatory in nature). This idea is supported by our finding that pure-tone anticipation is visible over a widespread prestimulus interval, instead of being locked to sound onset”</p>
<p>“....we suggest that active (ocular) sensing does not necessarily convey feature- or content-specific information, it is merely used to boost (and conversely filter) sensory input at specific timescales (similar to neural oscillations). This assumption is supported by our finding that semantic violations are not differentially encoded in gaze behaviour than lexical controls.”</p>
<p>And we put a strong focus on highlighting the boundaries of these ideas, in order to avoid theoretical confusion, misunderstandings or implicit theoretical assumption that are not grounded in data, in particular:</p>
<p>“In fact, when rejecting the notion of a single bottom-up flow of information and replacing it with a model of distributed parallel and dynamic processing, it seems only reasonable to assume that the direction of communication (between our eyes and our brain) will depend on where (within the brain) as well as when we look at the effect. Thus, the regions and time-windows reported here should be taken as an illustration of oculo-neural communication during speech processing rather than an attempt to &quot;explain&quot; neural speech processing by ocular movements.”</p>
<p>“Even though the terminology [“hierarchy”] is suggestive of a fixed sequence (similar to a multi storey building) with levels that must be traversed one after each other (and even the more spurious idea of a rooftop, where the final perceptual experience is formed and stored into memory), we distance ourselves from these (possibly unwarranted) ideas. Our usage of “higher” or “lower” simply refers to the observation that the probability of a feature at a higher (as in more associative) level affects the interpretation (and thus the representation and prediction) of a feature at lower (as in more segregated) levels (Caucheteux et al., 2023).”</p>
<p>Additionally, we have made substantial efforts to present complementary results (see response to Reviewer #2, point 8) to further substantiate our interpretation. Importantly, we have updated the illustration of the model (see response to Reviewer #, minor point 1) and refined both our interpretations and the conceptual language in the Discussion. Furthermore, we have included additional citations where appropriate to strengthen our argument.</p>
<p>We would also like to briefly note that this section of the Discussion aimed to highlight existing literature that bridges the gap our model seeks to address. However, as this is a relatively underexplored area, the references available are necessarily limited.</p>
<disp-quote content-type="editor-comment">
<p>(2) Given my many reservations about the data, as presented in the current version of the manuscript, I find much of the discussion to be an over-interpretation of the results. This might change if the authors are able to present more robust results, as per some of my earlier comments.</p>
</disp-quote>
<p>We sincerely hope that our comprehensive revisions have addressed your concerns and improved the manuscript to your satisfaction.</p>
</body>
</sub-article>
</article>