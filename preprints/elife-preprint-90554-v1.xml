<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">90554</article-id>
<article-id pub-id-type="doi">10.7554/eLife.90554</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90554.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Foveated metamers of the early visual system</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8999-9003</contrib-id>
<name>
<surname>Broderick</surname>
<given-names>William F.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rufo</surname>
<given-names>Gizem</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7475-5586</contrib-id>
<name>
<surname>Winawer</surname>
<given-names>Jonathan</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1206-527X</contrib-id>
<name>
<surname>Simoncelli</surname>
<given-names>Eero P.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Flatiron Institute, Simons Foundation</institution></aff>
<aff id="a2"><label>2</label><institution>Meta, Inc.</institution></aff>
<aff id="a3"><label>3</label><institution>Department of Psychology, New York University</institution></aff>
<aff id="a4"><label>4</label><institution>Center for Neural Science, New York University</institution></aff>
<aff id="a5"><label>5</label><institution>Courant Inst. for Mathematical Sciences, New York University</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>de Lange</surname>
<given-names>Floris P</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Donders Institute for Brain, Cognition and Behaviour</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>For correspondence</bold>: <email>billbrod@gmail.com</email> (WFB)</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-11-01">
<day>01</day>
<month>11</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP90554</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-08-02">
<day>02</day>
<month>08</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-08-02">
<day>02</day>
<month>08</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.05.18.541306"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Broderick et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Broderick et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-90554-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Human ability to discriminate and identify visual attributes varies across the visual field, and is generally worse in the periphery than in the fovea. This decline in performance is revealed in many kinds of tasks, from detection to recognition. A parsimonious hypothesis is that the representation of any visual feature is blurred (spatially averaged) by an amount that differs for each feature, but that in all cases increases with eccentricity. Here, we examine models for two such features: local luminance and spectral energy. Each model averages the corresponding feature in pooling windows whose diameters scale linearly with eccentricity. We performed psychophysical experiments with synthetic stimuli to determine the window scaling for which human and model discrimination abilities match, called the critical scaling. We used much larger stimuli than those of previous studies, subtending 53.6 by 42.2 degrees of visual angle. We found the critical scaling for the luminance model was approximately one-fourth that of the energy model, and consistent with earlier studies, that a smaller critical scaling value was required when discriminating a synthesized image from a natural image than when discriminating two synthesized images. We offer a coherent explanation for these results in terms of alignments and misalignments of the models with human perceptual representations.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>1. Main paper and appendix are split into separate files so that main paper with all figures included falls below 40MB file size limit (rather than requiring some files to be uploaded separately)
2. Figure 3 is slightly compressed</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://users.flatironinstitute.org/~wbroderick/metamers/">https://users.flatironinstitute.org/~wbroderick/metamers/</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://osf.io/67tbe/">https://osf.io/67tbe/</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/billbrod/foveated-metamers">https://github.com/billbrod/foveated-metamers</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Vision science is often concerned with what things look like (appearance), but a long and fruitful thread of research has investigated what humans cannot see, that is, the information they are insensitive to. Perceptual metamers — images that are physically distinct but perceptually indistinguishable — provide direct evidence of such information loss in visual representations. <bold><italic><xref ref-type="bibr" rid="c13">Cohen and Kappauf (1985)</xref></italic></bold> identify this concept in the writings of Isaac Newton, who noted that particular colors of light could be created by mixing other colors together, and trace the word “metamer” to a 1919 chapter by Wilhelm Ostwald, the first Nobel laureate in chemistry. Color metamers were instrumental in the development of the Young-Helmholtz theory of trichromacy (<bold><italic><xref ref-type="bibr" rid="c27">Helmholtz, 1852)</xref></italic></bold>. In this context, metamers clarified human sensitivity to light wavelengths, demonstrating that the human visual system projects the infinite dimensionality of the physical signal to three dimensions. It took more than a century before the physiological basis for this — the three classes of cone photoreceptor — was revealed experimentally (<bold><italic><xref ref-type="bibr" rid="c54">Schnapf et al., 1987)</xref></italic></bold>.</p>
<p>The visual system also discards a great deal of spatial detail, more so in portions of the visual field farthest from the center of gaze. Specifically, the reduction of visual capabilities with increasing eccentricity has been demonstrated for both acuity (<bold><italic><xref ref-type="bibr" rid="c21">Frisen and Glansholm, 1975)</xref></italic></bold> and contrast sensitivity (<bold><italic><xref ref-type="bibr" rid="c5">Banks et al., 1987</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c52">Robson and Graham, 1981</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c53">Rovamo et al., 1978)</xref></italic></bold>, and is reflected in the physiology: fewer cortical resources are dedicated to the periphery (<bold><italic><xref ref-type="bibr" rid="c55">Schwartz, 1977)</xref></italic></bold> and receptive fields in all stages of the visual hierarchy grow with eccentricity (e.g., <bold><italic><xref ref-type="bibr" rid="c15">Daniel and Whitteridge (1961)</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c14">Dacey and Petersen (1992)</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c23">Gattass et al. (1981</xref>, <xref ref-type="bibr" rid="c24">1988)</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c40">Maunsell and Newsome (1987)</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c68">Wandell and Winawer (2015)</xref></italic></bold>). This decrease in acuity has been demonstrated by either scaling the size of features, as in the Anstis eye chart (<bold><italic><xref ref-type="bibr" rid="c2">Anstis, 1974)</xref></italic></bold>, or by progressively blurring the image, as in <bold><italic><xref ref-type="bibr" rid="c3">Anstis (1998)</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c59">Thibos (2020)</xref></italic></bold>. More generally, one can explain this decreasing sensitivity to spatial information with “pooling models”, which compute local averages of image features in windows that grow larger with eccentricity (<bold><italic><xref ref-type="bibr" rid="c4">Balas et al., 2009</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli, 2011</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c33">Keshvari and Rosenholtz, 2016)</xref></italic></bold>. These models assume that peripheral representations are qualitatively similar to those of the fovea: the same local computations are performed over larger regions. Here, we test two such models — one that averages local luminance (<bold>luminance model</bold>) and one that averages both local spectral energy and luminance (<bold>energy model</bold>). We generate images pairs in which one or both images have been manipulated such that the two are <bold>model metamers</bold> (images that are physically distinct but with identical model representations). The pair of model metamers are also perceptual metamers if the human visual system is insensitive to the differences between them, as schematized in <xref rid="fig1" ref-type="fig">figure 1</xref> (see <bold><italic><xref ref-type="bibr" rid="c72">Watson et al. (1986)</xref></italic></bold> for an analogous presentation with respect to sensitivity to spatial and temporal frequency). By comparing model and human perceptual metamers, we investigate how well the models’ sensitivities (and insensitivities) align with those of the human visual system.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Schematic diagram of perceptual metamers. Each panel shows a two-dimensional depiction of the set of all possible images: every image corresponds to a point in this space and every point in this space represents an image. Perceptual metamers — images that cannot be reliably distinguished — are equivalence classes in this space and we illustrate this by partitioning the space into distinct perceptually identical regions. <bold>Left:</bold> example image (black point), and surrounding metameric images (region enclosed by black polygon). <bold>Center:</bold> In a hierarchical visual system, in which each stage transforms the signals of the previous stage and discards some information, metamers of earlier stages are nested within metamers of later stages. That is, every pair of images that are metamers for an early visual area <italic>𝒩</italic><sub>1</sub> are also metamers for the later visual area <italic>𝒩</italic><sub>2</sub>. The converse does not hold: there are images that <italic>𝒩</italic><sub>1</sub> can distinguish but that <italic>𝒩</italic><sub>2</sub> cannot. <bold>Right:</bold> Two particular image families: Samples of white noise (distribution represented as a grayscale intensity map in the lower right corner) and the set (manifold) of natural images (distribution represented by the curved gray line). Typical white noise samples fall within a single perceptual metamer class (humans are unable to distinguish them). Natural images, on the other hand, are generally distinguishable from each other, as well as from un-natural images (those that lie off the manifold).</p></caption>
<graphic xlink:href="541306v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>This procedure rests on the assumption that the visual system processes information hierarchically, in a sequence of stages, and information discarded in early stages cannot be recovered by later stages (the “data-processing inequality”)<sup><xref ref-type="fn" rid="fn1">1</xref></sup>. For example, metameric color stimuli produce identical cone responses and thus they cannot be distinguished by any additional downstream neural processing. Perceptually, if two images generate identical responses in all neurons at any stage of processing preceding that of perceptual decision making (e.g., retinal ganglion cells or primary visual cortical neurons), they will appear identical. This is schematized in the central panel of <xref rid="fig1" ref-type="fig">figure 1</xref>: two images are perceptual metamers if they are indistinguishable to an early visual area <italic>𝒩</italic><sub>1</sub> or if this early stage is sensitive to their differences, but those differences are discarded by later stages in the hierarchy. A number of authors have created perceptual metamers by matching complex statistics thought to be represented at a high level of the visual processing hierarchy (<bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli, 2011</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c33">Keshvari and Rosenholtz, 2016</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al., 2019</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c32">Jagadeesh and Gardner, 2022</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c19">Feather et al., 2019)</xref></italic></bold> and at the level of photoreceptors. However, there are fewer examples of perceptual metamers that match simpler image statistics.</p>
<p>Here, we synthesize metamers for two different models and measure their perceptual discriminability. The two models are based on local pooling of luminance and spectral energy, respectively, which can be loosely associated with two stages of visual physiology. Specifically, retinal ganglion cells encode local light level, and V1 cells encode local spectral energy. For each set of natural images, we used a stochastic gradient descent method to generate model metamers for both models, and measured discrimination capabilities of human observers when comparing the metamers with their corresponding original images, as well as with each other. We also examined the influence of the initial image used for the metamer synthesis algorithm. The two types of models and multiple types of comparisons shed light on and raise new questions about what makes images distinguishable.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Foveated pooling models</title>
<p>We constructed foveated models of human perception that capture sensitivity to local luminance and spectral energy (see <xref rid="fig3" ref-type="fig">figure 3</xref>). Both models are “pooling models” (<bold><italic><xref ref-type="bibr" rid="c4">Balas et al., 2009</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli, 2011</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c33">Keshvari and Rosenholtz, 2016</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al., 2019)</xref></italic></bold>, which compute statistics as weighted averages over local windows. A specific pooling model is characterized by both the statistics that are pooled and the shapes/sizes of the pooling regions. In the human visual system, receptive field sizes grow proportionally with distance from the fovea, as has been documented in monkey physiology and human fMRI (e.g., <bold><italic><xref ref-type="bibr" rid="c23">Gattass et al. (1981)</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c68">Wandell and Winawer (2015)</xref></italic></bold>). We reduce this to a single <bold>scaling parameter</bold>, by assuming smooth overlapping pooling regions that are separable and of constant size when expressed in polar angle and log-eccentricity (“log-polar”, which corresponds to the approximate log-polar geometry of visual cortical maps (<bold><italic><xref ref-type="bibr" rid="c55">Schwartz, 1977)</xref></italic></bold>). The value of this parameter, along with the choice of statistics, determine the sets of model metamers; for a given set of statistics, increasing the scaling value will increase the size of the sets of metameric images, in a nested manner (<xref rid="fig2" ref-type="fig">figure 2</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p><bold>Left:</bold> Pooling models <italic>ℳ</italic> are parameterized by their scaling value, <italic>s</italic>, and the statistics they pool, <italic>θ</italic>. Like any system that discards information, including the human visual system, these models have metamers: sets of images that are perceived as identical, represented graphically as enclosed non-overlapping regions. We draw samples from these sets using an optimization procedure: starting with initial image <bold><italic>I</italic></bold><sub><italic>i</italic></sub> we adjust the pixel values until their pooled statistics match those of the target (original) image <bold><italic>T</italic></bold>. The synthetic image depends on the target image, the metamer model, and also on the initial point and the stochastic synthesis algorithm. For a given set of statistics <italic>θ</italic>, increasing the scaling value will increase the size of the metamer set in a nested manner: any image that is a metamer for <italic>ℳ<sub>s,θ</sub></italic> will also be a metamer for <italic>ℳ<sub>αs,θ</sub></italic>, for factor <italic>α</italic> &gt; 1. <bold>Right:</bold> Changing the set of pooled statistics from <italic>θ</italic> to <italic>ϕ</italic> will result in different sets of model metamers, which may or may not overlap with those of the original model (though both must include the target image, <bold><italic>T</italic></bold>). If the model’s metamer classes differ substantially in their shape from the perceptual metamer classes, they will not provide a good description of the perceptual metamers at critical scaling (e.g., the blue ellipse contains only a small portion of the surrounding perceptual metamer region).</p></caption>
<graphic xlink:href="541306v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Two pooling models. Both models compute local image statistics weighted by a Gaussian that is separable in a log-polar coordinate system, such that radial extent is approximately twice the angular extent (half-maximum levels indicated by red contours). Windows are laid out in a log-polar grid, with peaks separated by one standard deviation. A single scaling factor governs the size of all pooling windows. The luminance model (top) computes average luminance, approximating the spatial pooling performed by retinal ganglion cells. The spectral energy model (bottom) computes average spectral energy at 4 orientation and 6 scales, as well as luminance, for a total of 25 statistics per window, approximating the representation of complex cells in primary visual cortex (V1). Spectral energy is computed using the complex steerable pyramid constructed in the Fourier domain (<bold><italic><xref ref-type="bibr" rid="c56">Simoncelli and Freeman, 1995)</xref></italic></bold>, squaring and summing across the real and imaginary components. Full resolution version of this figure can found on the <ext-link ext-link-type="uri" xlink:href="https://osf.io/74apz/">OSF</ext-link>.</p></caption>
<graphic xlink:href="541306v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We implemented log-polar pooling windows, with size proportional to their distance from the fovea. Like previous studies (<bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli, 2011</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al., 2019)</xref></italic></bold>, these pooling windows are overlapping and radially-elongated (the radial extent is roughly twice the angular extent), but unlike previous studies, we use Gaussian profiles with more extensive overlap, yielding a smoother representation and higher-quality synthesized images. The proportional overlap between adjacent windows is chosen to alleviate ringing and blocking artifacts in the synthesis, and is held fixed at all scaling values.</p>
<p>In the current study, we examine two models that compute different statistics within their pooling windows. The <italic>luminance</italic> model pools pixel intensities, and thus, a pair of luminance model metamers have the same average luminance within each pooling windows. Since the responses are insensitive to the highest frequencies, luminance model metamers include blurred versions of the target image (in which high frequencies are discarded), but also variants of the target image in which high frequencies are randomized or even amplified. In general, synthesized luminance model metamers will inherit the high frequency information of their initialization image, as can be seen in <xref rid="fig4" ref-type="fig">figure 4</xref>. The middle row of <xref rid="fig4" ref-type="fig">figure 4</xref> shows model metamers computed with two example scaling values. While the high-scaling model metamer is clearly perceptually distinct from the target image (regardless of observer fixation location), the low-scaling image is not easily discriminated from the target when fixating at the center of the image (i.e., when the human fovea is aligned with the model fovea).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Example synthesized model metamers. <bold>Top:</bold> Target image. <bold>Middle:</bold> Luminance model metamers, computed for two different scaling values (values as indicated, red ellipses to right of fixation indicate pooling window contours at half-max at that eccentricity). The left image is computed with a small scaling value, and is a perceptual metamer for most subjects: when fixating at the cross in the center of the image, the two images appear perceptually identical to the target image. Note, however, that when fixating in the periphery (e.g., the blue box), one can clearly see that the image differs from the target (see enlarged versions of the foveal and peripheral neighborhoods to right). The right image is computed with a larger scaling value, and is no longer a perceptual metamer (for any choice of observer fixation). <bold>Bottom:</bold> Energy model metamers. Again, the left image is computed with a small scaling value and is a perceptual metamer for most observers when fixated on the center cross. Peripheral content (e.g., blue box) contains more complex distortions, readily visible when viewed directly. The right image, computed with a large scaling value, differs in appearance from the target regardless of observer fixation. Full resolution version of this figure can be found on the <ext-link ext-link-type="uri" xlink:href="https://osf.io/nckpt/">OSF</ext-link>.</p></caption>
<graphic xlink:href="541306v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The <italic>spectral energy</italic> model pools the squared outputs of oriented bandpass filter responses at multiple scales and orientations. It also pools the pixel intensities. The energies are computed using a complex steerable pyramid, which decomposes images into frequency channels selective for 6 different scales and 4 different orientations. Energy is computed by squaring and summing across the real and imaginary responses (arising from even- and odd-symmetric filters) within each channel. These energies, along with the luminances, are then averaged within the spatial pooling windows. Thus, a pair of energy model metamers have the same average oriented energy and luminance within each of these windows. The bottom row of <xref rid="fig4" ref-type="fig">figure 4</xref> shows model metamers for two different scaling values. The low scaling value for the energy model is approximately matched to the higher scaling value for the luminance model, while the higher scaling value is approximately that associated with V1 receptive fields (<bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli, 2011)</xref></italic></bold>. The high-scaling model metamer is perceptually distinct from the target image, and also perceptually distinct from the high-scaling luminance model metamer. The low-scaling model metamer, on the other hand, is difficult to distinguish from the original image (when fixating at the center), but is readily distinguished when one fixates peripherally.</p>
<p>The appearance of these two high-scaling metamers reflects the measurements that the models are matching and the seed images used for synthesis. The luminance model matches average pixel intensity, but has no constraints on spatial frequency, and thus its metamers retain the high frequencies present in the initial white-noise images. The energy model, on the other hand, matches the average contrast energy at all scales and orientations, but discards exact position information (which depends on phase structure). Hence, unlike the luminance model metamers, it reduces the high frequency power to match the typical content of natural images. Instead, it essentially scrambles the phase spectrum, leading to the cloud-like appearance of its metamers.</p>
<p>As can be seen in <xref rid="fig4" ref-type="fig">figure 4</xref>, both models can generate perceptual metamers. More generally, all pooling models can generate perceptual metamers if the scaling value is made sufficiently small (in the limit as scaling goes to zero, the model metamers must be identical to the target, in every pixel). For statistics that capture the relevant features for responses at some stage of the visual system, metamers can be achieved with windows whose size is matched to that of the underlying visual neurons. The maximal scaling at which synthetic images are perceptual metamers is thus highly dependent on the choice of underlying statistics: in our examples, the energy model perceptual metamer (<xref rid="fig4" ref-type="fig">Fig. 4</xref>, bottom left) is generated with a scaling value about six times larger than that for the luminance model perceptual metamer (middle left), and about five times smaller than those found in <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold> using texture statistics. The goal of the present study is to use psychophysics to find the largest scaling value for which these two models generate perceptual metamers, known as the <bold>critical scaling</bold>.</p>
</sec>
<sec id="s2b">
<title>Psychophysical experiment</title>
<p>We synthesized model metamers matching 20 different natural images (the <bold>target images</bold>) collected from the authors’ personal collections, as well as from the UPenn Natural Image Database (<bold><italic><xref ref-type="bibr" rid="c60">Tkačik et al., 2011)</xref></italic></bold>, and an unpublished collection by David Brainard. The images were chosen to span a variety of natural image content types, including buildings, animals, and natural textures (<xref rid="fig5" ref-type="fig">figure 5</xref>). Model metamers were generated via gradient descent on the squared error between target and synthetic pooled statistics, and initialized with either an image of white noise or another image drawn from the set of target images.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Target images used in the experiments. Images contain a variety of content, including textures, objects, and scenes. All are relatively high-resolution RAW camera images, with values proportional to luminance and quantized to 16 bits. Images were converted to grayscale, cropped to 2048 x 2600 pixels, displayed at 53:6 x 42:2 degrees, with intensity values rescaled to lie within the range of [0.05, 0.95] of the display intensities. All subjects saw target images 1-10, half saw 11-15, and half saw 16-20. A full resolution version of this figure can be found on the <ext-link ext-link-type="uri" xlink:href="https://osf.io/sqyh3/">OSF</ext-link>.</p></caption>
<graphic xlink:href="541306v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the experiments, observers discriminated two grayscale images, of size 53.6 by 42.2 degrees, sequentially displayed. Each image was presented for 200 msecs, separated by a 500 msec interval in which the screen was blank (mid gray). Each image was separated into two halves by a superimposed vertical bar (mid gray, 2 deg wide, see <xref rid="fig15" ref-type="fig">figure 15</xref>). One side, selected at random on each trial, was identical in the two images, while the other differed (e.g., one interval contains the target image, the other a synthesized model metamer). After the second image, a midgray screen appeared with text prompting the observer to report which side of the image had changed.</p>
</sec>
<sec id="s2c">
<title>Critical scaling is four times smaller for the luminance than the energy model</title>
<p>We fit the behavioral data using the 2-parameter function introduced in <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold>, estimating the critical scaling (<italic>s<sub>c</sub></italic>) and maximum <italic>d</italic>′ (<italic>α</italic>) parameters with a Markov Chain Monte Carlo procedure and a hierarchical, partial-pooling model similar to that used by <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold>.</p>
<p>For a given model and comparison, performance increases monotonically with scaling, and is fit well by this particular psychometric function (<xref rid="fig6" ref-type="fig">figure 6A</xref>). The exception is the synthesized vs. synthesized comparison for the luminance model, where performance remains poor at all scales (see next section). In the original vs. synthesized cases (for both models), performance is near chance for the smallest tested scaling values tested and exceeds 90% for the largest. The critical scaling values, as seen in <xref rid="fig6" ref-type="fig">figure 6B</xref> are approximately 0.016 for the luminance model and 0.06 for the energy model. For comparison, we show the approximate scaling values for the receptive field diameters of retinal ganglion cells (both Midget, and Parasol), as well as for V1 cells (see appendix 2 for details on retinal ganglion cell scaling, and <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold> for V1 scaling). The critical scaling for the luminance model falls between the two types of retinal ganglion cell. The critical scaling for the energy model is larger than that of both ganglion cell types, and approximately half of the lower end of the V1 range.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Performance and psychophysical curve parameters values values for different models and image comparisons. The luminance model has a substantially smaller critical scaling than the energy model, and original vs. synthesized comparisons yield smaller critical scaling values than synthesized vs. synthesized comparisons. (A) Psychometric functions, expressing probability correct as a function of scaling parameter, for both energy and luminance models (aqua and beige, respectively), and original vs. synthesized (solid line) and synthesized vs. synthesized (dashed line) comparisons. Data points represent average values across subjects and images, 4320 trials per data point except for luminance model synthesized vs. synthesized comparison, which have only 180 trials per data point (one subject, five images). Lines represent the posterior predictive means of fitted curves across subjects and images, with the shaded region indicating the 95% high-density interval (HDI, <bold><italic><xref ref-type="bibr" rid="c36">Kruschke (2015)</xref></italic></bold>). Horizontal bars (below dashed line at 0.5) indicate the range of physiological scaling values for the associated retinal ganglion cell type or cortical area. (B) Estimated parameter values, separated by image (left) or subject (right). Top row shows the critical scaling value and the bottom the value of the maximum <italic>d</italic>′ parameter. Points represent the posterior means, shaded regions the 95% HDI, and horizontal dashed lines and shaded regions the global means and 95% HDI. Note that the luminance model, synthesized vs. synthesized comparison is not shown, because the data are poorly fit (panel A, beige dashed line).</p></caption>
<graphic xlink:href="541306v3_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>Critical scaling is lower for original vs. synthesized than synthesized vs. synthesized comparisons</title>
<p>For both luminance and energy models, it is generally easier to distinguish an original image from a synthesized image than to distinguish two synthesized images initialized with different white noise seeds (with same target image and scaling value). For the luminance model, discrimination of two synthesized images is nearly impossible at all scaling values. For the energy model, discriminating two synthesized images is possible but difficult, with performance only approaching 60%, on average (although note that there are substantial differences across subjects, see <xref rid="fig6" ref-type="fig">figure 6B</xref> and appendix 6). The critical scaling value for this comparison, 0.25, is close to the physiological scaling value estimated for V1, and comparable to that reported in <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold>. The asymptotic performance, however, is much lower in our data. We attribute this to experimental differences (see appendix section 4).</p>
<p>The difficulty of differentiating between two synthesized images is striking, as illustrated in <xref rid="fig7" ref-type="fig">Figure 7</xref>. In the limit of global pooling windows, luminance metamers are global samples of white noise which cannot be distinguished (<bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold> made a similar point when discussing their use of the original vs. synthesized task). Analogously, synthesis with the energy model forces local orientated spectral energy to match, without explicitly constraining the phase. Two instances of phase scrambling within peripheral windows are not easily discriminable, even though either of the two might be discriminable from an image with more structure.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Comparison of two synthesized metamers is more difficult than comparison of a synthesized metamer with the original image. For the highest tested scaling value (1.5) the original vs. synthesized comparison is trivial while the synthesized vs. synthesized comparison is difficult (energy model) or impossible (luminance model). <bold>Top:</bold> target image. <bold>Middle:</bold> Two luminance model metamers, generated from different initial uniform noise images. <bold>Bottom:</bold> Two energy model metamers, generated from different initial uniform noise images. All four of the model metamers can be easily distinguished from the natural image at top (original vs. synthesized), but are difficult to distinguish from each other, despite the fact that their pooling windows have grown very large (synthesized vs. synthesized). Full resolution version of this figure can be found on the <ext-link ext-link-type="uri" xlink:href="https://osf.io/ab6c2/">OSF</ext-link>.</p></caption>
<graphic xlink:href="541306v3_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2e">
<title>The interaction between model sensitivities and image content affects performance</title>
<p>To the extent that the models capture something important about human perception, image pairs that are model metamers will be perceptual metamers, and hence discrimination should be at chance. Neither model offers predictions of perceptual discriminability (they are deterministic, and do not specify any method of decoding or comparing stimuli). Consistent with this, the critical scaling, which measures the point at which image pairs become indistinguishable, does not vary much across images for a given model and comparison, unlike performance at super-threshold scaling values and the asymptotic levels of <italic>d</italic>′ (<xref rid="fig6" ref-type="fig">figure 6B</xref>). Variations in max <italic>d</italic>′ are especially clear in the images-specific psychometric functions for the original vs. synthesized energy model comparison (<xref rid="fig8" ref-type="fig">figure 8</xref>). Specifically, for the llama image, performance only rises slightly above chance, even at very large scaling windows. The respective target images in panel B suggest an explanation: much of the llama image is cloud-like pink noise, while the nyc image is full of sharp edges in the cardinal directions, with arise from precise alignment of phases across positions and scales. As discussed above, synthetic energy model metamers have matching local oriented spectral energy, with randomized phase information; in order to generate sharp, elongated contours for the buildings of nyc, the windows must be very small. Conversely, the appearance of the llama is captured even when the pooling windows are large. Thus, when scaling is larger than critical scaling, some comparisons become easy and some do not. However, this pattern depends on the interaction between the model’s sensitivities and the target image content, see appendix 6 for more details.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><p>The interaction between image content and model sensitivities greatly affects asymptotic performance, most noticeably on the synthesized vs. synthesized comparison for the energy model, while critical scaling does not vary as much. (A) Performance for each image, averaged across subjects, comparing synthesized images to natural images. Most images show similar performance, with one obvious outlier whose performance never rises above 60%. Data points represent the average across subjects, 288 trials per data point for half the images, 144 per data point for the other half. Lines represent the posterior predictive means across subjects, with the shaded region giving the 95% HDI. (B) Example model metamers for two extreme images. The top row (nyc) is the image with the best performance (purple line in panel A), while the bottom row (llama) has the worst performance (red line in panel A). In each row, the leftmost image is the target image, and the next two show model metamers with the lowest and highest tested scaling values for this comparison. Performance on the llama image is poor because much of the image content resembles pink noise. Thus, even with larger scaling values, the model metamers are very difficult to distinguish from the target image. The nyc image, on the other hand, contains hard edges with precise alignment of phase across scales. As the energy model discards phase information, this phase structure is lost in the model metamers, which are consequently easy to distinguish from the target image at all tested scaling values. However, this pattern does not hold in the luminance model, or for synthesized vs. synthesized comparisons, for which both images exhibit typical performance (see appendix figure 6). Full resolution version of this figure can be found on the <ext-link ext-link-type="uri" xlink:href="https://osf.io/tz7dx/">OSF</ext-link></p></caption>
<graphic xlink:href="541306v3_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2f">
<title>When discriminating two synthesized images, the initial image affects performance</title>
<p>The two types of comparisons shown in <xref rid="fig6" ref-type="fig">figure 6</xref> — original vs. synthesized and synthesized vs. synthesized — show very different critical scaling values. This indicates that for a particular scaling value and set of image statistics, some image pairs are much easier to discriminate than others. We hypothesize that metamers synthesized from white noise seeds are restricted to a relatively small region of the full set of model metamers. As a result, these images are more perceptually similar to each other than they are to the target image. To generate metamers outside of this set, we also used other natural images from our data set to initialize the synthesis procedure (which was not done in previous studies, <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold>).</p>
<p><xref rid="fig9" ref-type="fig">Figure 9A</xref> shows behavior for these additional comparisons in a single subject, sub-00. Changing the initialization image has a large effect on the synthesized vs. synthesized comparison but little-to-no effect on the original vs. synthesized comparison. For synthesized vs. synthesized, initializing with a different natural image improves performance compared to initializing with white noise, but is still worse than performance for original vs. synthesized. Importantly, these results cannot be predicted from the model, which gives no specific insight as to why some pairs are more discriminable than others.</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><p>Initializing model metamers with natural images does not affect performance in the original vs. synthesized comparison, but reduces critical scaling and increases max <italic>d</italic>′ for the synthesized vs. synthesized comparison. Note all data in this figure is for a single subject and 15 of the 20 target images. (A) Probability correct for one subject (sub-00), as a function of scaling. Each point represents the average of 540 trials (over all fifteen images), except for the synthesized vs. synthesized luminance model white noise comparison (averaged over 5 images). Vertical black line indicates scaling value where difficulty ran from chance to 100%, based on initialization and comparison, as discussed in panel B. (B) Three comparisons corresponding to the three psychophysical curves intersected by the vertical black line in panel A. See text for details. Full resolution version of this figure can be found on the <ext-link ext-link-type="uri" xlink:href="https://osf.io/qmsj8/">OSF</ext-link>.</p></caption>
<graphic xlink:href="541306v3_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig9" ref-type="fig">Figure 9B</xref> shows three comparisons involving five metamers arising from different initializations, each with scaling corresponding to the vertical line in panel A, but with dramatically different human performance. The top row shows the easiest comparison, between the original image and a synthesized image initialized with a different natural image (bike); the subject was able to distinguish these two images with near-perfect accuracy (in this case, when comparing against a natural image, performance is identical regardless of whether the metamer was initialized with white noise or natural image). The bottom row shows the hardest comparison, between two synthesized images initialized with different samples of white noise. As discussed above, comparing two images of this type is difficult even with large pooling windows; at this scaling level, humans are insensitive to the differences between them, and so performance was at chance. The middle row shows two synthesized images, initialized with different natural images, which the subject was able to distinguish with moderate accuracy. When comparing these two images, one can see features in the periphery that remain from the initial image (tiles and highway, respectively). Even when fixating, the subject was able to use these features to distinguish the two images, i.e., the human was sensitive to them while the model was not. This reinforces the notion that the initialization of the synthesis process matters. In both the middle and bottom row, both images are synthesized (i.e., neither row contains the target image,) yet one comparison is much harder than the other.</p>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure">
<label>Figure 10.</label>
<caption><p>Parameter values for the comparisons shown in <xref rid="fig9" ref-type="fig">figure 9A</xref> (Top: critical scaling value; Bottom: max <italic>d</italic>′). Data shown is from the single subject who completed all comparisons. Points represent the posterior means, shaded regions the 95% HDI, and horizontal dashed lines and shaded regions average across all shown images for this subject. Note that the luminance model, synthesized vs. synthesized: white noise comparison is not shown in this panel, because the data was poorly fit by this curve.</p></caption>
<graphic xlink:href="541306v3_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2g">
<title>The models reach critical scaling at different dimensionality</title>
<p>For each model, the number of statistics is proportional to the number of pooling regions, and thus decreases quadratically with scaling. <xref rid="tbl1" ref-type="table">Table 1</xref> shows average critical scaling values across all conditions, along with the corresponding number of model statistics. We can see that critical scaling does not correspond to a fixed number of statistics. We should also note that if one were to use the model outputs as a compressed representation of the image, the number of statistics in each representation is almost certainly an overcount, for several reasons. First, in order to ensure that the Gaussian pooling windows uniformly tile the image, the most peripheral windows in the model have the majority of their mass off the image, which is necessary to avoid synthesis artifacts. Second, for the energy model, we did not attempt to determine how the precise number of scales or orientations affected metamer synthesis, and currently all scales are equally-weighted across the image. As the human visual system is insensitive to high frequencies in the periphery and low frequencies in the fovea, this is probably unnecessary, and so some of these statistics can likely be discarded. Finally, our pooling windows are highly overlapping and thus the pooled statistics are far from independent; this redundancy means that the effective dimensionality of our model representations is lower than the quoted number of statistics.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>Critical scaling (posterior mean over all subjects and images) and number of statistics (as a percentage of number of input image pixels - a type of “compression rate”), for each model and comparison. Note that the critical scaling for original vs. synthesized comparisons does not result in the same number of statistics across models and, in particular, at their critical scaling values, all models have dimensionality less than that of the input image.</p></caption>
<graphic xlink:href="541306v3_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We measured perceptual discriminability of wide field-of-view metamers of two foveated pooling models of human vision. We found that performance depended on the model type (i.e., the type of image statistics that are pooled), the nature of the comparison (original vs. synthesized, or synthesized vs. synthesized), the seed image used for synthesis, and to a modest extent, the natural image target. Specifically, we found that critical scaling was much smaller for the luminance than for the energy model, and much smaller for original vs. synthesized than for synthesized vs. synthesized comparisons. For the former, we also found smaller critical scaling values when the synthesis was initialized from another natural image rather than white noise.</p>
<sec id="s3a">
<title>The linking proposition underlying the metamer paradigm</title>
<p>In <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold>, the authors propose a link between perceptual pooling models of the type found in this study and visual system physiology. They hypothesize that the critical scaling reflects the receptive field sizes of visual areas representing the corresponding statistics. Using this logic, they found a critical scaling for their texture model of approximately 0.5, and interpreted this as evidence of texture representation in V2, which has receptive field sizes with similar scaling.</p>
<p>This logic follows the “Converse Identity” linking proposition in the framework proposed by <bold><italic><xref ref-type="bibr" rid="c58">Teller (1984)</xref></italic></bold>: identical perceptual states imply identical physiological states (at some stage of visual processing, and thereafter). Model-generated metamers provide an accessible experimental extension of this logic: at the critical scaling value, identical model outputs imply identical perceptual states, which imply identical physiological states. However, when attempting to link the critical scaling of a pooling model to the physiological scaling of a corresponding brain area, it is important to remember that receptive field size is a function of not just the visual area, but is also influenced by at least the cell class, cortical layer, mapping method, and stimulus type. Therefore, a visual area cannot be fully characterized by a single scaling value. This complicates the task of linking the psychophysical scaling value to the neural scaling value at a particular stage of visual processing. Hence we focus here on how the results vary with the type of model and type of comparison, but we do not seek to unambiguously assign a model to a stage of visual processing.</p>
</sec>
<sec id="s3b">
<title>Visual computation: cascades of feature extraction and local pooling</title>
<p>Visual representations are formed through a cascade of transformations. An appealing hypothesis is that each of these is comprised of the same canonical “feature extraction and blur” computation, differing only in what is extracted and the spatial extent of the blurring (<bold><italic><xref ref-type="bibr" rid="c22">Fukushima, 1980</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c17">Douglas et al., 1989</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c38">LeCun et al., 1989</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c26">Heeger et al., 1996</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c51">Riesenhuber and Poggio, 1999</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c11">Bruna and Mallat, 2013)</xref></italic></bold>. In the perception literature, <bold><italic><xref ref-type="bibr" rid="c39">Lettvin (1976)</xref></italic></bold> provides an early, informal discussion of this “compulsory feature averaging”, non-foveated versions have been described in <bold><italic><xref ref-type="bibr" rid="c43">Parkes et al. (2001)</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c46">Pelli et al. (2004)</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c25">Greenwood et al. (2009)</xref></italic></bold>, and foveated proposals appear in <bold><italic><xref ref-type="bibr" rid="c4">Balas et al. (2009)</xref></italic></bold> and <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold>. In these, as in the current article, the stages are distinguished by their features, and the scaling of the pooling regions with eccentricity. The optics and photoreceptors pool the incident light over a small regions, V1 pools spectral energy over larger regions, V2 pools texture-like statistics over yet larger regions, and so forth. Consistent with this view, we find that for synthetic metamer stimuli, the pooled model statistic has a dramatic effect on the critical scaling value, which is approximately four times larger for the energy model than for the luminance model in the original vs. synthesized comparison (<xref rid="fig6" ref-type="fig">figure 6</xref> and <xref rid="tbl1" ref-type="table">table 1</xref>). This result is consistent across all subjects and all target images (<xref rid="fig6" ref-type="fig">figure 6B</xref>). The relationship between critical scaling and image statistic can be further appreciated by comparing these results to the model of <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold>, which was also fit to original vs. synthesized discrimination data: the critical scaling of our average energy model is about three times smaller than the average value for their texture model. Together, the three results show critical scaling ratios of approximately 1:4:12 for features corresponding to luminance:spectral-energy:texture, respectively (solid circles in <xref rid="fig11" ref-type="fig">figure 11</xref>).</p>
<fig id="fig11" position="float" orientation="portrait" fig-type="figure">
<label>Figure 11.</label>
<caption><p>Critical scaling values for the two pooling models presented in this paper (Luminance and Energy) and the Texture model (originally tested in <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold>, data from <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold>, averaging across the two image classes). Solid points indicate the original vs. synthesized white noise comparisons, while hollow points indicate synthesized vs. synthesized white noise comparisons (for the luminance model, this is effectively infinite, since participants were unable to complete the task). For all three models, critical values are smaller in the original vs. synthesized comparison than the synthesized vs. synthesized one, and their ratio decreases with increasing complexity of image statistics. A potential explanation for this is that the more complex models approximate computations performed in deeper levels of the visual hierarchy, beyond which there are fewer remaining stages to discard information.</p></caption>
<graphic xlink:href="541306v3_fig11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3c">
<title>Why does critical scaling depend on the comparison being performed?</title>
<p>We found large effects of comparison type on performance, consistent with those reported in <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold> and <bold><italic><xref ref-type="bibr" rid="c16">Deza et al. (2019)</xref></italic></bold>. For the energy model, with synthesis initialized from white noise, the critical scaling for synthesized vs. synthesized was about four times larger than that for original vs. synthesized. The maximum discriminability was also much lower for the former than the latter. For the luminance model, the difference was even more dramatic: participants were generally unable to discriminate any pairs for the synthesized vs. synthesized comparison. Differences between the two types of comparisons, which are summarized in <xref rid="fig11" ref-type="fig">figure 11</xref>, arise from an interaction between human perception, the model, and the synthesis process. Here we consider two abstract scenarios, one in which the comparison does matter and one in which it does not, in order to illustrate this interaction.</p>
<p>In the left panel of <xref rid="fig12" ref-type="fig">figure 12</xref>, the comparison does not matter: model metamers are distinguishable from each other <italic>if and only if</italic> they are distinguishable from the target image. The right panel illustrates a configuration in which this condition does not hold: two synthetic images can be indistinguishable, even though each are distinguishable from the target image. The idealized version implicitly assumes that the synthesized images sample the manifold of possible model metamers broadly, but our synthesis procedure (similar to those of <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold> and <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold>) does not guarantee this. Initialization with natural images provides an intuitive, but ad-hoc, method of sampling from a broader portion of the manifold of possible model metamers, and results in smaller critical scaling values. More principled statistical sampling approaches (e.g., Markov Chain Monte Carlo) could result in more representative metamers.</p>
<fig id="fig12" position="float" orientation="portrait" fig-type="figure">
<label>Figure 12.</label>
<caption><p>In the idealized version of the metamer paradigm, synthesized images broadly sample the space of possible model metamers. Thus, at large scaling values, synthesized images are distinguishable from each other <italic>and</italic> the target image, and they become indistinguishable from each other at the same scaling value for which they are indistinguishable from the target (left panel). In this case, model metamers are metameric with each other if and only if they are metameric with the target image. In our experements, however, this is not the case: at some scaling values, two synthesized images can be metameric with each other, but distinguishable from the target image (right panel).</p></caption>
<graphic xlink:href="541306v3_fig12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3d">
<title>Why does this critical scaling discrepancy decrease with increasing feature complexity?</title>
<p>As seen in <xref rid="fig11" ref-type="fig">figure 11</xref>, the difference in critical scaling between the two types of comparisons declines as the image statistics being pooled become more complex. While the original vs. synthesized critical scaling value is lower for all three models, the gap between the two decreases as the models increase in complexity: infinite for the luminance model, roughly quadruple for energy, and less than double for texture (see <xref rid="fig11" ref-type="fig">figure 11</xref>).</p>
<p>One potential explanation for this observation is that these computations are being performed in deeper stages of the visual hierarchy and there are progressively fewer opportunities to discard information later in the hierarchy. For example, the difference in V1 responses for a pair of images may be discarded by a later stage (e.g., area IT), but there are not many steps of processing between IT and the perceptual read out where differences between IT responses can be discarded. This may explain why we see no overlap between the critical scaling values for original vs. synthesized and synthesized vs. synthesized comparisons across images in <xref rid="fig6" ref-type="fig">figure 6B</xref>, whereas <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold> find substantial overlap for the texture model. Ultimately, these possibilities can only be distinguished through further physiological measurements.</p>
<p>Another potential explanation is schematized in <xref rid="fig13" ref-type="fig">figure 13</xref>: for the luminance model, the model metamer classes are aligned with the perceptual metamer classes and the white noise samples such that, for a given scaling value, all model metamers initialized with white noise fall into the same perceptual metamer class. This is an extreme case of the synthesis issue described in the previous section. On the other hand, the texture model’s metamer classes are orthogonal to the white noise samples, so that synthesized model metamers easily fall into distinct perceptual metamer classes, leading to a critical scaling value that is much more similar to the value for the original vs. synth comparison.</p>
<fig id="fig13" position="float" orientation="portrait" fig-type="figure">
<label>Figure 13.</label>
<caption><p>The difference between the synth vs. synth and original vs. synth white noise comparisons decreases as model complexity increases (compare with <xref rid="fig12" ref-type="fig">figure 12</xref>). Because of the alignment between the luminance model metamer classes, the underlying perceptual metamer classes, and the white noise samples used to initialize the synthesis process, synthesized images <italic>always</italic> lie within the same perceptual metamer class and are thus never distinguishable. On the other hand, the texture model’s metamer classes grow orthogonally to the white noise samples, resulting in a much smaller critical scaling value for the synth vs. synth white noise comparison.</p></caption>
<graphic xlink:href="541306v3_fig13.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3e">
<title>Why and when does the critical scaling depend on synthesis initialization?</title>
<p>The previous sections show that the critical scaling depends on the comparison type and feature complexity. We also find that it depends on synthesis initialization. Natural images are more likely than synthetic images to include information that the human visual system has evolved to discriminate, as opposed to information that is discarded at some later stage of processing. Using natural images to initialize synthesis (or development of novel synthesis methods to better explore the space of metamers) may reduce the discrepancy between the two conditions.</p>
<p>The schematic presented in <xref rid="fig14" ref-type="fig">figure 14</xref> provides a potential explanation for why critical scaling values depend on the comparison and synthesis initialization. Ideally, synthesized metamers are discriminable from each other if and only if they are also discriminable from the target, as described in section Why does critical scaling depend on the comparison being performed?. The variability in critical scaling across comparisons indicates that this condition is being violated: the synthesized vs. synthesized conditions have a higher critical scaling value indicates that the synthesized images used in those comparisons are falling into the same metamer class. This effect is reduced when initializing with a natural image.</p>
<fig id="fig14" position="float" orientation="portrait" fig-type="figure">
<label>Figure 14.</label>
<caption><p>Schematics describing results presented in this paper. Unlike the idealized metamer paradigm (see section Why does critical scaling depend on the comparison being performed? and <xref rid="fig12" ref-type="fig">figure 12A</xref>), the critical scaling value for our models depends on the comparison being performed and, to a lesser extent, the image used to initialize synthesis. For the original vs. synthesized comparison, this does not affect the critical scaling value, as the synthesized images are always distinct from the target. However, initializing with white noise can result in synthesized images that lie in the same metamer class as each other even while they are distinct from the target, resulting in a relatively large scaling value for the synthesized vs. synthesized comparison. Initializing with natural images reduces the magnitude of this phenomenon.</p></caption>
<graphic xlink:href="541306v3_fig14.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>This suggested the noise-initialized algorithm produces a biased sampling of the space of all possible model metamers. For each target image, model, scaling value, and initialization condition (white noise or natural image), we generated three model metamers. If these were distributed across the space of all possible model metamers, we would not see the dependence on initialization depicted in <xref rid="fig14" ref-type="fig">figure 14</xref>. Initializing with a natural image was one attempt to sample a different portion of this space, and it did reduce the discrepancy between comparison conditions. However, further work needs to be done to better understand the effects of initialization on generated samples. One possibility that seems promising would be to synthesize model metamers for a given target image, model, and scaling value in sets that are as different from each other as possible, quantified using different pooling models, other visual models, or image quality metrics. We believe the metamer paradigm can be made more informative by paying more attention to the synthesis procedure.</p>
<p>We believe a more extreme version of this situation is reflected in the data for the luminance model: the synth vs. synth comparison is <italic>never</italic> possible when initializing model metamers with white noise (so we cannot estimate the model’s critical scaling), but initializing with natural images does allow these model metamers to be distinguished from each other (see appendix 1). We believe this reflects an extreme version of the biased sampling discussed above: the local luminance matching enforced by the luminance model is a fairly lax constraint and thus initializing with white noise leads to model metamers that consistently lie within the same perceptual metamer class, while at the same time being very discriminable from the target image (see <xref rid="fig7" ref-type="fig">figure 7</xref>).</p>
<p>By gathering discriminability data for different types of comparisons, we are able to get a better sense of how the metamer classes of our pooling models align with those of the visual system. Our current data supports the proposal from <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold> that the critical scaling should be estimated using only the original vs. synthesized comparison, as those values are minimal across comparisons and do not seem to depend on how metamer synthesis is initialized. Furthermore, there are many applications where that is the only comparison that matters (e.g., for generating images that must be indistinguishable from natural images only). However, it is not clear that this will always be the case: while we never sampled from the metamer class containing the target image while the critical scaling value was large, it is theoretically possible, and additional comparisons can provide more protection against over-interpretation results stemming from biased sampling.</p>
</sec>
<sec id="s3f">
<title>Asymptotic performance, but not critical scaling, depends on image content</title>
<p>Similar to <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c10">Brown et al. (2021)</xref></italic></bold>, we find that metamer discrimination performance is somewhat dependent on image content. Both of those studies synthesize model metamers based on pooled texture statistics, and <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold> shows that texture-like original images are harder to distinguish from their synthesized images than scene-like ones, while <bold><italic><xref ref-type="bibr" rid="c10">Brown et al. (2021)</xref></italic></bold> show that original textures with higher global and local regularity (e.g., woven baskets) are the easiest to distinguish from their synthesized images than those with low regularity (e.g., animal fur). This aligns with our result: the most distinguishable pairs include natural images with features not well-captured by the synthesizing model, whereas the least distinguishable include those natural images whose features are all adequately captured.</p>
<p>However, we should note that we found this image-level variability largely in super-threshold performance, and this variability does not constitute a failure of these pooling models. As pointed out by <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold>, asymptotic performance also varies with experimental manipulation, while critical scaling remains relatively unaffected. The metamer paradigm makes strong predictions about what happens when the representation of two stimuli are matched: they are indistinguishable, and so performance on a discrimination task will be at chance, as captured by the critical scaling value. However, it makes <italic>no</italic> predictions about performance at super-threshold levels, as captured by the max <italic>d</italic>′ parameter. An analogy with color vision seems apt: color matching experiments provide evidence for what spectral distributions of light are perceived as identical colors, but provide no information about whether humans consider blue more similar to green or to red; further investigations are necessary to understand color appearance. Thus, while this image-level variability is worth investigating in order to better understand the sensitivities of our model, it does not much affect the inferences we want to make about the human visual system, and speaks more to the need for a complementary approach (see next section).</p>
</sec>
<sec id="s3g">
<title>Observer models are needed to predict discriminability of non-metameric images</title>
<p>As discussed above, the metamer paradigm’s converse identity linking proposition is silent on what is implied by <italic>distinct</italic> model outputs and so a complementary approach is required, such as building observer models to predict perceptual distances. Specifically, the metamer paradigm makes no predictions about discriminability for model metamers synthesized with a super-critical scaling value. Such images are discriminable to the synthesizing pooling model at critical scaling. However, the information distinguishing them might be discarded by later stages of visual processing (center panel, <xref rid="fig1" ref-type="fig">figure 1</xref>). A complementary approach investigating how such differences are handled by later brain areas is necessary to gain a better understanding of image discriminability beyond “identical or not”. Such work could use the models presented here as a starting point and could draw on the substantial literature of observer models in vision science and image processing.</p>
<p>There are several important properties of the pooling models used in metamer studies that should be revisited if attempting to extend them into observer models. First, the models assume equal sensitivity to all statistics, and that the sensitivity to these statistics does not change across the visual field. Second, the models assume that every statistic is pulled in regions of equal size (e.g., that high frequency is pooled over the same size region as low frequency). Finally, the shape of the pooling windows (e.g., whether windows should be radially elongated with a 2:1 ratio) and their scaling with eccentricity can probably benefit from refinement. When performing the task (especially the original vs. synthesized comparisons), subjects reported that the most informative portions of the image were in the mid-periphery, rather than close to fixation or at the edges of the image. This suggests that the windows may be too large in the mid-periphery, and that window width may be better modeled in a non-linear manner.</p>
<p>The investigation of super-threshold performance and appearance is a necessary complement to the metamer paradigm, which focuses on the question of whether two images are perceptually identical or not. Whereas observer models and image quality metrics often rely on natural images, common experimental stimuli, or sets of common distortions, the metamer paradigm relies on the synthesis of novel images, often turning up unexpected exemplars. Combining and extending this synthesis-focused approach with an attention to super-threshold performance would help lead to a fuller understanding of human perceptual sensitivities and insensitivities.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<p>All experimental materials, data, and code for this project are available online under the MIT or similarly permissive licenses. Specifically, software is on <ext-link ext-link-type="uri" xlink:href="https://github.com/billbrod/foveated-metamers/">GitHub</ext-link>, synthesized metamers can be browsed on <ext-link ext-link-type="uri" xlink:href="https://users.flatironinstitute.org/~wbroderick/metamers/">this website</ext-link>, and all images and data can be downloaded from the <ext-link ext-link-type="uri" xlink:href="https://osf.io/67tbe/">OSF</ext-link>. The GitHub site provides instructions for downloading and using data.</p>
<sec id="s4a">
<title>Synthesis</title>
<p>We synthesized model metamers matching 20 different natural images (the target images) from the authors’ (W.F.B and E.P.S) personal collections, as well as from the UPenn Natural Image Database (<bold><italic><xref ref-type="bibr" rid="c60">Tkačik et al., 2011)</xref></italic></bold> and from an unpublished collection by David Brainard. The selected photos were high-resolution with 16-bit pixel intensities proportional to luminance, that had not undergone lossy compression (which could result in artifacts). They were converted to grayscale using scikit-image’s <monospace>color.rgb2gray</monospace> function (<bold><italic><xref ref-type="bibr" rid="c66">van der Walt et al., 2014)</xref></italic></bold>, cropped to 2048 by 2600 pixels (the Brainard photos were 2014 pixels tall, so a small amount of reflection padding was used to reach 2048 pixels), and had their pixel values rescaled to lie between 0.05 and 0.95. Synthesized images were still allowed to have pixel values between 0 and 1; without rescaling the target images, synthesis resulted in strange artifacts with pixels near 0, as this was the minimum allowed value. The images were chosen to span a variety of natural image content types, including buildings, animals, and natural textures (see <xref rid="fig5" ref-type="fig">figure 5</xref>).</p>
<p>We synthesized the model metamers using custom software written in PyTorch (<bold><italic><xref ref-type="bibr" rid="c44">Paszke et al., 2019)</xref></italic></bold>, using the AMSGrad variant of the Adam optimization algorithm (<bold><italic><xref ref-type="bibr" rid="c34">Kingma and Ba, 2014</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c50">Reddi et al., 2018)</xref></italic></bold>, with learning rate 0.01. Slightly different approaches were used for the luminance and energy model metamers. For the luminance model metamers, the objective function was to minimize the mean-squared error between the model representation of the target and synthesized images, <inline-formula><inline-graphic xlink:href="541306v3_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and synthesis was run for 5000 iterations. For the energy model metamers, the objective function also contained a quadratic range penalty term, which penalized any pixel values outside of [0, 1], <inline-formula><inline-graphic xlink:href="541306v3_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where
<disp-formula>
<graphic xlink:href="541306v3_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<p>Synthesis was run for 15000 iterations. Additionally, energy model metamer synthesis used stochastic weight averaging (<bold><italic><xref ref-type="bibr" rid="c31">Izmailov et al., 2018)</xref></italic></bold>, which helped avoid local optima by averaging over pixel values as synthesis neared convergence, and used coarse-to-fine optimization (<bold><italic><xref ref-type="bibr" rid="c48">Portilla and Simoncelli, 2000)</xref></italic></bold>. Additionally, each statistic (in both models) was z-scored using the average statistic value computed across the entire image on a selection of grayscale texture images. For both models, synthesis terminated early if the loss had not decreased by more than 1<italic>e</italic> − 9 over the past 50 iterations. While not all model metamers achieved the same loss values, with differences in synthesis loss across target images, there was no relationship between the remaining loss and behavioral performance.</p>
<p>For each model, its windows were represented as two tensors, one for angular slices and one for annuli, which, when multiplied together, would give the individual windows, with separate sets of windows for each scale in the energy model. This required a large amount of memory, and so for scaling values below 0.09, models were too large to perform synthesis on the available NVIDIA GPUs with 32GB of memory. Thus, all luminance model metamers were computed on the CPU, and synthesis of a single image took from about an hour for scaling 1.5 to 2 days for scaling 0.058 to 14 days for scaling 0.01. For the energy model metamers, the lowest two scaling values were computed on the CPU, with synthesis taking about a week. For those energy model metamers which were able to be computed on the GPU, synthesis took from 5 hours for scaling 0.095 to 1.5 hours for scaling 0.27 and above. This synthesis procedure was completed in parallel using the high-performance computing cluster at the Flatiron Institute.</p>
<p>Synthesized images for original vs. synthesized and synthesized vs. synthesized white noise comparisons (see Psychophysical experiment) were initialized with full-field patches of white noise (each pixel sampled from a uniform distribution between 0 and 1). For each model, scaling value, and target image, three different initialization seeds were used. A unique set of three seeds was used for each scaling value and target image, except for the following, which all used seeds {0, 1, 2}:
<list list-type="bullet">
<list-item><p>Luminance model: azulejos, bike, graffti, llama, terraces, tiles; scaling 0.01, 0.013, 0.017, 0.021, 0.027, 0.035, 0.045, 0.058, 0.075 and 0.5.</p></list-item>
<list-item><p>Energy model: azulejos, bike, graffti, llama, terraces, tiles; scaling 0.095, 0.12, 0.14, 0.18, 0.22,0.27, 0.33, 0.4, and 0.5.</p></list-item>
</list></p>
<p>For original vs. synthesized and synthesized vs. synthesized natural image comparison, synthesized images for each model, scaling value, and target image were initialized with three random choices from among the rest of the target images.</p>
</sec>
<sec id="s4b">
<title>Pooling windows</title>
<p>Pooling model windows are laid out in a log-polar grid, with peaks spaced one standard deviation apart, such that adjacent window functions cross at a value of 0.352 (relative to max of 0.4). They have a single parameter, scaling, which specifies the ratio of the pooling window diameter at full-width half-max in the radial direction and the distance of its center from the fovea, both in degrees. For example, the pooling windows of a model with scaling factor 0.1 have a radial diameter of 1 degree at 10 degrees eccentricity, 2 at 20 degrees, etc.</p>
<p>Image pixels within 0.5 degree from the fixation point were exactly matched in our synthesized images, approximating the fovea, where no pooling occurs. Additionally, for small scaling values, windows for some distance beyond this region would be smaller than a pixel and so the only solution is to match the pixel values in that region directly. For example, with image resolution of 2048 by 2600 and display size of 53.6 by 42.2 degrees, models with scaling value of 0.063 have windows whose diameter at FWHM is smaller than a pixel out to 0.52 degrees, with this number increasing quadratically as scaling decreases, reaching 3.29 degrees for scaling 0.01 (see 5 for more discussion).</p>
</sec>
<sec id="s4c">
<title>Observers</title>
<p>Eight participants (5 women and 3 men, aged 24 to 33), including an author (W.F.B.), participated in the study and were recruited from New York University. All subjects had normal or corrected-to-normal vision. Each subject completed nine one-hour sessions. One subject (sub-00) also performed seven additional sessions. All subjects provided informed consent before participating in the study. The experiment was conducted in accordance with the Declaration of Helsinki and was approved by the New York University ethics committee on activities involving human subjects.</p>
</sec>
<sec id="s4d">
<title>Psychophysical experiment</title>
<p>A psychophysical experiment was run in order to determine which of the synthesized model metamers were also perceptual metamers. We first describe the structure of a single trial, then how the trials were organized into blocks and sessions.</p>
<sec id="s4d1">
<title>Trial structure</title>
<p>See <xref rid="fig15" ref-type="fig">figure 15</xref> for schematic. Observers viewed a series of grayscale images on a monitor, at a size of 53.6 by 42.2 degrees. An initial image, divided in half by a vertical midgray bar 2 degrees wide, was displayed for 200 msecs, before being replaced by a midgray screen for 500 msecs, followed by a second image for another 200 msecs (also divided by a vertical midgray bar). Images were presented for 200 msecs to minimize the possibility of eye movements. The dividing bar prevented participants’ use of discontinuities between the two image halves to perform the task. One side of the second image (left half or right half) was identical to the first image, and the other side changed. After the second image was viewed, a midgray screen appeared with text prompting a response, and the observer’s task was to report which half had changed. The observer had as much time as necessary to respond. The two compared images were either two synthesized images (synthesized for identical models with the same scaling value and target image, but different initializations) or one synthesized image and its target image. Either image could be presented first.</p>
<fig id="fig15" position="float" orientation="portrait" fig-type="figure">
<label>Figure 15.</label>
<caption><p>Schematic of psychophysics task. Top shows the structure for a single trial: a single image is presented for 200 msec, bisected in the center by a gray bar, followed by a blank screen for 500 msec. The image is re-displayed, with a random half of the image changed to the comparison image, for 200 msec. The participants then have as long as needed to say which half of the image changed, followed by a 500 msec intertrial interval. Bottom table shows possible comparisons. In original vs. synthesized, one image was the target image whose model representation the synthesized images match (see <xref rid="fig5" ref-type="fig">figure 5</xref>), and the other was one such synthesized image. In synthesized vs. synthesized, both were synthesized images targeting the same original image, with the same model scaling, but different initialization. In experiments, dividing bar, blanks, and backgrounds were all midgray. For more details see text.</p></caption>
<graphic xlink:href="541306v3_fig15.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The midgray blank screen presented between the two image presentations reduces motion cues participants could use to discriminate the two images. Our models aim to capture the steady state response to the images, not the transient response. The mask forces the participants to use the image content to discriminate between the two images, rather than relying on temporal edges (analogous to our use of the vertical bar to prevent the use of spatial edges). This introduces a short-term memory component in the task (participants must remember the first image in order to compare it to the second image), as in previous metamer discrimination experiments (<bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli, 2011</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c16">Deza et al., 2019</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al., 2019)</xref></italic></bold>. We believe the precise duration of this mask is unimportant for our results: first, <bold><italic><xref ref-type="bibr" rid="c6">Bennett and Cortese (1996)</xref></italic></bold> found the duration of a blank screen did not affect thresholds in a spatial frequency discrimination task over a range from 200 to 10,000 msec, and second, mask duration is likely to have a similar effect on performance as image presentation duration, which <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold> found affected asymptotic performance but not critical scaling.</p>
</sec>
<sec id="s4d2">
<title>Session and block organization</title>
<p>Across 9 sessions, each subject completed a total of 12,960 trials, factored into 3 model/comparison combinations by 8 scaling values by 15 target images by 36 repetitions. The 3 model/comparison combinations were 1) luminance model, original vs. synth, white noise; 2) energy model, original vs. synth, white noise; and 3) energy model, synth vs. synth, white noise. The 8 scaling values were logarithmically spaced, with the range chosen separately for each model/comparison to span an appropriate range of values. For luminance model, original vs. synth, white noise, the scaling endpoints were 0.01 and 0.058; for energy model, original vs. synth, white noise, the endpoints were 0.063 and 0.27; and energy model, synth vs. synth, white noise, the endpoints were 0.27 and 1.5. There were a total of 20 target images, but each subject only saw 15. Every subject saw images 1 through 10. Half the subjects also saw images 11 through 15, and half saw images 16 through 20 (see <xref rid="fig5" ref-type="fig">figure 5</xref>). The 36 repetitions were averaged for analysis and included 12 trials for each of 3 synthesis seeds. For the white noise-initialized comparisons, these seeds were independent samples of white noise used to initialize the synthesis procedure, resulting in 3 distinct model metamers.</p>
<p>Each of the above model/comparisons was tested across 3 sessions, each lasting approximately one hour. Each subject started with either the luminance or energy model, original vs. synth, white noise. The 3 sessions required for the model/comparison tested first were completed before moving onto 3 sessions testing the other model. The order of the two models was randomized across subjects. After completing these 6 sessions, the subject completed 3 sessions testing the energy model, synth vs. synth, white noise. This comparison was last as it was the most difficult.</p>
<p>Each of the 9 sessions consisted of 1,440 trials, containing all 36 repetitions for all 8 scaling values for 5 of the 15 target images viewed by the subject (target images were randomly assigned to sessions, independently for each subject). The 1,440 trials per session were broken up into 5 blocks of 288 trials each. Each block took about 8 to 12 minutes, and consisted of 12 repetitions for all 8 scaling values for 3 of the 5 target images.</p>
<p>In addition, one subject (sub-00) completed 7 additional sessions (10,080 additional trials). This included 1 session for luminance model, synth vs. synth, white noise; 3 for energy model, original vs. synth, natural image; and 3 for energy model, synth vs. synth, natural image. As with the comparisons that all subjects completed, these sessions each included 1,440 trials, factored into 5 target images by 8 scaling values by 36 repetitions. Only 1 session was included for luminance model, original vs. synth, white noise because performance was at chance for all images and all scaling values (see <xref rid="fig6" ref-type="fig">figures 6</xref> and <xref rid="fig7" ref-type="fig">7</xref>). No sessions were completed for the luminance model, natural image comparisons due to the time required for synthesis; see appendix section 1 for more information.</p>
<p>The four types of comparisons are explained in full below:
<list list-type="order">
<list-item><p>Original vs. synthesized, white noise: the two images being compared were always one synthesized image and its target image, and the synthesized image was initialized with a sample of white noise.</p></list-item>
<list-item><p>Synthesized vs. synthesized, white noise: both images were synthesized, with the same model, scaling value, and target image, but different white noise seeds as synthesis initialization.</p></list-item>
<list-item><p>Original vs. synthesized, natural image: the two images being compared were always one synthesized image and its target image, and the synthesized image was initialized with a different natural image drawn randomly from our set.</p></list-item>
<list-item><p>Synthesized vs. synthesized, natural image: both images were synthesized, with the same model, scaling value, and target image, but initialized with different natural images from our set.</p></list-item>
</list>
Subjects completed several training blocks. Before their first session, they completed an initial training block, comparing two natural images and two noise samples (one white, one pink). Before their first session of each comparison type including a natural image, they completed a secondary training block showing two natural images and two synthesized images of the type included in the session, one with the largest scaling included in the task and one with the smallest. Before the session comparing two synthesized images, they similarly completed a training block comparing four synthesized images, two with a low scaling value and two with a high scaling value, for each of two target images. Each training block took one to two minutes and was repeated if performance on the high scaling synthesized images was below 90% or subjects expressed uncertainty about their ability to perform the task (participants were expected to perform close to chance for the low scaling synthesized images). Additionally, before each session which included a natural image (the original vs. synthesized comparisons), subjects were shown the five natural images that would be part of that session, as well as two example synthesized images per target image, one with a low scaling value, one with a high scaling value. Before each session comparing two synthesized images (the synthesized vs. synthesized comparison), subjects were shown four example synthesized images per target image, two with the lowest scaling value and two with the highest scaling value for that comparison. A video of a single energy model training block, original vs. synthesized: white noise comparison, can be found on the <ext-link ext-link-type="uri" xlink:href="https://osf.io/7vm43/">OSF</ext-link>.</p>
</sec>
</sec>
<sec id="s4e">
<title>Apparatus</title>
<p>The stimuli were displayed on an Eizo CS2740 LED flat monitor running at 60 Hz with resolution 3840×2160. The monitor was gamma-corrected to yield a linear relationship between luminance and pixel value. The maximum, minimum, and mean luminances were 147.73, .3939, and 77.31 cd/m<sup>2</sup>, respectively.</p>
<p>The experiment was run with a viewing distance of 40 cm, giving 48.5 pixels per degree of visual angle. A chin and forehead rest was used to maintain head position, but the subjects’ eyes were not tracked.</p>
<p>The experiment was run using custom code written in Python 3.7.0 using PsychoPy 3.1.5 (<bold><italic><xref ref-type="bibr" rid="c45">Peirce et al., 2019)</xref></italic></bold>, run on an Ubuntu 20.04 LTS desktop. A button box was used to record the psychophysical response data. All stimuli were presented as 8-bit grayscale images.</p>
</sec>
<sec id="s4f">
<title>Data analysis</title>
<p>All trials were analyzed, a total of 4,320 trials per subject per model per comparison (across 15 images and 8 scaling values) for all energy model comparisons and for luminance model original vs. synthesized white noise comparison. Luminance model synthesized vs. synthesized, white noise comparison had 1,440 trials (across 5 images and 8 scaling values) for a single subject. Where behavioral data is plotted in this paper, the proportion correct is the average across all relevant trials.</p>
<p>We fit psychophysical curves describing proportion correct as a function of model scaling using the two-parameter function for discriminability <italic>d</italic>′ derived in <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold>:
<disp-formula id="eqn1">
<graphic xlink:href="541306v3_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>s<sub>c</sub></italic> is the critical scaling value (performance is at chance for scaling values at or below <italic>s<sub>c</sub></italic>) and <italic>α</italic> is the max <italic>d</italic>′ value (called the “proportionality factor” in <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold>).</p>
<p>Psychophysical curves were constructed by converting this <italic>d</italic>′ into the probability correct using the same function as in <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold>:
<disp-formula id="eqn2">
<graphic xlink:href="541306v3_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where ϕ is the cumulative of the normal distribution. The probability correct is 50% when <italic>d</italic>′ = 0 (and thus when scaling is at or below the critical scaling), reaches about 79% when <italic>d</italic>′ = 2 and 98% when <italic>d</italic>′ = 4. As the <italic>α</italic> parameter above gives the maximum <italic>d</italic>′ value, it has a monotonic relationship with the asymptotic performance, which can be seen in <xref rid="fig16" ref-type="fig">figure 16</xref>.</p>
<fig id="fig16" position="float" orientation="portrait" fig-type="figure">
<label>Figure 16.</label>
<caption><p>Relationship between the max <italic>d</italic>′ parameter, <italic>α</italic> and asymptotic performance. As max <italic>d</italic>′ increases beyond approximately 5 (where asymptotic performance is at ceiling), the slope of the psychophysical curve continues to increase (for example, compare the slope of the luminance and energy model original vs. synth white noise comparisons in <xref rid="fig9" ref-type="fig">figure 9A</xref>).</p></caption>
<graphic xlink:href="541306v3_fig16.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The posterior distribution over parameters <italic>s<sub>c</sub></italic> and <italic>α</italic> was estimated using a hierarchical, partial-pooling model, with independent subject- and image-level effects for both <italic>s<sub>c</sub></italic> and <italic>α</italic>, with each model and comparison estimated separately, following the procedure used in <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold>. Subject responses were modeled as samples from a Bernoulli distribution with probability (1 − <italic>π</italic>)<italic>P</italic>(<italic>s</italic>)+.5<italic>π</italic>, where <italic>π</italic> is the lapse rate, estimated independently for each subject. Estimates were obtained using a Markov Chain Monte Carlo (MCMC) procedure written in Python 3.7.10 (<bold><italic><xref ref-type="bibr" rid="c61">Van Rossum and Drake, 2009)</xref></italic></bold> using the numpyro package, version 0.8.0 (<bold><italic><xref ref-type="bibr" rid="c47">Phan et al., 2019</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c7">Bingham et al., 2018)</xref></italic></bold>. MCMC sampling was conducted using the No U-Turn Sampler algorithm (<bold><italic><xref ref-type="bibr" rid="c28">Hoffman and Gelman, 2014)</xref></italic></bold>, with parameters selected to ensure convergence, which was assessed using the <inline-formula><inline-graphic xlink:href="541306v3_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> statistics (<bold><italic><xref ref-type="bibr" rid="c9">Brooks and Gelman (1998)</xref></italic></bold>, looking for <inline-formula><inline-graphic xlink:href="541306v3_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, <bold><italic><xref ref-type="bibr" rid="c63">Vehtari et al. (2021)</xref></italic></bold>) and by examining traceplots.</p>
<p>Parameters were given weakly-informative priors and both <italic>s<sub>c</sub></italic> and <italic>α</italic> were estimated on natural logarithmic scales.</p>
<p>In sum, for model <italic>m</italic> ∈ {<italic>E, L</italic>}, comparison <italic>t</italic>, subject <italic>x</italic>, image <italic>i</italic>, and scaling <italic>s</italic>:
<disp-formula id="eqn3">
<graphic xlink:href="541306v3_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn4">
<graphic xlink:href="541306v3_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="541306v3_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with the following priors:
<disp-formula>
<graphic xlink:href="541306v3_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<p>The priors for <italic>s<sub>c,mt</sub></italic> of the energy and luminance models correspond to critical scales of 0.25 and 0.018, respectively, the centers of the V1 physiological range provided in <bold><italic><xref ref-type="bibr" rid="c20">Freeman and Simoncelli (2011)</xref></italic></bold> <xref rid="fig5" ref-type="fig">figure 5</xref>, and from the slope of a line fit to the dendritic field diameter vs. eccentricity of midget retinal ganglion cells in <bold><italic><xref ref-type="bibr" rid="c14">Dacey and Petersen (1992)</xref></italic></bold> <xref rid="fig2" ref-type="fig">figure 2B</xref> (see appendix section 2). This captures our prediction that the models’ critical scaling values should be similar to those of the physiological scaling in the brain area sensitive to the same image features, should be independent of comparison type and consistent across images and subjects, while not placing too much of a constraint on the parameters.</p>
<p>The posterior distribution represents the model’s beliefs about the parameters given the priors and data and is summarized throughout this paper as the posterior mean and 95% high density intervals. The latter represents the range of values containing 95% of the distribution with the highest probability, as opposed to the more common 95% confidence interval, which is symmetrically arranged around the mean. The two are identical for symmetric distributions, but can diverge markedly if the distribution is highly skewed (<bold><italic><xref ref-type="bibr" rid="c36">Kruschke, 2015)</xref></italic></bold>).</p>
</sec>
<sec id="s4g">
<title>Software</title>
<p>These experiments relied on a variety of custom scripts written in Python 3.7.10 (<bold><italic><xref ref-type="bibr" rid="c61">Van Rossum and Drake, 2009)</xref></italic></bold>, all found in the <ext-link ext-link-type="uri" xlink:href="https://github.com/billbrod/foveated-metamers/">GitHub repository</ext-link> associated with this paper. The following packages were used: <monospace>snakemake</monospace> (<bold><italic><xref ref-type="bibr" rid="c42">Mölder et al., 2021)</xref></italic></bold>, <monospace>JAX</monospace> (<bold><italic><xref ref-type="bibr" rid="c8">Bradbury et al., 2018)</xref></italic></bold>, <monospace>matplotlib</monospace> (<bold><italic><xref ref-type="bibr" rid="c30">Hunter, 2007)</xref></italic></bold>, <monospace>psychopy</monospace> (<bold><italic><xref ref-type="bibr" rid="c45">Peirce et al., 2019)</xref></italic></bold>, <monospace>scipy</monospace> (<bold><italic><xref ref-type="bibr" rid="c64">Virtanen et al., 2020)</xref></italic></bold>, <monospace>scikit-image</monospace> (<bold><italic><xref ref-type="bibr" rid="c66">van der Walt et al., 2014)</xref></italic></bold>, <monospace>pytorch</monospace> (<bold><italic><xref ref-type="bibr" rid="c44">Paszke et al., 2019)</xref></italic></bold>, <monospace>arviz</monospace> (<bold><italic><xref ref-type="bibr" rid="c37">Kumar et al., 2019)</xref></italic></bold>, <monospace>numpyro</monospace> (<bold><italic><xref ref-type="bibr" rid="c47">Phan et al., 2019</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c7">Bingham et al., 2018)</xref></italic></bold>, <monospace>pandas</monospace> (<bold><italic><xref ref-type="bibr" rid="c49">Reback et al., 2021</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c41">McKinney, 2010</xref></italic></bold>), <monospace>seaborn</monospace> (<bold><italic><xref ref-type="bibr" rid="c70">Waskom, 2021)</xref></italic></bold>, <monospace>jupyterlab</monospace> (<bold><italic><xref ref-type="bibr" rid="c35">Kluyver et al., 2016)</xref></italic></bold>, and <monospace>xarray</monospace> (<bold><italic><xref ref-type="bibr" rid="c29">Hoyer and Hamman, 2017)</xref></italic></bold>.</p>
</sec>
</sec>
<sec id="d1e1855" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1940">
<label>Appendix</label>
<media xlink:href="supplements/541306_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>The authors would like to thank David Brainard for the use of his photographs, both from the published <ext-link ext-link-type="uri" xlink:href="https://web.sas.upenn.edu/upennidb/">UPenn Natural Image Database</ext-link> (<bold><italic><xref ref-type="bibr" rid="c60">Tkačik et al., 2011)</xref></italic></bold> and the unpublished set of images from around Philadelphia. They would also like to thank Tony Movshon, David Heeger, David Brainard, Corey Ziemba, and Colin Bredenberg for their feedback on the manuscript, Mike Landy for his assistance with the design of the psychophysical task and feedback on the manuscript, Heiko Schütt for his assistance with the Markov Chain Monte Carlo analysis, and the authors of <bold><italic><xref ref-type="bibr" rid="c65">Wallis et al. (2019)</xref></italic></bold> for sharing their code and data. Furthermore, they would like to thank Liz Lovero, Paul Murray, Dylan Simon, and Aaron Watters for their work in creating the metamer browser website.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Anderson</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Mullen</surname> <given-names>KT</given-names></string-name>, <string-name><surname>Hess</surname> <given-names>RF</given-names></string-name>. <article-title>Human Peripheral Spatial Resolution for Achromatic and Chromatic Stimuli: Limits Imposed By Optical and Retinal Factors</article-title>. <source>The Journal of Physiology</source>. <year>1991</year>; <volume>442</volume>(<issue>1</issue>):<fpage>47</fpage>–<lpage>64</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Anstis</surname> <given-names>SM</given-names></string-name>. <article-title>A Chart Demonstrating Variations in Acuity With Retinal Position</article-title>. <source>Vision Research</source>. <year>1974</year> <month>jul</month>; <volume>14</volume>(<issue>7</issue>):<fpage>589</fpage>–<lpage>592</lpage>. <pub-id pub-id-type="doi">10.1016/0042-6989(74)90049-2</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/0042-6989(74)90049-2</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Anstis</surname> <given-names>S.</given-names></string-name> <article-title>Picturing Peripheral Acuity</article-title>. <source>Perception</source>. <year>1998</year> <month>jul</month>; <volume>27</volume>(<issue>7</issue>):<fpage>817</fpage>–<lpage>825</lpage>. <pub-id pub-id-type="doi">10.1068/p270817</pub-id>, doi: <pub-id pub-id-type="doi">10.1068/p270817</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Balas</surname> <given-names>B</given-names></string-name>, <string-name><surname>Nakano</surname> <given-names>L</given-names></string-name>, <string-name><surname>Rosenholtz</surname> <given-names>R</given-names></string-name>. <article-title>A Summary-Statistic Representation in Peripheral Vision Explains Visual Crowding</article-title>. <source>Journal of Vision</source>. <year>2009</year> <month>nov</month>; <volume>9</volume>(<issue>12</issue>):<fpage>13</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1167/9.12.13</pub-id>, doi: <pub-id pub-id-type="doi">10.1167/9.12.13</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Banks</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Geisler</surname> <given-names>WS</given-names></string-name>, <string-name><surname>Bennett</surname> <given-names>PJ</given-names></string-name>. <article-title>The Physical Limits of Grating Visibility</article-title>. <source>Vision Research</source>. <year>1987</year> <month>jan</month>; <volume>27</volume>(<issue>11</issue>):<fpage>1915</fpage>–<lpage>1924</lpage>. <pub-id pub-id-type="doi">10.1016\%2F0042-6989\%2887\%2990057-5</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/0042-6989(87)90057-5</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Bennett</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Cortese</surname> <given-names>F</given-names></string-name>. <article-title>Masking of Spatial Frequency in Visual Memory Depends on Distal, Not Retinal, Frequency</article-title>. <source>Vision Research</source>. <year>1996</year> <month>jan</month>; <volume>36</volume>(<issue>2</issue>):<fpage>233</fpage>–<lpage>238</lpage>. <pub-id pub-id-type="doi">10.1016/0042-6989(95)00085-e</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/0042-6989(95)00085-e</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="other"><string-name><surname>Bingham</surname> <given-names>E</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Jankowiak</surname> <given-names>M</given-names></string-name>, <string-name><surname>Obermeyer</surname> <given-names>F</given-names></string-name>, <string-name><surname>Pradhan</surname> <given-names>N</given-names></string-name>, <string-name><surname>Karaletsos</surname> <given-names>T</given-names></string-name>, <string-name><surname>Singh</surname> <given-names>R</given-names></string-name>, <string-name><surname>Szerlip</surname> <given-names>P</given-names></string-name>, <string-name><surname>Horsfall</surname> <given-names>P</given-names></string-name>, <string-name><surname>Goodman</surname> <given-names>ND</given-names></string-name>. <article-title>Pyro: Deep Universal Probabilistic Programming</article-title>. <source>arXiv preprint arXiv:181009538</source>. <year>2018</year>;.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="web"><string-name><surname>Bradbury</surname> <given-names>J</given-names></string-name>, <string-name><surname>Frostig</surname> <given-names>R</given-names></string-name>, <string-name><surname>Hawkins</surname> <given-names>P</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Leary</surname> <given-names>C</given-names></string-name>, <string-name><surname>Maclaurin</surname> <given-names>D</given-names></string-name>, <string-name><surname>Necula</surname> <given-names>G</given-names></string-name>, <string-name><surname>Paszke</surname> <given-names>A</given-names></string-name>, <string-name><surname>VanderPlas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wanderman-Milne</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Q</given-names></string-name>, <source>JAX: composable transformations of Python+NumPy programs</source>; <year>2018</year>. <ext-link ext-link-type="uri" xlink:href="http://github.com/google/jax">http://github.com/google/jax</ext-link>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Brooks</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Gelman</surname> <given-names>A</given-names></string-name>. <article-title>General Methods for Monitoring Convergence of Iterative Simulations</article-title>. <source>Journal of Computational and Graphical Statistics</source>. <year>1998</year> <month>dec</month>; <volume>7</volume>(<issue>4</issue>):<fpage>434</fpage>–<lpage>455</lpage>. <pub-id pub-id-type="doi">10.1080/10618600.1998.10474787</pub-id>, doi: <pub-id pub-id-type="doi">10.1080/10618600.1998.10474787</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="other"><string-name><surname>Brown</surname> <given-names>R</given-names></string-name>, <string-name><surname>DuTell</surname> <given-names>V</given-names></string-name>, <string-name><surname>Walter</surname> <given-names>B</given-names></string-name>, <string-name><surname>Rosenholtz</surname> <given-names>R</given-names></string-name>, <string-name><surname>Shirley</surname> <given-names>P</given-names></string-name>, <string-name><surname>McGuire</surname> <given-names>M</given-names></string-name>, <string-name><surname>Luebke</surname> <given-names>D</given-names></string-name>, <source>Effcient Dataflow Modeling of Peripheral Encoding in the Human Visual System</source>; <year>2021</year>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Bruna</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mallat</surname> <given-names>S</given-names></string-name>. <article-title>Invariant Scattering Convolution Networks</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2013</year> <month>aug</month>; <volume>35</volume>(<issue>8</issue>):<fpage>1872</fpage>–<lpage>1886</lpage>. <pub-id pub-id-type="doi">10.1109%2Ftpami.2012.230</pub-id>, doi: <pub-id pub-id-type="doi">10.1109/tpami.2012.230</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="book"><string-name><surname>Burt</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Adelson</surname> <given-names>EH</given-names></string-name>. <chapter-title>The Laplacian pyramid as a compact image code</chapter-title>. In: <source>Readings in computer vision</source> <publisher-name>Elsevier</publisher-name>; <year>1987</year>.p. <fpage>671</fpage>–<lpage>679</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Cohen</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Kappauf</surname> <given-names>WE</given-names></string-name>. <article-title>Color Mixture and Fundamental Metamers: Theory, Algebra, Geometry, Application</article-title>. <source>The American Journal of Psychology</source>. <year>1985</year>; <volume>98</volume>(<issue>2</issue>):<fpage>171</fpage>. <pub-id pub-id-type="doi">10.2307/1422442</pub-id>, doi: <pub-id pub-id-type="doi">10.2307/1422442</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Dacey</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>MR</given-names></string-name>. <article-title>Dendritic Field Size and Morphology of Midget and Parasol Ganglion Cells of the Human Retina</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1992</year> <month>oct</month>; <volume>89</volume>(<issue>20</issue>):<fpage>9666</fpage>–<lpage>9670</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.89.20.9666</pub-id>, doi: <pub-id pub-id-type="doi">10.1073/pnas.89.20.9666</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Daniel</surname> <given-names>PM</given-names></string-name>, <string-name><surname>Whitteridge</surname> <given-names>D</given-names></string-name>. <article-title>The Representation of the Visual Field on the Cerebral Cortex in Monkeys</article-title>. <source>The Journal of Physiology</source>. <year>1961</year> <month>dec</month>; <volume>159</volume>(<issue>2</issue>):<fpage>203</fpage>–<lpage>221</lpage>. <pub-id pub-id-type="doi">10.1113%2Fjphysiol.1961.sp006803</pub-id>, doi: <pub-id pub-id-type="doi">10.1113/jphysiol.1961.sp006803</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="confproc"><string-name><surname>Deza</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jonnalagadda</surname> <given-names>A</given-names></string-name>, <string-name><surname>Eckstein</surname> <given-names>MP</given-names></string-name><source>. Towards Metamerism via Foveated Style Transfer</source>. In: <conf-name>International Conference on Learning Representations</conf-name>; <year>2019</year>. <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=BJzbG20cFQ">https://openreview.net/forum?id=BJzbG20cFQ</ext-link>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Douglas</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>KAC</given-names></string-name>, <string-name><surname>Whitteridge</surname> <given-names>D</given-names></string-name>. <article-title>A Canonical Microcircuit for Neocortex</article-title>. <source>Neural Computation</source>. <year>1989</year> <month>dec</month>; <volume>1</volume>(<issue>4</issue>):<fpage>480</fpage>–<lpage>488</lpage>. <pub-id pub-id-type="doi">10.1162%2Fneco.1989.1.4.480</pub-id>, doi: <pub-id pub-id-type="doi">10.1162/neco.1989.1.4.480</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Duncan</surname> <given-names>RO</given-names></string-name>, <string-name><surname>Boynton</surname> <given-names>GM</given-names></string-name>. <article-title>Cortical Magnification Within Human Primary Visual Cortex Correlates With Acuity Thresholds</article-title>. <source>Neuron</source>. <year>2003</year> <month>may</month>; <volume>38</volume>(<issue>4</issue>):<fpage>659</fpage>–<lpage>671</lpage>. <pub-id pub-id-type="doi">10.1016/s0896-6273(03)00265-4</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/s0896-6273(03)00265-4</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="other"><string-name><surname>Feather</surname> <given-names>J</given-names></string-name>, <string-name><surname>Durango</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gonzalez</surname> <given-names>R</given-names></string-name>, <string-name><surname>McDermott</surname> <given-names>J</given-names></string-name>. <article-title>Metamers of neural networks reveal divergence from human perceptual systems</article-title>. In: <source>NeurIPS</source>; <year>2019</year>. p. <fpage>10078</fpage>–<lpage>10089</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Freeman</surname> <given-names>J</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>. <article-title>Metamers of the ventral stream</article-title>. <source>Nature Neuroscience</source>. <year>2011</year> <month>aug</month>; <volume>14</volume>(<issue>9</issue>):<fpage>1195</fpage>–<lpage>1201</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn.2889</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Frisen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Glansholm</surname> <given-names>A</given-names></string-name>. <article-title>Optical and Neural Resolution in Peripheral Vision</article-title>. <source>Investigative Ophthalmology &amp; Visual Science</source>. <year>1975</year>; <volume>14</volume>(<issue>7</issue>):<fpage>528</fpage>–<lpage>536</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Fukushima</surname> <given-names>K</given-names></string-name>. <article-title>Neocognitron: a Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected By Shift in Position</article-title>. <source>Biological Cybernetics</source>. <year>1980</year> <month>apr</month>; <volume>36</volume>(<issue>4</issue>):<fpage>193</fpage>–<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1007%2Fbf00344251</pub-id>, doi: <pub-id pub-id-type="doi">10.1007/bf00344251</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Gattass</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>CG</given-names></string-name>, <string-name><surname>Sandell</surname> <given-names>JH</given-names></string-name>. <article-title>Visual Topography of V2 in the Macaque</article-title>. <source>The Journal of Comparative Neurology</source>. <year>1981</year>; <volume>201</volume>(<issue>4</issue>):<fpage>519</fpage>–<lpage>539</lpage>. <pub-id pub-id-type="doi">10.1002/cne.902010405</pub-id>, doi: <pub-id pub-id-type="doi">10.1002/cne.902010405</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Gattass</surname> <given-names>R</given-names></string-name>, <string-name><surname>Sousa</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>C</given-names></string-name>. <article-title>Visuotopic Organization and Extent of V3 and V4 of the Macaque</article-title>. <source>Journal of Neuroscience</source>. <year>1988</year>; <volume>8</volume>(<issue>6</issue>):<fpage>1831</fpage>–<lpage>1845</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/content/8/6/1831">http://www.jneurosci.org/content/8/6/1831</ext-link>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Greenwood</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Bex</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Dakin</surname> <given-names>SC</given-names></string-name>. <article-title>Positional Averaging Explains Crowding With Letter-Like Stimuli</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2009</year> <month>aug</month>; <volume>106</volume>(<issue>31</issue>):<fpage>13130</fpage>–<lpage>13135</lpage>. <pub-id pub-id-type="doi">10.1073%2Fpnas.0901352106</pub-id>, doi: <pub-id pub-id-type="doi">10.1073/pnas.0901352106</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Movshon</surname> <given-names>JA</given-names></string-name>. <article-title>Computational Models of Cortical Visual Processing</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1996</year>; <volume>93</volume>(<issue>2</issue>):<fpage>623</fpage>–<lpage>627</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Helmholtz</surname> <given-names>H. LXXXI</given-names></string-name>. <article-title>On the Theory of Compound Colours</article-title>. <source>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</source>. <year>1852</year>; <volume>4</volume>(<issue>28</issue>):<fpage>519</fpage>–<lpage>534</lpage>. <pub-id pub-id-type="doi">10.1080/14786445208647175</pub-id>, doi: <pub-id pub-id-type="doi">10.1080/14786445208647175</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Hoffman</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Gelman</surname> <given-names>A</given-names></string-name>. <article-title>The No-U-turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo</article-title>. <source>Journal of Machine Learning Research</source>. <year>2014</year>; <volume>15</volume>(<issue>1</issue>):<fpage>1593</fpage>–<lpage>1623</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Hoyer</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hamman</surname> <given-names>J</given-names></string-name>. <article-title>Xarray: N-D Labeled Arrays and Datasets in Python</article-title>. <source>Journal of Open Research Software</source>. <year>2017</year>; <volume>5</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.5334/jors.148</pub-id>, doi: <pub-id pub-id-type="doi">10.5334/jors.148</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Hunter</surname> <given-names>JD</given-names></string-name>. <article-title>Matplotlib: a 2d Graphics Environment</article-title>. <source>Computing in Science &amp; Engineering</source>. <year>2007</year>; <volume>9</volume>(<issue>3</issue>):<fpage>90</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>, doi: <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="other"><string-name><surname>Izmailov</surname> <given-names>P</given-names></string-name>, <string-name><surname>Podoprikhin</surname> <given-names>D</given-names></string-name>, <string-name><surname>Garipov</surname> <given-names>T</given-names></string-name>, <string-name><surname>Vetrov</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>AG</given-names></string-name>. <article-title>Averaging Weights Leads To Wider Optima and Better Generalization</article-title>. <source>arXiv preprint arXiv:180305407</source>. <year>2018</year>;.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="other"><string-name><surname>Jagadeesh</surname> <given-names>AV</given-names></string-name>, <string-name><surname>Gardner</surname> <given-names>JL</given-names></string-name>. <article-title>Texture-Like Representation of Objects in Human Visual Cortex</article-title>. <source>bioRxiv</source>. <year>2022</year>; <pub-id pub-id-type="doi">10.1101/2022.01.04.474849</pub-id>, doi: <pub-id pub-id-type="doi">10.1101/2022.01.04.474849</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Keshvari</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rosenholtz</surname> <given-names>R</given-names></string-name>. <article-title>Pooling of Continuous Features Provides a Unifying Account of Crowding</article-title>. <source>Journal of Vision</source>. <year>2016</year> <month>feb</month>; <volume>16</volume>(<issue>3</issue>):<fpage>39</fpage>. <pub-id pub-id-type="doi">10.1167/16.3.39</pub-id>, doi: <pub-id pub-id-type="doi">10.1167/16.3.39</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="other"><string-name><surname>Kingma</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Ba</surname> <given-names>J.</given-names></string-name> <article-title>Adam: A Method for Stochastic Optimization</article-title>. <source>ArXiv e-prints</source>. <year>2014</year> <month>dec</month>;.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="book"><string-name><surname>Kluyver</surname> <given-names>T</given-names></string-name>, <string-name><surname>Ragan-Kelley</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pérez</surname> <given-names>F</given-names></string-name>, <string-name><surname>Granger</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bussonnier</surname> <given-names>M</given-names></string-name>, <string-name><surname>Frederic</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kelley</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hamrick</surname> <given-names>J</given-names></string-name>, <string-name><surname>Grout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Corlay</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ivanov</surname> <given-names>P</given-names></string-name>, <string-name><surname>Avila</surname> <given-names>D</given-names></string-name>, <string-name><surname>Abdalla</surname> <given-names>S</given-names></string-name>, <string-name><surname>Willing</surname> <given-names>C</given-names></string-name>, <collab>development team J</collab>. <chapter-title>Jupyter Notebooks - a publishing format for reproducible computational workflows</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Loizides</surname> <given-names>F</given-names></string-name>, <string-name><surname>Scmidt</surname> <given-names>B</given-names></string-name></person-group>, editors. <source>Positioning and Power in Academic Publishing: Players, Agents and Agendas</source> <publisher-loc>Netherlands</publisher-loc>: <publisher-name>IOS Press</publisher-name>; <year>2016</year>. p. <fpage>87</fpage>–<lpage>90</lpage>. <ext-link ext-link-type="uri" xlink:href="https://eprints.soton.ac.uk/403913/">https://eprints.soton.ac.uk/403913/</ext-link>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Kruschke</surname> <given-names>JK</given-names></string-name>. <article-title>Doing Bayesian Data Analysis</article-title>. <source>Second ed. Elsevier</source>; <year>2015</year>. <pub-id pub-id-type="doi">10.1016/c2012-0-00477-2</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/c2012-0-00477-2</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Kumar</surname> <given-names>R</given-names></string-name>, <string-name><surname>Carroll</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hartikainen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>O</given-names></string-name>. <article-title>Arviz a Unified Library for Exploratory Analysis of Bayesian Models in Python</article-title>. <source>Journal of Open Source Software</source>. <year>2019</year>; <volume>4</volume>(<issue>33</issue>):<fpage>1143</fpage>. <pub-id pub-id-type="doi">10.21105/joss.01143</pub-id>, doi: <pub-id pub-id-type="doi">10.21105/joss.01143</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>LeCun</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Boser</surname> <given-names>B</given-names></string-name>, <string-name><surname>Denker</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Henderson</surname> <given-names>D</given-names></string-name>, <string-name><surname>Howard</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Hubbard</surname> <given-names>W</given-names></string-name>, <string-name><surname>Jackel</surname> <given-names>LD</given-names></string-name>. <article-title>Backpropagation Applied To Handwritten Zip Code Recognition</article-title>. <source>Neural Computation</source>. <year>1989</year> <month>dec</month>; <volume>1</volume>(<issue>4</issue>):<fpage>541</fpage>–<lpage>551</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id>, doi: <pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Lettvin</surname> <given-names>JY</given-names></string-name>. <article-title>On Seeing Sidelong</article-title>. <source>The Sciences</source>. <year>1976</year> <month>jul</month>; <volume>16</volume>(<issue>4</issue>):<fpage>10</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1002%2Fj.2326-1951.1976.tb01231.x</pub-id>, doi: <pub-id pub-id-type="doi">10.1002/j.2326-1951.1976.tb01231.x</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Maunsell</surname> <given-names>JHR</given-names></string-name>, <string-name><surname>Newsome</surname> <given-names>WT</given-names></string-name>. <article-title>Visual Processing in Monkey Extrastriate Cortex</article-title>. <source>Annual Review of Neuroscience</source>. <year>1987</year> <month>mar</month>; <volume>10</volume>(<issue>1</issue>):<fpage>363</fpage>–<lpage>401</lpage>. <pub-id pub-id-type="doi">10.1146%2Fannurev.ne.10.030187.002051</pub-id>, doi: <pub-id pub-id-type="doi">10.1146/an-nurev.ne.10.030187.002051</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="confproc"><string-name><surname>McKinney</surname> <given-names>W</given-names></string-name>. <chapter-title>Data Structures for Statistical Computing in Python</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>van der Walt</surname> <given-names>S</given-names></string-name>, <string-name><surname>Millman</surname> <given-names>J</given-names></string-name></person-group>, editors. <conf-name>Proceedings of the 9th Python in Science Conference</conf-name>; <year>2010</year>. p. <fpage>56</fpage> – <lpage>61</lpage>. <pub-id pub-id-type="doi">10.25080/Majora-92bf1922-00a</pub-id>, doi: <pub-id pub-id-type="doi">10.25080/Majora-92bf1922-00a</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Mölder</surname> <given-names>F</given-names></string-name>, <string-name><surname>Jablonski</surname> <given-names>KP</given-names></string-name>, <string-name><surname>Letcher</surname> <given-names>B</given-names></string-name>, <string-name><surname>Hall</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Tomkins-Tinch</surname> <given-names>CH</given-names></string-name>, <string-name><surname>Sochat</surname> <given-names>V</given-names></string-name>, <string-name><surname>Forster</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>S</given-names></string-name>, <string-name><surname>Twardziok</surname> <given-names>SO</given-names></string-name>, <string-name><surname>Kanitz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wilm</surname> <given-names>A</given-names></string-name>, <string-name><surname>Holtgrewe</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rahmann</surname> <given-names>S</given-names></string-name>, <string-name><surname>Nahnsen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Köster</surname> <given-names>J</given-names></string-name>. <article-title>Sustainable Data Analysis With Snakemake</article-title>. <source>F1000Research</source>. <year>2021</year> <month>apr</month>; <volume>10</volume>:<fpage>33</fpage>. <pub-id pub-id-type="doi">10.12688/f1000research.29032.2</pub-id>, doi: <pub-id pub-id-type="doi">10.12688/f1000research.29032.2</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Parkes</surname> <given-names>L</given-names></string-name>, <string-name><surname>Lund</surname> <given-names>J</given-names></string-name>, <string-name><surname>Angelucci</surname> <given-names>A</given-names></string-name>, <string-name><surname>Solomon</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Morgan</surname> <given-names>M</given-names></string-name>. <article-title>Compulsory Averaging of Crowded Orientation Signals in Human Vision</article-title>. <source>Nature Neuroscience</source>. <year>2001</year> <month>jul</month>; <volume>4</volume>(<issue>7</issue>):<fpage>739</fpage>–<lpage>744</lpage>. <pub-id pub-id-type="doi">10.1038%2F89532</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/89532</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="book"><string-name><surname>Paszke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>S</given-names></string-name>, <string-name><surname>Massa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lerer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bradbury</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chanan</surname> <given-names>G</given-names></string-name>, <string-name><surname>Killeen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Gimelshein</surname> <given-names>N</given-names></string-name>, <string-name><surname>Antiga</surname> <given-names>L</given-names></string-name>, <string-name><surname>Desmaison</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kopf</surname> <given-names>A</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>E</given-names></string-name>, <string-name><surname>DeVito</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Raison</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tejani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chilamkurthy</surname> <given-names>S</given-names></string-name>, <string-name><surname>Steiner</surname> <given-names>B</given-names></string-name>, <string-name><surname>Fang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bai</surname> <given-names>J</given-names></string-name>, <etal>et al.</etal> <chapter-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Wallach</surname> <given-names>H</given-names></string-name>, <string-name><surname>Larochelle</surname> <given-names>H</given-names></string-name>, <string-name><surname>Beygelzimer</surname> <given-names>A</given-names></string-name>, <string-name><surname>d’Alché-Buc</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fox</surname> <given-names>E</given-names></string-name>, <string-name><surname>Garnett</surname> <given-names>R</given-names></string-name></person-group>, editors. <source>Advances in Neural Information Processing Systems</source> <volume>32</volume> <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2019</year>.p. <fpage>8024</fpage>–<lpage>8035</lpage>. <ext-link ext-link-type="uri" xlink:href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</ext-link>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Peirce</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gray</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Simpson</surname> <given-names>S</given-names></string-name>, <string-name><surname>MacAskill</surname> <given-names>M</given-names></string-name>, <string-name><surname>Höchenberger</surname> <given-names>R</given-names></string-name>, <string-name><surname>Sogo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kastman</surname> <given-names>E</given-names></string-name>, <string-name><surname>Lindeløv</surname> <given-names>JK</given-names></string-name>. <article-title>PsychoPy2: Experiments in Behavior Made Easy</article-title>. <source>Behavior Research Methods</source>. <year>2019</year> <month>feb</month>; <volume>51</volume>(<issue>1</issue>):<fpage>195</fpage>–<lpage>203</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id>, doi: <pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name>, <string-name><surname>Palomares</surname> <given-names>M</given-names></string-name>, <string-name><surname>Majaj</surname> <given-names>NJ</given-names></string-name>. <article-title>Crowding Is Unlike Ordinary Masking: Distinguishing Feature Integration From Detection</article-title>. <source>Journal of Vision</source>. <year>2004</year> <month>dec</month>; <volume>4</volume>(<issue>12</issue>):<fpage>12</fpage>. <pub-id pub-id-type="doi">10.1167%2F4.12.12</pub-id>, doi: <pub-id pub-id-type="doi">10.1167/4.12.12</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="other"><string-name><surname>Phan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Pradhan</surname> <given-names>N</given-names></string-name>, <string-name><surname>Jankowiak</surname> <given-names>M</given-names></string-name>. <article-title>Composable Effects for Flexible and Accelerated Probabilistic Programming in Numpyro</article-title>. <source>arXiv preprint arXiv:191211554</source>. <year>2019</year>;.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Portilla</surname> <given-names>J</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>. <article-title>A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients</article-title>. <source>International journal of computer vision</source>. <year>2000</year>; <volume>40</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="other"><string-name><surname>Reback</surname> <given-names>J</given-names></string-name>, <string-name><surname>jbrockmendel</surname></string-name>, <string-name><surname>McKinney</surname> <given-names>W</given-names></string-name>, <string-name><surname>den Bossche</surname> <given-names>JV</given-names></string-name>, <string-name><surname>Augspurger</surname> <given-names>T</given-names></string-name>, <string-name><surname>Cloud</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hawkins</surname> <given-names>S</given-names></string-name>, <string-name><surname>gfyoung</surname></string-name>, <string-name><surname>Roeschke</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sinhrks</surname></string-name>, <etal>et al</etal>, <source>pandas-dev/pandas: Pandas 1.2.3. Zenodo</source>; <year>2021</year>. doi: <pub-id pub-id-type="doi">10.5281/zenodo.4572994</pub-id>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="confproc"><string-name><surname>Reddi</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Kale</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kumar</surname> <given-names>S</given-names></string-name>. <article-title>On the Convergence of Adam and Beyond</article-title>. In: <conf-name>International Conference on Learning Representations</conf-name>; <year>2018</year>. <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=ryQu7f-RZ">https://openreview.net/forum?id=ryQu7f-RZ</ext-link>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Riesenhuber</surname> <given-names>M</given-names></string-name>, <string-name><surname>Poggio</surname> <given-names>T</given-names></string-name>. <article-title>Hierarchical Models of Object Recognition in Cortex</article-title>. <source>Nature Neuroscience</source>. <year>1999</year> <month>nov</month>; <volume>2</volume>(<issue>11</issue>):<fpage>1019</fpage>–<lpage>1025</lpage>. <pub-id pub-id-type="doi">10.1038%2F14819</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/14819</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Robson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Graham</surname> <given-names>N</given-names></string-name>. <article-title>Probability Summation and Regional Variation in Contrast Sensitivity Across the Visual Field</article-title>. <source>Vision research</source>. <year>1981</year>; <volume>21</volume>(<issue>3</issue>):<fpage>409</fpage>–<lpage>418</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Rovamo</surname> <given-names>J</given-names></string-name>, <string-name><surname>Virsu</surname> <given-names>V</given-names></string-name>, <string-name><surname>Näsänen</surname> <given-names>R</given-names></string-name>. <article-title>Cortical Magnification Factor Predicts the Photopic Contrast Sensitivity of Peripheral Vision</article-title>. <source>Nature</source>. <year>1978</year> <month>jan</month>; <volume>271</volume>(<issue>5640</issue>):<fpage>54</fpage>–<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1038/271054a0</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/271054a0</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Schnapf</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Kraft</surname> <given-names>TW</given-names></string-name>, <string-name><surname>Baylor</surname> <given-names>DA</given-names></string-name>. <article-title>Spectral Sensitivity of Human Cone Photoreceptors</article-title>. <source>Nature</source>. <year>1987</year> <month>jan</month>; <volume>325</volume>(<issue>6103</issue>):<fpage>439</fpage>–<lpage>441</lpage>. <pub-id pub-id-type="doi">10.1038%2F325439a0</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/325439a0</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Schwartz</surname> <given-names>EL</given-names></string-name>. <article-title>Spatial Mapping in the Primate Sensory Projection: Analytic Structure and Relevance To Perception</article-title>. <source>Biological Cybernetics</source>. <year>1977</year> <month>dec</month>; <volume>25</volume>(<issue>4</issue>):<fpage>181</fpage>–<lpage>194</lpage>. <pub-id pub-id-type="doi">10.1007%2Fbf01885636</pub-id>, doi: <pub-id pub-id-type="doi">10.1007/bf01885636</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="confproc"><string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Freeman</surname> <given-names>WT</given-names></string-name>. <article-title>The Steerable Pyramid: A flexible architecture for multi-scale derivative computation</article-title>. In: <conf-name>Proc 2nd IEEE Int’l Conf on Image Proc (ICIP)</conf-name>, vol. <volume>III</volume> <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Sig Proc Society</publisher-name>; <year>1995</year>. p. <fpage>444</fpage>–<lpage>447</lpage>. doi: <pub-id pub-id-type="doi">10.1109/ICIP.1995.537667</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Song</surname> <given-names>S</given-names></string-name>, <string-name><surname>Levi</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name>. <article-title>A Double Dissociation of the Acuity and Crowding Limits To Letter Identification, and the Promise of Improved Visual Screening</article-title>. <source>Journal of Vision</source>. <year>2014</year> <month>may</month>; <volume>14</volume>(<issue>5</issue>):<fpage>3</fpage>–<lpage>3</lpage>. <pub-id pub-id-type="doi">10.1167/14.5.3</pub-id>, doi: <pub-id pub-id-type="doi">10.1167/14.5.3</pub-id>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Teller</surname> <given-names>DY</given-names></string-name>. <article-title>Linking Propositions</article-title>. <source>Vision research</source>. <year>1984</year>; <volume>24</volume>(<issue>10</issue>):<fpage>1233</fpage>–<lpage>1246</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Thibos</surname> <given-names>LN</given-names></string-name>. <article-title>Retinal Image Formation and Sampling in a Three-Dimensional World</article-title>. <source>Annual Review of Vision Science</source>. <year>2020</year> <month>sep</month>; <volume>6</volume>(<issue>1</issue>):<fpage>469</fpage>–<lpage>489</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-121219-081840</pub-id>, doi: <pub-id pub-id-type="doi">10.1146/annurev-vision-121219-081840</pub-id>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Tkačik</surname> <given-names>G</given-names></string-name>, <string-name><surname>Garrigan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ratliff</surname> <given-names>C</given-names></string-name>, <string-name><surname>Milčinski</surname> <given-names>G</given-names></string-name>, <string-name><surname>Klein</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Seyfarth</surname> <given-names>LH</given-names></string-name>, <string-name><surname>Sterling</surname> <given-names>P</given-names></string-name>, <string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Balasubramanian</surname> <given-names>V</given-names></string-name>. <article-title>Natural Images From the Birthplace of the Human Eye</article-title>. <source>PLoS ONE</source>. <year>2011</year> <month>jun</month>; <volume>6</volume>(<issue>6</issue>):<fpage>e20409</fpage>. <pub-id pub-id-type="doi">10.1371%2Fjournal.pone.0020409</pub-id>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0020409</pub-id>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="book"><string-name><surname>Van Rossum</surname> <given-names>G</given-names></string-name>, <string-name><surname>Drake</surname> <given-names>FL</given-names></string-name>. <source>Python 3 Reference Manual. Scotts Valley</source>, <publisher-loc>CA</publisher-loc>: <publisher-name>CreateSpace</publisher-name>; <year>2009</year>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Vehtari</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gelman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gabry</surname> <given-names>J</given-names></string-name>. <article-title>Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC</article-title>. <source>Statistics and Computing</source>. <year>2016</year> <month>aug</month>; <volume>27</volume>(<issue>5</issue>):<fpage>1413</fpage>–<lpage>1432</lpage>. <pub-id pub-id-type="doi">10.1007/s11222-016-9696-4</pub-id>, doi: <pub-id pub-id-type="doi">10.1007/s11222-016-9696-4</pub-id>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Vehtari</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gelman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Simpson</surname> <given-names>D</given-names></string-name>, <string-name><surname>Carpenter</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bürkner</surname> <given-names>PC</given-names></string-name>. <article-title>Rank-Normalization, Folding, and Localization: an Improved <italic>R”</italic> for Assessing Convergence of Mcmc (with Discussion)</article-title>. <source>Bayesian Analysis</source>. <year>2021</year>; <volume>16</volume>(<issue>2</issue>):<fpage>667</fpage> – <lpage>718</lpage>. <pub-id pub-id-type="doi">10.1214/20-BA1221</pub-id>, doi: <pub-id pub-id-type="doi">10.1214/20-BA1221</pub-id>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>Virtanen</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gommers</surname> <given-names>R</given-names></string-name>, <string-name><surname>Oliphant</surname> <given-names>TE</given-names></string-name>, <string-name><surname>Haberland</surname> <given-names>M</given-names></string-name>, <string-name><surname>Reddy</surname> <given-names>T</given-names></string-name>, <string-name><surname>Cournapeau</surname> <given-names>D</given-names></string-name>, <string-name><surname>Burovski</surname> <given-names>E</given-names></string-name>, <string-name><surname>Peterson</surname> <given-names>P</given-names></string-name>, <string-name><surname>Weckesser</surname> <given-names>W</given-names></string-name>, <string-name><surname>Bright</surname> <given-names>J</given-names></string-name>, <string-name><surname>van der Walt</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Brett</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jarrod Millman</surname> <given-names>K</given-names></string-name>, <string-name><surname>Mayorov</surname> <given-names>N</given-names></string-name>, <string-name><surname>Nelson</surname> <given-names>ARJ</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kern</surname> <given-names>R</given-names></string-name>, <string-name><surname>Larson</surname> <given-names>E</given-names></string-name>, <string-name><surname>Carey</surname> <given-names>C</given-names></string-name>, <etal>et al.</etal> <article-title>SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</article-title>. <source>Nature Methods</source>. <year>2020</year>; <volume>17</volume>:<fpage>261</fpage>–<lpage>272</lpage>. <pub-id pub-id-type="doi">https://doi.org/10.1038/s41592-019-0686-2</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Wallis</surname> <given-names>TS</given-names></string-name>, <string-name><surname>Funke</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Ecker</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Gatys</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Wichmann</surname> <given-names>FA</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M</given-names></string-name>. <article-title>Image Content Is More Important Than Bouma’s Law for Scene Metamers</article-title>. <source>eLife</source>. <year>2019</year> <month>apr</month>; <volume>8</volume>. <pub-id pub-id-type="doi">10.7554/elife.42512</pub-id>, doi: <pub-id pub-id-type="doi">10.7554/elife.42512</pub-id>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><surname>van der Walt</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schönberger</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Nunez-Iglesias</surname> <given-names>J</given-names></string-name>, <string-name><surname>Boulogne</surname> <given-names>F</given-names></string-name>, <string-name><surname>Warner</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Yager</surname> <given-names>N</given-names></string-name>, <string-name><surname>Gouillart</surname> <given-names>E</given-names></string-name>, <article-title>Yu T, the scikit-image contributors. Scikit-Image: Image Processing in Python</article-title>. <source>PeerJ</source>. <year>2014</year> <volume>6</volume>; <issue>2</issue>:<fpage>e453</fpage>. <pub-id pub-id-type="doi">10.7717/peerj.453</pub-id>, doi: <pub-id pub-id-type="doi">10.7717/peerj.453</pub-id>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="web"><string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name>. <source>Foundations of vision</source>. <publisher-name>Sinauer Associates</publisher-name>; <year>1995</year>. <ext-link ext-link-type="uri" xlink:href="https://foundationsofvision.stanford.edu/">https://foundationsofvision.stanford.edu/</ext-link>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Winawer</surname> <given-names>J</given-names></string-name>. <article-title>Computational Neuroimaging and Population Receptive Fields</article-title>. <source>Trends in cognitive sciences</source>. <year>2015</year>; <volume>19</volume>(<issue>6</issue>):<fpage>349</fpage>–<lpage>357</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Bovik</surname> <given-names>AC</given-names></string-name>. <article-title>Mean Squared Error: Love It Or Leave It? A New Look At Signal Fidelity Measures</article-title>. <source>IEEE Signal Processing Magazine</source>. <year>2009</year> <month>jan</month>; <volume>26</volume>(<issue>1</issue>):<fpage>98</fpage>–<lpage>117</lpage>. <pub-id pub-id-type="doi">10.1109%2Fmsp.2008.930649</pub-id>, doi: <pub-id pub-id-type="doi">10.1109/msp.2008.930649</pub-id>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><string-name><surname>Waskom</surname> <given-names>ML</given-names></string-name>. <article-title>Seaborn: Statistical Data Visualization</article-title>. <source>Journal of Open Source Software</source>. <year>2021</year>; <volume>6</volume>(<issue>60</issue>):<fpage>3021</fpage>. <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id>, doi: <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><string-name><surname>Watanabe</surname> <given-names>S</given-names></string-name>. <article-title>A Widely Applicable Bayesian Information Criterion</article-title>. <source>Journal of Machine Learning Research</source>. <year>2013</year>; <volume>14</volume>(<issue>Mar</issue>):<fpage>867</fpage>–<lpage>897</lpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><string-name><surname>Watson</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Ahumada</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Farrell</surname> <given-names>JE</given-names></string-name>. <article-title>Window of Visibility: a Psychophysical Theory of Fidelity in Time-Sampled Visual Motion Displays</article-title>. <source>JOSA A</source>. <year>1986</year>; <volume>3</volume>(<issue>3</issue>):<fpage>300</fpage>–<lpage>307</lpage>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><string-name><surname>Ziemba</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>. <article-title>Opposing Effects of Selectivity and Invariance in Peripheral Vision</article-title>. <source>Nature Communications</source>. <year>2021</year> <month>jul</month>; <volume>12</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1038/s41467-021-24880-5</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41467-021-24880-5</pub-id>.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1"><label>1</label><p>Note that this assumption does not preclude recurrent processing or feedback from later stages or other modalities. It only asserts that these signals do not carry additional information about the visual stimulus.</p></fn>
</fn-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90554.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>de Lange</surname>
<given-names>Floris P</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Donders Institute for Brain, Cognition and Behaviour</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study provides <bold>valuable</bold> insights into how researchers can use perceptual metamers to formally explore the limits of visual representations at different processing stages. While the study is overall <bold>convincing</bold> in terms of approach and results, issues were identified with respect to novelty, sample size, <bold>incomplete</bold> psychophysical methodology, and better motivation of the models tested.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90554.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This is an interesting study of the nature of representations across the visual field. The question of how peripheral vision differs from foveal vision is a fascinating and important one. The majority of our visual field is extra-foveal yet our sensory and perceptual capabilities decline in pronounced and well-documented ways away from the fovea. Part of the decline is thought to be due to spatial averaging ('pooling') of features. Here, the authors contrast two models of such feature pooling with human judgments of image content. They use much larger visual stimuli than in most previous studies, and some sophisticated image synthesis methods to tease apart the prediction of the distinct models.</p>
<p>More importantly, in so doing, the researchers thoroughly explore the general approach of probing visual representations through metamers-stimuli that are physically distinct but perceptually indistinguishable. The work is embedded within a rigorous and general mathematical framework for expressing equivalence classes of images and how visual representations influence these. They describe how image-computable models can be used to make predictions about metamers, which can then be compared to make inferences about the underlying sensory representations. The main merit of the work lies in providing a formal framework for reasoning about metamers and their implications, for comparing models of sensory processing in terms of the metamers that they predict, and for mapping such models onto physiology. Importantly, they also consider the limits of what can be inferred about sensory processing from metamers derived from different models.</p>
<p>Overall, the work is of a very high standard and represents a significant advance over our current understanding of perceptual representations of image structure at different locations across the visual field. The authors do a good job of capturing the limits of their approach and I particularly appreciated the detailed and thoughtful Discussion section and the suggestion to extend the metamer-based approach described in the MS with observer models. The work will have an impact on researchers studying many different aspects of visual function including texture perception, crowding, natural image statistics, and the physiology of low- and mid-level vision.</p>
<p>The main weaknesses of the original submission relate to the writing. A clearer motivation could have been provided for the specific models that they consider, and the text could have been written in a more didactic and easy-to-follow manner. The authors could also have been more explicit about the assumptions that they make.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90554.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>
This paper expands on the literature on spatial metamers, evaluating different aspects of spatial metamers including the effect of different models and initialization conditions, as well as the relationship between metamers of the human visual system and metamers for a model. The authors conduct psychophysics experiments testing variations of metamer synthesis parameters including type of target image, scaling factor, and initialization parameters, and also compare two different metamer models (luminance vs energy). An additional contribution is doing this for a field of view larger than has been explored previously.</p>
<p>General Comments</p>
<p>
Overall, this paper addresses some important outstanding questions regarding comparing original to synthesized images in metamer experiments and begins to explore the effect of noise vs image seed on the resulting syntheses. While the paper tests some model classes that could be better motivated, and the results are not particularly groundbreaking, the contributions are convincing and undoubtedly important to the field. The paper includes an interesting Voronoi-like schematic of how to think about perceptual metamers, which I found helpful, but for which I do have some questions and suggestions. I also have some major concerns regarding incomplete psychophysical methodology including lack of eye-tracking, results inferred from a single subject, and a huge number of trials. I have only minor typographical criticisms and suggestions to improve clarity. The authors also use very good data reproducibility practices.</p>
<p>Specific Comments</p>
<p>Experimental Setup</p>
<p>
Firstly, the experiments do not appear to utilize an eye tracker to monitor fixation. Without eye tracking or another manipulation to ensure fixation, we cannot ensure the subjects were fixating the center of the image, and viewing the metamer as intended. While the short stimulus time (200ms) can help minimize eye movements, this does not guarantee that subjects began the trial with correct fixation, especially in such a long experiment. While Covid-19 did at one point limit in-person eye-tracked experiments, the paper reports no such restrictions that would have made the addition of eye-tracking impossible. While such a large-scale experiment may be difficult to repeat with the addition of eye tracking, the paper would be greatly improved with, at a minimum, an explanation as to why eye tracking was not included.</p>
<p>Secondly, many of the comparisons later in the paper (Figures 9,10) are made from a single subject. N=1 is not typically accepted as sufficient to draw conclusions in such a psychophysics experiment. Again, if there were restrictions limiting this it should be discussed. Also (P11) Is subject sub-00 is this an author? Other expert? A naive subject? The subject's expertise in viewing metamers will likely affect their performance.</p>
<p>Finally, the number of trials per subject is quite large. 13,000 over 9 sessions is much larger than most human experiments in this area. The reason for this should be justified.</p>
<p>Model</p>
<p>
For the main experiment, the authors compare the results of two models: a 'luminance model' that spatially pools mean luminance values, and an 'energy model' that spatially pools energy calculated from a multi-scale pyramid decomposition. They show that these models create metamers that result in different thresholds for human performance, and therefore different critical scaling parameters, with the basic luminance pooling model producing a scaling factor 1/4 that of the energy model. While this is certain to be true, due to the luminance model being so much simpler, the motivation for the simple luminance-based model as a comparison is unclear.</p>
<p>The authors claim that this luminance model captures the response of retinal ganglion cells, often modeled as a center-surround operation (Rodieck, 1964). I am unclear in what aspect(s) the authors claim these center-surround neurons mimic a simple mean luminance, especially in the context of evidence supporting a much more complex role of RGCs in vision (Atick &amp; Redlich, 1992). Why do the authors not compare the energy model to a model that captures center-surround responses instead? Do the authors mean to claim that the luminance model captures only the pooling aspects of an RGC model? This is particularly confusing as Figures 6 and 9 show the luminance and energy models for original vs synth aligning with the scaling of Midget and Parasol RGCs, respectively. These claims should be more clearly stated, and citations included to motivate this. Similarly, with the energy model, the physiological evidence is very loosely connected to the model discussed.</p>
<p>Prior Work:</p>
<p>
While the explorations in this paper clearly have value, it does not present any particularly groundbreaking results, and those reported are consistent with previous literature. The explorations around critical eccentricity measurement have been done for texture models (Figure 11) in multiple papers (Freeman 2011, Wallis, 2019, Balas 2009). In particular, Freeman 20111 demonstrated that simpler models, representing measurements presumed to occur earlier in visual processing need smaller pooling regions to achieve metamerism. This work's measurements for the simpler models tested here are consistent with those results, though the model details are different. In addition, Brown, 2023 (which is miscited) also used an extended field of view (though not as large as in this work). Both Brown 2023, and Wallis 2019 performed an exploration of the effect of the target image. Also, much of the more recent previous work uses color images, while the author's exploration is only done for greyscale.</p>
<p>Discussion of Prior Work:</p>
<p>
The prior work on testing metamerism between original vs. synthesized and synthesized vs. synthesized images is presented in a misleading way. Wallis et al.'s prior work on this should not be a minor remark in the post-experiment discussion. Rather, it was surely a motivation for the experiment. The text should make this clear; a discussion of Wallis et al. should appear at the start of that section. The authors similarly cite much of the most relevant literature in this area as a minor remark at the end of the introduction (P3L72).</p>
<p>White Noise:</p>
<p>
The authors make an analogy to the inability of humans to distinguish samples of white noise. It is unclear however that human difficulty distinguishing samples of white noise is a perceptual issue- It could instead perhaps be due to cognitive/memory limitations. If one concentrates on an individual patch one can usually tell apart two samples. Support for these difficulties emerging from perceptual limitations, or a discussion of the possibility of these limitations being more cognitive should be discussed, or a different analogy employed.</p>
<p>Relatedly, in Figure 14, the authors do not explain why the white noise seeds would be more likely to produce syntheses that end up in different human equivalence classes.</p>
<p>It would be nice to see the effect of pink noise seeds, which mirror the power spectrum of natural images, but do not contain the same structure as natural images - this may address the artifacts noted in Figure 9b.</p>
<p>Finally, the authors note high-frequency artifacts in Figure 4 &amp; P5L135, that remain after syntheses from the luminance model. They hypothesize that this is due to a lack of constraints on frequencies above that defined by the pooling region size. Could these be addressed with a white noise image seed that is pre-blurred with a low pass filter removing the frequencies above the spatial frequency constrained at the given eccentricity?</p>
<p>Schematic of metamerism:</p>
<p>
Figures 1,2,12, and 13 show a visual schematic of the state space of images, and their relationship to both model and human metamers. This is depicted as a Voronoi diagram, with individual images near the center of each shape, and other images that fall at different locations within the same cell producing the same human visual system response. I felt this conceptualization was helpful. However, implicitly it seems to make a distinction between metamerism and JND (just noticeable difference). I felt this would be better made explicit. In the case of JND, neighboring points, despite having different visual system responses, might not be distinguishable to a human observer.</p>
<p>In these diagrams and throughout the paper, the phrase 'visual stimulus' rather than 'image' would improve clarity, because the location of the stimulus in relation to the fovea matters whereas the image can be interpreted as the pixels displayed on the computer.</p>
<p>Other</p>
<p>
The authors show good reproducibility practices with links to relevant code, datasets, and figures.</p>
</body>
</sub-article>
</article>