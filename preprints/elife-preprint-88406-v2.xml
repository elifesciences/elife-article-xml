<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">88406</article-id>
<article-id pub-id-type="doi">10.7554/eLife.88406</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.88406.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Passive exposure to task-relevant stimuli enhances categorization learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3979-9169</contrib-id>
<name>
<surname>Schmid</surname>
<given-names>Christian</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Haziq</surname>
<given-names>Muhammad</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4855-9319</contrib-id>
<name>
<surname>Baese-Berk</surname>
<given-names>Melissa M.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Murray</surname>
<given-names>James M.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="author-notes" rid="n2">‡</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6595-8450</contrib-id>
<name>
<surname>Jaramillo</surname>
<given-names>Santiago</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="author-notes" rid="n2">‡</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Institute of Neuroscience, University of Oregon</institution>. Eugene, OR 97403</aff>
<aff id="a2"><label>2</label><institution>Department of Linguistics, University of Oregon</institution>. Eugene, OR 97403</aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Frank</surname>
<given-names>Michael J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Brown University</institution>
</institution-wrap>
<city>Providence</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>For correspondence:</bold> <email>jmurray9@uoregon.edu</email> (JMM); <email>sjara@uoregon.edu</email> (SJ)</corresp>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors contributed equally to this work</p></fn>
<fn id="n2" fn-type="others"><label>‡</label><p>Co-senior authors</p></fn>
<fn id="n3" fn-type="others"><p>The authors declare no competing financial interests.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-07-21">
<day>21</day>
<month>07</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2023-12-27">
<day>27</day>
<month>12</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP88406</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-04-17">
<day>17</day>
<month>04</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-04-04">
<day>04</day>
<month>04</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.04.04.535463"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-07-21">
<day>21</day>
<month>07</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.88406.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.88406.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.88406.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.88406.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.88406.1.sa0">Reviewer #3 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.88406.1.sa4">Author Response</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Schmid et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Schmid et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-88406-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Learning to perform a perceptual decision task is generally achieved through sessions of effortful practice with feedback. Here, we investigated how passive exposure to task-relevant stimuli, which is relatively effortless and does not require feedback, influences active learning. First, we trained mice in a sound-categorization task with various schedules combining passive exposure and active training. Mice that received passive exposure exhibited faster learning, regardless of whether this exposure occurred entirely before active training or was interleaved between active sessions. We next trained neural-network models with different architectures and learning rules to perform the task. Networks that use the statistical properties of stimuli to enhance separability of the data via unsupervised learning during passive exposure provided the best account of the behavioral observations. We further found that, during interleaved schedules, there is an increased alignment between weight updates from passive exposure and active training, such that a few interleaved sessions can be as effective as schedules with long periods of passive exposure before active training, consistent with our behavioral observations. These results provide key insights for the design of efficient training schedules that combine active learning and passive exposure in both natural and artificial systems.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Added minor clarifications and references in discussion section, fixed typos.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Active learning of a perceptual decision task requires both expending effort to perform the task and having access to feedback about task performance. Passive exposure to sensory stimuli, on the other hand, is relatively effortless and does not require feedback about performance. Since animals are continuously exposed to stimuli in their environment, the nervous system could take advantage of this passive exposure, for example by learning features related to the statistical structure of the stimulus distribution, to increase the speed and efficiency of active task learning. For auditory learning in particular, schedules that effectively combine active training and passive exposure could yield more efficient approaches for learning to discriminate ethologically relevant sounds (as needed for example during second-language learning or musical training in humans) compared to active training alone.</p>
<p>A large body of research has demonstrated that exposure to sounds early in life influences the ability to discriminate acoustic stimuli (<bold><italic><xref ref-type="bibr" rid="c23">Kuhl et al., 2003</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c25">Maye et al., 2002</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c22">Kral, 2013</xref></italic></bold>). However, the conditions under which passive exposure in the adult can help auditory learning are not well understood. In humans, previous work has demonstrated that, under specific conditions, interleaved passive exposure is beneficial for learning, sometimes to the extent that active sessions can be replaced with passive exposure and still yield similar performance (<bold><italic><xref ref-type="bibr" rid="c37">Wright et al., 2015</xref></italic></bold>). In other animals, which provide greater experimental access for investigating the neural mechanisms of learning, studies have focused mostly on the effects of perceptual learning (the experience-dependent enhancement in sensory discrimination) from exposure to stimuli during active training (<bold><italic><xref ref-type="bibr" rid="c4">Bao et al., 2004</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c33">Polley et al., 2006</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c7">Caras and Sanes, 2017</xref></italic></bold>). Although some progress has been made for other sensory modalities, such as olfaction (<bold><italic><xref ref-type="bibr" rid="c11">Fleming et al., 2019</xref></italic></bold>), the question of whether and how the combination of passive exposure with active training improves auditory learning in animal models has received little attention, limiting the ability to investigate the neural mechanisms that might be involved.</p>
<p>Using inexpensive unlabeled data during passive exposure to improve the efficiency of active training is also of great interest for machine learning, where large quantities of labeled training data are not always readily available. Recent approaches for training deep networks for speech recognition have successfully used large quantities of unlabeled data to achieve state-of-the-art levels of performance with minimal active training (<bold><italic><xref ref-type="bibr" rid="c3">Baevski et al., 2020</xref>, <xref ref-type="bibr" rid="c2">2021</xref></italic></bold>). Moreover, theoretical work inspired by neurobiology has argued that unsupervised learning, which may occur during passive exposure, could modify neural representations in such a way as to later facilitate more efficient supervised learning (<bold><italic><xref ref-type="bibr" rid="c27">Nassar et al., 2021</xref></italic></bold>). However, optimal ways to combine supervised and unsupervised learning, as well as the mechanisms that may underlie such benefits from passive exposure, remain unknown.</p>
<p>As a first step toward addressing these gaps in knowledge, we evaluated whether passive exposure to sounds improves learning of a sound categorization task in mice. We found that passive presentation of stimuli enhanced learning speed in mice, either when passive presentation occurred before any active training or when passive exposure sessions were interleaved with active training. Then, we performed a theoretical analysis of learning in artificial neural networks that combine different learning rules to identify the conditions under which they account for the experimental data. Our theoretical analysis indicates that the experimentally observed benefits of passive exposure can be accounted for by networks in which unsupervised learning in early layers shapes neural representations of sensory stimuli, while supervised learning in later layers uses these representations to drive behavior.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Learning a sound categorization task</title>
<p>We first designed a two-alternative choice sound categorization task to allow testing the effects of passive exposure on categorization learning. In this task, freely moving mice had to discriminate whether the slope of a frequency-modulated sound was positive or negative. Animals initiated each trial by poking the center port of a 3-port chamber, at which point a 200-ms sound was presented after a brief silent delay. Mice then had to choose the left or right reward ports depending on the slope of the stimulus (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). Animals were trained once per day using a schedule with the following stages (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>): shaping stages (S0-S2), in which animals learned to poke and obtain water reward; the main training stage with two stimuli (S3), in which animals learned to associate a stimulus with a reward port; and a psychometrics testing stage (S4), in which mice were tested with 6 different stimuli, including the two extremes presented in S3. <xref rid="fig1" ref-type="fig">Fig. 1C</xref> illustrates the learning performance for one mouse, showing the time to reach 70% correct trials and the performance at 21 days, as estimated from a linear fit to the daily average performance. Performance starts around 50% correct (chance level for the binary choice) and reaches a level above 80% by the end of stage S3. During this training stage, animals were limited to 500 trials in each session to facilitate comparisons across animals. Overall, the number of trials in which animals made no choice after initiating a trial was negligible (averaging 0.18% of trials across mice). <xref rid="fig1" ref-type="fig">Fig. 1D</xref> illustrates the performance of the same mouse during one session of the psychometrics testing stage (S4). As expected, stimuli with FM slopes closer to zero result in responses closer to chance level (50%).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Two-alternative choice sound categorization task for mice.</title>
<p><bold>(A)</bold> Mice initiated a trial by poking their nose into the center port of a three-port chamber, triggering the presentation of a frequency modulated (FM) sound. To obtain reward, animals had to choose the correct side port according to the slope of the frequency modulation (left for positive slopes, right for negative slopes). <bold>(B)</bold> Training schedule: mice underwent several shaping stages (S0-S2) before learning the full task; the main learning stage (stage S3) used only the highest and lowest FM slopes; psychometric performance was evaluated using 6 different FM slopes (stage S4). <bold>(C)</bold> Daily performance for one mouse during S3. Arrows indicate estimates of the time to reach 70% correct and the performance at 21 days given a linear fit (black line). <bold>(D)</bold> Average leftward choices for each FM slope during one session of S4 for the mouse in C. Error bars indicate 95% confidence intervals.</p></caption>
<graphic xlink:href="535463v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Passive exposure to sounds improves learning</title>
<p>To test whether passive exposure to sounds enhances acoustic categorization learning of these sounds, we created 3 cohorts of mice (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). The first cohort, named “active training only” (A only), followed the training schedule described above with no additional exposure to the sounds. The second cohort, “active training with passive exposure” (A+P), received additional passive exposure to sounds during stages S3 and S4. The last cohort, “passive exposure before active training” (P:A), received passive exposure to sounds before starting the main learning stage S3. Passive exposure for the A+P and P:A cohorts consisted of additional presentation of all 6 sounds used in S4, randomly ordered, while animals were in their home cages inside a sound isolation booth. Animals received an average of about 3600 passive trials each day, corresponding to 600 daily passive presentations of each of the 6 stimuli. The amount of passive exposure for P:A mice matched what A+P mice received during stage 3. During stage S4, A+P mice received additional passive sessions.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Passive exposure to sounds improves learning speed.</title>
<p><bold>(A)</bold> Training schedule for each mouse cohort: A-only mice received no passive exposure; A+P mice received passive exposure sessions during stage S3; P:A mice received a similar number of passive exposure sessions before S2. <bold>(B)</bold> Distributions of performance at 21 days of S3 given estimates from linear fits to the learning curve for each mouse from each cohort. Solid lines represent the results from a Gaussian mixture model with 2 components, separating “fast” from “slow” learners. <bold>(C)</bold> Distributions of times to reach 70% correct given estimates from linear fits. <bold>(D)</bold> Average learning curves across fast learners from each cohort. Shading represent the standard error of the mean across mice. Estimates of the time to reach 70% for fast learners from each group. Each circle is one mouse. Horizontal bar represents the median. <bold>(F)</bold> Estimates of the performance at 21 days for fast learners from each group. <bold>(G)</bold> Actual performance averaged across the last 4 days of S3 for fast learners from each group. Stars indicate <italic>p</italic> &lt; 0.05.</p></caption>
<graphic xlink:href="535463v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We evaluated the learning performance of animals from each cohort by fitting a straight line to the daily performance of each mouse during S3 and estimating two quantities from these fits: the performance at 21 days (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>), and the number of days required to reach 70% of trials correct (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). The distributions of these estimates suggested that each cohort included two types of learners: one group of fast learners and one group of slow learners. To test whether this bimodality was indeed present, we applied a Gaussian mixture model to the data in <xref rid="fig2" ref-type="fig">Fig. 2B</xref> and compared the Bayesian Information Criterion (BIC) for models with <italic>k</italic> = 1 vs. <italic>k</italic> = 2 Gaussian components. When comparing BIC, models with lower BIC are generally preferred. We found that, for both cohorts that included passive exposure, the BIC for <italic>k</italic> = 2 was lower than for <italic>k</italic> = 1 (<xref rid="tbl1" ref-type="table">Table 1</xref>), indicating that a model with two components best captured the data. For the cohort with no passive exposure, the BIC was lower when using a single component. However, to make comparisons across cohorts more equitable, we applied the 2-component Gaussian mixture model to all cohorts and focused further analysis on the group of faster learners from each cohort. The mice that were categorized as fast learners based on their performance at 21 days (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>) were the same mice that were categorized as fast learners based on the number of days to reach 70% performance (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Hyperparamters used for shown results</title></caption>
<graphic xlink:href="535463v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>As a first test of whether passive exposure influenced learning speed, we quantified the average learning curve across fast learners from each cohort (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>). The learning curves show a clear improvement for animals that had passive exposure compared to those who did not. These curves, however, suggested no differences between animals that had interleaved passive-exposure sessions during active-training days (A+P) and animals that had all of their passive exposure occur before the main learning stage (P:A). A quantification of these effects across animals confirmed these observations. Specifically, the time to reach 70% correct trials (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>) was shorter for animals that experienced passive exposure to sounds compared to those who did not (<italic>p</italic> = 0.01 for A only vs. A+P, <italic>p</italic> = 0.006 for A only vs. P:A, Wilcoxon rank-sum test), while we found no statistically significant difference between the A+P and P:A cohorts (<italic>p</italic> = 0.71, Wilcoxon rank-sum test). A similar result was observed for the estimated performance at 21 days (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>), where animals with passive exposure showed better performance (<italic>p</italic> = 0.01 for A only vs. A+P, <italic>p</italic> = 0.006 for A only vs. P:A, Wilcoxon rank-sum test), while we found no difference between the A+P and P:A cohorts (<italic>p</italic> = 0.47, Wilcoxon rank-sum test). These observations point to an unexpected result: the performance of animals after a few interleaved passive exposure sessions was as high as that for animals that had received all passive exposures before learning the task, suggesting an interaction between passive exposure and active learning.</p>
<p>To ensure that the apparent effects of passive exposure were not the result of using a simple linear fit to the learning data, we also compared the performance of each animal averaged over the last four days of the learning stage (<xref rid="fig2" ref-type="fig">Fig. 2G</xref>). Comparisons across cohorts matched those observed from the linear fit estimates, where mice with passive exposure displayed higher performance (<italic>p</italic> = 0.01 for A only vs. A+P, <italic>p</italic> = 0.006 for A only vs. P:A, <italic>p</italic> = 0.71 for A+P vs. P:A, Wilcoxon rank-sum test). An analysis of the slow learners from each cohort revealed similar trends, where the performance on the last four days for animals with only active sessions was lower (66.5% across 2 mice) than for mice with passive exposure (70.4% across 3 A+P mice and 76.5% across 4 P:A mice), although these differences were not statistically significant (p-values in the range 0.064–0.16, Wilcoxon rank-sum test). Overall, these results indicate that passive exposure to task-relevant sounds—either before or during learning—can enhance acoustic categorization learning in adult mice, and they point to a non-trivial interaction between passive exposure and active learning, which we further investigate in our theoretical analysis below.</p>
</sec>
<sec id="s2c">
<title>Passive exposure influences responses to intermediate sounds not used during training</title>
<p>To test whether passive exposure influenced the behavioral responses to sounds beyond those used during the active training sessions, we evaluated the psychometric performance of animals from each cohort during stage S4. We first estimated the average psychometric curves across all mice from each cohort during the first 4 days of S4 (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>). These curves illustrate that, as expected from the results of S3, the performance on the extreme sounds is better for animals that received passive exposure. Moreover, these curves hinted at differences across cohorts in the responses to intermediate sounds, which were presented during passive exposure, but had not been part of the active training during the learning stage. To quantify these effects, we first measured the average performance across all stimuli and found that animals that experienced passive exposure achieved a higher fraction of correct trials overall compared to those that did not experience passive exposure (<italic>p</italic> = 0.01 for A only vs. A+P, <italic>p</italic> = 0.01 for A only vs. P:A, <italic>p</italic> = 0.27 for A+P vs. P:A, Wilcoxon rank-sum test) (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>). This disparity in overall performance was the result of differences in both the responses to the extreme sounds (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>) as well the responses to intermediate sounds (<italic>p</italic> = 0.037 for A only vs. A+P, <italic>p</italic> = 0.028 for A only vs. P:A, <italic>p</italic> = 0.36 for A+P vs. P:A, Wilcoxon rank-sum test) (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>). To evaluate the possibility that having no passive exposure resulted in shallower psychometric curves, we compared the psychometric slopes across cohorts. We found no significant difference in psychometric slopes across cohorts (<italic>p</italic> = 0.07 for A only vs. A+P, <italic>p</italic> = 1 for A only vs. P:A, <italic>p</italic> = 0.86 for A+P vs. P:A, Wilcoxon rank-sum test, <xref rid="fig3" ref-type="fig">Fig. 3E</xref>). This observation, together with the changes in performance for extreme stimuli, suggest that the observed effects of passive exposure are not simply captured by psychometric curves becoming less shallow. Overall, these observations indicate that passive exposure can have an effect on the behavioral responses to stimuli beyond those used during the active training sessions.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Passive exposure improves categorization of intermediate stimuli.</title>
<p><bold>(A)</bold> Average psychometric performance for the first 4 days of stage S4 across fast learners from each group. Error bars show the standard error of the mean across mice. <bold>(B)</bold> Performance averaged across all stimuli is better for mice with passive exposure. Horizontal lines indicate median across mice. <bold>(C)</bold> Performance for extreme stimuli (included in S3) is better for mice with passive exposure. <bold>(D)</bold> Performance for intermediate stimuli (which were not used in the task before S4) is better for mice with passive exposure. <bold>(E)</bold> Psychometric slope is not different across groups. <bold>(F-J)</bold> All groups achieve similar levels of performance after 3 weeks of additional training. Stars indicate <italic>p</italic> &lt; 0.05.</p></caption>
<graphic xlink:href="535463v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To test whether the asymptotic performance of animals was affected by passive exposure, we compared the psychometric performance across cohorts after 21 days of S4 sessions. We found that animals with no passive exposure improved their average performance during these few weeks (<italic>p</italic> = 0.031 when comparing early and late periods of S4, Wilcoxon signed-rank test), while other cohorts did not change in any consistent manner, suggesting that most of these mice had already reached asymptotic performance (<italic>p</italic> = 0.56 for A+P, <italic>p</italic> = 1.0 for P:A, Wilcoxon signed-rank test). During this period, animals with no passive exposure improved until they were indistinguishable from those with passive exposure (<italic>p</italic> &gt; 0.26 for all comparisons, Wilcoxon rank-sum test) (<xref rid="fig3" ref-type="fig">Fig. 3F-J</xref>). These results indicate that the differences observed across cohorts were not the result of specific sets of animals having predisposition for poorer learning, but rather, that passive exposure speeds up learning performance without an apparent change in final performance.</p>
</sec>
<sec id="s2d">
<title>A one-layer model does not benefit from passive pre-exposure</title>
<p>In the experiments described above, we found that passive exposure to task-relevant stimuli benefits learning, regardless of whether it occurred before or in-between active training sessions. In order to gain insight into the neural mechanisms that might underlie this observation, we analyzed the effects of active learning and passive exposure in a family of artificial neural network models combining supervised and unsupervised learning. Specifically, we evaluated the consequences of different learning algorithms, learning schedules, network architectures, and stimulus distributions on the learning outcomes. To simulate frequency-modulated sound inputs, we provided our models with input representations <inline-formula><inline-graphic xlink:href="535463v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="535463v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> deterministically encodes the stimulus, and <inline-formula><inline-graphic xlink:href="535463v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is isotropic noise with covariance <inline-formula><inline-graphic xlink:href="535463v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The binary output of the models corresponds to the choice to lick the left or right port in the experiment. We assumed that one direction in the space of all possible input representations corresponds to the FM slope parameter. The sounds with the most extreme FM slopes presented to the models lie at the points <inline-formula><inline-graphic xlink:href="535463v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and, as for the mice in the experiment, the task the models were tested on was to associate these two most extreme input representations with the labels ±1. We trained the models with combinations of:</p>
<list list-type="bullet">
<list-item><p>Active Learning: The model was provided with a sample from the extremes of the stimulus distribution and the corresponding sample label.</p></list-item>
<list-item><p>Passive Exposure: The model was provided with a sample but no label. To replicate the passive exposure sessions in the experiment, these passive samples were drawn from normal distributions with means at six points on the line segment between <inline-formula><inline-graphic xlink:href="535463v2_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="535463v2_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and noise covariance Σ.</p></list-item>
</list>
<p>We aimed to find models that can replicate the general experimental observation that passive exposure improves learning speed. Thus, all the models we considered included some parameters that were trained using unsupervised learning, which does not require feedback about task performance, during the active and passive training sessions. Since the models also had to learn the association between labels and stimuli, they also needed to include parameters that were trained during active sessions using supervised learning, which makes use of feedback about task performance.</p>
<p>Active trials and passive exposures were combined into the following three training schedules:</p>
<list list-type="bullet">
<list-item><p>Active only (A only): The model was always provided with a sample and its label during 5000 total trials of training.</p></list-item>
<list-item><p>Active and Passive (A + P): The model also underwent 5000 active trials, but each one was followed by 9 passive exposures.</p></list-item>
<list-item><p>Passive then Active (P : A): The model was first presented with 45000 passive exposures, then underwent 5000 active trials.</p></list-item>
</list>
<p>The first model (Model 1) we considered was the simplest possible neural network model, which consisted of a single output neuron reading out from an input representation (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>). In this one-layer model, the input representation <inline-formula><inline-graphic xlink:href="535463v2_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> was multiplied by a weight vector <inline-formula><inline-graphic xlink:href="535463v2_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to produce an output <inline-formula><inline-graphic xlink:href="535463v2_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>S</italic> is the logistic sigmoid function. The task the model had to perform is illustrated in <xref rid="fig4" ref-type="fig">Fig. 4B</xref>: the weight vector <inline-formula><inline-graphic xlink:href="535463v2_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is orthogonal to the decision hyperplane, so its optimal orientation would be along the direction <inline-formula><inline-graphic xlink:href="535463v2_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Accounting for the experimental data requires a model that changes even when no labels are provided during passive exposure. Therefore, we trained the model with both <italic>supervised learning</italic> (in which the weight <inline-formula><inline-graphic xlink:href="535463v2_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> undergoes gradient descent, equivalent to logistic regression) and <italic>unsupervised learning</italic> (where the weight <inline-formula><inline-graphic xlink:href="535463v2_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is trained with Hebbian learning with weight decay using Oja’s rule (<bold><italic><xref ref-type="bibr" rid="c28">Oja, 1982</xref></italic></bold>)). The unsupervised learning rule used here aligns the normal vector to the decision hyperplane with the direction of highest variance in the input representation, which in this case is the coding direction spanned by <inline-formula><inline-graphic xlink:href="535463v2_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. For training, we used the three learning schedules introduced above. During the active sessions, the weight <inline-formula><inline-graphic xlink:href="535463v2_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> was trained by both supervised and unsupervised learning, while, during the passive sessions, only the unsupervised learning rule was used.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>A single-layer model (Model 1) does not benefit from passive pre-exposure.</title>
<p><bold>(A)</bold>: Network architecture for the one-layer model. <bold>(B)</bold>: The network is trained to find a hyperplane orthogonal to the decoding direction <inline-formula><inline-graphic xlink:href="535463v2_inline52.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. <bold>(C)</bold>: Learning performance for different training schedules.</p></caption>
<graphic xlink:href="535463v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As in the experiment, interleaving active trials with passive exposure (the A + P condition) facilitated learning and slightly sped up training relative to active-only training (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>). Long passive pre-exposure before active learning (P:A condition), however, did not contribute to task learning in this model. This can be explained by the symmetry of the task: the decision hyperplane oriented itself in the optimal direction to separate the point clouds, however, because the algorithm didn’t know about the data labels, there was a 50% chance it was correctly oriented, and a 50% chance it was oriented exactly in the wrong direction. These two possible configurations of the model after the passive pre-exposure sessions averaged out, giving no net benefit to the P:A schedule over the active-only training. Because of this failure, the one-layer model cannot capture the experimental observation that passive pre-exposure improves the speed of learning.</p>
</sec>
<sec id="s2e">
<title>Passive exposure is beneficial when building latent representations with unsupervised learning</title>
<p>To remedy this shortcoming, we studied a simple extension to the above model by adding an additional layer of hidden neurons. In addition to the readout weights <inline-formula><inline-graphic xlink:href="535463v2_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, this two-layer model has initial weights <italic>W</italic> mapping the input representation <inline-formula><inline-graphic xlink:href="535463v2_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to a hidden representation <inline-formula><inline-graphic xlink:href="535463v2_inline19.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The output of the model is then <inline-formula><inline-graphic xlink:href="535463v2_inline20.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. For our simulations, the dimension of the hidden layer <italic>d</italic><sub>hid</sub> was smaller than the input dimension <italic>d</italic>. We trained <inline-formula><inline-graphic xlink:href="535463v2_inline21.gif" mimetype="image" mime-subtype="gif"/></inline-formula> using the same algorithms as in the one-layer model. In addition, we have the option to train <italic>W</italic> with either supervised learning, unsupervised learning, or both. As for the one-layer model, we needed to include both supervised and unsupervised learning to account for the experimental observations. The simplest way to incorporate this is to have supervised learning in one layer and unsupervised learning in the other one.</p>
<p>First, we investigated what happens if we use supervised learning for the input weights <italic>W</italic> and unsupervised learning for the readout weights <inline-formula><inline-graphic xlink:href="535463v2_inline22.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. This defines Model 2 (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>). In this model, the relative learning performance for all schedules was similar to that of Model 1 (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>). This can be understood by observing that, since the hidden-layer representation did not change during passive exposure in the P:A schedule, the active sessions started from a representation that did not allow the signal to be decoded, as illustrated in <xref rid="fig5" ref-type="fig">Fig. 5B</xref>. Thus, there was no benefit from the initial passive sessions in this model, which contradicts the experimental outcomes. Here, we do not make claims about the relative performance of the learners in Model 2 with those of Model 1 (since the hyperparameters of each model were chosen independently such that they give rise to similar asymptotic accuracies). Instead, the conclusions are drawn by comparing different learning schedules for a given model.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Passive exposure benefits learning in a two-layer model with unsupervised learning in the first layer.</title>
<p><bold>(A)</bold>: Network architecture for Model 2, which has supervised learning (SL) at the input layer, and unsupervised learning (UL) at the readout. <bold>(B)</bold>: Learning dynamics of the hidden-layer representation of this model for the P:A schedule. <bold>(C)</bold>: Learning performance for different training schedules for Model 2. <bold>(D-F)</bold>: Model 3, which has unsupervised learning (UL) at the input layer, and supervised learning (SL) at the readout.</p></caption>
<graphic xlink:href="535463v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Another possibility (Model 3) to incorporate both types of learning in this network is to learn the input weights <italic>W</italic> by unsupervised learning and the readout weights <inline-formula><inline-graphic xlink:href="535463v2_inline23.gif" mimetype="image" mime-subtype="gif"/></inline-formula> using supervised learning (<xref rid="fig5" ref-type="fig">Fig. 5D</xref>). Doing so allowed the passive exposures to build a representation of the first principal component of the input representation, which enhanced the decodability in the hidden layer (<xref rid="fig5" ref-type="fig">Fig. 5E</xref>). For this model, the speed of learning for both the P:A and A+P training schedules increased compared to the active-only case (<xref rid="fig5" ref-type="fig">Fig. 5F</xref>), accounting for the main feature of behavioral experiments, namely, that passive exposure enhances learning speeds.</p>
<p>While this general behavior is consistent with the experimental results, two main issues remain to better account for the data. First, the learning curve for the P:A learners in <xref rid="fig5" ref-type="fig">Fig. 5D</xref> rises very quickly compared to the A+P models (which built up the same representation more gradually during the active trial period). In contrast, these two conditions had similar learning speeds in our experiments. Second, because of the unsupervised learning rule chosen for Models 1-3, the coding direction of the neural representation of stimuli must align with the first principal component of the neural representation for the system to benefit from passive exposure. In general, however, features that are relevant for learning will not always be encoded along the first principal component. The following section presents solutions to both of these issues.</p>
</sec>
<sec id="s2f">
<title>Building higher-dimensional latent representations improves learning for more-general input distributions</title>
<p>A limitation of Model 3 is that Hebbian learning only builds a representation of the direction of highest variance (the first principal component) of the input representation. In real neural representations, however, the coding direction will not always align with the first principal component. To investigate what happens in this case, we trained Model 3 on a non-isotropic input distribution, setting <inline-formula><inline-graphic xlink:href="535463v2_inline24.gif" mimetype="image" mime-subtype="gif"/></inline-formula> with <italic>σ</italic><sub>1</sub> ≫ <italic>σ</italic><sub>2</sub>. We set the extreme means <inline-formula><inline-graphic xlink:href="535463v2_inline25.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="535463v2_inline26.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the unit vector along the <italic>i</italic>th dimension. For these inputs, the coding direction represents the second principal component (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>). In this case, passive exposure is counterproductive for learning because the variance in the coding direction is lost in the hidden-layer representation, so it cannot be used to distinguish the two input distributions, leading to worse performance of A+P and P:A learners relative to A-only learners (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>An alternative unsupervised learning rule can be used to learn hidden representations of higher principal components.</title>
<p><bold>(A)</bold>: A non-isotropic input distribution in which the coding dimension does not align with the direction of highest variance. <bold>(B)</bold>: Learning performance for Model 3 on the non-isotropic input distribution. <bold>(C)</bold>: Network architecture for a two-layer model (Model 4) that uses the similarity matching algorithm for the input weights and supervised learning at the readout. <bold>(D)</bold>: Learning performance for Model 4 on the non-isotropic input distributions.</p></caption>
<graphic xlink:href="535463v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To address this issue, we implemented a model that uses the <italic>similarity matching</italic> unsupervised learning algorithm (<bold><italic><xref ref-type="bibr" rid="c31">Pehlevan et al., 2015</xref></italic></bold>) to build a higher-dimensional hidden representation that includes higher principal components. To do this, we modified the network to include lateral weights <italic>M</italic> connecting the hidden units in our two-layer model (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>). These new connections were trained using anti-Hebbian learning and have the function of decorrelating the hidden units.</p>
<p>With these modifications, which define Model 4, the performance (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>) was similar to that of Model 3 with the isotropic input distribution (<xref rid="fig5" ref-type="fig">Fig. 5F</xref>). Specifically, in the P:A schedule, the representation built up during passive sessions aided decodability, slightly increased the speed of learning, and led to a higher final performance relative to the A-only schedule. However, while the P:A learning performance exhibited some improvement over the A-only performance, the A+P learning performance did not show a comparable improvement for any set of hyperparameters that we investigated (see Methods). This is because, in the P:A schedule, the active training benefits from a representation that aids decoding, while this representation must be built up over time for the learners in the A+P schedule. Thus, the P:A curve in <xref rid="fig6" ref-type="fig">Fig. 6D</xref> initially rises faster than the A+P curve, unlike in the experimental outcomes shown in <xref rid="fig2" ref-type="fig">Fig. 2D</xref>. Together, these results show that, in the case where the task-relevant encoding direction is not aligned with the direction of highest variance in the input representation, a more-sophisticated unsupervised learning algorithm is capable of accounting for the enhanced learning due to passive exposure, but not for the similarity in the improvements exhibited by the A+P and P:A training schedules.</p>
<p>So far, we have only considered input stimulus representations for which the decoding direction lay in the subspace spanned by the highest principal components, such that unsupervised learning at the input layer is sufficient to create an optimal latent representation. In a more natural setting, parts of the decoding direction might be aligned with the highest principal components, but part of it might not be. To study this case, we created a model (Model 5) with two new features. First, this model receives an input representation in which the coding direction has a nonzero projection along the first 30 principal components. Second, because an unsupervised learning rule that finds leading principal components alone is insufficient to reach an optimal solution when the signal is not entirely contained within the leading principal components, this model combines both supervised and unsupervised learning at the input layer (<xref rid="fig7" ref-type="fig">Fig. 7A</xref>, <xref rid="fig7" ref-type="fig">7B</xref>). Compared to Models 3 and 4, P:A and A+P schedules led to similar improvements over A-only training (<xref rid="fig7" ref-type="fig">Fig. 7C</xref>), consistent with the experimental results shown in <xref rid="fig2" ref-type="fig">Fig. 2D</xref>.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>A more-general two-layer model accounts for similar benefits of A+P and P:A training schedules.</title>
<p><bold>(A)</bold>: Schematic illustration of Model 5, which combines supervised and unsupervised learning at the input layer of weights. <bold>(B)</bold>: Input distribution, in which the decoding direction is not aligned with any particular principal component. <bold>(C)</bold>: Learning performance of Model 5. <bold>(D)</bold>: Psychometric curves showing classification performance for all stimuli after 1500 trials, where the stimulus parameter <italic>ρ</italic> linearly interpolates between the two extreme stimulus values. <bold>(E)</bold>: Psychometric curves showing classification performance after 5000 trials. <bold>(F)</bold>: Schematic illustration of the angle between the summed weight updates during active training and passive exposure for A+P (top) and P:A (bottom) learners. <bold>(G)</bold>: The alignment of active and passive weight updates for <italic>n</italic> = 50 networks trained with either the A+P or P:A schedule after 1500 trials (<italic>p</italic> &lt; 10<sup>−3</sup>, Wilcoxon rank-sum test; box percentiles are 25/75).</p></caption>
<graphic xlink:href="535463v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In addition, when we tested the performance for all 6 stimuli at different points in learning, this model reproduced the main features of the psychometric curves from the experimental data. In particular, midway through training we found that the psychometric curves for the A+P and P:A learners had fully converged and were mostly overlapping, while the A-only learners exhibited relatively poorer classification performance for all stimulus values (<xref rid="fig7" ref-type="fig">Fig. 7D</xref>; cf. <xref rid="fig3" ref-type="fig">Fig. 3A</xref>). At the end of training, all three curves converged to very similar values for all stimulus values (<xref rid="fig7" ref-type="fig">Fig. 7E</xref>; cf. <xref rid="fig3" ref-type="fig">Fig. 3F</xref>). Thus, Model 5 reproduced all of the key findings from the experiments, including the behaviors of the learning curves and psychometric curves in all three training conditions.</p>
<p>In Models 3 and 4, we found that P:A learning was initially faster than A+P learning, which we attributed to the larger number of representation-improving passive exposures that the P:A learners received in the early phases of training. Somewhat surprisingly, we then found in Model 5 that the P:A and A+P learning curves rose comparably quickly during the initial phase of active training. We hypothesized that this occurred due to an improved alignment in the weight updates during active learning and passive exposure in the A+P case (in which these updates occur in alternation) <italic>vs</italic>. the P:A case (in which all of the passive-exposure updates occur before the active-training updates). To test this, we computed the angle of the sum of all active updates relative to the sum of all passive updates in A+P and P:A learners (<xref rid="fig7" ref-type="fig">Fig. 7F</xref>). Consistent with our hypothesis, we found that the active and passive updates were more aligned for the A+P learners than for the P:A learners (<xref rid="fig7" ref-type="fig">Fig. 7G</xref>). This result establishes a potential mechanism by which an interleaved schedule of active training and passive exposure leads to more-efficient learning (in the sense of requiring, for a given number of active-training steps, fewer passive exposures to achieve a given performance) than a schedule in which passive exposure entirely precedes active training.</p>
<p>Together, our experimental and theoretical results have shown that the experimentally observed benefit of passive exposure in both the P:A and the A+P schedules is consistent with neural network models that build latent representations of features that are determined by statistical properties of the input distribution, as long as those features aid the decoding of task-relevant variables.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this work, we have shown that passive exposure to task-relevant stimuli increases the speed of learning during active training in adult mice performing a sound-categorization task. Specifically, for the amount of passive exposure used here, we found similar increases in cases where the passive exposure occurred before active training or interleaved with active training, even at early points where the cumulative number of passive exposures in the latter case was far smaller. Using artificial neural networks, we showed that these results are consistent with a multi-layer model in which unsupervised learning in an early layer creates a latent hidden representation that reflects the statistics of the input stimuli, and supervised learning in a later layer is then used to decode the stimulus properties and map them onto appropriate behaviors. Finally, we found that improved learning efficiency when passive exposure is interleaved with active training rather than occurring entirely before active training can be accounted for by active and passive weight updates adding together more constructively in interleaved training—a result that may have implications for designing optimal training schedules in humans, animals, and artificial neural networks.</p>
<p>Various lines of research have investigated the idea that exposure to stimuli may influence perceptual judgements. Multiple studies have demonstrated that the statistics of sensory stimulation during an animal’s development have a strong influence on the perceptual abilities (and associated neural correlates) in the adult (<bold><italic><xref ref-type="bibr" rid="c19">Hensch, 2004</xref></italic></bold>). Other studies in adults have focused on perceptual learning, generally defined as experience-dependent enhancements of the ability to perceive and discriminate sensory stimuli during perceptual decision tasks (<bold><italic><xref ref-type="bibr" rid="c15">Gold and Watanabe, 2010</xref></italic></bold>). These studies have shown decreases in the strength, quality or duration of a stimulus needed to obtain a particular level of accuracy, as animals get more exposure to the task stimuli. A key observation from these studies is that these effects can be disambiguated from other forms of learning, such as those that establish task rules. In addition to these perceptual enhancements, learning related to specific stimulus features can occur even when subjects are not told of the relevance of these features for a given task. Studies of this phenomenon, usually called “incidental learning”, have shown for example that subjects can incidentally learn categories of complex acoustic exemplars that occur before visual stimuli, even when the instructed task is visual detection (<bold><italic><xref ref-type="bibr" rid="c12">Gabay et al., 2015</xref></italic></bold>). Beyond these effects of incidental learning, studies in humans have found that, under specific conditions, passive exposure to sounds interleaved with training can be beneficial for learning, sometimes to the extent that active sessions can be replaced with passive exposure and still yield similar performance (<bold><italic><xref ref-type="bibr" rid="c37">Wright et al., 2015</xref></italic></bold>)—an effect that was later replicated for olfactory learning in mice (<bold><italic><xref ref-type="bibr" rid="c11">Fleming et al., 2019</xref></italic></bold>). Our experimental results complement these observations by demonstrating that passive exposure in adult mice (either interleaved with training or before training) enhances the learning of acoustic categories, opening new avenues for the detailed investigation of the neural mechanisms of the improvements that result from passive exposure in audition.</p>
<p>One important question to address in future studies is to which extent the passive stimuli need to be related to those presented in the task. Related to this point, previous work has shown that sensory enrichment alone can change cortical sensory maps (<bold><italic><xref ref-type="bibr" rid="c32">Polley et al., 2004</xref></italic></bold>) and improve task-performance (<bold><italic><xref ref-type="bibr" rid="c24">Mandairon et al., 2006</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c1">Alwis and Rajan, 2014</xref></italic></bold>) . Beyond the effects on learning and perceptual judgment, studies in rodents have shown that the effect of stimulus pre-exposure in classical conditioning paradigms can vary depending on test procedures, the similarity of preexposure and training procedures, and the choice of response measure (<bold><italic><xref ref-type="bibr" rid="c8">De Hoz and Nelken, 2014</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c20">Holland, 2018</xref></italic></bold>). This suggests that learning associations related to reward or punishment and the perceptual enhancements that come from passive exposure may rely on different mechanisms and, under some conditions, compete with each other (<bold><italic><xref ref-type="bibr" rid="c26">McLaren et al., 1989</xref></italic></bold>). Therefore, the design of schedules that benefit optimally from passive exposure must take these mechanisms into account. Given these constraints, a better understanding of the neural mechanisms underlying the influence of passive exposure, achievable through a combination of theoretical approaches and experiments in animals that provide sufficient experimental access, have the potential to guide the design of appropriate schedules in a more efficient manner compared to behavioral experiments alone.</p>
<p>In our experiments, we found that providing animals with passive exposure before task training <italic>vs</italic>. interleaved with task training led to comparable benefits. This unexpected result could be a consequence of the large number of passive-exposure trials provided to the animal on each day. A comprehensive evaluation of the effects on learning performance as a function of number of passive exposures may be needed to test this hypothesis. Our models, in contrast, most often found that exposure before task training led to larger gains (Models 3-4), compared to interleaved exposure, although we also found a model that led to comparably large gains in these two cases (Model 5). These theoretical results suggest that different schedules of passive exposure and active training might lead to significant differences in learning performance, and future experimental work could test whether this in fact occurs.</p>
<p>Our models make the experimental prediction that stimulus features should become more easily decodable from neural representations following frequent exposure to those stimulus features, even before those stimuli have occurred within the context of a learned task. Related to this idea, previous work has shown that neural responses in primary sensory cortices exhibit within-session adaptation to stimulus statistics (<bold><italic><xref ref-type="bibr" rid="c9">Dean et al., 2005</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c34">Sharpee et al., 2006</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c13">Garcia-Lazaro et al., 2007</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c18">Gutnisky and Dragoi, 2008</xref></italic></bold>). However, less is known about how such within-session adaptation relates to long-term plasticity occurring across days. Future experiments that include recording throughout task learning could test whether neural representations evolve in a manner consistent with our models.</p>
<p>In the machine learning literature, various approaches to combine labeled and unlabeled data in a semi-supervised learning classification algorithm have been put forward (<bold><italic><xref ref-type="bibr" rid="c36">Van Engelen and Hoos, 2020</xref></italic></bold>), including some biologically plausible implementations (<bold><italic><xref ref-type="bibr" rid="c17">Gu et al., 2019</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c14">Genkin et al., 2019</xref></italic></bold>). In most of these, unlabeled data is either used for regularization (<italic>e</italic>.<italic>g</italic>., <bold><italic>Belkin et al</italic></bold>. (<bold><italic>2005</italic></bold>)) or is assigned pseudo-labels and then used for training (<bold><italic><xref ref-type="bibr" rid="c35">Triguero et al., 2015</xref></italic></bold>). In contrast, in our model we used unsupervised learning in an early layer to create a useful representation for supervised learning downstream, a simple form of semi-supervised feature learning.</p>
<p>One limitation of our modeling approach is that the set of models that we consider does not include some features that may be important for fully capturing the mechanisms of unsupervised learning during passive exposure in the brain. More-sophisticated approaches beyond learning rules that implement linear dimensionality reduction will be required for cases in which relevant stimulus features are encoded in highly nonlinear ways, as would likely be the case for natural sound stimuli. If the input statistics are very complex, simple forms of initial unsupervised learning such as the ones that we used might not be helpful for improving hidden-layer representations and learning (<bold><italic><xref ref-type="bibr" rid="c21">Iyer et al., 2020</xref></italic></bold>). Recent years have seen tremendous advances in addressing this challenge by the use of self-supervised learning to learn complex stimulus features in deep neural networks (<italic>e</italic>.<italic>g</italic>., <bold><italic>Devlin et al</italic></bold>. (<bold><italic>2018</italic></bold>); <bold><italic>Oord et al</italic></bold>. (<bold><italic>2018</italic></bold>); <bold><italic>Grill et al</italic></bold>. (<bold><italic>2020</italic></bold>)). While the models that we have presented make simplifying assumptions about the stimulus statistics and learning rules, we conjecture that the principle they are meant to illustrate—namely, that unsupervised learning can make subsequent supervised learning more efficient by improving neural representations— applies broadly across different stimulus statistics and learning rules.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Animal subjects</title>
<p>A total of 27 wild-type C57BL/6J adult mice of both sexes, ages 2.5–4 months at the beginning of behavioral training, were used in this study. All mice were housed in groups of same-sex littermates in a 12:12 hour light-dark cycle. Experiments were carried out during the dark period, when mice are most active. Mice were water-restricted to motivate them to perform the behavioral task. Mice were weighed and their health checked after each behavioral session, and they were provided with a water supplement if their weight was below 80% of their baseline. Except for these supplements, access to water was restricted to the time of the task during experimental days. Free water was provided on days with no experimental sessions. All procedures were carried out in accordance with National Institutes of Health Standards and were approved by the University of Oregon Institutional Animal Care and Use Committee.</p>
<p>The behavioral data was collected using the taskontrol software platform (<ext-link ext-link-type="uri" xlink:href="https://taskontrol.readthedocs.io">https://taskontrol.readthedocs.io</ext-link>) written in the Python programming language (<ext-link ext-link-type="uri" xlink:href="http://www.python.org">www.python.org</ext-link>). Freely moving mice were trained to discriminate whether the slope of a 200-ms frequency-modulated sound was positive or negative. Animals initiated each trial by poking the center port of a 3-port chamber, at which point the sound was presented after a brief silent delay (150-250 ms, uniformly distributed). Mice then had to choose the left or right reward ports depending on the slope of the stimulus: left for an upward frequency sweep and right for a downward sweep. Animals were allowed to withdraw before the end of the sound to make a choice, and had up to 4 seconds after the end of the sound to enter a side port. If a mouse did not respond in this period of time, the trial was aborted and not considered during data analysis. Correct choices were rewarded with a 2ul water, while incorrect choices yielded nothing and animals had to start a new trial by poking again in the center port.</p>
<p>Animals were first trained with frequency-modulated sounds that spanned the frequency range from 6 to 13 kHz, resulting in an FM slope of +/-5.6 oct/s. To evaluate psychometric performance, the frequency range was varied to achieve intermediate FM slopes (3.4 and 1.1 oct/s), while keeping the duration of the sounds and the middle frequency constant. All sounds were presented at an intensity of 70 dB SPL.</p>
</sec>
<sec id="s4b">
<title>Training stages</title>
<p>Mice were trained to perform the task through a sequence of shaping stages and having a single 1-hour behavior session each day. In stage 0, the goal was to familiarize animals with the reward delivery ports. During this stage, whenever an animal poked in the side port corresponding to that trial (the port was randomized on each trial), water was delivered immediately. Animals stayed in this stage for 2 days. The goal of Stage 1 was to teach animals that a trial starts by poking in the center. During this stage, whenever the animal poked in the center port, water was delivered immediately in the corresponding side port for that trial. Animals stayed in this stage for 4 days. The goal of Stage 2 was to teach animals to wait for the beginning of the sound and only then make a choice by reaching the correct reward side port. If animals reached the incorrect port, they still had a chance to get a reward by going to the other side port within 4 seconds of the end of the sound stimulus. During this stage, the delay between the center poke and the stimulus was increased by 10 ms every 10 trials, starting at 10 ms. Animals stayed in this stage until 70% of the mice achieved 300 rewarded trials in a session (corresponding to 12 days for “A only” and “A+P” cohorts, and 9 days for the “P:A” cohort).</p>
<p>Stage 3 was the main learning stage in which animals only got rewarded if they made the correct choice in their first attempt on each trial. During this stage, we implemented a bias-correction method as follows. If the percentage of correct choices on either side was lower than 20%, the next session was set in a mode where error trials were followed by identical trials, until the mouse made the correct choice. Animals were taken off bias correction when the percentage of correct choices for both sides was above 30%. Bias-correction sessions were not included in the analysis of learning speed. Learning performance during stage 3 was evaluated for 26 days.</p>
<p>After the main learning stage, animals transitioned to Stage 4 where we evaluated their psycho-metric performance by introducing 4 new sounds of intermediate FM slope, for a total of 6 sounds per session. Which sound was presented on each trial was randomized according to a uniform distribution. The 3 sounds with positive FM slope were rewarded on the left port, while those with negative FM slope were rewarded on the right port.</p>
</sec>
<sec id="s4c">
<title>Passive exposure</title>
<p>Mice were grouped into 3 cohorts: an “active training only” (A only) cohort, an “active training with passive exposure” (A+P) cohort, and a “passive exposure before active training” (P:A) cohort. Animals that eventually formed the first two cohorts were trained simultaneously in stages 0-2. This group was then split into the “A only” and “A+P” by selecting animals to match as closely as possible the average initial performance after shaping between the two cohorts. The “P:A” mice were trained as a separate cohort. Animals in this cohort had free access to water until their active training sessions started. One mouse that did not perform enough trials in stage 2 was removed from the study and excluded from further analysis. The total number of animals included in each cohort was therefore: 8 A only mice, 9 A+P mice, and 9 P:A mice.</p>
<p>Passive exposure consisted of the additional presentation of all 6 sounds used in stage 4, randomly ordered, while animals were in their home cages inside a sound isolation booth. Animals received an average of about 3600 passive trials each day, corresponding to 600 daily passive presentations of each of the 6 stimuli. Stimuli were presented every 4.5 seconds. During these sessions, animals showed both periods of activity (running, climbing, etc.) and periods of inactivity. For animals in the A+P cohort, passive exposure sessions took place the same day as active training sessions, usually a few hours after training.</p>
</sec>
<sec id="s4d">
<title>Analysis of behavioral data</title>
<p>To characterize the learning performance of each animal, we calculated the percentage of correct trials for each behavioral session during Stage 3 and fit a straight line (without constraints) to these data. Using these linear fits, we determined the performance at 21 days and the number of days required to reach 70% of trials correct for each animal. To test for bimodality of the distributions of these estimates, we used a Mixture Gaussian Model (implemented in the scikit-learn Python package: <bold><italic>Pedregosa et al</italic></bold>. (<bold><italic>2011</italic></bold>)).</p>
<p>The psychometric performance for each mouse was estimated by fitting a sigmoidal curve to the percentage of trials with leftward choices for each stimulus averaged across all sessions of interest (days 1-4 or days 21-24 of stage 4). The psychometric slope presented in <xref rid="fig3" ref-type="fig">Fig. 3</xref> was determined from the maximum slope of this sigmoidal fit. The average psychometric performance for each cohort was calculated by first estimating the average performance for each stimulus for each animal, and then averaging across animals. To test differences between cohorts we used the non-parametric Wilcoxon rank-sum test.</p>
</sec>
<sec id="s4e">
<title>Modeling</title>
<p>The neural network models described in this article were implemented in JAX (<bold><italic><xref ref-type="bibr" rid="c6">Bradbury et al., 2018</xref></italic></bold>), their source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/cschmidat/behaviour-models">https://github.com/cschmidat/behaviour-models</ext-link>. The networks were trained on inputs drawn from normal distributions <inline-formula><inline-graphic xlink:href="535463v2_inline27.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, parameterized by the mean <inline-formula><inline-graphic xlink:href="535463v2_inline28.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We assume these means all lie on a line segment, ending at the values <inline-formula><inline-graphic xlink:href="535463v2_inline29.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We trained the models in three different settings: With <italic>isotropic</italic> input, <italic>non-isotropic</italic> input, and a <italic>non-aligned</italic> input, in which the decoding direction is only partially aligned with the highest principal components. For the isotropic input, we chose an input dimension <italic>d</italic> = 50, and means along <inline-formula><inline-graphic xlink:href="535463v2_inline30.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="535463v2_inline31.gif" mimetype="image" mime-subtype="gif"/></inline-formula> denotes the unit vector along the <italic>i</italic>th direction. Because the model is linear, this choice entails no loss of generality. The covariance matrix was chosen to be Σ = <bold>I</bold>. For the non-isotropic input, we set <italic>d</italic> = 50 and <inline-formula><inline-graphic xlink:href="535463v2_inline32.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and the covariance to <inline-formula><inline-graphic xlink:href="535463v2_inline33.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with <italic>σ</italic><sub>1</sub> = 1 and <inline-formula><inline-graphic xlink:href="535463v2_inline34.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. For the non-aligned input, we set <italic>d</italic> = 100, <inline-formula><inline-graphic xlink:href="535463v2_inline35.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="535463v2_inline36.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="535463v2_inline37.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <italic>σ</italic><sub>2</sub> = 1. With this choice of parameters, the decoding direction is only partially aligned with the first 20 principal-component directions of the input distribution. For all three of these input distributions, the optimal performance for a classifier is about 95%.</p>
<p>The models were trained in discrete steps, corresponding to either:</p>
<list list-type="bullet">
<list-item><p>One passive exposure, where the models were supplied with a sample drawn from <inline-formula><inline-graphic xlink:href="535463v2_inline38.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, with <italic>μ</italic> randomly chosen from six regularly interspaced points on the line segment from <inline-formula><inline-graphic xlink:href="535463v2_inline39.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to <inline-formula><inline-graphic xlink:href="535463v2_inline40.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p></list-item>
<list-item><p>One active trial, where the models were supplied with either (i) a sample drawn from <inline-formula><inline-graphic xlink:href="535463v2_inline41.gif" mimetype="image" mime-subtype="gif"/></inline-formula> with target output <italic>y</italic> = 1 (corresponding to label +1) or (ii) a sample drawn from <inline-formula><inline-graphic xlink:href="535463v2_inline42.gif" mimetype="image" mime-subtype="gif"/></inline-formula> with target output <italic>y</italic> = 0 (corresponding to label −1)</p></list-item>
</list>
<p>Active trials and passive exposures were combined into the following three training schedules:</p>
<list list-type="bullet">
<list-item><p>A only: The model underwent 5000 active trials.</p></list-item>
<list-item><p>A + P: Each of 5000 active trials was followed by 9 passive exposures.</p></list-item>
<list-item><p>P : A: The model was first presented with 45000 passive exposures, then underwent 5000 active trials.</p></list-item>
</list>
<p>With these input representations and learning schedules, we trained 5 models with distinct architectures and learning rules. Model 1 was a one-layer model with supervised and unsupervised learning for the readout weights <inline-formula><inline-graphic xlink:href="535463v2_inline43.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Supervised learning corresponds to stochastic gradient descent on the binary cross-entropy as the loss function, which results in the learning rule
<disp-formula id="ueqn1">
<graphic xlink:href="535463v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>η</italic> is the learning rate, ŷ is the model output and <italic>λ</italic> is the weight decay parameter. Unsupervised learning corresponds to Hebbian learning with learning rule:
<disp-formula id="ueqn2">
<graphic xlink:href="535463v2_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where, as before, <italic>η</italic> is the learning rate and <italic>λ</italic> is a weight-decay constant. In Model 1, we trained <inline-formula><inline-graphic xlink:href="535463v2_inline44.gif" mimetype="image" mime-subtype="gif"/></inline-formula> using unsupervised learning during all passive exposures, and using both unsupervised learning and supervised learning during active trials.</p>
<p>The two-layer architecture introduces additional weights <italic>W</italic> to map the input <inline-formula><inline-graphic xlink:href="535463v2_inline45.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to a latent representation <inline-formula><inline-graphic xlink:href="535463v2_inline46.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which is used by the readout weights <inline-formula><inline-graphic xlink:href="535463v2_inline47.gif" mimetype="image" mime-subtype="gif"/></inline-formula> to produce the output <inline-formula><inline-graphic xlink:href="535463v2_inline48.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. As for <inline-formula><inline-graphic xlink:href="535463v2_inline49.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the input weights <italic>W</italic> can be trained by supervised learning and unsupervised learning. For supervised learning, we again used stochastic gradient descent on the binary cross-entropy, while, for unsupervised learning, Hebbian learning with weight decay was used:
<disp-formula id="ueqn3">
<graphic xlink:href="535463v2_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In Model 2, we trained <inline-formula><inline-graphic xlink:href="535463v2_inline50.gif" mimetype="image" mime-subtype="gif"/></inline-formula> using unsupervised learning during all passive exposures and active trials, and <italic>W</italic> using supervised learning during the active trials. In Model 3, we trained <italic>W</italic> using unsupervised learning during all passive exposures and active trials, and <inline-formula><inline-graphic xlink:href="535463v2_inline51.gif" mimetype="image" mime-subtype="gif"/></inline-formula> using supervised learning during the active trials. In Model 4, we expanded Model 3 by introducing an additional lateral set of weights <italic>M</italic> between the hidden-layer neurons, which was trained during the passive exposures and active trials using anti-Hebbian learning:
<disp-formula id="ueqn4">
<graphic xlink:href="535463v2_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Assuming that the neurons in the hidden layer quickly settle to a steady state, the effect of the lateral weights can be taken into account by using the hidden layer representation at this equilibrium (<bold><italic><xref ref-type="bibr" rid="c31">Pehlevan et al., 2015</xref></italic></bold>):
<disp-formula id="ueqn5">
<graphic xlink:href="535463v2_ueqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In Model 5, we used the same architecture and learning rules as in Model 4, but additionally trained <italic>W</italic> using supervised learning during active trials.</p>
<p>We chose the hyperparameters <italic>η</italic> for all supervised learning rules such that the learning curve for the learners in the A-only schedule approximately matched the learning performance of the mice in the experiments. The learning rate for the unsupervised algorithms determines the separation of the learners with passive exposure and was chosen such that it maximized the separation while maintaining stability of the learning algorithm (the instability occurs when the learning rate for the unsupervised algorithms is set too high). The weight-decay parameters determine the asymptotic size of the weights when the learning algorithms converge. They were chosen such that the asymptotic norm of all weights matched the norm at initialization. The values for all hyperparameters can be found in <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Ciarra Thomas, Anna Easton, Komal Kaur, Gabriel Toea, and Brigid Deck for assistance with data collection.</p>
</ack>
<sec id="s5">
<title>Additional information</title>
<sec id="s5a">
<title>Funding</title>
<p>This research was supported by the National Science Foundation (Grant #2024926), the National Institutes of Health (grants R00NS114194 and R01NS118461), and the Office of the Vice President for Research &amp; Innovation at the University of Oregon (through an I3 award).</p>
</sec>
<sec id="s5b">
<title>Author contribution</title>
<p>SJ, MBB and JM conceived the project. MH collected the behavioral data. CS, JM and SJ analyzed the data. CS, JM and SJ wrote the paper.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Alwis</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Rajan</surname> <given-names>R.</given-names></string-name> <article-title>Environmental enrichment and the sensory brain: the role of enrichment in remediating brain injury</article-title>. <source>Frontiers in Systems Neuroscience</source>. <year>2014</year>; <volume>8</volume>:<fpage>156</fpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Baevski</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hsu</surname> <given-names>WN</given-names></string-name>, <string-name><surname>Conneau</surname> <given-names>A</given-names></string-name>, <string-name><surname>Auli</surname> <given-names>M.</given-names></string-name> <article-title>Unsupervised speech recognition</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2021</year>; <volume>34</volume>:<fpage>27826</fpage>–<lpage>27839</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Baevski</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Mohamed</surname> <given-names>A</given-names></string-name>, <string-name><surname>Auli</surname> <given-names>M.</given-names></string-name> <article-title>wav2vec 2.0: A framework for self-supervised learning of speech representations</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2020</year>; <volume>33</volume>:<fpage>12449</fpage>–<lpage>12460</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Bao</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Woods</surname> <given-names>J</given-names></string-name>, <string-name><surname>Merzenich</surname> <given-names>MM</given-names></string-name>. <article-title>Temporal plasticity in the primary auditory cortex induced by operant perceptual learning</article-title>. <source>Nature Neuroscience</source>. <year>2004</year>; <volume>7</volume>(<issue>9</issue>):<fpage>974</fpage>–<lpage>981</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="book"><string-name><surname>Belkin</surname> <given-names>M</given-names></string-name>, <string-name><surname>Niyogi</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sindhwani</surname> <given-names>V.</given-names></string-name> <chapter-title>On manifold regularization</chapter-title>. <source>In: International Workshop on Artificial Intelligence and Statistics</source> <publisher-name>PMLR</publisher-name>; <year>2005</year>. p. <fpage>17</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="web"><string-name><surname>Bradbury</surname> <given-names>J</given-names></string-name>, <string-name><surname>Frostig</surname> <given-names>R</given-names></string-name>, <string-name><surname>Hawkins</surname> <given-names>P</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Leary</surname> <given-names>C</given-names></string-name>, <string-name><surname>Maclaurin</surname> <given-names>D</given-names></string-name>, <string-name><surname>Necula</surname> <given-names>G</given-names></string-name>, <string-name><surname>Paszke</surname> <given-names>A</given-names></string-name>, <string-name><surname>VanderPlas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wanderman-Milne</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Q</given-names></string-name>, <source>JAX: composable transformations of Python+NumPy programs</source>; <year>2018</year>. <ext-link ext-link-type="uri" xlink:href="http://github.com/google/jax">http://github.com/google/jax</ext-link>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Caras</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Sanes</surname> <given-names>DH</given-names></string-name>. <article-title>Top-down modulation of sensory cortex gates perceptual learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2017</year>; <volume>114</volume>(<issue>37</issue>):<fpage>9972</fpage>–<lpage>9977</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>De Hoz</surname> <given-names>L</given-names></string-name>, <string-name><surname>Nelken</surname> <given-names>I.</given-names></string-name> <article-title>Frequency tuning in the behaving mouse: different bandwidths for discrimination and generalization</article-title>. <source>PloS One</source>. <year>2014</year>; <volume>9</volume>(<issue>3</issue>):<fpage>e91676</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Dean</surname> <given-names>I</given-names></string-name>, <string-name><surname>Harper</surname> <given-names>NS</given-names></string-name>, <string-name><surname>McAlpine</surname> <given-names>D.</given-names></string-name> <article-title>Neural population coding of sound level adapts to stimulus statistics</article-title>. <source>Nature Neuroscience</source>. <year>2005</year>; <volume>8</volume>(<issue>12</issue>):<fpage>1684</fpage>–<lpage>1689</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="other"><string-name><surname>Devlin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toutanova</surname> <given-names>K.</given-names></string-name> <article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>. <source>arXiv</source> preprint arXiv:181004805. <year>2018</year>; .</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Fleming</surname> <given-names>G</given-names></string-name>, <string-name><surname>Wright</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>DA</given-names></string-name>. <article-title>The Value of homework: exposure to odors in the home cage enhances odor-discrimination learning in mice</article-title>. <source>Chemical senses</source>. <year>2019</year>; <volume>44</volume>(<issue>2</issue>):<fpage>135</fpage>–<lpage>143</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Gabay</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Dick</surname> <given-names>FK</given-names></string-name>, <string-name><surname>Zevin</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Holt</surname> <given-names>LL</given-names></string-name>. <article-title>Incidental auditory category learning</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>2015</year>; <volume>41</volume>(<issue>4</issue>):<fpage>1124</fpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Garcia-Lazaro</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ho</surname> <given-names>S</given-names></string-name>, <string-name><surname>Nair</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schnupp</surname> <given-names>J.</given-names></string-name> <article-title>Shifting and scaling adaptation to dynamic stimuli in somatosensory cortex</article-title>. <source>European Journal of Neuroscience</source>. <year>2007</year>; <volume>26</volume>(<issue>8</issue>):<fpage>2359</fpage>–<lpage>2368</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="book"><string-name><surname>Genkin</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sengupta</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Chklovskii</surname> <given-names>D.</given-names></string-name> <chapter-title>A neural network for semi-supervised learning on manifolds</chapter-title>. <source>In: Artificial Neural Networks and Machine Learning–ICANN 2019: Theoretical Neural Computation: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings, Part I 28</source> <publisher-name>Springer</publisher-name>; <year>2019</year>. p. <fpage>375</fpage>–<lpage>386</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Watanabe</surname> <given-names>T.</given-names></string-name> <article-title>Perceptual learning</article-title>. <source>Current Biology</source>. <year>2010</year>; <volume>20</volume>(<issue>2</issue>):<fpage>R46</fpage>–<lpage>R48</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Grill</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Strub</surname> <given-names>F</given-names></string-name>, <string-name><surname>Altché</surname> <given-names>F</given-names></string-name>, <string-name><surname>Tallec</surname> <given-names>C</given-names></string-name>, <string-name><surname>Richemond</surname> <given-names>P</given-names></string-name>, <string-name><surname>Buchatskaya</surname> <given-names>E</given-names></string-name>, <string-name><surname>Doersch</surname> <given-names>C</given-names></string-name>, <string-name><surname>Avila Pires</surname> <given-names>B</given-names></string-name>, <string-name><surname>Guo</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Gheshlaghi Azar</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal> <article-title>Bootstrap your own latent-a new approach to self-supervised learning</article-title>. <source>Advances in neural information processing systems</source>. <year>2020</year>; <volume>33</volume>:<fpage>21271</fpage>–<lpage>21284</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="web"><string-name><surname>Gu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Greydanus</surname> <given-names>S</given-names></string-name>, <string-name><surname>Metz</surname> <given-names>L</given-names></string-name>, <string-name><surname>Maheswaranathan</surname> <given-names>N</given-names></string-name>, <string-name><surname>Sohl-Dickstein</surname> <given-names>J.</given-names></string-name> <article-title>Meta-Learning Biologically Plausible Semi-Supervised Update Rules</article-title>. <source>bioRxiv</source>. <year>2019</year>; <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2019/12/30/2019.12.30.891184">https://www.biorxiv.org/content/early/2019/12/30/2019.12.30.891184</ext-link>, doi: <pub-id pub-id-type="doi">10.1101/2019.12.30.891184</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Gutnisky</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Dragoi</surname> <given-names>V.</given-names></string-name> <article-title>Adaptive coding of visual information in neural populations</article-title>. <source>Nature</source>. <year>2008</year>; <volume>452</volume>(<issue>7184</issue>):<fpage>220</fpage>–<lpage>224</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Hensch</surname> <given-names>TK</given-names></string-name>. <article-title>Critical period regulation</article-title>. <source>Annu Rev Neurosci</source>. <year>2004</year>; <volume>27</volume>:<fpage>549</fpage>–<lpage>579</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Holland</surname> <given-names>PC</given-names></string-name>. <article-title>Stimulus preexposure speeds or slows subsequent acquisition of associative learning depending on learning test procedures and response measure</article-title>. <source>Learning &amp; Behavior</source>. <year>2018</year>; <volume>46</volume>(<issue>2</issue>):<fpage>134</fpage>–<lpage>156</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Iyer</surname> <given-names>R</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mihalas</surname> <given-names>S.</given-names></string-name> <article-title>Contextual integration in cortical and convolutional neural networks</article-title>. <source>Frontiers in computational neuroscience</source>. <year>2020</year>; <volume>14</volume>:<fpage>31</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Kral</surname> <given-names>A.</given-names></string-name> <article-title>Auditory critical periods: a review from system’s perspective</article-title>. <source>Neuroscience</source>. <year>2013</year>; <volume>247</volume>:<fpage>117</fpage>–<lpage>133</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Kuhl</surname> <given-names>PK</given-names></string-name>, <string-name><surname>Tsao</surname> <given-names>FM</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>HM</given-names></string-name>. <article-title>Foreign-language experience in infancy: Effects of short-term exposure and social interaction on phonetic learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2003</year>; <volume>100</volume>(<issue>15</issue>):<fpage>9096</fpage>–<lpage>9101</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Mandairon</surname> <given-names>N</given-names></string-name>, <string-name><surname>Stack</surname> <given-names>C</given-names></string-name>, <string-name><surname>Linster</surname> <given-names>C.</given-names></string-name> <article-title>Olfactory enrichment improves the recognition of individual components in mixtures</article-title>. <source>Physiology &amp; Behavior</source>. <year>2006</year>; <volume>89</volume>(<issue>3</issue>):<fpage>379</fpage>–<lpage>384</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Maye</surname> <given-names>J</given-names></string-name>, <string-name><surname>Werker</surname> <given-names>JF</given-names></string-name>, <string-name><surname>Gerken</surname> <given-names>L.</given-names></string-name> <article-title>Infant sensitivity to distributional information can affect phonetic discrimination</article-title>. <source>Cognition</source>. <year>2002</year>; <volume>82</volume>(<issue>3</issue>):<fpage>B101</fpage>–<lpage>B111</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="book"><string-name><surname>McLaren</surname> <given-names>IPL</given-names></string-name>, <string-name><surname>Kaye</surname> <given-names>H</given-names></string-name>, <string-name><surname>Mackintosh</surname> <given-names>NJ</given-names></string-name>. <chapter-title>In: An associative theory of the representation of stimuli: Applications to perceptual learning and latent inhibition</chapter-title>. <source>Parallel distributed processing: Implications for psychology and neurobiology</source>., <publisher-loc>New York, NY, US</publisher-loc>: <publisher-name>Clarendon Press/Oxford University Press</publisher-name>; <year>1989</year>. p. <fpage>102</fpage>–<lpage>130</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Nassar</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Scott</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bhandari</surname> <given-names>A.</given-names></string-name> <article-title>Noise correlations for faster and more robust learning</article-title>. <source>Journal of Neuroscience</source>. <year>2021</year>; <volume>41</volume>(<issue>31</issue>):<fpage>6740</fpage>–<lpage>6752</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Oja</surname> <given-names>E.</given-names></string-name> <article-title>A Simplified Neuron Model as a Principal Component Analyzer</article-title>. <source>Journal of Mathematical Biology</source>. <year>1982</year>; <volume>15</volume>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="other"><string-name><given-names>Oord</given-names> <surname>Avd</surname></string-name>, <string-name><surname>Li</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Vinyals</surname> <given-names>O.</given-names></string-name> <article-title>Representation learning with contrastive predictive coding</article-title>. <source>arXiv</source> preprint arXiv:180703748. <year>2018</year>; .</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Pedregosa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Varoquaux</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Michel</surname> <given-names>V</given-names></string-name>, <string-name><surname>Thirion</surname> <given-names>B</given-names></string-name>, <string-name><surname>Grisel</surname> <given-names>O</given-names></string-name>, <string-name><surname>Blondel</surname> <given-names>M</given-names></string-name>, <string-name><surname>Prettenhofer</surname> <given-names>P</given-names></string-name>, <string-name><surname>Weiss</surname> <given-names>R</given-names></string-name>, <string-name><surname>Dubourg</surname> <given-names>V</given-names></string-name>, <string-name><surname>Vanderplas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Passos</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cournapeau</surname> <given-names>D</given-names></string-name>, <string-name><surname>Brucher</surname> <given-names>M</given-names></string-name>, <string-name><surname>Perrot</surname> <given-names>M</given-names></string-name>, <string-name><surname>Duchesnay</surname> <given-names>E.</given-names></string-name> <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>Journal of Machine Learning Research</source>. <year>2011</year>; <volume>12</volume>:<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Pehlevan</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>T</given-names></string-name>, <string-name><surname>Chklovskii</surname> <given-names>DB</given-names></string-name>. <article-title>A Hebbian/Anti-Hebbian Neural Network for Linear Subspace Learning: A Derivation from Multidimensional Scaling of Streaming Data</article-title>. <source>Neural Computation</source>. <year>2015</year>; <volume>27</volume>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Polley</surname> <given-names>DB</given-names></string-name>, <string-name><surname>Kvašñák</surname> <given-names>E</given-names></string-name>, <string-name><surname>Frostig</surname> <given-names>RD</given-names></string-name>. <article-title>Naturalistic experience transforms sensory maps in the adult cortex of caged animals</article-title>. <source>Nature</source>. <year>2004</year>; <volume>429</volume>(<issue>6987</issue>):<fpage>67</fpage>–<lpage>71</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Polley</surname> <given-names>DB</given-names></string-name>, <string-name><surname>Steinberg</surname> <given-names>EE</given-names></string-name>, <string-name><surname>Merzenich</surname> <given-names>MM</given-names></string-name>. <article-title>Perceptual learning directs auditory cortical map reorganization through top-down influences</article-title>. <source>Journal of Neuroscience</source>. <year>2006</year>; <volume>26</volume>(<issue>18</issue>):<fpage>4970</fpage>–<lpage>4982</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Sharpee</surname> <given-names>TO</given-names></string-name>, <string-name><surname>Sugihara</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kurgansky</surname> <given-names>AV</given-names></string-name>, <string-name><surname>Rebrik</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Stryker</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>KD</given-names></string-name>. <article-title>Adaptive filtering enhances information transmission in visual cortex</article-title>. <source>Nature</source>. <year>2006</year>; <volume>439</volume>(<issue>7079</issue>):<fpage>936</fpage>–<lpage>942</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Triguero</surname> <given-names>I</given-names></string-name>, <string-name><surname>García</surname> <given-names>S</given-names></string-name>, <string-name><surname>Herrera</surname> <given-names>F.</given-names></string-name> <article-title>Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study</article-title>. <source>Knowledge and Information systems</source>. <year>2015</year>; <volume>42</volume>:<fpage>245</fpage>–<lpage>284</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Van Engelen</surname> <given-names>JE</given-names></string-name>, <string-name><surname>Hoos</surname> <given-names>HH</given-names></string-name>. <article-title>A survey on semi-supervised learning</article-title>. <source>Machine learning</source>. <year>2020</year>; <volume>109</volume>(<issue>2</issue>):<fpage>373</fpage>–<lpage>440</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Wright</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Baese-Berk</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Marrone</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bradlow</surname> <given-names>AR</given-names></string-name>. <article-title>Enhancing speech learning by combining task practice with periods of stimulus exposure without practice</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2015</year>; <volume>138</volume>(<issue>2</issue>):<fpage>928</fpage>–<lpage>937</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88406.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ostojic</surname>
<given-names>Srdjan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>École Normale Supérieure - PSL</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study reports <bold>valuable</bold> behavioral and computational observations regarding how passive exposure to auditory stimuli can facilitate auditory categorization. The combination of behavioral results in mice with a study of artificial neural network models provides <bold>solid</bold> evidence for the authors' conclusions. This paper will likely be of broad interest to the general neuroscience community.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88406.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Schmid et al. investigate the question of how sensory learning in animals and artificial networks is driven both by passive exposure to the environment (unsupervised) and from reinforcing feedback (supervised) and how these two systems interact. They first demonstrate in mice that passive exposure to the same auditory stimuli used in a discrimination task modify learning and performance in the task. Based on this data, they then tested how the interaction of supervised and unsupervised learning in an artificial network could account for the behavioural results.</p>
<p>The clear behavioural impact of the passive exposure to sounds on accelerating learning is a major strength of the paper. Moreover, the observation that passive exposure had a positive impact on learning whether it was prior to the task or interleaved with learning sessions provides interesting constraints for modelling the interaction between supervised and unsupervised learning. A practical fallout for labs performing long training procedures is that the periods of active learning that require water-restriction could be reduced by using passive sessions. This could increase both experimental efficiency and animal well-being.</p>
<p>The modelling section clearly exhibits the differences between models and the step-by-step presentation building to the final model provides the reader with a lot of intuition about how supervised and unsupervised learning interact. In particular the authors highlight situations in which the task-relevant discrimination does not align with the directions of highest variance, thus reinforcing the relevance of their conclusions for the complex structure of sensory stimuli. A great strength of these models is that they generate clear predictions about how neural activity should evolve during the different training regimes that would be exciting to test.</p>
<p>As the authors acknowledge, the experimental design presented cannot clearly show that the effect of passive exposure was due to the specific exposure to task-relevant stimuli since there is no control group exposed to irrelevant stimuli. Studies have shown that exposure to a richer sensory environment, even in the adult, swiftly (ie within days) enhances responses even in the adult and even when the stimuli are different from those present in the task (1-3). Clearly distinguishing between these two options would require further experiments and could be a possible direction for future research.</p>
<p>1. Mandairon, N., Stack, C. &amp; Linster, C. Olfactory enrichment improves the recognition of individual components in mixtures. Physiol. Behav. 89, 379-384 (2006).</p>
<p>
2. Alwis, D. S. &amp; Rajan, R. Environmental enrichment and the sensory brain: The role of enrichment in remediating brain injury. Front. Syst. Neurosci. 8, 1-20 (2014).</p>
<p>
3. Polley, D. B., Kvašňák, E. &amp; Frostig, R. D. Naturalistic experience transforms sensory maps in the adult cortex of caged animals. Nature 429, 67-71 (2004).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88406.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Schmid et al present a lovely study looking at the effect of passive auditory exposure on learning a categorization task.</p>
<p>
The authors utilize a two-alternative choice task where mice have to discriminate between upward and downward moving frequency sweeps. Once mice learn to discriminate easy stimuli, the task is made psychometric and additional intermediate stimuli are introduced (as is standard in the literature). The authors introduce an additional two groups of animals, one that was passively exposed to the task stimuli before any behavioral shaping, and one that had passive exposure interleaved with learning. The major behavioral finding is that passive exposure to sounds improves learning speed. The authors show this in a number of ways through linear fits to the learning curves. Additionally, by breaking down performance based on the &quot;extreme&quot; vs &quot;psychometric&quot; stimuli, the authors show that passive exposure can influence responses to sounds that were not present during the initial training period. One limitation here is that the presented analysis is somewhat simplistic, does not include any detailed psychometric analysis (bias, lapse rates etc), and primarily focuses on learning speed. Ultimately though, the behavioral results are interesting and seem supported by the data.</p>
<p>To investigate the neural mechanisms that may underlie their behavioral findings, the authors turn to a family of artificial neural network models and evaluate the consequences of different learning algorithms and schedules, network architectures, and stimulus distributions, on the learning outcomes. The authors work through five different architectures that fail to recapitulate the primary behavior findings before settling on a final model, utilizing a combination of supervised and unsupervised learning, that was capable of reproducing the key aspects of the experiments. Ultimately, the behavioral results presented are consistent with network models that build latent representations of task-relevant features that are determined by statistical properties of the input distribution.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88406.2.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary of Author's Results/Intended Achievements</p>
<p>
The authors were trying to ascertain the underlying learning mechanisms and network structure that could explain their primary experimental finding: passive exposure to a stimulus (independent of when the exposure occurs) can lead to improvements in active (supervised) learning. They modeled their task with 5 progressively more complex shallow neural networks classifying vectors drawn from multi-variate Gaussian distributions.</p>
<p>Account of Major Strengths:</p>
<p>
Overall, the experimental findings were interesting. The modelling was also appropriate, with a solid attempt at matching the experimental condition to simplified network models.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88406.2.sa4</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Schmid</surname>
<given-names>Christian</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3979-9169</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Haziq</surname>
<given-names>Muhammad</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Baese-Berk</surname>
<given-names>Melissa M.</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4855-9319</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Murray</surname>
<given-names>James M.</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jaramillo</surname>
<given-names>Santiago</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6595-8450</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>The experimental design presented cannot clearly show that the effect of passive exposure was due to the specific exposure to task-relevant stimuli since there is no control group exposed to irrelevant stimuli.</p>
</disp-quote>
<p>We acknowledge the possibility that exposure to task-irrelevant stimuli could result in improvements in learning. Testing this possibility would be a worthwhile goal of future experiments, but it is outside the scope of our current study. We have been careful in our paper to only draw conclusions about the effects of exposure to task-relevant stimuli compared to no exposure. We have added a discussion of this point and relevant references to the literature in the Discussion section of our manuscript.</p>
<disp-quote content-type="editor-comment">
<p>The conclusion that &quot;passive exposure influences responses to sounds not used during training&quot; (line 147) does not seem fully supported by the authors' analysis. The authors show that there is an increase in accuracy for intermediate sweep speeds despite the fact that this is the first time the animals encounter them in the active session. However, it seems impossible to exclude that this effect is not simply due to the increased accuracy of the extreme sounds that the animals had been trained on.</p>
</disp-quote>
<p>We have modified this sentence to emphasize that it refers to “intermediate” sounds. Regarding the reviewer’s concern, the conclusion is drawn from Figure 3, in which we show that mice exhibit an improvement on non-extreme stimuli after training on extreme stimuli. Panel 3D illustrates that the observed improvements are not just changes in psychometric performance driven by the extreme sounds. In the context of this result, the conclusion relates to generalization in performance on task-relevant stimuli that are closely related to the training stimuli. In our view, it was not entirely obvious a priori that this result would have to occur, since it is possible that performance could improve at the extremes without improving at the intermediate stimuli.</p>
<disp-quote content-type="editor-comment">
<p>In the modelling section, the authors adjusted the hyper-parameters to maximize the difference between pure active and passive/active learning. This makes a comparison of learning rates between models somewhat confusing.</p>
</disp-quote>
<p>We apologize for the confusion. None of our conclusions are based on comparisons of learning speed between models, but perhaps this was not pointed out sufficiently clearly. The relevant comparisons between conditions for each specific model are made using the same hyperparameters. We have clarified this point in the modeling section of our manuscript.</p>
<disp-quote content-type="editor-comment">
<p>The description of the sound does not state whether when reducing the slope of the sweeps the center or the onset frequency of the sounds is preserved.</p>
</disp-quote>
<p>Frequency modulated sounds of different FM slopes were generated such that the center frequency was always the same. This is now clarified in the updated version of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations for the authors):</bold></p>
<p>As mentioned, the specificity of the stimuli presented during the passive period is not explicitly addressed in either modelling or behaviour. For modelling, this could be quite straightforward to assess by manipulating the input stimuli during passive episodes. For the behaviour, this would require repeating the experiment with passive sessions during which unrelated sounds are presented (for example varying in frequency or intensity instead of frequency slope). I mainly include this suggestion to clarify my previous comment because this would require a huge amount of work.</p>
</disp-quote>
<p>We agree that varying the extent to which the presented passive stimuli are task-related to the task is an interesting point to study for future experiments. However, doing so for the experiments is outside the scope of the current study, and we believe exploring this only in the modeling part would add little value to the current study, because the outcome will highly depend on the details of the implementation.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>One limitation here is that the presented analysis is somewhat simplistic, does not include any detailed psychometric analysis (bias, lapse rates etc), and primarily focuses on learning speed.</p>
</disp-quote>
<p>In our preliminary analyses of trials that included extreme and intermediate stimuli after animals had learned the task (Figure 3), we investigated some metrics of the type that the reviewer suggests here. However, since such additional psychometric analyses were somewhat tangential to our main results (which are about learning speed and responses to sounds not included during training), we did not include these in our manuscript. In agreement with the reviewer’s concern, a main limitation of our study is that the available data does not allow for an analysis of psychometrics during the initial learning stages, since only the extreme stimuli were presented during the task.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>The International Brain Lab has shown quite nicely that psychometric curves continue to improve (increased slope, decreased bias) across learning. This was not really discussed or presented in your data - is this observed during the S4 training portion?</p>
</disp-quote>
<p>We indeed saw improvements in the psychometric performance during stage S4, in particular for the active-only learners, as can be seen in Figure 3. We quantified these changes (now presented in the Results section), and added a discussion to the main text.</p>
<disp-quote content-type="editor-comment">
<p>Why use a linear fit to extract the various quantities of interest? All of these quantities could be extracted from the raw behavioral data itself.</p>
</disp-quote>
<p>Because of the large variations in performance from day-to-day, a linear fit allowed us to extract a more reliable estimate of quantities like “Time to achieve 70%” and “Performance at 21 days” for each animal.</p>
<disp-quote content-type="editor-comment">
<p>The analysis presented was focussed primarily on the fast learners. What about the slow learners? Are the ANN models able to recapitulate different aspects of their behavior?</p>
</disp-quote>
<p>We agree with the reviewer that the observation that the learners clustered into two groups calls for further investigation. In this study, we focused on the mice that learned more efficiently, because those allowed us to address our main research question about the influence of passive exposure. We believe, the slow learners could be modeled with ANNs that start with a less-easily discriminable input representation, which limits the performance that the trained network is ultimately able to achieve. This additional analysis is outside the scope of the current manuscript, but we hope to address these questions in the future.</p>
<disp-quote content-type="editor-comment">
<p>Although I appreciate the thoroughness of the modeling, I was not entirely convinced by the narrative underlying models 1-5, since none of these models were able to successfully recapitulate your core findings. Would it not make more sense to focus primarily on the final model?</p>
</disp-quote>
<p>By starting with the simplest possible model that incorporates supervised and unsupervised learning, we were able to determine which ingredients were necessary to capture the behavioral data. We believe this could not have been clearly established by considering the final model alone.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>The first [major weakness] is that even Model 5 differs from their data. For example, the A+P (passive interleaved condition) learning curve in Figure 7 seems to be non-monotonic, and has some sort of complex eigenvalue in its decay to the steady state performance as trials increase. This wasn't present in their experimental data (Figure 2D), and implies a subtle but important difference. There also appear to be differences in how quickly the initial learning (during early trials) occurs for the A+P and A:P conditions. While both A+P and A:P conditions learn faster than A only in M5, A+P and A:P seem to learn in different ways, which isn't supported in their data.</p>
</disp-quote>
<p>The reviewer is correct that there are subtle differences between the two learning curves produced by Model 5. Due to expected variability in the experimental data, however, it is difficult to conclude whether such subtle distinctions also appear in the learning curves of the mice. Further, the slight overshoot of the learning curve that the reviewer mentions is not constrained by the experimental data due to different mice reaching asymptotic performance at different times, and many of them not having even reached asymptotic performance by the end of the training period.</p>
<p>However, even if there are minor discrepancies between the learning curves produced by the final version of the model and by the mice, we do not see this as being especially surprising or problematic. As in any model, there are a large number of potentially important features that are not included in any of our models–for example, realistic spectrotemporal neural responses, nonlinearity in neural activations, heterogeneity across mice, and many others. The aim of our modeling was to choose a space of possible models (which is inevitably restricted) and show which model version within that space best captures our experimental observations. Expanding the space of possible models that we considered to capture further nuances in the data will be a task for future work.</p>
<disp-quote content-type="editor-comment">
<p>The second major weakness is that the authors also don't generate any predictions with M5.
Can they test this model of learning somehow in follow-up behavioural experiments in mice? ... Without follow-up experiments to test their mechanism of why passive exposure helps in a schedule-independent way, the impact of this paper will be limited.</p>
</disp-quote>
<p>Although testing predictions from our models was beyond the scope of the current study, we do generate specific predictions with model M5 (in particular, about neural representations). Our model produces predictions about neural representations and the ways in which they evolve through learning, and we hope to test these predictions in future work.</p>
<disp-quote content-type="editor-comment">
<p>I believe the authors need to place this work in the context of a large amount of existing literature on passive (unsupervised) and active (supervised) learning interactions. This field is broad both experimentally and computationally. For example, there is an entire sub-field of machine learning, called semi-supervised learning that is not mentioned at all in this work.</p>
</disp-quote>
<p>We thank the reviewer for pointing this out. The Discussion section of the updated manuscript now includes a discussion on how our results fit in with this literature.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations for the authors):</bold></p>
</disp-quote>
<p>All points made by the reviewer in their Recommendations For The Authors are associated with those presented in the Public Review and they are addressed in our response above.</p>
</body>
</sub-article>
</article>