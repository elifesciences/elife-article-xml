<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">89996</article-id>
<article-id pub-id-type="doi">10.7554/eLife.89996</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89996.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Asymmetric distribution of color-opponent response types across mouse visual cortex supports superior color vision in the sky</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8649-4835</contrib-id>
<name>
<surname>Franke</surname>
<given-names>Katrin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cai</surname>
<given-names>Chenchen</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7776-1914</contrib-id>
<name>
<surname>Ponder</surname>
<given-names>Kayla</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3342-557X</contrib-id>
<name>
<surname>Fu</surname>
<given-names>Jiakun</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sokoloski</surname>
<given-names>Sacha</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0199-4727</contrib-id>
<name>
<surname>Berens</surname>
<given-names>Philipp</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4305-6376</contrib-id>
<name>
<surname>Tolias</surname>
<given-names>Andreas S.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Neuroscience, Baylor College of Medicine</institution>, Houston, TX, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine</institution>, Houston, TX, <country>USA</country></aff>
<aff id="a3"><label>3</label><institution>Institute for Ophthalmic Research, University of Tübingen</institution>, Tübingen, <country>Germany</country></aff>
<aff id="a4"><label>4</label><institution>Graduate Training Center of Neuroscience, International Max Planck Research School, University of Tübingen</institution>, Tübingen, <country>Germany</country></aff>
<aff id="a5"><label>5</label><institution>Hertie Institute for AI in Brain Health, University of Tübingen</institution>, Tübingen, <country>Germany</country></aff>
<aff id="a6"><label>6</label><institution>Department of Electrical and Computer Engineering, Rice University</institution>, Houston, TX, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Rieke</surname>
<given-names>Fred</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Washington</institution>
</institution-wrap>
<city>Seattle</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>✉</label>Correspondence: <email>katrin.franke@bcm.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-09-12">
<day>12</day>
<month>09</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-04-26">
<day>26</day>
<month>04</month>
<year>2024</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP89996</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-06-26">
<day>26</day>
<month>06</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-06-05">
<day>05</day>
<month>06</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.06.01.543054"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-09-12">
<day>12</day>
<month>09</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89996.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.89996.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.89996.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.89996.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.89996.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Franke et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Franke et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-89996-v2.pdf"/>
<abstract>
<p>Color is an important visual feature that informs behavior, and the retinal basis for color vision has been studied across various vertebrate species. While we know how color information is processed in visual brain areas of primates, we have limited understanding of how it is organized beyond the retina in other species, including most dichromatic mammals. In this study, we systematically characterized how color is represented in the primary visual cortex (V1) of mice. Using large-scale neuronal recordings and a luminance and color noise stimulus, we found that more than a third of neurons in mouse V1 are color-opponent in their receptive field center, while the receptive field surround predominantly captures luminance contrast. Furthermore, we found that color-opponency is especially pronounced in posterior V1 that encodes the sky, matching the statistics of natural scenes experienced by mice. Using unsupervised clustering, we demonstrate that the asymmetry in color representations across cortex can be explained by an uneven distribution of green-On/UV-Off color-opponent response types that are represented in the upper visual field. This type of color-opponency in the receptive field center of V1 neurons was not present in the receptive field center of retinal ganglion cells and, therefore, is likely computed by integrating center and surround information down-stream of the retina. Finally, a simple model with natural scene-inspired parametric stimuli shows that green-On/UV-Off color-opponent response types may enhance the detection of “predatory”-like dark UV-objects in noisy day-light scenes. The results from this study highlight the relevance of color processing in the mouse visual system and contribute to our understanding of how color information is organized in the visual hierarchy across species. More broadly, they support the hypothesis that the visual cortex combines upstream information towards computing neuronal selectivity to behaviorally-relevant sensory features.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Revised manuscript based on reviewer comments of eLife reviewed preprint</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Color is an important property of the visual world informing behavior. The retinal basis for color vision has been studied in many vertebrate species, including zebrafish, mice and primates (reviewed in <xref ref-type="bibr" rid="c1">Baden and Osorio, 2019</xref>): Signals from different photoreceptor types which are sensitive to different wavelengths are compared by retinal circuits, thereby creating color-opponent cell types. In primates, it is well established how color-opponent signals from the retina are processed in downstream brain areas (Living-stone and Hubel, 1984; <xref ref-type="bibr" rid="c3">Wiesel and Hubel, 1966</xref>; <xref ref-type="bibr" rid="c4">Gegen-furtner et al., 1996</xref>; <xref ref-type="bibr" rid="c5">Tanigawa et al., 2010</xref>; <xref ref-type="bibr" rid="c6">Chatterjee and Callaway, 2003</xref>). In most other species, however, we know relatively little about how color information is processed beyond the retina. Thus, our understanding of color processing along the visual hierarchy across species remains limited, highlighting the need for further research to un-cover general rules governing this fundamental aspect of vision.</p>
<p>Here, we systematically studied how color is represented in the primary visual cortex (V1) of mice. Like most mammals, mice are dichromatic and have two cone photoreceptor types, expressing ultraviolet (UV)- and green-sensitive S- and M-opsin (<xref ref-type="bibr" rid="c7">Szél et al., 1992</xref>), respectively. In addition, they have one type of rod photoreceptor which is green-sensitive. Importantly, UV- and green-sensitive cone photoreceptors predominantly sample the upper and lower visual field, respectively, through an uneven opsin distribution across the retina (<xref ref-type="bibr" rid="c7">Szél et al., 1992</xref>; <xref ref-type="bibr" rid="c8">Baden et al., 2013</xref>). Behavioral studies have demonstrated that mice can discriminate different colors (<xref ref-type="bibr" rid="c9">Jacobs et al., 2004</xref>), at least in the upper visual field (<xref ref-type="bibr" rid="c10">Denman et al., 2018</xref>). However, a thorough understanding of the neuronal correlates underlying this behavior is still missing.</p>
<p>At the level of the mouse retina, a large body of literature has identified mechanisms underlying color-opponent responses, including cone-type selective (<xref ref-type="bibr" rid="c11">Stabio et al., 2018</xref>; <xref ref-type="bibr" rid="c12">Nadal-Nicolás et al., 2020</xref>; <xref ref-type="bibr" rid="c13">Haverkamp et al., 2005</xref>) or cone-type unselective wiring (<xref ref-type="bibr" rid="c14">Chang et al., 2013</xref>) and rod-cone opponency (<xref ref-type="bibr" rid="c15">Joesch and Meister, 2016</xref>; <xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Khani and Gollisch, 2021</xref>). The latter is wide-spread across many neuron types located in the ventral retina sampling the sky, where rod and cone photoreceptors exhibit the strongest difference in spectral sensitivity, and requires the integration across center and surround components of receptive fields (RFs). In visual areas downstream to the retina, the frequency of color-opponency has remained controversial. Some studies have reported very low numbers of color-opponent neurons in mouse dLGN (<xref ref-type="bibr" rid="c18">Denman et al., 2017</xref>) and V1 (<xref ref-type="bibr" rid="c19">Tan et al., 2015</xref>), while two more recent studies identified pronounced cone- and rod-cone-dependent color-opponency (<xref ref-type="bibr" rid="c20">Mouland et al., 2021</xref>; <xref ref-type="bibr" rid="c21">Rhim and Nauhaus, 2023</xref>). This discrepancy might be due to differences in stimulus design or light levels.</p>
<p>In this study, we systematically characterized color and luminance center-surround RF properties of mouse V1 neurons across different light levels using large-scale neuronal recordings and a luminance and color noise stimulus. This revealed that more than a third of neurons in mouse V1 are highly sensitive to color features of the visual input in their RF center, while the RF surround predominantly captures luminance contrast. Color-opponency in the RF center was strongest for photopic light levels largely activating cone photoreceptors and greatly decreased for mesopic light levels, suggesting that the observed color-opponency in V1 is at least partially mediated by the comparison of cone photoreceptor signals. We further showed that color-opponency is especially pronounced in posterior V1 which encodes the sky, in line with previous work in the retina (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>), and matching the statistics of mouse natural scenes (<xref ref-type="bibr" rid="c22">Qiu et al., 2021</xref>; <xref ref-type="bibr" rid="c23">Abballe and Asari, 2022</xref>). Using unsupervised clustering we demon-strated that the asymmetry in color representations across cortex can be explained by an uneven distribution of green-On/UV-Off color-opponent response types that almost exclusively represented the upper visual field. We showed that this type of color-opponency in the RF center was not already present in the RF center of retinal output neurons and, therefore, may be computed in the cortex by upstream visual signals. Finally, by implementing a simple model with natural scene inspired parametric stimuli, we showed that green-On/UV-Off color-opponent response types may enhance the detection of “predatory”- like dark UV-objects in noisy daylight scenes.</p>
<p>The results of our study support the hypothesis that the visual cortex combines upstream information towards computing neuronal selectivity specialized to allow specific visual tasks, such as the robust detection of an aerial predator in noisy natural scene.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Characterizing color and luminance center-surround receptive fields of mouse V1 neurons</title>
<p>To study the neuronal representation of color in mouse primary visual cortex (V1), we characterized center (i.e. classical) and surround (i.e. extra-classical) receptive fields (RFs) of excitatory V1 neurons in awake, head-fixed mice in response to a luminance and color noise stimulus (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). The noise stimulus consisted of a center spot (37.5 degrees visual angle in diameter) and a surround annulus (approx. 120 × 90 degrees visual angle without the center spot) that simultaneously flickered in UV and green based on 5 Hz binary random sequences (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>), thereby capturing chromatic, temporal as well as one spatial dimension of the neurons’ RFs. Neuronal responses to such relatively simple, parametric stimuli are easy to interpret and allow to systematically quantify chromatic RF properties of visual neurons in the mouse (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>) and zebrafish retina (<xref ref-type="bibr" rid="c24">Zimmermann et al., 2018</xref>), as well as in primate V1 (<xref ref-type="bibr" rid="c6">Chatterjee and Callaway, 2003</xref>). We presented visual stimuli to awake, head-fixed mice positioned on a treadmill while at the same time recording the population calcium activity within L2/3 of V1 using two-photon imaging (700 × 700 μm recordings at 15 Hz). Visual stimulation was performed in the photopic light regime that predominantly activates cone photoreceptors. We back-projected visual stimuli on a Teflon screen using a custom projector with UV and green LEDs that allow differential activation of mouse cone photoreceptors (<xref ref-type="bibr" rid="c25">Franke et al., 2019</xref>, <xref ref-type="bibr" rid="c26">2022</xref>). Functional recordings were obtained from posterior and anterior V1 (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>), encoding the upper and lower visual field (<xref ref-type="bibr" rid="c27">Schuett et al., 2002</xref>), respectively.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>Color noise stimulus identifies center-surround receptive field properties of mouse V1 neurons.</title>
<p><bold>a</bold>, Schematic illustrating experimental setup: Awake, head-fixed mice on a treadmill were presented with a center-surround color noise stimulus while recording the population calcium activity in L2/3 neurons of V1 using two-photon imaging. Stimuli were back-projected on a Teflon screen by a DLP-based projector equipped with a UV (390 nm) and green (460 nm) LED, allowing to differentially activate mouse cone photoreceptors. <bold>b</bold>, Schematic drawing illustrating stimulus paradigm: UV and green center spot (UV<sub>C</sub>/Green<sub>C</sub>) and surround annulus (UV<sub>S</sub>/Green<sub>S</sub>) flickered independently at 5 Hz according to binary random sequences. Top images depict example stimulus frames. See also <xref rid="figS1" ref-type="fig">Suppl. Fig. 1</xref>. <bold>c</bold>, Left side shows a schematic of V1 with a posterior and anterior recording field, and the recorded neurons of the posterior field overlaid on top of the mean projection of the recording. Right side shows the activity of n=150 neurons of this recording in response to the stimulus sequence shown in (b). <bold>d</bold>, Event-triggered-averages (ETAs) of six example neurons, shown for the four stimulus conditions. Gray: Original ETA. Black: Reconstruction using principal component analysis (PCA). See also <xref rid="figS2" ref-type="fig">Suppl. Fig. 2</xref>. Cells are grouped based on their ETA properties and include luminance sensitive, color selective and color-opponent neurons. Black dotted lines indicate time of response.</p></caption>
<graphic xlink:href="543054v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To record from many V1 neurons simultaneously, we used a center stimulus size of 37.5 degree visual angle in diameter, which is slightly larger than the center RFs we estimated for single V1 neurons (26.2 <italic>±</italic> 4.6 degrees visual angle in diameter) using a sparse noise paradigm (<xref ref-type="bibr" rid="c28">Jones and Palmer, 1987</xref>). The disadvantage of this approach is that the stimulus is only roughly centered on the neurons’ center RFs. To reduce the impact of potential stimulus misalignment on our results, we used the following steps and controls. First, for each recording, we positioned the monitor such that the population RF across all neurons, estimated using a short sparse noise stimulus, lies within the center of the stimulus field of view. Second, we confirmed that this procedure results in good stimulus alignment at the level of individual neurons by using a longer sparse noise stimulus for a subset of experiments (<xref rid="figS1" ref-type="fig">Suppl. Fig. 1a,b</xref>). Specifically, we found that for the majority of tested neurons (83%), more than two thirds of their center RF overlapped with the center spot of the color noise stimulus (<xref rid="figS1" ref-type="fig">Suppl. Fig. 1b</xref>). Finally, we excluded neurons from analysis, which did not show significant center responses (n=1,937 neurons excluded from n=5,248; <xref rid="figS2" ref-type="fig">Suppl. Fig. 2d,e</xref>), which may be caused by misalignment of the stimulus. Together, this suggests that the center spot and the surround annulus of the noise stimulus predominantly drive center (i.e. classical RF) and surround (i.e. extra-classical RF), respectively, of the recorded V1 neurons.</p>
<p>For analysis of the neurons’ stimulus preference, we first deconvolved the calcium traces (<xref rid="figS2" ref-type="fig">Suppl. Fig. 2a</xref>) to account for the slow kinetics of the calcium sensor (e.g. <xref ref-type="bibr" rid="c29">Pachitariu et al., 2018</xref>) and then used the deconvolved noise responses of each neuron to estimate an “event-triggered-average” (ETA) for the four stimulus conditions – center (C) and surround (S) for both UV and green (Green<sub>C</sub>, UV<sub>C</sub>, Green<sub>S</sub>, UV<sub>C</sub>). Specifically, deconvolved neuronal responses were reverse-correlated with the stimulus trace and the raw ETAs were then transformed into a lower dimensional representation using principal component analysis (PCA; <xref rid="fig1" ref-type="fig">Fig. 1d</xref> and <xref rid="figS3" ref-type="fig">Suppl. Fig. 3</xref>). Note that the deconvolution of raw calcium responses changes the kinetics of the ETAs, but not the neurons’ stimulus selectivity (<xref rid="figS2" ref-type="fig">Suppl. Fig. 2b,c</xref>).</p>
<p>Using this approach, we obtained ETAs of n=3,331 excitatory V1 neurons (n=6 recording fields, n=3 mice) with diverse center-surround stimulus preferences (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>, <xref rid="figS1" ref-type="fig">Suppl. Fig. 1c</xref>). This included neurons sensitive to luminance contrast that did not discriminate between stimulus color (cells 1 and 2 in <xref rid="fig1" ref-type="fig">Fig. 1d</xref>) and color selective cells only responding to one color of the stimulus (cells 3 and 4). In addition, some neurons exhibited color-opponency in the center (cells 5 and 6) or surround, meaning that a neuron prefers a stimulus of opposite polarity in the UV and green channel (e.g. UV-On and green-Off). To validate our experimental approach, we confirmed that the noise stimulus recovers well-described RF properties of mouse V1 neurons. First, the majority of neurons showed negatively correlated center and surround ETAs for both the UV and green channels (<xref rid="figS1" ref-type="fig">Suppl. Fig. 1d</xref>), meaning that a neuron preferring an increment of light in the center (“On”) favors a light decrement in the surround (“Off”) and vice versa. This finding is consistent with On-Off center-surround antagonism of neurons in early visual areas, and has been described in both the mouse retina (e.g. <xref ref-type="bibr" rid="c30">Franke et al., 2017</xref>) and mouse thalamus (e.g. <xref ref-type="bibr" rid="c31">Grubb and Thompson, 2003</xref>). Second, neurons recorded in posterior and anterior V1 preferred UV and green stimuli (<xref rid="figS1" ref-type="fig">Suppl. Fig. 1e,f</xref>), respectively, in line with the distribution of cone opsins across the retina (<xref ref-type="bibr" rid="c7">Szél et al., 1992</xref>; <xref ref-type="bibr" rid="c8">Baden et al., 2013</xref>) and previous cortical work (<xref ref-type="bibr" rid="c26">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="c32">Rhim et al., 2017</xref>; <xref ref-type="bibr" rid="c33">Aihara et al., 2017</xref>). This asymmetry in color preference was less pronounced for the surround (<xref rid="figS1" ref-type="fig">Suppl. Fig. 1e,f</xref>), as has been reported for retinal neurons in mice (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>). Taken together, these results show that our experimental paradigm using a parametric luminance and color noise stimulus accurately captures known center-surround RF properties of cortical neurons in mice.</p>
</sec>
<sec id="s2b">
<title>Color contrast is represented by the receptive field center in a large number of mouse V1 neurons</title>
<p>To systematically study how color is represented by the population of mouse V1 neurons, we mapped each cell’s center and surround ETA into a 2-dimensional space depicting neuronal sensitivity for luminance and color contrast (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). For each neuron, we extracted ETA peak amplitudes relative to baseline for all four stimulus conditions, with positive and negative peak amplitudes for On and Off stimulus preference, respectively. In this space, neurons sensitive to luminance contrast responding with the same polarity (i.e. On versus Off) to either color of the stimulus fall along the diagonal (cell 1 in <xref rid="fig2" ref-type="fig">Fig. 2a</xref>) and coloropponent neurons scatter along the off-diagonal (cell 2). In addition, the neuronal selectivity for UV- and green-stimuli is indicated by the relative distance to the x- and y-axis (cell 3), respectively. We found that most V1 neurons were sensitive to luminance contrast and fell in the upper right or lower left quadrant along the diagonal, both for the center and surround component of V1 RFs (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>). Nevertheless, a substantial fraction of neurons (33.1%) preferred color-opponent stimuli and scattered along the off-diagonal in the upper left and lower right quadrants, especially for the RF center. We quantified the fraction of variance explained by the luminance versus the color axis across the neuronal population by performing PCA on the center and surround contrast space, respectively (<xref ref-type="bibr" rid="c22">Qiu et al., 2021</xref>). The luminance axis captured the major part of the variance of stimulus sensitivity for the RF surround (82%), while it explained less of the variance for the RF center (67%). As a result, one third (33%) of the variance within the tested stimulus sensitivity space of the RF center was explained by the color axis. These results were consistent for a more conservative quality threshold, which only considered the best 25% of neurons (<xref rid="figS2" ref-type="fig">Suppl. Fig. 2e,f</xref>). In addition, the above results obtained by pooling data across animals were consistent within all three mice tested (<xref rid="figS4" ref-type="fig">Suppl. Fig. 4a</xref>). Therefore, all following analyses were based on data pooled across animals.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2.</label>
<caption><title>Strong neuronal representation of color in mouse primary visual cortex.</title>
<p><bold>a</bold>, Top left panel shows schematic drawing illustrating the green and UV contrast space used in the other panels and Figs. <bold>??</bold>. Amplitudes below and above zero indicates an Off and On cell, respectively. Achromatic On and Off cells will scatter in the lower left and upper right quadrants along the diagonal (“luminance contrast”), while color-opponent cells will fall within the upper left and lower right quadrants along the off-diagonal (“color contrast”). Blue and green shading indicates stronger responses to UV and green stimuli, respectively. The three other panels show ETAs of three example neurons (top) with the peak amplitudes (“contrast”) of their center (dot) and surround responses (triangle) indicated in the bottom. <bold>b</bold>, Density plot of peak amplitudes of center (top) and surround (bottom) ETAs across all neurons (n=3,331 cells, n=6 recording fields, n=3 mice). Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA) on the center or surround data, with percentage of variance explained along the polarity and color axis indicated. For reproducibility across animals, see <xref rid="figS3" ref-type="fig">Suppl. Fig. 3</xref>. <bold>c</bold>, Decoding discriminability of stimulus luminance (top) and stimulus color (bottom) based on center (black) and surround (gray) responses of different numbers of neurons. Decoding was performed using a support vector machine (SVM). Lines indicate the mean of 10-fold cross-validation (shown as dots). For luminance contrast, decoding discriminability was significantly different between center and surround for n=50 and n=100 neurons (T-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction). For color contrast, decoding discriminability was significantly different between center and surround for all numbers of neurons tested, except n=1 neuron.</p></caption>
<graphic xlink:href="543054v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We confirmed and quantified the pronounced color-opponency in mouse V1 we observed based on the neurons’ preferred stimuli (i.e. ETAs) using an independent decoding analysis. For that, we trained a non-linear support vector machine (SVM) to decode stimulus luminance (On versus Off) or color (UV versus green) based on the recorded and deconvolved neuronal activity. Decoding was performed using 10-fold cross-validation and decoding accuracy (in %) was transformed into mutual information (in bits). The discriminability of luminance contrast rapidly increased with the number of neurons used by the SVM decoder, and saturated close to perfect discriminability (&gt;0.92 bits, i.e. &gt;99% accuracy) when including more than 1,000 neurons (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). Interestingly, decoding performance was similar for center and surround stimuli, indicating that V1 responses are as informative about luminance contrast in the center as in the surround. We next trained a decoder to discriminate stimulus color. The decoding performance was lower for stimulus color compared to stimulus luminance (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>), consistent with the finding described above that V1 neurons are more sensitive to luminance than color contrast. In addition, discriminability of stimulus color was significantly better for stimuli presented in the RF center compared to stimuli shown in the RF surround, thereby verifying our ETA results. Together, our results demonstrate that for photopic light levels, neurons in mouse visual cortex strongly encode color features of the visual input in their RF center, while the RF surround predominantly captures luminance contrast.</p>
<p>The strong representation of color by the center component of V1 RFs was not solely inherited by color-opponency present in the center RF of retinal output neurons (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Khani and Gollisch, 2021</xref>). We tested this by using a publicly available dataset of retinal ganglion cell responses to a center and surround color flicker stimulus (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>), similar to the one used here, but with center and surround stimulus sizes adjusted to the smaller RF sizes of retinal neurons. We embedded each cell’s center and surround ETA into the luminance and color contrast space described above (<xref rid="figS5" ref-type="fig">Suppl. Fig. 5</xref>). We found that at the level of the retinal output, the color axis explained only 12% of the variance in the tested sensitivity space for the RF center (10 degrees visual angle), which is much lower than what we observed for V1 center RFs (37.5 degrees visual angle). The low fraction of center color-opponent retinal ganglion cells is in line with a recent study that characterized RF properties of these neurons using natural movies recorded in the mouse’s natural environment (<xref ref-type="bibr" rid="c34">Höfling et al., 2022</xref>). Together, this suggests that the pronounced center color-opponency in V1 neurons likely depends on the remapping of retinal center and surround RFs in downstream processing stages.</p>
</sec>
<sec id="s2c">
<title>The neuronal representation of color in mouse V1 decreases with lower ambient light levels</title>
<p>Previous studies have reported varying numbers of color-opponent neurons in mouse visual areas, ranging from very few in mouse dLGN (<xref ref-type="bibr" rid="c18">Denman et al., 2017</xref>) and V1 (<xref ref-type="bibr" rid="c19">Tan et al., 2015</xref>) to a large number in the thalamus (<xref ref-type="bibr" rid="c20">Mouland et al., 2021</xref>), visual cortex (<xref ref-type="bibr" rid="c21">Rhim and Nauhaus, 2023</xref>) and the retina (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>). In part, this discrepancy regarding the role of color for visual processing in mice is likely due to the fact that different studies have used different light levels, resulting in varying activations of rod and cone photoreceptors that are both involved in chromatic processing. To systematically study how ambient light levels affect the neuronal representation of color in mouse visual cortex, we repeated our experiments with the noise stimulus performed in photopic conditions (approx. 15,000 photoisomerizations (P*) per cone and second) in high (approx. 400 P* per cone and second) and low mesopic light conditions (approx. 50 P* per cone and second). The high mesopic light condition is expected to equally activate rod and cone photoreceptors, while the low mesopic condition largely drives rod photoreceptors, with only a small cone contribution. Indeed, decreasing ambient light levels resulted in a reduction of UV-sensitivity in posterior V1 neurons (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>), indicative for a gradual activation shift from UV-sensitive cones to rods, which are green sensitive. The neuronal representation of color greatly decreased when reducing ambient light levels: Both the fraction of ETA variance explained by the color axis (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>) and the decoding discriminability of stimulus color dropped significantly (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>). The drop in decoding discriminability was not due to lower signal-to-noise levels for the mesopic light levels, as the decoding of stimulus luminance was not significantly higher for the photopic condition compared to the mesopic conditions (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>). Across all light levels tested, the RF surround of V1 neurons was less informative about stimulus color, resulting in 40-60% lower discriminability in the surround compared to the center (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>). Interestingly, even for the lowest light level, where only a a small fraction of ETA variance was explained by stimulus color (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>), V1 neurons reliably encoded color information (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>). This suggests that weak cone activation in low mesopic light levels is sufficient to extract color features from the visual input. In summary, our results demonstrate that the neuronal representation of color in mouse visual cortex greatly depends on ambient light levels, and is strongest for photopic light levels that predominantly drive cone photoreceptors.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>Reduced representation of color contrast in mouse V1 for lower ambient light levels.</title>
<p><bold>a</bold>, Distribution of spectral contrast of center ETAs of all neurons recorded in posterior V1, for photopic (top, n=1,616 cells, n=3 recording fields, n=3 mice), high mesopic (middle, n=1,485 cells, n=3 recording fields, n=3 mice) and low mesopic (bottom, n=1,295 cells, n=3 recording fields, n=3 mice) ambient light levels. Black dotted lines indicate mean of distribution. Spectral contrast significantly differed across all combinations of light levels (T-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction). The triangle on the right indicates UV sensitivity of the neurons, which is decreasing with lower ambient light levels. <bold>b</bold> Density plot of peak amplitudes of center (left) and surround (right) ETAs. Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA) on the center or surround data, with percentage of variance explained along the polarity and color axis indicated. Top row shows high mesopic (n=3,522 cells, n=6 recording fields, n=3 mice) and bottom row low mesopic (n=2,705 cells, n=6 recording fields, n=3 mice) light levels. <bold>c</bold>, Discriminability (in bits) of luminance contrast (On versus Off) for the center across the three light levels tested, obtained from training support vector machine (SVM) decoders based on recorded noise responses of V1 neurons. Right plot shows the discriminability of luminance contrast for n=500 neurons for center and surround. Dots show decoding performance of 10 train/test trial splits. For n=500 neurons, decoding discriminability of the center was not significantly different across light levels (T-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction). The surround discriminability was significantly lower than the center for the photopic condition. <bold>d</bold>, Like (c), but showing discriminability of color contrast (green versus UV). Decoding discriminability was significantly different between center and surround for all three light levels. In addition, discriminability for the center was significantly different between photopic and mesopic conditions, but not between the two mesopic conditions (T-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction).</p></caption>
<graphic xlink:href="543054v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>Cortical representation of color changes across the visual field</title>
<p>Chromatic and achromatic features present in natural scenes systematically vary across the visual field, with notable differences between regions below and above the horizon (<xref ref-type="bibr" rid="c22">Qiu et al., 2021</xref>; <xref ref-type="bibr" rid="c35">Nilsson et al., 2022</xref>). Recently, it has been demonstrated that color contrast in scenes from the mouse’s natural environment is enriched in the upper visual field (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>; <xref ref-type="bibr" rid="c22">Qiu et al. (2021)</xref>; <xref ref-type="bibr" rid="c23">Abballe and Asari (2022)</xref>). To encode the sensory input efficiently, these scene statistics should ideally be reflected in the neuronal representations, as has been observed at the level of the mouse retina (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Khani and Gollisch, 2021</xref>). To study how the representation of color changes across the visual field in mouse V1, we separately analyzed the neurons recorded in posterior and anterior V1, which encode visual information from the upper and lower visual field, respectively. We focused this analysis on the RF center because V1 surround RFs were on average predominantly explained by luminance contrast (cf. <xref rid="fig2" ref-type="fig">Fig. 2</xref>). We found that the color axis explained twice as much ETA variance in posterior compared to anterior V1 (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>): It captured 39% of the variance in the upper visual field and only 19% of the variance in the lower visual field. This finding was consistent across animals (<xref rid="figS3" ref-type="fig">Suppl. Fig. 3b</xref>). In line with this, the discriminability of stimulus color was significantly higher when using the responses of posterior V1 neurons for decoding (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>). Together, this revealed a stronger cortical representation of color in posterior than anterior mouse visual cortex, which might be an adaptation to efficiently encode the enriched color contrast in the upper visual field of mouse natural scenes (<xref ref-type="bibr" rid="c22">Qiu et al., 2021</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4.</label>
<caption><title>Cortical representation of color changes across visual space.</title>
<p><bold>a</bold>, Natural scene captured in the natural environment of mice using a custom-built camera adjusted to the mouse’s spectral sensitivity (<xref ref-type="bibr" rid="c22">Qiu et al., 2021</xref>). Dashed line indicates the horizon and separates the scene into lower and upper visual field. Previous studies (<xref ref-type="bibr" rid="c22">Qiu et al., 2021</xref>; <xref ref-type="bibr" rid="c23">Abballe and Asari, 2022</xref>) have reported higher color contrast in the upper compared to the lower visual field. <bold>b</bold> Density plot of peak amplitudes of center ETAs across neurons recorded in posterior (top) and anterior V1 (bottom). Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA), with percentage of variance explained along the polarity and color axis indicated. For reproducibility across animals, see <xref rid="figS3" ref-type="fig">Suppl. Fig. 3</xref>. <bold>c</bold> Discriminability (in bits) of color contrast contrast (UV or green) for neurons recorded in posterior (blue) and anterior V1 (green), obtained from training support vector machine (SVM) decoders based on recorded noise responses of V1 neurons. The decoding discriminability was significantly different between anterior and posterior neurons (T-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction).</p></caption>
<graphic xlink:href="543054v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2e">
<title>Asymmetric distribution of color response types explains higher color sensitivity in posterior V1</title>
<p>Next, we investigated the mechanism underlying this asymmetry in color encoding across mouse visual cortex. In the mouse retina, different retinal ganglion cell types are differentially distributed across the retina and, therefore, asymmetrically sample the visual space (reviewed in <xref ref-type="bibr" rid="c36">Baden et al., 2020</xref>). For example, W3 cells that have been linked to aerial predator detection exhibit the highest density in the ventral retina looking at the sky (<xref ref-type="bibr" rid="c37">Zhang et al., 2012</xref>). Similarly, we hypothesized that the difference in decoding performance of stimulus color in posterior and anterior V1 might be due to an asymmetric distribution of functional neuron types sensitive to color versus luminance contrast. To test this, we clustered the ETAs of all neurons into “functional response types” and quantified the distribution of the identified response types across cortical position. Specifically, we used the features extracted from the ETAs by PCA (cf. <xref rid="figS3" ref-type="fig">Suppl. Fig. 3</xref>) and clustered the feature weights into 17 response types using a Gaussian of Mixture model (GMM; <xref rid="fig5" ref-type="fig">Fig. 5a</xref> and <xref rid="figS4" ref-type="fig">Suppl. Fig. 4</xref>). We used 17 components for the GMM because this resulted in the best model on held-out test data (<xref rid="figS4" ref-type="fig">Suppl. Fig. 4a</xref>), although the performance was relatively flat for a wide range of components. The mean assignment accuracy of generated ground-truth labels was 89.2% (<italic>±</italic> 6%) and all response types were present in all mice (<xref rid="figS4" ref-type="fig">Suppl. Fig. 4b,c</xref>), indicating that the response types are well-separated and robust. The response types greatly differed with respect to functional properties, such as color-opponency, response polarity, and surround antagonism (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>), and, therefore, covered distinct sub-spaces of the color and luminance sensitivity space (<xref rid="figS4" ref-type="fig">Suppl. Fig. 4d</xref>). Approximately half of the response types were sensitive to luminance contrast (types 1-8) and exhibited different response polarities and surround strengths. The other half consisted of types with a strong selectivity for UV or green center stimuli (types 9-13) and color-opponency in the center (types 14-17).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5.</label>
<caption><title>Asymmetric distribution of color response types explains higher color sensitivity in posterior V1.</title>
<p><bold>a</bold>, Clustering result of the Gaussian Mixture model with n=17 clusters (see <xref rid="figS3" ref-type="fig">Suppl. Fig. 3</xref> for details). Model input corresponded to the weights of the principal components used for reconstructing ETAs. Left panel shows ETAs of all cells, respectively, sorted by cluster assignment. Right panel shows mean ETA of each cluster (s.d. shading in gray). Clusters are sorted based on broad response categories, which are indicated on the right. <bold>b</bold>, Left shows distribution of cells assigned to three different clusters (color) in a posterior and anterior recording field of one example animal. Gray dots show cells assigned to other clusters. Distribution index for each cluster is indicated below. Right shows the mean distribution index per cluster, with different marker shapes indicating the indices for individual animals. Zero indicates an even distribution across anterior and posterior V1 and values above and below zero indicate that cells are enriched in anterior and posterior V1, respectively. Dotted horizontal lines at -0.33/0.33 indicate twice as many cells in posterior than anterior cortex and vice versa. <bold>c</bold>, Histograms of peak amplitudes of center ETAs for clusters that are evenly distributed across V1 (left, distribution index &gt;-0.33 and &lt;0.33), enriched in anterior V1 (middle, distribution index) and enriched in posterior V1 (right). Cluster means and s.d. are indicated in color. Dotted lines correspond to axes of PCs obtained from a PCA, with percentage of variance explained along the luminance and color axis indicated. <bold>d</bold>, Discriminability (in bits) of stimulus color contrast based on response types enriched in anterior (black) and posterior (gray) V1. Dots show decoding performance across 10 train/test trial splits. Decoding discriminability was significantly different between anterior- and posterior-enriched types for all numbers of neurons tested (T-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction). <bold>e</bold>, Noise images with or without a “predator”-like dark object in the UV channel were convolved with simulated center RFs, depicting the mean amplitudes of the green and UV center ETA per response type (shown for types 4 and 14). The resulting activity maps were summed and thresholded to simulate responses to n=1,000 noise and object scenes. <bold>f</bold>, Discriminability (in bits) of the presence of a “predator”-like dark object in the UV channel per response type. Error bars show s.d. across 10 train/test trial splits.</p></caption>
<graphic xlink:href="543054v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We next investigated the distribution of individual response types across anterior and posterior V1 by computing a cortical distribution index (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). This index was -1 and 1 if all cells of one response type were located in posterior and anterior V1, respectively, and 0 if the respective response type was evenly distributed across cortex. We found that approximately half the response types were equally distributed in mouse V1 (distribution index from -0.3 to 0.3), including mostly response types sensitive to luminance contrast. Interestingly, response types with green-Off/UV-On color-opponency were also uniformly spread across the anterior-posterior axis of mouse V1, suggesting that a neuronal substrate supporting color vision exists in both the upper and lower visual field. Response types enriched in anterior V1 (distribution index &gt;0.3) fell along the luminance contrast axis but showed a preference for green center stimuli, consistent with the higher green sensitivity of cone photoreceptors sampling the ground (<xref ref-type="bibr" rid="c8">Baden et al., 2013</xref>). Similarly, as expected from the high density of UV-sensitive cone photoreceptors in the ventral retina (<xref ref-type="bibr" rid="c8">Baden et al., 2013</xref>), one response type strongly enriched in the posterior cortex (distribution index &lt;0.3) preferred UV in the RF center. To our surprise, response types with a green-On/UV-Off color-opponency were almost exclusively confined to posterior V1. As a result of this, the color axis explained 73% of ETA variance for the response types enriched in posterior cortex, while it explained only 17% for the anterior-enriched response types. We confirmed the higher sensitivity for color versus luminance contrast of posterior response types by showing that their decoding discriminability of color was significantly better than that for anterior response types (<xref rid="fig5" ref-type="fig">Fig. 5d</xref>). Together, these results demonstrate that the asymmetry in neuronal color tuning across cortical position we report in mice can be explained by an uneven distribution of color-opponent response types.</p>
<p>We next speculated about the computational role of the green-On/UV-Off color-opponent response types largely present in posterior V1. As most predators are expected to approach the mouse from above, color-opponency in the upper visual field could well support threat detection. Especially for visual scenes with inhomogeneous illumination (e.g. in the forest), which result in large intensity fluctuations at the photoreceptor array, color-opponent RF structures may result in a more reliable signal (discussed in <xref ref-type="bibr" rid="c38">Maximov, 2000</xref>; <xref ref-type="bibr" rid="c39">Kelber et al., 2003</xref>). To test this prediction, we used parametric stimuli inspired by noisy natural scenes, containing only noise, or a dark ellipse of varying size, angle and position on top of noise (<xref rid="fig5" ref-type="fig">Fig. 5e</xref>). The dark object had a higher contrast in the UV than green channel, as it has been shown that objects in the sky (<xref ref-type="bibr" rid="c22">Qiu et al., 2021</xref>), underwater (<xref ref-type="bibr" rid="c40">Cronin and Bok, 2016</xref>; <xref ref-type="bibr" rid="c41">Losey et al., 1999</xref>) or in the snow (<xref ref-type="bibr" rid="c42">Tyler et al., 2014</xref>) are often more visible in the UV than the green wavelength range. For each response type, we first simulated responses to these scenes based on the type’s luminance and color contrast sensitivity of the RF center using a simple linear-nonlinear model (<xref rid="fig5" ref-type="fig">Fig. 5e</xref>). We then used the simulated responses to train an SVM decoder to discriminate between object and noise-only scenes. While all Off-center response types sensitive to luminance contrast could decode the dark object, the two best performing types corresponded to the green-On/UV-Off response types enriched in posterior V1 (<xref rid="fig5" ref-type="fig">Fig. 5f</xref>). Interestingly, the reason for their good performance was the absence of responses to the noise scenes, rather than strong responses to the object scenes per se (<xref rid="fig5" ref-type="fig">Fig. 5e</xref>). Our results suggest that functional neuron types in mouse V1 with distinct color properties unevenly sample different parts of the visual scene, and might thereby serve a distinct role in driving visually-guided behavior like predator detection.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Here, we found that a large fraction of neurons in mouse visual cortex encode color features of the visual input in their RF center. Color-opponency was strongest for photopic light levels and especially pronounced in posterior V1 encoding the sky. This asymmetry in color processing across visual space was due to an inhomogeneous distribution of color-opponent response types, with Green-On/UV-Off response types predominantly being present in posterior V1. Using a simple model and natural scene inspired parametric stimuli, we showed that this type of color-opponency may enhance the detection of aerial predators in noisy daylight scenes.</p>
<sec id="s3a">
<title>Neuronal correlates of color vision in mice</title>
<p>In most species, color vision is mediated by comparing signals from different cone photoreceptor types sensitive to different wavelengths (reviewed in <xref ref-type="bibr" rid="c1">Baden and Osorio, 2019</xref>). This includes circuits with cone-type selective wiring present in many vertebrate species and circuits with random and cone type-unselective wiring like red-green opponency in primates. Recently, it has been demonstrated that there is extensive rod-cone opponency in mice, comparing signals from UV-sensitive cones in the ventral retina to rod signals (<xref ref-type="bibr" rid="c15">Joesch and Meister, 2016</xref>; <xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Khani and Gollisch, 2021</xref>; <xref ref-type="bibr" rid="c21">Rhim and Nauhaus, 2023</xref>). In the retina, this mechanism relies on integrating information from cone signals in the RF center with rod signals in the RF surround (<xref ref-type="bibr" rid="c15">Joesch and Meister, 2016</xref>; <xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Khani and Gollisch, 2021</xref>). Interestingly, there is also evidence for rod-cone opponency in monochromatic humans (<xref ref-type="bibr" rid="c43">Reitner et al., 1991</xref>), suggesting that a neuronal circuit to compare rod and cone signals exists in other mammals as well. At this point, it is still unclear to what extent behavioral color discrimination in mice (<xref ref-type="bibr" rid="c9">Jacobs et al., 2004</xref>; <xref ref-type="bibr" rid="c10">Denman et al., 2018</xref>) is driven by rod-cone versus cone-cone comparisons.</p>
<p>Here, we found that the neuronal representation of color in mouse visual cortex is most prominent for photopic light levels and decreases for mesopic conditions, indicating that color-opponency in mouse V1 is at least partially mediated by the comparison of cone signals and not purely by cone-rod comparisons. Our result is consistent with a recent study reporting pronounced cone-mediated color-opponency in mouse dLGN (<xref ref-type="bibr" rid="c20">Mouland et al., 2021</xref>). In our experimental paradigm, dissecting the relative contribution of rods and M-cones in color-opponency of mouse V1 neurons was not possible, due to the highly overlapping wavelength sensitivity profiles of mouse M-opsin and Rhodopsin. However, it is very likely that rods contribute to the prominent color-opponent neuronal responses we observed in mouse V1. This is especially true for posterior V1 receiving input from the ventral retina where cone-cone comparisons are challenging due to the co-expression of S-opsin in M-cones (<xref ref-type="bibr" rid="c7">Szél et al., 1992</xref>; <xref ref-type="bibr" rid="c8">Baden et al., 2013</xref>). The involvement of rods in generating color-opponent responses is supported by retinal data (<xref ref-type="bibr" rid="c15">Joesch and Meister, 2016</xref>; <xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Khani and Gollisch, 2021</xref>) and a recent study performed in mouse visual cortex showing that color-opponency in posterior V1 is best explained by a model that compares S-opsin with Rhodopsin (<xref ref-type="bibr" rid="c21">Rhim and Nauhaus, 2023</xref>). Surprisingly, we found that the mouse visual system still extracts color information for relatively low light levels present during early dusk and dawn, potentially by comparing rod to remaining cone signals. Future studies will tell whether color opponency under dim light, as observed here for low mesopic conditions, require a specific neuronal pathway amplifying the relatively weak cone signals to encode color features present in the environment.</p>
<p>The results from our study, together with recent findings across the visual hierarchy of mice (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Khani and Gollisch, 2021</xref>; <xref ref-type="bibr" rid="c20">Mouland et al., 2021</xref>; <xref ref-type="bibr" rid="c21">Rhim and Nauhaus, 2023</xref>), demonstrate a pronounced neuronal representation of color in mouse visual brain areas that is mediated by both cone-cone and cone-rod comparisons. While this highlights the relevance of color information in mouse vision, it remains unclear as to how mice use color vision to inform natural behaviors. Two behavioral studies using parametric stimuli and relatively simple behavioral paradigms have shown that mice can discriminate different colors (<xref ref-type="bibr" rid="c9">Jacobs et al., 2004</xref>; <xref ref-type="bibr" rid="c10">Denman et al., 2018</xref>), at least in their central and upper visual field (<xref ref-type="bibr" rid="c10">Denman et al., 2018</xref>). Here, we found that Green-Off/UV-On color-opponency was equally distributed across cortex, suggesting that there is a neuronal substrate for color vision in mice across the entire visual field. In contrast, Green-On/UV-Off color-opponency was confined to posterior V1, where it might aid the detection of aerial predators present in cluttered and noisy daylight scenes, such as in the forest. Testing this hypothesis and further elucidating the role of color vision in mouse natural behaviors will require combining more unrestrained behavioral paradigms with ecologically-relevant stimuli.</p>
</sec>
<sec id="s3b">
<title>Limitations of the stimulus paradigm</title>
<p>To study color processing in mouse primary visual cortex, we used a parametric center-surround color flicker stimulus, similar to other work (<xref ref-type="bibr" rid="c6">Chatterjee and Callaway, 2003</xref>; <xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c24">Zimmermann et al., 2018</xref>). The advantage of this relatively simple stimulus is that the corresponding neuronal responses are easy to interpret using linear methods, like the spike-triggered-average approach (<xref ref-type="bibr" rid="c44">Schwartz et al., 2006</xref>). For this analysis, the stimulus should ideally be aligned to the center RF of each neuron, which requires detailed RF mapping of individual neurons. As this procedure is relatively time consuming and low throughput, we instead used a center stimulus that was slightly larger than RFs of single neurons, and was centered on the mean RF across a population of V1 neurons. To reduce the impact of potential stimulus misalignment on the single cell level on our results, we used different experimental steps and controls, such as confirming that the RF center of most recorded neurons greatly overlaps with the center noise stimulus. The fact that response types identified using an automated clustering approach were consistent across animals suggests that stimulus alignment did not significantly contribute to the neurons’ visual responses. Nevertheless, we cannot exclude that the stimulus was misaligned for a subset of the recorded neurons used for analysis. Stimulus misalignment might have contributed to single cells not having surround ETAs, due to simultaneous activation of antagonistic center and surround RF components by the surround stimulus.</p>
</sec>
<sec id="s3c">
<title>Asymmetric processing of color information across the visual field</title>
<p>The spatial arrangements of sensory neurons are ordered in a way that encodes particular characteristics of the surrounding environment. One classical example in the visual system is that the density of all retinal output neurons increases and their dendritic arbor size decreases towards retinal locations with higher sampling frequency, such as the fovea in primates and the area centralis in carnivores (discussed in <xref ref-type="bibr" rid="c45">Peichl, 2005</xref>). More recent research has uncovered how the visual circuits in certain species are customized to suit the statistics of the visual information they receive, including the distribution of spatial, temporal, and spectral information, as well as the specific requirements of their behavior (discussed in <xref ref-type="bibr" rid="c36">Baden et al., 2020</xref>). For example, a study in zebrafish larvae showed that UV cones in one particular retinal location are specifically tuned for UV-bright objects, thereby supporting prey capture in their upper frontal visual field (<xref ref-type="bibr" rid="c46">Yoshimatsu et al., 2020</xref>).</p>
<p>Here, we found that there is a pronounced asymmetry in how color is represented across visual space in mouse primary visual cortex. A similar asymmetry in color processing was reported at the level of the mouse retina (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Khani and Gollisch, 2021</xref>) and dLGN (<xref ref-type="bibr" rid="c20">Mouland et al., 2021</xref>), and has been linked to an inhomogeneous distribution of color contrast across natural scenes from the mouse’s environment (<xref ref-type="bibr" rid="c22">Qiu et al., 2021</xref>; <xref ref-type="bibr" rid="c23">Abballe and Asari, 2022</xref>). Specifically, it has been speculated that the higher color contrast present in the upper visual field of natural scenes captured in the natural habitat of mice might have driven superior color-opponency in the ventral retina (<xref ref-type="bibr" rid="c22">Qiu et al., 2021</xref>), thereby supporting color discrimination in the sky (<xref ref-type="bibr" rid="c10">Denman et al., 2018</xref>). Our results extend these previous studies by demonstrating that the asymmetry across visual cortex can be explained by the asymmetric distribution of response types with distinct color tuning in their RF center, and by linking them to a neuronal computation relevant for the upper visual field, namely the detection of aerial predators.</p>
<p>At the level of the mouse retina, color-opponency is largely mediated by center-surround interactions (<xref ref-type="bibr" rid="c15">Joesch and Meister, 2016</xref>; <xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Khani and Gollisch, 2021</xref>) and only very few neurons exhibit color-opponency in their center (<xref ref-type="bibr" rid="c34">Höfling et al., 2022</xref>). In line with this, we found that the pronounced representation of color by the center component of V1 RFs was not solely inherited by color-opponency present in the RF center of retinal output neurons. Similarly, a recent study concluded that the extensive and sophisticated color processing in the mouse LGN cannot simply be explained by the so far proposed retinal opponency mechanisms (<xref ref-type="bibr" rid="c20">Mouland et al., 2021</xref>). Note that we compare properties of the center component of retinal (3-10 degrees visual angle) and V1 RFs (10-25 degrees visual angle), and that the integration of center and surround retinal signals might still contribute to color-opponency observed in downstream visual areas. Together, this suggests that color-opponency in the RF center of visual neurons in the brain may be created downstream of the retina by integrating across space and/or combining different retinal output channels. Based on this example about color processing, one might speculate that in mice retinal representations are remapped in downstream brain areas to form a more specific and behaviorally-relevant representation of visual features.</p>
</sec>
<sec id="s3d">
<title>Strategies of color processing across animal species: Distributed versus specialized code</title>
<p>In primates, physiological and anatomical evidence suggest that a small number of distinct retinal cell types transmit color information to downstream visual areas (reviewed in <xref ref-type="bibr" rid="c47">Thoreson and Dacey, 2019</xref>), where the neuronal representation of color remains partially segregated from the representation of other visual features like form (<xref ref-type="bibr" rid="c48">Livingstone and Hubel, 1988</xref>; <xref ref-type="bibr" rid="c49">Zeki, 1978</xref>, but see <xref ref-type="bibr" rid="c50">Garg et al. (2019)</xref>). For example, color-sensitive neurons in primary and secondary visual cortex are enriched in so-called “blob” (<xref ref-type="bibr" rid="c51">Hubel and Livingstone, 1987</xref>) and “inter-stripe” regions (<xref ref-type="bibr" rid="c52">DeYoe and Van Essen, 1985</xref>), respectively. Interestingly, in other vertebrate species, color processing is distributed across many neuron types and cannot easily be separated from the processing of other visual features. In zebrafish, birds, Drosophila and mice, a large number of retinal output types encode information about stimulus color (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Khani and Gollisch, 2021</xref>; <xref ref-type="bibr" rid="c53">Seifert et al., 2022</xref>; <xref ref-type="bibr" rid="c54">Zhou et al., 2020</xref>), in addition to each type’s preferred feature like direction of motion. In addition, there is evidence for distributed processing of color in visual areas down-stream to the retina in zebrafish (Guggiana <xref ref-type="bibr" rid="c55">Nilo et al., 2021</xref>), mice (<xref ref-type="bibr" rid="c20">Mouland et al., 2021</xref>; <xref ref-type="bibr" rid="c21">Rhim and Nauhaus, 2023</xref>), Drosophila (<xref ref-type="bibr" rid="c56">Longden et al., 2021</xref>) and tree shrew (<xref ref-type="bibr" rid="c57">Johnson et al., 2010</xref>). Our results demonstrate a prominent neuronal representation of color in mouse primary visual cortex, which is distributed across many neurons and multiple response types.</p>
<p>What might be the benefit of such a distributed code of color processing? It is important to note that chromatic signals may not only be used for color discrimination per se, but instead different spectral channels might facilitate the extraction of specific features from the environment. For example, it has been shown that the UV wavelength range aids the detection of objects like prey, predators, and food (reviewed in <xref ref-type="bibr" rid="c40">Cronin and Bok, 2016</xref>) by increasing their contrast, as recently shown for leaf surface contrasts in forest environments (<xref ref-type="bibr" rid="c58">Tedore and Nilsson, 2019</xref>). Indeed, it is hypothesized that different photoreceptor types sensitive to distinct wavelength bands did not evolve to support color discrimination, but instead to reduce lighting noise in the natural environment of early vertebrates (discussed in <xref ref-type="bibr" rid="c38">Maximov, 2000</xref>; <xref ref-type="bibr" rid="c39">Kelber et al., 2003</xref>). In line with this idea, our analysis suggests that Green-On/UV-Off color-opponency might facilitate the detection of predatory-like dark objects in the UV channel by reducing the neurons’ activation to noise, rather than increasing the neurons’ activation to the object. If chromatic signals are predominantly used to boost contrast of specific aspects of the environment, it might make sense to widely distribute chromatic tuning and color-opponency across visual neurons. Further experiments and analysis will uncover the computational relevance of the pronounced and distributed color representations observed in mice and other vertebrate species.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Neurophysiological experiments</title>
<p>All procedures were approved by the Institutional Animal Care and Use Committee of Baylor College of Medicine. Owing to the explanatory nature of our study, we did not use randomization and blinding. No statistical methods were used to predetermine sample size.</p>
<p>Mice of either sex (Mus musculus, n=9; 2-5 months of age) expressing GCaMP6s in excitatory neurons via Slc17a7-Cre and Ai162 transgenic lines (stock number 023527 and 031562, respectively; The Jackson Laboratory) were anesthetized and a 4 mm craniotomy was made over the visual cortex of the right hemisphere as described previously (<xref ref-type="bibr" rid="c59">Reimer et al., 2014</xref>; <xref ref-type="bibr" rid="c60">Froudarakis et al., 2014</xref>). For functional recordings, awake mice were head-mounted above a cylindrical treadmill and calcium imaging was performed using a Ti-Sapphire laser tuned to 920 nm and a two-photon microscope equipped with resonant scanners (Thorlabs) and a 25x objective (MRD77220, Nikon). Laser power after the objective was kept below 60mW. The rostro-caudal treadmill movement was measured using a rotary optical encoder with a resolution of 8,000 pulses per revolution. We used light diffusing from the laser through the pupil to capture eye movements and pupil size. Images of the pupil were reflected through a hot mirror and captured with a GigE CMOS camera (Genie Nano C1920M; Teledyne Dalsa) at 20 fps at a 1,920 × 1,200 pixel resolution. The contour of the pupil for each frame was extracted using DeepLabCut (<xref ref-type="bibr" rid="c61">Mathis et al., 2018</xref>) and the center and major radius of a fitted ellipse were used as the position and dilation of the pupil.</p>
<p>For image acquisition, we used ScanImage. To identify V1 boundaries, we used pixelwise responses to drifting bar stimuli of a 2,400 × 2,400 μm scan at 200 μm depth from cortical surface (<xref ref-type="bibr" rid="c62">Garrett et al., 2014</xref>), recorded using a large field of view mesoscope (<xref ref-type="bibr" rid="c63">Sofroniew et al., 2016</xref>). Functional imaging was performed using 512 × 512 pixel scans (700 × 700 μm) recorded at approx. 15 Hz and positioned within L2/3 (depth 200 μm) in posterior or anterior V1. Imaging data were motion-corrected, automatically segmented and deconvolved using the CNMF algorithm (<xref ref-type="bibr" rid="c64">Pnevmatikakis et al., 2016</xref>); cells were further selected by a classifier trained to detect somata based on the segmented masks. This resulted in approx. 500-1,200 selected soma masks per scan depending on response quality and blood vessel pattern.</p>
<p>To achieve photopic stimulation of the mouse visual system, we dilated the pupil pharmacologically with atropine eye drops (<xref ref-type="bibr" rid="c26">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="c65">Rhim et al., 2021</xref>). Specifically, atropine was applied to the left eye of the animal facing the screen for visual stimulation. Functional recordings started after the pupil was dilated. Pharmacological pupil dilation lasted <italic>&gt;</italic>2 hours, thereby ensuring a constant pupil size during all functional recordings.</p>
</sec>
<sec id="s4b">
<title>Visual stimulation</title>
<p>Visual stimuli were presented to the left eye of the mouse on a 42 × 26 cm light-transmitting teflon screen (McMaster-Carr) positioned 12 cm from the animal, covering approx. 120 × 90 degree visual angle. Light was back-projected onto the screen by a DLP-based projector (EKB Technologies Ltd; <xref ref-type="bibr" rid="c25">Franke et al., 2019</xref>) with UV (395 nm) and green (460 nm) LEDs that differentially activated mouse S- and M-opsin. LEDs were synchronized with the microscope’s scan retrace.</p>
<p>Light intensity (estimated as photoisomerization rate, P* per second per cone) was calibrated using a spectrometer (USB2000+, Ocean Optics) to result in equal activation rates for mouse M- and S-opsin (for details see <xref ref-type="bibr" rid="c25">Franke et al., 2019</xref>). In brief, the spectrometer output was divided by the integration time to obtain counts/s and then converted into electrical power (in nW) using the calibration data (in μJ/count) provided by Ocean Optics. To obtain the estimated photoisomerization rate per photoreceptor type, we first converted electrical power into energy flux (in eV/s) and then calculated the photon flux (in photons/s) using the photon energy (in eV). The photon flux density (in photons/s/μm2) was then computed and converted into photoisomerization rate using the effective activation of mouse cone photoreceptors by the LEDs and the light collection area of cone outer segments. In addition, we considered both the wavelength-specific transmission of the mouse optical apparatus (<xref ref-type="bibr" rid="c66">Henriksson et al., 2010</xref>) and the ratio between pupil size and retinal area (Schmucker and Scha-effel, 2004). Please see the calibration iPython notebook provided online for further details.</p>
<p>We used three different light levels, ranging from photopic levels primarily activating cone photoreceptors to low mesopic levels that predominantly drive rod photoreceptors. For a mean pupil size across recordings within one light level and a maximal stimulus intensity (255 pixel values), this resulted in 50 P*, 400 P* and 15.000 P* for low mesopic, high mesopic and photopic light levels, respectively. Please note that the difference between photopic and high mesopic light levels is higher than between high and low photopic light levels.</p>
<p>Prior to functional recordings, the screen was positioned such that the population RF across all neurons, estimated using an achromatic sparse noise paradigm, was within the center of the screen. Screen position was fixed and kept constant across recordings of the same neurons. We used Psychtoolbox in Matlab for stimulus presentation and showed the following light stimuli:</p>
<sec id="s4b1">
<title>Center-surround luminance and color noise</title>
<p>We used a center (diameter: 37.5° visual angle) and surround (full screen except the center) binary noise stimulus of UV and green LED to characterize center and surround chromatic properties of mouse V1 neurons. For that, the intensity of UV and green center and surround spots was determined independently by a binary and balanced 25-minute random sequence updated at 5 Hz. A similar stimulus was recently used in recordings of the mouse retina (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>). The center size of 37.5° visual angle in diameter is larger than the mean center RF size of mouse V1 neurons (26.2 <italic>±</italic> 4.6 degrees visual angle in diameter). This allowed to record from a large neuron population, despite some variability in RF center location. We verified that the center RF of the majority of neurons lies within the center spot of the noise stimulus using a sparse noise stimulus for spatial RF mapping (<xref rid="figS1" ref-type="fig">Suppl. Fig. 1a,b</xref>).</p>
</sec>
<sec id="s4b2">
<title>Sparse noise</title>
<p>To map the spatial RFs of V1 neurons, we used a sparse noise paradigm. UV and green bright (pixel value 255) and dark (pixel value 0) dots of approx. 12° visual angle were presented on a gray background (pixel value 127) in randomized order. Dots were presented for 8 and 5 positions along the horizontal and vertical axis of the screen, respectively, excluding screen margins. Each presentation lasted 200 ms and each condition (e.g. UV bright dot at position x=1 and y=1) was repeated 50 times.</p>
</sec>
</sec>
<sec id="s4c">
<title>Preprocessing of neural responses and behavioral data</title>
<p>Neuronal calcium responses were deconvolved using constrained non-negative calcium deconvolution (<xref ref-type="bibr" rid="c64">Pnevmatikakis et al., 2016</xref>) to obtain estimated spike trains. For the decoding paradigm, we subsequently extracted the accumulated activity of each neuron between 50 ms after stimulus onset and offset using a Hamming window. Behavioral traces (treadmill velocity and pupil size) were synchronized to the recorded neuronal response traces, but not used for further processing - i.e. we did not distinguish between arousal states of the animal.</p>
</sec>
<sec id="s4d">
<title>Receptive field mapping based on the center-surround color noise stimulus</title>
<p>We used the responses to the 5 Hz center-surround noise stimulus of UV and green LED to compute temporal ETAs of V1 neurons. Specifically, we upsampled both stimulus and responses to 30 Hz, normalized each upsampled response trace by its sum and then multiplied the stimulus matrix with the response matrix for each neuron. Per cell, this resulted in a temporal ETA for center (C) and surround (S) in response to UV and green flicker, respectively (Green<sub>C</sub>, UV<sub>C</sub>, Green<sub>S</sub>, UV<sub>C</sub>). For each of the four stimulus conditions, kernel quality was measured by comparing the variance of the ETA with the variance of the baseline, defined as the first 500 ms of the ETA. Only cells with at least 10-times more variance of the kernel compared to baseline for UV or green center ETA were considered for further analysis.</p>
</sec>
<sec id="s4e">
<title>Sparse noise spatial receptive field mapping and overlap index</title>
<p>We estimated spatial ETAs of V1 neurons in response to the sparse noise stimulus by multiplying the stimulus matrix with the response matrix of each neuron (<xref ref-type="bibr" rid="c44">Schwartz et al., 2006</xref>). For that, we averaged across On and Off and UV and green stimuli, thereby obtaining a two-dimensional (8 × 5 pixels) spatial ETA per neuron. To assess ETA quality, we generated response predictions by multiplying the flattened ETA of each neuron with the flattened stimulus frames and compared the predictions to the recorded responses by estimating the linear correlation coefficient. For analysis, we only included cells where correlation <italic>&gt;</italic>0.25. For these cells, we upsampled and peak-normalized the spatial ETAs (resulting in 40 × 25 pixels), and then estimated the overlap with the center spot of the noise stimulus using a contour threshold of 0.25. Specifically, we calculated the ratio of pixels <italic>&gt;</italic>0.25 with respect to the peak of the ETA inside and outside the area of the noise center spot.</p>
</sec>
<sec id="s4f">
<title>Principal component analysis for ETA reconstruction</title>
<p>To increase the signal-to-noise ratio of the ETAs, we converted them into lower dimensional representations using sparse PCA (<xref rid="figS2" ref-type="fig">Suppl. Fig. 2</xref>). Specifically, we concatenated the ETAs of the four stimulus conditions and used the resulting matrix with dimensions <italic>neurons</italic> x <italic>time</italic> to perform sparse PCA using the package <italic>sklearn</italic>.<italic>decomposition</italic>.<italic>SparseP CA</italic> in Python. We used sparse PCA because each principal component (PC) then captured one of the four stimulus conditions (<xref rid="figS2" ref-type="fig">Suppl. Fig. 2b</xref>), thereby making the PCs interpretable. We tested different numbers of PCs (n=2 to n=12 components) and evaluated the quality of the PCA reconstructions by computing the mean squared error (<italic>mse</italic>) between the original ETA and the one reconstructed based on the PCs. We decided to use 8 PCs because (i) reconstruction <italic>mse</italic> dropped only slightly with more PCs (<xref rid="figS2" ref-type="fig">Suppl. Fig. 2a</xref>) and (ii) additional PCs captured variance outside the time window of expected stimulus sensitivity, e.g. after the response time.</p>
</sec>
<sec id="s4g">
<title>Spectral contrast</title>
<p>For estimating the chromatic preference of the recorded neurons, we used spectral contrast (<italic>SC</italic>). It is estimated as Michelson contrast ranging from -1 and 1 for a neuron responding solely to UV and green contrast, respectively. We define <italic>SC</italic> as
<disp-formula id="ueqn1">
<graphic xlink:href="543054v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>r</italic><sub><italic>green</italic></sub> and <italic>r</italic><sub><italic>UV</italic></sub> correspond to the amplitude of UV and green ETA to estimate the neurons’ chromatic preference.</p>
</sec>
<sec id="s4h">
<title>Luminance and color contrast sensitivity space</title>
<p>To represent each neuron in a two-dimensional luminance and color contrast space, we extracted ETA peak amplitudes relative to baseline for all four stimulus conditions, with positive and negative peak amplitudes for On and Off cells, respectively. Peak amplitudes of green and UV ETA were then used as <italic>x</italic> and <italic>y</italic> coordinates, respectively, in the two-dimensional contrast spaces for center and surround. To obtain the fraction of variance explained by the luminance and color axis within the contrast space for center and surround RF components, we performed PCA on the two-dimensional matrix with dimensions <italic>cells × x</italic> − <italic>y</italic>. The relative weights of the resulting PCs were used as a measure of fraction variance explained. A similar method was recently used to quantify chromatic and achromatic contrasts in mouse natural scenes (<xref ref-type="bibr" rid="c22">Qiu et al., 2021</xref>).</p>
</sec>
<sec id="s4i">
<title>Decoding analysis</title>
<p>We used a support vector machine classifier with a radial basis function kernel to estimate decoding accuracy between the neuronal representations of two stimulus classes - either On or Off (stimulus luminance) and UV or green (stimulus color). We used varying numbers of neurons for decoding and built separate decoders for stimulus luminance and stimulus color. Specifically, we split the data into 10 equally sized trial blocks, trained the decoder on 90% of the data, tested its accuracy on the remaining 10% of the data and computed the mean accuracy across n=10 different training/test trial splits. Finally, we converted the decoding accuracy into discriminability, the mutual information between the true class and its estimate using
<disp-formula id="ueqn2">
<graphic xlink:href="543054v2_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>p</italic><sub><italic>ij</italic></sub> is the probability of observing the true class <italic>i</italic> and predicted class <italic>j</italic> and <italic>p</italic><sub><italic>i</italic>:</sub> and <italic>p</italic><sub>:<italic>j</italic></sub> denote the respective marginal probabilities.</p>
</sec>
<sec id="s4j">
<title>Retinal data</title>
<p>We used an available dataset from (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>) to test how color is represented in the luminance and color contrast space at the level of the retinal output. This dataset consisted of UV and green center and surround ETAs of n=3,215 retinal ganglion cells (n=88 recording fields, n=18 mice), obtained from responses to a center (10 degrees visual angle) and surround (30 × 30 degrees visual angle without the center) luminance and color noise stimulus. We estimated the ETAs and embedded each neuron in the sensitivity space as described above.</p>
</sec>
<sec id="s4k">
<title>Functional clustering using Gaussian Mixture Model</title>
<p>For clustering of center and surround ETAs into distinct response types, we used a Gaussian Mixture model (GMM; <italic>sklearn</italic>.<italic>mixture</italic>.<italic>GaussianMixture</italic> package). We used the weights of the principal components extracted from the ETAs as input to the GMM (cf. <xref rid="figS2" ref-type="fig">Suppl. Fig. 2</xref>). To test how many GMM components (i.e. response types) best explain the data, we built GMMs with varying numbers of components and cross-validated the models’ log likelihood on 10% of left out test data, using 10 different test/train trial splits (<xref rid="figS4" ref-type="fig">Suppl. Fig. 4a</xref>). We picked the model with <italic>n</italic> = 17 components for further analysis because this resulted in the highest log likelihood. However, please note that the models’ performance was relatively ETAble across a wide range of components. To test the assignment accuracy of the final model, we used the mean and covariance matrix of each GMM component to generate data with ground-truth labels and compared those to the GMM-predicted labels (<xref rid="figS4" ref-type="fig">Suppl. Fig. 4b</xref>), as described previously (<xref ref-type="bibr" rid="c68">Tolias et al., 2007</xref>). Assignment accuracy ranged between 75% and 98%, with a mean <italic>±</italic> std. of 89% <italic>±</italic> 6%. Most response types were evenly distributed across mice and all response types were present in all mice (<xref rid="figS4" ref-type="fig">Suppl. Fig. 4c</xref>), suggesting that clustering was not predominantly driven by inter-experimental variations.</p>
</sec>
<sec id="s4l">
<title>Cortical distribution index</title>
<p>For estimating the distribution of response types across cortical position, we used the cortical distribution index. It was estimated as Michel-son contrast ranging from -1 and 1 for a response type solely present in posterior and anterior V1, respectively. We define the distribution index as
<disp-formula id="ueqn3">
<graphic xlink:href="543054v2_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>n</italic><sub><italic>anterior</italic></sub> and <italic>n</italic><sub><italic>posterior</italic></sub> correspond to the fraction of neurons in anterior and posterior V1 assigned to a specific response type.</p>
</sec>
<sec id="s4m">
<title>Decoding of noise and object scenes</title>
<p>For decoding noise versus object scenes based on simulated responses, we used natural scene inspired parametric stimuli. Specifically, we generated images with independent Perlin noise (<xref ref-type="bibr" rid="c69">Perlin, 1985</xref>) in each color channel using the perlin-noise package for Python. Then, for the object images, we added a dark ellipse of varying size, position, and angle to the UV color channels. We adjusted the contrast of all images with a dark object to match the contrast of noise images, such that the distribution of image contrasts did not differ between noise and object images. We then simulated responses to 1,000 object and noise scenes that were used by an SVM decoder to decode stimulus class (object or noise) as described above. For simulating responses, we modeled each response type to have a square RF with 10 degrees visual angle in diameter, with the luminance and color contrast sensitivity of the response type’s RF center. Then, we created response maps by convolving the simulated RFs with the scenes and summed up all positive values to result in one response value per scene and response type.</p>
</sec>
<sec id="s4n">
<title>Statistical analysis</title>
<p>We used the T-test for two independent samples to test whether the decoding performance of 10 test/train trial splits differ between (i) center and surround, (i) photopic and mesopic light levels, (iii) anterior and posterior V1, and (iv) anterior and posterior response types. For all these tests, the p-value was adjusted for multiple comparisons using the Bonferroni correction.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Data and code availability</title>
<p>The analysis code and all data will be publicly available in an online repository latest upon journal publication. Please contact us if you would like access before that time.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Thomas Euler and Tom Baden for feedback on the manuscript. This work was supported by the Hertie Foundation (to PB), the German Research Foundation (DFG CRC 1233 “Robust Vision” to PB and KF, DFG Excellence Cluster 2064 “Machine Learning - New Perspectives for Science” to PB), the European Research Council (ERC) under the European Union’s Horizon Europe research and innovation programme (“NextMechMod” grant agreement No. 101039115 to PB), and the National Institute of Health (UF1 NS126566 and RF1 MH126883 to AST).</p>
</ack>
<sec id="s6">
<title>Author contributions</title>
<p><bold>KF</bold>: Conceptualization, Methodology, Validation, Software, Formal Analysis, Investigation, Data Acquisition, Writing - Original Draft, Visualization, Supervision, Funding Acquisition; <bold>CC</bold>: Formal Analysis, Validation, Writing - Review &amp; Editing; <bold>KP</bold>: Data Acquisition, Investigation; <bold>JF</bold>: Methodology, Investigation, Writing - Review &amp; Editing; <bold>SS</bold>: Formal Analysis, Methodology, Writing - Review &amp; Editing; <bold>PB</bold>: Methodology, Funding Acquisition, Supervision, Writing - Review &amp; Editing; <bold>AST</bold>: Conceptualization, Investigation, Writing - Review &amp; Editing, Supervision, Funding Acquisition.</p>
</sec>
<ref-list>
<title>Reference</title>
<ref id="c1"><mixed-citation publication-type="other"><string-name><given-names>T.</given-names> <surname>Baden</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Osorio</surname></string-name>. <article-title>The retinal basis of vertebrate color vision</article-title>. <source>Annu Rev Vis Sci</source>, <month>June</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><given-names>M. S.</given-names> <surname>Livingstone</surname></string-name> and <string-name><given-names>D. H.</given-names> <surname>Hubel</surname></string-name>. <article-title>Anatomy and physiology of a color system in the primate visual cortex</article-title>. <source>J. Neurosci</source>., <volume>4</volume>(<issue>1</issue>):<fpage>309</fpage>–<lpage>356</lpage>, <month>January</month> <year>1984</year>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><given-names>T. N.</given-names> <surname>Wiesel</surname></string-name> and <string-name><given-names>D. H.</given-names> <surname>Hubel</surname></string-name>. <article-title>Spatial and chromatic interactions in the lateral geniculate body of the rhesus monkey</article-title>. <source>J. Neurophysiol</source>., <volume>29</volume>(<issue>6</issue>):<fpage>1115</fpage>–<lpage>1156</lpage>, <month>November</month> <year>1966</year>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><given-names>K. R.</given-names> <surname>Gegenfurtner</surname></string-name>, <string-name><given-names>D. C.</given-names> <surname>Kiper</surname></string-name>, and <string-name><given-names>S. B.</given-names> <surname>Fenstemaker</surname></string-name>. <article-title>Processing of color, form, and motion in macaque area V2</article-title>. <source>Vis. Neurosci</source>., <volume>13</volume>(<issue>1</issue>):<fpage>161</fpage>–<lpage>172</lpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Tanigawa</surname></string-name>, <string-name><given-names>H. D.</given-names> <surname>Lu</surname></string-name>, and <string-name><given-names>A. W.</given-names> <surname>Roe</surname></string-name>. <article-title>Functional organization for color and orientation in macaque V4</article-title>. <source>Nat. Neurosci</source>., <volume>13</volume>(<issue>12</issue>):<fpage>1542</fpage>–<lpage>1548</lpage>, <month>December</month> <year>2010</year>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Chatterjee</surname></string-name> and <string-name><given-names>E. M.</given-names> <surname>Callaway</surname></string-name>. <article-title>Parallel colour-opponent pathways to primary visual cortex</article-title>. <source>Nature</source>, <volume>426</volume>(<issue>6967</issue>):<fpage>668</fpage>–<lpage>671</lpage>, <month>December</month> <year>2003</year>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Szél</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Röhlich</surname></string-name>, <string-name><given-names>A. R.</given-names> <surname>Gaffé</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Juliusson</surname></string-name>, <string-name><given-names>G. v.</given-names> <surname>Aguirre</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Van Veen</surname></string-name>. <article-title>Unique topographic separation of two spectral classes of cones in the mouse retina</article-title>. <source>J. Comp. Neurol</source>., <volume>325</volume>(<issue>3</issue>): <fpage>327</fpage>–<lpage>342</lpage>, <year>1992</year>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Baden</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Schubert</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Wei</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Zaichuk</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Wissinger</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Euler</surname></string-name>. <article-title>A tale of two retinal domains: near-optimal sampling of achromatic contrasts in natural scenes through asymmetric photoreceptor distribution</article-title>. <source>Neuron</source>, <volume>80</volume>(<issue>5</issue>):<fpage>1206</fpage>–<lpage>1217</lpage>, <month>December</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><given-names>G. H.</given-names> <surname>Jacobs</surname></string-name>, <string-name><given-names>G. A.</given-names> <surname>Williams</surname></string-name>, and <string-name><given-names>J. A.</given-names> <surname>Fenwick</surname></string-name>. <article-title>Influence of cone pigment coexpression on spectral sensitivity and color vision in the mouse</article-title>. <source>Vision Res</source>., <volume>44</volume>(<issue>14</issue>):<fpage>1615</fpage>–<lpage>1622</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><given-names>D. J.</given-names> <surname>Denman</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Luviano</surname></string-name>, <string-name><given-names>D. R.</given-names> <surname>Ollerenshaw</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Cross</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Williams</surname></string-name>, <string-name><given-names>M. A.</given-names> <surname>Buice</surname></string-name>, <string-name><given-names>S. R.</given-names> <surname>Olsen</surname></string-name>, and <string-name><given-names>R. C.</given-names> <surname>Reid</surname></string-name>. <article-title>Mouse color and wavelength-specific luminance contrast sensitivity are non-uniform across visual space</article-title>. <source>Elife</source>, <volume>7</volume>, <month>January</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><given-names>M. E.</given-names> <surname>Stabio</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Sabbah</surname></string-name>, <string-name><given-names>L. E.</given-names> <surname>Quattrochi</surname></string-name>, <string-name><given-names>M. C.</given-names> <surname>Ilardi</surname></string-name>, <string-name><given-names>P. M.</given-names> <surname>Fogerson</surname></string-name>, <string-name><given-names>M. L.</given-names> <surname>Leyrer</surname></string-name>, <string-name><given-names>M. T.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Schiel</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Renna</surname></string-name>, <string-name><given-names>K. L.</given-names> <surname>Briggman</surname></string-name>, and <string-name><given-names>D. M.</given-names> <surname>Berson</surname></string-name>. <article-title>The M5 cell: A Color-Opponent intrinsically photosensitive retinal ganglion cell</article-title>. <source>Neuron</source>, <volume>97</volume>(<issue>1</issue>):<fpage>150</fpage>–<lpage>163</lpage>.e4, <month>Jauary</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><given-names>F. M.</given-names> <surname>Nadal-Nicolás</surname></string-name>, <string-name><given-names>V. P.</given-names> <surname>Kunze</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Ball</surname></string-name>, <string-name><given-names>B. T.</given-names> <surname>Peng</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Krishnan</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Dong</surname></string-name>, and <string-name><given-names>W.</given-names> <surname>Li</surname></string-name>. <article-title>True s-cones are concentrated in the ventral mouse retina and wired for color detection in the upper visual field</article-title>. <source>Elife</source>, <volume>9</volume>:<fpage>e56840</fpage>, <month>May</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Haverkamp</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Wässle</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Duebel</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Kuner</surname></string-name>, <string-name><given-names>G. J.</given-names> <surname>Augustine</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Feng</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Euler</surname></string-name>. <article-title>The primordial, blue-cone color system of the mouse retina</article-title>.<source>J. Neurosci</source>., <volume>25</volume>(<issue>22</issue>):<fpage>5438</fpage>–<lpage>5445</lpage>, <month>June</month> <year>2005</year>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Breuninger</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Euler</surname></string-name>. <article-title>Chromatic coding from cone-type unselective circuits in the mouse retina</article-title>. <source>Neuron</source>, <volume>77</volume>(<issue>3</issue>):<fpage>559</fpage>–<lpage>571</lpage>, <month>February</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Joesch</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Meister</surname></string-name>. <article-title>A neuronal circuit for colour vision based on rod-cone opponency</article-title>. <source>Nature</source>, <volume>532</volume>(<issue>7598</issue>):<fpage>236</fpage>–<lpage>239</lpage>, <month>April</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><given-names>K. P.</given-names> <surname>Szatko</surname></string-name>, <string-name><given-names>M. M.</given-names> <surname>Korympidou</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Ran</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Berens</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Dalkara</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Schubert</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Euler</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Franke</surname></string-name>. <article-title>Neural circuits in the mouse retina support color vision in the upper visual field</article-title>. <source>Nat. Commun</source>., <volume>11</volume>(<issue>1</issue>):<fpage>3481</fpage>, <month>July</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><given-names>M. H.</given-names> <surname>Khani</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Gollisch</surname></string-name>. <article-title>Linear and nonlinear chromatic integration in the mouse retina</article-title>. <source>Nat. Commun</source>., <volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>21</lpage>, <month>March</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><given-names>D. J.</given-names> <surname>Denman</surname></string-name>, <string-name><given-names>J. H.</given-names> <surname>Siegle</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Koch</surname></string-name>, <string-name><given-names>R. C.</given-names> <surname>Reid</surname></string-name>, and <string-name><given-names>T. J.</given-names> <surname>Blanche</surname></string-name>. <article-title>Spatial organization of chromatic pathways in the mouse dorsal lateral geniculate nucleus</article-title>. <source>J. Neurosci</source>., <volume>37</volume>(<issue>5</issue>):<fpage>1102</fpage>– <lpage>1116</lpage>, <month>February</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><given-names>Z.</given-names> <surname>Tan</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>T.-W.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Kim</surname></string-name>, and <string-name><given-names>N.</given-names> <surname>Ji</surname></string-name>. <article-title>Neuronal representation of ultraviolet visual stimuli in mouse primary visual cortex</article-title>. <source>Scientific Reports</source>, <volume>5</volume>(<issue>1</issue>), <month>July</month> <year>2015</year>. doi: <pub-id pub-id-type="doi">10.1038/srep12597</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><given-names>J. W.</given-names> <surname>Mouland</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Pienaar</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Williams</surname></string-name>, <string-name><given-names>A. J.</given-names> <surname>Watson</surname></string-name>, <string-name><given-names>R. J.</given-names> <surname>Lucas</surname></string-name>, and <string-name><given-names>T. M.</given-names> <surname>Brown</surname></string-name>. <article-title>Extensive cone-dependent spectral opponency within a discrete zone of the lateral geniculate nucleus supporting mouse color vision</article-title>. <source>Curr. Biol</source>., <volume>31</volume>(<issue>15</issue>):<fpage>3391</fpage>–<lpage>3400</lpage>.e4, <month>August</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="other"><string-name><given-names>I.</given-names> <surname>Rhim</surname></string-name> and <string-name><given-names>I.</given-names> <surname>Nauhaus</surname></string-name>. <article-title>Joint representations of color and form in mouse visual cortex described by random pooling from rods and cones</article-title>. <source>J. Neurophysiol</source>., <month>January</month> <year>2023</year>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="other"><string-name><given-names>Y.</given-names> <surname>Qiu</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Klindt</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Kautzky</surname></string-name>, <string-name><given-names>K. P.</given-names> <surname>Szatko</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Schaeffel</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Rifai</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Franke</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Busse</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Euler</surname></string-name>. <article-title>Natural environment statistics in the upper and lower visual field are reflected in mouse retinal specializations</article-title>. <source>Curr. Biol</source>., <month>June</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Abballe</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Asari</surname></string-name>. <article-title>Natural image statistics for mouse vision</article-title>. <source>PLoS One</source>, <volume>17</volume>(<issue>1</issue>):<fpage>e0262763</fpage>, <month>January</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><given-names>M. J. Y.</given-names> <surname>Zimmermann</surname></string-name>, <string-name><given-names>N. E.</given-names> <surname>Nevala</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Yoshimatsu</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Osorio</surname></string-name>, <string-name><given-names>D.-E.</given-names> <surname>Nilsson</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Berens</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Baden</surname></string-name>. <article-title>Zebrafish differentially process color across visual space to match natural scenes</article-title>. <source>Curr. Biol</source>., <volume>28</volume>(<issue>13</issue>):<fpage>2018</fpage>–<lpage>2032</lpage>.e5, <month>July</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><given-names>K.</given-names> <surname>Franke</surname></string-name>, <string-name><given-names>A. Maia</given-names> <surname>Chagas</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Zimmermann</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Bartel</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Qiu</surname></string-name>, <string-name><given-names>K. P.</given-names> <surname>Szatko</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Baden</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Euler</surname></string-name>. <article-title>An arbitrary-spectrum spatial visual stimulator for vision research</article-title>. <source>Elife</source>, <volume>8</volume>, <month>September</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="other"><string-name><given-names>K.</given-names> <surname>Franke</surname></string-name>, <string-name><given-names>K. F.</given-names> <surname>Willeke</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Ponder</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Galdamez</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Muhammad</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Patel</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Froudarakis</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Reimer</surname></string-name>, <string-name><given-names>F. H.</given-names> <surname>Sinz</surname></string-name>, and <string-name><given-names>A. S.</given-names> <surname>Tolias</surname></string-name>. <article-title>State-dependent pupil dilation rapidly shifts visual feature selectivity</article-title>. <source>Nature</source>, <month>September</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Schuett</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Bonhoeffer</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Hübener</surname></string-name>. <article-title>Mapping retinotopic structure in mouse visual cortex with optical imaging</article-title>. <source>J. Neurosci</source>., <volume>22</volume>(<issue>15</issue>):<fpage>6549</fpage>–<lpage>6559</lpage>, <month>August</month> <year>2002</year>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><given-names>J. P.</given-names> <surname>Jones</surname></string-name> and <string-name><given-names>L. A.</given-names> <surname>Palmer</surname></string-name>. <article-title>The two-dimensional spatial structure of simple receptive fields in cat striate cortex</article-title>. <source>J. Neurophysiol</source>., <volume>58</volume>(<issue>6</issue>):<fpage>1187</fpage>–<lpage>1211</lpage>, <month>December</month> <year>1987</year>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Pachitariu</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Stringer</surname></string-name>, and <string-name><given-names>K. D.</given-names> <surname>Harris</surname></string-name>. <article-title>Robustness of spike deconvolution for neuronal calcium imaging</article-title>. <source>J. Neurosci</source>., <volume>38</volume>(<issue>37</issue>):<fpage>7976</fpage>–<lpage>7985</lpage>, <month>September</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><given-names>K.</given-names> <surname>Franke</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Berens</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Schubert</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bethge</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Euler</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Baden</surname></string-name>. <article-title>Inhibition decorrelates visual feature representations in the inner retina</article-title>. <source>Nature</source>, <volume>542</volume>(<issue>7642</issue>):<fpage>439</fpage>–<lpage>444</lpage>, <month>February</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><given-names>M. S.</given-names> <surname>Grubb</surname></string-name> and <string-name><given-names>I. D.</given-names> <surname>Thompson</surname></string-name>. <article-title>Quantitative characterization of visual response properties in the mouse dorsal lateral geniculate nucleus</article-title>. <source>J. Neurophysiol</source>., <volume>90</volume>(<issue>6</issue>):<fpage>3594</fpage>–<lpage>3607</lpage>, <month>December</month> <year>2003</year>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><given-names>I.</given-names> <surname>Rhim</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Coello-Reyes</surname></string-name>, <string-name><given-names>H.-K.</given-names> <surname>Ko</surname></string-name>, and <string-name><given-names>I.</given-names> <surname>Nauhaus</surname></string-name>. <article-title>Maps of cone opsin input to mouse V1 and higher visual areas</article-title>. <source>J. Neurophysiol</source>., <volume>117</volume>(<issue>4</issue>):<fpage>1674</fpage>–<lpage>1682</lpage>, <month>April</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Aihara</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Yoshida</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Hashimoto</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Ohki</surname></string-name>. <article-title>Color representation is retinotopically biased but locally intermingled in mouse V1</article-title>. <source>Front. Neural Circuits</source>, <volume>11</volume>:<issue>22</issue>, <month>March</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="other"><string-name><given-names>L.</given-names> <surname>Höfling</surname></string-name>, <string-name><given-names>K. P.</given-names> <surname>Szatko</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Behrens</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Qiu</surname></string-name>, <string-name><given-names>D. A.</given-names> <surname>Klindt</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Jessen</surname></string-name>, <string-name><given-names>G. W.</given-names> <surname>Schwartz</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bethge</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Berens</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Franke</surname></string-name>, <string-name><given-names>A. S.</given-names> <surname>Ecker</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Euler</surname></string-name>. <source>A chromatic feature detector in the retina signals visual context changes</source>. <month>December</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><given-names>D.-E.</given-names> <surname>Nilsson</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Smolka</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Bok</surname></string-name>. <article-title>The vertical light-gradient and its potential impact on animal distribution and behavior</article-title>. <source>Frontiers in Ecology and Evolution</source>, <volume>10</volume>, <year>2022</year>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Baden</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Euler</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Berens</surname></string-name>. <article-title>Understanding the retinal basis of vision across species</article-title>. <source>Nat. Rev. Neurosci</source>., <volume>21</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>20</lpage>, <month>January</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>I.-J.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>J. R.</given-names> <surname>Sanes</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Meister</surname></string-name>. <article-title>The most numerous ganglion cell type of the mouse retina is a selective feature detector</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>., <volume>109</volume>(<issue>36</issue>):<fpage>E2391</fpage>–<lpage>8</lpage>, <month>September</month> <year>2012</year>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><given-names>V. V.</given-names> <surname>Maximov</surname></string-name>. <source>Environmental factors which may have led to the appearance of colour vision. Philos. Trans. R. Soc. Lond. B Biol. Sci</source>., <volume>355</volume>(<issue>1401</issue>):<fpage>1239</fpage>–<lpage>1242</lpage>, <month>September</month> <year>2000</year>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Kelber</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Vorobyev</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Osorio</surname></string-name>. <article-title>Animal colour vision–behavioural tests and physiological concepts</article-title>. <source>Biol. Rev. Camb. Philos. Soc</source>., <volume>78</volume>(<issue>1</issue>):<fpage>81</fpage>–<lpage>118</lpage>, <month>February</month> <year>2003</year>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><given-names>T. W.</given-names> <surname>Cronin</surname></string-name> and <string-name><given-names>M. J.</given-names> <surname>Bok</surname></string-name>. <article-title>Photoreception and vision in the ultraviolet</article-title>. <source>J. Exp. Biol</source>., <volume>219</volume>(<issue>Pt 18</issue>): <fpage>2790</fpage>–<lpage>2801</lpage>, <month>September</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><given-names>G. S.</given-names> <surname>Losey</surname></string-name>, <string-name><given-names>T. W.</given-names> <surname>Cronin</surname></string-name>, <string-name><given-names>T. H.</given-names> <surname>Goldsmith</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Hyde</surname></string-name>, <string-name><given-names>N. J.</given-names> <surname>Marshall</surname></string-name>, and <string-name><given-names>W. N.</given-names> <surname>McFarland</surname></string-name>. <article-title>The UV visual world of fishes: a review</article-title>. <source>J. Fish Biol</source>., <volume>54</volume>(<issue>5</issue>):<fpage>921</fpage>–<lpage>943</lpage>, <month>May</month> <year>1999</year>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><given-names>N. J. C.</given-names> <surname>Tyler</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Jeffery</surname></string-name>, <string-name><given-names>C. R.</given-names> <surname>Hogg</surname></string-name>, and <string-name><given-names>K. A.</given-names> <surname>Stokkan</surname></string-name>. <article-title>Ultraviolet vision may enhance the ability of reindeer to discriminate plants in snow</article-title>. <source>Arctic</source>, <volume>67</volume>(<issue>2</issue>):<fpage>159</fpage>–<lpage>166</lpage>, <month>May</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Reitner</surname></string-name>, <string-name><given-names>L. T.</given-names> <surname>Sharpe</surname></string-name>, and <string-name><given-names>E.</given-names> <surname>Zrenner</surname></string-name>. <article-title>Is colour vision possible with only rods and blue-sensitive cones?</article-title> <source>Nature</source>, <volume>352</volume>(<issue>6338</issue>):<fpage>798</fpage>–<lpage>800</lpage>, <month>August</month> <year>1991</year>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><given-names>O.</given-names> <surname>Schwartz</surname></string-name>, <string-name><given-names>J. W.</given-names> <surname>Pillow</surname></string-name>, <string-name><given-names>N. C.</given-names> <surname>Rust</surname></string-name>, and <string-name><given-names>E. P.</given-names> <surname>Simoncelli</surname></string-name>. <article-title>Spike-triggered neural characterization</article-title>. <source>J. Vis</source>., <volume>6</volume>(<issue>4</issue>):<fpage>484</fpage>–<lpage>507</lpage>, <month>July</month> <year>2006</year>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Peichl</surname></string-name>. <article-title>Diversity of mammalian photoreceptor properties: adaptations to habitat and lifestyle?</article-title> <source>Anat. Rec. A Discov. Mol. Cell. Evol. Biol</source>., <volume>287</volume>(<issue>1</issue>):<fpage>1001</fpage>–<lpage>1012</lpage>, <month>November</month> <year>2005</year>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Yoshimatsu</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Schröder</surname></string-name>, <string-name><given-names>N. E.</given-names> <surname>Nevala</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Berens</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Baden</surname></string-name>. <article-title>Fovea-like photoreceptor specializations underlie single UV cone driven Prey-Capture behavior in zebrafish</article-title>. <source>Neuron</source>, <volume>107</volume>(<issue>2</issue>):<fpage>320</fpage>–<lpage>337</lpage>.e6, <month>July</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><given-names>W. B.</given-names> <surname>Thoreson</surname></string-name> and <string-name><given-names>D. M.</given-names> <surname>Dacey</surname></string-name>. <article-title>Diverse cell types, circuits, and mechanisms for color vision in the vertebrate retina</article-title>. <source>Physiol. Rev</source>., <volume>99</volume>(<issue>3</issue>):<fpage>1527</fpage>–<lpage>1573</lpage>, <month>July</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Livingstone</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Hubel</surname></string-name>. <article-title>Segregation of form, color, movement, and depth: anatomy, physiology, and perception</article-title>. <source>Science</source>, <volume>240</volume>(<issue>4853</issue>):<fpage>740</fpage>–<lpage>749</lpage>, <month>May</month> <year>1988</year>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><given-names>S. M.</given-names> <surname>Zeki</surname></string-name>. <article-title>Functional specialisation in the visual cortex of the rhesus monkey</article-title>. <source>Nature</source>, <volume>274</volume>(<issue>5670</issue>): <fpage>423</fpage>–<lpage>428</lpage>, <month>August</month> <year>1978</year>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><given-names>A. K.</given-names> <surname>Garg</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>M. S.</given-names> <surname>Rashid</surname></string-name>, and <string-name><given-names>E. M.</given-names> <surname>Callaway</surname></string-name>. <article-title>Color and orientation are jointly coded and spatially organized in primate primary visual cortex</article-title>. <source>Science</source>, <volume>364</volume>(<issue>6447</issue>):<fpage>1275</fpage>–<lpage>1279</lpage>, <month>June</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><given-names>D. H.</given-names> <surname>Hubel</surname></string-name> and <string-name><given-names>M. S.</given-names> <surname>Livingstone</surname></string-name>. <article-title>Segregation of form, color, and stereopsis in primate area 18</article-title>. <source>J. Neurosci</source>., <volume>7</volume>(<issue>11</issue>):<fpage>3378</fpage>–<lpage>3415</lpage>, <month>November</month> <year>1987</year>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><given-names>E. A.</given-names> <surname>DeYoe</surname></string-name> and <string-name><given-names>D. C.</given-names> <surname>Van Essen</surname></string-name>. <article-title>Segregation of efferent connections and receptive field properties in visual area V2 of the macaque</article-title>. <source>Nature</source>, <volume>317</volume>(<issue>6032</issue>):<fpage>58</fpage>–<lpage>61</lpage>, <year>1985</year>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>Seifert</surname></string-name>, <string-name><given-names>P. A.</given-names> <surname>Roberts</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Kafetzis</surname></string-name>, <string-name><given-names>D. A.</given-names> <surname>Osorio</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Baden</surname></string-name>. <source>Birds multiplex spectral and temporal visual information via retinal on– and off–channels</source>. <month>October</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Bear</surname></string-name>, <string-name><given-names>P. A.</given-names> <surname>Roberts</surname></string-name>, <string-name><given-names>F. K.</given-names> <surname>Janiak</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Semmelhack</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Yoshimatsu</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Baden</surname></string-name>. <article-title>Zebrafish retinal ganglion cells asymmetrically encode spectral and temporal information across visual space</article-title>. <source>Curr. Biol</source>., <volume>30</volume>(<issue>15</issue>):<fpage>2927</fpage>–<lpage>2942</lpage>.e7, <month>August</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><given-names>D. A. Guggiana</given-names> <surname>Nilo</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Riegler</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hübener</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>Engert</surname></string-name>. <article-title>Distributed chromatic processing at the interface between retina and brain in the larval zebrafish</article-title>. <source>Curr. Biol</source>., <volume>31</volume>(<issue>9</issue>):<fpage>1945</fpage>–<lpage>1953</lpage>.e5, <month>May</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="other"><string-name><given-names>K. D.</given-names> <surname>Longden</surname></string-name>, <string-name><given-names>E. M.</given-names> <surname>Rogers</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Nern</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Dionne</surname></string-name>, and <string-name><given-names>M. B.</given-names> <surname>Reiser</surname></string-name>. <source>Synergy of color and motion vision for detecting approaching objects in drosophila</source>. <month>November</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><given-names>E. N.</given-names> <surname>Johnson</surname></string-name>, <string-name><given-names>S. D.</given-names> <surname>Van Hooser</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Fitzpatrick</surname></string-name>. <article-title>The representation of s-cone signals in primary visual cortex</article-title>. <source>J. Neurosci</source>., <volume>30</volume>(<issue>31</issue>):<fpage>10337</fpage>–<lpage>10350</lpage>, <month>August</month> <year>2010</year>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Tedore</surname></string-name> and <string-name><given-names>D.-E.</given-names> <surname>Nilsson</surname></string-name>. <article-title>Avian UV vision enhances leaf surface contrasts in forest environments</article-title>. <source>Nat. Commun</source>., <volume>10</volume>(<issue>1</issue>):<fpage>238</fpage>, <month>January</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Reimer</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Froudarakis</surname></string-name>, <string-name><given-names>C. R.</given-names> <surname>Cadwell</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Yatsenko</surname></string-name>, <string-name><given-names>G. H.</given-names> <surname>Denfield</surname></string-name>, and <string-name><given-names>A. S.</given-names> <surname>Tolias</surname></string-name>. <article-title>Pupil fluctuations track fast switching of cortical states during quiet wakefulness</article-title>. <source>Neuron</source>, <volume>84</volume>(<issue>2</issue>): <fpage>355</fpage>–<lpage>362</lpage>, <month>October</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><given-names>E.</given-names> <surname>Froudarakis</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Berens</surname></string-name>, <string-name><given-names>A. S.</given-names> <surname>Ecker</surname></string-name>, <string-name><given-names>R. J.</given-names> <surname>Cotton</surname></string-name>, <string-name><given-names>F. H.</given-names> <surname>Sinz</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Yatsenko</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Saggau</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bethge</surname></string-name>, and <string-name><given-names>A. S.</given-names> <surname>Tolias</surname></string-name>. <article-title>Population code in mouse V1 facilitates readout of natural scenes through increased sparseness</article-title>. <source>Nat. Neurosci</source>., <volume>17</volume>(<issue>6</issue>):<fpage>851</fpage>–<lpage>857</lpage>, <month>June</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Mathis</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Mamidanna</surname></string-name>, <string-name><given-names>K. M.</given-names> <surname>Cury</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Abe</surname></string-name>, <string-name><given-names>V. N.</given-names> <surname>Murthy</surname></string-name>, <string-name><given-names>M. W.</given-names> <surname>Mathis</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Bethge</surname></string-name>. <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat. Neurosci</source>., <volume>21</volume>(<issue>9</issue>):<fpage>1281</fpage>–<lpage>1289</lpage>, <month>August</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><given-names>M. E.</given-names> <surname>Garrett</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Nauhaus</surname></string-name>, <string-name><given-names>J. H.</given-names> <surname>Marshel</surname></string-name>, and <string-name><given-names>E. M.</given-names> <surname>Callaway</surname></string-name>. <article-title>Topography and areal organization of mouse visual cortex</article-title>. <source>J. Neurosci</source>., <volume>34</volume>(<issue>37</issue>):<fpage>12587</fpage>–<lpage>12600</lpage>, <month>September</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><given-names>N. J.</given-names> <surname>Sofroniew</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Flickinger</surname></string-name>, <string-name><given-names>J.</given-names> <surname>King</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Svoboda</surname></string-name>. <article-title>A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging</article-title>. <source>Elife</source>, <volume>5</volume>:<fpage>e14472</fpage>, <month>June</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><given-names>E. A.</given-names> <surname>Pnevmatikakis</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Soudry</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Gao</surname></string-name>, <string-name><given-names>T. A.</given-names> <surname>Machado</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Merel</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Pfau</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Reardon</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Mu</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Lacefield</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Ahrens</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Bruno</surname></string-name>, <string-name><given-names>T. M.</given-names> <surname>Jessell</surname></string-name>, <string-name><given-names>D. S.</given-names> <surname>Peterka</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Yuste</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>. <article-title>Simultaneous denoising, deconvolution, and demixing of calcium imaging data</article-title>. <source>Neuron</source>, <volume>89</volume>(<issue>2</issue>):<fpage>285</fpage>–<lpage>299</lpage>, <month>January</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><given-names>I.</given-names> <surname>Rhim</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Coello-Reyes</surname></string-name>, and <string-name><given-names>I.</given-names> <surname>Nauhaus</surname></string-name>. <article-title>Variations in photoreceptor throughput to mouse visual cortex and the unique effects on tuning</article-title>. <source>Sci. Rep</source>., <volume>11</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>21</lpage>, <month>June</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><given-names>J. T.</given-names> <surname>Henriksson</surname></string-name>, <string-name><given-names>J. P. G.</given-names> <surname>Bergmanson</surname></string-name>, and <string-name><given-names>J. E.</given-names> <surname>Walsh</surname></string-name>. <article-title>Ultraviolet radiation transmittance of the mouse eye and its individual media components</article-title>. <source>Exp. Eye Res</source>., <volume>90</volume>(<issue>3</issue>):<fpage>382</fpage>–<lpage>387</lpage>, <month>March</month> <year>2010</year>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Schmucker</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Schaeffel</surname></string-name>. <article-title>A paraxial schematic eye model for the growing C57BL/6 mouse</article-title>. <source>Vision Res</source>., <volume>44</volume>(<issue>16</issue>):<fpage>1857</fpage>–<lpage>1867</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><given-names>A. S.</given-names> <surname>Tolias</surname></string-name>, <string-name><given-names>A. S.</given-names> <surname>Ecker</surname></string-name>, <string-name><given-names>A. G.</given-names> <surname>Siapas</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Hoenselaar</surname></string-name>, <string-name><given-names>G. A.</given-names> <surname>Keliris</surname></string-name>, and <string-name><given-names>N. K.</given-names> <surname>Logothetis</surname></string-name>. <article-title>Recording chronically from the same neurons in awake, behaving primates</article-title>. <source>J. Neurophysiol</source>., <volume>98</volume>(<issue>6</issue>): <fpage>3780</fpage>–<lpage>3790</lpage>, <month>December</month> <year>2007</year>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><given-names>K.</given-names> <surname>Perlin</surname></string-name>. <article-title>An image synthesizer</article-title>. <source>SIGGRAPH Comput. Graph</source>., <volume>19</volume>(<issue>3</issue>):<fpage>287</fpage>–<lpage>296</lpage>, <month>July</month> <year>1985</year>.</mixed-citation></ref>
</ref-list>
<sec id="s7">
<title>Supplementary Information</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplemental Fig. 1.</label>
<caption><title>Verification of stimulus paradigm. 1,</title>
<p>Peak positions of spatial spike-triggered averages (ETA) estimated in response to a sparse noise stimulus (n=1,434 cells, n=5 recording field, n=3 mice) relative to area of center spot of color noise stimulus (black). The peak position of the vast majority of cells lies within the center spot area. <bold>b</bold>, Peak-normalized spatial ETA of four example neurons, capturing the center receptive field (RF) of the neurons. White solid line shows spatial ETA border (contour drawn at level=0.25) and gray dot corresponds to peak of spatial ETA (see (a)). White dotted line indicates area of center spot of color noise stimulus shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. Overlap values depict the overlap of the spatial ETA with the center spot area, ranging from 1 (spatial ETA lies within the center spot area) to 0 (spatial ETA outside center spot area). Bottom shows distribution of spatial ETA overlap values (n=1,434 cells, n=5 recording field, n=3 mice). For most cells (83%), the spatial ETA exhibited an overlap with the center spot area of the color noise stimulus of more than 0.65. <bold>c</bold>, PCA-reconstructed ETAs of all neurons above quality threshold (n=3,331 cells, n=6 recording fields, n=3 mice). <bold>d</bold>, Distribution of Pearson correlation coefficients of center and surround ETAs, estimated by correlating center and surround ETAs for the UV (blue) and green stimulus condition, respectively. <bold>e</bold>, Neurons recorded in a posterior and anterior recording field of an example mouse, color-coded based on the cells’ color preference for center (left) and surround ETA (right), quantified as spectral contrast. <bold>f</bold>, Distribution of center (left) and surround ETA (right) spectral contrast values for posterior (black; n=1,616 cells) and anterior (gray; n=1,695 cells) neurons from n=3 mice. Spectral contrast significantly differed between posterior and anterior neurons, for both center (p&lt;0.001, two-sided two-sample t-test) and surround (p&lt;0.001, two-sided two-sample t-test). Spectral contrast significantly differed between center and surround, for both posterior (p&lt;0.001, two-sided two-sample t-test) and anterior neurons (p&lt;0.001, two-sided two-sample t-test).</p></caption>
<graphic xlink:href="543054v2_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplemental Fig. 2.</label>
<caption><title>Event-triggered averages and quality control.</title>
<p><bold>a</bold>, Raw calcium trace (black) and trace deconvolved with the calcium sensor’s decay function (red) for one example neuron. <bold>b</bold>, Even-triggered averages (ETAs) estimated based on raw (left) and deconvolved calcium traces (right), respectively, for one example neuron. <bold>c</bold>, Same as (c) but for another example neuron. <bold>d</bold>, Histogram of quality measure for green center ETA versus UV center ETA. The quality measure indicates the ratio of ETA to baseline variance (the higher the better quality). <bold>e</bold>, Fraction of neurons above quality threshold, for varying quality thresholds. All analyses were performed for neurons with a UV or green center ETA quality &gt;10 (red), corresponding to 63% of neurons. The blue indicates a more conservative threshold (&gt;50), corresponding to only the the best 25% of neurons. <bold>f</bold>, Density plot of peak amplitudes of center (left) and surround (right) ETAs across all neurons above conservative threshold (blue in (e)). Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA) on the center or surround data, with percentage of variance explained along the polarity and color axis indicated.</p></caption>
<graphic xlink:href="543054v2_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Supplemental Fig. 3.</label>
<caption><title>Reconstruction of event triggered averages using sparse principal component analysis.</title>
<p><bold>a</bold>, Mean reconstruction error across neurons (s.d. in gray) quantified as mean squared error (mse) between the original ETA and the reconstruction using sparse principal component analysis (PCA) for varying numbers of principal components (PCs). For further analysis, we used sparse PCA with eight PCs, because (i) adding more PCs only slightly decreased reconstruction error and (ii) PCs ETArted to capture noise. <bold>b</bold>, PCs obtained from sparse PCA on the ETAs (cf. <xref rid="figS1" ref-type="fig">Suppl. Fig. 1c</xref>) used for reconstructions. <bold>c</bold>, Distribution of reconstruction mse values for sparse PCA with eight PCs. <bold>d</bold>, Original ETA (gray) and PCA reconstruction (black) for four example neurons with varying mse.</p></caption>
<graphic xlink:href="543054v2_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Supplemental Fig. 4.</label>
<caption><title>Consistency across mice.</title>
<p><bold>a</bold>, Density plots of peak amplitudes of center (top) and surround (bottom) ETAs, separately for mouse 1 (n=2 recording fields, n=1,170 cells), mouse 2 (n=2 recording fields, n=1,102 cells) and mouse 3 (n=2 recording fields, n=1,039 cells). Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA) on the center or surround data, with percentage of variance explained along the polarity and color axis indicated. <bold>b</bold>, Density plots of peak amplitudes of center ETAs for neurons encoding the upper visual field (posterior V1; top) and lower visual field (anterior V1; bottom), respectively, separately for mouse 1 (n=2 recording fields, n=1,170 cells), mouse 2 (n=2 recording fields, n=1,102 cells) and mouse 3 (n=2 recording fields, n=1,039 cells). Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA) on the center or surround data, with percentage of variance explained along the polarity and color axis indicated.</p></caption>
<graphic xlink:href="543054v2_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Supplemental Fig. 5.</label>
<caption><title>Neuronal representation of color in the mouse retina.</title>
<p><bold>a</bold>, Top panel shows schematic of a flat-mounted ex-vivo retina, with distribution of all recording fields (n=88 fields) from an available dataset (<xref ref-type="bibr" rid="c16">Szatko et al., 2020</xref>) that has recorded the responses of ganglion cell layer (GCL) cells in response to center and surround flicker of UV and green LED. The bottom panel shows one example scan field of the GCL with 64 <italic>×</italic> 64 pixels, recorded at 7.8 Hz. The cells indicated with numbers are shown in panel (b). D: Dorsal, T: Temporal. <bold>b</bold>, ETAs of three example neurons, concatenated across the four stimulus conditions (center and surround for UV and green flicker). Gray: Original ETA. Black: Reconstruction using PCA. Similar to V1, there are luminance sensitive neurons (cell 1) and color selective neurons (cells 2 and 3). In contrast to V1, color-opponent neurons were rare. <bold>c</bold>, This panel shows the ETA of neuron 1 in (b), with its peak amplitudes of center and surround indicated in the luminance and contrast sensitivity space. <bold>d</bold>, Density plot of peak amplitudes of center (top) and surround (bottom) ETAs across all retinal ganglion cell (RGCs) (n=3,215 cells, n=88 recording fields, n=18 mice). Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA) on the center or surround data, with percentage of variance explained along the polarity and color axis indicated.</p></caption>
<graphic xlink:href="543054v2_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Supplemental Fig. 6.</label>
<caption><title>Unsupervised clustering of spike-triggered averages.</title>
<p><bold>a</bold>, Log likelihood of Gaussian Mixture models (GMMs) with varying numbers of clusters. Model input corresponded to the weights of the principal components used for reconstructing ETAs (cd. <xref rid="figS2" ref-type="fig">Suppl. Fig. 2</xref>). Black solid trace corresponds to the mean across 10 train/test data splits (gray dots). Black dotted trace indicates maximum log likelihood for n=17 clusters. <bold>b</bold>, Box plot shows distribution of assignment accuracy across clusters, obtained from comparing ground-truth labels generated using mean and covariance matrix of each Gaussian (i.e. cluster) with labels predicted by the pre-trained GMM (see also (<xref ref-type="bibr" rid="c68">Tolias et al., 2007</xref>). Right panel shows confusion matrix of true versus predicted labels. Please note that false positives are usually across clusters with similar response properties. <bold>c</bold>, Number of cells assigned to the different clusters, sorted by animal. <bold>d</bold>, Scatter plot of peak amplitudes of center (top) and surround (bottom) ETAs across all neurons, with mean and s.d. of each cluster from <xref rid="fig5" ref-type="fig">Fig. 5a</xref> indicated in color.</p></caption>
<graphic xlink:href="543054v2_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89996.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Rieke</surname>
<given-names>Fred</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Washington</institution>
</institution-wrap>
<city>Seattle</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>Franke et al. explore and characterize the color response properties in the mouse primary visual cortex, revealing specific color opponent encoding strategies across the visual field. The data is <bold>solid</bold>; however, the evidence supporting some conclusions is <bold>incomplete</bold>. In its current form, the paper makes a <bold>useful</bold> contribution to how color is coded in mouse V1. Significance would be enhanced with some additional analyses and a clearer discussion of the limitations of the data presented.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89996.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this study, Franke et al. explore and characterize color response properties across primary visual cortex, revealing specific color opponent encoding strategies across the visual field. The authors use awake 2P imaging to define the spectral response properties of visual interneurons in layer 2/3. They find that opponent responses are more pronounced at photopic light levels, and that diversity in color opponent responses exists across the visual field, with green ON/ UV OFF responses more strongly represented in the upper visual field. This is argued to be relevant for the detection of certain features that are more salient when using chromatic space, possibly due to noise reduction. In the revised version, Franke et al. have addressed the potential pitfalls in the discussion, which is an important point for the non-expert reader. Thus, this study provides a solid characterization of the color properties of V1 and is a valuable addition to visual neuroscience research.</p>
<p>My remaining concerns are based more on the interpretation. I'm still not convinced by the statement &quot;This type of color-opponency in the receptive field center of V1 neurons was not present in the receptive field center of retinal ganglion cells and, therefore, is likely computed by integrating center and surround information downstream of the retina.&quot; and I would suggest rewording it in the abstract.</p>
<p>As discussed previously and now nicely added to the discussion, it is difficult to make a direct comparison given the different stimulus types used to characterize the retina and V1 recordings and the different levels of adaptation in both tissues. I will leave this point to the discussion, which allows for a more nuanced description of the phenomenon. Why do I think this is important? In the introduction, the authors argue that &quot;the discrepancy [of previous studies] may be due to differences in stimulus design or light levels.&quot; However, while different light levels can be tested in V1, this cannot be done properly in the retina with 2P experiments. To address this, one would have to examine color-opponency in RGC terminals in vivo, which is beyond the scope of this study. Addressing these latter points directly in the discussion would, in my opinion, only strengthen the study.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89996.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Franke et al. characterize the representation of color in the primary visual cortex of mice, highlighting how this changes across the visual field. Using calcium imaging in awake, head-fixed mice, they characterize the properties of V1 neurons (layer 2/3) using a large center-surround stimulation where green and ultra-violet colors were presented in random combinations. Clustering of responses revealed a set of functional cell-types based on their preference to different combinations of green and UV in their center and surround. These functional types were demonstrated to have different spatial distributions across V1, including one neuronal type (Green-ON/UV-OFF) that was much more prominent in the posterior V1 (i.e. upper visual field). Modelling work suggests that these neurons likely support the detection of predator-like objects in the sky.</p>
<p>Strengths:</p>
<p>The large-scale single-cell resolution imaging used in this work allows the authors to map the responses of individual neurons across large regions of the visual cortex. Combining this large dataset with clustering analysis enabled the authors to group V1 neurons into distinct functional cell types and demonstrate their relative distribution in the upper and lower visual fields. Modelling work demonstrated the different capacity of each functional type to detect objects in the sky, providing insight into the ethological relevance of color opponent neurons in V1.</p>
<p>Weaknesses:</p>
<p>While the study presents convincing evidence about the asymmetric distribution of color-opponent neurons in V1, the paper would greatly benefit from a more in-depth discussion of the caveats related to the conclusions drawn about their origin. This is particularly relevant regarding the conclusion drawn about the contribution of color opponent neurons in the retina. The mismatch between retinal color opponency and V1 color opponency could imply that this feature is not solely inherited from the retina, however, there are other plausible explanations that are not discussed here. Direct evidence for this statement remains weak.</p>
<p>In addition, the paper would benefit from adding explicit neuron counts or percentages to the quadrants of each of the density plots in Figures 2-5. The variance explained by the principal components does not capture the percentage of color opponent cells. Additionally, there appear to be some remaining errors in the figure legend and labels that have not been addressed (e.g. '??' in Fig 2 legend).</p>
<p>Overall, this study will be a valuable resource for researchers studying color vision, cortical processing, and the processing of ethologically relevant information. It provides a useful basis for future work on the origin of color opponency in V1 and its ethological relevance.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89996.2.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper studies chromatic coding in mouse primary visual cortex. Calcium responses of a large collection of cells are measured in response to a simple spot stimulus. These responses are used to estimate chromatic tuning properties - specifically sensitivity to UV and green stimuli presented in a large central spot or a larger still surrounding region. Cells are divided based on their responses to these stimuli into luminance or chromatic sensitive groups. The results are interesting and many aspects of the experiments and conclusions are well done; several technical concerns, however, limit the support for several main conclusions,</p>
<p>Limitations of stimulus choice</p>
<p>
The paper relies on responses to a large (37.5 degree diameter) modulated spot and surround region. This spot is considerably larger than the receptive fields of both V1 cells and retinal ganglion cells (it is twice the area of the average V1 receptive field). As a result, the spot itself is very likely to strongly activate both center and surround mechanisms, and responses of cells are likely to depend on where the receptive fields are located within the spot (and, e.g., how much of the true neural surround samples the center spot vs the surround region). Most importantly, the surrounds of most of the recorded cells will be strongly activated by the central spot. This brings into question statements in the paper about selective activation of center and surround (e.g. page 2, right column). This in turn raises questions about several subsequent analyses that rely on selective center and surround activation.</p>
<p>Comparison with retina</p>
<p>
A key conclusion of the paper is that the chromatic tuning in V1 is not inherited from retinal ganglion cells. This conclusion comes from comparing chromatic tuning in a previously-collected data set from retina with the present results. But the retina recordings were made using a considerably smaller spot, and hence it is not clear that the comparison made in the paper is accurate. For example, the stimulus used for the V1 experiments almost certainly strongly stimulates both center and surround of retinal ganglion cells. The text focuses on color opponency in the receptive field centers of retinal ganglion cells, but center-surround opponency seems at least as relevant for such large spots. This issue needs to be described more clearly and earlier in the paper.</p>
<p>Limitations associated with ETA analysis</p>
<p>
One of the reviewers in the previous round of reviews raised the concern that the ETA analysis may not accurately capture responses of cells with nonlinear receptive field properties such as On/Off cells. This possibility and whether it is a concern should be discussed.</p>
<p>Discrimination performance poor</p>
<p>
Discriminability of color or luminance is used as a measure of population coding. The discrimination performance appears to be quite poor - with 500-1000 neurons needed to reliably distinguish light from dark or green from UV. Intuitively I would expect that a single cell would provide such discrimination. Is this intuition wrong? If not, how do we interpret the discrimination analyses?</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89996.2.sa4</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Franke</surname>
<given-names>Katrin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8649-4835</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Cai</surname>
<given-names>Chenchen</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ponder</surname>
<given-names>Kayla</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7776-1914</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Fu</surname>
<given-names>Jiakun</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3342-557X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Sokoloski</surname>
<given-names>Sacha</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Berens</surname>
<given-names>Philipp</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0199-4727</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Tolias</surname>
<given-names>Andreas S.</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4305-6376</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<p>We thank the reviewers for their thoughtful comments. We were pleased that they thought our study was &quot;well crafted and written&quot;, &quot;important&quot;, and that it provides a &quot;valuable resource for researchers studying color vision&quot;. They also expressed several constructive criticisms, concerning – among other things – the lack of details regarding experimental procedures and analysis, the challenge in relating retinal data to cortical recordings, and consistency of results across animals. In response to the reviewers’ comments and following their suggestions, we performed additional analyses, and substantially revised the paper:</p>
<p>We added a section in the Discussion about &quot;Limitations of the stimulus paradigm&quot;. In addition, we added a new Suppl. Figure that illustrates the effect of deconvolution of calcium traces on our results and clarified in the text why we use deconvolved signals for all analyses. The new Suppl. Figure also shows an additional analysis with a more conservative threshold of neuron exclusion.</p>
<p>We now clarify how retinal signaling relates to our cortical results and rewrote the text to be more conservative regarding our conclusions.</p>
<p>In addition, we added a new Suppl. Figure showing the key analyses from Figures 2 and 4 separately for each animal. We now mention consistency across animals in the Results section and clearly state which analyses were performed an data pooled across animals.</p>
<p>We are positive that these additions address the issues raised by the reviewers. Please find our point-by-point replies to all comments below.</p>
<disp-quote content-type="editor-comment">
<p><bold>eLife assessment</bold></p>
<p>Franke et al. explore and characterize the color response properties in the mouse primary visual cortex, revealing specific color opponent encoding strategies across the visual field. The data is solid; however, the evidence supporting some conclusions and details about some procedures are incomplete. In its current form, the paper makes a useful contribution to how color is coded in mouse V1. Significance would be enhanced with some additional analyses and resolution of some technical issues.</p>
</disp-quote>
<p>We thank the reviewers for appreciating our manuscript and their thoughtful comments.</p>
<disp-quote content-type="editor-comment">
<p><bold>Referee 1 (Remarks to the Author):</bold></p>
<p>Summary:</p>
<p>In this study, Franke et al. explore and characterize the color response properties across the primary visual cortex, revealing specific color opponent encoding strategies across the visual field. The authors use awake-behaving 2P imaging to define the spectral response properties of visual interneurons in Layer 2/3. They find that opponent responses are more prominent at photopic light levels, and diversity in color opponent responses exists across the visual science, with green ON/ UV OFF responses being stronger represented in the upper visual field. This is argued to be relevant for detecting certain features that are more salient when the chromatic space is used, possibly due to noise reductions.</p>
<p>Strengths:</p>
<p>The work is well crafted and written and provides a thorough characterization that reveals an uncharacterized diversity of visual properties in V1. I find this characterization important because it reveals how strongly chromatic information can modulate the response properties in V1. In the upper visual field, 25% of the cells differentially relay chromatic information, and one may wonder how this information will be integrated and subsequently used to aid vision beyond the detection of color per see. I personally like the last paragraph of the discussion that highlights this fact.</p>
</disp-quote>
<p>We thank the reviewer for appreciating our manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses: One major point highlighted in this paper is the fact that Green ON/UV OFF responses are not generated in the retina. But glancing through the literature, I saw this is not necessarily true. Fig 1. of Joesch and Meister, a paper cited, shows this can be the case. Thus, I would not emphasize that this wasn’t present in the retina. This is a minor point, but even if the retina could not generate these signals, I would be surprised if the diversity of responses would only arise through feed-forward excitation, given the intricacies of cortical connectivity. Thus, I would argue that the argument holds for most of the responses seen in V1; they need to be further processed by cortical circuitries.</p>
</disp-quote>
<p>We thank the reviewer for this comment. When analyzing available data from the retina using a similar center-surround color flicker stimulus (Szatko et al. 2020), we found that Green On/UV Off color opponency is very rare in the RF center of retinal ganglion cells (Suppl. Fig. 5). This suggests that center Green On/UV Off color opponency in V1 neurons is not inherited by the RF center of retinal neurons. However, we agree with the reviewer that retinal neurons might still contribute to V1 color opponency, for example by being center-surround color opponent (e.g. Joesch et al. 2016 and Szatko et al. 2020). We rephrased the text to acknowledge this fact.</p>
<disp-quote content-type="editor-comment">
<p>This takes me to my second point, defining center and surround. The center spot is 37.5 deg of visual angle, more than 1 mm of the retinal surface. That means that all retinal cells, at least half and most likely all of their surrounds will also be activated. Although 37.5 deg is roughly the receptive field size previously determined for V1 neurons, the one-to-one comparison with retinal recording, particularly with their center/surround properties, is difficult. This should be discussed. I assume that the authors tried a similar approach with sparse or dense checker white noise stimuli. If so, it would be interesting if there were better ways of defining the properties of V1 neurons on their complex/simple receptive field properties to define how much of their responses are due to an activation of the true &quot;center&quot; or a coactivation of the surround. Interestingly, at least some of the cells (Fig. 1d, cells 2 and 5) don’t have a surround. Could it be that in these cases, the &quot;center&quot; and &quot;surround&quot; are being excited together? How different would the overall statistics change if one used a full-filed flicker stimulus instead of a center/surround stimulus? How stable are the results if the center/surround flicker stimulus is shifted? These results won’t change the fact that chromatic coding is present in the VC and that there are clear differences depending on their position, but it might change the interpretation. Thus, I would encourage you to test these differences and discuss them.</p>
</disp-quote>
<p>Thanks for this comment. We agree with the reviewer that a one-to-one comparison of retina and V1 data is challenging, due to differences in both RF and stimulus size. We rephrased the Results text to clarify this point and now also mention it in the Discussion.</p>
<p>To be able to record from many V1 neurons simultaneously, we used a stimulus size of 37.5 degree visual angle in diameter, which is slightly larger than center RFs of single V1 neurons. As the reviewer mentions, the disadvantage of this approach is that the stimulus is only roughly centered on the neurons’ center RFs. To reduce the impact of potential stimulus misalignment on our results, we used the following steps:</p>
<p>For each recording, we positioned the monitor such that the mean RF across all neurons lies within the center of the stimulus field of view.</p>
<p>We confirmed that this procedure results in good stimulus alignment for the large majority of recorded neurons within individual recording fields by using a sparse noise stimulus (Suppl. Fig. 1a-c). Specifically, we found that for 83% of tested neurons, more than two thirds of their center RF, determined by the sparse noise stimulus, overlapped with the center spot of the color noise stimulus.</p>
<p>For analysis, we excluded neurons without a significant center STA, which may be caused by misalignment of the stimulus.</p>
<p>Together, we believe these points strongly suggest that the center spot and the surround annulus of the noise stimulus predominantly drive center (i.e. classical RF) and surround (i.e. extraclassical RF), respectively, of the recorded V1 neurons. This is further supported by the fact that color response types identified using an automated clustering method were robust across mice (Suppl. Fig. 6c), indicating consistent stimulus centering.</p>
<p>Nevertheless, we cannot exclude that the stimulus was misaligned for a subset of the recorded neurons used for analysis. We agree with the reviewer that such misalignment might have contributed to cells not having surround STAs, due to simultaneous activation of antagonistic center and surround RF components by the surround stimulus. While a full-field stimulus would get rid of the misalignment problem, it would not allow to study color tuning in center and surround RF components separately. Instead, one could compare the results of our approach with an approach that centers the stimulus on individual neurons. However, we believe that performing these additional experiments is out of the scope of the current study.</p>
<p>To acknowledge the experimental limitations of our study and the concerns brought up by the reviewer, we now explicitly mention the steps we perform to reduce the effects of stimulus misalignment in the Results section and discuss the problem of stimulus alignment in the Discussion. We believe these changes will help the reader to interpret our results.</p>
<disp-quote content-type="editor-comment">
<p><bold>Referee 2 (Remarks to the Author):</bold></p>
<p>Summary:</p>
<p>Franke et al. characterize the representation of color in the primary visual cortex of mice and how it changes across the visual field, with a particular focus on how this may influence the ability to detect aerial predators. Using calcium imaging in awake, head-fixed mice, they characterize the properties of V1 neurons (layer 2/3) using a large center-surround stimulation where green and ultra-violet were presented in random combinations. Using a clustering approach, a set of functional cell-types were identified based on their preference to different combinations of green and UV in their center and surround. These functional types were demonstrated to have varying spatial distributions in V1, including one neuronal type (Green-ON/UV-OFF) that was much more prominent in the posterior V1 (i.e. upper visual field). Modelling work suggests that these neurons likely support the detection of predator-like objects in the sky.</p>
<p>Strengths:</p>
<p>The large-scale single-cell resolution imaging used in this work allows the authors to map the responses of individual neurons across large regions of the visual cortex. Combining this large dataset with clustering analysis enabled the authors to group V1 neurons into distinct functional cell types and demonstrate their relative distribution in the upper and lower visual fields. Modelling work demonstrated the different capacity of each functional type to detect objects in the sky, providing insight into the ethological relevance of color opponent neurons in V1.</p>
</disp-quote>
<p>We thank the reviewer for appreciating our manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>While the study presents solid evidence a few weaknesses exist, including the size of the dataset, clarity regarding details of data included in each step of the analysis and discussion of caveats of the work. The results presented here are based on recordings of 3 mice. While the number of neurons recorded is reasonably large (n &gt; 3000) an analysis that tests for consistency across animals is missing. Related to this, it is unclear how many neurons at each stage of the analysis come from the 3 different mice (except for Suppl. Fig 4).</p>
</disp-quote>
<p>Thank you for this comment. We apologize that the original manuscript did not clearly indicate the consistency of our results across animals. We have revised the manuscript in the following
ways:</p>
<p>We have added an additional Suppl. Figure, which shows the variability of the data within and across animals (Suppl. Fig. 4). Specifically, we show the distribution of color and luminance selectivity for (i) center and surround components of V1 RFs and (ii) for upper and lower visual field. This data is used for all analyses shown in Figures 2-4. The figure legend of this figure also states the number of neurons per animal.</p>
<p>We now clearly state in the Results section that all analyses in the main figures were performed by pooling data across animals, and refer to the Suppl. Figures for consistency across animals.</p>
<p>We believe these changes help the reader to interpret our results.</p>
<disp-quote content-type="editor-comment">
<p>Finally, the paper would greatly benefit from a more in depth discussion of the caveats related to the conclusion drawn at each stage of the analysis. This is particularly relevant regarding the caveats related to using spike triggered averages to assess the response preferences of ON-OFF neurons, and the conclusions drawn about the contribution of retinal color opponency.</p>
</disp-quote>
<p>Thanks. We substantially revised the text to discuss caveats and limitations of the approach. For example, we added a section into the Discussion called &quot;Limitations of the stimulus paradigm&quot;. In addition, we clarified how retinal signals relate to cortical ones and phrased our conclusions more conservatively.</p>
<disp-quote content-type="editor-comment">
<p>The authors provide solid evidence to support an asymmetric distribution of color opponent cells in
V1 and a reduced color contrast representation in lower light levels. Some statements would benefit from more direct evidence such as the integration of upstream visual signals for color opponency in V1.</p>
</disp-quote>
<p>Based on the comments from Reviewer 1, we have rephrased the statements regarding the integration of upstream visual signals for color opponency in V1. We think these revisions increase the clarity of the results and help the reader with interpretation.</p>
<disp-quote content-type="editor-comment">
<p>Overall, this study will be a valuable resource for researchers studying color vision, cortical processing, and the processing of ethologically relevant information. It provides a useful basis for future work on the origin of color opponency in V1 and its ethological relevance.</p>
</disp-quote>
<p>Thanks! We thank the reviewer again for the helpful comments.</p>
<disp-quote content-type="editor-comment">
<p><bold>Referee 3 (Remarks to the Author):</bold></p>
<p>This paper studies chromatic coding in mouse primary visual cortex. Calcium responses of a large collection of cells are measured in response to a simple spot stimulus. These responses are used to estimate chromatic tuning properties - specifically sensitivity to UV and green stimuli presented in a large central spot or a larger still surrounding region. Cells are divided based on their responses to these stimuli into luminance or chromatic sensitive groups. Several technical concerns limit how clearly the data support the conclusions. If these issues can be fixed, the paper would make a valuable contribution to how color is coded in mouse V1.</p>
</disp-quote>
<p>We thank the reviewer for the helpful comments.</p>
<disp-quote content-type="editor-comment">
<p>Analysis: The central tool used to analyze the data is a &quot;spike triggered average&quot; of the responses to randomly varying stimuli. There are several steps in this analysis that are not documented, and hence evaluating how well it works is difficult. Central to this is that the paper does not measure spikes. Instead, measured calcium traces are converted to estimated spike rates, which are then used to estimate STAs. There are no raw calcium traces shown, and the approach to estimate spike rates is not described in any detail. Confirming the accuracy of these steps is essential for a reader to be able to evaluate the paper. Further, it is not clear why the linear filters connecting the recorded calcium traces and the stimulus cannot be estimated directly, without the intermediate step of estimating spike rates.</p>
</disp-quote>
<p>Thank you for this comment. We have used the genetically encoded calcium sensor GCaMP6s in our recordings. This sensor is a very sensitive GCaMP6 variant, but also one with slow kinetics. To remove the effect of the slow sensor kinetics from recorded calcium responses, the recorded traces are commonly deconvolved with the impulse function of the sensor to obtain the deconvolved calcium traces. We now include this reasoning in the Results section. To illustrate the effect of the deconvolution, we added a new Suppl. Figure (Suppl. Fig. 2) showing raw calcium and deconvolved traces, and the STAs estimated from both types of traces. This illustrates that the results regarding neuronal color preferences are consistent across raw and deconvolved calcium traces.</p>
<p>We agree with the reviewer that the term STA might be confusing. We have replaced it with the term &quot;even-triggered-average&quot; (ETA). In addition, we have replaced the phrase &quot;estimated spike rate&quot; with &quot;deconvolved calcium trace&quot; throughout the manuscript because the unit of the deconvolved traces is not interpretable, like spike rate would be (spikes per second). In the revised version, we now clarify in the Methods section that we estimate the ETAs based on deconvolved calcium traces, which is correlated with and an approximation for spike rate.</p>
<disp-quote content-type="editor-comment">
<p>A further issue about the STAs is that the inclusion criterion (correlation of predicted vs measured responses of 0.25) is pretty forgiving. It would be helpful to see a distribution of those correlation values, and some control analyses to check whether the STA is providing a sufficiently accurate measure to support the results (e.g. do the central results hold for the cells with the highest correlations).</p>
</disp-quote>
<p>We thank the reviewer for this comment. To exclude noisy neurons from analysis, we used the following procedure:</p>
<p>For each of the four stimulus conditions (center and surround for green and UV stimuli), kernel quality was measured by comparing the variance of the STA with the variance of the baseline, defined as the first 500 ms of the STA. Only cells with at least 10-times more variance of the kernel compared to baseline for UV or green center STA were considered for further analysis.</p>
<p>We have added the distribution of quality values to a new Suppl. Figure (Suppl. Fig. 2d,e). We now also show the percentage of neurons above threshold, given different quality thresholds. Finally, we have repeated the analysis shown in Figure 2 for a much more conservative threshold, including only the top 25% of neurons (Suppl. Fig. 2e,f). We now mention this new analysis in the Methods and Results section.</p>
<disp-quote content-type="editor-comment">
<p>Limitations of stimulus choice: The paper relies on responses to a large (37.5 degree diameter) modulated spot and surrounding region. This spot is considerably larger than the receptive fields of both V1 cells and retinal ganglion cells. As a result, the spot itself is very likely to strongly activate both center and surround mechanisms, and responses of cells are likely to depend on where the receptive fields are located within the spot (and, e.g., how much of the true neural surround samples the center spot vs the surround region). The impact of these issues on the conclusions is considered briefly at the start of the results but needs to be evaluated in considerably more detail. This is particularly true for retinal ganglion cells given the size of their receptive fields (see also next point).</p>
</disp-quote>
<p>We agree with the reviewer that the centering of the stimulus is critical and apologize if this point was not discussed sufficiently. To be able to record from many V1 neurons simultaneously, we used a stimulus size of 37.5 degree visual angle in diameter, which is slightly larger than center RFs of single V1 neurons. As the reviewer mentions, the disadvantage of this approach is that the stimulus is only roughly centered on the neurons’ center RFs. To reduce the impact of potential stimulus misalignment on our results, we have used different experimental and analysis steps and controls (see also second comment of Reviewer 1):</p>
<p>For each recording, we positioned the monitor such that the mean RF across all neurons lies within the center of the stimulus field of view.</p>
<p>We confirmed that this procedure results in good stimulus alignment for the large majority of recorded neurons within individual recording fields by using a sparse noise stimulus (Suppl. Fig. 1a-c). Specifically, we found that for 83% of tested neurons, more than two thirds of their center RF, determined by the sparse noise stimulus, overlapped with the center spot of the color noise stimulus.</p>
<p>For analysis, we excluded neurons without a significant center STA, which may be caused by misalignment of the stimulus.</p>
<p>We now mention those clearly in the Results section and added the limitations of our approach to the Discussion section.</p>
<disp-quote content-type="editor-comment">
<p>Comparison with retina: A key conclusion of the paper is that the chromatic tuning in V1 is not inherited from retinal ganglion cells. This conclusion comes from comparing chromatic tuning in a previously-collected data set from retina with the present results. But the retina recordings were made using a considerably smaller spot, and hence it is not clear that the comparison made in the paper is accurate. This issue may be handled by the analysis presented in the paper, but if so it needs to be described more clearly. The paper from which the retina data is taken argues that rod-cone chromatic opponency originates largely in the outer retina. This mechanism would be expected to be shared across retinal outputs. Thus it is not clear how the Green-On/UV-Off vs Green-Off/UV-On asymmetry could originate. This should be discussed.</p>
</disp-quote>
<p>We agree with the reviewer that a one-to-one comparison of retina and V1 data is challenging, due to differences in both RF and stimulus size. We rephrased the Results text to clarify this point and now also mention it in the Discussion.</p>
<p>When analyzing available data from the retina using a similar center-surround color flicker stimulus (Szatko et al. 2020), we found that Green On/UV Off color opponency is very rare in the RF center of retinal ganglion cells (Suppl. Fig. 5). This suggests that center Green On/UV Off color opponency in V1 neurons is not inherited by the RF center of retinal neurons. However, we agree with the reviewer that retinal neurons might still contribute to V1 color opponency, for example by being center-surround color opponent (e.g. Joesch et al. 2016 and Szatko et al. 2020). We rephrased the text to acknowledge this fact.</p>
<disp-quote content-type="editor-comment">
<p>Residual chromatic cells at low mesopic light levels The presence of chromatically tuned cells at the lowest light level probed is surprising. The authors describe these conditions as rod-dominated, in which case chromatic tuning should not be possible. This again is discussed only briefly. It either reflects the presence of an unexpected pathway that amplifies weak cone signals under low mesopic conditions such that they can create spectral opponency or something amiss in the calibrations or analysis. Data collected at still lower light levels would help resolve this.</p>
</disp-quote>
<p>Thank you for this comment. We call the lowest light level &quot;low mesopic&quot; and &quot;rod-dominated&quot; because the spectral contrast of V1 center responses in posterior recording fields is green-shifted for this light level (Fig. 3a). This is only expected if responses in the UV-cone dominant ventral retina are predominantly driven by rod photoreceptors. We now explain this rationale in the Results section. In addition, we mention in the Discussion that future studies are required to test whether cone signals need to be amplified for low light levels. While we agree with the reviewer that it would be exciting to use even lower light levels during recordings, we believe this is out of the scope of the current study due to the technical challenges involved in achieving scotopic stimulation.</p>
</body>
</sub-article>
</article>