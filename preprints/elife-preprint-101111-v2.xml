<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">101111</article-id>
<article-id pub-id-type="doi">10.7554/eLife.101111</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101111.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>AVN: A Deep Learning Approach for the Analysis of Birdsong</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5327-3219</contrib-id>
<name>
<surname>Koch</surname>
<given-names>Therese MI</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>therese.koch1@gmail.com</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Marks</surname>
<given-names>Ethan S</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0967-6598</contrib-id>
<name>
<surname>Roberts</surname>
<given-names>Todd F</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>Todd.Roberts@utsouthwestern.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05byvp690</institution-id><institution>Department of Neuroscience, UT Southwestern Medical Center</institution></institution-wrap>, <city>Dallas</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Goldberg</surname>
<given-names>Jesse H</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Cornell University</institution>
</institution-wrap>
<city>Ithaca</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>King</surname>
<given-names>Andrew J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-09-12">
<day>12</day>
<month>09</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-10-01">
<day>01</day>
<month>10</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP101111</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-07-09">
<day>09</day>
<month>07</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-08-24">
<day>24</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.05.10.593561"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-09-12">
<day>12</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101111.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.101111.1.sa8">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101111.1.sa7">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101111.1.sa6">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101111.1.sa5">Reviewer #3 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.101111.1.sa4">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Koch et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Koch et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-101111-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Deep learning tools for behavior analysis have enabled important new insights and discoveries in neuroscience. Yet, they often compromise interpretability and generalizability for performance, making it difficult to quantitively compare phenotypes across datasets and research groups. We developed a novel deep learning-based behavior analysis pipeline, <italic>Avian Vocalization Network</italic> (AVN), for the learned vocalizations of the most extensively studied vocal learning model species – the zebra finch. AVN annotates songs with high accuracy across multiple animal colonies without the need for any additional training data and generates a comprehensive set of interpretable features to describe the syntax, timing, and acoustic properties of song. We use this feature set to compare song phenotypes across multiple research groups and experiments, and to predict a bird’s stage in song development. Additionally, we have developed a novel method to measure song imitation that requires no additional training data for new comparisons or recording environments and outperforms existing similarity scoring methods in its sensitivity and agreement with expert human judgements of song similarity. These tools are available through the open-source AVN Python package and graphical application, which makes them accessible to researchers without any prior coding experience. Altogether, this behavior analysis toolkit stands to facilitate and accelerate the study of vocal behavior by enabling a standardized mapping of phenotypes and learning outcomes, thus helping scientists better link behavior to the underlying neural processes.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>this manuscript was reviewed at eLife and this is the revised version. The first round reviews and our initial response can be found on the eLife website.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>A deep understanding of animal behavior is fundamental to a deep understanding of the brain. However, accurate, quantitative description of animal behavior, particularly in ethologically relevant contexts, remains a substantial challenge in neuroscience research. In recent years, careful observation of motor and vocal behaviors is increasingly being replaced with machine learning and deep learning-based approaches. These tools allow researchers to consider much greater volumes of data than was previously possible and uncover patterns in animal behavior that are undetectable to humans, and have led to important insights into ethologically relevant behaviors and the effects of experimental interventions thereupon [<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref>]. However, this increased power often comes at the expense of interpretability and generalizability.</p>
<p>An increasing number of supervised deep learning methods are being developed for the automated annotation of animal vocalization behavior [<xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c7">7</xref>], and unsupervised methods for dimensionality reduction and analysis [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c11">11</xref>]. While unsupervised approaches are very powerful, and have been shown to explain more variance in vocalization repertoires than hand-selected acoustic features [<xref ref-type="bibr" rid="c8">8</xref>], the features that they generate are notoriously difficult to interpret, and specific to the exact dataset from which they were derived [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>]. As a result, unsupervised data-driven methods, while allowing detailed comparison of individuals within the same data set, make it more difficult to compare the nature and severity of vocal phenotypes across experiments and research groups.</p>
<p>To truly maximize the benefits of machine learning and deep learning methods for behavior analysis, their power must be balanced with interpretability and generalizability. This can be achieved by combining automated annotation with a carefully selected set of meaningful features, thereby creating a common feature space for the comparison of behavioral phenotypes across research groups, experimental conditions, and studies. For speed, ease of use, and standardization, the annotations should be generated without the need for any training data or hyperparameter setting for new individuals or recording conditions. The features should be consistent across recording conditions, allowing direct, meaningful comparisons between research groups. The feature set should be comprehensive, describing multiple aspects of the behavior. Finally, the features should be interpretable, allowing researchers to form concrete hypotheses about how different manipulations will affect specific features, and use observed feature values to guide future experimental design.</p>
<p>We have developed an analysis pipeline called <italic>Avian Vocalization Network (AVN)</italic> which satisfies these criteria for zebra finch song analysis. Zebra finches are the most popular animal model for the study of vocal learning. They learn to sing a single, highly stereotyped song by memorizing the song of an adult tutor, then refining their vocalizations to match this song template during a sensorimotor learning period early in development (<xref rid="fig1" ref-type="fig">Figure 1a</xref>); a process which bears many parallels to human speech learning [<xref ref-type="bibr" rid="c12">12</xref>]. Typical zebra finch songs consist of a variable number of introductory notes, followed by multiple repetitions of a motif, composed of 3 to 10 unique syllable types produced in a stereotyped sequence. Traditionally, zebra finch song has been analyzed by segmenting the song into syllables, then manually labeling syllables based on visual inspection of their spectrograms [<xref ref-type="bibr" rid="c13">13</xref>–<xref ref-type="bibr" rid="c15">15</xref>]. This process is very labor intensive, which limits the number of songs that can be considered at a time. Manual syllable labeling and motif identification can also be subjective and therefore susceptible to experimenter bias, as motif composition and syllable types can be somewhat ambiguous, particularly in young birds with immature song and in birds with experimentally disrupted songs.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1</label>
<caption><title>Overview of AVN song analysis pipeline.</title>
<p><bold>a.</bold> Schematic timeline of zebra finch song learning. <bold>b.</bold> Overview of AVN song analysis pipeline. Spectrograms of songs are automatically segmented into syllables then syllables are labeled. The raw spectrograms are used to calculate features describing the rhythm of a bird’s song, the segmentations are used to calculate syllable-level timing features, and the labeled syllables are used to calculate syntax-related features and acoustic features of a bird’s song. <bold>c.</bold> Birds from different research groups, with multiple different song phenotypes can all be processed by the AVN pipeline, generating a matrix of directly comparable, interpretable features, which can be used for downstream analyses including phenotype comparisons, tracking the emergence of a phenotype over time, investigating song development, and detecting individual outlier birds with atypical song phenotypes.</p></caption>
<graphic xlink:href="593561v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We tested and compared two deep-learning approaches for syllable segmentation, which don’t require any additional training data or hyperparameter setting for new birds. These were WhisperSeg[<xref ref-type="bibr" rid="c16">16</xref>], and a novel application of Tweetynet[<xref ref-type="bibr" rid="c5">5</xref>]. We then applied unsupervised dimensionality reduction and clustering methods to assign labels to these automatically segmented syllables, as in [<xref ref-type="bibr" rid="c17">17</xref>], and used thorough validation approaches to examine how these unsupervised methods perform when combined with the segmentation approaches. Finally, we use the resulting annotated songs to calculate a set of 55 interpretable features which describe the syntax, timing, and acoustic properties of a set of songs (<xref rid="fig1" ref-type="fig">Figure 1b</xref>). We show that the automated annotation performs consistently well across multiple zebra finch colonies, and that the feature set can be used to glean mechanistic insights from the comparison of vocal phenotypes, and to predict a bird’s stage in song development (<xref rid="fig1" ref-type="fig">Figure 1c</xref>). We also developed a new method to compare two birds’ syllable repertoires to measure song learning, which outperforms existing song similarity scoring methods on multiple key metrics. The complete pipeline is available as an open-source Python package and as an application with a graphical user interface, allowing researchers with no prior coding experience to easily annotate their songs, calculate the feature set, and calculate song similarity scores (<xref ref-type="fig" rid="fig2s1">Figure 2–figure supplement 1</xref>).</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Comparing deep learning methods for fully automated syllable segmentation</title>
<p>To accurately segment and label zebra finch songs without the need for any individual-specific training data or hyperparameter tuning, we tested and compared two different deep-learning based approaches for syllable segmentation. Traditionally, zebra finch song is segmented based on an amplitude threshold [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c18">18</xref>]. The best value for this amplitude threshold depends heavily on recording conditions and background noise levels, and setting this threshold often requires careful trial and error by a human annotator. Amplitude-based segmentation methods also cannot distinguish between song syllables and noises, like wing flaps or other non-vocal artifacts, which can contaminate downstream analyses. Instead of relying on amplitude alone, we compared two deep learning models, TweetyNet [<xref ref-type="bibr" rid="c5">5</xref>] and WhisperSeg [<xref ref-type="bibr" rid="c16">16</xref>], which take the full spectral content of the audio into account when performing segmentation.</p>
<p>We tested these two segmentation methods with a dataset of over 1,000 manually annotated songs from 35 adult zebra finches, including birds with typical song production, isolate birds raised without a song tutor, and birds with disrupted song production due to knockdown of the transcription factor FoxP1 (FP1) in the song nucleus HVC [<xref ref-type="bibr" rid="c19">19</xref>]. The annotation consists of manual correction of amplitude-segmented syllables and assignment of syllable labels based on the appearance of the spectrograms by expert annotators. TweetyNet was designed to simultaneously segment and label syllable types by assigning syllable labels to short spectrogram frames. This application requires re-training for each individual bird, so we instead trained TweetyNet to label spectrogram frames as simply containing vocalizations, silence, or noise. We trained it with 34 of the 35 birds in the dataset and evaluated segmentation accuracy with the remaining bird, repeating this once for each bird in the dataset. This novel approach allows the model to learn an abstract notion of vocalization vs. non-vocalization which generalizes well to new individuals not included in training. The WhisperSeg model is already trained for segmentation of new individuals, so we used the existing standard model to segment each of our birds. Segmentation accuracy was evaluated against expert human annotations by calculating the precision, recall and F1 scores of syllable onset detections within 10ms of a syllable onset in the manual annotations.</p>
<p>WhisperSeg shows the best performance, with a mean F1 score of 0.882(+-SEM 0.02), compared to TweetyNet’s score of 0.824 (+-SEM 0.03) and a simple amplitude segmentation algorithm (RMSE) with a mean score of 0.593 (+-SEM 0.02) (<xref rid="fig2" ref-type="fig">Figure 2a</xref>). WhisperSeg’s precise onset times were also more consistent with expert human annotations than both other methods (median absolute time difference of 1.75ms for WhisperSeg, 2.22ms for TweetyNet, and 3.81ms for RMSE) (<xref rid="fig2" ref-type="fig">Figure 2b</xref>). All 3 methods performed similarly for the typical, isolate and FP1 KD birds (<xref ref-type="fig" rid="fig2s1">Figure 2–figure supplement 1a-f</xref>). As a further test of the generalization of these methods, we applied them to a dataset of manually annotated songs from 25 birds from the Rockefeller University Field Research Center Song Library [<xref ref-type="bibr" rid="c20">20</xref>]. Using the pre-trained WhisperSeg model, and a TweetyNet model trained on all 35 birds from the UTSW colony and none from the Rockefeller Song Library, each of these models yielded very similar segmentation accuracy scores to those obtained with the UTSW colony (<xref rid="fig2" ref-type="fig">Figure 2a,c</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2–figure supplements 1-2</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2</label>
<caption><title>Automated syllable annotation metrics.</title>
<p><bold>a.</bold> F1 scores for syllable onset detections within 10ms of a syllable onset in the manual annotations of each bird (n=35 from UTSW and n=25 from Rockefeller) across segmentation methods. <bold>b.</bold> Distribution of time-differences between predicted syllable onsets and their best matches in the manual annotation, across segmentation methods. Distributions include all matched syllables across all 35 birds from the UT Southwestern colony (UTSW) and (<bold>c.</bold>) 25 from Rockefeller. <bold>d.</bold> Example spectrogram of a typical adult zebra finch. The song was segmented with WhisperSeg and labeled using UMAP C HDBSCAN clustering. Colored rectangles reflect the labels of each syllable. <bold>e.</bold> Example UMAP plot of 3131 syllables from the same bird as in <italic>d</italic> and <italic>f</italic>. Each point represents one syllable segmented with WhisperSeg, and colors reflect the AVN label of each syllable. <bold>f.</bold> Example confusion matrix for the bird depicted in <italic>d</italic> and <italic>e.</italic> The matrix shows the percentage of syllables bearing each manual annotation label which fall into each of the possible AVN labels. <bold>g.</bold> V-measure scores for AVN syllable labels compared to manual annotations for each bird (n=35 from UTSW and n=25 from Rockefeller), across segmentation methods.</p></caption>
<graphic xlink:href="593561v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Accurate, fully unsupervised syllable labeling</title>
<p>Next, we assign syllable labels to these segmented units. To achieve this, we first performed UMAP dimensionality reduction [<xref ref-type="bibr" rid="c21">21</xref>] on spectrograms of the segmented syllables, then performed hierarchical density based clustering (HDBSCAN) [<xref ref-type="bibr" rid="c22">22</xref>] on the syllables’ UMAP embeddings, as in [<xref ref-type="bibr" rid="c17">17</xref>]. We calculated the UMAP embeddings of all segmented syllables for each of the 35 birds in our dataset using manual segmentations, WhisperSeg, or TweetyNet segmentations. In all cases, we found that the syllables formed multiple dense clusters, which corresponded well to manually annotated syllable labels (<xref rid="fig2" ref-type="fig">Figure 2 d-g</xref>; <xref ref-type="fig" rid="fig2s3">Figure 2–figure supplement 3</xref>).</p>
<p>Using manual segmentation yielded the best agreement with manual labels (mean v-measure score = 0.87 +-0.01) which is to be expected, as no discrepancies are introduced during segmentation (<xref rid="fig2" ref-type="fig">Figure 2g</xref>). When clustered, WhisperSeg’s segments yielded better agreement with manual labels than TweetyNet’s (WhisperSeg mean v-measure = 0.80 +-0.02, TweetyNet’s mean v-measure - 0.77 +-0.02). We observe similar performance on our second dataset of 25 typical adult zebra finches from the Rockefeller Song Library, suggesting that these methods generalize well across colonies and recording environments (<xref rid="fig2" ref-type="fig">Figure 2g</xref>). UMAP embeddings have an inherent stochasticity, meaning that running this embedding and clustering multiple times on the same dataset will result in slight changes to the embeddings and labels across runs. To measure the degree of these changes, we calculated the v-measure score for clustering labels for each of 30 different random initializations of the UMAP embedding for each of 25 birds from the UTSW colony, and found negligible variation in the v-measure score across runs (<xref ref-type="fig" rid="fig2s4">Figure 2–figure supplement 4</xref>).</p>
<p>Most of the errors in TweetyNet’s syllable labeling stem from inconsistencies in syllable segmentation. For example, if two syllables with labels ‘a’ and ‘b’ in the manual annotation are segmented sometimes as separate syllables and sometimes as a merged syllable ‘ab’, the clustering is likely to find 3 different syllables clusters: one for ‘a’, one for ‘b’, and one for the combined ‘ab’. Because of how we align syllables across segmentations for validation, syllable ‘b’ will sometimes carry one cluster label, and sometimes carry the missing segment label, whereas syllable ‘a’ can carry two different clustering labels: one for ‘a’ and one for the ‘ab’ segmentation. This scenario would result in lower completeness score (a measure of the extent to which syllables with the same manual annotation also carry the same cluster label) without any impact on the homogeneity score (a measure of the extent to which syllables with the same cluster label carry the same manual annotation). Consistent with this, Tweetynet segmentation results in higher homogeneity scores for typical birds than WhisperSeg, but lower completeness scores (<xref ref-type="fig" rid="fig2s5">Figure 2–figure supplement 5</xref>).</p>
<p>In cases where it is somewhat ambiguous as to whether syllables should be segmented separately (‘a’ and ‘b’) vs. merged (‘ab’), WhisperSeg is much more likely to merge them than is Tweetynet, resulting in a lower homogeneity score when the manual annotation has the syllables segmented separately. However, WhisperSeg will be more consistent in its choice to merge or split than Tweetynet within the same bird, which accounts for WhisperSeg’s higher completeness score (<xref ref-type="fig" rid="fig2s5">Figure 2–figure supplement 5</xref>). WhisperSeg’s internal consistency in this regard also results in less artifactual inflation of downstream syntax and syllable duration variability metrics, making it the better overall choice for syllable segmentation in the context of AVN.</p>
<p>Altogether, we conclude that WhisperSeg followed by UMAP-HDBSCAN clustering produces the most accurate syllable labels. These will be referred to as AVN labels henceforth in this manuscript. AVN labels are produced without the need for any per-bird parameter tuning or model training. This approach not only saves experimenters time when analyzing many birds but also reduces the potential for experimenter bias during song annotation. AVN labeling generalizes well across multiple zebra finch colonies, suggesting that it can be easily adopted by new research groups without the need for extensive additional validation. Thus, we hope it can serve as a standard for song annotations when manual annotation is not required.</p>
</sec>
<sec id="s2c">
<title>Analyzing Song Syntax</title>
<p>The automatically generated AVN labels can be used to visualize and quantify a bird’s song syntax. Typical zebra finches produce syllables in a very predictable order, where the syllable type that a bird will sing can be reliably predicted based on the immediately preceding syllable type [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c23">23</xref>]. Many studies have found that manipulations of the neural circuitry underlying song learning, and production can disrupt a bird’s syntax, leading to more variable sequences [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>]. Methods used to quantify these syntax disruptions vary across papers and research groups, making it impossible to directly compare the severity of disruptions. We propose a comprehensive suite of features to describe a bird’s song syntax, which can all be calculated using the AVN Python package and AVN graphical application.</p>
<p>First, we developed a new song syntax visualization, called a <italic>syntax raster plot</italic> which lets researchers view many song bouts’ syllable sequences simultaneously (<xref rid="fig3" ref-type="fig">Figure 3a</xref>). We can also visualize syntax using a transition matrix, which gives the probability of a syllable type being produced, given the preceding syllable type (<xref rid="fig3" ref-type="fig">Figure 3b</xref>). We quantify the stereotypy of a bird’s syntax by calculating the entropy rate of the transition matrix, and find a strong correlation (<italic>r</italic> = 0.89, p &lt;0.05) between entropy rates calculated using AVN labels and manual annotations, showing that our AVN labels are sufficiently reliable to describe a bird’s syntax stereotypy (<xref rid="fig3" ref-type="fig">Figure 3c</xref>). We also find the same statistical relationship between groups as with manual annotations, namely that birds with FP1 KD and isolate birds have significantly higher entropy rates than typical birds (One way ANOVA F(2, 32) = 15.05, Tukey HSD p-adj FP1 vs. typical &lt; 0.005, p-adj isolate vs. typical &lt;0.005) (<xref rid="fig3" ref-type="fig">Figure 3d</xref>; <xref ref-type="fig" rid="fig3s1">Figure 3–figure supplement 1a-d</xref>). Multiple studies have also found that neural song-circuit manipulations can induce a ‘stutter’ in birds, i.e. increase the rate of syllable repetitions in their songs [<xref ref-type="bibr" rid="c26">26</xref>–<xref ref-type="bibr" rid="c28">28</xref>], so we’ve introduced two additional metrics to specifically look at the rate of syllable repetitions in a bird’s song; the mean number of times a syllable is produced in a row each time it is sung (repetition bout length), and the CV of the number of syllable repetitions (CV repetition bout length) (<xref ref-type="fig" rid="fig3s1">Figure 3–figure supplement 1e-f</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3</label>
<caption><title>Song syntax and timing analysis with AVN.</title>
<p><bold>a.</bold> Example syntax raster plot for a typical adult zebra finch made with AVN labels. Each row represents a song bout, and each colored block represents a syllable, colored according to its AVN label. <bold>b.</bold> Example transition matrix from the bird featured in <italic>a.</italic> Each cell gives the probability of the bird producing the ‘following syllable’, given that they just produced a syllable with the ‘preceding syllable’ label. <bold>c.</bold> Correlation between normalized entropy rate scores calculated for each bird using manual annotations or AVN labels (n=35 birds from UTSW, <italic>r</italic> = 0.89, p&lt;0.005). <bold>d.</bold> Comparison of normalized entropy rates calculated with AVN labels across typical (n=20), isolate (n=8), and FP1 KD (n=7) adult zebra finches (One Way ANOVA F(2, 32) = 15.05, p &lt;0.005, Tukey HSD * indicates p-adj &lt; 0.005). <bold>e.</bold> Schematic representing the generation of rhythm spectrograms. The amplitude trace of each song file is calculated, then the spectrum of the first derivative of the amplitude trace is computer. The spectra of multiple song files are concatenated to form a rhythm spectrogram, with bout index on the x-axis and frequency along the y axis. The example rhythm spectrograms show the expected banding structure of a typical adult zebra finch, and the less structured rhythm of a typical juvenile zebra finch (50dph). <bold>f.</bold> Comparison of rhythm spectrum entropies cross typical (n=20), isolate (n=8), FP1 KD (n=7) adult zebra finches (&gt;90dph), and juvenile zebra finches (n = 11, 50-51dph) (One Way ANOVA F(3, 43) = 17.0, p &lt; 0.05, Tukey HSD * indicates p-adj &lt; 0.05).</p></caption>
<graphic xlink:href="593561v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>Analyzing Song Timing</title>
<p>Song timing can refer to the durations of individual syllables and gaps, or to the rhythmic patterns of a song bout. We have developed and validated multiple metrics to describe song timing at each of those scales, which can easily be calculated using the AVN Python package or graphical application. First, we look at the timing of individual syllables and gaps by plotting the distribution of their durations based on our WhisperSeg segmentations. Typical mature zebra finches have very stereotyped syllable durations across renditions of the same syllable type, which result in a distribution of syllable durations consisting of multiple narrow peaks, each corresponding to a different syllable type (<xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2</xref>). Immature birds, on the other hand, have very variable syllable durations, and tend to have a single broad peak and long positive tail in their syllable duration distributions [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>]. We observe these same patterns using our WhisperSeg segmentations when we apply them to our dataset of 35 mature birds, and an additional 11 juvenile birds aged 50-51 days post hatch (dph). We quantify the maturity of a bird’s syllable timing by calculating the entropy of their syllable duration distribution, which will approach 1 when density is evenly spread across syllable durations (as in juvenile birds), and approach 0 when density is concentrated in a narrow range of syllable durations, as was done in [<xref ref-type="bibr" rid="c30">30</xref>]. Indeed, using our WhisperSeg segmentation, we find that juvenile birds have significantly higher syllable duration entropies than adult birds (F(3, 43) = 17.43, p&lt;0.005, Tukey HSD p-adj juvenile vs. typical adult &lt;0.05) (<xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2</xref>). The syllable duration entropy values that we obtain with WhisperSeg segmentation are also highly correlated with those scores that we obtain from manual segmentation (<italic>r</italic> = 0.85, p&lt;0.05) (<xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2</xref>), further indicating that our automated segmentation is sufficiently accurate for downstream analyses.</p>
<p>In addition to syllable level timing, we have implemented multiple metrics describing a song’s rhythm at the bout level based on the ‘rhythm spectrogram’ first proposed in [<xref ref-type="bibr" rid="c31">31</xref>]. A rhythm spectrum is constructed by taking the Fourier transform of the derivative of the amplitude trace of a song. If the song consists of multiple motifs with a consistent rhythm, the song’s amplitude will have a repeating fluctuation pattern, which will be reflected in its spectrum. By calculating the ‘rhythm spectrum’ of multiple bouts, then concatenating them into a rhythm spectrogram, we can get a clear impression of a bird’s overall rhythmicity, and the consistency of that rhythm across song bouts (<xref rid="fig3" ref-type="fig">Figure 3e</xref>). We quantify the strength of this rhythm by computing the Wiener Entropy of the mean rhythm spectrum, and we quantify the consistency of the rhythm across bouts by calculating the coefficient of variation of the peak frequency across each bout in the rhythm spectrogram (<xref ref-type="fig" rid="fig3s3">Figure 3–figure supplement 3</xref>). We find that juvenile birds have significantly higher rhythm spectrum entropies (<xref rid="fig3" ref-type="fig">Figure 3f</xref>, One Way ANOVA F(3, 43) = 17.0, Tukey HSD juvenile vs. typical adult p-adj &lt; 0.005) and higher peak frequency CVs than adult birds (<xref ref-type="fig" rid="fig3s3">Figure 3– figure supplement 3</xref>, One Way ANOVA F(3, 43) = 8.23, Tukey HSD juvenile vs. typical adult p-adj &lt; 0.05). Whereas the FP1 KD birds’ syllable duration entropies are squarely in line with typical adults (<xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2e</xref>, Tukey HSD FP1 KD vs typical adult p-adj = 0.53), their rhythm spectrum entropies (<xref rid="fig3" ref-type="fig">Figure 3f</xref>) and gap duration entropies (<xref ref-type="fig" rid="fig3s2">Figure 3–figure supplement 2f</xref>) are significantly higher (Tukey HSD FP1 KD vs. typical adult p-adj &lt;0.05). This is consistent with our earlier finding that the FP1 KD birds have more variable syllable sequencing and highlights the complementary nature of these metrics. When considered together, they provide a comprehensive description of a bird’s song production.</p>
</sec>
<sec id="s2e">
<title>Comparing Song Disruptions with AVN Features</title>
<p>In addition to syntax and timing features, AVN can also calculate a suite of acoustic features, including goodness of pitch, mean frequency, frequency modulation, amplitude modulation, entropy, amplitude, and pitch. This feature set is well established for describing zebra finch song, thanks to the Sound Analysis Pro application [<xref ref-type="bibr" rid="c32">32</xref>]. These features are calculated for each frame of a spectrogram, but to facilitate comparisons between birds, we take the mean value of each feature for every syllable rendition, then compute the overall mean value and the coefficient of variation across renditions of the same syllable type. We then select the syllable types with the minimum, median and maximum values with respect to each feature to represent the overall acoustic properties of a bird’s song. This results in a total of 48 acoustic features for each bird. When combined with our 3 syntax related features and 4 timing related features, we are left with a complete set of 55 features to describe all major aspects of a bird’s song production. This feature set represents an extremely valuable resource for comparing experimental groups, for tracking song phenotypes over time, or for detecting birds with atypical song production.</p>
<p>To showcase the AVN feature set’s potential for comparing birds across experiments and research groups, we calculated this feature set for 53 typical adult zebra finches, 16 isolate-reared zebra finches, and 7 FP1 KD zebra finches from the UTSW colony; 25 typical adult zebra finches from the Rockefeller Song Library [<xref ref-type="bibr" rid="c20">20</xref>]; and 4-sham deafened birds and 5 early-deafened birds from Hokkaido University, originally recorded for [<xref ref-type="bibr" rid="c33">33</xref>]. We fit a Linear Discriminant Analysis (LDA) model to a dataset containing only typical and isolate zebra finches. LDA finds the linear combination of features that best separates groups of points (in this case typical vs. isolate birds’ AVN features), allowing us to identify which features are most informative for discriminating between these groups. This allows us to calculate an ‘isolate score’ for every bird, based on the LD1 value (<xref rid="fig4" ref-type="fig">Figure 4a</xref>). We achieved a 95% classification accuracy between these two groups (<xref rid="fig4" ref-type="fig">Figure 4a</xref>), with the most important features being higher syntax entropy rates, higher syllable duration variance, and higher rhythm entropies for isolates compared to typical birds (<xref ref-type="fig" rid="fig4s1">Figure 4– figure supplement 1</xref>). We repeated this process for typical hearing and deaf birds and achieved a 99% classification accuracy (<xref rid="fig4" ref-type="fig">Figure 4b</xref>), with the most important features being higher mean frequency variance, lower absolute mean frequency, and higher syllable duration variance for deaf birds compared to hearing birds (<xref ref-type="fig" rid="fig4s1">Figure 4–figure supplement 1</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4</label>
<caption><title>Song phenotypes classification with AVN features.</title>
<p><bold>a.</bold> Linear discriminant values for multiple groups of birds generated from a model trained to discriminate between typical and isolate zebra finches (n=16 isolate birds, 7 FP1 KD birds, 5 deaf birds, 4 sham deafening birds, 53 typical zebra finches from the UTSW colony and 25 typical zebra finches from Rockefeller). <bold>b.</bold> Linear discriminant values for multiple groups of birds generated from a model trained to discriminate between typical and deaf zebra finches. Same birds as in <italic>a</italic>. <bold>c.</bold> Confusion matrix indicating the LDA model’s classification of typical, deaf, isolate and FP1 KD birds from aa model trained to discriminate between typical, deaf, and isolate birds. Scores for typical, deaf, and isolate birds were obtained using leave-one-out cross validation, and FP1 KD scores were obtained using a model fit to all typical, deaf and isolate birds. <bold>d.</bold> Plot of the linear discriminant coordinates of isolate (n=16), typical (n=78), and FP1 KD birds (n=7) for a model trained to discriminate between typical, deaf, and isolate birds. FP1 KD birds overlap most with isolate birds in this LDA space, indicating that their song production most closely resembles that of isolates.</p></caption>
<graphic xlink:href="593561v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Finally, we fit an LDA model to a dataset containing typical, isolate, and deaf zebra finches, and used this model to assess which of these groups the FP1 KD birds most closely resemble. Previous work suggests that FP1 KD in the song nucleus HVC impairs tutor song memory formation while leaving song motor learning intact [<xref ref-type="bibr" rid="c19">19</xref>]. Thus, we would expect the FP1 KD birds’ songs to most closely resemble those of isolates, who have no tutor song memory but do have access to auditory feedback of their own vocalizations, and to not resemble deaf birds who have neither tutor song memories, nor access to auditory feedback. Indeed, we find that 5/7 FP1 KD birds are classified as isolates by the LDA classifier, with the other 2/7 being classified as typical birds (<xref rid="fig4" ref-type="fig">Figure 4c,d</xref>). This supports our hypothesis about the nature of the song disruption in FP1 KD birds and highlights the utility of a common feature set for comparing song phenotypes. We hope that the ease of calculation and completeness of this feature set will facilitate phenotypic comparisons across the field of songbird neuroscience, particularly as the field grows in its ability to conduct genetic manipulations of the neural circuits involved in different aspects of song learning and production.</p>
</sec>
<sec id="s2f">
<title>Tracking Song Development with AVN Features</title>
<p>To further showcase the potential of these AVN features for zebra finch song analysis, we also used them to track song development. We calculated the AVN feature sets for 14 birds from UTSW and 5 birds from Duke University [<xref ref-type="bibr" rid="c9">9</xref>] at multiple timepoints during song development, ranging from 46 dph to 102 dph. We fit a Generalized Additive Model (GAM) to predict a bird’s age based on its AVN features and find that we achieve the most accurate age predictions with a model that considers a bird’s syllable duration entropy, their syntax entropy, absolute syllable durations, and the variability of goodness of pitch, syllable duration and Weiner entropy across renditions (<xref rid="fig5" ref-type="fig">Figure 5</xref>). When trained with data from all but one bird and tested on the remaining bird, this model can predict a bird’s age within 7 dph for 50% of age points, and within 11 dph for 75% of age points. Its performance is best for younger birds, with prediction accuracy dropping considerably for birds over 80 dph, which is expected as song changes slow with age and eventually stabilize when birds reach around 90 dph. This could be a powerful tool to measure whether an experimental manipulation causes a bird’s song maturity to systematically lead or lag their true biological age, and which song features are most responsible for this effect, particularly as this method doesn’t require typical mature song from each subject bird to make its estimate, unlike[<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>], for instance.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5</label>
<caption><title>Age prediction with AVN features.</title>
<p><bold>a.</bold> Generalized additive model’s age predictions vs. true ages for 103 days of song recordings across 19 individual birds. Model predictions were generated using leave-one-bird-out cross validation. The grey line indicates where points would lie if the model were perfectly accurate. <bold>b.</bold> Partial dependence functions for each feature in the GAM model. The values of each feature along the x-axis map onto learned contributions to the age prediction along the y-axis. The GAM model’s prediction is the sum of these age contributions based on each day of song’s feature values, plus an intercept term.</p></caption>
<graphic xlink:href="593561v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2g">
<title>Measuring Song Imitation</title>
<p>So far, we’ve demonstrated how AVN’s features can be used to describe and compare adult and juvenile song production across experiments and research groups. While these features are sufficient to predict a bird’s song learning stage and to detect abnormalities in experimental groups, they don’t directly reflect song learning success. Zebra finches learn song by imitating an adult tutor, and this song learning is typically assessed by comparing a pupil bird’s song to its tutor’s, with higher similarity reflecting more successful learning ([<xref ref-type="bibr" rid="c32">32</xref>], but see [<xref ref-type="bibr" rid="c20">20</xref>]). Many methods for zebra finch song similarity scoring currently exist, however they all require either the manual identification of pupil and/or tutor motifs [<xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>], which limits the number of renditions that can be considered and has the potential to introduce experimenter bias, or require retraining or re-calibration when applied to new tutor-pupil pairs [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c38">38</xref>], which makes it impossible to directly compare learning outcomes across experiments.</p>
<p>To overcome these limitations, we have developed a novel similarity scoring system which doesn’t require any manual motif identification, or any retraining or re-calibration for new tutor-pupil comparisons. Our approach involves a deep convolutional neural network which is trained with a dataset of over 16,000 manually annotated syllables from 21 adult zebra finches from the UTSW colony. These syllables are presented to the model in triplets, consisting of a randomly selected ‘anchor’ syllable, a ‘positive’ syllable which belongs to the same type as the anchor, and a ‘negative’ syllable, which belongs to a different syllable type. The model learns to map spectrograms of syllables to an 8-dimensional embedding space, such that the anchor syllable’s embedding is closer to the positive’s embedding than to the negative’s. We use the trained network to compute the syllable embeddings for hundreds of syllables produced by a pupil bird and by its tutor and measure the similarity between their songs by calculating the Maximum Mean Discrepancy (MMD) between their syllable distributions (<xref rid="fig6" ref-type="fig">Figure 6a</xref>). Deep convolutional networks trained with triplet loss have been shown to be useful data compression steps before further downstream tasks are applied on their output, particularly in contexts where the data consists of many discrete classes (syllable types), and relatively few samples per class (renditions of each syllable type) [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>].</p>
<p>This process of learning a low dimensional representation of syllables, then calculating repertoire similarity by comparing distributions of syllables in the low dimensional space was inspired by [<xref ref-type="bibr" rid="c41">41</xref>], and is conceptually similar to the method proposed in [<xref ref-type="bibr" rid="c34">34</xref>], who use a variational autoencoder (VAE) to compute their syllable embeddings, followed by MMD to score syllable distribution similarity. As such, we’ve compared the similarity scoring results we obtain with our triplet loss model and MMD to [<xref ref-type="bibr" rid="c34">34</xref>]’s VAE and MMD, as well as each of these methods combined with another empirical distribution comparison metric: Earth Mover’s Distance (EMD).</p>
<p>We first validated these approaches with a dataset of 30 typical tutor-pupil pairs from the UTSW colony segmented using WhisperSeg, none of whom share a song tutor with any of the birds used to train the triplet loss model or VAE. The triplet loss model and VAE consistently yielded higher MMD and EMD dissimilarity scores between a pupil and unrelated bird, compared to a pupil vs. another bird with the same tutor (a ‘sibling’) and a pupil vs. its tutor, as expected (<xref rid="fig6" ref-type="fig">Figure 6b</xref>; <xref ref-type="fig" rid="fig6s1">Figure 6–figure supplement 1a-d</xref>). To more quantitatively compare the sensitivity of each of these approaches in distinguishing different types of relationships between typical zebra finches, we computed a contrast index, and tutor contrast index score for each pupil bird. The contrast index represents the difference in dissimilarity score between two subsets of syllables from the same pupil bird, and between the syllables of the given pupil bird and 3 randomly selected unrelated birds. We find that the VAE with MMD yields the highest contrast index score (contrast index = 0.967 +-SEM 0.009), and triplet loss with MMD as a close second (contrast index = 0.938 +-SEM 0.015) (<xref ref-type="fig" rid="fig6s1">Figure 6–figure supplement 1e</xref>). Notably, all the methods result in much higher average contrast index scores than Sound Analysis Pro [<xref ref-type="bibr" rid="c42">42</xref>] (contrast index = 0.156) and [<xref ref-type="bibr" rid="c36">36</xref>] (contrast index = 0.41).</p>
<p>Traditional contrast index doesn’t paint a complete picture of the reliability of a method for similarity scoring, however. Typically, similarity scores are computed between a pupil bird and its tutor to measure the pupil’s song learning success. In cases of successful song learning, there will be a high degree of similarity between a pupil’s song and their tutor’s, but these will still be much more different than two subsets of a pupil’s song compared to each other, as is used to calculate contrast index. A useful similarity scoring method must therefore have sufficient sensitivity among higher dissimilarity comparisons to distinguish between imitated song (a pupil vs. tutor comparison) and non-imitated song (a pupil vs. an unrelated bird). This is precisely what is measured by the tutor contrast index, which was highest for the triplet loss model and MMD (tutor contrast index = 0.316 +-SEM 0.035) (<xref ref-type="fig" rid="fig6s1">Figure 6–figure supplement 1f</xref>). Therefore, our triplet loss model combined with MMD is the most sensitive method for evaluating song learning.</p>
<p>The triplet loss model with MMD dissimilarity scores shows the same pattern and absolute range of scores for pupil vs. self, pupil vs. tutor, pupil vs. ‘sibling’ and pupil vs. unrelated bird comparisons for the UTSW dataset and for a dataset of 25 tutor-pupil pairs from the Rockefeller Song Library, despite these birds being recorded under different conditions from any of the birds used in model training (<xref rid="fig6" ref-type="fig">Figure 6b</xref>; <xref ref-type="fig" rid="fig6s2">Figure 6–figure supplement 2</xref>). This shows that the trained model generalizes well to birds from other research groups without the need for any additional fine tuning and thus can serve as a standard approach for the field. This approach also yields similarly high contrast indices and tutor contrast indices for both the UTSW and Rockefeller datasets (<xref ref-type="fig" rid="fig6s2">Figure 6–figure supplement 2</xref>, UTSW mean contrast index = 0.938 +-SEM 0.015, Rockefeller mean contrast index = 0.920 +-SEM 0.020, t-test p = 0.47; UTSW mean tutor contrast index = 0.316 +-SEM 0.035, Rockefeller mean tutor contrast index = 0.407 +-SEM 0.031, t-test p = 0.06). MMD scores produced by this model also agree better with expert human judgements of song similarity than do %similarity scores calculated with Sound Analysis Pro (<xref rid="fig6" ref-type="fig">Figure 6c</xref>; MMD vs. human expert absolute <italic>r</italic> = 0.80, <xref ref-type="fig" rid="fig6s2">Figure 6–figure supplement 2b</xref>, SAP %similarity vs. human expert absolute <italic>r</italic> = 0.33).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6</label>
<caption><title>Illustration and validation of AVN’s song similarity scoring method.</title>
<p><bold>a.</bold> Schematic of the similarity scoring method. A deep convolutional neural network is used to embed syllables in an 8-dimensional space, where each syllable is a single point, and similar syllables are embedded close together. The first 2 principal components of the 8-dimensional space are used for visualization purposes only here. The syllable embedding distributions for two random subsets of syllables produced by the same pupil on the same day have a high degree of overlap. The empirical distributions of all syllables from a pupil and his song tutor are less similar than a pupil compared to himself, but still much more similar than a pupil and a random unrelated bird. <bold>b.</bold> Maximum Mean Discrepancy (MMD) dissimilarity score distribution for comparisons between a pupil and itself (n=30 comparisons for UTSW, n = 25 for Rockefeller), a pupil and its tutor (n=30 comparisons for UTSW, n=25 for Rockefeller), two pupils which share the same tutor (aka pupil vs. ‘Sibling’ comparisons, n = 58 comparisons for UTSW, n = 64 for Rockefeller), and between two pupils who don’t share song tutor (aka pupil vs. unrelated bird, n = 90 comparisons for UTSW, n = 75 for Rockefeller). Calculated with a dataset of 30 typical tutor-pupil pairs from UTSW and 25 from Rockefeller. <bold>c.</bold> Correlation between MMD dissimilarity scores and human expert judgements of song similarity for 14 tutor-pupil comparisons from the UTSW colony (<italic>r</italic> = -0.80, p&lt;0.005). <bold>d.</bold> Tutor-pupil MMD dissimilarity scores for typical pupils from the UTSW colony (n = 30), typical pupils from the Rockefeller Song Library (n = 25), and FP1 KD pupils from the UTSW colony (n = 7) (One Way ANOVA F(2, 57) = 9.57, p &lt; 0.005. * Indicates Tukey HSD post hoc p-adj &lt; 0.05). <bold>e.</bold> MMD Dissimilarity score between birds at various age points across development, compared to their mature song recorded when the bird is over 90dph. Each point represents one comparison (n = 91 comparisons across 11 birds). Grey line is an exponential function fit to the data to emphasize the slowing of song maturation as birds approach maturity.</p></caption>
<graphic xlink:href="593561v3_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Using this method, we find that FP1 KD pupils have significantly higher MMD dissimilarity to tutor scores when compared to typical birds from either UTSW or Rockefeller (<xref rid="fig6" ref-type="fig">Figure 6d</xref>; One Way ANOVA F(2, 57) = 9.6, Tukey HSD FP1 KD vs typical UTSW p-adj &lt; 0.005, FP1 KD vs typical Rockefeller p-adj &lt; 0.005), showing that this method can be used to assess song learning outcomes in experimentally manipulated birds. We also used the model to look at how a bird’s song changes over development, by comparing song at multiple age points to a bird’s mature song. As expected, we find that birds gradually become more similar to their mature song over the course of development, and that the rate of this change slows as birds approach maturity (<xref rid="fig6" ref-type="fig">Figure 6e</xref>). Altogether, these tests showcase that this method (triplet loss model embedding with MMD) is more reliable at assessing tutor-pupil song similarity than existing methods, while also not requiring any manual motif identification or dataset-specific fine tuning. As a result, as with the AVN acoustic, timing, and syntax features, its scores are directly comparable across research groups, facilitating the quantitative comparison of song learning outcomes across studies.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Here, we have presented the AVN song analysis pipeline, which performs highly accurate syllable segmentation and syllable labeling. We have shown that this approach yields consistently high performance across multiple zebra finch colonies, suggesting that it can standardize and simplify large scale behavioral annotation across research groups, without the need for additional training or fine-tuning. The AVN labels are used to calculate syntax features which agree well with manual annotations, and which are sufficient to discriminate between typical birds and birds with known genetic disruptions. The AVN segmentations and raw song files are used to calculate timing features, which again are consistent across colonies, and which reflect a bird’s stage in song development. Standard acoustic features are also calculated for each AVN syllable type, which can be used to describe the overall acoustic properties of a bird’s song.</p>
<p>To showcase the utility of these song features, we presented how they can be used to compare multiple different song phenotypes, to test our hypothesis that the songs of FP1 KD birds would more closely resemble isolate birds’ compared to typical or deaf birds’ songs [<xref ref-type="bibr" rid="c43">43</xref>]. We also showed how these features can be used to create an interpretable model to predict a bird’s age within 7 days while their song is rapidly evolving from immature subsong to stable adult song. As more research groups use the AVN feature set to describe their birds’ song phenotypes, these analyses will only become more sensitive and powerful. Ultimately, we hope that these song features can be used to establish a comprehensive map of song phenotypes, which more closely link abnormal song phenotypes with the neural circuit dysfunctions underlying them.</p>
<p>Finally, we developed a novel similarity scoring system which outperforms existing methods in its sensitivity and fidelity to expert human judgements of song similarity, all without requiring any manual song annotation. Again, we expect this to be an invaluable tool for describing the nature and severity of song learning phenotypes in experimentally manipulated birds, where existing similarity scoring methods perform particularly poorly.</p>
<p>AVN is available to researchers as an open-source Python package and as a graphical application. The Python package allows researchers with some coding experience to take full advantage of the flexibility of these tools and integrate this pipeline into their data collection and processing workflows, while the application allows other researchers to easily annotate their songs and calculate AVN features with minimal coding, in a highly reproducible fashion.</p>
<p>Altogether, we see this pipeline as an example of the integration of deep learning tools and expertly curated features to automated behavior analysis without compromising the interpretability or generalizability of results. This feature set and annotation approach was designed with zebra finches in mind, but should be easily adaptable to other species with discrete syllables that can be clustered according to their acoustic features, such as Bengalese finches and Canaries, for example [<xref ref-type="bibr" rid="c17">17</xref>]. These species have more complex syllable sequencing than zebra finches and would therefore also benefit from additional syntax and timing features specific to their species. Additionally, while we’ve strived for a comprehensive set of features, it is possible that our 55-feature set will fail to reflect certain interesting song phenotypes that haven’t yet been observed. We hope that the open-source nature and extensive documentation of the AVN pipeline will allow and encourage researchers to contribute additional song features to the pipeline as they encounter such cases where the current feature set may be insufficient.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<p>The AVN documentation, AVN-GUI, and code necessary to produce all figures in this manuscript can be found through the following links:</p>
<p>AVN Documentation: <ext-link ext-link-type="uri" xlink:href="https://avn.readthedocs.io/en/latest/index.html">https://avn.readthedocs.io/en/latest/index.html</ext-link></p>
<p>AVN GUI: <ext-link ext-link-type="uri" xlink:href="https://avn.readthedocs.io/en/latest/AVN_GUI.html">https://avn.readthedocs.io/en/latest/AVN_GUI.html</ext-link></p>
<p>Code for Figures: <ext-link ext-link-type="uri" xlink:href="https://github.com/theresekoch/AVN_paper">https://github.com/theresekoch/AVN_paper</ext-link></p>
<sec id="s4a">
<title>Data Acquisition</title>
<p>A complete list of birds and the analyses in which they were included can be found in Supplementary Table 1.</p>
<sec id="s4a1">
<title>UTSW Dataset</title>
<p>Many birds included in this study were previously recorded and analyzed in [<xref ref-type="bibr" rid="c43">43</xref>]. This includes 7 birds which were injected with a pscAAV-GFP-shFoxP1 virus before exposure to a song tutor, leading to disrupted songs (referred to as FP1 KD birds in this manuscript and FP1-KD SE in [<xref ref-type="bibr" rid="c43">43</xref>]). 8 birds were included in this group in the previous paper. One was omitted from this manuscript because it exhibited completely typical song, likely due to weak viral expression. A further 10 birds which were injected with a control virus before exposure to a song tutor (Ctrl SE in [<xref ref-type="bibr" rid="c43">43</xref>]), and 10 which were injected with the pscAAV-GFP-shFoxP1 after tutor song exposure (FP1-KD BI in [<xref ref-type="bibr" rid="c43">43</xref>]) are included in the current study. Both of these groups exhibit species-typical song production and are included in the ‘typical’ group in this study. Finally, 8 additional birds which were raised in isolation from an adult song model until they were at least 90 days post hatch (‘Full isolates’ in the FP1 paper and ‘Isolates’ in the current study) were included in this study, for a total of 35 birds. See [<xref ref-type="bibr" rid="c43">43</xref>] for more information on viral injections and rearing conditions.</p>
<p>An additional 8 adult isolate birds, 37 typically reared adult birds, and 10 juvenile birds were recorded for this study. For isolate birds, fathers were removed from breeding cages before the young reached 12dph. These young remained housed with their mother and siblings in a room containing only other isolate breeding cages, until they were weaned between 40 and 60dph, at which point they were housed individually, with auditory but no visual access to other isolate reared males. Typically, reared and juvenile birds were raised in our main colony room which contained about 55 breeding pairs. They had unlimited access to their father’s song in their home cages, until they were weaned between 40 and 60dph, at which point they were housed in group cages with other males.</p>
<p>Recordings were obtained by individually housing birds in sound attenuating chambers. They were continuously recorded using Sound Analysis Pro 2011 [<xref ref-type="bibr" rid="c32">32</xref>]. All birds were placed on a 14h:10h day:night cycle and provided ad libitum access to food, water and grit. All procedures were performed in accordance with protocols approved by the Animal Care and Use Committee at UT Southwestern Medical Center.</p>
</sec>
<sec id="s4a2">
<title>Additional Song Data</title>
<p>In addition to the birds recorded at UTSW, this study includes recordings of 25 pupils and 6 tutors from the Rockefeller University Field Research Center Song Library [<xref ref-type="bibr" rid="c20">20</xref>], 5 juvenile birds from Duke University [<xref ref-type="bibr" rid="c9">9</xref>], and 5 early deafened and 4 sham deafened birds from Hokkaido University [<xref ref-type="bibr" rid="c33">33</xref>]. See citations for more information on rearing and recording conditions.</p>
</sec>
</sec>
<sec id="s4b">
<title>Manual Song Annotation</title>
<p>A random subset of 30 song files from a single day of recording were annotated for each of 35 adult birds from UTSW, and 15 song files were annotated for each of 25 adult birds from the Rockefeller Song Library. Manual annotation was performed using the <italic>evsonganaly</italic> application in MATLAB [<xref ref-type="bibr" rid="c44">44</xref>], and involved 1) amplitude threshold syllable segmentation with a threshold selected for each song file based on visual inspection of the amplitude trace and spectrogram, 2) manual correction of erroneous syllable onsets or offsets, and 3) assignment of syllable labels to each syllable based on visual inspection of the spectrogram. All annotations were prepared by one of two expert annotators. These annotators consulted with each other in cases of ambiguous syllable segmentation or labeling and used the same existing set of labeled songs as a reference. Both annotators agreed to adopt the more conservative approach of splitting syllables into smaller segments in cases of ambiguous segmentation and assigning different syllable labels in cases of ambiguous labeling.</p>
<p>For all applications except training TweetyNet [<xref ref-type="bibr" rid="c5">5</xref>], segments that reflect cage noise were dropped from the annotations based on visual inspection of the spectrograms. A second set of annotations were made which retained noise segments, labeling them as such, with all syllables and calls labeled as simply ‘vocalizations’ for the purpose of training TweetyNet.</p>
</sec>
<sec id="s4c">
<title>Segmentation</title>
<sec id="s4c1">
<title>Amplitude Segmentation</title>
<p>Amplitude segmentation was performed using the ‘RMSEDerivative’ class in the AVN Python package’s segmentation module. Each song file is bandpass filtered between 200 Hz and 9000 Hz, then the root mean square energy (RMSE) of each audio frame is computed with a hop length of 512 samples, and a frame length of 2048 samples. The RMSE values of each song file are normalized, then the RMSE’s first derivative is compared against user-specified thresholds. A syllable onset is identified as a positive crossing of the ‘onset’ threshold. Syllable offsets tend to be marked by more gradual changes in RMSE compared to syllable onsets, making it difficult to identify them consistently. To mitigate this, we perform onset to onset segmentation with this method, meaning each segment included a song syllable, and the silent gap that immediately followed it. If a syllable onset is not followed by another onset within 300ms (as in the end of a song bout), the offset is set as the first negative crossing of an ‘offset’ threshold after the syllable onset.</p>
<p>In keeping with the TweetyNet and WhisperSeg segmentation methods which don’t require per-bird parameter adjustments, the same ‘onset’ and ‘offset’ thresholds were used for all birds in the dataset. These thresholds were selected using AVN’s segmentation.Utils.threshold_optimization_many_birds() function, which compares the F1 scores relative to manual segmentation obtained with multiple different threshold values to identify the threshold value that results in the lowest mean F1 score across all 35 UTSW birds used for amplitude segmentation validation. The same thresholds selected based on the UTSW birds were used to segment the 25 Rockefeller Song Library birds, as a test of the generalization of this method without the need for manual segmentations.</p>
</sec>
<sec id="s4c2">
<title>TweetyNet</title>
<p>The <italic>vak</italic> Python package was used to prepare datasets for, train, and generate segmentation predictions with the TweetyNet model [<xref ref-type="bibr" rid="c5">5</xref>]. TweetyNet is a deep neural network consisting of a block of convolutional layers followed by a bidirectional long short-term memory (LSTM) layer. The model takes a 1s spectrogram of song as input, and labels each frame within that spectrogram. TweetyNet was designed for simultaneous syllable labeling and segmentation, in which case it would label each frame of the spectrogram with a syllable label or as silence. However, to make this model generalize to new birds without any additional training data, we instead trained TweetyNet to label each frame as a vocalization, silence, or noise (common sources of noise include the bird hopping around its cage and flapping its wings), rather than a more specific syllable type. When trained with such data from many birds, it can learn to distinguish vocalizations from noise and silence in a sufficiently general manner that the model can be applied to previously unseen individuals.</p>
<p>Manually annotated song files with label classes ‘noise’ and ‘vocalization’ were used to train TweetyNet in a leave-one-out cross validation scheme, meaning the model was trained with data from all but one bird and tested on the withheld bird for each of the 35 birds in the UTSW dataset. A model trained with data from all 35 UTSW birds was used to segment the 25 validation birds from the Rockefeller Song Library dataset, to test the model’s ability to generalize to new colonies. Full model training and prediction procedures can be found in this paper’s accompanying github repository. For more information on the TweetyNet model itself, see [<xref ref-type="bibr" rid="c5">5</xref>].</p>
</sec>
<sec id="s4c3">
<title>WhisperSeg</title>
<p>WhisperSeg is an instance of the Whisper Transformer model which was pre-trained for automatic human speech recognition, and fine-tuned for animal voice activity detection with a multi-species animal vocalization dataset [<xref ref-type="bibr" rid="c6">6</xref>]. It takes a spectrogram representation of up to 2.5s of song as input and outputs the indices of vocalization onsets and offsets in the spectrogram. These indices are then converted to timestamps, and a consistent labeling scheme for an entire song file is achieved through a ‘majority-vote’ post-processing step across overlapping 2.5s song segments. Syllable segmentation was performed using the <italic>whisperseg-large-ms-ct2</italic> model, with hyperparameters optimized for zebra finch song segmentation, based on [<xref ref-type="bibr" rid="c6">6</xref>]. This was done using scripts shared in this paper’s accompanying github repository, which must be installed separately from AVN according to the instructions provided in the repository. For more information on the WhisperSeg model and its training, see [<xref ref-type="bibr" rid="c6">6</xref>].</p>
</sec>
<sec id="s4c4">
<title>Validation</title>
<p>Segmentation methods were compared on the basis of their precision, recall, and F1 scores relative to manual annotations.
<disp-formula>
<graphic xlink:href="593561v3_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where a true positive is a syllable onset in the automatic segmentation that is within 10ms of a syllable onset in the manual annotation, a false positive is a syllable onset in the automatic segmentation that doesn’t match with a syllable onset in the manual annotation within 10ms, and a false negative is a syllable onset that is present in the manual annotation which doesn’t match with an automatic segmentation onset within 10ms. When determining onset alignments, we ensure that each syllable onset in the manual annotation can only ‘match’ with a single syllable onset in the automatic segmentation, and vis-versa. This was done using AVN’s ‘segmentation.Metrics.calc_F1()’ function. Across all 3 metrics, scores closer to 1 indicate better agreement between automatic and manual segmentations. These same features were calculated for syllable offsets as well, but with an allowance of 20ms rather than 10ms, to account for the greater variability in exact offset segmentation across all methods tested.</p>
<p>To further examine the temporal precision of each method relative to manual annotation, we also calculated the time difference in milliseconds between matched syllable onsets and offsets between automatic segmentations and manual annotations. This was done using AVN’s ‘segmentation.Metrics.get_time_delta_df()’ function.</p>
</sec>
</sec>
<sec id="s4d">
<title>Labeling</title>
<sec id="s4d1">
<title>UMAP Dimensionality Reduction</title>
<p>UMAP dimensionality reduction [<xref ref-type="bibr" rid="c21">21</xref>] is performed on spectrograms of syllables, prior to HDBSCAN clustering for label assignment [<xref ref-type="bibr" rid="c22">22</xref>]. First, spectrograms of each segmented syllable are produced. The audio is first bandpass filtered between 500Hz and 15KHz, then amplitude normalized independently for each syllable rendition. The short term Fourier transform of the normalized audio for each syllable is computed with a window length of 512 samples and a hop length of 128 samples. The resulting amplitude spectrogram is converted to decibels using the librosa ‘amplitude_to_db()’ function [<xref ref-type="bibr" rid="c45">45</xref>]. The db-scaled spectrogram is then padded to match the dimensions of the longest syllable in a given bird’s dataset, or clipped to 870ms if it exceeds that duration. This limit is very generous, and only ever applies to segmentation errors, but it is necessary to avoid memory issues during UMAP computation. Normalized spectrograms are then flattened from an array to a single long vector, and the vectors corresponding to each spectrogram are concatenated into an array separately for each bird. This spectrogram array is used to calculate the UMAP embeddings of each syllable using the UMAP Python package’s ‘UMAP()’ function. This approach is based on [<xref ref-type="bibr" rid="c17">17</xref>].</p>
<p>At a high level, UMAP dimensionality reduction involves constructing a graphical representation of the syllable set, where each syllable spectrogram can be thought of as a point in high dimensional space which is connected to other syllables near it by edges. These edges are weighted based on the distance between points and the local density of those data points. The high dimensional graph is then projected into lower dimensions in a way that best preserves its overall structure.</p>
<p>UMAP dimensionality reduction can be a useful initial step when attempting to cluster high-dimensional data points because many clustering algorithms, especially density-based clustering algorithms such as HDBSCAN can suffer from the ‘curse of dimensionality’. When clustering spectrograms directly, each pixel in the spectrogram is a dimension, meaning each spectrogram exists as a point in a space with thousands of dimensions. In such a high dimensional space, points will be very sparsely distributed, even if the spectrograms appear largely very similar. As a result, it is very difficult to detect regions of higher point density to serve as the basis of clusters. Reducing the dimensionality of the dataset forces points closer together, such that regions of high density separated by lower density can be more easily detected. UMAP is particularly adept at emphasizing local clusters in high dimensional data because of how its initial embedding graph is constructed.</p>
<p>UMAP parameters were selected based on suggestions in the UMAP-learn documentation for clustering using UMAP embeddings and based on visual inspection of plots and labeling outcomes compared to manual annotations for birds from the UTSW dataset.</p>
</sec>
<sec id="s4d2">
<title>HDBSCAN Clustering</title>
<p>The “Hierarchical Density-based Spatial Clustering of Applications with Noise” (HDBSCAN) clustering algorithm [<xref ref-type="bibr" rid="c22">22</xref>] was applied to the UMAP embeddings of syllable spectrograms for each bird independently, in order to assign syllable labels. This method was selected based on the results in [<xref ref-type="bibr" rid="c17">17</xref>] for clustering Bengalese finch syllables, a species closely related to zebra finches.</p>
<p>Essentially, HDBSCAN works by calculating the ‘mutual reachability’ distance between points in the UMAP space, based on the distance between them and their local densities. These mutual reachability distances serve as edges connecting nodes (points representing individual song syllables) in a graph, which are then pruned to obtain a minimum spanning tree (a graph using the minimum number of total edges to connect all points). The minimum spanning tree is then converted to a hierarchy by sorting the edges on the basis of their mutual reachability scores. Clusters of points are identified by defining a minimum cluster size and selecting the clusters that persist over the longest span of the hierarchy. As with the UMAP parameters, the same HDBSCAN hyperparameter set was used for all birds. The hyperparameter values were selected based on v-measure scores and visual inspection of confusion matrices for WhisperSeg segments compared to manual annotations for birds from the UTSW colony.</p>
</sec>
<sec id="s4d3">
<title>Validation</title>
<p>Syllable labeling was assessed by comparing automatically assigned syllable labels to manual annotations. Automatically labeled segments first had to be aligned to the manual annotations to identify pairs of labels in the automatic clustering and manual annotation that referred to the same vocalization. This was done using the same method described in the <italic>Segmentation Validation</italic> section, in which syllable onsets are uniquely matched to their closest counterpart across segmentation methods, this time up to a maximum distance of 100ms. False positive syllable detections (i.e. syllable present in the automatic segmentation without a manual annotation counterpart) are assigned to their own manual annotation category (‘x’), and False negative syllables detections (i.e. syllables present in the manual annotation without an automatic segmentation counterpart) are assigned to their own cluster (‘1000’) for the purposes of visualization and quantification.</p>
<p>Once syllables have been aligned between automatic segmentation and manual annotations, the HDBSCAN cluster labels are compared to manual labels for each bird to construct a confusion matrix, which gives the number of syllables in each HDBSCAN cluster that carry each of the possible manual labels. The confusion matrix values can then be used to compute homogeneity, completeness, and v-measure scores, to evaluate the correspondence between HDBSCAN labels and manual annotations for each bird. Homogeneity measures the extent to which syllables with the same AVN label also carry the same manual annotation label, completeness measures the extent to which syllables with the same manual annotation label also carry the same AVN label, and the V-measure is the harmonic mean of these two scores.
<disp-formula>
<graphic xlink:href="593561v3_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Where <inline-formula><inline-graphic xlink:href="593561v3_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the conditional entropy of the manual labels given the cluster labels, <inline-formula><inline-graphic xlink:href="593561v3_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the entropy of the manual labels, and vis-versa. In all cases, a higher score indicates better correspondence between clusters and manual labels, with a maximum possible score of 1, and minimum score of 0.</p>
<p>As UMAP embeddings are inherently stochastic, the subsequent HDBSCAN labels can vary over repeated runs with the exact same syllable dataset, depending on the random initialization of the UMAP embedding. To examine how consistent the labels are across different random UMAP initializations, we calculated the v-measure scores for 35 birds from the UTSW colony across 30 different random initializations, and calculated the standard deviation of the v-measure scores for each bird, as a measure of the outcome variability due to UMAP’s stochasticity.</p>
</sec>
</sec>
<sec id="s4e">
<title>Syntax Features</title>
<sec id="s4e1">
<title>Syntax Raster Plot</title>
<p>Beginning with a table of all AVN labels, syllables that are preceded and followed by a period of silence longer than 200 ms are removed, as they likely reflect calls produced outside of song. Song bouts are then identified as sequences of at least two syllables that are separated by silent gaps no longer than 200ms. These bouts are aligned based on a user-specified alignment syllable, such that the first instance of the alignment syllable is in the same position across all bouts. This alignment is important as bouts typically begin with a variable number of introductory notes, which will obscure patterns in syllable sequence across bouts when they are not aligned to the first non-introductory note syllable. After alignment, bouts are ordered such that bouts with similar sequences after the alignment syllable are together in the final plot, which also helps emphasize patterns across bouts. This is done using AVN’s syntax.Syntax_Data.make_syntax_raster() function. See avn’s documentation for additional information and examples.</p>
</sec>
<sec id="s4e2">
<title>Syllable Transition Matrix</title>
<p>As with syntax raster plots, syllables that are preceded and followed by more than 200ms of silence are dropped from the AVN labels as they likely reflect calls produced outside of song. Silent gaps longer than 200ms and file bounds are then added as states to the AVN label sequence. All syllable transitions between AVN labels are then counted, including transitions to and from periods of over 200ms of silence; meaningful transitions as they reflect the beginnings or ends of song bouts. Transitions to and from file bounds are ignored, as these are artifacts of the recording and don’t reflect meaningful behavioral states. The transition counts are then divided by the total number of renditions of the first syllable type in the transition to get the conditional probability of the second syllable, given the first syllable. This is done using AVN’s syntax.Syntax_Data.make_transition_matrix() function.</p>
</sec>
<sec id="s4e3">
<title>Syntax Entropy Rate</title>
<p>Syntax stereotypy is quantified using the entropy rate of the syllable transition matrix.
<disp-formula>
<graphic xlink:href="593561v3_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="593561v3_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the probability of transitioning from initial syllable type <italic>i</italic> to following syllable type <italic>k</italic>, and <inline-formula><inline-graphic xlink:href="593561v3_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> the probability of syllable type <italic>i</italic> occurring, regardless of what syllable precedes or follows it. An entropy rate approaching 0 indicates that all transitions are highly predictable. The maximum possible entropy rate score is <inline-formula><inline-graphic xlink:href="593561v3_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>N</italic> is the number of syllable types in the bird’s repertoire plus one to account for silence as a possible state. To directly compare scores between birds without being biased by the number of syllable types in their song (a feature which depends strongly on the number of syllable types present in their tutor’s song), we divide the entropy rate score by <inline-formula><inline-graphic xlink:href="593561v3_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> such that is it now bounded between 0 and 1. This is done using AVN’s syntax.Syntax_Data.get_entropy_rate() function.</p>
</sec>
<sec id="s4e4">
<title>Repetition Bouts</title>
<p>A repetition bout refers to every instance in which a syllable is produced, either a single time or multiple times in a row. For example, in the syllable sequence <italic>abcaaabc</italic>, syllable <italic>a</italic> has 2 repetition bouts, one of length one, meaning the syllable was produced without being repeated, and one of length 3, meaning the syllable was produced 3 times in a row. The number and length of repetition bouts is calculated for each syllable type in a bird’s repertoire. The mean repetition bout length and coefficient of variation (CV) of repetition bout length is then calculated for each syllable type.</p>
<p>To facilitate comparisons across birds, which have different numbers of syllable types, the mean repetition bout length and CV of repetition bout length for the syllable type with the highest mean repetition bout length are selected to represent the bird’s overall tendency to repeat syllables, excluding syllable types that reflect putative calls or introductory notes. Typical zebra finches often repeat calls or introductory notes, but rarely repeat song syllables, so looking at the repetition of song syllables is more informative when detecting or comparing birds with abnormal syntax. That said, in certain experiments repetition bout features of calls or introductory notes may be of greater interest, in which case they can also be specifically identified using AVN.</p>
<p>An AVN syllable type is considered a putative introductory note if it 1) is no less than 5% less likely to be transitioned to from silence than the syllable type most commonly transitioned to from silence, meaning it tends to occur at the start of a vocalization bout, and 2) it has a single dominant transition to a syllable type other than itself which is not silence, meaning that after a number of repetitions, it is eventually followed by a predictable next syllable type, which should reflect the start of a motif. These criteria were determined based on inspection of the syntax properties of introductory notes in manual song annotations. An AVN syllable type is considered a putative call if it is 1) not a putative introductory note, and 2) is produced in a bout of one or two syllables preceded and followed by at least 200ms of silence in more than ⅓ of all utterances. This criterion was again determined based on visual inspection of manual song annotations.</p>
</sec>
</sec>
<sec id="s4f">
<title>Song Timing Features</title>
<sec id="s4f1">
<title>Syllable and Gap Duration Entropy</title>
<p>A syllable duration distribution is constructed based on the segment durations output by WhisperSeg for each bird. A histogram of the <inline-formula><inline-graphic xlink:href="593561v3_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> of syllable durations is calculated, with 50 evenly spaced bins ranging from -2.5 to 0. As in [<xref ref-type="bibr" rid="c30">30</xref>] the log of syllable durations are used because the syllable duration distributions of juvenile birds are roughly exponential, and therefore linear in log space. Histograms are normalized to produce a probability density function across syllable durations. The entropy of this distribution is then calculated as
<disp-formula>
<graphic xlink:href="593561v3_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="593561v3_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the density the <italic>i</italic>th bin in the histogram, and <italic>N</italic> is the total number of bins (50, in this case). The resulting entropy can range from 0 to 1, with higher scores indicating less predictable syllable durations, consistent with the songs of immature birds.</p>
<p>Entropy is calculated similarly for silent gap durations, where gaps durations are defined as the time difference between a syllable offset and the immediately following syllable onset, up to a maximum duration of 200 ms. A log transform was not applied to gap durations before constructing a histogram with 20 10ms bins.</p>
</sec>
<sec id="s4f2">
<title>Rhythm Spectrograms</title>
<p>Rhythm spectrograms are a visualization of the strength and stereotypy of rhythmic patterns in a bird’s song, generated by concatenating the rhythm spectra of multiple song bouts, as first proposed in [<xref ref-type="bibr" rid="c31">31</xref>]. A song bout’s rhythm spectrum is the spectrum of the first derivative of its amplitude. If a song’s amplitude has consistent repeating fluctuation patterns (as we expect for a bout composed of multiple repetitions of the same stereotyped motif), then its spectrum will exhibit harmonic banding patterns. If, by contrast, there are no repeating rhythms in the song’s amplitude, the rhythm spectrum will have a more even spread of energy across frequency bands. To detect these harmonic patterns more easily in the rhythm spectrogram, all rhythm spectrograms are plotted as the rolling average of 10 song bouts, smoothing out some bout-to-bout variation in the spectra to make harmonic bands more obvious.</p>
<p>To ensure consistent dimensions and resolution across song bouts, the rhythm spectrum is calculated for segments of song of a fixed duration, rather than complete song bouts. This also eliminates the need for any segmentation and labeling of song files to identify bouts, making this timing analysis method completely independent of possible segmentation and labeling errors. Each .wav file is broken into multiple 3-second-long frames, with a hop length of 0.2 seconds. The 3 frames with the highest total amplitude (ie the 3 windows containing the most vocalizations) from each file have their rhythm spectra calculated, and the mean of their spectra is taken as the rhythm spectrum for that file. Because of this windowing system, only files at least 3 + 3 x 0.2 seconds in duration can be windowed this way, so shorter .wav files are ignored.</p>
<p>The derivative of the amplitude of each frame is centered at 0 by subtracting the mean value, then multiplied by a Hanning window to reduce spectral leakage when calculating the spectrum. The transformed amplitude derivative is then padded to a total length of 100000 frames, resulting in a smoother spectrum with more interpolated values. A bandpass filter is then applied, keeping only frequency components above 1 Hz and below 500Hz, as these are the frequencies consistent with typical zebra finch motif and syllable periods. Finally, the real component of the Fourier transform is calculated, constituting the frame’s ‘rhythm spectrum’. Only portions of the rhythm spectrum corresponding to frequencies between 0 and 30Hz are included in rhythm spectrograms and downstream feature calculations, as this is the range with the strongest harmonic banding for typical zebra finches. This is all done using AVN’s avn.timing.RhythmAnlysis.make_rhythm_spectrogram() function.</p>
</sec>
<sec id="s4f3">
<title>Rhythm Spectrum Entropy</title>
<p>We quantify the strength of the harmonic content of a bird’s rhythm spectrum (i.e. the strength of its rhythm) by calculating the Wiener Entropy of the mean rhythm spectrum across bouts. Wiener entropy is a common acoustic feature used to assess the harmonic nature of zebra finch syllables, with scores near 0 reflecting signals with little harmonic structure, and scores ranging to negative infinity for signals with more harmonic structure.
<disp-formula>
<graphic xlink:href="593561v3_ueqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This is calculated using AVN’s avn.timing.RhythmAnalysis.calc_rhythm_spectrogram_entropy() function.</p>
</sec>
<sec id="s4f4">
<title>Peak Frequency Variability</title>
<p>Whereas the rhythm spectrum entropy measures the overall strength of the rhythms in a set of songs, the peak frequency variability reflects the consistency of the rhythm across multiple song renditions. The exact spacing of the harmonics in a rhythm spectrogram depends on the shape of the amplitude trace of the bird’s motif. It isn’t obvious how different motifs and motif lengths affect the banding pattern of the rhythm spectrogram, so it doesn’t make sense to compare the appearance of different birds’ rhythm spectra beyond the prominence of harmonic bands. Likewise, the frequency of the harmonic band with the highest magnitude doesn’t carry any special meaning. However, in a very stereotyped bird, that harmonic band will be consistent across songs. If the bird sings its song slightly faster or slower the band can shift slightly in the frequency domain. So, we measure a bird’s rhythm stereotypy by looking at the variability of the frequency with the highest magnitude across song files (the peak frequency).</p>
<p>In practice the frequency band with the highest energy can jump between harmonic bands across files, even while the overall timing is largely unchanged, so to truly capture fluctuations in the underlying rhythms in a bird’s song, we restrict the range of ‘peak frequency’ values to a 3Hz band about the median peak frequency across bouts. The coefficient of variation of the peak frequency within this range is calculated as the peak frequency variability. This is done using AVN’s avn.timing.RhythmAnalysis.calc_peak_freq_cv() function.</p>
</sec>
</sec>
<sec id="s4g">
<title>Acoustic Features</title>
<p>Goodness of pitch, Mean Frequency, Weiner Entropy, Amplitude, Amplitude Modulation, Frequency Modulation, and Pitch were all calculated using AVN in Python, with implementations based on the Sound Analysis Tools for MATLAB [<xref ref-type="bibr" rid="c32">32</xref>]. Each of these features is calculated for each frame in a spectrogram, resulting in a time series of values. We summarize these time series of varying lengths by taking the mean value of each feature for each AVN segmented syllable. We then calculate the mean and coefficient of variation of the mean feature values for each syllable type according to their AVN labels. As each bird has a different number of syllable types, we need to further summarize these features so that we have a consistent set of values for comparisons across individuals. To do this, we take the syllable type with the minimum, maximum and median mean value and CV for each feature. This results in 6 values summarizing the variability and absolute values of each feature for each bird. Across 7 acoustic features plus syllable duration, this results in a total set of 48 features.</p>
</sec>
<sec id="s4h">
<title>Linear Discriminant Analysis</title>
<p>We fit 3 different linear discriminant analysis models in this paper. One to discriminate between typical zebra finches and isolate zebra finches, one to discriminate between typical zebra finches and deaf zebra finches, and one to discriminate between all 3 groups at once. For each of these models, L1 regularization was used to reduce the number of features considered in the model. This improves both the generalization of the model, and its interpretability by focusing on just a subset of the most informative features. L1 feature selection was performed considering all AVN features from each bird, excluding amplitude and amplitude-modulation features, as these were found to vary according to recording conditions. Once the feature set was reduced, classification accuracy of the models was tested using a stratified k-folds cross validation approach. Classification accuracy is the fraction of data points in the withheld test set which were correctly classified. Plotted LDA values and feature weights were obtained from a model trained with the complete dataset. This was all done using the scikit-learn Python package [<xref ref-type="bibr" rid="c46">46</xref>].</p>
</sec>
<sec id="s4i">
<title>Age Prediction Generalized Additive Model</title>
<p>The full AVN feature set was calculated for 19 individual birds across 103 age points, using songs produced within the first 4 hours after lights on. Juvenile birds have been shown to have more variable songs in the early morning [<xref ref-type="bibr" rid="c9">9</xref>], which exaggerates the difference between immature and mature song and improves the model’s ability to predict a bird’s age, compared to features calculated with a full day of songs, or songs produced in the afternoon.</p>
<p>Before fitting a Generalized Additive Model (GAM) for age prediction, we pruned our feature set to include only the most informative features. We first excluded all amplitude and amplitude-modulation features, as these were strongly affected by recording conditions and differed between colonies. We then calculated the mutual information between each remaining feature and age, considering 43 age points from 12 individual birds. We automatically excluded all features with a mutual information score lower than 0.05 (20/44 features). We further refined this feature set by performing forward feature selection with our 43 age point dataset. This means we iteratively added individual features to the model based on which additional feature resulted in the lowest mean squared error (MSE) predictions in a bird-fold cross validation. We then selected the feature set with the lowest overall MSE, further reducing our feature set to just 7 features.</p>
<p>A GAM model with the 7 selected features was used to predict bird’s ages in a leave-one-bird-out (aka bird-fold) cross validation scheme. Here, we included the 43 age points from 12 individual birds used for feature selection, plus an additional test set of 60 age points from 7 individual birds. We saw no significant difference in model performance between this test set and the dataset used for feature selection, so we pooled results across these groups.</p>
<p>To investigate the contribution of each feature to the overall model, we fit a model with all birds in the dataset, and used the pyGAM Python package [<xref ref-type="bibr" rid="c47">47</xref>] to extract the partial dependence functions for each feature.</p>
</sec>
<sec id="s4j">
<title>Similarity Scoring</title>
<sec id="s4j1">
<title>Data Preparation</title>
<p>Spectrograms of manually segmented and labeled syllables from 21 adult zebra finches from the UTSW colony were used for model training. All validation was performed with spectrograms of WhisperSeg segmented syllables from a test set of 30 tutor-pupil pairs from UTSW and 25 tutor pupil pairs from the Rockefeller Song Library [<xref ref-type="bibr" rid="c20">20</xref>], none of which were included in training. For the triplet loss model these spectrograms were normalized for amplitude, then clipped or padded to a uniform duration of 180ms. To reduce computational costs, all frequency bands below 2kHz and above 6kHz are discarded. For the variational autoencoder model, spectrograms were computed for each syllable then interpolated to consistent time and frequency representations; 128 frequency bands ranging from 400 Hz to 10kHz, and 128 time bins. Syllables longer than 300ms were discarded. Short syllables were stretched in time by a factor of <inline-formula><inline-graphic xlink:href="593561v3_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, then symmetrically zero padded to a size of 128 time bins, such that all syllables’ spectrograms had the same dimensions.</p>
</sec>
<sec id="s4j2">
<title>Triplet Loss Model Architecture</title>
<p>The proposed neural network model is composed of 5 convolutional layers alternating with 4 ‘Multiscale Analysis Modules’ (MAMs), followed by a global pooling layer and 3 fully connected linear layers (<xref ref-type="fig" rid="fig6s3">Figure 6–figure supplement 3</xref>). This architecture is based on the model proposed in [<xref ref-type="bibr" rid="c39">39</xref>], which was used for species classification with field recordings of bird, frog and toad vocalizations. The first convolutional layer consists of 32 3 x 3 kernels, with the 4 subsequent convolutional layers consisting of 64 3 x 3 kernels with a stride length of 2 along the frequency axis, resulting in down sampling by a factor of 2 along that dimension. Each MAM is composed of 4 parallel strands, each processing the data at different scales. The first strand consists of a single convolutional layer with 32 1 x 1 kernels, the second, third and fourth strands start with a 32 filter 1 x 1 kernel convolutional layer, followed by a convolutional layer with 32 3x3, 5x5, and 7x7 kernels, respectively. The output of each of these strands is concatenated channel-wise, resulting in a 128-channel representation of the data which is passed to the next layer. This parallel strand organization allows the model to perform feature extraction at multiple different scales without increasing the depth of the model, saving computational cost and limiting potential overfitting. This approach was first proposed in [<xref ref-type="bibr" rid="c48">48</xref>]. The ReLU activation function is used after every layer [<xref ref-type="bibr" rid="c49">49</xref>]. The output of the final linear layer is an 8-dimensional vector, which represents an input syllable’s embedding. These vectors are normalized to have a length of 1, such that all embeddings lie on a unit 8-dimensional hypersphere.</p>
</sec>
<sec id="s4j3">
<title>Triplet Loss Model Training</title>
<p>The model is trained using dynamic triplet loss with triplet mining. Triplet loss involves presenting the model with batches of triplets, where each triplet consists of an anchor, a positive, and negative syllable. The anchor and positive syllables carry the same manual annotation label from the same bird, and the anchor and negative syllables carry different labels, either from the same bird or from an unrelated bird. The loss function to be minimized is:
<disp-formula>
<graphic xlink:href="593561v3_ueqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>N</italic> is the total number of possible triplets in training, <inline-formula><inline-graphic xlink:href="593561v3_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the embedding of the anchor in triplet <inline-formula><inline-graphic xlink:href="593561v3_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> <italic>i</italic>, is the embedding of the positive, <inline-formula><inline-graphic xlink:href="593561v3_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the embedding of the negative, and αis a margin parameter. If the positive is closer to the anchor than the negative by at least , α the loss for that triplet is 0.</p>
<p>During training, many randomly sampled triplets will already yield a loss of 0, and therefore will not lead to any change in the model. As a result, such triplets are not presented to the model during training. The remaining triplets which result in a positive loss value can be divided into two groups, hard triplets and semi-hard triplets. Hard triplets are cases where <inline-formula><inline-graphic xlink:href="593561v3_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, and semi-hard triplets are cases where <inline-formula><inline-graphic xlink:href="593561v3_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Hard triplets result in higher loss values, and therefore larger model weight updates, which can result in unstable training. Previous studies have shown that, as a result, training with semi-hard triplets alone can lead to faster and more stable model convergence [<xref ref-type="bibr" rid="c40">40</xref>]. We found that we achieved the best model performance when training with a ratio of 75 semi-hard triplets : 25 hard triplets, so this ratio was used to train the final model. A forward pass of the model is performed for each mini batch in training to determine the ‘hardness’ of all possible triplets from that batch. Triplets are then sampled according to the specified ratio of semi-hard to hard triplets, and are presented to the model for training.</p>
<p>Our approach is said to be <italic>dynamic</italic> triplet loss because the value of the margin parameter α is updated dynamically over the course of training. The value is initially set to 0.1 and increased stepwise by 0.2 every time a training epoch had fewer than 2,500 hard or semi hard triplets per batch on average, up to a maximum value of 0.7. This allows the model to begin learning an easier task, of separating syllables of different classes by a smaller margin. As the margin increases, the task gradually becomes more difficult, leading to more stable model convergence compared to starting with a higher margin value. Each time the margin parameter is increased, triplets that previously resulted in a loss of 0 can become semi-hard triplets, meaning the set of triplets presented to the model also expands over the course of training. The initial margin value, maximum margin value, margin step size and non-zero triplet threshold were all determined empirically. The weight optimization was performed with the Adam optimizer, with weight decay of 0.0001 [<xref ref-type="bibr" rid="c50">50</xref>].</p>
<p>Ultimately, this dynamic triplet loss training constitutes a form of deep metric learning, where high dimensional inputs (e.g., spectrograms of syllables) are mapped onto a lower-dimensional space where the similarity between samples is proportional to the distance between them. Training with triplets is advantageous in a context where the total amount of labeled training data is limited, as the number of possible triplets is proportional to the cube of the number of training samples.</p>
</sec>
<sec id="s4j4">
<title>VAE</title>
<p>In addition to using this triplet loss model, we also calculated similarity score using syllable embeddings generated by a variational autoencoder (VAE) model, as described in [<xref ref-type="bibr" rid="c34">34</xref>]. Briefly, using the <italic>autoencoded-vocal-analysis</italic> Python package [<xref ref-type="bibr" rid="c34">34</xref>], we trained the base VAE model with an 8 dimensional latent embedding using syllable spectrograms from the 21 UTSW zebra finches used to train the triplet loss model.</p>
</sec>
<sec id="s4j5">
<title>EMD</title>
<p>Syllable embeddings were obtained by running a forward pass of the trained triplet loss or VAE model with all segmented syllables that an individual bird produced on a given day. This results in a set of thousands of syllables in the 8-dimensional embedding space. Two birds songs are compared by calculating the Earth Mover’s Distance (EMD) between their syllable embeddings using the PyEMD package [<xref ref-type="bibr" rid="c51">51</xref>]. If one were to imagine the syllable embedding empirical distributions as piles of dirt, the earth mover’s distance is the minimum cost of moving the earth from one distribution to match the other, where cost is defined as the amount of dirt moved multiplied by the distance over which it is moved. This value can range from 0 for identical distributions, to positive infinity for distributions that are infinitely far apart. As our triplet loss model’s embedding space is limited to points lying on a unit radius 8-dimensional hypersphere, the maximum possible value for EMD is 1.41 for distributions that are each concentrated on a single point, maximally separated within the constraints of the embedding space.</p>
<p>The EMD score considers many song renditions from each bird being compared, allowing a better overall comparison of the similarity between two birds’ song production as compared to multiple pairwise comparisons between renditions. One limitation of EMD, however, is that it is completely agnostic to syllable sequencing, so a pupil that imitated all syllables from his tutor but sings them in a completely different order will have a similar EMD score to a pupil that imitated all syllables from a tutor and produces them in the same order. That said, there is new evidence that zebra finches recognize songs independently of syllable order, raising questions about the importance of syllable order in song perception [<xref ref-type="bibr" rid="c52">52</xref>]. The EMD score is also symmetrical, meaning that the presence of syllables in the tutor’s song that weren’t imitated by the pupil will have the same impact on the EMD score as new, improvised syllables present in the pupil’s song but not in the tutor’s song.</p>
</sec>
<sec id="s4j6">
<title>MMD</title>
<p>In addition to EMD, we calculate the dissimilarity between distributions of embedded syllables using maximum mean discrepancy (MMD), as in [<xref ref-type="bibr" rid="c34">34</xref>]. MMD compares two distributions of syllables by computing the squared distance between their mean features mapped in a reproducing kernel Hilbert space (RKHS) using a Gaussian radial basis function (RBF) kernel, with a kernel bandwidth equal to half the median pairwise distance between 1000 randomly sampled syllable embeddings from our validation dataset. The result is a measure of the dissimilarity between distributions, where a larger score indicates greater dissimilarity between the two syllable distributions. As with EMD, this is a symmetrical score which doesn’t consider the syllable sequencing similarity between the two birds being compared.</p>
</sec>
<sec id="s4j7">
<title>Similarity Scores across Comparison Types</title>
<p>To validate the performance of our models and EMD or MMD scores for similarity scoring, we compute EMD and MMD scores between a pupil and itself, a pupil and its tutor, pupil and a ‘sibling’, and a pupil and an unrelated bird. For comparisons between a pupil and itself, embeddings of all recorded syllables from a day of song are computed. If the bird has fewer than 4,000 syllables in this dataset, the dataset is randomly split in half and the two halves are compared. If a bird has more than 4,000 syllables, two sets of 2,000 syllables are randomly sampled and compared. For comparisons between a pupil and its tutor, up to 4,000 syllables are sampled from each of the tutor and the pupil and compared. In the case of pupil and ‘sibling’ comparisons, a sibling is defined as another bird sharing the same song tutor. We expect that typical zebra finches that learned from the same tutor will have similar songs, but that these will generally be less similar than a pupil compared to its tutor directly. Each pupil is compared to up to 3 ‘siblings’, depending on availability in our dataset. Finally, each pupil is compared to 3 randomly selected pupils who don’t share their song tutor. For each of these comparisons, up to 4,000 syllables are randomly sampled from each bird as well, to help reduce the compute time for EMD and MMD.</p>
</sec>
<sec id="s4j8">
<title>Contrast Index</title>
<p>As in [<xref ref-type="bibr" rid="c36">36</xref>], contrast index is calculated as:
<disp-formula>
<graphic xlink:href="593561v3_ueqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>self-similarity</italic> is the EMD or MMD score between pupil and itself, calculated as described in the previous section, and <italic>cross similarity</italic> is the mean similarity between a pupil and 3 unrelated birds, again calculated as described in the previous section. As EMD and MMD are <italic>dissimilarity</italic> scores, rather than similarity scores, more negative values reflect better contrast between comparisons, so we report the absolute value for ease of comparison to existing similarity scoring methods.</p>
</sec>
<sec id="s4j9">
<title>Tutor Contrast Index</title>
<p>Similarly to the contrast index score, the tutor contrast index is calculated as:
<disp-formula>
<graphic xlink:href="593561v3_ueqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>tutor similarity</italic> is the EMD or MMD score between a pupil and its tutor, calculated as described previously, and <italic>cross similarity</italic> is the mean similarity between a pupil and 3 unrelated birds. As with contrast index, absolute values are reported, with higher values indicating larger contrast in score between a pupil vs. tutor comparison and pupil vs. unrelated bird comparison for a given pupil bird.</p>
</sec>
<sec id="s4j10">
<title>Expert Human Similarity Scores</title>
<p>A panel of 11 expert human annotators were each presented with 126 pairs of spectrograms and were instructed to rate their similarity on a scale from 1 (not similar) to 10 (very similar). The spectrograms were generated using Sound Analysis Pro 2011 [<xref ref-type="bibr" rid="c32">32</xref>], began at the beginning of a song bout, and included at least one full motif when motif structure was present. Raters were presented with 4 pupil-tutor spectrogram pairs per pupil, 8 comparisons between a tutor and itself to ensure that raters were using the full rating scale, and 10 duplicated tutor-pupil comparisons to ensure that the scorers were internally consistent. No individual scorer differed from the mean score by more than an average of 2 standard deviations, none differed by more than 2 points on the duplicated comparisons, and all but one made use of the full scale (this scorer never assigned a perfect 10/10 score, so their scores were rescaled such that they spanned the full range). Similarity scores for each tutor-pupil pair were obtained by taking the mean similarity score across their 4 spectrogram pairs, across all scorers. The full scoring set is available at <ext-link ext-link-type="uri" xlink:href="https://forms.gle/9TDu1fwGGYXWKhgB6">https://forms.gle/9TDu1fwGGYXWKhgB6</ext-link>. These scores were previously generated for and published in [<xref ref-type="bibr" rid="c43">43</xref>]. Of the birds evaluated, 15 were also in the similarity scoring validation set, so the correlation between these 15 birds’ mean human similarity scores and tutor-pupil EMD and MMD scores were used to evaluate the agreement between methods.</p>
<p>For comparison to the expert human similarity scores, Sound Analysis Pro 2011 % similarity scores were calculated for the same set of 15 pupils. A representative motif from the tutor song was selected and compared to between 30 and 60 motif renditions from the pupil bird when the pupil was over 90dph, using the asymmetric time-courses similarity tool. The final reported scores are the mean %similarity across all comparisons for a given pupil.</p>
</sec>
<sec id="s4j11">
<title>Comparisons to Mature Song</title>
<p>For the 6 birds from UTSW and 5 birds from Duke University [<xref ref-type="bibr" rid="c9">9</xref>] from which we had recordings at 90-100dph and earlier time points, we computed the MMD between their juvenile and adult songs. For each bird, 4,000 WhisperSeg segmented syllables were sampled from a full day of song recordings when the birds were between 90 and 100 dph, to serve as the mature song distribution. Up to 4,000 WhisperSeg segmented syllables were sampled from each day of available recordings prior to or shortly following the mature song date for comparison. MMD scores were calculated using embeddings from the triplet loss model as described previously. As the scores appeared to follow an exponential pattern, where the rate of song dissimilarity change slowed over development, we fit an exponential function to the data using the scipy.optimize curve_fit() function [<xref ref-type="bibr" rid="c53">53</xref>], and plotted this function alongside the data.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data availability</title>
<p>Song recordings and annotations for all birds recorded at UTSW are available through the Texas Data repository (<ext-link ext-link-type="uri" xlink:href="https://dataverse.tdl.org/dataverse/avn">https://dataverse.tdl.org/dataverse/avn</ext-link>). Annotations of songs from the Rockefeller University Song Library [<xref ref-type="bibr" rid="c20">20</xref>] generated for this paper are also available through the Texas Data Repository (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18738/T8/DN0SIV">https://doi.org/10.18738/T8/DN0SIV</ext-link>), while the songs are available at <ext-link ext-link-type="uri" xlink:href="http://ofer.hunter.cuny.edu/songs">http://ofer.hunter.cuny.edu/songs</ext-link>. Recordings of juvenile birds from Duke University are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7924/r4j38x43h">https://doi.org/10.7924/r4j38x43h</ext-link>. Recordings of early-deafened zebra finches are available upon request from Dr. Kazuhiro Wada.</p>
</sec>
<sec id="sec53">
<title>Supplemental figures</title>
<fig id="fig1s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1–figure supplement 1.</label>
<caption><p>Screenshots of the AVN graphical application, showing <bold>a.</bold> how syllable labels can be generated from a table of syllable segmentations, including visual inspection of labels overlaid on a spectrogram, <bold>b.</bold> how the complete AVN feature set can be generated from a table of labeled syllables with a single click, and <bold>c.</bold> how users can adjust different hyperparameters for pertaining to spectrogram generation of feature calculations, with all hyperparameters clearly explained. Hyperparameters can also be saved and loaded from files, to encourage reproducibility of analyses.</p></caption>
<graphic xlink:href="593561v3_fig1s1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2–figure supplement 1.</label>
<caption><p><bold>a-c.</bold> The precision, recall, and F1 scores for syllable onset predictions within 10ms of a syllable onset in the manual annotation for each bird (n = 20 typical birds, 8 isolate birds, and 7 FP1 KD birds) across segmentation methods. <bold>e-f.</bold> The precision, recall and F1 scores for syllable offset predictions within 20ms of a syllable offset in the manual annotation for each bird (n = 20 typical birds, 8 isolate birds, and 7 FP1 KD birds) across segmentation methods. <bold>g.</bold> Distribution of time-differences between predicted syllable offsets and their best matches in the manual annotation, across segmentation methods. Distributions include all matched syllables across all 35 birds from UTSW.</p></caption>
<graphic xlink:href="593561v3_fig2s1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2s2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2–figure supplement 2.</label>
<caption><p><bold>a-c.</bold> The precision, recall and F1 scores for syllable offset predictions within 20ms of a syllable offset in the manual annotation for each bird (n = 25 typical birds from the Rockefeller song library) across segmentation methods. <bold>d.</bold> Distribution of time-differences between predicted syllable offsets and their best matches in the manual annotation, across segmentation methods. Distributions include all matched syllables across all 25 birds from Rockefeller.</p></caption>
<graphic xlink:href="593561v3_fig2s2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2s3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2–figure supplement 3.</label>
<caption><p><bold>a G d.</bold> Example UMAP plot of syllables from a typical adult zebra finch. Each point represents a syllable, colored according to their AVN label. <bold>b G e.</bold> Confusion matrix showing the proportion of syllables bearing each manual annotation label which fall into each of the possible AVN labels for an example typical adult zebra finch. AVN label 1000 refers to syllables which were not segmented correctly by WhisperSeg, and therefore don’t carry an AVN label. Hand label ‘x’ refers to ‘syllables’ which were segmented by WhisperSeg, but don’t have a counterpart in the manual annotation. <bold>c G f.</bold> Example spectrogram of a song bout produced by a typical adult zebra finch, with overlaid AVN syllable labels</p></caption>
<graphic xlink:href="593561v3_fig2s3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2s4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2–figure supplement 4.</label>
<caption><p><bold>aGb</bold> 4 replicate UMAP embeddings with different random initializations, and the corresponding clustering validation for each of 2 different birds. <bold>c</bold> The standard deviation of v-measure scores for each bird (n = 35, 20 typical birds, 8 isolate birds, and 7 FP1 KD birds from the UTSW colony) across 30 different random initializations for UMAP dimensionality reduction and HDBSCAN clustering. Despite the stochasticity inherent in UMAP embeddings, the clustering and accuracy of the clustering is very consistent within each bird.</p></caption>
<graphic xlink:href="593561v3_fig2s4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2s5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2–figure supplement 5.</label>
<caption><p><bold>a-c.</bold> The homogeneity, completeness, and v-measures scores for AVN labels compared to manual annotations for each bird (n = 20 typical birds, 8 isolate birds, and 7 FP1 KD birds), across segmentation methods.</p></caption>
<graphic xlink:href="593561v3_fig2s5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3–figure supplement 1.</label>
<caption><p><bold>a-b.</bold> Example Syntax Raster plot for <bold>a.</bold> an example FP1 KD zebra finch with disrupted syntax, and <bold>b.</bold> a typical adult zebra finch with stereotyped syntax, made using AVN labels. <bold>c-d.</bold> Transition matrix of an example <bold>c.</bold> FP1 KD zebra finch with disrupted syntax, and <bold>d.</bold> a typical adult zebra finch with stereotyped syntax, made using AVN labels. Cells represent the probability of the bird producing a ‘following syllable’ with a particular label, given that the last syllable produced belonged to the type ‘preceding syllable’. <bold>e.</bold> Mean repetition bout lengths (number of times a syllable is produced in a row each time it is sung) for the syllable type with the highest mean repetition bout length per bird (n = 20 typical birds, 8 isolate birds and 7 FP1 KD birds)(One Way ANOVA F(2, 32) = 1.09, p = 0.35). <bold>f.</bold> Coefficient of variation of repetition bout length for the syllable type with the highest mean repetition bout length per bird (n = 20 typical birds, 8 isolate birds and 7 FP1 KD birds) (One Way ANOVA F(2, 32) = 4.65, p &lt;0.05. * indicates Tukey HSD p-adj &lt; 0.05).</p></caption>
<graphic xlink:href="593561v3_fig3s1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3s2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3–figure supplement 2.</label>
<caption><p><bold>a.</bold> Correlation between the syllable duration distribution entropies calculated using AVN labels or manual annotation (n = 35 birds, <italic>r</italic> = 0.85, p &lt;0.005). <bold>b.</bold> Example syllable duration distribution calculated using manual annotations or WhisperSeg segmentations of syllables from a typical adult zebra finch. <bold>c.</bold> Correlation between the short silent gap duration distribution entropies calculated using AVN labels or manual annotation (n = 35 birds, <italic>r</italic> = 0.88, p&lt;0.005). <bold>d.</bold> Example silent gap duration distribution calculated using manual annotations for WhisperSeg Segmentations of syllables from a typical adult zebra finch. <bold>e.</bold> Comparison of syllable duration entropies across typical (n = 20), isolate (n = 8) and FP1 KD (n = 7) adult zebra finches (&gt;90dph) and juvenile zebra finches (n = 11, 50-51 dph) (One Way ANOVA F(3, 43) = 17.43, p &lt; 0.005. * indicates Tukey HSD p-adj &lt; 0.05). <bold>f.</bold> Comparison of silent gap duration entropies across across typical (n = 20), isolate (n = 8) and FP1 KD (n = 7) adult zebra finches (&gt;90dph) and juvenile zebra finches (n = 11, 50-51 dph) (One Way ANOVA F(3, 43) = 8.03, p &lt; 0.005. * indicates Tukey HSD p-adj &lt; 0.05).</p></caption>
<graphic xlink:href="593561v3_fig3s2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3s3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3–figure supplement 3.</label>
<caption><p><bold>a-b.</bold> Example rhythm spectrogram for <bold>a.</bold> a typical adult zebra finch and <bold>b.</bold> a typical juvenile zebra finch (50dph). Cyan points indicate the frequency band with the highest power in each song file’s rhythm spectrum. These frequencies are used to calculate the rhythm spectrogram peak frequency CVs, as in <italic>c</italic>. <bold>c.</bold> CV of the peak frequency across song files in a bird’s rhythm spectrogram for typical (n = 20), isolate (n = 8) and FP1 KD (n = 7) adult zebra finches (&gt;90dph) and juvenile zebra finches (n = 11, 50-51 dph) (One Way ANOVA F(3, 43) = 8.26, p &lt; 0.005. * indicates Tukey HSD p-adj &lt; 0.05). <bold>d.</bold> Additional example rhythm spectrograms from 3 typical adult zebra finches (&gt;90dph) and <bold>e.</bold> 3 juvenile zebra finches (50-51 dph).</p></caption>
<graphic xlink:href="593561v3_fig3s3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig4s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4–figure supplement 1.</label>
<caption><p><bold>a.</bold> Feature weights of a LDA model fit to discriminate between typical and isolate zebra finches. Features with positive weights tend to have higher values for isolate birds, and features with negative weights tend to have higher values for typical birds. <bold>b.</bold> Feature weights of an LDA model fit to discriminate between typical and deaf zebra finches. Features with positive weights tend to have higher values for deaf birds, and features with negative weights tend to have higher values for typical birds. <bold>c.</bold> Feature weights for an LDA model trained to discriminate between typical, isolate, and deaf zebra finches. Each column reflects the feature weights for binary classification between the named group and both other groups combined. Features are ordered according to their total absolute weights.</p></caption>
<graphic xlink:href="593561v3_fig4s1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6–figure supplement 1.</label>
<caption><p><bold>a-d.</bold> Dissimilarity score distributions for comparisons between a pupil and itself (n = 30 comparisons), a pupil and it’s tutor (n = 30 comparisons), two pupils which share the same tutor (aka pupil vs. ‘Sibling’, n = 58 comparisons), and between two pupils who don’t share a tutor (aka pupil vs. unrelated bird, n = 90 comparisons). Calculated with a dataset of 30 typical tutor-pupil pairs from the UTSW colony using different combinations of VAE or Triplet loss for dimensionality reduction and EMD or MMD for dissimilarity calculations. <bold>e.</bold> Contrast index values for pupil vs. self and pupil vs. unrelated bird comparisons for 30 birds from UTSW across different dimensionality reduction and dissimilarity calculation methods. <bold>f.</bold> Tutor contrast index values for pupil vs. tutor and pupil vs. unrelated bird comparisons for 30 birds from UTSW across different dimensionality reduction and dissimilarity calculation methods.</p></caption>
<graphic xlink:href="593561v3_fig6s1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6s2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6–figure supplement 2.</label>
<caption><p><bold>a.</bold> MMD dissimilarity score distributions for comparisons between a pupil and itself (n = 5 comparisons), a pupil and it’s tutor (n = 5 comparisons), two pupils which share the same tutor (aka pupil vs. ‘Sibling’, n = 14 comparisons), and between two pupils who don’t share a tutor (aka pupil vs. unrelated bird, n = 15 comparisons). Calculated with a dataset of 5 FP1 KD pupils from the UTSW colony, and an additional 30 typical pupils for the ‘Sibling’ and ‘unrelated’ comparisons. <bold>b.</bold> Correlation between Sound Analysis Pro 2011 % similarity scores and human expert judgements of song similarity for 14 tutor-pupil pairs from UTSW (<italic>r =</italic> 0.32, p = 0.23). <bold>c.</bold> Contrast index values for pupil vs. self and pupil vs. unrelated bird comparisons for 30 birds from UTSW and 25 birds from Rockefeller (t-test, p = 0.47). <bold>d.</bold> Tutor contrast index values for pupils vs. tutor and pupil vs. unrelated bird comparisons for 30 birds from UTSW and 25 birds from Rockefeller (t-test, p = 0.06).</p></caption>
<graphic xlink:href="593561v3_fig6s2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6s3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6–figure supplement 3.</label>
<caption><p>Similarity scoring deep neural network architecture.</p></caption>
<graphic xlink:href="593561v3_fig6s3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We would like to thank Fayha Zia for help with manual syllable labeling, Chihito Mori and Kazuhiro Wada for sharing recordings of early-deafened zebra finches, Ofer Tchernichovski and Erich Jarvis for publicly sharing the Rockefeller University Field Research Center Song Library, and Richard Mooney for publicly sharing recordings of juvenile zebra finches across development. We thank Tyler Lee and Daisuke Hattori for their valuable feedback and suggestions on AVN’s design and validation. We also thank Luis Garcia, Andrea Guerrero, and all members of the Roberts Lab for bird care support and their generous feedback on this work.</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Funding</title>
<p>This research was supported by the US National Institutes of Health R01 DC020333 to TFR. TMIK was supported by a Neural Scientist Training Program Fellowship from the UT Southwestern O’Donnell Brain Institute. ESM was supported by the UTD/UTSW Green Fellowship.</p>
</sec>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplemental table 1</label>
<media xlink:href="supplements/593561_file03.xlsx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wiltschko</surname>, <given-names>A.B.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Revealing the structure of pharmacobehavioral space through motion sequencing</article-title>. <source>Nature Neuroscience</source>, <year>2020</year>. <volume>23</volume>(<issue>11</issue>): p. <fpage>1433</fpage>–<lpage>1443</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hsu</surname>, <given-names>A.I.</given-names></string-name> and <string-name><given-names>E.A.</given-names> <surname>Yttri</surname></string-name></person-group>, <article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title>. <source>Nature Communications</source>, <year>2021</year>. <volume>12</volume>(<issue>1</issue>): p. <fpage>5188</fpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alam</surname>, <given-names>D.</given-names></string-name>, <string-name><given-names>F.</given-names> <surname>Zia</surname></string-name>, and <string-name><given-names>T.F.</given-names> <surname>Roberts</surname></string-name></person-group>, <article-title>The hidden fitness of the male zebra finch courtship song</article-title>. <source>Nature</source>, <year>2024</year>. <volume>628</volume>(<issue>8006</issue>): p. <fpage>117</fpage>–<lpage>121</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinfath</surname>, <given-names>E.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Fast and accurate annotation of acoustic signals with deep neural networks</article-title>. <source>eLife</source>, <year>2021</year>. <volume>10</volume>: p. <elocation-id>e68837</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.68837</pub-id></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>Y.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Automated annotation of birdsong with a neural network that segments spectrograms</article-title>. <source>eLife</source>, <year>2022</year>. <volume>11</volume>: p. <elocation-id>e63853</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.63853</pub-id></mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Gu</surname>, <given-names>N.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Positive Transfer of the Whisper Speech Transformer to Human and Animal Voice Activity Detection</article-title>. <source>bioRxiv</source>, <year>2023</year>: p. <elocation-id>2023.09.30.560270</elocation-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Coffey</surname>, <given-names>K.R.</given-names></string-name>, <string-name><given-names>R.E.</given-names> <surname>Marx</surname></string-name>, and <string-name><given-names>J.F.</given-names> <surname>Neumaier</surname></string-name></person-group>, <article-title>DeepSqueak: a deep learning-based system for detection and analysis of ultrasonic vocalizations</article-title>. <source>Neuropsychopharmacology</source>, <year>2019</year>. <volume>44</volume>(<issue>5</issue>): p. <fpage>859</fpage>–<lpage>868</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goffinet</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</article-title>. <source>eLife</source>, <year>2021</year>. <volume>10</volume>: p. <elocation-id>e67855</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.67855</pub-id></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brudner</surname>, <given-names>S.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Pearson</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Mooney</surname></string-name></person-group>, <article-title>Generative models of birdsong learning link circadian fluctuations in song variability to changes in performance</article-title>. <source>PLOS Computational Biology</source>, <year>2023</year>. <volume>1G</volume>(<issue>5</issue>): p. <fpage>e1011051</fpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sainburg</surname>, <given-names>T.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Thielk</surname></string-name>, and <string-name><given-names>T.Ǫ.</given-names> <surname>Gentner</surname></string-name></person-group>, <article-title>Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires</article-title>. <source>PLOS Computational Biology</source>, <year>2020</year>. <volume>16</volume>(<issue>10</issue>): p. <fpage>e1008228</fpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Roeser</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal></person-group>, <source>The songbird lateral habenula projects to dopaminergic midbrain and is important for normal vocal development</source>. <year>2023</year>, <publisher-name>Cold Spring Harbor Laboratory</publisher-name>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Doupe</surname>, <given-names>A.J.</given-names></string-name> and <string-name><given-names>P.K.</given-names> <surname>Kuhl</surname></string-name></person-group>, <article-title>BIRDSONG AND HUMAN SPEECH: Common Themes and Mechanisms</article-title>. <source>Annual Review of Neuroscience</source>, <year>1999</year>. <volume>22</volume>(Volume 22, 1999): p. <fpage>567</fpage>–<lpage>631</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lachlan</surname>, <given-names>R.F.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Zebra Finch Song Phonology and Syntactical Structure across Populations and Continents-A Computational Comparison</article-title>. <source>Front Psychol</source>, <year>2016</year>. <volume>7</volume>: p. <fpage>980</fpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tchernichovski</surname>, <given-names>O.</given-names></string-name> and <string-name><given-names>F.</given-names> <surname>Nottebohm</surname></string-name></person-group>, <article-title>Social inhibition of song imitation among sibling male zebra finches</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <year>1998</year>. <volume>G5</volume>(<issue>15</issue>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scharff</surname>, <given-names>C.</given-names></string-name> and <string-name><given-names>F.</given-names> <surname>Nottebohm</surname></string-name></person-group>, <article-title>A comparative study of the behavioral deficits following lesions of various parts of the zebra finch song system: implications for vocal learning</article-title>. <source>The Journal of Neuroscience</source>, <year>1991</year>. <volume>11</volume>(<issue>9</issue>): p. <fpage>2896</fpage>–<lpage>2913</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Gu</surname>, <given-names>N.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Positive Transfer of the Whisper Speech Transformer to Human and Animal Voice Activity Detection</article-title>. in <conf-name>ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</conf-name>. <year>2024</year>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sainburg</surname>, <given-names>T.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Thielk</surname></string-name>, and <string-name><given-names>T.Ǫ.</given-names> <surname>Gentner</surname></string-name></person-group>, <article-title>Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires</article-title>. <source>PLoS Comput Biol</source>, <year>2020</year>. <volume>16</volume>(<issue>10</issue>): p. <fpage>e1008228</fpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koumura</surname>, <given-names>T.</given-names></string-name> and <string-name><given-names>K.</given-names> <surname>Okanoya</surname></string-name></person-group>, <article-title>Automatic Recognition of Element Classes and Boundaries in the Birdsong with Variable Sequences</article-title>. <source>PLOS One</source>, <year>2016</year>. <volume>11</volume>(<issue>7</issue>): p. <fpage>e0159188</fpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garcia-Oscos</surname>, <given-names>F.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Autism-linked gene FoxP1 selectively regulates the cultural transmission of learned vocalizations</article-title>. <source>Sci Adv</source>, <year>2021</year>. <volume>7</volume>(<issue>6</issue>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tchernichovski</surname>, <given-names>O.</given-names></string-name>, <string-name><given-names>S.</given-names> <surname>Eisenberg-Edidin</surname></string-name>, and <string-name><given-names>E.D.</given-names> <surname>Jarvis</surname></string-name></person-group>, <article-title>Balanced imitation sustains song culture in zebra finches</article-title>. <source>Nature Communications</source>, <year>2021</year>. <volume>12</volume>(<issue>1</issue>): p. <fpage>2562</fpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>McInnes</surname>, <given-names>L.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Healy</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Melville</surname></string-name></person-group>, <article-title>Umap: Uniform manifold approximation and projection for dimension reduction</article-title>. <source>arXiv</source> <elocation-id>1802.03426</elocation-id>, <year>2018</year>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>McInnes</surname>, <given-names>L.</given-names></string-name> and <string-name><given-names>J.</given-names> <surname>Healy</surname></string-name></person-group>. <article-title>Accelerated hierarchical density based clustering</article-title>. in <conf-name>2017 IEEE international conference on data mining workshops (ICDMW)</conf-name>. <year>2017</year>. IEEE.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hyland Bruno</surname>, <given-names>J.</given-names></string-name> <string-name><surname>and</surname> <given-names>O.</given-names></string-name> <string-name><surname>Tchernichovski</surname></string-name></person-group>, <article-title>Regularities in zebra finch song beyond the repeated motif</article-title>. <source>Behavioural Processes</source>, <year>2019</year>. <volume>163</volume>: p. <fpage>53</fpage>–<lpage>59</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xiao</surname>, <given-names>L.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Expression of FoxP2 in the basal ganglia regulates vocal motor sequences in the adult songbird</article-title>. <source>Nature Communications</source>, <year>2021</year>. <volume>12</volume>(<issue>1</issue>): p. <fpage>2617</fpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanaka</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Focal expression of mutant huntingtin in the songbird basal ganglia disrupts cortico-basal ganglia networks and vocal sequences</article-title>. <source>Proc Natl Acad Sci U S A</source>, <year>2016</year>. <volume>113</volume>(<issue>12</issue>): p. <fpage>E1720</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanchez-Valpuesta</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Corticobasal ganglia projecting neurons are required for juvenile vocal learning but not for adult vocal plasticity in songbirds</article-title>. <source>Proc Natl Acad Sci U S A</source>, <year>2019</year>. <volume>116</volume>(<issue>45</issue>): p. <fpage>22833</fpage>–<lpage>22843</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Norton</surname>, <given-names>P.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Differential Song Deficits after Lentivirus-Mediated Knockdown of FoxP1, FoxP2, or FoxP4 in Area X of Juvenile Zebra Finches</article-title>. <source>The Journal of Neuroscience</source>, <year>2019</year>. <volume>3G</volume>(<issue>49</issue>): p. <fpage>9782</fpage>–<lpage>9796</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kubikova</surname>, <given-names>L.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Basal ganglia function, stuttering, sequencing, and repair in adult songbirds</article-title>. <source>Sci Rep</source>, <year>2014</year>. <volume>4</volume>: p. <fpage>6590</fpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aronov</surname>, <given-names>D.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Two distinct modes of forebrain circuit dynamics underlie temporal patterning in the vocalizations of young songbirds</article-title>. <source>J Neurosci</source>, <year>2011</year>. <volume>31</volume>(<issue>45</issue>): p. <fpage>16353</fpage>–<lpage>68</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goldberg</surname>, <given-names>J.H.</given-names></string-name> and <string-name><given-names>M.S.</given-names> <surname>Fee</surname></string-name></person-group>, <article-title>Vocal babbling in songbirds requires the basal ganglia-recipient motor thalamus but not the basal ganglia</article-title>. <source>J Neurophysiol</source>, <year>2011</year>. <volume>105</volume>(<issue>6</issue>): p. <fpage>2729</fpage>–<lpage>39</lpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saar</surname>, <given-names>S.</given-names></string-name> and <string-name><given-names>P.P.</given-names> <surname>Mitra</surname></string-name></person-group>, <article-title>A technique for characterizing the development of rhythms in bird song</article-title>. <source>PLoS One</source>, <year>2008</year>. <volume>3</volume>(<issue>1</issue>): p. <fpage>e1461</fpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tchernichovski</surname>, <given-names>O.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>A procedure for an automated measurement of song similarity</article-title>. <source>Animal Behaviour</source>, <year>2000</year>. <volume>5G</volume>(<issue>6</issue>): p. <fpage>1167</fpage>–<lpage>1176</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mori</surname>, <given-names>C.</given-names></string-name> and <string-name><given-names>K.</given-names> <surname>Wada</surname></string-name></person-group>, <article-title>Audition-Independent Vocal Crystallization Associated with Intrinsic Developmental Gene Expression Dynamics</article-title>. <source>The Journal of Neuroscience</source>, <year>2015</year>. <volume>35</volume>(<issue>3</issue>): p. <fpage>878</fpage>–<lpage>889</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goffinet</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</article-title>. <source>eLife</source>, <year>2021</year>. <volume>10</volume>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brudner</surname>, <given-names>S.</given-names></string-name>, <string-name><given-names>J.</given-names> <surname>Pearson</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Mooney</surname></string-name></person-group>, <article-title>Generative models of birdsong learning link circadian fluctuations in song variability to changes in performance</article-title>. <source>PLoS Comput Biol</source>, <year>2023</year>. p. <fpage>e1011051</fpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mandelblat-Cerf</surname>, <given-names>Y.</given-names></string-name> and <string-name><given-names>M.S.</given-names> <surname>Fee</surname></string-name></person-group>, <article-title>An Automated Procedure for Evaluating Song Imitation</article-title>. <source>PLOS One</source>, <year>2014</year>. p. <fpage>e96484</fpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Lachlan</surname>, <given-names>R.F</given-names></string-name></person-group>., <article-title>Luscinia: a bioacoustics analysis computer program</article-title>. <source>sourceforge</source>, <year>2007</year>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mets</surname>, <given-names>D.G.</given-names></string-name> and <string-name><given-names>M.S.</given-names> <surname>Brainard</surname></string-name></person-group>, <article-title>An automated approach to the quantitation of vocalizations and vocal learning in the songbird</article-title>. <source>PLOS Computational Biology</source>, <year>2018</year>. <volume>14</volume>(<issue>8</issue>): p. <fpage>e1006437</fpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thakur</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Deep metric learning for bioacoustic classification: Overcoming training data scarcity using dynamic triplet loss</article-title>. <source>J Acoust Soc Am</source>, <year>2019</year>. <volume>146</volume>(<issue>1</issue>): p. <fpage>534</fpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Schroff</surname>, <given-names>F.</given-names></string-name>, <string-name><given-names>D.</given-names> <surname>Kalenichenko</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Philbin</surname></string-name></person-group>. <article-title>FaceNet: A unified embedding for face recognition and clustering</article-title>. in <conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>. <year>2015</year>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mets</surname>, <given-names>D.G.</given-names></string-name> and <string-name><given-names>M.S.</given-names> <surname>Brainard</surname></string-name></person-group>, <article-title>An automated approach to the quantitation of vocalizations and vocal learning in the songbird</article-title>. <source>PLoS Comput Biol</source>, <year>2018</year>. <volume>14</volume>(<issue>8</issue>): p. <fpage>e1006437</fpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tchernichovski</surname>, <given-names>O.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>A procedure for an automated measurement of song similarity</article-title>. <source>Anim Behav</source>, <year>2000</year>. p. <fpage>1167</fpage>–<lpage>1176</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garcia-Oscos</surname>, <given-names>F.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Autism-linked gene FoxP1 selectively regulates the cultural transmission of learned vocalizations</article-title>. <source>Science Advances</source>, <year>2021</year>. <volume>7</volume>(<issue>6</issue>): p. <fpage>eabd2827</fpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tumer</surname>, <given-names>E.C.</given-names></string-name> and <string-name><given-names>M.S.</given-names> <surname>Brainard</surname></string-name></person-group>, <article-title>Performance variability enables adaptive plasticity of ‘crystallized’ adult birdsong</article-title>. <source>Nature</source>, <year>2007</year>. <volume>450</volume>(<issue>7173</issue>): p. <fpage>1240</fpage>–<lpage>1244</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>McFee</surname>, <given-names>B.</given-names></string-name>, <string-name><given-names>Matt</given-names> <surname>McVicar</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Faronbi</surname></string-name>, <string-name><given-names>Iran</given-names> <surname>Roman</surname></string-name>, <string-name><given-names>Matan</given-names> <surname>Gover</surname></string-name>, <string-name><given-names>Stefan</given-names> <surname>Balke</surname></string-name>, <string-name><given-names>Scott</given-names> <surname>Seyfarth</surname></string-name>, <string-name><given-names>Ayoub</given-names> <surname>Malek</surname></string-name>, <string-name><given-names>Colin</given-names> <surname>Raffel</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Lostanlen</surname></string-name>, <string-name><given-names>Benjamin</given-names> <surname>van Niekirk</surname></string-name>, <string-name><given-names>Dana</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Frank</given-names> <surname>Cwitkowitz</surname></string-name>, <string-name><given-names>Frank</given-names> <surname>Zalkow</surname></string-name>, <string-name><given-names>Oriol</given-names> <surname>Nieto</surname></string-name>, <string-name><given-names>Dan</given-names> <surname>Ellis</surname></string-name>, <string-name><given-names>Jack</given-names> <surname>Mason</surname></string-name>, <string-name><given-names>Kyungyun</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Bea</given-names> <surname>Steers</surname></string-name></person-group>, <article-title>librosa/librosa</article-title>: <version>0.10.1</version>. <year>2023</year>: <source>Zenodo</source>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Fabian</given-names> <surname>Pedregosa</surname></string-name>, <string-name><given-names>G.V.</given-names>, <surname>Alexandre Gramfort</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Michel</surname></string-name>, <string-name><given-names>Bertrand</given-names> <surname>Thirion</surname></string-name>, <string-name><given-names>Olivier</given-names> <surname>Grisel</surname></string-name>, <string-name><given-names>Mathieu</given-names> <surname>Blondel</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Prettenhofer</surname></string-name>, <string-name><given-names>Ron</given-names> <surname>Weiss</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Dubourg</surname></string-name>, <string-name><given-names>Jake</given-names> <surname>Vanderplas</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Passos</surname></string-name>, <string-name><given-names>David</given-names> <surname>Cournapeau</surname></string-name>, <string-name><given-names>Matthieu</given-names> <surname>Brucher</surname></string-name>, <string-name><given-names>Matthieu</given-names> <surname>Perrot</surname></string-name>, <string-name><given-names>Édouard</given-names> <surname>Duchesnay</surname></string-name></person-group>, <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>Journal of Machine Learning Research</source>, <year>2011</year>. <volume>12</volume>: p. <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Servén</surname>, <given-names>D.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Brummitt</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Abedi</surname></string-name></person-group>, <article-title>pyGAM</article-title>. <year>2018</year>: <source>Zenodo</source>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Szegedy</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Going deeper with convolutions</article-title>. <source>arXiv</source> <year>2015</year>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Nair</surname>, <given-names>V.</given-names></string-name> and <string-name><given-names>G.E.</given-names> <surname>Hinton</surname></string-name></person-group>, <article-title>Rectified linear units improve restricted boltzmann machines</article-title>, in <conf-name>Proceedings of the 27th International Conference on International Conference on Machine Learning</conf-name>. <year>2010</year>, Omnipress: Haifa, Israel. p. <fpage>807</fpage>–<lpage>814</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Kingma</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>J.</given-names> <surname>Ba</surname></string-name></person-group>, <article-title>Adam: A Method for Stochastic Optimization</article-title>. <conf-name>International Conference on Learning Representations</conf-name>, <year>2014</year>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Doran</surname>, <given-names>G.</given-names></string-name></person-group>, <article-title>PyEMD: Earth Mover’s Distance for Python</article-title>. <source>GitHub</source> <year>2014</year>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ning</surname>, <given-names>Z.-Y.</given-names></string-name>, <string-name><given-names>H.</given-names> <surname>Honing</surname></string-name>, and <string-name><given-names>C.</given-names> <surname>ten Cate</surname></string-name></person-group>, <article-title>Zebra finches (Taeniopygia guttata) demonstrate cognitive flexibility in using phonology and sequence of syllables in auditory discrimination</article-title>. <source>Animal Cognition</source>, <year>2023</year>. <volume>26</volume>(<issue>4</issue>): p. <fpage>1161</fpage>–<lpage>1175</lpage>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Virtanen</surname>, <given-names>P.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title>. <source>Nature Methods</source>, <year>2020</year>. <volume>17</volume>(<issue>3</issue>): p. <fpage>261</fpage>–<lpage>272</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101111.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Goldberg</surname>
<given-names>Jesse H</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Cornell University</institution>
</institution-wrap>
<city>Ithaca</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This work introduces a new Python package, Avian Vocalization Analysis (AVN) that provides several key analysis pipelines for birdsong research. This tool is likely to prove <bold>useful</bold> to researchers in neuroscience and beyond, as demonstrated by <bold>convincing</bold> experiments using a wide range of publicly available birdsong data.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101111.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this work, the authors present a new Python software package, Avian Vocalization Network (AVN) aimed at facilitating the analysis of birdsong, especially the song of the zebra finch, the most common songbird model in neuroscience. The package handles some of the most common (and some more advanced) song analyses, including segmentation, syllable classification, featurization of song, calculation of tutor-pupil similarity, and age prediction, with a view toward making the entire process friendlier to experimentalists with limited coding experience working in the field.</p>
<p>For many years, Sound Analysis Pro has served as a standard in the songbird field, the first package to extensively automate songbird analysis and facilitate the computation of acoustic features that have helped define the field. More recently, the increasing popularity of Python as a language, along with the emergence of new machine learning methods, has resulted in a number of new software tools, including the vocalpy ecosystem for audio processing, TweetyNet (for segmentation), t-SNE and UMAP (for visualization), and autoencoder-based approaches for embedding.</p>
<p>As with any software package, this one necessarily makes a number of design choices, which may or may not fit the needs of all users. Those who prefer a more automated pipeline with fewer knobs to turn may appreciate AVN in cases where the existing recipes fit their needs, while those who require more customization and flexibility may require a more bespoke (and thus code-intensive) approach.</p>
<p>Strengths:</p>
<p>The AVN package overlaps several of these earlier efforts, albeit with a focus on more traditional featurization that many experimentalists may find more interpretable than deep learning-based approaches. Among the strengths of the paper are its clarity in explaining the several analyses it facilitates, along with high-quality experiments across multiple public datasets collected from different research groups. As a software package, it is open source, installable via the pip Python package manager, and features high-quality documentation, as well as tutorials. For experimentalists who wish to replicate any of the analyses from the paper, the package is likely to be a useful time saver.</p>
<p>Weaknesses:</p>
<p>I think the potential limitations of the work are predominantly on the software end, with one or two quibbles about the methods.</p>
<p>First, the software: It's important to note that the package is trying to do many things, of which it is likely to do several well and a few comprehensively. Rather than a package that presents a number of new analyses or a new analysis framework, it is more a codification of recipes, some of which are reimplementations of existing work (SAP features), some of which are essentially wrappers around other work (interfacing with WhisperSeg segmentations), and some of which are new (similarity scoring). All of this has value, but in my estimation, it has less value as part of a standalone package and potentially much more as part of an ecosystem like vocalpy that is undergoing continuous development and has long-term support. While the code is well-documented, including web-based documentation for both the core package and the GUI, the latter is available only on Windows, which might limit the scope of adoption.</p>
<p>That is to say, whether AVN is adopted by the field in the medium term will have much more to do with the quality of its maintenance and responsiveness to users than any particular feature, but I believe that many of the analysis recipes that the authors have carefully worked out may find their way into other code and workflows.</p>
<p>In the revised version of the paper, the authors have expanded their case for the design choices made in AVN and remain committed to maintaining the tool. Given the low cost for users in trying new methods and the work the authors have put into further reducing this overhead via documentation, those curious about the package are likely best served by simply downloading it and giving it a try on their own data.</p>
<p>Second, two notes about new analysis approaches:</p>
<p>(1) The authors propose a new means of measuring tutor-pupil similarity based on first learning a latent space of syllables via a self-supervised learning (SSL) scheme and then using the earth mover's distance (EMD) to calculate transport costs between the distributions of tutors' and pupils' syllables. While, to my knowledge, this exact method has not previously been proposed in birdsong, I suspect it is unlikely to differ substantially from the approach of autoencoding followed by MMD used in the Goffinet et al. paper. That is, SSL, like the autoencoder, is a latent space learning approach, and EMD, like MMD, is an integral probability metric that measures discrepancies between two distributions. (Indeed, the two are very closely related: <ext-link ext-link-type="uri" xlink:href="https://stats.stackexchange.com/questions/400180/earth-movers-distance-and-maximum-mean-discrepency">https://stats.stackexchange.com/questions/400180/earth-movers-distance-and-maximum-mean-discrepency</ext-link>.) Without further experiments, it is hard to tell whether these two approaches differ meaningfully. Likewise, while the authors have trained on a large corpus of syllables to define their latent space in a way that generalizes to new birds, it is unclear why such an approach would not work with other latent space learning methods.</p>
<p>Update: The authors now provide an extensive comparison with the Goffinet et al. paper and also consider differences between MMD and EMD. This comparison both adds value to the original paper and provides useful benchmarking for others looking to develop latent space comparison methods.</p>
<p>(2) The authors propose a new method for maturity scoring by training a model (a generalized additive model) to predict the age of the bird based on a selected subset of acoustic features. This is distinct from the &quot;predicted age&quot; approach of Brudner, Pearson, and Mooney, which predicts based on a latent representation rather than specific features, and the GAM nicely segregates the contribution of each. As such, this approach may be preferred by many users who appreciate its interpretability.</p>
<p>In summary, my view is that this is a nice paper detailing a well-executed piece of software whose future impact will be determined by the degree of support and maintenance it receives from others over the near and medium term.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101111.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper introduces the Avian Vocalization Network (AVN), a novel birdsong analysis pipeline using deep learning. By automating vocal annotation tasks, the AVN generates interpretable song features and song similarity scores on novel datasets without retraining. The performance of the network is solid and is comparable to that of human annotators.</p>
<p>The authors have improved the manuscript in several aspects, such as the comparison with the Goffinet work. Overall, the AVN feature set could become a useful tool for evaluating birdsongs. But the authors also chose not to address a certain number of criticisms, and some issues remain poorly addressed, and the work is not reproducible at this stage. With a little effort, these issues could get resolved in my view. I will just pick on four issues that I think can be easily addressed:</p>
<p>(1) Limitation of feature set: They claim that AVN satisfies the criteria (line 60) of &quot;creating a common feature space for the comparison of behavioural phenotypes ...&quot;(line 51), but then on LDA analysis, explained on line 910 they say &quot;excluding amplitude and amplitude modulation features as they were found to vary&quot;. Since their feature set is not stable and not truly 'common' to all tasks, this limitation needs addressing in the discussion (that some features seem to vary undesirably, and they need exclusion based on some criteria to be defined).</p>
<p>(2) Missing information on classification training loss: The Authors insist that their triplet loss is not related to classification, and they brush off my request for more information. In their rebuttal, they write: 'The loss function is related to the relative distance between embeddings of syllables with the same or different labels, not the classification of syllables as same or different.' Perplexingly, however, in the revised paper, authors speak themselves of 'classes', in Line 1004: this allows the model to begin learning an easier task, of separating syllables of different classes by a smaller margin.' So it seems the authors actually agree with me that there is an underlying classification task. I am therefore going to make it a bit more explicit here what I'm asking for, hoping this will better resonate with them.</p>
<p>In line 984 they define their loss function and in lines 994-996 they define 'hard' and 'semi-hard' triplets. Authors then train a system to minimize the loss with a ratio of 75 percent semi-hard triplets and 25 percent hard triplets and a final weighing parameter value alpha=0.7. What I'm asking for is this 'classification' loss their trained model achieves, or in other words, the fraction of triplets that end up producing a loss, either of the 'hard' or 'semi-hard' type. For example, if their model manages to separate all 'possible triplets' by a margin of at least alpha, then the loss would be zero. If the model achieves to separate all triplets except one, then the loss would correspond to the amount by which the separation differences between the anchor and the positive vs negative samples exceeds alpha. So, an important number to provide in the paper is the fraction of triplets that incur a nonzero loss, i.e., the fraction of semi-hard triplets. And another important quantity is the fraction of hard triplets, i.e. the fraction of triplets that would incur a loss if alpha were set to zero, or, in other words, the triplets for which the negative sample is closer to the anchor than the positive sample. By the way, I assume this latter fraction of hard cases will be zero - that their model does not confuse any positive and negative training samples...</p>
<p>
Note: the quantification chosen by the authors termed 'contrast index' is interesting, but it is a derived quantity, it is not the quantity authors chose to optimize during training. If authors were to report both the training loss achieved and the 'contrast index', follow-up work could be benchmarked against both these quantities. If for example, a follow-up model achieves smaller loss but worse contrast, then the loss is not a good placeholder measure for optimizing contrast. Alternatively, follow-up work could focus on the contrast index as training objective, obliterating the need for the triplet loss as an intermediate step (I don't buy the authors' argument that such an optimization would be infeasible).</p>
<p>(3) Reproducibility: they explain the way they train the CNN with triplet loss to produce the embeddings, but we're missing both actual scripts on GitHub to train and inference from scratch, and model weights, or even hyper parameters they used. Authors only provide the architecture, and I don't think that's enough to be considered replicable in today's standards. I would suggest they release complete model checkpoint weights for the result they report, the exact data splits, the hyper parameters they used and training and testing code, so that one can very easily verify their claims and apply their methods to other datasets. Note: for example, the code to extract the embeddings is incomplete (the function definition of single_bird_extract_embeddings cannot be found on GitHub) and the model weights they used are missing.</p>
<p>(4) With regards to the age prediction model, the authors should specify that this model is mainly useful for comparisons across studies but less so for precise evaluation of the effects of a treatment within a study. Namely, the effect on song of a treatment is best assessed by comparison to within-subject past song, and by comparison to age-matched control birds (ideally siblings) raised in identical conditions, rather than to invoke a generic model trained on other birds and from different colonies and breeding conditions as authors propose to do. In other words, to introduce a generic model for evaluation of song maturity introduces measurement noise in terms of the additional birds and their variable conditions, which can hinder precise assessment of treatment effects. Note that to state that in past work such maturity models were used is not a good justification, scientifically speaking.</p>
<p>Finally, the authors write that methods for syllable segmentation have not been systematically compared but the whisperseg work they use did such a comparison. So the authors should revise their novelty claim of being the first to compare syllable segmentation methods.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101111.2.sa0</article-id>
<title-group>
<article-title>Author Response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Koch</surname>
<given-names>Therese MI</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5327-3219</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Marks</surname>
<given-names>Ethan S</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Roberts</surname>
<given-names>Todd F</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0967-6598</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>This paper applies methods for segmentation, annotation, and visualization of acoustic analysis to zebra finch song. The paper shows that these methods can be used to predict the stage of song development and to quantify acoustic similarity. The methods are solid and are likely to provide a useful tool for scientists aiming to label large datasets of zebra finch vocalizations. The paper has two main parts: 1) establishing a pipeline/ package for analyzing zebra finch birdsong and 2) a method for measuring song imitation.</p>
<p>Strengths:</p>
<p>It is useful to see existing methods for syllable segmentation compared to new datasets.</p>
<p>It is useful, but not surprising, that these methods can be used to predict developmental stage, which is strongly associated with syllable temporal structure.</p>
<p>It is useful to confirm that these methods can identify abnormalities in deafened and isolated songs.</p>
<p>Weaknesses:</p>
<p>For the first part, the implementation seems to be a wrapper on existing techniques. For instance, the first section talks about syllable segmentation; they made a comparison between whisperseg (Gu et al, 2024), tweetynet (Cohen et al, 2022), and amplitude thresholding. They found that whisperseg performed the best, and they included it in the pipeline. They then used whisperseg to analyze syllable duration distributions and rhythm of birds of different ages and confirmed past findings on this developmental process (e.g. Aronov et al, 2011). Next, based on the segmentation, they assign labels by performing UMAP and HDBScan on the spectrogram (nothing new; that's what people have been doing). Then, based on the labels, they claimed they developed a 'new' visualization - syntax raster ( line 180 ). That was done by Sainburg et. al. 2020 in Figure 12E and also in Cohen et al, 2020 - so the claim to have developed 'a new song syntax visualization' is confusing. The rest of the paper is about analyzing the finch data based on AVN features (which are essentially acoustic features already in the classic literature).</p>
</disp-quote>
<p>First, we would like to thank this reviewer for their kind comments and feedback on this manuscript. It is true that many of the components of this song analysis pipeline are not entirely novel in isolation. Our real contribution here is bringing them together in a way that allows other researchers to seamlessly apply automated syllable segmentation, clustering, and downstream analyses to their data. That said, our approach to training TweetyNet for syllable segmentation is novel. We trained TweetyNet to recognize vocalizations vs. silence across multiple birds, such that it can generalize to new individual birds, whereas Tweetynet had only ever been used to annotate song syllables from birds included in its training set previously. Our validation of TweetyNet and WhisperSeg in combination with UMAP and HDBSCAN clustering is also novel, providing valuable information about how these systems interact, and how reliable the completely automatically generated labels are for downstream analysis. We have added a couple sentences to the introduction to emphasize the novelty of this approach and validation.</p>
<p>Our syntax raster visualization does resemble Figure 12E in Sainburg et al. 2020, however it differs in a few important ways, which we believe warrant its consideration as a novel visualization method. First, Sainburg et al. represent the labels across bouts in real time; their position along the x axis reflects the time at which each syllable is produced relative to the start of the bout. By contrast, our visualization considers only the index of syllables within a bout (ie. First syllable vs. second syllable etc) without consideration of the true durations of each syllable or the silent gaps between them. This makes it much easier to detect syntax patterns across bouts, as the added variability of syllable timing is removed. Considering only the sequence of syllables rather than their timing also allows us to more easily align bouts according to the first syllable of a motif, further emphasizing the presence or absence of repeating syllable sequences without interference from the more variable introductory notes at the start of a motif. Finally, instead of plotting all bouts in the order in which they were produced, our visualization orders bouts such that bouts with the same sequence of syllables will be plotted together, which again serves to emphasize the most common syllable sequences that the bird produces. These additional processing steps mean that our syntax raster plot has much starker contrast between birds with stereotyped syntax and birds with more variable syntax, as compared to the more minimally processed visualization in Sainburg et al. 2020. There doesn’t appear to be any similar visualizations in Cohen et al. 2020.</p>
<disp-quote content-type="editor-comment">
<p>The second part may be something new, but there are opportunities to improve the benchmarking. It is about the pupil-tutor imitation analysis. They introduce a convolutional neural network that takes triplets as an input (each tripled is essentially 3 images stacked together such that you have (anchor, positive, negative), Anchor is a reference spectrogram from, say finch A; positive means a different spectrogram with the same label as anchor from finch A, and negative means a spectrogram not related to A or different syllable label from A. The network is then trained to produce a low-dimensional embedding by ensuring the embedding distance between anchor and positive is less than anchor and negative by a certain margin. Based on the embedding, they then made use of earth mover distance to quantify the similarity in the syllable distribution among finches. They then compared their approach performance with that of sound analysis pro (SAP) and a variant of SAP. A more natural comparison, which they didn't include, is with the VAE approach by Goffinet et al. In this paper (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.67855">https://doi.org/10.7554/eLife.67855</ext-link>, Fig 7), they also attempted to perform an analysis on the tutor pupil song.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion. We have included a comparison of our triplet loss embedding model to the VAE model proposed in Goffinet et al. 2021. We also included comparisons of similarity scoring using each of these embedding models combined with either earth mover’s distance (EMD) or maximum mean discrepancy (MMD) to calculate the similarity of the embeddings, as was done in Goffinet et al. 2021. As discussed in the updated results section of the paper and shown in the new Figure 6–figure supplement 1, the Triplet loss model with MMD performs best for evaluating song learning on new birds, not included in model training. We’ve updated the main text of the paper to reflect this switch from EMD to MMD for the primary similarity scoring approach.</p>
<disp-quote content-type="editor-comment">
<p>Reviewer #2 (Public Review):</p>
<p>Summary:</p>
<p>In this work, the authors present a new Python software package, Avian Vocalization Network (AVN) aimed at facilitating the analysis of birdsong, especially the song of the zebra finch, the most common songbird model in neuroscience. The package handles some of the most common (and some more advanced) song analyses, including segmentation, syllable classification, featurization of song, calculation of tutor-pupil similarity, and age prediction, with a view toward making the entire process friendlier to experimentalists working in the field.</p>
<p>For many years, Sound Analysis Pro has served as a standard in the songbird field, the first package to extensively automate songbird analysis and facilitate the computation of acoustic features that have helped define the field. More recently, the increasing popularity of Python as a language, along with the emergence of new machine learning methods, has resulted in a number of new software tools, including the vocalpy ecosystem for audio processing, TweetyNet (for segmentation), t-SNE and UMAP (for visualization), and autoencoder-based approaches for embedding.</p>
<p>Strengths:</p>
<p>The AVN package overlaps several of these earlier efforts, albeit with a focus on more traditional featurization that many experimentalists may find more interpretable than deep learning-based approaches. Among the strengths of the paper are its clarity in explaining the several analyses it facilitates, along with high-quality experiments across multiple public datasets collected from different research groups. As a software package, it is open source, installable via the pip Python package manager, and features high-quality documentation, as well as tutorials. For experimentalists who wish to replicate any of the analyses from the paper, the package is likely to be a useful time saver.</p>
<p>Weaknesses:</p>
<p>I think the potential limitations of the work are predominantly on the software end, with one or two quibbles about the methods.</p>
<p>First, the software: it's important to note that the package is trying to do many things, of which it is likely to do several well and few comprehensively. Rather than a package that presents a number of new analyses or a new analysis framework, it is more a codification of recipes, some of which are reimplementations of existing work (SAP features), some of which are essentially wrappers around other work (interfacing with WhisperSeg segmentations), and some of which are new (similarity scoring). All of this has value, but in my estimation, it has less value as part of a standalone package and potentially much more as part of an ecosystem like vocalpy that is undergoing continuous development and has long-term support.</p>
</disp-quote>
<p>We appreciate this reviewer’s comments and concerns about the structure of the AVN package and its long-term maintenance. We have considered incorporating AVN into the VocalPy ecosystem but have chosen not to for a few key reasons. (1) AVN was designed with ease of use for experimenters with limited coding experience top of mind. VocalPy provides excellent resources for researchers with some familiarity with object-oriented programming to manage and analyze their datasets; however, we believe it may be challenging for users without such experience to adopt VocalPy quickly. AVN’s ‘recipe’ approach, as you put it, is very easily accessible to new users, and allows users with intermediate coding experience to easily navigate the source code to gain a deeper understanding of the methodology. AVN also consistently outputs processed data in familiar formats (tables in .csv files which can be opened in excel), in an effort to make it more accessible to new users, something which would be challenging to reconcile with VocalPy’s emphasis on their `dataset`classes. (2) AVN and VocalPy differ in their underlying goals and philosophies when it comes to flexibility vs. standardization of analysis pipelines. VocalPy is designed to facilitate mixing-and-matching of different spectrogram generation, segmentation, annotation etc. approaches, so that researchers can design and implement their own custom analysis pipelines. This flexibility is useful in many cases. For instance, it could allow researchers who have very different noise filtering and annotation needs, like those working with field recordings versus acoustic chamber recordings, to analyze their data using this platform. However, when it comes to comparisons across zebra finch research labs, this flexibility comes at the expense of direct comparison and integration of song features across research groups. This is the context in which AVN is most useful. It presents a single approach to song segmentation, labeling, and featurization that has been shown to generalize well across research groups, and which allows direct comparisons of the resulting features. AVN’s single, extensively validated, standard pipeline approach is fundamentally incompatible with VocalPy’s emphasis on flexibility. We are excited to see how VocalPy continues to evolve in the future, and recognize the value that both AVN and VocalPy bring to the songbird research community, each with their own distinct strengths, weaknesses, and ideal use cases.</p>
<disp-quote content-type="editor-comment">
<p>While the code is well-documented, including web-based documentation for both the core package and the GUI, the latter is available only on Windows, which might limit the scope of adoption.</p>
</disp-quote>
<p>We thank the reviewer for their kind words about AVN’s documentation. We recognize that the GUI’s exclusive availability on Windows is a limitation, and we would be happy to collaborate with other researchers and developers in the future to build a Mac compatible version, should the demand present itself. That said, the python package works on all operating systems, so non-Windows users still have the ability to use AVN that way.</p>
<disp-quote content-type="editor-comment">
<p>That is to say, whether AVN is adopted by the field in the medium term will have much more to do with the quality of its maintenance and responsiveness to users than any particular feature, but I believe that many of the analysis recipes that the authors have carefully worked out may find their way into other code and workflows.</p>
<p>Second, two notes about new analysis approaches:</p>
<p>(1) The authors propose a new means of measuring tutor-pupil similarity based on first learning a latent space of syllables via a self-supervised learning (SSL) scheme and then using the earth mover's distance (EMD) to calculate transport costs between the distributions of tutors' and pupils' syllables. While to my knowledge this exact method has not previously been proposed in birdsong, I suspect it is unlikely to differ substantially from the approach of autoencoding followed by MMD used in the Goffinet et al. paper. That is, SSL, like the autoencoder, is a latent space learning approach, and EMD, like MMD, is an integral probability metric that measures discrepancies between two distributions. (Indeed, the two are very closely related: <ext-link ext-link-type="uri" xlink:href="https://stats.stackexchange.com/questions/400180/earth-movers-distance-andmaximum-mean-discrepency">https://stats.stackexchange.com/questions/400180/earth-movers-distance-andmaximum-mean-discrepency</ext-link>.) Without further experiments, it is hard to tell whether these two approaches differ meaningfully. Likewise, while the authors have trained on a large corpus of syllables to define their latent space in a way that generalizes to new birds, it is unclear why such an approach would not work with other latent space learning methods.</p>
</disp-quote>
<p>We recognize the similarities between these approaches and have included comparisons of the VAE and MMD as in the Goffinet paper to our triplet loss model and EMD.  As discussed in the updated results section of the paper and shown in the new Figure 6–figure supplement 1, the Triplet loss model with MMD performs best for evaluating song learning on new birds, not included in model training. We’ve updated the main text of the paper to reflect this switch from EMD to MMD for the primary similarity scoring approach.</p>
<disp-quote content-type="editor-comment">
<p>(2) The authors propose a new method for maturity scoring by training a model (a generalized additive model) to predict the age of the bird based on a selected subset of acoustic features. This is distinct from the &quot;predicted age&quot; approach of Brudner, Pearson, and Mooney, which predicts based on a latent representation rather than specific features, and the GAM nicely segregates the contribution of each. As such, this approach may be preferred by many users who appreciate its interpretability.</p>
<p>In summary, my view is that this is a nice paper detailing a well-executed piece of software whose future impact will be determined by the degree of support and maintenance it receives from others over the near and medium term.</p>
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>The authors invent song and syllable discrimination tasks they use to train deep networks. These networks they then use as a basis for routine song analysis and song evaluation tasks. For the analysis, they consider both data from their own colony and from another colony the network has not seen during training. They validate the analysis scores of the network against expert human annotators, achieving a correlation of 80-90%.</p>
<p>Strengths:</p>
<p>(1) Robust Validation and Generalizability: The authors demonstrate a good performance of the AVN across various datasets, including individuals exhibiting deviant behavior. This extensive validation underscores the system's usefulness and broad applicability to zebra finch song analysis, establishing it as a potentially valuable tool for researchers in the field.</p>
<p>(2) Comprehensive and Standardized Feature Analysis: AVN integrates a comprehensive set of interpretable features commonly used in the study of bird songs. By standardizing the feature extraction method, the AVN facilitates comparative research, allowing for consistent interpretation and comparison of vocal behavior across studies.</p>
<p>(3) Automation and Ease of Use. By being fully automated, the method is straightforward to apply and should introduce barely an adoption threshold to other labs.</p>
<p>(4) Human experts were recruited to perform extensive annotations (of vocal segments and of song similarity scores). These annotations released as public datasets are potentially very valuable.</p>
<p>Weaknesses:</p>
<p>(1) Poorly motivated tasks. The approach is poorly motivated and many assumptions come across as arbitrary. For example, the authors implicitly assume that the task of birdsong comparison is best achieved by a system that optimally discriminates between typical, deaf, and isolated songs. Similarly, the authors assume that song development is best tracked using a system that optimally estimates the age of a bird given its song. My issue is that these are fake tasks since clearly, researchers will know whether a bird is an isolated or a deaf bird, and they will also know the age of a bird, so no machine learning is needed to solve these tasks. Yet, the authors imagine that solving these placeholder tasks will somehow help with measuring important aspects of vocal behavior.</p>
</disp-quote>
<p>We appreciate this reviewer’s concerns and apologize for not providing sufficiently clear rationale for the inclusion of our phenotype classifier and age regression models in the original manuscript. These tasks are not intended to be taken as a final, ultimate culmination of the AVN pipeline. Rather, we consider the carefully engineered 55-interpretable feature set to be AVN’s final output, and these analyses serve merely as examples of how that feature set can be applied. That said, each of these models do have valid experimental use cases that we believe are important and would like to bring to the attention of the reviewer.</p>
<p>For one, we showed how the LDA model that can discriminate between typical, deaf, and isolate birds’ songs not only allows us to evaluate which features are most important for discriminating between these groups, but also allows comparison of the FoxP1 knock-down (FP1 KD) birds to each of these phenotypes. Based on previous work (Garcia-Oscos et al. 2021), we hypothesized that FP1 KD in these birds specifically impaired tutor song memory formation while sparing a bird’s ability to refine their own vocalizations through auditory feedback. Thus, we would expect their songs to resemble those of isolate birds, who lack a tutor song memory, but not to resemble deaf birds who lack a tutor song memory and auditory feedback of their own vocalizations to guide learning. The LDA model allowed us to make this comparison quantitatively for the first time and confirm our hypothesis that FP1 KD birds’ songs are indeed most like isolates’. In the future, as more research groups publish their birds’ AVN feature sets, we hope to be able to make even more fine-grained comparisons between different groups of birds, either using LDA or other similar interpretable classifiers.</p>
<p>The age prediction model also has valid real-world use cases. For instance, one might imagine an experimental manipulation that is hypothesized to accelerate or slow song maturation in juvenile birds. This age prediction model could be applied to the AVN feature sets of birds having undergone such a manipulation to determine whether their predicted ages systematically lead or lag their true biological ages, and which song features are most responsible for this difference. We didn’t have access to data for any such birds for inclusion in this paper, but we hope that others in the future will be able to take inspiration from our methodology and use this or a similar age regression model with AVN features in their research. We have added a couple lines to the ‘Comparing Song Disruptions with AVN Features’ and ‘Tracking Song Development with AVN Features’ sections of the results to make this more clear.</p>
<disp-quote content-type="editor-comment">
<p>Along similar lines, authors assume that a good measure of similarity is one that optimally performs repeated syllable detection (i.e. to discriminate same syllable pairs from different pairs). The authors need to explain why they think these placeholder tasks are good and why no better task can be defined that more closely captures what researchers want to measure. Note: the standard tasks for self-supervised learning are next word or masked word prediction, why are these not used here?</p>
</disp-quote>
<p>This reviewer appears to have misunderstood our similarity scoring embedding model and our rationale for using it. We will explain it in more depth here and have added a paragraph to the ‘Measuring Song Imitation’ section of the results explaining this rationale more briefly.</p>
<p>First, nowhere are we training a model to discriminate between same and different syllable pairs. The triplet loss network is trained to embed syllables in an 8-dimensional space such that syllables with the same label are closer together than syllables with different labels. The loss function is related to the relative distance between embeddings of syllables with the same or different labels, not the classification of syllables as same or different. This approach was chosen because it has repeatedly been shown to be a useful data compression step (Schorff et al. 2015, Thakur et al. 2019) before further downstream tasks are applied on its output, particularly in contexts where there is little data per class (syllable label). For example, Schorff et al. 2015 trained a deep convolutional neural network with triplet loss to embed images of human faces from the same individual closer together than images of different individuals in a 128dimensional space. They then used this model to compute 128-dimensional representations of additional face images, not included in training, which were used for individual facial recognition (this is a same vs. different category classifier), and facial clustering, achieving better performance than the previous state of the art. The triplet loss function results in a model that can generate useful embeddings of previously unseen categories, like new individuals’ faces, or new zebra finches’ syllables, which can then be used in downstream analyses. This meaningful, lower dimensional space allows comparisons of distributions of syllables across birds, as in Brainard and Mets 2008, and Goffinet et al. 2021.</p>
<p>Next word and masked word prediction are indeed common self-supervised learning tasks for models working with text data, or other data with meaningful sequential organization. That is not the case for our zebra finch syllables, where every bird’s syllable sequence depends only on its tutor’s sequence, and there is no evidence for strong universal syllable sequencing rules (James et al. 2020). Rather, our embedding model is an example of a computer vision task, as it deals with sets of two-dimensional images (spectrograms), not sequences of categorical variables (like text). It is also not, strictly speaking, a selfsupervised learning task, as it does require syllable labels to generate the triplets. A common selfsupervised approach for dimensionality reduction in a computer vision task such as this one would be to train an autoencoder to compress images to a lower dimensional space, then faithfully reconstruct them from the compressed representation.  This has been done using a variational autoencoder trained on zebra finch syllables in Goffinet et al. 2021. In keeping with the suggestions from reviewers #1 and #2, we have included a comparison of our triplet loss model with the Goffinet et al. VAE approach in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(2) The machine learning methodology lacks rigor. The aims of the machine learning pipeline are extremely vague and keep changing like a moving target. Mainly, the deep networks are trained on some tasks but then authors evaluate their performance on different, disconnected tasks. For example, they train both the birdsong comparison method (L263+) and the song similarity method (L318+) on classification tasks. However, they evaluate the former method (LDA) on classification accuracy, but the latter (8-dim embeddings) using a contrast index. In machine learning, usually, a useful task is first defined, then the system is trained on it and then tested on a held-out dataset. If the sensitivity index is important, why does it not serve as a cost function for training?</p>
</disp-quote>
<p>Again, this reviewer seems not to understand our similarity scoring methodology. Our similarity scoring model is not trained on a classification task, but rather on an embedding task. It learns to embed spectrograms of syllables in an 8-dimensional space such that syllables with the same label are closer together than syllables with different labels. We could report the loss values for this embedding task on our training and validation datasets, but these wouldn’t have any clear relevance to the downstream task of syllable distribution comparison where we are using the model’s embeddings. We report the contrast index as this has direct relevance to the actual application of the model and allows comparisons to other similarity scoring methods, something that the triplet loss values wouldn’t allow.</p>
<p>The triplet loss method was chosen because it has been shown to yield useful low-dimensional representations of data, even in cases where there is limited labeled training data (Thakur et al. 2019). While we have one of the largest manually annotated datasets of zebra finch songs, it is still quite small by industry deep learning standards, which is why we chose a method that would perform well given the size of our dataset. Training a model on a contrast index directly would be extremely computationally intensive and require many more pairs of birds with known relationships than we currently have access to. It could be an interesting approach to take in the future, but one that would be unlikely to perform well with a dataset size typical to songbird research.</p>
<disp-quote content-type="editor-comment">
<p>Also, usually, in solid machine learning work, diverse methods are compared against each other to identify their relative strengths. The paper contains almost none of this, e.g. authors examined only one clustering method (HDBSCAN).</p>
</disp-quote>
<p>We did compare multiple methods for syllable segmentation (WhisperSeg, TweetyNet, and Amplitude thresholding) as this hadn’t been done previously. We chose not to perform extensive comparison of different clustering methods as Sainburg et al. 2020 already did so and we felt no need to reduplicate this effort. We encourage this reviewer to refer to Sainburg et al.’s excellent work for comparisons of multiple clustering methods applied to zebra finch song syllables.</p>
<disp-quote content-type="editor-comment">
<p>(3) Performance issues. The authors want to 'simplify large-scale behavioral analysis' but it seems they want to do that at a high cost. (Gu et al 2023) achieved syllable scores above 0.99 for adults, which is much larger than the average score of 0.88 achieved here (L121). Similarly, the syllable scores in (Cohen et al 2022) are above 94% (their error rates are below 6%, albeit in Bengalese finches, not zebra finches), which is also better than here. Why is the performance of AVN so low? The low scores of AVN argue in favor of some human labeling and training on each bird.</p>
</disp-quote>
<p>Firstly, the syllable error rate scores reported in Cohen et al. 2022 are calculated very differently than the F1 scores we report here and are based on a model trained with data from the same bird as was used in testing, unlike our more general segmentation approach where the model was tested on different birds than were used in training. Thus, the scores reported in Cohen et al. and the F1 scores that we report cannot be compared.</p>
<p>The discrepancy between the F1<sub>seg</sub> scores reported in Gu et al. 2023 and the segmentation F1 scores that we report are likely due to differences in the underlying datasets. Our UTSW recordings tend to have higher levels of both stationary and non-stationary background noise, which make segmentation more challenging. The recordings from Rockefeller were less contaminated by background noise, and they resulted in slightly higher F1 scores. That said, we believe that the primary factor accounting for this difference in scores with Gu et al. 2023 is the granularity of our ‘ground truth’ syllable segments. In our case, if there was never any ambiguity as to whether vocal elements should be segmented into two short syllables with a very short gap between them or merged into a single longer syllable, we chose to split them. WhisperSeg had a strong tendency to merge the vocal elements in ambiguous cases such as these. This results in a higher rate of false negative syllable onset detections, reflected in the low recall scores achieved by WhisperSeg (see Figure 2–figure supplement 1b), but still very high precision scores (Figure 2–figure supplement 1a). While WhisperSeg did frequently merge these syllables in a way that differed from our ground truth segmentation, it did so consistently, meaning it had little impact on downstream measures of syntax entropy (Figure 3c) or syllable duration entropy (Figure 3–figure supplement 2a). It is for that reason that, despite a lower F1 score, we still consider AVN’s automatically generated annotations to be sufficiently accurate for downstream analyses.</p>
<p>Should researchers require a higher degree of accuracy and precision with their annotations (for example, to detect very subtle changes in song before and after an acute manipulation) we suggest they turn toward one of the existing tools for supervised song annotation, such as TweetyNet.</p>
<disp-quote content-type="editor-comment">
<p>(4) Texas bias. It is true that comparability across datasets is enhanced when everyone uses the same code. However, the authors' proposal essentially is to replace the bias between labs with a bias towards birds in Texas. The comparison with Rockefeller birds is nice, but it amounts to merely N=1. If birds in Japanese or European labs have evolved different song repertoires, the AVN might not capture the associated song features in these labs well.</p>
</disp-quote>
<p>We appreciate the author’s concern about a bias toward birds from the UTSW colony. However, this paper shows that despite training (for the similarity scoring) and hyperparameter fitting (for the HDBSCAN clustering) on the UTSW birds, AVN performs as well if not better on birds from Rockefeller than from UTSW. To our knowledge, there are no publicly available datasets of annotated zebra finch songs from labs in Europe or in Asia but we would be happy to validate AVN on such datasets, should they become available. Furthermore, there is no evidence to suggest that there is dramatic drift in zebra finch vocal repertoire between continents which would necessitate such additional validation. While we didn’t have manual annotations for this dataset (which would allow validation of our segmentation and labeling methods), we did apply AVN to recordings shared with us by the Wada lab in Japan, where visual inspection of the resulting annotations suggested comparable accuracy to the UTSW and Rockefeller datasets.</p>
<disp-quote content-type="editor-comment">
<p>(5) The paper lacks an analysis of the balance between labor requirement, generalizability, and optimal performance. For tasks such as segmentation and labeling, fine-tuning for each new dataset could potentially enhance the model's accuracy and performance without compromising comparability. E.g. How many hours does it take to annotate hundred song motifs? How much would the performance of AVN increase if the network were to be retrained on these? The paper should be written in more neutral terms, letting researchers reach their own conclusions about how much manual labor they want to put into their data.</p>
</disp-quote>
<p>With standardization and ease of use in mind, we designed AVN specifically to perform fully automated syllable annotation and downstream feature calculations. We believe that we have demonstrated in this manuscript that our fully automated approach is sufficiently reliable for downstream analyses across multiple zebra finch colonies. That said, if researchers require an even higher degree of annotation precision and accuracy, they can turn toward one of the existing methods for supervised song annotation, such as TweetyNet. Incorporating human annotations for each bird processed by AVN is likely to improve its performance, but this would require significant changes to AVN’s methodology, and is outside the scope of our current efforts.</p>
<disp-quote content-type="editor-comment">
<p>(6) Full automation may not be everyone's wish. For example, given the highly stereotyped zebra finch songs, it is conceivable that some syllables are consistently mis-segmented or misclassified. Researchers may want to be able to correct such errors, which essentially amounts to fine-tuning AVN. Conceivably, researchers may want to retrain a network like the AVN on their own birds, to obtain a more fine-grained discriminative method.</p>
</disp-quote>
<p>Other methods exist for supervised or human-in-the-loop annotation of zebra finch songs, such as TweetyNet and DAN (Alam et al. 2023). We invite researchers who require a higher degree of accuracy than AVN can provide to explore these alternative approaches for song annotation. Incorporating human feedback into AVN was never the goal of our pipeline, would require significant changes to AVN’s design and is outside the scope of this manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(7) The analysis is restricted to song syllables and fails to include calls. No rationale is given for the omission of calls. Also, it is not clear how the analysis deals with repeated syllables in a motif, whether they are treated as two-syllable types or one.</p>
</disp-quote>
<p>It is true that we don’t currently have any dedicated features to describe calls. This could be a useful addition to AVN in the future.</p>
<p>What a human expert inspecting a spectrogram would typically call ‘repeated syllables’ in a bout are almost always assigned the same syllable label by the UMAP+HDBSCAN clustering. The syntax analysis module includes features examining the rate of syllable repetitions across syllable types, as mentioned in lines 222-226 of the revised manuscript. See <ext-link ext-link-type="uri" xlink:href="https://avn.readthedocs.io/en/latest/syntax_analysis_demo.html#Syllable-Repetitions">https://avn.readthedocs.io/en/latest/syntax_analysis_demo.html#Syllable-Repetitions</ext-link> for further details.</p>
<disp-quote content-type="editor-comment">
<p>(8) It seems not all human annotations have been released and the instruction sets given to experts (how to segment syllables and score songs) are not disclosed. It may well be that the differences in performance between (Gu et al 2023) and (Cohen et al 2022) are due to differences in segmentation tasks, which is why these tasks given to experts need to be clearly spelled out. Also, the downloadable files contain merely labels but no identifier of the expert. The data should be released in such a way that lets other labs adopt their labeling method and cross-check their own labeling accuracy.</p>
</disp-quote>
<p>All human annotations used in this manuscript have indeed been released as part of the accompanying dataset. Syllable annotations are not provided for all pupils and tutors used to validate the similarity scoring, as annotations are not necessary for similarity comparisons. We have expanded our description of our annotation guidelines in the methods section of the revised manuscript. All the annotations were generated by one of two annotators. The second annotator always consulted with the first annotator in cases of ambiguous syllable segmentation or labeling, to ensure that they had consistent annotation styles. Unfortunately, we haven’t retained records about which birds were annotated by which of the two annotators, so we cannot share this information along with the dataset. The data is currently available in a format that should allow other research groups to use our annotations either to train their own annotation systems or check the performance of their existing systems on our annotations.</p>
<disp-quote content-type="editor-comment">
<p>(9) The failure modes are not described. What segmentation errors did they encounter, and what syllable classification errors? It is important to describe the errors to be expected when using the method.</p>
</disp-quote>
<p>As we discussed in our response to this reviewer’s point (3), WhisperSeg has a tendency to merge syllables when the gap between them is very short, which explains its lower recall score compared to its precision on our dataset (Figure 2–figure supplement 1). In rare cases, WhisperSeg also fails to recognize syllables entirely, again impacting its precision score. TweetyNet hardly ever completely ignores syllables, but it does tend to occasionally merge syllables together or over-segment them. Whereas WhisperSeg does this very consistently for the same syllable types within the same bird, TweetyNet merges or splits syllables more inconsistently. This inconsistent merging and splitting has a larger effect on syllable labeling, as manifested in the lower clustering v-measure scores we obtain with TweetyNet compared to WhisperSeg segmentations. TweetyNet also has much lower precision than WhisperSeg, largely because TweetyNet often recognizes background noises (like wing flaps or hopping) as syllables whereas WhisperSeg hardly ever segments non-vocal sounds.</p>
<p>Many errors in syllable labeling stem from differences in syllable segmentation. For example, if two syllables with labels ‘a’ and ‘b’ in the manual annotation are sometimes segmented as two syllables, but sometimes merged into a single syllable, the clustering is likely to find 3 different syllable types; one corresponding to ‘a’, one corresponding to ‘b’ and one corresponding to ‘ab’ merged. Because of how we align syllables across segmentation schemes for the v-measure calculation, this will look like syllable ‘b’ always has a consistent cluster label (or is missing a label entirely), but syllable ‘a’ can carry two different cluster labels, depending on the segmentation. In certain cases, even in the absence of segmentation errors, a group of syllables bearing the same manual annotation label may be split into 2 or 3 clusters (it is extremely rare for a single manual annotation group to be split into more than 3 clusters). In these cases, it is difficult to conclusively say whether the clustering represents an error, or if it actually captured some meaningful systematic difference between syllables that was missed by the annotator. Finally, sometimes rare syllable types with their own distinct labels in the manual annotation are merged into a single cluster. Most labeling errors can be explained by this kind of merging or splitting of groups relative to the manual annotation, not to occasional mis-classifications of one manual label type as another.</p>
<p>For examples of these types of errors, we encourage this reviewer and readers to refer to the example confusion matrices in figure 2f and Figure 2–figure supplement 3b&amp;e. We also added two paragraphs to the end of the ‘Accurate, fully unsupervised syllable labeling’ section of the Results in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(10) Usage of Different Dimensionality Reduction Methods: The pipeline uses two different dimensionality reduction techniques for labeling and similarity comparison - both based on the understanding of the distribution of data in lower-dimensional spaces. However, the reasons for choosing different methods for different tasks are not articulated, nor is there a comparison of their efficacy.</p>
</disp-quote>
<p>We apologize for not making this distinction sufficiently clear in the manuscript and have added a paragraph to the ‘Measuring Song Imitation’ section of the Results explaining the rational for using an embedding model for similarity scoring.</p>
<p>We chose to use UMAP for syllable labeling because it is a common embedding methodology to precede hierarchical clustering and has been shown to result in reliable syllable labels for birdsong in the past (Sainburg et al. 2020). However, it is not appropriate for similarity scoring, because comparing EMD or MMD scores between birds requires that all the birds’ syllable distributions exist within the same shared embedding space. This can be achieved by using the same triplet loss-trained neural network model to embed syllables from all birds. This cannot be achieved with UMAP because all birds whose scores are being compared would need to be embedded in the same UMAP space, as distances between points cannot be compared across UMAPs. In practice, this would mean that every time a new tutor-pupil pair needs to be scored, their syllables would need to be added to a matrix with all previously compared birds’ syllables, a new UMAP would need to be computed, and new EMD or MMD scores between all bird pairs would need to be calculated using their new UMAP embeddings. This is very computationally expensive and quickly becomes unfeasible without dedicated high power computing infrastructure. It also means that similarity scores couldn’t be compared across papers without recomputing everything each time, whereas EMD and MMD scores obtained with triplet loss embeddings can be compared, provided they use the same trained model (which we provide as part of AVN) to embed their syllables in a common latent space.</p>
<disp-quote content-type="editor-comment">
<p>(11) Reproducibility: are the measurements reproducible? Systems like UMAP always find a new embedding given some fixed input, so the output tends to fluctuate.</p>
</disp-quote>
<p>There is indeed a stochastic element to UMAP embeddings which will result in different embeddings and therefore different syllable labels across repeated runs with the same input. We observed that v-measures scores were quite consistent within birds across repeated runs of the UMAP, and have added an additional supplementary figure to the revised manuscript showing this (Figure 2–figure supplement 4).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>(1) Benchmark their similarity score to the method used by Goffinet et al, 2021 from the Pearson group. Such a comparison would be really interesting and useful.</p>
</disp-quote>
<p>This has been added to the paper.</p>
<disp-quote content-type="editor-comment">
<p>(2) Please clarify exactly what is new and what is applied from existing methods to help the reader see the novelty of the paper.</p>
</disp-quote>
<p>We have added more emphasis on the novel aspects of our pipeline to the paper’s introduction.</p>
<disp-quote content-type="editor-comment">
<p>Minor:</p>
<p>It's unclear if AVN is appropriate as the paper deals only with zebra finch song - the scope is more limited than advertised.</p>
</disp-quote>
<p>We assume this is in reference to ‘Birdsong’ in the paper’s title and ‘Avian’ in Avian Vocalization Network. There is a brief discussion of how these methods are likely to perform on other commonly studied songbird species at the end of the discussion section.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>A few points for the authors to consider that might strengthen or inform the paper:</p>
<p>(1) In the public review, I detailed some ways in which the SSL+EMD approach is unlikely to be appreciably distinct from the VAE+MMD approach -- in fact, one could mix and match here. It would strengthen the authors' claim if they showed via experiments that their method outperforms VAE+MMD, but in the absence of that, a discussion of the relation between the two is probably warranted.</p>
</disp-quote>
<p>This comparison has been added to the paper.</p>
<disp-quote content-type="editor-comment">
<p>(2) ll. 305-310: This loss of accuracy near the edge is expected on general Bayesian grounds. Any regression approach should learn to estimate the conditional mean of the age distribution given the data, so ages estimated from data will be pulled inward toward the location of most training data. This bias is somewhat mitigated in the Brudner paper by a more flexible model, but it's a general (and expected) feature of the approach.</p>
<p>(3) While the online AVA documentation looks good, it might benefit from a page on design philosophy that lays out how the various modules fit together - something between the tutorials and the nitty-gritty API. That way, users would be able to get a sense of where they should look if they want to harness pieces of functionality beyond the tutorials.</p>
</disp-quote>
<p>Thank you for this suggestion. We will add a page on AVN’s design philosophy to the online documentation.</p>
<disp-quote content-type="editor-comment">
<p>(4) While the manuscript does compare AVN to packages like TweetyNet and AVA that share some functionality, it doesn't really mention what's been going on with the vocalpy ecosystem, where the maintainers have been doing a lot to standardize data processing, integrate tools, etc. I would suggest a few words about how AVN might integrate with these efforts.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion.</p>
<disp-quote content-type="editor-comment">
<p>(5) ll. 333-336: It would be helpful to provide a citation to some of the self-supervised learning literature this procedure is based on. Some citations are provided in methods, but the general approach is worth citing, in my opinion.</p>
</disp-quote>
<p>We have added a paragraph to the results section with more background on self-supervised learning for dimensionality reduction, particularly in the context of similarity scoring.</p>
<disp-quote content-type="editor-comment">
<p>(6) One software concern for medium-term maintenance: AVN docs say to use Python 3.8, and GitHub says the package is 3.9 compatible. I also saw in the toml file that 3.10 and above are not supported. It's worth noting that Python 3.9 reaches its end of life in October 2025, so some dependencies may have to be altered or changed for the package to be viable going forward.</p>
</disp-quote>
<p>Thank you for this comment. We will continue to maintain AVN and update its dependencies as needed.</p>
<disp-quote content-type="editor-comment">
<p>Minor points:</p>
<p>(1) It might be good to note that WhisperSeg is a different install from AVN. May be hard for novice users, though there's a web interface that's available.</p>
</disp-quote>
<p>We’ve added a line to the methods section making this clear.</p>
<disp-quote content-type="editor-comment">
<p>(2) Figure 6b: Some text in the y-axis labels is overlapping here.</p>
</disp-quote>
<p>This has been fixed. Thank you for bringing it to our attention.</p>
<disp-quote content-type="editor-comment">
<p>(3) The name of the Python language is always capitalized.</p>
</disp-quote>
<p>We’ve fixed this capitalization error throughout the manuscript. Thank you.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>(1) I recommend that the authors improve the motivation of the chosen tasks and data or choose new tasks that more clearly speak to the optimizations they want to perform.</p>
</disp-quote>
<p>We have included more details about the motivation for our LDA classification analysis, age prediction model and embedding model for similarity scoring in the results of the revised manuscript, as discussed in more detail in the above responses to this reviewer. Thank you for these suggestions.</p>
<disp-quote content-type="editor-comment">
<p>(2) They need to rigorously report the (classification) scores on the test datasets: these are the scores associated with the cost function used during training.</p>
</disp-quote>
<p>Based on this reviewer’s ‘Weaknesses: 3’ comment in the public reviews, we believe that they are referring to a classification score for the triplet loss model. As we explained in response to that comment, this is not a classification task, therefor there is no classification score to report. The loss function used to train the model was a triplet loss function. While we could report these values, they are not informative for how well this approach would perform in a similarity scoring context, as explained above. As such, we prefer to include contrast index and tutor contrast index scores to compare the models’ performance for similarity score, as these are directly relevant to the task and are established in the field for said task.</p>
<disp-quote content-type="editor-comment">
<p>(3) They need to explain the reasons for the poor performance (or report on the inconsistencies with previous work) and why they prefer a fully automated system rather than one that needs some fine-tuning on bird-specific data.</p>
</disp-quote>
<p>We’ve addressed this comment in the public response to this reviewer’s weakness points 3, 5, and 6.</p>
<disp-quote content-type="editor-comment">
<p>(4) They should consider applying their method to data from Japanese and European labs.</p>
</disp-quote>
<p>We’ve addressed this comment in the public response to this reviewer’s weakness point 4.</p>
<disp-quote content-type="editor-comment">
<p>(5) The need to document the failure modes and report all details about the human annotations.</p>
</disp-quote>
<p>We’ve added additional description of the failure modes for our segmentation and labeling approaches in the results section of the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Details:</p>
<p>The introduction is very vague, it fails to make a clear case of what the problem is and what the approach is. It reads a bit like an advertisement for machine learning: we are given a hammer and are looking for a nail.</p>
</disp-quote>
<p>We thank the reviewer for this viewpoint; however, we disagree and have decided to keep our Introduction largely unchanged.</p>
<disp-quote content-type="editor-comment">
<p>L46 That interpretability is needed to maximize the benefits of machine learning is wrong, see self-driving cars and chat GPT.</p>
</disp-quote>
<p>This line states that ‘To truly maximize the benefits of machine learning and deep learning methods for behavior analysis, their power must be balanced with interpretability and generalizability’. We firmly believe that interpretability is critically important when using machine learning tools to gain a deeper scientific understanding of data, including animal behavior data in a neuroscience context. We believe that the introduction and discussion of this paper already provide strong evidence for this claim.</p>
<disp-quote content-type="editor-comment">
<p>L64 What about zebra finches that repeat a syllable in the motif, how are repetitions dealt with by AVN?</p>
</disp-quote>
<p>This is already described in the results section in lines 222-226, and in the methods in the ‘Syntax Features: Repetition Bouts’ section.</p>
<disp-quote content-type="editor-comment">
<p>L107 Say a bit more here, what exactly has been annotated?</p>
</disp-quote>
<p>We’ve added a sentence in the introduction to clarify this. Line 113-115.</p>
<disp-quote content-type="editor-comment">
<p>L112 Define spectrogram frames. Do these always fully or sometimes partially contain a vocalization?</p>
</disp-quote>
<p>Spectrogram frames are individual time bins used to compute the spectrogram using a short-term Fourier transform. As described in the ‘Methods; Labeling : UMAP Dimensionality Reduction” section, our spectrograms are computed using ‘The short term Fourier transform of the normalized audio for each syllable […] with a window length of 512 samples and a hop length of 128 samples’. Given that the song files have a standard sampling rate of 44.1kHz, this means each time bin represents 11.6ms of song data, with successive frames advancing in time by 2.9ms. These contain only a small fraction of a vocalization.</p>
<disp-quote content-type="editor-comment">
<p>L122 The reported TweetyNet score of 0.824 is lower than the one reported in Figure 2a.</p>
</disp-quote>
<p>The center line in the box plot in Figure 2a represents the median of the distribution of TweetyNet vmeasure scores. Given that there are a couple outlying birds with very low scores, the mean (0.824 as reported in the text of the results section) is lower than the median. This is not an error.</p>
<disp-quote content-type="editor-comment">
<p>L155 Some of the differences in performance are very small, reporting of the P value might be necessary.</p>
</disp-quote>
<p>These methods are unlikely to statistically significantly differ in their validation scores. This doesn’t mean that we cannot use the mean/median values reported to justify favoring one method over another. This is why we’ve chosen not to report p-values here.</p>
<disp-quote content-type="editor-comment">
<p>L161 The authors have not really tested more than a single clustering method, failing to show a serious attempt to achieve good performance.</p>
</disp-quote>
<p>We’ve addressed this comment in the public response to this reviewer’s weakness point 2.</p>
<disp-quote content-type="editor-comment">
<p>L186 Did isolate birds produce stereotyped syllables that can be clustered?</p>
</disp-quote>
<p>Yes, they did. The validation for clustering of isolate bird songs can be found in Figure 2–figure supplement 4.</p>
<disp-quote content-type="editor-comment">
<p>Fig. 3e: How were the multiple bouts aligned?</p>
</disp-quote>
<p>This is described in lines 857-876 in the ‘Methods: Song Timing Features: Rhythm Spectrograms” section of the paper.</p>
<disp-quote content-type="editor-comment">
<p>L199 There is a space missing in front of (n=8).</p>
</disp-quote>
<p>Thank you for bringing this to our attention. It’s been corrected in the updated manuscript.</p>
<disp-quote content-type="editor-comment">
<p>L268 Define classification accuracy.</p>
</disp-quote>
<p>We’ve added a sentence in lines 953-954 of the methods section defining classification accuracy.</p>
<disp-quote content-type="editor-comment">
<p>L325 How many motifs need to be identified, why does this need to be done manually? There are semiautomated methods that can allow scaling, these should be  cited here. Also, the mention of bias here should be removed in favor of a more extensive discussion on the experimenter bias (traditionally vs Texas bias (in this paper).</p>
</disp-quote>
<p>All of the methods cited in this line have graphical user interfaces that require users to select a file containing song and manually highlight the start and end each motif to be compared. The exact number of motifs required varies depending on the specific context (e.g. more examples are needed to detect more subtle differences or changes in song similarity) but it is fairly standard for reviewers to score 30 – 100 pairs of motifs.</p>
<p>We’ve discussed the tradeoffs between full automation and supervised or human-in-the loop methods in response to this reviewer’s public comment ‘weakness #5 and 6’. Briefly, AVN’s aim is to standardize song analysis, to allow direct comparisons between song features and similarity scores across research groups. We believe, as explained in the paper, that this can be best achieve by having different research groups use the same deep learning models, which perform consistently well across those groups. Introducing semi-automated methods would defeat this benefit of AVN.</p>
<p>We’ve also addressed the question of ‘Texas bias’ in response to their reviewer’s public comment ‘Weakness #4’.</p>
<disp-quote content-type="editor-comment">
<p>L340 How is EMD applied? Syllables are points in 8-dim space, but now suddenly authors talk about distributions without explaining how they got from points to distributions. Same in L925.</p>
</disp-quote>
<p>We apologize for the confusion here. The syllable points in the 8-d space are collectively an empirical distribution, not a probability distribution. We referred to them simply as ‘distributions’ to limit technical jargon in the results of the paper, but have changed this to more precise language in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>L351 Why do authors now use 'contrast index' to measure performance and no longer 'classification accuracy'?</p>
</disp-quote>
<p>We’ve addressed this comment in the public response to this reviewer’s weakness points 1 and 2.</p>
<disp-quote content-type="editor-comment">
<p>Figure 6 What is the confusion matrix, i.e. how well can the model identify pupil-pupil pairings from pupiltutor and from pupil-unrelated pairings? I guess that would amount to something like classification accuracy.</p>
</disp-quote>
<p>There is no model classifying comparisons as pupil-pupil vs. pupil-tutor etc. These comparisons exist only to show the behavior of the similarity scoring approach, which consists of a dissimilarity measure (MMD or EMD) applied to low dimensional representations of syllable generated by the triplet loss model or VAE. This was clarified further in our public response to this reviewer’s weakness points 1 and 2.</p>
<disp-quote content-type="editor-comment">
<p>L487 What are 'song files', and what do they contain?</p>
</disp-quote>
<p>‘Song files’ are .wav files containing recordings of zebra finch song. They typically contain a single song bout, but they can include multiple song bouts if they are produced close together, or incomplete song bouts if the introductory notes were very soft or the bouts were very long (&gt;30s from the start of the file). Details of these recordings are provided in the ‘Methods: Data Acquisition: UTSW Dataset’ section of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>L497 Calls were only labelled for tweetynet but not for other tasks.</p>
</disp-quote>
<p>That is correct. The rationale for this is provided in the ‘Methods: Manual Song Annotation’ section of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>L637 There is a contradiction (can something be assigned to the 'own manual annotation category' when the same sentence states that this is done 'without manual annotation'?)</p>
</disp-quote>
<p>We believe there is confusion here between automated annotation and validation. Any bird can be automatically annotated without the need for any existing manual annotations for that individual bird. However, manual labels are required to compare automatically generated annotations against for validation of the method.</p>
<disp-quote content-type="editor-comment">
<p>L970 Spectograms of what? (what is the beginning of a song bout, L972).</p>
</disp-quote>
<p>The beginning of a song bout is the first introductory note produced by a bird after a period without vocalizations. This is standard.</p>
</body>
</sub-article>
</article>