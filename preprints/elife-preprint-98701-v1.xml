<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">98701</article-id>
<article-id pub-id-type="doi">10.7554/eLife.98701</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98701.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Hierarchical cortical entrainment orchestrates the multisensory processing of biological motion</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Shen</surname>
<given-names>Li</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Shuo</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tian</surname>
<given-names>Yuhao</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5756-2480</contrib-id>
<name>
<surname>Wang</surname>
<given-names>Ying</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5746-7301</contrib-id>
<name>
<surname>Jiang</surname>
<given-names>Yi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>State Key Laboratory of Brain and Cognitive Sciences, CAS Center for Excellence in Brain Science and Intelligence Technology, Institute of Psychology, Chinese Academy of Sciences</institution>, Beijing 100101, <country>China</country></aff>
<aff id="a2"><label>2</label><institution>Department of Psychology, University of Chinese Academy of Sciences</institution>, Beijing 100049, <country>China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Press</surname>
<given-names>Clare</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>Corresponding author</bold>: Ying Wang (<email>wangying@psych.ac.cn</email>)</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-07-16">
<day>16</day>
<month>07</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP98701</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-23">
<day>23</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-04-25">
<day>25</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.23.590751"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Shen et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Shen et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-98701-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>When observing others’ behaviors, we continuously integrate their movements with the corresponding sounds to achieve efficient perception and develop adaptive responses. However, how human brains integrate these complex audiovisual cues based on their natural temporal correspondence remains unknown. Using electroencephalogram, we demonstrated that cortical oscillations entrained to hierarchical rhythmic structures in audiovisually congruent human walking movements and footstep sounds. Remarkably, the entrainment effects at different time scales exhibit distinct modes of multisensory integration, i.e., an additive integration effect at a basic-level integration window (step-cycle) and a super-additive multisensory enhancement at a higher-order temporal integration window (gait-cycle). Moreover, only the cortical tracking of higher-order rhythmic structures is specialized for the multisensory integration of human motion signals and correlates with individuals’ autistic traits, suggesting its functional relevance to biological motion perception and social cognition. These findings unveil the multifaceted roles of entrained cortical activity in the multisensory perception of human motion, shedding light on how hierarchical cortical entrainment orchestrates the processing of complex, rhythmic stimuli in natural contexts.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>multisensory integration</kwd>
<kwd>biological motion</kwd>
<kwd>cortical entrainment</kwd>
<kwd>cortical tracking</kwd>
<kwd>neural oscillations</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The perception of biological motion (BM), the movements of living creatures, is a fundamental ability of the human visual system that is crucial in survival and social situations. Extensive evidence shows that humans can readily perceive BM from a visual display depicting just a handful of light dots attached to the head and major joints of a moving person (<xref ref-type="bibr" rid="c9">Blake &amp; Shiffrar, 2007</xref>). Nevertheless, in real life, BM perception often occurs in multisensory situations, e.g., one may simultaneously hear footstep sounds while seeing others walking. The integration of these visual and auditory BM cues based on congruency in stimulus contents or temporal relationships can facilitate the detection and discrimination of BM stimuli (<xref ref-type="bibr" rid="c31">Mendonça et al., 2011</xref>; <xref ref-type="bibr" rid="c39">Shen, Lu, Wang, et al., 2023</xref>; <xref ref-type="bibr" rid="c47">Thomas &amp; Shiffrar, 2013</xref>; <xref ref-type="bibr" rid="c49">van der Zwan et al., 2009</xref>). Remarkably, such a cross-modal effect appears to engage an audiovisual integration (AVI) mechanism specific to BM, as the effect disappeared when the visual BM signals were deprived of characteristic kinematic cues but not low-level motion attributes through stimulus inversion (<xref ref-type="bibr" rid="c11">Brooks et al., 2007</xref>; <xref ref-type="bibr" rid="c46">Thomas &amp; Shiffrar, 2010</xref>), and the temporal windows of perceptual audiovisual synchrony are different between BM and other motion stimuli with constant motion speed or gravity-incompatible accelerations (<xref ref-type="bibr" rid="c1">Arrighi et al., 2006</xref>; <xref ref-type="bibr" rid="c38">Saygin et al., 2008</xref>). Despite the behavioral evidence, the neural basis for the AVI of BM signals based on their natural multisensory correspondence remains largely unclear.</p>
<p>An intrinsic property of human movements (such as walking and running) is that they are rhythmic and accompanied by frequency-congruent sounds. The AVI of such rhythmic stimuli may involve cortical entrainment, a process that the neural oscillations in cortical networks entrain to external rhythms and show increased activity or phase coherence at corresponding frequencies (<xref ref-type="bibr" rid="c3">Bauer et al., 2020</xref>; <xref ref-type="bibr" rid="c26">Lakatos et al., 2019</xref>). Studies based on simple or discrete stimuli have found that temporal congruency in auditory and visual rhythms significantly enhances the cortical tracking of rhythmic stimulations in both modalities (<xref ref-type="bibr" rid="c35">Nozaradan et al., 2012b</xref>). Unlike these stimuli, BM conveys complex hierarchical rhythmic structures that could be extracted from integration windows at different temporal scales. For example, the human locomotion movement has a narrower integration window consisting of each step (i.e., step cycle) and a broader integration window incorporating the opponent motion of the two feet (i.e., gait cycle). A recent study suggests that neural entrainment to these hierarchical kinematic structures contributes to the spatiotemporal integration of visual BM cues in different manners (<xref ref-type="bibr" rid="c40">Shen, Lu, Yuan, et al., 2023</xref>). However, it remains open whether and how the cortical tracking of rhythmic signals underpins the AVI of BM information.</p>
<p>To tackle this issue, we recorded electroencephalogram (EEG) signals from participants who viewed rhythmic point-light walkers or/and listened to the corresponding footstep sounds under visual (V), auditory (A), and audiovisual (AV) conditions in Experiments 1a &amp; 1b (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). A greater cortical entrainment effect in the AV condition compared to each unisensory condition will indicate significant multisensory gains. Moreover, contrasting the multisensory response with the summation of the unisensory responses serves to distinguish among sub-additive (AV &lt; A+V), additive (AV = A+V), and super-additive (AV &gt; A+V) modes of multisensory integration (see a review by <xref ref-type="bibr" rid="c45">Stevenson et al., 2014</xref>). Experiment 2 further examined to what extent the AVI effect was specific to the multisensory processing of BM by using non-BM (inverted visual stimuli) as a control. Inversion disrupts the unique, gravity-compatible kinematic features of BM but not the rhythmic signals generated by low-level motion cues (<xref ref-type="bibr" rid="c27">Ma et al., 2022</xref>; <xref ref-type="bibr" rid="c40">Shen, Lu, Yuan, et al., 2023</xref>; <xref ref-type="bibr" rid="c41">Simion et al., 2008</xref>; <xref ref-type="bibr" rid="c48">Troje &amp; Westhoff, 2006</xref>; <xref ref-type="bibr" rid="c52">Wang et al., 2022</xref>), thus is expected to interfere with the BM-specific neural responses. Participants perceived the visual stimuli accompanied by temporally congruent or incongruent BM sounds. Comparing the congruency effect in neural responses between the upright and inverted conditions provides a unique opportunity to verify whether the AVI of BM involves a mechanism distinct from that underlies the AVI of non-BM information.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1</label>
<caption><title>Illustrations of audiovisual stimuli and experimental procedures.</title>
<p>The illustration was based on stimuli with a gait-cycle frequency of 1 Hz. <bold>(a) Visual stimuli.</bold> The left panel depicts the static schematic of upright and inverted point-light walkers. The right panel shows the keyframes from a gait cycle of the BM sequence. The colors of dots and lines between dots are for illustration only and are not shown in the experiments. <bold>(b) Auditory stimuli.</bold> The auditory sequences contain periodic impulses of footstep sounds whose peak amplitudes occur around the points when the foot strikes the ground. The duration of two successive impulses defines the gait cycle of footstep sounds, which is temporally congruent (Con) or incongruent (InC) with the visual stimuli. <bold>(c) Experimental procedure</bold>. The color of the visual stimuli changed one or two times within 6 s in the catch trials but did not change in the experimental trials. Participants were required to report the number of changes when the point-light stimulus was replaced by a red fixation. In Experiment 1, participants viewed rhythmic point-light walkers or/and listened to the corresponding footstep sounds under visual (V), auditory (A), and audiovisual (AV) conditions. The visual stimulus was the BM sequence in the V and AV conditions but a static frame from the sequence in the A condition. Experiment 2 included only the AV condition with different stimulus orientations (upright vs. inverted) and audiovisual congruency (congruent vs. incongruent).</p></caption>
<graphic xlink:href="590751v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>It is also worthy to note that the abilities to process BM information and integrate multisensory inputs vary across individuals and are diminished in populations with autism spectrum disorder (ASD) or even high autistic traits (<xref ref-type="bibr" rid="c17">Feldman et al., 2018</xref>; <xref ref-type="bibr" rid="c36">Pavlova, 2012</xref>; <xref ref-type="bibr" rid="c51">Wang et al., 2018</xref>). Specifically, ASD individuals showed reduced orienting to audiovisually synchronized BM stimuli (<xref ref-type="bibr" rid="c25">Klin et al., 2009</xref>), and such impairment in 10-month infancy can predict autism diagnosis at 3 years of age (<xref ref-type="bibr" rid="c15">Falck-Ytter et al., 2018</xref>). These findings suggest a possible link between compromised audiovisual BM processing ability and higher autistic-like traits, given that social cognitive deficits in ASD lie on a continuum extending from the clinical to nonclinical populations with different levels of autistic traits (<xref ref-type="bibr" rid="c2">Baron-Cohen et al., 2001</xref>). Therefore, here we examined the potential relationship between participants’ neural responses to synchronous audiovisual BM signals and their autistic traits in Experiment 2.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>In all experiments, 17%–23% of the trials were randomly selected as catch trials, in which the color of the walker changed one or two times throughout the trial, and there was no color change in other trials. Participants were required to detect the color change of visual stimuli (0-2 times during one trial) to maintain attention. Behavioral analysis on all trials showed that their performances for the task were generally high and equally well in all conditions of Experiment 1a (mean accuracy &gt; 98%; <italic>F</italic> (2, 46) = 0.814, <italic>p</italic> = .450, <inline-formula><inline-graphic xlink:href="590751v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.034), Experiment 1b (mean accuracy &gt; 98%; <italic>F</italic> (2, 46) = 0.615, <italic>p</italic> = .545, <inline-formula><inline-graphic xlink:href="590751v1_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.026), and Experiment 2 (mean accuracy &gt; 98%; <italic>F</italic> (3, 69) = 0.493, <italic>p</italic> = .688, <inline-formula><inline-graphic xlink:href="590751v1_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.021), indicating comparable attention state across conditions. The catch trials were excluded from the following EEG analysis.</p>
<sec id="s2a">
<title>Cortical tracking of rhythmic structures in audiovisual BM reveals AVI</title>
<sec id="s2a1">
<title>Experiment 1a</title>
<p>In Experiment 1a, we examined the cortical tracking of rhythmic BM information under V, A, and AV conditions. We were interested in two critical rhythmic structures in the walking motion sequence, i.e., the gait cycle and the step cycle (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). During walking, each step of the left or right foot occurs alternatively to form a step cycle, and the antiphase oscillations of limbs during two steps characterize a gait cycle (<xref ref-type="bibr" rid="c40">Shen, Lu, Yuan, et al., 2023</xref>). In Experiment 1a, the frequency of a full gait cycle is 1 Hz, and the step-cycle frequency is 2 Hz. The strength of the cortical tracking effect was quantified by the amplitude peaks emerging from the EEG spectra at these frequencies.</p>
<p>As shown in the grand average amplitude spectra (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>), both the responses in three conditions showed clear peaks at step-cycle frequency (2 Hz; V: <italic>t</italic> (23) = 6.964, <italic>p</italic> &lt; 0.001; A: <italic>t</italic> (23) = 6.073, <italic>p</italic> &lt; .001; AV: <italic>t</italic> (23) = 7.054, <italic>p</italic> &lt; 0.001; FDR corrected). In contrast, at gait-cycle frequency (1 Hz), only the response to AV stimulation showed significant peaks (V: t (23) = −2.072, <italic>p</italic> = 0.975; A: t (23) = −0.054, <italic>p</italic> = 0.521; AV: t (23) = 4.059, <italic>p</italic> &lt; 0.001; FDR corrected). Besides, we also observed a significant peak at 4 Hz in all three conditions (<italic>p</italic>s &lt; 0.001, FDR corrected), which might be the harmonic of 2 Hz (see the results of harmonics in Supplementary Information).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2</label>
<caption><title>Cortical tracking of visual (V), auditory (A), and audiovisual (AV) BM signals at gait-cycle and step-cycle frequencies.</title>
<p><bold>(a) &amp; (d)</bold> The amplitude spectra of EEG response in three conditions in Experiment 1a and Experiment 1b, respectively. The solid lines show the grand average amplitude over all electrodes and subjects. The shaded regions depict standard errors of the group mean. Asterisks indicate significant spectra peaks (one-sample t-test against zero; <italic>p</italic> &lt; .05, FDR corrected). <bold>(b) &amp; (e)</bold> The normalized amplitude at gait-cycle frequency in the AV condition exceeded the arithmetical sum of those in V and A conditions (AV &gt; A+V), <bold>(c) &amp; (f)</bold> but the normalized amplitude at step-cycle frequency in the AV condition was comparable to the sum of V and A (AV = A+V). Colored dots represent individual data in each condition. Error bars represent ±1 standard error of means. *: <italic>p</italic> &lt; .05; **: <italic>p</italic> &lt; .01; ***: <italic>p</italic> &lt; .001; <italic>m.s.</italic>: .05&lt; <italic>p</italic> &lt; .10; <italic>n.s.</italic>: <italic>p</italic> &gt; .05.</p></caption>
<graphic xlink:href="590751v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Furthermore, we directly compared the cortical tracking effects between different conditions via a two-tailed paired t-test. At both 1 Hz (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>) and 2 Hz (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>), the amplitude in the AV condition was greater than that in the V condition (1 Hz: <italic>t</italic> (23) = 4.664, <italic>p</italic> &lt; 0.001, Cohen’s <italic>d</italic> = 0.952; 2 Hz: <italic>t</italic> (23) = 5.132, <italic>p</italic> &lt; 0.001, Cohen’s <italic>d</italic> = 1.048) and the A condition (1 Hz: <italic>t</italic> (23) = 2.391, <italic>p</italic> = 0.025, Cohen’s <italic>d</italic> = 0.488; 2 Hz: <italic>t</italic> (23) = 3.808, <italic>p</italic> &lt; 0.001, Cohen’s <italic>d</italic> = 0.777), respectively, suggesting multisensory gains. More importantly, at 1Hz, the amplitude in the AV condition was significantly larger than the algebraic sum of those in the A and V conditions (<italic>t</italic> (23) = 3.028, <italic>p</italic> = 0.006, Cohen’s <italic>d</italic> = 0.618), indicating a super-additive audiovisual integration effect. While at 2Hz, the amplitude in the AV condition was comparable to the unisensory sum (<italic>t</italic> (23) = −0.623, <italic>p</italic> = 0.539, Cohen’s <italic>d</italic> = −0.127), indicating additive audiovisual integration.</p>
</sec>
<sec id="s2a2">
<title>Experiment 1b</title>
<p>To further test whether cortical entrainment can apply to stimuli with a different speed, Experiment 1b altered the frequencies of the gait cycle and the corresponding step cycle to 0.83 Hz and 1.67 Hz while adopting the same paradigm as Experiment 1a. Consistent with Experiment 1a, the frequency-domain analysis revealed significant cortical entrainment to the audiovisual stimuli at the new speeds. As shown in <xref rid="fig2" ref-type="fig">Fig. 2d</xref>, both the responses to V, A, and AV stimuli showed clear peaks at step-cycle frequency (1.67 Hz; V: <italic>t</italic> (23) = 3.473, <italic>p</italic> = .001; A: <italic>t</italic> (23) = 9.194, <italic>p</italic> &lt; .001; AV: <italic>t</italic> (23) = 8.756, <italic>p</italic> &lt; .001; FDR corrected) and its harmonics (3.33 Hz, <italic>p</italic>s &lt; .001, FDR corrected, see Supplementary Information for additional analysis). In contrast, at gait-cycle frequency (0.83 Hz), only the response to AV stimuli showed significant peaks (V: t (23) = −1.125, <italic>p</italic> = .846; A: t (23) = −2.449, <italic>p</italic> = .989; AV: t (23) = 3.052, <italic>p</italic> = .003; FDR corrected).</p>
<p>At both 0.83 Hz (<xref rid="fig2" ref-type="fig">Fig. 2e</xref>) and 1.67 Hz (<xref rid="fig2" ref-type="fig">Fig. 2f</xref>), the amplitude in the AV condition was stronger or marginally stronger than that in the V condition (0.83 Hz: <italic>t</italic> (23) = 2.665, <italic>p</italic> = .014, Cohen’s <italic>d</italic> = 0.544; 1.67 Hz: <italic>t</italic> (23) = 6.380, <italic>p</italic> &lt; .001, Cohen’s <italic>d</italic> = 1.302) and the A condition (0.83 Hz: <italic>t</italic> (23) = 3.625, <italic>p</italic> &lt; .001, Cohen’s <italic>d</italic> = 0.740; 1.67 Hz: <italic>t</italic> (23) = 1.752, <italic>p</italic> = .093, Cohen’s <italic>d</italic> = 0.358), respectively, suggesting multisensory gains. More importantly, at 0.83 Hz, the amplitude in the AV condition was significantly larger than the sum of those in the A and V conditions (<italic>t</italic> (23) = 3.240, <italic>p</italic> = .004, Cohen’s <italic>d</italic> = 0.661), indicating a super-additive audiovisual integration effect. While at 1.67 Hz, the amplitude in the AV condition was comparable to the unisensory sum (<italic>t</italic> (23) = −0.735, <italic>p</italic> = .470, Cohen’s <italic>d</italic> = −0.150), indicating linear audiovisual integration.</p>
<p>In summary, results from Experiments 1a &amp; 1b consistently showed that the cortical tracking of the audiovisual signals at different temporal scales exhibit distinct audiovisual integration modes, i.e., the super-additive effect at gait-cycle frequency and the additive effect at step-cycle frequency, indicating that the cortical entrainment effects at the two temporal scales might be driven by functionally different mechanisms.</p>
</sec>
</sec>
<sec id="s2b">
<title>Cortical tracking of higher-order rhythmic structure contributes to the AVI of BM</title>
<p>To further explore whether and how the cortical tracking of the rhythmic information contributes to the specialized audiovisual process of BM, both upright and inverted BM stimuli were adopted in Experiment 2. The task and the frequencies of visual stimuli in Experiment 2 were same as Experiment 1a. Specifically, participants were required to perform the change detection task when perceiving upright and inverted visual BM sequences (1 Hz for gait-cycle frequency and 2 Hz for step-cycle frequency) accompanied by frequency congruent (1 Hz) or incongruent (0.6 Hz and 1.4 Hz) footstep sounds. The audiovisual congruency effect, characterized by stronger neural responses in the audiovisual congruent condition compared with the incongruent condition, can be taken as an index of AVI (<xref ref-type="bibr" rid="c18">Fleming et al., 2020</xref>; <xref ref-type="bibr" rid="c20">Jones &amp; Jarick, 2006</xref>; <xref ref-type="bibr" rid="c28">Maddox et al., 2015</xref>; <xref ref-type="bibr" rid="c53">Wuerger, Crocker-Buque, et al., 2012</xref>). A stronger congruency effect in the upright condition relative to the inverted condition characterizes the AVI process specific to BM information.</p>
<p>We calculated the audiovisual congruency effect for the upright (AVIupr) and the inverted (AVIinv) conditions, respectively. Then, we identified the clusters showing significantly different congruency effects between the upright and inverted conditions using a cluster-based permutation test over all electrodes (<italic>n</italic> = 1000, <italic>alpha</italic> = 0.05; see <italic>Methods</italic>). At 1 Hz, the congruency effect in the upright condition was significantly stronger than that in the inverted condition in a cluster at the right hemisphere (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, lower panel, <italic>p</italic> = 0.029; C2, CPz, CP2, CP4, CP6, Pz, P2, P4, P6), revealing a BM-specific AVI process. Then we averaged the amplitude of electrodes within the significant cluster and further performed a two-tailed paired t-test. The results showed that (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>) the audiovisually congruent BM information enhanced the oscillatory amplitude relative to the incongruent ones only for upright BM stimuli (<italic>t</italic> (23) = 4.632, <italic>p</italic> &lt; 0.001, Cohen’s <italic>d</italic> = 0.945) but not when visual BM was inverted (<italic>t</italic> (23) = 0.480, <italic>p</italic> = 0.635, Cohen’s <italic>d</italic> = 0.098). The congruency effect in the upright condition was significantly larger than that in the inverted condition (<italic>t</italic> (23) = 3.099, <italic>p</italic> = 0.005, Cohen’s <italic>d</italic> = 0.633).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3</label>
<caption><title>Cortical tracking at gait-cycle rather than step-cycle frequency contributes to BM-specific AVI effect.</title>
<p><bold>(a) &amp; (d)</bold> Topographic maps for the congruency effect at 1Hz and 2 Hz for each condition: Upright, Inverted, and for the difference between these conditions (Upright-Inverted), respectively. The black dots indicate the electrodes showing a significant congruency effect (upper panels) or a significantly enhanced congruency effect in the upright condition relative to the inverted condition (lower panel). Then, the amplitude at the electrodes shown in the lower panel of (a) were averaged to quantify the cortical entrainment effect at 1 Hz <bold>(b)</bold> and 2 Hz <bold>(e)</bold>. Error bars represent ±1 standard error of means. Individuals’ autistic traits correlated with the BM-specific AVI at 1 Hz <bold>(c)</bold> but not 2 Hz <bold>(f)</bold>. Shaded regions indicate the 95% confidence intervals.</p></caption>
<graphic xlink:href="590751v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In contrast, at 2 Hz, no cluster showed a significantly different congruency effect between the upright and inverted conditions (<xref rid="fig3" ref-type="fig">Fig. 3d</xref>). We then conducted further analysis on the averaged amplitude of the electrodes marked in <xref rid="fig3" ref-type="fig">Fig. 3a</xref> (lower panel). A two-tailed paired t-test showed that both upright and inverted stimuli induced a significant congruency effect at 2 Hz (<xref rid="fig3" ref-type="fig">Fig. 3e</xref>; Upright: <italic>t</italic> (23) = 3.096, <italic>p</italic> = 0.005, Cohen’s <italic>d</italic> = 0.632; Inverted: <italic>t</italic> (23) = 2.672, <italic>p</italic> = 0.014, Cohen’s <italic>d</italic> = 0.545). The congruency effect between the upright and inverted conditions was not different (<italic>t</italic> (23) = 0.434, <italic>p</italic> = 0.668, Cohen’s <italic>d</italic> = 0.089), suggesting a comparable audiovisual congruency effect between two conditions at 2 Hz. Importantly, a three-way repeated-measures ANOVA with frequency (1 Hz vs. 2 Hz), orientation (upright vs. inverted), and audiovisual congruency (congruent vs. incongruent) as within-subject factors revealed a marginal significant three-way interaction (<italic>F</italic> (1,23) = 3.190, <italic>p</italic> = 0.087, <inline-formula><inline-graphic xlink:href="590751v1_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> = 0.122), further implying that the audiovisual integration processing of BM is different between 1 Hz and 2 Hz.</p>
</sec>
<sec id="s2c">
<title>BM-specific cortical tracking correlates with autistic traits</title>
<p>Furthermore, we examined the link between individuals’ autistic traits and the neural responses underpinning the AVI of BM, measured by the difference of congruency effect between the upright and the inverted BM conditions, using Pearson correlation analysis. After removing one outlier (exceeded 3 SD), we observed an evident negative correlation between individuals’ AQ scores and their neural responses at 1 Hz (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>, <italic>r</italic> = −0.493, <italic>p</italic> = 0.017) but not at 2 Hz (<xref rid="fig3" ref-type="fig">Fig. 3f</xref>, <italic>r</italic> = −0.158, <italic>p</italic> = .460). The lack of significant results at 2 Hz was not attributable to electrode selection bias based on the significant cluster at 1 Hz, as similar results were observed when we performed the analysis on electrodes within the clusters showing significant congruency effects at 2 Hz (see the control analysis in Supplementary Information for details).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>The current study investigated the neural implementation for the AVI of human BM information and its functional implications. We found that, even under a motion-irrelevant color detection task, neural oscillations of observers entrained to temporally corresponding audiovisual BM signals at the frequencies of two rhythmic structures, i.e., the higher-order structure of gait cycle at a larger integration window and the basic-level structure of step cycle at a smaller integration window. Moreover, the strength of these cortical entrainment effects was enhanced under the audiovisual condition than in the visual-only or auditory-only condition, indicating multisensory gains in the cortical tracking of BM information (Experiments 1a &amp; 1b).</p>
<p>Crucially, although the entrainment processes at both gait-cycle frequency and step-cycle frequency gain benefits from multisensory correspondence, the mechanisms underlying these two processes appear to be different. At step-cycle frequency, the cortical entrainment effect in the AV condition equals the additive sum of the unisensory conditions. Such linear integration might result from concurrent, independent processing of unisensory inputs without additional interaction of them (<xref ref-type="bibr" rid="c44">Stein et al., 2009</xref>). In contrast, at gait-cycle frequency, the congruent audiovisual signals led to a super-additive multisensory enhancement over the linear combination of auditory and visual conditions (AV &gt; A+V), despite that there was no evident cortical tracking effect in the visual condition, different from previous findings obtained with a motion-relevant change detection task (<xref ref-type="bibr" rid="c40">Shen, Lu, Yuan, et al., 2023</xref>). This multisensory enhancement may bring about decreased thresholds of detection and identification (<xref ref-type="bibr" rid="c43">Stanford et al., 2005</xref>), allowing us to achieve a more clear and stable perception of the external environment and detect weak stimulus changes in time and respond adaptively.</p>
<p>Furthermore, results from Experiment 2 demonstrated that the cortical entrainment to gait-cycle rather than step-cycle is specific to the AVI of BM. In particular, the AVI effect at step-cycle frequency was significant for both upright and inverted BM signals and comparable between the two conditions, while the AVI effect at gait-cycle frequency was only significant in the upright condition and was greater than that in the inverted condition. These findings suggest that the cortical entrainment at step-cycle frequency reflects the integration of basic motion signals and corresponding sounds, while the cortical entrainment at gait-cycle frequency reflects the AVI of higher-level BM information. Together, these results reveal that the neural tracking of different levels of kinematic structures plays distinct roles in the AVI of BM, which may result from the interplay of stimulus-driven and domain-specific mechanisms.</p>
<p>Besides the temporal dynamics of neural activity revealed by the cortical entrainment process, we found that the BM-specific AVI effect was associated with enhanced cortical tracking of gait cycles in the right temporoparietal electrodes. This finding likely relates to neural activity in the right posterior superior temporal sulcus (pSTS), a region responding to both auditory and visual BM information and being causally involved in BM perception (<xref ref-type="bibr" rid="c8">Bidet-Caulet et al., 2005</xref>; <xref ref-type="bibr" rid="c19">Grossman et al., 2005</xref>; <xref ref-type="bibr" rid="c52">Wang et al., 2022</xref>). While previous fMRI studies have observed STS activation when processing spatial or semantic correspondence between audiovisual BM (<xref ref-type="bibr" rid="c32">Meyer et al., 2011</xref>; <xref ref-type="bibr" rid="c54">Wuerger, Parkes, et al., 2012</xref>), whether this region also engages in the audiovisual processing of BM signals based on temporal correspondence remains unknown. The current study provides preliminary evidence for such a possibility, inviting future research to localize the exact source of the multisensory integration processes based on imaging data with high temporal resolution and spatial resolution, such as MEG.</p>
<p>In a broad sense, the current study deepens our understanding of the neural processing of audiovisual signals in natural stimuli with complex temporal structures. Cortical entrainment can track simple rhythmic stimuli like tone sequences or luminance-varying patches (C. <xref ref-type="bibr" rid="c23">Keitel et al., 2017</xref>; <xref ref-type="bibr" rid="c55">Yuan et al., 2021</xref>) as well as complex rhythmic structures in speech (<xref ref-type="bibr" rid="c12">Brookshire et al., 2017</xref>; <xref ref-type="bibr" rid="c14">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="c22">Keitel et al., 2018</xref>) and BM (<xref ref-type="bibr" rid="c40">Shen, Lu, Yuan, et al., 2023</xref>). Beyond unisensory processing, cortical entrainment also plays a role in the multisensory processing of simple or discrete rhythmic signals generated by physical stimulation (<xref ref-type="bibr" rid="c4">Bauer et al., 2021</xref>; <xref ref-type="bibr" rid="c33">Miller et al., 2013</xref>; <xref ref-type="bibr" rid="c35">Nozaradan et al., 2012b</xref>; <xref ref-type="bibr" rid="c42">Simon &amp; Wallace, 2017</xref>). These findings may partially explain the AVI effect at 2 Hz for BM and non-BM stimuli in the current study. However, we found that the cortical tracking of the perceived higher-order rhythmic structure based on spatiotemporal integration of meaningful BM information (i.e., the gait cycle of upright walkers rather than inverted walkers) is selectively engaged by the AVI of BM, suggesting that the multisensory processing of natural continuous stimuli may involve unique mechanisms besides the purely stimulus-driven AVI process. Similar to BM, other natural rhythmic stimuli, like auditory speech, also convey hierarchical structures that can entrain neural oscillations at different temporal scales (<xref ref-type="bibr" rid="c14">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="c22">Keitel et al., 2018</xref>). Previous studies have observed the AVI of speech at theta band (4-6 Hz), a temporal scale that corresponds to the rate of syllables (<xref ref-type="bibr" rid="c13">Crosse et al., 2015</xref>), and that the asynchrony detection of prosodic fluctuation in audiovisual speech linked with delta oscillations (∼1-3 Hz) (<xref ref-type="bibr" rid="c7">Biau et al., 2022</xref>). These findings raise a possibility that the AVI of speech also occurs at multiple temporal scales and that the multi-scale entrainment effects play different roles in speech perception. Further investigation into these issues and comparing the results with BM studies will help complete the picture of how the human brain integrates complex, rhythmic information sampled from different sensory modalities to orchestrate perception in a natural scenario.</p>
<p>Last but not least, our study demonstrated that the selective cortical tracking of higher-level rhythmic structure in audiovisually congruent BM signals negatively correlated with individual autistic traits. This finding highlights the functional significance of cortical tracking and integration of audiovisual BM signals in social cognition. It also offers the first evidence that differences in audiovisual BM processing are already present in nonclinical individuals and associated with their autistic traits, beyond previous evidence for atypical audiovisual BM processing in ASD populations (<xref ref-type="bibr" rid="c16">Falck-Ytter et al., 2013</xref>, <xref ref-type="bibr" rid="c15">2018</xref>; <xref ref-type="bibr" rid="c25">Klin et al., 2009</xref>), lending support to the continuum view of ASD (<xref ref-type="bibr" rid="c2">Baron-Cohen et al., 2001</xref>). Meanwhile, given that impaired audiovisual BM processing at the early stage may influence social development and result in cascading consequences for lifetime impairments in social interaction (<xref ref-type="bibr" rid="c15">Falck-Ytter et al., 2018</xref>; <xref ref-type="bibr" rid="c24">Klin et al., 2005</xref>), it is worth exploring neural entrainment to the temporal correspondence of audiovisual BM signals in children with different autistic levels, which may help reveal whether deficits in such ability could serve as an early neural hallmark for ASD.</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Seventy-two participants (mean age ± SD = 22.4 ± 2.6 years, 35 females) took part in the study, 24 for each of Experiment 1a, Experiment 1b, and Experiment 2. All of them had normal or corrected-to-normal vision and reported no history of neurological, psychiatric, or hearing disorders. They were naïve to the purpose of the study and gave informed consent according to procedures and protocols approved by the institutional review board of the Institute of Psychology, Chinese Academy of Sciences.</p>
</sec>
<sec id="s4b">
<title>Stimuli</title>
<sec id="s4b1">
<title>Visual stimuli</title>
<p>The visual stimuli (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>, left panel) consisted of 13 point-light dots attached to the head and major joints of a human walker (<xref ref-type="bibr" rid="c50">Vanrie &amp; Verfaillie, 2004</xref>). This point-light walker looked like walking on a treadmill and did not translate on the screen. It conveys rhythmic structures specified by recurrent forward motions of bilateral limbs (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>, right panel). Each step, regardless of left or right foot, occurs recurrently to form a step cycle. The antiphase oscillations of limbs during two steps characterize a gait cycle (<xref ref-type="bibr" rid="c40">Shen, Lu, Yuan, et al., 2023</xref>). In Experiment 1a, a full gait cycle took 1 second and was repeated 6 times to form a 6-second walking sequence. That is, the gait-cycle frequency is 1 Hz and the step-cycle frequency is 2 Hz. In Experiment 1b, the gait-cycle frequency was 0.83 Hz and the step-cycle frequency was 1.67 Hz. The gait cycle was repeated 6 times to form a 7.2-second walking sequence. The stimuli in Experiment 2 were the same as that in Experiment 1a. Meanwhile, the point-light BM was mirror-flipped vertically to generate inverted BM (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>, left panel), which preserves the temporal structure of the stimuli but distorts its distinctive kinematic features, such as movement that is compatible with the effect of gravity (<xref ref-type="bibr" rid="c40">Shen, Lu, Yuan, et al., 2023</xref>; <xref ref-type="bibr" rid="c48">Troje &amp; Westhoff, 2006</xref>; <xref ref-type="bibr" rid="c52">Wang et al., 2022</xref>).</p>
</sec>
<sec id="s4b2">
<title>Auditory stimuli</title>
<p>Auditory stimuli were continuous footstep sounds (6 s) with a sampling rate of 44,100 Hz. As shown in <xref rid="fig1" ref-type="fig">Fig. 1b</xref>, in Experiments 1a &amp; 2, the gait-cycle frequency of congruent sounds was 1 Hz, which consisted of two steps or two impulses generated by each foot striking the ground within one gait cycle. The incongruent sounds included a faster (1.4 Hz) and a slower (0.60 Hz) sound. Both congruent and incongruent sounds were generated by manipulating the temporal interval between two successive impulses based on the same auditory stimuli. In Experiment 1b, the gait-cycle frequency of sound was 0.83 Hz.</p>
</sec>
<sec id="s4b3">
<title>Stimuli presentation</title>
<p>The visual stimuli were rendered white against a grey background and displayed on a CRT (cathode ray tube) monitor. Participants sat 60 cm from the computer screen (1280×1024 at 60 Hz; High: 37.5 cm; Width: 30 cm), with their heads held stationary on a chinrest. The auditory stimuli were presented binaurally over insert earphones. All stimuli were generated and presented using MATLAB together with the Psychophysics Toolbox (<xref ref-type="bibr" rid="c10">Brainard, 1997</xref>; <xref ref-type="bibr" rid="c37">Pelli, 1997</xref>).</p>
</sec>
</sec>
<sec id="s4c">
<title>Procedure and task</title>
<sec id="s4c1">
<title>Experiment 1a</title>
<p>The experiment was conducted in an acoustically dampened and electromagnetically shielded chamber. Participants completed the task under three conditions (Visual: V; Auditory: A; Audiovisual: AV) with the same procedure (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>) except for the stimuli. In the V condition, each trial began with a white fixation cross (0.42° × 0.42°) displayed at the center of a gray background for a random duration (0.8 s to 1 s). Subsequently, a 6-s point-light walker (3.05°×5.47°) walked toward the left or right at a constant walking cycle frequency (1 Hz). To maintain observers’ attention, 17%–23% of the trials were randomly selected as catch trials, in which the color of the walker changed (the RGB values changed from [255 255 255] to [207 207 207]) one or two times throughout the trial. Each change lasted 0.5 s. Observers were required to report the number of changes (0, 1, or 2) via keypresses as accurately as possible after the point-light display was replaced by a red fixation. The next trial started 2–3 s after the response. In the A condition, the 6 s-stimuli were replaced by a visually static BM figure accompanied by continuous footstep sounds. The frequency of footstep sounds was congruent with the frequency of visual BM in the V condition. In the AV condition, the stimuli were temporally congruent visual BM sequences (as in the V condition) and footstep sounds (as in the A condition). Three conditions were conducted in separate blocks. V condition was performed in the middle of A and AV conditions. The order of A and AV conditions was counterbalanced across participants. Each participant completed 40 experimental trials without changes and 10-15 catch trials in each condition, resulting in a total of 150-165 trials. In each condition, participants completed a practice session with 3 trials to get familiar with the task before the formal EEG experiment.</p>
</sec>
<sec id="s4c2">
<title>Experiment 1b</title>
<p>The procedure of Experiment 1b was the same as that for Experiment 1a but with two exceptions. First, to test if the cortical entrainment effect can apply to stimuli with a different speed, we altered the frequencies of gait and step cycles to 0.83 Hz and 1.67 Hz. Second, we presented the 3 conditions (V, A, and AV) in a completely random order to eliminate the influence of presentation order. To minimize the potential influence of condition switch, we increased the trial number in the practice session from 3 to 14 for each condition.</p>
</sec>
<sec id="s4c3">
<title>Experiment 2</title>
<p>The procedure in Experiment 2 was similar to the AV condition in Experiment 1a, except that the visually displayed BM was accompanied by frequency congruent (1 Hz) or incongruent (0.6 or 1.4 Hz) footstep sounds. Each participant completed a total of 76 experiment trials, consisting of 36 congruent-trials, 20 incongruent-trials with faster sounds (1.4 Hz), and 20 incongruent-trials with slower sounds (0.6 Hz). These trials were assigned to 3 blocks based on the frequency of the footstep sounds, with the order of the three frequencies balanced across participants. Besides, an inverted BM was used as a control to investigate whether there is a specialized mechanism tuned to the AVI of life motion signals. The order of upright and inverted conditions was balanced across participants. Meanwhile, we measured the participants’ autistic traits by using the Autism-Spectrum Quotient, or AQ questionnaire (<xref ref-type="bibr" rid="c2">Baron-Cohen et al., 2001</xref>). Higher AQ scores indicate a higher level of autistic traits.</p>
</sec>
</sec>
<sec id="s4d">
<title>EEG recording and analysis</title>
<p>EEG was recorded at 1000 Hz using a SynAmps<sup>2</sup> NeuroScan amplifier System with 64 electrodes placed on the scalp according to the international 10-20 system. Horizontal and vertical eye movements were measured via four additional electrodes placed on the outer canthus of each eye and the inferior and superior areas of the left orbit. Impedances were kept below 5 kΩ for all electrodes.</p>
<sec id="s4d1">
<title>Preprocessing</title>
<p>The catch trials were excluded from EEG analysis. All preprocessing and further analyses were performed using the FieldTrip toolbox (<xref ref-type="bibr" rid="c30">Maris &amp; Oostenveld, 2007</xref>) in the MATLAB environment. EEG recordings were pass-filtered between 0.1 and 30 Hz, and down-sampled to 100 Hz. Then the continuous EEG data were cut into epochs ranging from −1s to 6 gait cycles (7.2 s in Experiment 1b and 6 s in other experiments) time-locked to the onset of the visual point-light stimuli. The epochs were visually inspected, and trials contaminated with excessive noise were excluded from the analysis. After the trial rejection, eye and cardiac artifacts were removed via independent component analysis based on the Runica algorithm (<xref ref-type="bibr" rid="c5">Bell &amp; Sejnowski, 1995</xref>; <xref ref-type="bibr" rid="c21">Jung et al., 2000</xref>; <xref ref-type="bibr" rid="c29">Makeig, 2002</xref>). Then the cleaned data were re-referenced to the average mastoids (M1 and M2). To minimize the influence of stimulus-onset evoked activity on EEG spectral decomposition, the EEG recording before the onset of the stimulus and the first cycle (1 s in Experiments 1a &amp; 2; 1.2 s in Experiment 1b) of each trial was excluded (<xref ref-type="bibr" rid="c34">Nozaradan et al., 2012a</xref>). After that, the EEG epochs were averaged across trials for each participant and condition.</p>
</sec>
<sec id="s4d2">
<title>Frequency-Domain analysis and statistics</title>
<p>A Fast Fourier Transform (FFT) with zero padding (1200) was used to convert the averaged EEG signals from the temporal domain to the spectral domain, resulting in a frequency resolution of 0.083 Hz, i.e., 1/12 Hz, which is sufficient for observing neural responses around the frequency of the rhythmic BM structures in all experiments. When performing FFT, a Hanning window was adopted to minimize spectral leakage. Then, to remove the 1/f trend of the response amplitude spectrum and identify spectral peaks, the response amplitude at each frequency was normalized by subtracting the average amplitude measured at the neighboring frequency bins (two bins on each side) (<xref ref-type="bibr" rid="c34">Nozaradan et al., 2012a</xref>). We calculated the normalized amplitude separately for each electrode (except for electrooculogram electrodes, CB1, and CB2), participant, and condition.</p>
<p>In Experiment 1, the normalized amplitude in all electrodes was averaged and a right-tailed one-sample t-test against zero was performed on the grand average amplitude to test whether the neural response in each frequency bin showed a significant entrainment effect or spectral peak. This test was applied to all frequency bins below 5.33 Hz and multiple comparisons were controlled by false discovery rate (FDR) correction at <italic>p</italic> &lt; 0.05 (<xref ref-type="bibr" rid="c6">Benjamini &amp; Hochberg, 1995</xref>). In Experiment 2, to further identify the BM-specific AVI process, the audiovisual congruency effect was compared between the upright and inverted conditions using a cluster-based permutation test over all electrodes (1000 iterations, requiring a cluster size of at least 2 significant neighbors, a two-sided t-test at <italic>p</italic> &lt; 0.05 on the clustered data) (<xref ref-type="bibr" rid="c30">Maris &amp; Oostenveld, 2007</xref>). This allowed us to identify the spatial distribution of the BM-specific congruency effect.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This research was supported by grants from the Ministry of Science and Technology of China (STI2030-Major Projects 2021ZD0203800 and 2021ZD0204200), the National Natural Science Foundation of China (Nos. 32171059 and 31830037), the Interdisciplinary Innovation Team (JCTD-2021-06), the Youth Innovation Promotion Association of the Chinese Academy of Sciences, and the Fundamental Research Funds for the Central Universities.</p>
</ack>
<sec id="s5">
<title>Authors Contributions</title>
<p><bold>Li Shen</bold>: Conceptualization, Methodology, Formal analysis, Investigation, Visualization, Writing-original draft, Writing–review &amp; editing. <bold>Shuo Li &amp; Yuhao Tian</bold>: Investigation, Writing-original draft. <bold>Ying Wang</bold>: Conceptualization, Methodology, Supervision, Writing–review &amp; editing. <bold>Yi Jiang</bold>: Conceptualization, Supervision, Writing–review &amp; editing.</p>
</sec>
<sec id="s6">
<title>Conflict of interest declaration</title>
<p>The authors declare no conflicts of interest.</p>
</sec>
<sec id="s7">
<title>Data availability</title>
<p>The supplementary information files, data, and code accompanying this study are made available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/6f7t4/">https://osf.io/6f7t4/</ext-link>.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Arrighi</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Alais</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Burr</surname>, <given-names>D.</given-names></string-name> (<year>2006</year>). <article-title>Perceptual synchrony of audiovisual streams for natural and artificial motion sequences</article-title>. <source>Journal of Vision</source>, <volume>6</volume>(<issue>3</issue>), <fpage>260</fpage>–<lpage>268</lpage>. <pub-id pub-id-type="doi">10.1167/6.3.6</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Baron-Cohen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wheelwright</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Skinner</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Clubley</surname>, <given-names>E.</given-names></string-name> (<year>2001</year>). <article-title>The autism-spectrum quotient (AQ): Evidence from Asperger syndrome/high-functioning autism, males and females, scientists and mathematicians</article-title>. <source>Journal of Autism and Developmental Disorders</source>, <volume>31</volume>(<issue>1</issue>), <fpage>5</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1023/a:1005653411471</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Bauer</surname>, <given-names>A.-K. R.</given-names></string-name>, <string-name><surname>Debener</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> (<year>2020</year>). <article-title>Synchronisation of Neural Oscillations and Cross-modal Influences</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>24</volume>(<issue>6</issue>), <fpage>481</fpage>–<lpage>495</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2020.03.003</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Bauer</surname>, <given-names>A.-K. R.</given-names></string-name>, <string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Quinn</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> (<year>2021</year>). <article-title>Rhythmic Modulation of Visual Perception by Continuous Rhythmic Auditory Stimulation</article-title>. <source>The Journal of Neuroscience</source>, <volume>41</volume>(<issue>33</issue>), <fpage>7065</fpage>–<lpage>7075</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2980-20.2021</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Bell</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Sejnowski</surname>, <given-names>T. J.</given-names></string-name> (<year>1995</year>). <article-title>An Information-Maximization Approach to Blind Separation and Blind Deconvolution</article-title>. <source>Neural Computation</source>, <volume>7</volume>(<issue>6</issue>), <fpage>1129</fpage>–<lpage>1159</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1995.7.6.1129</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Hochberg</surname>, <given-names>Y.</given-names></string-name> (<year>1995</year>). <article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title>. <source>Journal of the Royal Statistical Society: Series B (Methodological)</source>, <volume>57</volume>(<issue>1</issue>), <fpage>289</fpage>–<lpage>300</lpage>. <pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Biau</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Schultz</surname>, <given-names>B. G.</given-names></string-name>, <string-name><surname>Gunter</surname>, <given-names>T. C.</given-names></string-name>, &amp; <string-name><surname>Kotz</surname>, <given-names>S. A.</given-names></string-name> (<year>2022</year>). <article-title>Left Motor δ Oscillations Reflect Asynchrony Detection in Multisensory Speech Perception</article-title>. <source>Journal of Neuroscience</source>, <volume>42</volume>(<issue>11</issue>), <fpage>2313</fpage>–<lpage>2326</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2965-20.2022</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Bidet-Caulet</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Voisin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bertrand</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Fonlupt</surname>, <given-names>P.</given-names></string-name> (<year>2005</year>). <article-title>Listening to a walking human activates the temporal biological motion area</article-title>. <source>NeuroImage</source>, <volume>28</volume>(<issue>1</issue>), <fpage>132</fpage>–<lpage>139</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.06.018</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Blake</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Shiffrar</surname>, <given-names>M.</given-names></string-name> (<year>2007</year>). <article-title>Perception of Human Motion</article-title>. <source>Annual Review of Psychology</source>, <volume>58</volume>(<issue>1</issue>), <fpage>47</fpage>–<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.psych.57.102904.190152</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name> (<year>1997</year>). <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), <fpage>433</fpage>–<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Brooks</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>van der Zwan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Billard</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Petreska</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Clarke</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Blanke</surname>, <given-names>O.</given-names></string-name> (<year>2007</year>). <article-title>Auditory motion affects visual biological motion processing</article-title>. <source>Neuropsychologia</source>, <volume>45</volume>(<issue>3</issue>), <fpage>523</fpage>–<lpage>530</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.12.012</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Brookshire</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Nusbaum</surname>, <given-names>H. C.</given-names></string-name>, <string-name><surname>Goldin-Meadow</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Casasanto</surname>, <given-names>D.</given-names></string-name> (<year>2017</year>). <article-title>Visual cortex entrains to sign language</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>114</volume>(<issue>24</issue>), <fpage>6352</fpage>–<lpage>6357</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1620350114</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Crosse</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Butler</surname>, <given-names>J. S.</given-names></string-name>, &amp; <string-name><surname>Lalor</surname>, <given-names>E. C.</given-names></string-name> (<year>2015</year>). <article-title>Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditions</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>42</issue>), <fpage>14195</fpage>–<lpage>14204</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1829-15.2015</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Melloni</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Tian</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> (<year>2016</year>). <article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>1</issue>), <fpage>158</fpage>–<lpage>164</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4186</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Falck-Ytter</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Nyström</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Gredebäck</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Gliga</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Bölte</surname>, <given-names>S.</given-names></string-name>, &amp; <collab>the EASE team</collab>. (<year>2018</year>). <article-title>Reduced orienting to audiovisual synchrony in infancy predicts autism diagnosis at 3 years of age</article-title>. <source>Journal of Child Psychology and Psychiatry</source>, <volume>59</volume>(<issue>8</issue>), <fpage>872</fpage>–<lpage>880</lpage>. <pub-id pub-id-type="doi">10.1111/jcpp.12863</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Falck-Ytter</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Rehnberg</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Bölte</surname>, <given-names>S.</given-names></string-name> (<year>2013</year>). <article-title>Lack of Visual Orienting to Biological Motion and Audiovisual Synchrony in 3-Year-Olds with Autism</article-title>. <source>PLoS ONE</source>, <volume>8</volume>(<issue>7</issue>), <fpage>e68816</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0068816</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Feldman</surname>, <given-names>J. I.</given-names></string-name>, <string-name><surname>Dunham</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Cassidy</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wallace</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Woynaroski</surname>, <given-names>T. G.</given-names></string-name> (<year>2018</year>). <article-title>Audiovisual multisensory integration in individuals with autism spectrum disorder: A systematic review and meta-analysis</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>95</volume>, <fpage>220</fpage>–<lpage>234</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2018.09.020</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Fleming</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Noyce</surname>, <given-names>A. L.</given-names></string-name>, &amp; <string-name><surname>Shinn-Cunningham</surname>, <given-names>B. G.</given-names></string-name> (<year>2020</year>). <article-title>Audio-visual spatial alignment improves integration in the presence of a competing audio-visual stimulus</article-title>. <source>Neuropsychologia</source>, <volume>146</volume>, <fpage>107530</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2020.107530</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Grossman</surname>, <given-names>E. D.</given-names></string-name>, <string-name><surname>Battelli</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name> (<year>2005</year>). <article-title>Repetitive TMS over posterior STS disrupts perception of biological motion</article-title>. <source>Vision Research</source>, <volume>45</volume>(<issue>22</issue>), <fpage>2847</fpage>–<lpage>2853</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2005.05.027</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Jones</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Jarick</surname>, <given-names>M.</given-names></string-name> (<year>2006</year>). <article-title>Multisensory integration of speech signals: The relationship between space and time</article-title>. <source>Experimental Brain Research</source>, <volume>174</volume>(<issue>3</issue>), <fpage>588</fpage>–<lpage>594</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-006-0634-0</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Jung</surname>, <given-names>T.-P.</given-names></string-name>, <string-name><surname>Makeig</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Westerfield</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Townsend</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Courchesne</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Sejnowski</surname>, <given-names>T. J.</given-names></string-name> (<year>2000</year>). <article-title>Removal of eye activity artifacts from visual event-related potentials in normal and clinical subjects</article-title>. <source>Clinical Neurophysiology</source>, <volume>111</volume>(<issue>10</issue>), <fpage>1745</fpage>–<lpage>1758</lpage>. <pub-id pub-id-type="doi">10.1016/S1388-2457(00)00386-2</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Keitel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gross</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name> (<year>2018</year>). <article-title>Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title>. <source>PLOS Biology</source>, <volume>16</volume>(<issue>3</issue>), <fpage>e2004473</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.2004473</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Keitel</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Thut</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Gross</surname>, <given-names>J.</given-names></string-name> (<year>2017</year>). <article-title>Visual cortex responses reflect temporal structure of continuous quasi-rhythmic sensory stimulation</article-title>. <source>NeuroImage</source>, <volume>146</volume>, <fpage>58</fpage>–<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.043</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="book"><string-name><surname>Klin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Schultz</surname>, <given-names>R. T.</given-names></string-name>, &amp; <string-name><surname>Volkmar</surname>, <given-names>F. R.</given-names></string-name> (<year>2005</year>). <chapter-title>The Enactive Mind-From Actions to Cognition: Lessons from Autism</chapter-title>. In <source>Handbook of autism and pervasive developmental disorders: Diagnosis, development, neurobiology, and behavior</source>, Vol. <volume>1</volume>, <edition>3rd</edition> ed (pp. <fpage>682</fpage>–<lpage>703</lpage>). <publisher-name>John Wiley &amp; Sons Inc</publisher-name>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Klin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Gorrindo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ramsay</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Jones</surname>, <given-names>W.</given-names></string-name> (<year>2009</year>). <article-title>Two-year-olds with autism orient to non-social contingencies rather than biological motion</article-title>. <source>Nature</source>, <volume>459</volume>(<issue>7244</issue>), <fpage>257</fpage>–<lpage>261</lpage>. <pub-id pub-id-type="doi">10.1038/nature07868</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Gross</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Thut</surname>, <given-names>G.</given-names></string-name> (<year>2019</year>). <article-title>A New Unifying Account of the Roles of Neuronal Entrainment</article-title>. <source>Current Biology</source>, <volume>29</volume>(<issue>18</issue>), <fpage>R890</fpage>–<lpage>R905</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2019.07.075</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Ma</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Yuan</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Jiang</surname>, <given-names>Y.</given-names></string-name> (<year>2022</year>). <article-title>Gravity-Dependent Animacy Perception in Zebrafish</article-title>. <source>Research</source>, <volume>2022</volume>. <pub-id pub-id-type="doi">10.34133/2022/9829016</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Maddox</surname>, <given-names>R. K.</given-names></string-name>, <string-name><surname>Atilgan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Bizley</surname>, <given-names>J. K.</given-names></string-name>, &amp; <string-name><surname>Lee</surname>, <given-names>A. K.</given-names></string-name> (<year>2015</year>). <article-title>Auditory selective attention is enhanced by a task-irrelevant temporally coherent visual stimulus in human listeners</article-title>. <source>eLife</source>, <volume>4</volume>, <fpage>e04995</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.04995</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Makeig</surname>, <given-names>S.</given-names></string-name> (<year>2002</year>). <article-title>Response: Event-related brain dynamics – unifying brain electrophysiology</article-title>. <source>Trends in Neurosciences</source>, <volume>25</volume>(<issue>8</issue>), <fpage>390</fpage>. <pub-id pub-id-type="doi">10.1016/S0166-2236(02)02198-7</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name> (<year>2007</year>). <article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>164</volume>(<issue>1</issue>), <fpage>177</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Mendonça</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Santos</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>López-Moliner</surname>, <given-names>J.</given-names></string-name> (<year>2011</year>). <article-title>The benefit of multisensory integration with biological motion signals</article-title>. <source>Experimental Brain Research</source>, <volume>213</volume>(<issue>2–3</issue>), <fpage>185</fpage>–<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-011-2620-4</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Meyer</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Greenlee</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Wuerger</surname>, <given-names>S.</given-names></string-name> (<year>2011</year>). <article-title>Interactions between auditory and visual semantic stimulus classes: Evidence for common processing networks for speech and body actions</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>23</volume>(<issue>9</issue>), <fpage>2291</fpage>–<lpage>2308</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2010.21593</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Miller</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Carlson</surname>, <given-names>L. A.</given-names></string-name>, &amp; <string-name><surname>McAuley</surname>, <given-names>J. D.</given-names></string-name> (<year>2013</year>). <article-title>When What You Hear Influences When You See: Listening to an Auditory Rhythm Influences the Temporal Allocation of Visual Attention</article-title>. <source>Psychological Science</source>, <volume>24</volume>(<issue>1</issue>), <fpage>11</fpage>–<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1177/0956797612446707</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Nozaradan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Peretz</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Mouraux</surname>, <given-names>A.</given-names></string-name> (<year>2012a</year>). <article-title>Selective Neuronal Entrainment to the Beat and Meter Embedded in a Musical Rhythm</article-title>. <source>Journal of Neuroscience</source>, <volume>32</volume>(<issue>49</issue>), <fpage>17572</fpage>–<lpage>17581</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3203-12.2012</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Nozaradan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Peretz</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Mouraux</surname>, <given-names>A.</given-names></string-name> (<year>2012b</year>). <article-title>Steady-state evoked potentials as an index of multisensory temporal binding</article-title>. <source>NeuroImage</source>, <volume>60</volume>(<issue>1</issue>), <fpage>21</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.11.065</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Pavlova</surname>, <given-names>M. A.</given-names></string-name> (<year>2012</year>). <article-title>Biological Motion Processing as a Hallmark of Social Cognition</article-title>. <source>Cerebral Cortex</source>, <volume>22</volume>(<issue>5</issue>), <fpage>981</fpage>–<lpage>995</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhr156</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname>, <given-names>D. G.</given-names></string-name> (<year>1997</year>). <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), <fpage>437</fpage>–<lpage>442</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Saygin</surname>, <given-names>A. P.</given-names></string-name>, <string-name><surname>Driver</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>de Sa</surname>, <given-names>V. R.</given-names></string-name> (<year>2008</year>). <article-title>In the Footsteps of Biological Motion and Multisensory Perception: Judgments of Audiovisual Temporal Relations Are Enhanced for Upright Walkers</article-title>. <source>Psychological Science</source>, <volume>19</volume>(<issue>5</issue>), <fpage>469</fpage>–<lpage>475</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9280.2008.02111.x</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Shen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Jiang</surname>, <given-names>Y.</given-names></string-name> (<year>2023</year>). <article-title>Audiovisual correspondence facilitates the visual search for biological motion</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>30</volume>(<issue>6</issue>), <fpage>2272</fpage>–<lpage>2281</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-023-02308-z</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Shen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Yuan</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Jiang</surname>, <given-names>Y.</given-names></string-name> (<year>2023</year>). <article-title>Cortical encoding of rhythmic kinematic structures in biological motion</article-title>. <source>NeuroImage</source>, <volume>268</volume>, <fpage>119893</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.119893</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Simion</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Regolin</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Bulf</surname>, <given-names>H.</given-names></string-name> (<year>2008</year>). <article-title>A predisposition for biological motion in the newborn baby</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>105</volume>(<issue>2</issue>), <fpage>809</fpage>–<lpage>813</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0707021105</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Simon</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Wallace</surname>, <given-names>M. T.</given-names></string-name> (<year>2017</year>). <article-title>Rhythmic Modulation of Entrained Auditory Oscillations by Visual Inputs</article-title>. <source>Brain Topography</source>, <volume>30</volume>(<issue>5</issue>), <fpage>565</fpage>–<lpage>578</lpage>. <pub-id pub-id-type="doi">10.1007/s10548-017-0560-4</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Stanford</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Quessy</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Stein</surname>, <given-names>B. E.</given-names></string-name> (<year>2005</year>). <article-title>Evaluating the Operations Underlying Multisensory Integration in the Cat Superior Colliculus</article-title>. <source>Journal of Neuroscience</source>, <volume>25</volume>(<issue>28</issue>), <fpage>6499</fpage>–<lpage>6508</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5095-04.2005</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Stein</surname>, <given-names>B. E.</given-names></string-name>, <string-name><surname>Stanford</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Ramachandran</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Perrault</surname>, <given-names>T. J.</given-names></string-name>, &amp; <string-name><surname>Rowland</surname>, <given-names>B. A.</given-names></string-name> (<year>2009</year>). <article-title>Challenges in quantifying multisensory integration: Alternative criteria, models, and inverse effectiveness</article-title>. <source>Experimental Brain Research</source>, <volume>198</volume>(<issue>2</issue>), <fpage>113</fpage>–<lpage>126</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-009-1880-8</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Stevenson</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Ghose</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Fister</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Sarko</surname>, <given-names>D. K.</given-names></string-name>, <string-name><surname>Altieri</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Nidiffer</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Kurela</surname>, <given-names>L. R.</given-names></string-name>, <string-name><surname>Siemann</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>James</surname>, <given-names>T. W.</given-names></string-name>, &amp; <string-name><surname>Wallace</surname>, <given-names>M. T.</given-names></string-name> (<year>2014</year>). <article-title>Identifying and Quantifying Multisensory Integration: A Tutorial Review</article-title>. <source>Brain Topography</source>, <volume>27</volume>(<issue>6</issue>), <fpage>707</fpage>–<lpage>730</lpage>. <pub-id pub-id-type="doi">10.1007/s10548-014-0365-7</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Thomas</surname>, <given-names>J. P.</given-names></string-name>, &amp; <string-name><surname>Shiffrar</surname>, <given-names>M.</given-names></string-name> (<year>2010</year>). <article-title>I can see you better if I can hear you coming: Action-consistent sounds facilitate the visual detection of human gait</article-title>. <source>Journal of Vision</source>, <volume>10</volume>(<issue>12</issue>), 14,<fpage>1</fpage>–<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1167/10.12.14</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Thomas</surname>, <given-names>J. P.</given-names></string-name>, &amp; <string-name><surname>Shiffrar</surname>, <given-names>M.</given-names></string-name> (<year>2013</year>). <article-title>Meaningful sounds enhance visual sensitivity to human gait regardless of synchrony</article-title>. <source>Journal of Vision</source>, <volume>13</volume>(<issue>14</issue>), 8,<fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1167/13.14.8</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Troje</surname>, <given-names>N. F.</given-names></string-name>, &amp; <string-name><surname>Westhoff</surname>, <given-names>C.</given-names></string-name> (<year>2006</year>). <article-title>The Inversion Effect in Biological Motion Perception: Evidence for a “Life Detector”?</article-title> <source>Current Biology</source>, <volume>16</volume>(<issue>8</issue>), <fpage>821</fpage>–<lpage>824</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2006.03.022</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>van der Zwan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>MacHatch</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kozlowski</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Troje</surname>, <given-names>N. F.</given-names></string-name>, <string-name><surname>Blanke</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Anna</surname>, <given-names>B.</given-names></string-name> (<year>2009</year>). <article-title>Gender bending: Auditory cues affect visual judgements of gender in biological motion displays</article-title>. <source>Experimental Brain Research</source>, <volume>198</volume>(<issue>2–3</issue>), <fpage>373</fpage>–<lpage>382</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-009-1800-y</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Vanrie</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Verfaillie</surname>, <given-names>K.</given-names></string-name> (<year>2004</year>). <article-title>Perception of biological motion: A stimulus set of human point-light actions</article-title>. <source>Behavior Research Methods, Instruments, &amp; Computers</source>, <volume>36</volume>(<issue>4</issue>), <fpage>625</fpage>–<lpage>629</lpage>. <pub-id pub-id-type="doi">10.3758/BF03206542</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Troje</surname>, <given-names>N. F.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Jiang</surname>, <given-names>Y.</given-names></string-name> (<year>2018</year>). <article-title>Heritable aspects of biological motion perception and its covariation with autistic traits</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>115</volume>(<issue>8</issue>), <fpage>1937</fpage>–<lpage>1942</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1714655115</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Jiang</surname>, <given-names>Y.</given-names></string-name> (<year>2022</year>). <article-title>Modulation of biological motion perception in humans by gravity</article-title>. <source>Nature Communications</source>, <volume>13</volume>(<issue>1</issue>), <fpage>Article 1</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-022-30347-y</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Wuerger</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Crocker-Buque</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Meyer</surname>, <given-names>G. F.</given-names></string-name> (<year>2012</year>). <article-title>Evidence for auditory-visual processing specific to biological motion</article-title>. <source>Seeing and Perceiving</source>, <volume>25</volume>(<issue>1</issue>), <fpage>15</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1163/187847611X620892</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Wuerger</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Parkes</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lewis</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Crocker-Buque</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rutschmann</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Meyer</surname>, <given-names>G. F.</given-names></string-name> (<year>2012</year>). <article-title>Premotor Cortex Is Sensitive to Auditory–Visual Congruence for Biological Motion</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>24</volume>(<issue>3</issue>), <fpage>575</fpage>–<lpage>587</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00173</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Yuan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Jiang</surname>, <given-names>Y.</given-names></string-name> (<year>2021</year>). <article-title>Cortical entrainment to hierarchical contextual rhythms recomposes dynamic attending in visual perception</article-title>. <source>eLife</source>, <volume>10</volume>, <fpage>e65118</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.65118</pub-id></mixed-citation></ref>
</ref-list>
<sec id="s8">
<title>Supplementary Information</title>
<sec id="s8a">
<title>Results on harmonics in Experiment 1</title>
<p>As shown in <xref rid="fig1" ref-type="fig">Fig. 1a&amp;d</xref>, the audiovisual BM signals induced significant amplitude peaks at 1f (1/0.83 Hz), 2f (2/1.67 Hz), and 4f (4/3.33 Hz) relative to the gait cycle frequency. No significant peak was observed at 3f (3/2.50 Hz) and 5f (5/4.17 Hz). Theoretically, 2f can be the harmonic component of f, and 4f can be the 4<sup>th</sup> harmonic of 1f and the 2<sup>nd</sup> harmonic of 2f (<xref ref-type="bibr" rid="c57">Norcia et al., 2015</xref>). If the fundamental oscillations and harmonic oscillations are generated via the same or tightly linked mechanisms (<xref ref-type="bibr" rid="c56">Abeysuriya et al., 2014</xref>), one may expect to observe similar patterns of results at the two frequencies. We conducted additional analyses to examine this issue. Given that Experiments 1a &amp; 1b yielded similar results, we collapsed the data and presented the results as follows.</p>
<p>To explore the functional relationship of the neural activity at different frequencies, we analyzed the audiovisual integration modes at each frequency, by comparing the neural responses in the AV condition with the sum of those in the A and V conditions. Results show that the integration mode at 1f is different from all others, while a similar additive audiovisual integration mode is observed at 2f and 4f (<xref rid="figs1" ref-type="fig">Fig. S1a</xref>, also see the results session in the main text for the detailed results at 1f and 2f). At 4f, the amplitude of neural responses showed significant peaks in all three conditions (V: <italic>t</italic> (47) = 6.869, <italic>p</italic> &lt; .001; A: <italic>t</italic> (47) = 7.938, <italic>p</italic> &lt; .001; AV: <italic>t</italic> (47) = 8.303, <italic>p</italic> &lt; .001; FDR corrected). Moreover, the amplitude in the AV condition was larger than that in the V condition (<italic>t</italic> (47) = 4.855, <italic>p</italic> &lt; .001, Cohen’s <italic>d</italic> = 0.701;) and the A condition (<italic>t</italic> (47) = 3.080, <italic>p</italic> = .003, Cohen’s <italic>d</italic> = 0.445), respectively, suggesting multisensory gains. In addition, the amplitude in the AV condition was comparable to the unisensory sum (<italic>t</italic> (47) = −1.049, <italic>p</italic> = .300, Cohen’s <italic>d</italic> = −0.151), indicating linear audiovisual integration. These results were similar to those observed at 2f but different from those at 1f, as reported in the main text. There were no significant multisensory gains at 3f or 5f (<xref rid="figs1" ref-type="fig">Fig. S1b</xref>).</p>
<p>These results indicate that the response at 4f might be the harmonic of 2f, which plays a similar role as 2f in the audiovisual integration of biological motion. However, the cortical entrainment effect at 2f is functionally independent of 1f and can not be fully explained by the harmonic relationship.</p>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S1</label>
<caption><title>Cortical tracking of audiovisual BM information at different frequencies</title></caption>
<graphic xlink:href="590751v1_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s8b">
<title>Control analysis of correlation in Experiment 2</title>
<p>The control analysis mainly aims to eliminate the potential bias due to electrode selection. As reported in the main text, both correlation analyses at 1 Hz and 2 Hz were performed based on electrodes in the significant cluster observed at 1 Hz because there was no significant cluster at 2 Hz (<xref rid="fig3" ref-type="fig">Fig. 3a&amp;d</xref>, lower panel<bold>)</bold>. There is a possibility that these electrodes did not show a significant congruency effect at 2 Hz, either in the upright or the inverted condition, thus were not able to capture the correlation between the variance in neural responses and that in autistic traits. To rule out such a possibility, we conducted a control analysis based on electrodes showing a significant congruency effect at 2 Hz, for the upright (<italic>p</italic> = .004, cluster-based permutation test) and inverted (<italic>p</italic> = .002, cluster-based permutation test) conditions (<xref rid="figs2" ref-type="fig">Fig. S2a</xref>), respectively. We further calculated the difference of congruency effects between these conditions. Note that while this index is not significant at the group level (t (23) = −0.689, <italic>p</italic> = 498), it shows individual variance (SD = 0.079, range: [-0.173 0.153]) larger than that for the 1 Hz condition (SD = 0.041, range: [-0.023 0.135]), which allows us to identify a correlation if existing. Analysis of these data showed a non-significant correlation (<xref rid="figs2" ref-type="fig">Fig. S2b</xref>, r = −0.091, <italic>p</italic> = .674), similar to the results illustrated in <xref rid="fig2" ref-type="fig">Fig. 2f</xref>.</p>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S2</label>
<caption><title>Control analysis at step-cycle frequency.</title>
<p><bold>(a)</bold> The amplitude at the electrodes marked by solid black dots was averaged to quantify the cortical entrainment effect under the upright and inverted conditions, respectively. The congruency effect was not significantly different between these conditions at the group level. <bold>(b)</bold> The individual congruency effect in the upright BM condition over the inverted condition was not significantly correlated with the AQ score.</p></caption>
<graphic xlink:href="590751v1_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Abeysuriya</surname>, <given-names>R. G.</given-names></string-name>, <string-name><surname>Rennie</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>P. A.</given-names></string-name>, &amp; <string-name><surname>Kim</surname>, <given-names>J. W.</given-names></string-name> (<year>2014</year>). <article-title>Experimental observation of a theoretically predicted nonlinear sleep spindle harmonic in human EEG</article-title>. <source>Clinical Neurophysiology</source>, <volume>125</volume>(<issue>10</issue>), <fpage>2016</fpage>–<lpage>2023</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinph.2014.01.025</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Norcia</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Appelbaum</surname>, <given-names>L. G.</given-names></string-name>, <string-name><surname>Ales</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Cottereau</surname>, <given-names>B. R.</given-names></string-name>, &amp; <string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2015</year>). <article-title>The steady-state visual evoked potential in vision research: A review</article-title>. <source>Journal of Vision</source>, <volume>15</volume>(<issue>6</issue>), 4,<fpage>1</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1167/15.6.4</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98701.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Press</surname>
<given-names>Clare</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>Wang et al. presented visual (dot) motion and/or the sound of a walking person and found that EEG activity tracks the step rhythm, as well as the gait (2-step cycle) rhythm, with tentative demonstration that the gait rhythm is tracked superadditively (power for A+V condition is higher than the sum of the A-only and V-only condition). The findings will be of wide interest to those examining biological motion perception and oscillatory processes more broadly, with the potential to be <bold>important</bold>. However, at present, due to some analysis concerns - most notably, evidence of double-dipping for one of the core findings - the evidence is <bold>incomplete</bold>. Furthermore, some of the theoretical interpretations concerning entrainment must remain speculative when the authors cannot dissociate evoked responses from entrained oscillatory effects.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98701.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Shen et al. conducted three experiments to study the cortical tracking of the natural rhythms involved in biological motion (BM), and whether these involve audiovisual integration (AVI). They presented participants with visual (dot) motion and/or the sound of a walking person. They found that EEG activity tracks the step rhythm, as well as the gait (2-step cycle) rhythm. The gait rhythm specifically is tracked superadditively (power for A+V condition is higher than the sum of the A-only and V-only condition, Experiments 1a/b), which is independent of the specific step frequency (Experiment 1b). Furthermore, audiovisual integration during tracking of gait was specific to BM, as it was absent (that is, the audiovisual congruency effect) when the walking dot motion was vertically inverted (Experiment 2). Finally, the study shows that an individual's autistic traits are negatively correlated with the BM-AVI congruency effect.</p>
<p>Strengths:</p>
<p>The three experiments are well designed and the various conditions are well controlled. The rationale of the study is clear, and the manuscript is pleasant to read. The analysis choices are easy to follow, and mostly appropriate.</p>
<p>Weaknesses:</p>
<p>I only have one potential worry. The analysis for gait tracking (1 Hz) in Experiment 2 (Figures 3a/b) starts by computing a congruency effect (A/V stimulation congruent (same frequency) versus A/V incongruent (V at 1 Hz, A at either 0.6 or 1.4 Hz), separately for the Upright and Inverted conditions. Then, this congruency effect is contrasted between Upright and Inverted, in essence computing an interaction score (Congruent/Incongruent X Upright/Inverted). Then, the channels in which this interaction score is significant (by cluster-based permutation test; Figure 3a) are subselected for further analysis. This further analysis is shown in Figure 3b and described in lines 195-202. Critically, the further analysis exactly mirrors the selection criteria, i.e. it is aimed at testing the effect of Congruent/Incongruent and Upright/Inverted. This is colloquially known as &quot;double dipping&quot;, the same contrast is used for selection (of channels, in this case) as for later statistical testing. This should be avoided, since in this case even random noise might result in a significant effect. To strengthen the evidence, either the authors could use a selection contrast that is orthogonal to the subsequent statistical test, or they could skip either the preselection step or the subsequent test. (It could be argued that the test in Figure 3b and related text is not needed to make the point - that same point is already made by the cluster-based permutation test.)</p>
<p>Related to the above: the test for the three-way interaction (lines 211-216) is reported as &quot;marginally significant&quot;, with a p-value of 0.087. This is not very strong evidence.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98701.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors evaluate spectral changes in electroencephalography (EEG) data as a function of the congruency of audio and visual information associated with biological motion (BM) or non-biological motion. The results show supra-additive power gains in the neural response to gait dynamics, with trials in which audio and visual information were presented simultaneously producing higher average amplitude than the combined average power for auditory and visual conditions alone. Further analyses suggest that such supra-additivity is specific to BM and emerges from temporoparietal areas. The authors also find that the BM-specific supra-additivity is negatively correlated with autism traits.</p>
<p>Strengths:</p>
<p>The manuscript is well-written, with a concise and clear writing style. The visual presentation is largely clear. The study involves multiple experiments with different participant groups. Each experiment involves specific considered changes to the experimental paradigm that both replicate the previous experiment's finding yet extend it in a relevant manner.</p>
<p>Weaknesses:</p>
<p>The manuscript interprets the neural findings using mechanistic and cognitive claims that are not justified by the presented analyses and results.</p>
<p>First, entrainment and cortical tracking are both invoked in this manuscript, sometimes interchangeably so, but it is becoming the standard of the field to recognize their separate evidential requirements. Namely, step and gate cycles are striking perceptual or cognitive events that are expected to produce event-related potentials (ERPs). The regular presentation of these events in the paradigm will naturally evoke a series of ERPs that leave a trace in the power spectrum at stimulation rates even if no oscillations are at play. Thus, the findings should not be interpreted from an entrainment framework except if it is contextualized as speculation, or if additional analyses or experiments are carried out to support the assumption that oscillations are present. Even if oscillations are shown to be present, it is then a further question whether the oscillations are causally relevant toward the integration of biological motion and for the orchestration of cognitive processes.</p>
<p>Second, if only a cortical tracking account is adopted, it is not clear why the demonstration of supra-additivity in spectral amplitude is cognitively or behaviorally relevant. Namely, the fact that frequency-specific neural responses to the [audio &amp; visual] condition are stronger than those to [audio] and [visual] combined does not mean this has implications for behavioral performance. While the correlation to autism traits could suggest some relation to behavior and is interesting in its own right, this correlation is a highly indirect way of assessing behavioral relevance. It would be helpful to test the relevance of supra-additive cortical tracking on a behavioral task directly related to the processing of biological motion to justify the claim that inputs are being integrated with the service of behavior. Under either framework, cortical tracking or entrainment, the causal relevance of neural findings toward cognition is lacking.</p>
<p>Overall, I believe this study finds neural correlates of biological motion, and it is possible that such neural correlates relate to behaviorally relevant neural mechanisms, but based on the current task and associated analyses this has not been shown.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98701.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The study demonstrates differential patterns of entrainment to biological motion (BM). At a basic, sensory level, the authors demonstrate entrainment to faster rhythms that make up BM (step-cycle) which seems to be separate from its audio aspects and its visual aspects (though to a much lesser degree). Ultimately this temporal scale seems to reside in a manner that does not indicate much multi-modal integration. At a higher-order, emergent rhythms in motion that are biologically relevant (gait-cycle) seem to be the result of multisensory integration. The work sheds light on the perceptual processes that are engaged in perceiving BM as well as the role of multisensory integration in these processes. Moreover, the work also outlines interesting links between shorter and longer integration windows along the sensory and multisensory processing stages.</p>
<p>In a series of experiments, the authors sought to investigate the role of multisensory integration in the processing of biological motion (BM). Specifically, they study neural entrainment in BM light-point walkers. Visual-only, auditory-only, and audio-visual (AV) displays were compared under different conditions.</p>
<p>Experiments 1a and b mainly characterized entrainment to these stimuli. Here, entrainment to step cycle (at different scales for 1a and 1b) was found to entrain in the presence of the auditory rhythm and to a certain degree also for the visual stimulus (though barely beyond the noise floor in 1b). The AV condition for this temporal scale seemed to follow an additive rule whereby the combined stimulation resulted in entrainment more or less equal to the sum of the unimodal effects. At the slower, gait cycle a slightly different pattern emerges whereby neither unimodal stimulation conditions result in entrainment however the AV condition does.</p>
<p>This finding was further explored in Experiment 2 where two extra manipulations were added. Point-light walkers could generally be either congruently paired with AV or incongruently. In addition, the visual BM stimulus was matched with a control consisting of an inverted BM and thus non-BM movement. This study enabled further discerning among the step- and gait-cycle findings seeing that the pattern that emerged suggested that step-cycle entrainment was consistent with a low-level process that is not selective to BM whilst gait-cycle entrainment was only found for BM. This generally replicated the findings in Experiment 1 and extended them further suggesting that entrainment seen for uni- and multisensory step cycles is reflects a different process than that captured in the gait-cycle multi-modal entrainment. The selective BM finding seemed to demonstrate a link to autistic traits within a sample of 24 participants informing a hypothesis that sensitivity to biological motion might be related to social cognition.</p>
<p>Strengths:</p>
<p>The main strengths of the paper relate to the conceptualization of BM and the way it is operationalized in the experimental design and analyses. The use of entrainment, and the tracking of different, nested aspects of BM result in seemingly clean data that demonstrate the basic pattern. The first experiments essentially provide the basic utility of the methodological innovation and the second experiment further hones in on the relevant interpretation of the findings by the inclusion of better control stimuli sets.</p>
<p>Another strength of the work is that it includes at a conceptual level two replications.</p>
<p>Weaknesses:</p>
<p>The statistical analysis is misleading and inadequate at times. The inclusion of the autism trait is not foreshadowed and adequately motivated and is likely underpowered. Finally, a broader discussion over other nested frequencies that might reside in the point-light walker stimuli would also be important to fully interpret the different peaks in the spectra.</p>
</body>
</sub-article>
</article>