<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95761</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95761</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95761.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The speed of detection vs. segmentation from continuous sequences: Evidence for an anticipation mechanism for detection through a computational model</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Luo</surname>
<given-names>Meili</given-names>
</name>
<xref ref-type="aff" rid="a1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cao</surname>
<given-names>Ran</given-names>
</name>
<xref ref-type="aff" rid="a1"/>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Wang</surname>
<given-names>Felix Hao</given-names>
</name>
<xref ref-type="aff" rid="a1"/>
<xref ref-type="corresp" rid="cor1"/>
<xref ref-type="author-notes" rid="fn1">*</xref>
<email xlink:href="mailto:haowang1@sas.upenn.edu">haowang1@sas.upenn.edu</email>
</contrib>
<aff id="a1"><institution>School of Psychology, Nanjing Normal University</institution>, <addr-line>Nanjing, Jiangsu</addr-line>, <country>China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="fn1"><label>*</label><p>To whom correspondence should be addressed.</p></fn>
<corresp id="cor1">Correspondence concerning this article should be addressed to Felix Hao Wang. Email address: <email xlink:href="mailto:haowang1@sas.upenn.edu">haowang1@sas.upenn.edu</email> (F. H. Wang).</corresp>
<fn id="fn2"><p>The data reported in this paper is available, at <ext-link ext-link-type="uri" xlink:href="https://osf.io/63y2g/">https://osf.io/63y2g/</ext-link>. The reported experiments were not preregistered.</p></fn>
</author-notes>
<pub-date pub-type="epub">
<day>02</day>
<month>01</month>
<year>2024</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2024-03-27">
<day>27</day>
<month>03</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95761</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2024-01-29">
<day>29</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-01-01">
<day>01</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.31219/osf.io/tb26v"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Luo et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Luo et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95761-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>To understand the latent structure of a language, one of the first steps in language learning is word segmentation. The rapid speed is an important feature of statistical segmentation, and exact quantifications would help us understand the underlying mechanism. In this study, we probe the speed of learning by using a novel experimental paradigm and compare them to results obtained through the traditional word segmentation paradigm. Using a novel target detection paradigm, we replicated and extended a study on when participants start to show learning effects. We successfully replicated a facilitation effect showing rapid learning, which showed that learners obtained statistical information following a single exposure. However, we also found a similar facilitation effect when the syllable sequence contained words that were uniform or mixed in length. Importantly, this contrasts with results from traditional word segmentation paradigms, where learning is significantly better in uniform-length sequences than in mixed-length sequences. Thus, even though the target detection paradigm showed robust effects, it may have required mechanisms different from those in word segmentation. To understand these mechanisms, we proposed both theoretical analyses and a computational model to simulate results from the target detection paradigm. We found that an anticipation mechanism could explain the data from target detection, and crucially, the anticipation mechanism can produce facilitation effects without performing segmentation. We discuss both the theoretical and empirical reasons why the target detection and word segmentation paradigm might engage different processes, and how these findings contribute to our understanding of statistical word segmentation.</p>
</abstract>
<kwd-group>
<title>Keywords</title>
<kwd>Statistical segmentation</kwd>
<kwd>Transitional probability</kwd>
<kwd>Prediction</kwd>
<kwd>Rapid learning</kwd>
</kwd-group>

</article-meta>
<notes>
<notes notes-type="conflict-interest-statement">
<p>Author asserted no Conflict of Interest</p>
</notes>
</notes>
</front>
<body>
<p>For novice language learners, one of the first tasks is to understand the structure of the continuous speech streams they hear by segmenting the speech into words. In the literature, two types of information that can be used for segmentation are discussed. Prosody, such as stress, though never perfectly correlate with world boundaries in natural languages, can often provide useful information to word boundaries and has been shown to be used for word segmentation (<xref ref-type="bibr" rid="c19">Johnson &amp; Jusczyk, 2001</xref>; <xref ref-type="bibr" rid="c23">Jusczyk, 1999</xref>; <xref ref-type="bibr" rid="c24">Jusczyk &amp; Aslin, 1995</xref>; <xref ref-type="bibr" rid="c25">Jusczyk, Houston, &amp; Newsome, 1999</xref>). Another type of information is the distributional information of the syllables in a sequence, which was shown to be used in word segmentation as well (e.g., <xref ref-type="bibr" rid="c30">Saffran, Aslin, &amp; Newport, 1996</xref>; <xref ref-type="bibr" rid="c1">Aslin Saffran, &amp; Newport, 1998</xref>). The theory is that learners would track the co-occurrence information between syllables, and use this co-occurrence information to compute transitional probability, which can be a cue to word boundaries. The seminal work on statistical learning (<xref ref-type="bibr" rid="c30">Saffran et al., 1996</xref>) demonstrated that young infants can segment word forms in a rapid syllable stream in two minutes where the syllables in the stream formed statistical patterns to word boundaries. In this influential study, learning only required exposure to a syllable stream, consisting of four trisyllabic words occurring 45 times each where prosodic cues such as stress and co-articulation to word boundaries were not present.</p>
<p>Following this initial work showing powerful learning, there is now a large literature on how the underlying computational mechanism can be best described, as well as the constraints for word segmentation to be successful. To understand the underlying computational mechanism, different computational models have been proposed (e.g., <xref ref-type="bibr" rid="c14">Frank, Goldwater, Griffiths &amp; Tenenbaum, 2010</xref>; <xref ref-type="bibr" rid="c15">Giroux &amp; Rey, 2009</xref>; <xref ref-type="bibr" rid="c28">Perruchet &amp; Vinter, 1998</xref>; <xref ref-type="bibr" rid="c32">Swingley, 2005</xref>). For example, different models implement ideas on boundary finding (e.g., <xref ref-type="bibr" rid="c32">Swingley, 2005</xref>) vs. chunking (e.g., the PARSER model from <xref ref-type="bibr" rid="c28">Perruchet &amp; Vinter, 1998</xref>). Through computational modeling, concrete predictions of different theoretical approaches can be generated, which offer testable hypotheses about these different mechanisms that researchers were able to test further, using experimental methods (<xref ref-type="bibr" rid="c10">Endress &amp; Mehler, 2009</xref>). In addition to computational models, leveraging the learning constraints also helps understand the computational mechanism. Elsewhere in the language acquisition literature, for example, learning constraints are an important piece in understanding why the nature of the learning problem requires a representation that’s structure-dependent when studying the acquisition of syntax. In this instance, knowing when a set of learning theories succeed and fail allows us to understand the intricacies of the learning mechanism. For word segmentation, one prominent constraint is that, even though infants and adults alike have shown success segmenting syllable sequences consisting of words that were uniform in length (i.e., all words were either disyllabic; <xref ref-type="bibr" rid="c11">Graf Estes, Evans, Alibali, &amp; Saffran, 2007</xref>; or trisyllabic, <xref ref-type="bibr" rid="c1">Aslin et al., 1998</xref>), both infants and adults have shown difficulty with syllable sequences consisting of words of mixed length (<xref ref-type="bibr" rid="c22">Johnson &amp; Tyler, 2010</xref>; <xref ref-type="bibr" rid="c20">Johnson &amp; Jusczyk, 2003a</xref>; <xref ref-type="bibr" rid="c21">2003b</xref>; <xref ref-type="bibr" rid="c16">Hoch, Tyler, &amp; Tillmann, 2013</xref>). For example, <xref ref-type="bibr" rid="c22">Johnson and Tyler (2010)</xref> showed that if the sequence is constructed by concatenating two trisyllabic and two disyllabic words, infants were unable to segment from such a sequence, even though the infants in the same study had no trouble segmenting a sequence with its four words being all trisyllabic. Similarly, <xref ref-type="bibr" rid="c16">Hoch et al., (2013)</xref> showed that adults learned much worse with a mixed-length language than with a uniform-length language.</p>
<p>Another way of understanding the mechanisms for segmentation is by studying how fast learning takes place. Fast learning has always been a feature of statistical word segmentation, with the initial work showing that infants can segment words with only 2 to 3 minutes of exposure (<xref ref-type="bibr" rid="c30">Saffran et al., 1996</xref>; <xref ref-type="bibr" rid="c1">Aslin et al., 1998</xref>). It is also an important theoretical question, as the relationship between the amount of exposure and learning can be leveraged to understand the mechanism. For example, after the initial studies showed that learning was fast with relatively simple stimuli, subsequent studies testing adults with more complicated sequences or with different sounds have used longer exposure periods. In <xref ref-type="bibr" rid="c12">Finn and Hudson Kam (2008)</xref> for example, adults were asked to segment a sequence and the amount of exposure was manipulated during different experiments. Interestingly, even though the duration of exposure has been extended from multiple minutes to double or even quadruple the original amount, the amount of learning has not changed as a result (also see <xref ref-type="bibr" rid="c27">Newport &amp; Aslin, 2004</xref> for a similar finding). An alternative direction is to shorten the exposure amount and present very short sequences as the input, and the distributional properties that allow successful learning under such conditions can help us understand the computational processes in segmentation, though not many studies have explored this line of inquiry. Among these, <xref ref-type="bibr" rid="c35">Wang, Luo, and Wang (2023)</xref> showed that learners can succeed at segmentation when learners are exposed to a stream where word forms occurred only two times. Using a word segmentation paradigm, <xref ref-type="bibr" rid="c35">Wang et al. (2023)</xref> repeated the cycle of learning and testing for many different short sequences: Learners were first presented with a continuous syllable stream, and then asked to rate the familiarity of words and part-words, and learned the next stream and so on. This finding suggested that learners can rapidly extract word forms and remember them, and learning did not require a slow accumulation process. However, though segmentation was successful under these minimal conditions, the effect size of learning was small. Testing the same set of syllable sequences but with each word occurring four times, <xref ref-type="bibr" rid="c35">Wang et al. (2023)</xref> found that the effect size of learning was significantly larger in the latter condition, suggesting that, even though word forms may be extracted rapidly, the memory component of the segmentation task may require repetition. Even more impressively, <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> found evidence that one exposure could bring about a facilitation effect consistent with word segmentation, using an online measure. This online measure involved the use of a target detection paradigm, where participants were asked to listen to syllable streams and press a key to detect a particular syllable in the stream. In each trial, twelve syllables were randomly grouped into four trisyllabic words, which were used to create a syllable sequence with all four words occurring 4 times. <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> found that statistical learning can be faster than previously thought: After <italic>one</italic> exposure to a trisyllabic word (e.g., tugola), learners were able to react faster to the second (or third) syllable of that trisyllabic word (<italic>go</italic> or <italic>la</italic>) than to the first syllable (<italic>tu</italic>). Thus, <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> showed that learners have sensitivity to the statistical structure of the stream after one exposure.</p>
<p>Understanding this effect is of great interest because it would inform the theories of statistical word segmentation and identify the computational models that can describe the effect best. <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> discussed that a chunking model, such as the one described in PARSER (<xref ref-type="bibr" rid="c28">Perruchet &amp; Vinter, 1998</xref>) is more consistent with the results than the use of conditional probabilities. She argued that computing conditional probabilities is often thought to involve multiple encounters so that a probability can be calculated. On the other hand, with a chunking model such as PARSER, exposure to the syllable sequence would result in random chunks, which are stored in memory. After a single exposure to a word form (say, ABC with different letters representing different syllables) from the syllable sequence, the random chunk may sometimes include a chunk that contains or partially contains the word form (such as ABC, or AB). Regardless of the computational framework, we believe that such an anticipation account would explain the facilitation effect: in the sense that stimuli follow, precede, or co-occur, the brain can encode such predictive relationship (e.g., <xref ref-type="bibr" rid="c8">Conway, 2020</xref>; <xref ref-type="bibr" rid="c9">Davachi &amp; DuBrow, 2015</xref>; <xref ref-type="bibr" rid="c31">Summerfield &amp; De Lange, 2014</xref>; <xref ref-type="bibr" rid="c33">Turk-Browne, Scholl, Johnson, &amp; Chun, 2010</xref>). It’s even possible that this account of prediction-based facilitation may even hold without segmentation, word extraction, or chunking, per se. For example, an encounter to a sequence in which two elements co-occur (say, AB) would theoretically allow the learner to use the predictive relationship during a subsequent encounter (that A predicts B).</p>
<p>In the current study, we investigate statistical word segmentation with an online measure further. We aim to leverage our knowledge of the learning constraint in a typical word segmentation task, i.e., to segmentation succeed when the input sequence contained uniform-length words, but failed when the words were mixed in length, to probe the mechanisms in the online target detection task and its relationship to the offline task. If the target detection task shares the same mechanism with word segmentation, we would expect that the facilitation effect is stronger in sequences with uniform-length words compared to sequences with mixed-length words. However, as our analysis suggested above, the online target detection task may not require the learner to segment the continuous input and remember segmented forms. That is, if the facilitation effect in the target detection task is based on a general prediction mechanism as we discussed above, it would only require participants to store the sequence they hear and use that for a general prediction process. In this case, it would not matter whether the sequence contained uniform- or mixed-length words, and the size of the facilitation effect would be the same in both the uniform- and mixed-length conditions.</p>
<p>We report two experiments in this paper. In <xref ref-type="sec" rid="s1">Experiment 1</xref>, we report a replication using the same material and the same uniform-length word design from the <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> study (which we call the uniform condition). This serves to establish the robustness of the finding. Additionally, we conducted the replication two times, an exact replication and a conceptual replication, which allowed a comparison of a nuance variable, namely whether the sequence initial (the first and the second) or the sequence final (the 47<sup>th</sup> and the 48<sup>th</sup>) syllables were included in the detection task. This manipulation was included to inform us of how specific the learning condition needs to be for the effect to occur. In <xref ref-type="sec" rid="s2">Experiment 2</xref>, we changed one aspect of the design, namely the lengths of the words in the sequences for target detection, while keeping all other variables the same (which we call the mixed condition). This allows us to examine the effect of learning in the mixed condition, and compare the effect size of learning in the mixed-length word condition to the uniform condition. Together, the two experiments should provide insight into the mechanisms involved in the target detection task, and its relationship to the word segmentation literature.</p>
<sec id="s1">
<title>Experiment 1</title>
<sec id="s1.1">
<title>Methods</title>
<sec id="s1.1.1">
<title>Participants</title>
<p>The number of participants for the replication was determined based on a power analysis based on the data from <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, with some over-sampling. The main effect of interest was the interaction for RTs between the first and second presentation, where the second and third syllables were predictable during the second presentation but unpredictable during the first presentation. Based on the data from <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, this difference was −13.6ms (a standard error was 4.91). In a one-sided test, this produced a post-hoc power of 0.85 with 19 subjects, which means that the original study was well-powered. As long as we have 19 subjects in any condition in our replication, it would also ensure the power of the replication study here.</p>
<p>We ran the study until the end of the semester, and by the time we stopped collecting data, in the exact-replication condition, we included data from twenty-one adult participants from both the University of Nevada, Las Vegas and the University of Southern California. In the conceptual-replication condition, we included forty-eight participants from the same two institutions. IRB approval was obtained at each institution separately prior to conducting the experiment.</p>
</sec>
<sec id="s1.1.2">
<title>Stimuli</title>
<p>The stimuli were the same set from Batterink, who provided open materials online (retrieved from <ext-link ext-link-type="uri" xlink:href="https://osf.io/z69fs/">https://osf.io/z69fs/</ext-link>). Syllable sequences are constructed by concatenating syllables from two syllable inventories (from a male and a female speaker), each consisting of 24 unique syllables at a rate of 300 ms per syllable.</p>
</sec>
</sec>
<sec id="s1.2">
<title>Design and Procedure</title>
<p>The study closely followed the design of <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>. To reiterate the design briefly here, each participant completed 144 iterations of the target detection task. In each iteration, 12 syllables were randomly chosen from a syllable inventory (male or female), which were used to create four trisyllabic words, exhausting all 12 syllables (i.e., one syllable occurred only in one word). Next, a syllable sequence was created by repeating the four words four times in a pseudo-random fashion, with the constraint that a word does not immediately follow itself. This meant that each syllable sequence was 48 (4*4*3) syllables long. The 144 iterations of the task included the use of 72 male- and female-voice syllable sequences, where either male or female first is counterbalanced between subjects. The experiment was self-paced and took about an hour to complete.</p>
<sec id="s1.2.1">
<title>Instructions</title>
<p>The experiment began with a short instruction phase. The following instruction was given, and the experimenter read the instructions aloud to the participants, allowing participants to ask questions at any point of the instruction phase.</p>
<p>“In this study, you will be presented with a rapid succession of syllables, and your job is to detect a particular syllable in a given sequence. In each trial, a target syllable will be presented (for example, ku), both visually on the screen and aurally in the headphones. After this, you will hear the syllable sequence (for example, bakufoka…) in the headphones and your job is to press Space every time you detect the target syllable.</p>
<p>The key to this task is that you need to press the Space as soon as you detect the target syllable. As it would become clear to you in a moment, the syllables go by very quickly, and your job is to detect all of the target syllables as quickly and as accurately as you possibly can.</p>
<p>If you have understood the instructions, you may press Space to move to the next screen. If you have any questions regarding the task, please ask the experimenter now.”</p>
</sec>
<sec id="s1.2.3">
<title>Syllable detection phase</title>
<p>After the instruction phase, the syllable detection phase began. First, the participant was given the opportunity to practice for two trials, while the experimenter was present; after the practice period and the experimenter made sure that the participant was doing the task correctly, the experimenter left the room.</p>
<p>Each trial in the syllable detection task began with the screen displaying “Get ready now. Press Space to start.” After the participant pressed the Space bar, they saw the target syllable displayed on the screen (e.g., “target syllable: vu”). After 1.5 seconds of silence, the participant heard the syllable from the headphones (e.g., the syllable vu), which lasted 0.3 seconds, and another 3.2 seconds of silence followed the target syllable. At this point (5 seconds after the start of the trial), the syllable stream began to play. The syllable stream lasted 14.4 seconds, during which the subjects were free to press Space to indicate that they detected the target syllable. At the end of the trial, the participant was informed as such and the next trial began (“That is the end of this trial. The next trial will begin now.”). The study ended after all 144 trials were done. An illustration is shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><title>An illustration of the target detection task, for both <xref ref-type="sec" rid="s1">Experiment 1</xref> and <xref ref-type="sec" rid="s2">2</xref>.</title><p>Two sets of sample vocabulary, targets, and syllable sequences are shown. The arrows indicate where the targets are in the syllable sequence.</p></caption>
<graphic xlink:href="tb26vv1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>There were two conditions in <xref ref-type="sec" rid="s1">Experiment 1</xref>, though the difference between the two was minimal. In the exact-replication condition, syllables were not detection targets if they were the first two or the last two in the syllable stream, the same as in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>. In the conceptual-replication condition, this constraint did not apply. There seemed, prima facie, no reason to exclude the detection of a syllable when it was among the first two syllables or the last two syllables of the sequence, and the conceptual-replication condition was conducted to test this effect. The conceptual-replication condition thus served to test whether this design difference would not make a difference in terms of the facilitation effect. Our null hypothesis here was whether the target syllable occurred in these arbitrary locations should not interfere with whether the learner could remember the sequence and use it for prediction.</p>
</sec>
</sec>
<sec id="s1.3">
<title>Predictions</title>
<p>We re-iterate the predictions for the replication study here. The prediction is that the second syllable in a trisyllabic word is detected faster than the first syllable after one (or more) exposure, and similarly for the third syllable compared to the first syllable, because while the first syllable is unpredictable, the second and the third syllable become predictable if the participant is able to remember the trisyllabic word given one exposure.</p>
</sec>
<sec id="s1.4">
<title>Results and Discussion</title>
<p>Prior to conducting the analysis, we dropped the trials that involved the first two/last two positions to make sure that the analysis examined the same type of data for the conceptual-replication condition. This meant that all the analyses below were based on reaction time data when the syllable to be detected was in the stream position 3-46. The rest of the analysis plan closely followed the analysis described in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>. Before the analysis, we combined the counterbalancing conditions (female voice/male voice first).</p>
<p>For the main analysis, the first step we took was to convert the raw reaction time data into RT data for the target syllables. This calculation included two parts, whether a target syllable was detected, and what the RT was for that syllable. A target syllable was treated as detected if there is a key press within 1200ms after the onset of the syllable. Given this criterion, participants in the exact-replication condition detected 87.7% of the syllables on average, and participants in the conceptual-replication condition detected 87.2% of the syllables on average. Thus, the detection rates of syllables in both conditions were comparable to the one reported in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, which is 87.4%. All subsequent analyses are conducted on these data (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><title>Reaction time (RT) data with syllable position (first, second, or third syllable in the word) on the x-axis, and word presentation (first, second, third, or fourth occurrence of the word in the stream) as different lines in the Figure.</title><p>The left panels show the raw data means and the right panels show the regression model fit. The top panels showed the data from the conceptual-replication condition and the bottom panel showed the data from the exact-replication condition. The style of the plot is similar to the ones in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> for ease of comparison. Error bars represent ±1 SEM.</p></caption>
<graphic xlink:href="tb26vv1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, the crucial prediction from <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> was examined, i.e., that after just one exposure, there is an effect of “word form extraction” where there is an interaction between syllable position and presentation order such that syllable position 2 and 3 as opposed to 1 should have a smaller reaction time in later presentations (2, 3, 4) as opposed to the first presentation. This pattern was found in both of the conditions, which showed up in the right panels (predicted values from the regression model) in <xref ref-type="fig" rid="fig1">Figure 1</xref>. For visual inspection, one easy way is to observe the slopes of the lines connecting the data points for syllable positions 1 through 3, as this slope is negative if syllables 2 and 3 are reacted to faster than syllable 1. The prediction is thus that, the slope for presentation 1 is not negative, but the slopes for presentation 2, 3 and 4 would be. Looking at <xref ref-type="fig" rid="fig1">Figure 1</xref>, we saw that the line for presentations 2 to 4 had negative slopes, whereas the slope for presentation 1 was not negative. To examine this effect statistically, we ran a linear mixed effect model in which RT is the dependent variable for each condition. The independent variable included fixed effects of word presentation (1-4, categorical; the choice of the variables being categorical vs. continuous was made in <xref ref-type="bibr" rid="c3">Batterink, 2017</xref>), syllable position (1-3, continuous), overall stream position (3rd through 46th syllable in the syllable sequence, continuous), and the interaction between word presentation and syllable position. Note that the overall stream position was found to be a significant predictor in addition to the rest of the variables in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> so it was included here. Random effects included participant as a random intercept and stream position as a random slope. For each condition, we first report the statistical significance of the omnibus interaction between word presentation and syllable position, and then report the pairwise comparisons between different pairs of presentation.</p>
<p>Two more aspects of the data were examined following the analysis from <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, which informs on the direction of the effect. If the effect was due to a slowdown of the unpredictable syllables for later presentations (i.e., presentation 2, 3, and 4) compared to the first presentation, this would predict the RTs for syllable position 1 to be smaller during presentation 1 compared to later presentations. This would also predict the RTs for syllable positions 2 and 3 to be the same between the presentations. On the other hand, if the effect was due to facilitation to react to the predictable syllables in later presentations, this would predict the RTs for syllable positions 2 and 3 to be smaller in later presentations compared to the first presentation, but the RTs for syllable position 1 to be similar for different presentations.</p>
<p>For the exact-replication condition, the omnibus interaction between word presentation and syllable position was significant (χ<sup>2</sup>(3) = 14.91, p = 0.002); also of note, stream position was not significant (β=0.0002, z=0.84, p=0.400); this might have been a result of a relatively small number of subjects in this condition. Next, pairwise comparisons between presentation 1 and later presentations were conducted; if the interaction coefficient is negative, it means the prediction was confirmed. The interaction between presentations 1 and 2 was negative and significant (β=-0.012, z=-2.95, p=0.003), and so was the interaction between presentations 1 and 4 (β=-0.015, z=-3.49, p&lt; 0.001). Only the interaction between presentations 1 and 3 did not reach significance (β= -0.005, z= -1.07, p= 0.287). Thus, all of the effects were numerically in the right direction and most of the predictions were confirmed in this condition.</p>
<p>For the conceptual-replication condition, the omnibus interaction between word presentation and syllable position was significant (χ<sup>2</sup>(3) = 16.66, p = 0.001). The stream position was also significant (β=0.001, z=5.65, p&lt;0.001), successfully replicated this effect from <xref ref-type="bibr" rid="c3">Battarink (2017)</xref>, where syllables occurring later in the syllable stream are detected slower than syllables occurring earlier in the syllable stream. The interaction between presentation 1 and presentation 2 was negative and significant (β=-0.007, z=-2.17, p=0.030), so was the interaction between presentation 1 and 3 (β=-0.010, z=-3.03, p=0.002) and between presentation 1 and 4 (β=-0.014, z=-3.94, p&lt;0.001). All of the effects were confirmed in the conceptual-replication condition. In sum, all of the results from the two samples showed that participants were able to react faster to the later syllables of a word compared to the first syllable following a single exposure.</p>
<p>Lastly, we examined the direction of the effect. Two analyses were carried out. First, we asked whether the RTs for syllable position 1 were different for presentation 1 vs. the later presentations. Secondly, we asked whether the RT for syllable positions 2 and 3 were different for presentation 1 vs. the later presentations. The results were the same between the two conditions. In the exact-replication condition, the RTs for syllable position 1 were not significantly different for presentation 1 vs. the later presentations (β=0.0001, z=0.32, p=0.747), and were significantly larger for position 2 and 3 for presentation 1 vs. the later presentations (β=-0.018, z=-3.01, p=0.003). In the conceptual-replication condition, the RTs for syllable position 1 were not significantly different for presentation 1 vs. the later presentations (β=0.008, z=1.41, p=0.160), and were significantly larger for position 2 and 3 for presentation 1 vs. the later presentations (β=-0.009, z=-2.58, p=0.010). Thus, the effect was due to the fact that the predictable syllables (from positions 2 and 3 in the later presentations) were responded to faster, rather than unpredictable syllables were responded to slower. This analysis thus pinpoints the origin of the effect.</p>
<p>In sum, both the exact-replication condition and the conceptual-replication condition were successful in replicating all of the aspects from <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>. The exclusion of the detection of a syllable when it is among the first two syllables or the last two syllables of the sequence did not make a difference in generating the facilitation effect.</p>
</sec>
</sec>
<sec id="s2">
<title>Experiment 2</title>
<p>As we noted above, part of testing a powerful learning mechanism involves testing conditions when the learning mechanism is known to fail in specific conditions. To this end, we conducted <xref ref-type="sec" rid="s2">Experiment 2</xref>, which differed from <xref ref-type="sec" rid="s1">Experiment 1</xref> in one crucial aspect. That is, we changed the lengths of the words that made up the continuous syllable sequences in <xref ref-type="sec" rid="s2">Experiment 2</xref>. Rather than having them be all three syllables long, which is the case in <xref ref-type="sec" rid="s1">Experiment 1</xref>, the four words making up sequences in <xref ref-type="sec" rid="s2">Experiment 2</xref> included 2 disyllabic and 2 trisyllabic words. In the word segmentation literature, using mixed-length designs leads to no segmentation (<xref ref-type="bibr" rid="c22">Johnson &amp; Tyler, 2010</xref>) or significantly weaker segmentation than with uniform sequences (<xref ref-type="bibr" rid="c16">Hoch et al., 2013</xref>). <xref ref-type="sec" rid="s2">Experiment 2</xref> allows us to examine whether the target detection paradigm employs the same mechanism as word segmentation, which would predict that there would be a weaker facilitation effect in <xref ref-type="sec" rid="s2">Experiment 2</xref> compared to <xref ref-type="sec" rid="s1">Experiment 1</xref>.</p>
<sec id="s2.1">
<title>Methods</title>
<sec id="s2.1.1">
<title>Participants</title>
<p>Twenty-one undergraduate students were recruited from Psychology Department subject pools at both the University of Nevada, Las Vegas and the University of Southern California.</p>
</sec>
<sec id="s2.1.2">
<title>Stimuli</title>
<p>The stimuli were identical to the stimuli in <xref ref-type="sec" rid="s1">Experiment 1</xref>.</p>
</sec>
</sec>
<sec id="s2.2">
<title>Design and Procedure</title>
<p>All aspects of the experiment were the same as <xref ref-type="sec" rid="s1">Experiment 1</xref>, except for the sequences used for target detection. In <xref ref-type="sec" rid="s2">Experiment 2</xref>, we generated the sequences by concatenating two disyllabic, and two trisyllabic words. In each sequence, the four words occurred 4 times, which is the same as in <xref ref-type="sec" rid="s1">Experiment 1</xref>. This meant that each sequence was 40 syllables long. Target syllables could have been any position for words of any length. All the rest of the dimensions are the same as the conceptual-replication condition from <xref ref-type="sec" rid="s1">Experiment 1</xref>.</p>
</sec>
<sec id="s2.3">
<title>Results and Discussion</title>
<p>We used the same analysis plan from <xref ref-type="sec" rid="s1">Experiment 1</xref>, combining the counterbalancing conditions (female voice/male voice first). Under the criterion that a syllable is detected if there is a key press within the 1200ms after the onset of the syllable, participants on average detected 88.9% of the syllables. Before the analysis was run, we only kept data for stream positions 3-38, where the data for the first and last two positions in the stream were dropped.</p>
<p>Below, we examined the facilitation effect of disyllabic and trisyllabic words, first separately and then together. Again, the prediction for the effect is an interaction between syllable position and presentation order such that syllable positions 2 and 3 compared to 1 should have a shorter reaction time in later presentations (2, 3, 4) as opposed to the first presentation for trisyllabic words, and for disyllabic words, this was the interaction between syllable position (2 compared to 1) with presentation order. A plot of the data from <xref ref-type="sec" rid="s2">Experiment 2</xref> can be seen in <xref ref-type="fig" rid="fig3">Figure 3</xref>, and again, negative slopes are predicted for presentations 2 through 4 but not 1. To examine the effect statistically, we conducted two linear mixed effect models, for disyllabic and trisyllabic words separately. In both regressions, the RT was the dependent variable, and the independent variable included fixed effects of word presentation (1-4, categorical), position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous), overall stream position (3rd through 46th syllable in the syllable sequence, continuous), and the interaction between word presentation and syllable position. Random effects included participant as a random intercept and stream position as a random slope.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
<caption><title>Reaction time (RT) data with syllable position (first, second, or third syllable in the word) on the x-axis, and word presentation (first, second, third, or fourth occurrence of the word in the stream) as different lines in the Figure.</title><p>The left panels show the raw data means and the right panels show the regression model fit. The top panels showed the data for the disyllabic words and the bottom panel showed the data for the trisyllabic words. Error bars represent ±1 SEM.</p></caption>
<graphic xlink:href="tb26vv1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>For trisyllabic words, the omnibus interaction between word presentation and syllable position was significant (χ<sup>2</sup>(3) = 46.40, p&lt;0.001). Stream position was found to be significant as well (β=0.002, z=5.00, p&lt;0.001). Next, we looked at interactions between syllable position and presentation pairs. The interaction between presentation 1 and presentation 2 was negative but not significant (β= -0.007, z=-1.08, p=0.281). The interaction between presentations 1 and 3 was negative and significant (β=-0.014, z=-2.17, p=0.030). The interaction between presentations 1 and 4 was negative and significant (β=-0.043, z=-6.11, p&lt;0.001). For disyllabic words, the regression containing both participant as a random intercept and stream position as a random slope did not converge, so we only kept the participant as a random intercept, which converged. In this regression, the omnibus interaction between word presentation and syllable position did not reach significance (χ<sup>2</sup>(3) =6.52, p= 0.089). Stream position was also found not to be significant (β=0.0001, z=0.37, p=0.711). The interaction between presentation 1 and presentation 2 was negative and significant (β= -0.024, z=-2.38, p=0.017). The interaction between presentation 1 and 3 was negative and marginally significant (β=-0.020, z=-1.84, p=0.066). The interaction between presentations 1 and 4 was negative and significant (β=-0.027, z=-2.21, p=0.027). Lastly, we analyzed whether the interaction between presentation order and syllable position significantly interacted with word length. For this analysis, we added an interaction term of word length to the previous regression model, such that the fixed effect became a three-way interaction between word presentation (1-4, categorical), position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous) and word-length (2/3, categorical), as well as the overall stream position. Random effects included participant as a random intercept and stream position as a random slope. The interaction was not significant (χ<sup>2</sup>(3) = 6.19, p= 0.103). Together, these analyses showed that there was a robust effect for trisyllabic and disyllabic words alike, and no difference between the two types of words. A plot of the data is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p>
<p>Lastly, we want to answer the question of whether the facilitation effect is larger in the uniform condition than in the mixed condition, which would be the prediction if the current target detection task engages the same mechanism as the word segmentation paradigm. Notably, there are some differences in terms of the structure of data in the uniform and mixed conditions. First, the mixed condition involved both disyllabic and trisyllabic words, whereas the uniform condition only had trisyllabic words. For the analysis below, we put word length in the fixed effect as a main effect, since we found the two types of words to have similar effects and no interactions, as we just discussed. Secondly, the length of syllable streams was shorter in mixed conditions compared to the uniform conditions, because half of the words were disyllabic in the mixed condition. This meant that streams were 48 syllables long in the uniform condition, but only 40 syllables long in the mixed condition. Since stream position has consistently been a significant predictor of reaction times, this is likely to affect the effects as well. Putting these two variables as main effects allowed us to observe the interaction of interest while controlling these important variables.</p>
<p>The prediction for the difference between the mixed and uniform conditions in the present target detection tasks, if they act similarly to word segmentation tasks, is that the effect is smaller in the mixed condition than in the uniform condition. For this analysis, we compared the data from <xref ref-type="sec" rid="s2">Experiment 2</xref> to the exact-replication condition in <xref ref-type="sec" rid="s1">Experiment 1</xref>, which had a similar number of subjects (though using data from the conceptual-replication condition yielded the same results; see <xref ref-type="app" rid="ap1">Appendix</xref>). To examine this effect, we set up the following mixed effect regression with a three-way interaction. The RT was the dependent variable, and the independent variable included fixed effects of condition (mixed/uniform, categorical), word presentation (1-4, categorical), and position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous), and the interaction between the three. Fixed effect further included overall stream position (3rd through 46th syllable in the syllable sequence in the uniform condition, 3rd through 38th syllable in the mixed condition, both continuous) and word length (disyllabic/trisyllabic, categorical). Random effects included participant as a random intercept and stream position as a random slope. The omnibus three-way interaction was significant (χ<sup>2</sup>(3) =15.79, p=0.001), suggesting that the ways syllable position and presentation interact in the two experiments are different. To understand this three-way interaction, we looked at the three-way interaction between syllable position, condition, and pairs of presentations (i.e., 1 and 2, 1 and 3, and 1 and 4). We found that the three-way interaction for presentations 1 and 2 (β= -0.003, z=-0.52, p=0.675) was negative and not significant, became positive and not significant for presentations 1 and 3 (β=0.011, z=1.65, p=0.099), and became positive and significant for presentations 1 and 4 (β=0.021, z=2.99, p=0.003). In other words, the coefficients grow as a function of presentation in this three-way interaction. Looking at a plot of model fit (<xref ref-type="fig" rid="fig4">Figure 4</xref>), this pattern becomes clear: while the slopes (from syllable position 1 to 3) for presentation 1 were flat for both conditions, the negative slope for presentation 4 for the mixed condition was the largest in absolute value (from 570ms to 494ms) for all slopes, more than in presentation 4 for the uniform condition (from 579ms to 546ms). This was the three-way interaction we saw. We could understand this result as the mixed condition having a larger effect than the uniform condition, but as we explore in the simulation below, this statistical difference is consistent with a scenario where the facilitation effect is the same in both conditions. Importantly, these results differ from our a priori hypothesis that there is less learning in the mixed condition: the mixed condition did not generate a smaller effect than the uniform condition. This suggests that the mechanism behind the target detection task examined in this paper was different than the mechanisms involved in word segmentation.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4</label>
<caption><title>Regression model fit from the three-way interaction between condition (mixed/uniform, categorical), word presentation (1-4, categorical), and position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous).</title><p>Figure 4A showed results from the uniform condition from <xref ref-type="sec" rid="s1">Experiment 1</xref>, and Figure 4B showed results from the mixed condition from <xref ref-type="sec" rid="s2">Experiment 2</xref>. Error bars represent ±1 SEM.</p></caption>
<graphic xlink:href="tb26vv1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Simulations</title>
<p>Having discussed an anticipation account for prediction in the introduction, the purpose of the current simulations is the implement the process computationally, which can provide insights into the nature of the computation required to produce the results we found in the experiments. We directly model RTs in this simulation, with the simple idea that syllables are either predictable or unpredictable in the input stream. RTs for predictable syllables are generated with one pattern and RTs for unpredictable syllables are generated with another pattern.</p>
<p>This model is to process syllable sequences online, and to generate a RT for each syllable that is processed. At the beginning of processing a syllable sequence, the model assumes the learner to detect the target with a baseline amount of time, RT0, which is a constant. From this point on, the model stores each bigram it encounters. Based on the bigrams that are stored at any point, the next syllable is either predictable or unpredictable. The core assumptions are that 1) predictable syllables get a facilitation effect when it is reacted to, and 2) unpredictable syllables do not. As such, we propose a simple relation between the RT of a syllable occurring for the n<sup>th</sup> time and the n+1<sup>th</sup> time, which is:
<disp-formula id="ed1"><alternatives><mml:math display="block" id="e1"><mml:mrow><mml:mtext>RT</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>RT</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>n</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mtext>stream_pos</mml:mtext><mml:mo>*</mml:mo><mml:mtext>stream_inc       </mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>RT</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>n</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mtext>stream_pos</mml:mtext><mml:mo>*</mml:mo><mml:mtext>stream_inc</mml:mtext><mml:mo>+</mml:mo><mml:mtext>occ_inc</mml:mtext><mml:mo>*</mml:mo><mml:mtext>occurrence  </mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><graphic xlink:href="tb26vv1_eqn1.tif" mimetype="image" mime-subtype="tiff"/></alternatives></disp-formula></p>
<p>and</p>
<p>RT(1) = RT0 + stream_pos * stream_inc, where the n in RT(n) represents the RT for the n<sup>th</sup> presentation of the target syllable, stream_pos is the position (3-46) in the stream, and occurrence is the number of occurrences that the syllable has occurred so far in the stream.</p>
<p>This process applies to the rest of the syllables in the sequence, until the end of the syllable stream. At this point, each syllable in the sequence will have a corresponding RT. To simulate the process of a participant reacting to a single target syllable, we will output the RTs corresponding to a random target syllable, such that the only data left for a syllable sequence are 4 RT values for the 4 occurrences of the target syllable.</p>
<p>Here is a more in-depth discussion of the assumptions behind this simple model. First, if a syllable occurs for the first time, we expect the learner to detect the target with a baseline amount of time. Theoretically, we take this to mean that it would take a certain amount of time to recognize and react to a syllable for the first time. Secondly, the next time the target syllable occurs, the amount of time it takes to react to the target syllable depends on whether this syllable is predictable or not. If it is unpredictable, the amount of time it takes to react is the same amount of time as the last time it was reacted to. If the target syllable is predictable, the amount of time it takes to react is different from the last time, by a constant (occ_inc) times the number of times this syllable has occurred so far. Theoretically, if it takes a certain amount of time to react to the target syllable the last time, this time, the reaction to the target syllable is facilitated by its predictability, where the amount is proportional to the number of times this target syllable has occurred so far. The assumption that the facilitation amount is proportional to the number of times the target syllable already occurred is based on the empirical finding that the more the target syllable was detected, the faster the RT is. The constant (occ_inc) represents the amount of facilitation effect due to predictability. In addition to the predictability factors, one more (positive) number needs to be added to each RT, which is a stream-position effect: the later the syllable is in the stream, the slower the RT is. This is also based on empirical findings from the task. For a discussion of the specifics of setting these parameters, see <xref ref-type="app" rid="ap1">Appendix</xref>.</p>
<p>There are three parameters in our set of equations. The first, the baseline RT (RT0), does not factor into the pattern of data results later, as all RTs share this component equally. We set this RT0 to be the constant from the regression coefficient, from previous regressions. The second constant is the stream_inc, the increment amount for stream position. Again, it is common to all RTs. We set it as a small, positive number, which represents the general trend that RTs are larger the later the target is in the stream. The third constant is occ_inc, the increment for the number of targets that already occurred. We know this number to be negative (i.e., more occurrences would mean smaller RTs). We took a small, negative number from the corresponding regression coefficient. Notably, though we took the estimates from the regressions, this by no means would mean that the resulting RT distribution would resemble the RT distributions from the humans. The point of this simulation is to consider the properties of the model when we only consider very few factors (predictability/structure of the syllable sequence), and see if RT distributions based on these factors can be similar to the RT distributions from the human data.</p>
<p>To implement this model computationally, we went through a few steps. First, we constructed the syllable sequences, in the same way as we did in the experiments. Note that, during this step, there is randomness in constructing the syllable sequences, as different words can be concatenated in different orders while maintaining the constraints for the order (i.e., no words can follow itself). Next, we implement the target detection section of the task, randomly picking a target syllable in the syllable stream. We generated RTs for all syllables based on the formula described above, though, for the data from this simulation, only the RTs associated with the targets were saved in the data. To do this, in an online fashion, the model stores the bigrams that it has encountered so far, and calculates the RT of the next syllable based on the bigrams from the collection of bigrams that are remembered, and the RT of the syllable from the last occurrence. Simply put, the RTs for the unpredictable syllables only include the baseline RT plus positive change as a function of the stream position. The RTs for the predictable syllables are a function of how predictable they are, on top of initial conditions. Again, note that no “word extraction” is required: the model only requires exposure to the input and stores the bigrams it encounters; There are bigrams that are predictable and unpredictable, and there is no need to make inferences over where the word boundaries are in the input sequence for the model to operate.</p>
<p>Given this model, we conducted two simulations, a uniform condition simulation, and a mixed condition simulation. These two simulations mirrored the structure of <xref ref-type="sec" rid="s1">Experiments 1</xref> and <xref ref-type="sec" rid="s2">2</xref> above, in terms of how the syllable sequences were set up. In each simulation, we generated the data for the same number of subjects (19, from <xref ref-type="bibr" rid="c3">Batterink 2017</xref>) and the same number of trials (144). For each trial, we generated the RT values according to the formula described above. Notably, the same parameters are used in both conditions. The simulations thus represent learners with the same learning characteristics: by using the same set of parameters going into the two conditions, we are assuming these learners behave the same for the two conditions.</p>
<p>Running the model generates simulated data for each condition. With the simulated data, we ran the same set of regressions as we did in the experiments. First, for each simulation, we looked at the (fixed) effect of syllable position (1-3), presentation (1-4), and their interaction, in addition to stream position (1-48). Next, we conducted a three-way interaction for syllable position (1-3), presentation (1-4), and condition (uniform/mixed). All these regressions included by-subject random intercepts and a random slope of stream position, the same as the regressions we ran for experiments.</p>
<p>The results for the model mirrored the qualitative pattern of data from human experiments. First, we found that the slope for the first presentation in the fitted model across three syllable presentations is the same, flat slope as we observed in the human data, for both the uniform and mixed conditions. Second, we found that there was a three-way interaction, the same way as the human results: The slope for the fourth presentation of the mixed condition is larger than in the uniform condition, given the same slopes for the first presentations in both conditions (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><title>Regression model fit from the three-way interaction between condition (mixed/uniform, categorical), word presentation (1-4, categorical), and position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous) for the simulated data.</title><p>Figure 5A showed results from the simulation for the uniform condition, and Figure 5B showed results from the simulation for the mixed condition.</p></caption>
<graphic xlink:href="tb26vv1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The fact that such a simple model can capture the same patterns from the human results is remarkable. The simplicity is based on the number of assumptions that went into the model, which are simply that predictable targets get shorter RTs, the amount of which is based on the number of times this particular target has occurred so far. This means that no other assumptions are required for the facilitation effect to occur. If one compares this model to other models for segmentation (e.g., the ones listed in <xref ref-type="bibr" rid="c4">Bernard et al., 2020</xref>), this model would have the least number of assumptions built in. More importantly perhaps, when we set the same parameter for the uniform and the mixed condition in the simulations, that is, setting the amount of change to be the same for predictable items in two conditions, we find that the same difference as we found in the human experiments, which is that the mixed condition showed a larger effect than the uniform condition. This may provide an explanation for our behavioral result, namely, that the larger effect in the mixed condition does not suggest that people reacted more quickly in the mixed condition, but is a reflection of mean lengths of the words in the syllable sequence – that is, the effect may be a result of total stream length difference between the two conditions (for a more thorough exploration of this effect, see the additional simulations in the <xref ref-type="app" rid="ap1">Appendix</xref>). Importantly, for the current discussion on the origin of the difference, the same amount of facilitation effect from previous occurrences in our computational model provides a good fit for the human data.</p>
</sec>
<sec id="s4" sec-type="discussion">
<title>General Discussion</title>
<p>This paper investigated the mechanisms involved in statistical word segmentation, reporting two experiments using the target detection task and comparing them to studies from the word segmentation literature. In <xref ref-type="sec" rid="s1">Experiment 1</xref>, we reported a successful replication of <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, including both a conceptual replication and an exact replication. The facilitation effect in question was successfully replicated, where the reaction time was shorter for predictable syllables (syllable position 2 and 3 in a triplet) compared to unpredictable syllables (syllable position 1) in later presentations (2, 3, 4) as opposed to the first presentation. In <xref ref-type="sec" rid="s2">Experiment 2</xref>, we changed the structure of the syllable sequences in the study, where instead of using words of uniform length (which was the case for <xref ref-type="sec" rid="s1">Experiment 1</xref>), we used sequences with mixed-length words. Such a change has been shown to generate a smaller amount of learning in the word segmentation literature, under the segmentation paradigm. However, with the target detection task, we found a similar facilitation effect in the mixed-length condition in <xref ref-type="sec" rid="s2">Experiment 2</xref>, with the same speed that a single exposure was enough for this effect. Contrary to our prediction based on the segmentation literature that uniform-length sequences are learned better than mixed-length sequences, we found that the effect in the mixed condition (<xref ref-type="sec" rid="s2">Experiment 2</xref>) was larger in the uniform condition (<xref ref-type="sec" rid="s1">Experiment 1</xref>). To explain these results, we computationally modeled a prediction process, where the only assumptions in the model involved changes to the RT based on predictability. Simulations provided evidence that the same computational processes and parameters generated similar effects for both the uniform and mixed conditions. In fact, with the same facilitation parameter in both conditions, we found a larger effect in the mixed condition, and this is consistent with our data. We took this as evidence supporting the hypothesis that humans employed the same processes for mixed and uniform conditions. Taken together, these results suggest that a simple prediction-based anticipation mechanism can explain the results from the target detection task, and the mechanisms involved in this task may be different from the ones employed in word segmentation.</p>
<sec id="s4.1">
<title>Mechanisms in the target detection tasks</title>
<p>What are the mechanisms behind the target detection task, if the mechanisms are not the same set of mechanisms involved in word segmentation? In the paragraphs below, we will discuss the following points. First, we discuss the mechanism behind the target detection task, which we argue to be a prediction-based anticipation mechanism. Notably, under such a mechanism, one exposure suffices for the prediction to occur. Secondly, we will provide a theoretical analysis of why segmentation based on distributional evidence would require more evidence. We argue that the prediction-based anticipation mechanism involves processes at word-internal locations, which is different from mechanisms for segmentation, which involves decisions at word boundaries. As such, whereas one exposure enables prediction-based anticipation, minimally two occurrences in a sequence are required for a set of syllables to form a word statistically.</p>
<p>First, we begin with a theoretical analysis that can shed light on the difference between the two tasks. As a general statement that has implications for all the discussion below, the two tasks require learners to use different information, which is used at different locations in the sequence. In terms of the information required in the target detection task, let’s consider the following sequence: GHIABCDEF<underline>ABC</underline>GHI, where the word “ABC” is preceded and followed by different words. By the second time “ABC” occurs (underlined in the sequence), the syllable B is preceded by the syllable A, and this is predictable because the AB transition occurred prior. This simple analysis suggests two things. First, the location of the information enabling the facilitation effect is word-internal – rather than word boundaries. Secondly, only one co-occurrence was enough for the facilitation effect to occur during the second encounter, because the prediction occurs word-internally. Together, this means that one prior occurrence can enable learners to generate a prediction.</p>
<p>In the literature, this has been discussed as a prediction-based anticipation mechanism for statistical learning (e.g., <xref ref-type="bibr" rid="c2">Barakat, Seitz, &amp; Shams, 2013</xref>; <xref ref-type="bibr" rid="c9">Davachi &amp; DuBrow, 2015</xref>; <xref ref-type="bibr" rid="c31">Summerfield &amp; De Lange, 2014</xref>; <xref ref-type="bibr" rid="c33">Turk-Browne et al., 2010</xref>). Under such a mechanism, the brain can encode the co-occurrences of stimuli from the past. In statistical learning terms, such a prediction-based anticipation mechanism can be viewed as a simpler version of a conditional probability model, where the conditional probability becomes 1 for two elements. That is to say, the conditional probability for two elements, after an initial encounter, can be calculated as 1 (i.e., p(B|A) = 1 when there is only a single encounter to AB). Notably, this fact is contrary to a specific claim in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, where it was argued that the calculation of conditional probabilities could not support the facilitation effect given a single exposure, because “the computation of conditional probabilities depends on accruing statistical data across a sample of input and cannot occur instantly after only a single exposure to an underlying pattern (Batterink, p. 926)”. However, a single exposure does provide information about the transitions within the single exposure, and the probability of B given A can indeed be calculated from a single occurrence of AB. In our model, for example, the second time a predictable syllable occurs, it is marked predictable because it occurred one time prior, and another syllable can predict it.</p>
<p>This brings us to the discussion of the difference between the mechanisms that one needs to explain word segmentation and target detection. As we just discussed, one single occurrence of AB is enough for the prediction of B the next time A appears. However, for statistical segmentation, a single occurrence is not enough. Let’s consider the example sequence above one more time, but this time only the section prior to the second occurrence of ABC (i.e., “GHIABCDEF”). Even if a learner can remember these syllables perfectly, there is no information for segmentation. That is to say, since all the syllables have occurred exactly once, there is no distributional information for segmentation. Only after ABC occurs the second time, word boundaries defined by distributional information begin to emerge: The fact that syllable A is preceded by different syllables (I and F) makes the forward transitional probability of I or F going to A to be 1/2, and the fact that syllable C is followed by different syllables (D and G) makes the forward transitional probability of C going to D or G to be 1/2. Thus, in this example, having had two exposures would enable the segmentation of ABC from this sequence (using a similar measure, such as backward transitional probability or mutual information, would require the same information). Going back to the location of the information for segmentation, it’s clear that the decisions to segment require information at word boundaries; and concretely, prior to A, and after C. This also marks the difference to the prediction-based anticipation mechanism, where the critical information for the effect is word-internal. In sum, this theoretical analysis suggests that multiple exposures are required to make segmentation possible, whereas a single exposure could allow predictions between syllables to occur<xref ref-type="fn" rid="fn3"><sup>1</sup></xref>.</p>
<p>Thus, both the difference in terms of the location of the information and the information requirements for segmentation means that there is a disassociation of the mechanisms between the facilitation effect in the target detection task and the word segmentation task. In target detection, the decision to react to the target is local to positions that involve specific transitions in the sequence. In this sense, no segmentation is required; remembering bigrams, as we demonstrated in our model, would suffice for this task. However, the segmentation task requires the learner not only to segment the sequence, but also to remember the segmented subsequences in memory. To segment a single word requires two decisions for word boundary, and then, the segmentation task requires the segmented sequence to be remembered (i.e., only representing where the word boundaries are would not be enough.) These differences mean that, detecting targets from sequences with uniform-length and mixed-length words would generate a similar amount of learning (as evidenced by our experiments above). However, segmenting words from the two types of sequences is differentially difficult, because sequences with mixed-length words are more complex (<xref ref-type="bibr" rid="c22">Johnson &amp; Tyler, 2010</xref>; <xref ref-type="bibr" rid="c36">Wang, Trueswell, Zevin, &amp; Mintz, under review</xref>). In sum, the two tasks may both require the learner to use co-occurrence information from the sequence, the two tasks require the learner to process different information to accomplish, and thus require different task-demands and mechanisms.</p>
</sec>
<sec id="s4.2">
<title>Time course for the facilitation and other similar effects</title>
<p>Through empirical work and a computational model, we provided evidence that the facilitation effect happened only after one exposure. However, for a complete theory for the time course of word segmentation, it’s not the case that an exposure or two should be considered the whole picture. For example, even though learning was successful within the word segmentation paradigm with only two exposures, four exposures produced significantly more robust learning (<xref ref-type="bibr" rid="c35">Wang et al., 2023</xref>). At the same time, it’s not the case that more exposure equals more learning. Other than the examples in the introduction, Bulgarelli and Weiss (2016) conducted a study looking at the time course of learning. Participants were presented with multiple 67-second syllable sequences (which contained hundreds of syllables), and tested between the presentation of each syllable sequence. Learning plateaued after a single block of learning, where the effect size of learning never changed following the first block or after several blocks of learning. In sum, the relationship between exposure and learning is complicated, requiring an examination of the cognitive mechanisms involved in segmentation as a function of time and complexity of the learning materials (e.g., sequences with uniform- vs. mixed-length words), a topic for future work.</p>
<p>The timing characteristics of target detection may be unique in the literature, as most tasks cannot detect learning so quickly. For example, even though serial reaction time (SRT) tasks have also been used to examine the learning of statistical dependencies (e.g., <xref ref-type="bibr" rid="c17">Howard &amp; Howard, 1997</xref>; <xref ref-type="bibr" rid="c18">Hunt &amp; Aslin, 2001</xref>; <xref ref-type="bibr" rid="c34">Wang &amp; Kaiser, 2022</xref>), the effect emerges much slower. The difference between SRT tasks and the target detection task is that, in SRT tasks, participants make a key press for every stimulus, whereas the target detection task requires key presses only for a single target. The slow emergence of the learning effect may have to do with the fact that making a key press for every stimulus requires the learner to pay constant attention to the upcoming stimulus in order for an action (making a key press). In contrast, in target detection tasks, there is no action required for most of the stimulus, so that the participants may plan their action while processing the stimuli. For example, in <xref ref-type="bibr" rid="c18">Hunt and Aslin (2001)</xref>, participants completed 70-word sessions, and completed 8 sessions a day for 6 consecutive days. While the question of how many sessions are required to produce a reliable effect was not explored directly in that study, the data showed participants took multiple sessions to show a learning effect in many experiments. Notably, even though the target detection task has been used in other studies (Bertels, Boursain, Destrebecqz, &amp; Gaillard, 2014; <xref ref-type="bibr" rid="c6">Bertels, Demoulin, Franco, &amp; Destrebecqz, 2013</xref>; <xref ref-type="bibr" rid="c7">Bertels, Franco, &amp; Destrebecqz, 2012</xref>; <xref ref-type="bibr" rid="c13">Franco, Eberlen, Destrebecqz, Cleeremans, &amp; Bertels, 2015</xref>; <xref ref-type="bibr" rid="c26">Kim, Seitz, Feenstra, &amp; Shams, 2009</xref>; <xref ref-type="bibr" rid="c33">Turk-Browne et al., 2010</xref>), <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> was the first study to demonstrate that learners can show a facilitation effect after a single exposure to our knowledge. Most of the other studies using the target detection task (e.g., <xref ref-type="bibr" rid="c13">Franco et al., 2015</xref>) provided an exposure phase to the participants before the target detection task began, making it unclear when the effect arose. Our current study provides further empirical evidence that the facilitation effect for predictable syllables emerges given a single exposure, demonstrating that the effect is equally applicable when learning from uniform-length and mixed-length sequences.</p>
</sec>
<sec id="s4.3">
<title>Target Detection and PARSER</title>
<p>Zooming in on the facilitation effect in the target detection task specifically, one of the candidates for explaining the effect involves clustering (<xref ref-type="bibr" rid="c3">Batterink, 2017</xref>). <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> discussed that clustering may explain the data better than the use of conditional probabilities, because obtaining conditional probabilities may require more than a single exposure, citing a computational model known as PARSER (<xref ref-type="bibr" rid="c28">Perruchet &amp; Vinter, 1998</xref>). PARSER accomplishes segmentation in two iterative steps. In the first step, PARSER randomly picks a number from 1 through n (typically 3), and clusters this random number of syllables as a chunk. This step creates chunks and stores them in memory with certain weights associated with each one (termed Perceptual Shaper). In the second step, PARSER either strengthens the weight or decreases the weight of items in the Perceptual Shaper: If the incoming chunk matches an existing chunk, the weight of the existing chunk (and its components) is increased. However, if the incoming chunk is completely new, it is added to the Perceptual Shaper, but at the same time, the weights of all the previous chunks are decreased. The updating of the weights occurs in time steps, and the two steps occur during each time step. With this iterative process, PARSER can successfully segment a syllable sequence into its component words, because these words (and their components) are more likely to repeatedly occur, much more likely than part-words.</p>
<p>So, can PARSER explain learning after a single exposure? To answer this question empirically, we created a simulation. In this simulation, we used the U-Learn program (<xref ref-type="bibr" rid="c29">Perruchet, Robinet, &amp; Lemaire, 2014</xref>) to examine the learning of short sequences (see <xref ref-type="app" rid="ap1">Appendix</xref>). Notably, there is a lack of a linking assumption translating the weights of different chunks to RT differences in the target detection. Here, we asked PARSER to evaluate words vs. part-words, which is a function built-in to the U-Learn program, and made a linking assumption: if the words and part-words are differentially weighted, this is equivalent to the facilitation effect in target detection (i.e., we take the learning effect from PARSER to indicate learning). The U-Learn program reports the rate words are preferred over part-words in 10 time-steps (which corresponds to 1/10 of the learning sequence, however long the learning sequence is). Thus, if we use 4 different words each of which occurs 10 times (modifying the existing “ready-to-use configurations”), 1/10 of the syllable sequence is 4 words long and 2/10 of the syllable sequence is 8 words long. Running the simulation 50 times to represent running 50 subjects on this task, we find that, out of 50 times, the percentage of time where words were segmented but part-words were not during the first 1/10 of the sequence was 0 times, and this percentage became 1% after 2/10 of the sequence. Thus, it’s not the case that PARSER can successfully segment words following a single exposure. In this instance, it would appear that humans are better learners than PARSER. In a second simulation, we created the training sequences from <xref ref-type="bibr" rid="c22">Johnson and Tyler (2010)</xref> which contained both sequences with uniform-length and mix-length words. The result of the simulation was that PARSER was equally successful with the uniform- and mixed-length conditions (see <xref ref-type="app" rid="ap1">Appendix</xref>). In this instance, PARSER is perhaps more powerful than humans in terms of segmentation. Thus, it would appear that PARSER cannot account for the kind of results that humans produce, where it is not as sensitive to statistical regularities as humans in a target detection task, and too powerfully equipped to learn when humans would have trouble.</p>
</sec>
<sec id="s4.4">
<title>Conclusions</title>
<p>In summary, the current study found that the facilitation effect from the target detection task is empirically robust, and can be shown with sequences with uniform-length words or mixed-length words alike. The speed for a facilitation effect following a predictable sequence to appear is indeed at its theoretical limit of just one prior encounter. Through empirical findings and a computational model, we provided a possible mechanism to explain this facilitation effect from the target detection task. Furthermore, by comparing the current results in the target detection task to work from the word segmentation literature, we argued that the mechanisms involved in the target detection task are different from the word segmentation task. Future exploration is needed to understand the relationship between the amount of exposure and learning in statistical word segmentation, as well as a characterization of the memory mechanisms that are involved during the segmentation process.</p>
</sec>
</sec>
</body>
<back>
<fn-group>
<fn id="fn3"><label>1</label><p>There are cases exceptional to these discussions. If the learner can segment through subtraction, it would only require a single exposure to the novel word. In this instance, the linguistic materials surrounding it need to be known. For example, consider the English sentence “I bought a dax yesterday”, where one knows all the other words in the sentence. In this case, the novel word can be segmented distributional through subtraction (Lignos &amp; Yang, 2010). But most discussions on word segmentation regard scenarios where most if not all word forms are unknown, so the example in this footnote counts more as an exception.</p></fn>
</fn-group>
<sec id="n1">
<title>Note</title>
<p>This reviewed preprint has been updated to correct the corresponding author's name.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, <string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Computation of conditional probability statistics by 8-month-old infants</article-title>. <source>Psychological science</source>, <volume>9</volume>(<issue>4</issue>), <fpage>321</fpage>–<lpage>324</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barakat</surname>, <given-names>B. K.</given-names></string-name>, <string-name><surname>Seitz</surname>, <given-names>A. R.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2013</year>). <article-title>The effect of statistical learning on internal stimulus representations: Predictable items are enhanced even when not predicted</article-title>. <source>Cognition</source>, <volume>129</volume>(<issue>2</issue>), <fpage>205</fpage>–<lpage>211</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Batterink</surname>, <given-names>L. J.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Rapid statistical learning supporting word extraction from continuous speech</article-title>. <source>Psychological Science</source>, <volume>28</volume>(<issue>7</issue>), <fpage>921</fpage>–<lpage>928</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bernard</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Thiolliere</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Saksida</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Loukatou</surname>, <given-names>G. R.</given-names></string-name>, <string-name><surname>Larsen</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>M.</given-names></string-name>, … &amp; <string-name><surname>Cristia</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>WordSeg: Standardizing unsupervised word form segmentation from text</article-title>. <source>Behavior research methods</source>, <volume>52</volume>, <fpage>264</fpage>–<lpage>278</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bertels</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Boursain</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Destrebecqz</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Gaillard</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Visual statistical learning in children and young adults: how implicit?</article-title> <source>Frontiers in Psychology</source>, <volume>5</volume>, <fpage>1541</fpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bertels</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Demoulin</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Franco</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Destrebecqz</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Side effects of being blue: influence of sad mood on visual statistical learning</article-title>. <source>PloS one</source>, <volume>8</volume>(<issue>3</issue>), <fpage>e59832</fpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bertels</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Franco</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Destrebecqz</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2012</year>). <article-title>How implicit is visual statistical learning?</article-title> <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>38</volume>(<issue>5</issue>), <fpage>1425</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conway</surname>, <given-names>C. M.</given-names></string-name></person-group> (<year>2020</year>). <article-title>How does the brain learn environmental structure? Ten core principles for understanding the neurocognitive mechanisms of statistical learning</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>112</volume>, <fpage>279</fpage>–<lpage>299</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davachi</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>DuBrow</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>How the hippocampus preserves order: the role of prediction and context</article-title>. <source>Trends in cognitive sciences</source>, <volume>19</volume>(<issue>2</issue>), <fpage>92</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Endress</surname>, <given-names>A. D.</given-names></string-name>, &amp; <string-name><surname>Mehler</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>The surprising power of statistical learning: When fragment knowledge leads to false memories of unheard words</article-title>. <source>Journal of Memory and Language</source>, <volume>60</volume>(<issue>3</issue>), <fpage>351</fpage>–<lpage>367</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Estes</surname>, <given-names>K. G.</given-names></string-name>, <string-name><surname>Evans</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Alibali</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Can infants map meaning to newly segmented words? Statistical segmentation and word learning</article-title>. <source>Psychological science</source>, <volume>18</volume>(<issue>3</issue>), <fpage>254</fpage>–<lpage>260</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Finn</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Kam</surname>, <given-names>C. L. H.</given-names></string-name></person-group> (<year>2008</year>). <article-title>The curse of knowledge: First language knowledge impairs adult learners’ use of novel statistics for word segmentation</article-title>. <source>Cognition</source>, <volume>108</volume>(<issue>2</issue>), <fpage>477</fpage>–<lpage>499</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franco</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Eberlen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Destrebecqz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cleeremans</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bertels</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Rapid serial auditory presentation</article-title>. <source>Experimental psychology</source>, <volume>62</volume>(<issue>5</issue>), <fpage>346</fpage>–<lpage>351</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frank</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Goldwater</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name>, &amp; <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Modeling human performance in statistical word segmentation</article-title>. <source>Cognition</source>, <volume>117</volume>(<issue>2</issue>), <fpage>107</fpage>–<lpage>125</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giroux</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Rey</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Lexical and sublexical units in speech perception</article-title>. <source>Cognitive Science</source>, <volume>33</volume>(<issue>2</issue>), <fpage>260</fpage>–<lpage>272</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoch</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Tyler</surname>, <given-names>M. D.</given-names></string-name>, &amp; <string-name><surname>Tillmann</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Regularity of unit length boosts statistical learning in verbal and nonverbal artificial languages</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>20</volume>, <fpage>142</fpage>–<lpage>147</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-012-0309-8</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Howard</surname>, <given-names>J. H.</given-names>, <suffix>Jr.</suffix></string-name>, &amp; <string-name><surname>Howard</surname>, <given-names>D. V.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Age differences in implicit learning of higher order dependencies in serial patterns</article-title>. <source>Psychology and Aging</source>, <volume>12</volume>, <fpage>634</fpage>–<lpage>656</lpage>. <pub-id pub-id-type="doi">10.1037/0882-7974.12.4.634</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hunt</surname>, <given-names>R. H.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Statistical learning in a serial reaction time task: access to separable statistical cues by individual learners</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>130</volume>(<issue>4</issue>), <fpage>658</fpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Word segmentation by 8-month-olds: When speech cues count more than statistics</article-title>. <source>Journal of memory and language</source>, <volume>44</volume>(<issue>4</issue>), <fpage>548</fpage>–<lpage>567</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name></person-group> (<year>2003a</year>). <article-title>Exploring possible effects of language-specific knowledge on infants’ segmentation of an artificial language</article-title>. <source>Jusczyk Lab Final Report</source>, <fpage>141</fpage>–<lpage>148</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name></person-group> (<year>2003b</year>). <article-title>Exploring statistical learning by 8-month-olds: The role of complexity and variation</article-title>. <source>Jusczyk Lab Final Report</source>, <fpage>141</fpage>–<lpage>148</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Tyler</surname>, <given-names>M. D.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Testing the limits of statistical learning for word segmentation</article-title>. <source>Developmental Science</source>, <volume>13</volume>(<issue>2</issue>), <fpage>339</fpage>–<lpage>345</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name></person-group> (<year>1999</year>). <article-title>How infants begin to extract words from speech</article-title>. <source>Trends in cognitive sciences</source>, <volume>3</volume>(<issue>9</issue>), <fpage>323</fpage>–<lpage>328</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name></person-group> (<year>1995</year>). <article-title>Infants’ detection of the sound patterns of words in fluent speech</article-title>. <source>Cognitive psychology</source>, <volume>29</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name>, <string-name><surname>Houston</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Newsome</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1999</year>). <article-title>The beginnings of word segmentation in English-learning infants</article-title>. <source>Cognitive psychology</source>, <volume>39</volume>(<issue>3-4</issue>), <fpage>159</fpage>–<lpage>207</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Seitz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Feenstra</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Testing assumptions of statistical learning: is it long-term and implicit?</article-title> <source>Neuroscience letters</source>, <volume>461</volume>(<issue>2</issue>), <fpage>145</fpage>–<lpage>149</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Learning at a distance I. Statistical learning of non-adjacent dependencies</article-title>. <source>Cognitive psychology</source>, <volume>48</volume>(<issue>2</issue>), <fpage>127</fpage>–<lpage>162</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perruchet</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Vinter</surname>, <given-names>A.</given-names></string-name></person-group> (<year>1998</year>). <article-title>PARSER: A model for word segmentation</article-title>. <source>Journal of Memory and Language</source>, <volume>39</volume>(<issue>2</issue>), <fpage>246</fpage>–<lpage>263</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Perruchet</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Robinet</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Lemaire</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2014</year>). <source>U-Learn: Finding optimal coding units from unsegmented sequential databases</source>. <comment>Unpublished manuscript</comment>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, &amp; <string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Statistical learning by 8-month-old infants</article-title>. <source>Science</source>, <volume>274</volume>(<issue>5294</issue>), <fpage>1926</fpage>–<lpage>1928</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Expectation in perceptual decision making: neural and computational mechanisms</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>15</volume>(<issue>11</issue>), <fpage>745</fpage>–<lpage>756</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Swingley</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Statistical clustering and the contents of the infant vocabulary</article-title>. <source>Cognitive Psychology</source>, <volume>50</volume>(<issue>1</issue>), <fpage>86</fpage>–<lpage>132</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turk-Browne</surname>, <given-names>N. B.</given-names></string-name>, <string-name><surname>Scholl</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>M. K.</given-names></string-name>, &amp; <string-name><surname>Chun</surname>, <given-names>M. M.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Implicit perceptual anticipation triggered by statistical learning</article-title>. <source>Journal of Neuroscience</source>, <volume>30</volume>, <fpage>11177</fpage>–<lpage>11187</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>F. H.</given-names></string-name>, &amp; <string-name><surname>Kaiser</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Linguistic Priming and Learning Adjacent and Nonadjacent Dependencies in Serial Reaction Time Tasks</article-title>. <source>Language Learning</source>, <volume>72</volume>(<issue>3</issue>), <fpage>695</fpage>–<lpage>727</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Luo</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Statistical word segmentation succeeds given the minimal amount of exposure</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Trueswell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zevin</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Mintz</surname>, <given-names>T. H.</given-names></string-name></person-group> (under review). <source>Repetition induced rhythm as an alternative account to statistical word segmentation: A model and meta-analysis</source>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="ap1">
<title>Appendix</title>
<p>Before we present our simulations in the Appendix, here is a summary of the simulations below. In <xref ref-type="sec" rid="s5">Part 1</xref>, we simulated with the computational model described in the paper. The purpose of <xref ref-type="sec" rid="s5.1">Simulation 1</xref> is to show the robustness of the results given a range of parameters, and that the model behavior is the same regardless of the parameter values. Next, we ask a bigger question, which is why the effect sizes in the mixed condition is larger than in the uniform condition. We test the hypothesis that, the effect sizes were a function of mean word length. By manipulating 5 different possible types of sequences, the purpose of <xref ref-type="sec" rid="s5.2">Simulation 2</xref> is to show that given the same structure and parameters of the model, the effect size of the facilitation effect is correlated with the mean word length.</p>
<p>In <xref ref-type="sec" rid="s6">Part 2</xref>, we present a different set of simulations with PARSER. In <xref ref-type="sec" rid="s6.1">Simulation 1</xref>, we want to track the relationship between the amount of exposure and learning. By giving PARSER a small amount of data, the model can show us how much data was needed to get PARSER started with segmentation. This simulation shows that, given a single exposure, there is no learning from PARSER; in fact, with two exposures to a word form, there is still no learning. In <xref ref-type="sec" rid="s6.2">Simulation 2</xref>, we simulate the <xref ref-type="bibr" rid="c22">Johnson and Tyler (2010)</xref> experiment, asking whether PARSER would be sensitive to the type of word length (uniform/mixed) during segmentation. We found that PARSER can segment both conditions equally well, and this is different from findings from humans.</p>
<p>In <xref ref-type="sec" rid="s7">Part 3</xref>, we present an additional analysis of the difference between the uniform and mixed conditions. In this analysis, we used the data from the conceptual replication. The results showed the same pattern as the analysis in the main text.</p>
<sec id="s5">
<title>Part 1. Simulations with the present computational model</title>
<sec id="s5.1">
<title>Simulation 1</title>
<p>In this first simulation, we test a range of parameter values and show that the simulation is robust to the choice of the parameters. As we said in the paper, the parameters include the baseline RT (RT0), stream_inc, the increment amount for stream position, and occ_inc, the increment for the number of targets that already occurred. The first two parameters do not factor into the behavior of the model (i.e., anything that leads to the facilitation effect), as the model is mostly concerned with the facilitation effect, which is a result of the interaction between syllable position and presentation. Neither the baseline RT nor the increment for stream position would influence this interaction. For the simulations reported in the paper and the simulations in the rest of this Appendix, we set the baseline RT value (RT0) to 500ms, and the increment amount for stream position to 0.72ms, all positive numbers. Note that, we could have drawn these values as a random number from a distribution, but again, such choices would not influence the interaction of interest. A priori, for the purpose of this simulation, we only considered the third parameter, the increment for the number of targets that already occurred, to hold any potential to influence the interaction.</p>
<p>From the outset, we knew this increment to be a negative number, because predictable syllables were reacted to faster than unpredictable syllables. To see how this parameter influenced the interaction between syllable position and presentation, we wanted to manipulate this parameter within a range. On the larger side of the range, the value can be a negative number close to 0. For our purpose, -0.1ms was a number close to 0 that is still meaningful for RT values. On the smaller side of the range, we did the following calculation. If this increment was a large number with a negative sign (say -1000), it would mean that the RT in presentation 4 would be smaller than 0 (i.e., RT0 plus 3 times this negative number), which was impossible. Thus, by setting predictable RTs in presentation 4 to be a positive number near 0, we calculated that the increment number was close to -70ms. Thus, -70ms was used as the larger end of the range.</p>
<p>Thus, we took 5 different values ranging from -0.1ms to -70ms, and ran the model to generate the simulated data. For data from each simulation, we conducted the same regression, and looked at the plots of the regression predictions. The regression estimates for shown in <xref ref-type="table" rid="tabA1">Table A1</xref>. We saw that for all of the different instances of occ_inc, the interaction between syllable position and presentation went the same direction, where the slope for presentation 1 was flat, and became more negative as the presentation number increased. The slope for presentation 4 was the most negative in all instances. Thus, we concluded that the parameter values, with a range where the values were reasonable, did not qualitatively change the results of the simulations reported in the paper.</p>
<table-wrap id="tabA1">
<label>Table A1</label>
<caption><title>The beta estimates for regressions for different simulations. On the top, we show the different occ_inc values we used in each simulation. On the left, we list the different pairs of presentation that interact with syllable position, and in the table, the beta coefficient for these interactions are shown.</title></caption>
<alternatives>
<graphic xlink:href="tb26vv1_tabA1.tif" mimetype="image" mime-subtype="tiff"/>
<table frame="hsides" rules="all">
<tbody>
<tr>
<td align="left">pairs of presentation</td>
<td align="left">occ_inc values</td>
<td align="center">-0.1</td>
<td align="center">-17.6</td>
<td align="center">-35.1</td>
<td align="center">-52.6</td>
<td align="center">-70</td>
</tr>
<tr>
<td align="left" colspan="2">Presentation 1 and 2</td>
<td align="center">-.046</td>
<td align="center">-8.352</td>
<td align="center">-16.362</td>
<td align="center">-24.020</td>
<td align="center">-32.663</td>
</tr>
<tr>
<td align="left" colspan="2">Presentation 1 and 3</td>
<td align="center">-.137</td>
<td align="center">-23.872</td>
<td align="center">-48.712</td>
<td align="center">-72.215</td>
<td align="center">-97.366</td>
</tr>
<tr>
<td align="left" colspan="2">Presentation 1 and 4</td>
<td align="center">-.285</td>
<td align="center">-50.091</td>
<td align="center">-99.800</td>
<td align="center">-147.916</td>
<td align="center">-198.262</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="s5.2">
<title>Simulation 2</title>
<p>In this simulation, we test 5 different ways of constructing a sequence in the target detection paradigm, and processed the data in the same way to explain why the mixed condition generated a larger effect than in the uniform condition. Our hypothesis is that the shorter the component words are (in terms of the number of syllables), the more negative the slope of the later presentations. To test this hypothesis, we created 5 different ways of constructing a sequence, which are listed in <xref ref-type="table" rid="tabA2">Table A2</xref>.</p>
<table-wrap id="tabA2">
<label>Table A2</label>
<caption><title>The content of the present simulation.</title></caption>
<alternatives>
<graphic xlink:href="tb26vv1_tabA2.tif" mimetype="image" mime-subtype="tiff"/>
<table frame="hsides" rules="all">
<thead>
<tr>
<th align="left">mean length</th>
<th align="left">content (words by number)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">2</td>
<td align="left">4 disyllabic words</td>
</tr>
<tr>
<td align="left">2.5</td>
<td align="left">2 disyllabic words, 2 trisyllabic words</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">4 trisyllabic words</td>
</tr>
<tr>
<td align="left">3.5</td>
<td align="left">2 trisyllabic words, 2 quadruple-syllabic words</td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">4 quadruple-syllabic words</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>As is shown in <xref ref-type="table" rid="tabA2">Table A2</xref>, we created 5 different ways of constructing a sequence, manipulating the mean lengths of words. All these conditions included 4 different words in them with unique syllables. The mean lengths of 2.5 and 3 are the same as the mixed and uniform conditions, respectively.</p>
<p>After we generated the RT values from the model, we conducted the same set of regressions for each model. The RT was the dependent variable, and the independent variable included fixed effects of word presentation (1-4, categorical), and position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous), and the interaction between the two. Random effects included participant as a random intercept and stream position as a random slope. Of particular interest are the beta estimates for the two-way interactions, specifically, the estimates between presentations 1 and 2, presentations 1 and 3, and presentations 1 and 4. The prediction is that, the smaller the mean length of words is, the larger the changes are for pairs of presentations, and the largest for presentations 1 and 4.</p>
<p>The results from the regressions are shown in <xref ref-type="table" rid="tabA3">Table A3</xref>.</p>
<table-wrap id="tabA3">
<label>Table A3</label>
<caption><title>The beta estimates for regressions for different simulations.</title></caption>
<alternatives>
<graphic xlink:href="tb26vv1_tabA3.tif" mimetype="image" mime-subtype="tiff"/>
<table frame="hsides" rules="all">
<tbody>
<tr>
<td align="left">Mean word length</td>
<td align="left">2</td>
<td align="left">2.5</td>
<td align="left">3</td>
<td align="left">3.5</td>
<td align="left">4</td>
</tr>
<tr>
<td align="left">Presentation 1 and 2</td>
<td align="left">-8.145</td>
<td align="left">-4.634</td>
<td align="left">-4.132</td>
<td align="left">-2.653</td>
<td align="left">-2.491</td>
</tr>
<tr>
<td align="left">Presentation 1 and 3</td>
<td align="left">-23.947</td>
<td align="left">-14.043</td>
<td align="left">-12.178</td>
<td align="left">-8.440</td>
<td align="left">-7.449</td>
</tr>
<tr>
<td align="left">Presentation 1 and 4</td>
<td align="left">-49.316</td>
<td align="left">-28.181</td>
<td align="left">-24.678</td>
<td align="left">-16.775</td>
<td align="left">-14.909</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>From these regression estimates, we see that our prediction is confirmed. We see that these effects grow linearly with respect to the mean length of words for the five conditions we tested. In addition, whether the syllable sequence was uniform or mixed in length did not matter, and only the mean length was predictive of the beta coefficients.</p>
</sec>
</sec>
<sec id="s6">
<title>Part 2. Simulations with PARSER</title>
<sec id="s6.1">
<title>Simulation 1</title>
<p>In this simulation, we used the U-Learn program (<xref ref-type="bibr" rid="c29">Perruchet, Robinet, &amp; Lemaire, 2014</xref>) to examine the learning of short sequences. Here, we asked PARSER to evaluate words vs. part-words, which is a function built-in to the U-Learn program. The U-Learn program reports the rate words are preferred over part-words in 10 time steps (which corresponds to 1/10 of the learning sequence, however long the learning sequence is). Thus, if we use 4 different words each of which occurs 10 times (modifying the existing <xref ref-type="bibr" rid="c1">Aslin et al. 1998</xref> “ready-to-use configurations”), 1/10 of the syllable sequence is 4 words long and 2/10 of the syllable sequence is 8 words long. We created the sequence and the test items by modifying the ready-to-se configurations (<xref ref-type="fig" rid="figA1">Figure A1</xref>).</p>
<fig id="figA1" position="float" fig-type="figure">
<label>Figure A1</label>
<caption><title>The set-up for simulation one, where four trisyllabic words occur 10 times each.</title></caption>
<graphic xlink:href="tb26vv1_figA1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Using this setup, we ran the simulation 50 times to represent running 50 subjects on this task. The results are shown in <xref ref-type="fig" rid="figA2">Figure A2</xref>. We find that, PARSER is successful after finishing running the 40-word sequence most of the time in simulation. The crucial question for the current simulation is whether there is any learning after 2/10<sup>th</sup> of the sequence. Observing both the percentage and weight changes in the learning curve, and we see that there is no learning.</p>
<fig id="figA2" position="float" fig-type="figure">
<label>Figure A2</label>
<caption><title>The results for the first simulation, where Figure 2A shows the weight changes of words and part-words over the course of learning and Figure 2B shows the percentage of words and part-words discovered over the course of learning.</title><p>Importantly, at time 2 on the x-axis, there is no learning of words. The y-axis represents the weights/percentages, but since these units are arbitrary and only meaningful when comparing two curves, the units are not displayed in the plotting function of U-Learn.</p></caption>
<graphic xlink:href="tb26vv1_figA2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s6.2">
<title>Simulation 2</title>
<p>In this simulation, we used the U-Learn program to examine the learning of uniform and mixed conditions in <xref ref-type="bibr" rid="c22">Johnson and Tyler, 2010</xref>. The uniform condition was the same as the existing <xref ref-type="bibr" rid="c1">Aslin et al. 1998</xref> configurations, and the mixed condition was modified to include two disyllabic and two trisyllabic words, with all test items being disyllabic. In both conditions, the four words had an unbalanced frequency profile (45, 45, 90, 90). The mixed condition setup configuration is shown in <xref ref-type="fig" rid="figA3">Figure A3</xref>.</p>
<fig id="figA3" position="float" fig-type="figure">
<label>Figure A3</label>
<caption><title>The set-up for the mixed condition in <xref ref-type="sec" rid="s6.2">Simulation 2</xref>.</title></caption>
<graphic xlink:href="tb26vv1_figA3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Using this set-up, we ran the simulation 50 times each in the uniform and mixed conditions to represent running 50 subjects on this task. The results are shown in <xref ref-type="fig" rid="figA4">Figure A4</xref>. We find that, PARSER is successful in both conditions. In the uniform condition, words were successfully segmented 79% of the time on average, and part-words were segmented 4% of the time on average. In the mixed condition, words were successfully segmented 82% of the time on average, and part-words were segmented 1% of the time on average. Thus, we see that PARSER is capable of learning in both the mixed and uniform conditions.</p>
<fig id="figA4" position="float" fig-type="figure">
<label>Figure A4</label>
<caption><title>The results for the second simulation, where Figures 4A and 4B show the weight and percentage changes of words and part-words over the course of learning in the uniform condition, and Figures 4C and 4D show the weight and percentage changes of words and part-words over the course of learning in the mixed condition.</title><p>On the x-axis, each number represents 1/10 of the sequence. The y-axis represents the weights/percentages, but since these units are arbitrary and only meaningful when comparing two curves, the units are not displayed in the plotting function of U-Learn.</p></caption>
<graphic xlink:href="tb26vv1_figA4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s7">
<title>Part 3. Additional analyses between the uniform and mixed conditions</title>
<p>The main text mentioned that the difference between the uniform and mixed condition is the same, whether the conceptual or exact replication data was used to compare to the mixed condition. The following section shows this analysis.</p>
<p>For this analysis, we compared the data from <xref ref-type="sec" rid="s2">Experiment 2</xref> to the conceptual-replication condition in <xref ref-type="sec" rid="s1">Experiment 1</xref>. The same set of regression setup was used. In this mixed effect regression with a three-way interaction, the RT was the dependent variable, and the independent variable included fixed effects of condition (mixed/uniform, categorical), word presentation (1-4, categorical), and position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous), and the interaction between the three. Fixed effect further included overall stream position (3rd through 46th syllable in the syllable sequence in the uniform condition, 3rd through 38th syllable in the mixed condition, both continuous) and word length (disyllabic/trisyllabic, categorical). Random effects included participant as a random intercept and stream position as a random slope. The omnibus three-way interaction was significant (χ<sup>2</sup>(3) = 16.06, p=0.001), suggesting that the ways syllable position and presentation interact in the two experiments are different. To understand this three-way interaction, we looked at the three-way interaction between syllable position, condition, and pairs of presentations (i.e., presentations 1 and 2, 1 and 3, and 1 and 4). We found that the three-way interactions for presentations 1 and 2 (β=0.001, z=0.26, p=0.797) was positive and not significant, and positive and not significant for presentations 1 and 3 (β=0.005, z=0.85, p=0.396), and positive and significant for presentation 1 and 4 (β=0.022, z=3.52, p&lt;0.001). This is the same pattern as the analyses in the main text, where the coefficients grow as a function of presentation in this three-way interaction, same as the exact-replication condition. Looking at a plot of model fit (<xref ref-type="fig" rid="figA5">Figure A5</xref>), this pattern becomes clear: while the slopes (from syllable position 1 to 3) for presentation 1 were similarly non-negative for both conditions, the negative slope for presentation 4 for the mixed condition was the largest slope (from 569ms to 493ms) for all slopes, more than in presentation 4 for the uniform condition (from 572ms to 554ms).</p>
<fig id="figA5" position="float" fig-type="figure">
<label>Figure A5</label>
<caption><title>Regression model fit from the three-way interaction between condition (mixed/uniform, categorical), word presentation (1-4, categorical), and position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous).</title><p>Figure A5A showed results from the conceptual-replication condition from <xref ref-type="sec" rid="s1">Experiment 1</xref>, and Figure A5B showed results from the mixed condition from <xref ref-type="sec" rid="s2">Experiment 2</xref>. Error bars represent ±1 SEM.</p></caption>
<graphic xlink:href="tb26vv1_figA5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95761.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>The authors present <bold>valuable</bold> empirical and modelling evidence that statistical learning in speech perception may contain processes like segmentation and anticipation. While the evidence for statistical learning effects is <bold>solid</bold>, the link between the pattern of effects (both empirical and simulated) and the theoretical concepts of segmentation and anticipation would need to be much stronger to exclude other accounts of the data. This work will be of broad interest to researchers working on, or with, statistical learning, and to any researcher interested in the challenges of whether data and modeling can effectively adjudicate between competing theoretical constructs.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95761.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper presents two experiments, both of which use a target detection paradigm to investigate the speed of statistical learning. The first experiment is a replication of Batterink, 2017, in which participants are presented with streams of uniform-length, trisyllabic nonsense words and asked to detect a target syllable. The results replicate previous findings, showing that learning (in the form of response time facilitation to later-occurring syllables within a nonsense word) occurs after a single exposure to a word. In the second experiment, participants are presented with streams of variable-length nonsense words (two trisyllabic words and two disyllabic words) and perform the same task. A similar facilitation effect was observed as in Experiment 1. The authors interpret these findings as evidence that target detection requires mechanisms different from segmentation. They present results of a computational model to simulate results from the target detection task and find that an &quot;anticipation mechanism&quot; can produce facilitation effects, without performing segmentation. The authors conclude that the mechanisms involved in the target detection task are different from those involved in the word segmentation task.</p>
<p>Strengths:</p>
<p>The paper presents multiple experiments that provide internal replication of a key experimental finding, in which response times are facilitated after a single exposure to an embedded pseudoword. Both experimental data and results from a computational model are presented, providing converging approaches for understanding and interpreting the main results. The data are analyzed very thoroughly using mixed effects models with multiple explanatory factors.</p>
<p>Weaknesses:</p>
<p>In my view, the main weaknesses of this study relate to the theoretical interpretation of the results.</p>
<p>(1) The key conclusion from these findings is that the facilitation effect observed in the target detection paradigm is driven by a different mechanism (or mechanisms) than those involved in word segmentation. The argument here I think is somewhat unclear and weak, for several reasons:</p>
<p>First, there appears to be some blurring in what exactly is meant by the term &quot;segmentation&quot; with some confusion between segmentation as a concept and segmentation as a paradigm.</p>
<p>
Conceptually, segmentation refers to the segmenting of continuous speech into words. However, this conceptual understanding of segmentation (as a theoretical mechanism) is not necessarily what is directly measured by &quot;traditional&quot; studies of statistical learning, which typically (at least in adults) involve exposure to a continuous speech stream followed by a forced-choice recognition task of words versus recombined foil items (part-words or nonwords). To take the example provided by the authors, a participant presented with the sequence GHIABCDEFABCGHI may endorse ABC as being more familiar than BCG, because ABC is presented more frequently together and the learned association between A and B is stronger than between C and G. However, endorsement of ABC over BCG does not necessarily mean that the participant has &quot;segmented&quot; ABC from the speech stream, just as faster reaction times in responding to syllable C versus A do not necessarily indicate successful segmentation. As the authors argue on page 7, &quot;an encounter to a sequence in which two elements co-occur (say, AB) would theoretically allow the learner to use the predictive relationship during a subsequent encounter (that A predicts B).&quot; By the same logic, encoding the relationship between A and B could also allow for the above-chance endorsement of items that contain AB over items containing a weaker relationship.</p>
<p>Both recognition performance and facilitation through target detection reflect different outcomes of statistical learning. While they may reflect different aspects of the learning process and/or dissociable forms of memory, they may best be viewed as measures of statistical learning, rather than mechanisms in and of themselves.</p>
<p>(2) The key manipulation between experiments 1 and 2 is the length of the words in the syllable sequences, with words either constant in length (experiment 1) or mixed in length (experiment 2). The authors show that similar facilitation levels are observed across this manipulation in the current experiments. By contrast, they argue that previous findings have found that performance is impaired for mixed-length conditions compared to fixed-length conditions. Thus, a central aspect of the theoretical interpretation of the results rests on prior evidence suggesting that statistical learning is impaired in mixed-length conditions. However, it is not clear how strong this prior evidence is. There is only one published paper cited by the authors - the paper by Hoch and colleagues - that supports this conclusion in adults (other mentioned studies are all in infants, which use very different measures of learning). Other papers not cited by the authors do suggest that statistical learning can occur to stimuli of mixed lengths (Thiessen et al., 2005, using infant-directed speech; Frank et al., 2010 in adults). I think this theoretical argument would be much stronger if the dissociation between recognition and facilitation through RTs as a function of word length variability was demonstrated within the same experiment and ideally within the same group of participants.</p>
<p>(3) The authors argue for an &quot;anticipation&quot; mechanism in explaining the facilitation effect observed in the experiments. The term anticipation would generally be understood to imply some kind of active prediction process, related to generating the representation of an upcoming stimulus prior to its occurrence. However, the computational model proposed by the authors (page 24) does not encode anything related to anticipation per se. While it demonstrates facilitation based on prior occurrences of a stimulus, that facilitation does not necessarily depend on active anticipation of the stimulus. It is not clear that it is necessary to invoke the concept of anticipation to explain the results, or indeed that there is any evidence in the current study for anticipation, as opposed to just general facilitation due to associative learning.</p>
<p>In addition, related to the model, given that only bigrams are stored in the model, could the authors clarify how the model is able to account for the additional facilitation at the 3rd position of a trigram compared to the 2nd position?</p>
<p>(4) In the discussion of transitional probabilities (page 31), the authors suggest that &quot;a single exposure does provide information about the transitions within the single exposure, and the probability of B given A can indeed be calculated from a single occurrence of AB.&quot; Although this may be technically true in that a calculation for a single exposure is possible from this formula, it is not consistent with the conceptual framework for calculating transitional probabilities, as first introduced by Saffran and colleagues. For example, Saffran et al. (1996, Science) describe that &quot;over a corpus of speech there are measurable statistical regularities that distinguish recurring sound sequences that comprise words from the more accidental sound sequences that occur across word boundaries. Within a language, the transitional probability from one sound to the next will generally be highest when the two sounds follow one another within a word, whereas transitional probabilities spanning a word boundary will be relatively low.&quot; This makes it clear that the computation of transitional probabilities (i.e., Y | X) is conceptualized to reflect the frequency of XY / frequency of X, over a given language inventory, not just a single pair. Phrased another way, a single exposure to pair AB would not provide a reliable estimate of the raw frequencies with which A and AB occur across a given sample of language.</p>
<p>(5) In experiment 2, the authors argue that there is robust facilitation for trisyllabic and disyllabic words alike. I am not sure about the strength of the evidence for this claim, as it appears that there are some conflicting results relevant to this conclusion. Notably, in the regression model for disyllabic words, the omnibus interaction between word presentation and syllable position did not reach significance (p= 0.089). At face value, this result indicates that there was no significant facilitation for disyllabic words. The additional pairwise comparisons are thus not justified given the lack of omnibus interaction. The finding that there is no significant interaction between word presentation, word position, and word length is taken to support the idea that there is no difference between the two types of words, but could also be due to a lack of power, especially given the p-value (p = 0.010).</p>
<p>(6) The results plotted in Figure 2 seem to suggest that RTs to the first syllable of a trisyllabic item slow down with additional word presentations, while RTs to the final position speed up. If anything, in this figure, the magnitude of the effect seems to be greater for 1st syllable positions (e.g., the RT difference between presentation 1 and 4 for syllable position 1 seems to be numerically larger than for syllable position 3, Figure 2D). Thus, it was quite surprising to see in the results (p. 16) that RTs for syllable position 1 were not significantly different for presentation 1 vs. the later presentations (but that they were significant for positions 2 and 3 given the same comparison). Is this possibly a power issue? Would there be a significant slowdown to 1st syllables if results from both the exact replication and conceptual replication conditions were combined in the same analysis?</p>
<p>(7) It is difficult to evaluate the description of the PARSER simulation on page 36. Perhaps this simulation should be introduced earlier in the methods and results rather than in the discussion only.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95761.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This valuable study investigates how statistical learning may facilitate a target detection task and whether the facilitation effect is related to statistical learning of word boundaries. Solid evidence is provided that target detection and word segmentation rely on different statistical learning mechanisms.</p>
<p>Strengths:</p>
<p>The study is well designed, using the contrast between the learning of words of uniform length and words of variable length to dissociate general statistical learning effects and effects related to word segmentation.</p>
<p>Weaknesses:</p>
<p>The study relies on the contrast between word length effects on target detection and word learning. However, the study only tested the target detection condition and did not attempt to replicate the word segmentation effect. It is true that the word segmentation effect has been replicated before but it is still worth reviewing the effect size of previous studies.</p>
<p>The paper seems to distinguish prediction, anticipation, and statistical learning, but it is not entirely clear what each term refers to.</p>
</body>
</sub-article>
</article>