<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">87881</article-id>
<article-id pub-id-type="doi">10.7554/eLife.87881</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.87881.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Revealing unexpected complex encoding but simple decoding mechanisms in motor cortex via separating behaviorally relevant neural signals</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7271-2993</contrib-id>
<name>
<surname>Li</surname>
<given-names>Yangang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhu</surname>
<given-names>Xinyun</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Qi</surname>
<given-names>Yu</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Wang</surname>
<given-names>Yueming</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Qiushi Academy for Advanced Studies, Zhejiang University</institution>, Hangzhou, <country>China</country></aff>
<aff id="a2"><label>2</label><institution>College of Computer Science and Technology, Zhejiang University</institution>, Hangzhou, <country>China</country></aff>
<aff id="a3"><label>3</label><institution>Affiliated Mental Health Center &amp; Hangzhou Seventh People’s Hospital and the MOE Frontier Science Center for Brain Science and Brain-machine Integration, Zhejiang University School of Medicine</institution>, Hangzhou, <country>China</country></aff>
<aff id="a4"><label>4</label>The State Key Lab of Brain-Machine Intelligence, <institution>Zhejiang University</institution>, Hangzhou, <country>China</country></aff>
<aff id="a5"><label>5</label><institution>Zhejiang Brain-Computer Interface Institute</institution>, Hangzhou, <country>China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Gallego</surname>
<given-names>Juan Alvaro</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Imperial College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Makin</surname>
<given-names>Tamar R</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>For correspondence:</bold> <email>qiyu@zju.edu.cn</email> (YQ); <email>ymingwang@zju.edu.cn</email> (YW)</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-07-14">
<day>14</day>
<month>07</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP87881</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-05-08">
<day>08</day>
<month>05</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-05-09">
<day>09</day>
<month>05</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.11.13.515644"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Li et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Li et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-87881-v1.pdf"/>
<abstract>
<title>Abstract</title><p>In motor cortex, behaviorally-relevant neural responses are entangled with irrelevant signals, which complicates the study of encoding and decoding mechanisms. It remains unclear whether behaviorally-irrelevant signals could conceal some critical truth. One solution is to accurately separate behaviorally-relevant and irrelevant signals, but this approach remains elusive due to the unknown ground truth of behaviorally-relevant signals. Therefore, we propose a framework to define, extract, and validate behaviorally-relevant signals. Analyzing separated signals in three monkeys performing different reaching tasks, we found neural responses previously considered useless encode rich behavioral information in complex nonlinear ways. These responses are critical for neuronal redundancy and reveal movement behaviors occupy a higher-dimensional neural space than previously expected. Surprisingly, when incorporating often-ignored neural dimensions, behavioral information can be decoded linearly as accurately as nonlinear decoding, suggesting linear readout is performed in motor cortex. Our findings prompt that separating behaviorally-relevant signals may help uncover more hidden cortical mechanisms.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Added a synthetic benchmark and compared d-VAE with CEBRA</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Understanding how motor cortex encodes and decodes movement behaviors is a fundamental goal of neuroscience (<bold><italic><xref rid="c26" ref-type="bibr">Kriegeskorte and Douglas, 2019</xref></italic></bold>; <bold><italic><xref rid="c40" ref-type="bibr">Saxena and Cunningham, 2019</xref></italic></bold>). Here, we define behaviors as behavioral variables of interest measured within a given task, such as arm kine-matics during a motor control task; we employ terms like ‘behaviorally-relevant’ and ‘behaviorally-irrelevant’ only regarding such measured behavioral variables. However, achieving this goal faces significant challenges because behaviorally-relevant neural responses are entangled with behaviorally-irrelevant factors such as responses for other variables of no interest (<bold><italic><xref rid="c12" ref-type="bibr">Fusi et al., 2016</xref></italic></bold>; <bold><italic><xref rid="c36" ref-type="bibr">Rigotti et al., 2013</xref></italic></bold>) and ongoing noise (<bold><italic><xref rid="c3" ref-type="bibr">Azouz and Gray, 1999</xref></italic></bold>; <bold><italic><xref rid="c11" ref-type="bibr">Faisal et al., 2008</xref></italic></bold>). Generally, irrelevant signals would hinder the accurate investigation of the relationship between neural activity and movement behaviors. This raises concerns about whether irrelevant signals could conceal some critical facts about neural encoding and decoding mechanisms.</p>
<p>If the answer is yes, a natural question arises: what critical facts about neural encoding and de-coding would irrelevant signals conceal? In terms of neural encoding, irrelevant signals may mask some small neural components, making their encoded information dificult to detect (<bold><italic><xref rid="c28" ref-type="bibr">Moreno-Bote et al., 2014</xref></italic></bold>), thereby misleading us to neglect the role of these signals, leading to a partial under-standing of neural mechanisms. For example, at the single-neuron level, weakly-tuned neurons are often assumed useless and not analyzed (<bold><italic><xref rid="c16" ref-type="bibr">Georgopoulos et al., 1986</xref></italic></bold>; <bold><italic><xref rid="c19" ref-type="bibr">Hochberg et al., 2012</xref></italic></bold>; <bold><italic><xref rid="c47" ref-type="bibr">Wodlinger et al., 2014</xref></italic></bold>; <bold><italic><xref rid="c20" ref-type="bibr">Inoue et al., 2018</xref></italic></bold>); at the population level, neural signals composed of lower variance principal components (PCs) are typically treated as noise and discarded (<bold><italic><xref rid="c5" ref-type="bibr">Churchland et al., 2012</xref></italic></bold>; <bold><italic><xref rid="c15" ref-type="bibr">Gallego et al., 2018</xref></italic></bold>, <bold><italic>2020</italic></bold>; <bold><italic><xref rid="c7" ref-type="bibr">Cunningham and Yu, 2014</xref></italic></bold>). So are these ignored signals truly use-less or appear that way only due to being covered by irrelevant signals? And what’s the role of these ignored signals? In terms of neural decoding, irrelevant signals would significantly complicate the information readout (<bold><italic><xref rid="c35" ref-type="bibr">Pitkow et al., 2015</xref></italic></bold>; <bold><italic><xref rid="c49" ref-type="bibr">Yang et al., 2021</xref></italic></bold>), potentially hindering the discovery of the true readout mechanism of behaviorally-relevant responses. Specifically, in motor cortex, in what form (linear or nonlinear) downstream neurons read out behavioral information from neural responses is an open question. The linear readout is biologically plausible and widely used (<bold><italic>Geor-gopoulos et al., 1986</italic></bold>; <bold><italic><xref rid="c19" ref-type="bibr">Hochberg et al., 2012</xref></italic></bold>; <bold><italic><xref rid="c47" ref-type="bibr">Wodlinger et al., 2014</xref></italic></bold>), but recent studies (<bold><italic><xref rid="c17" ref-type="bibr">Glaser et al., 2020</xref></italic></bold>; <bold><italic><xref rid="c46" ref-type="bibr">Willsey et al., 2022</xref></italic></bold>) demonstrate nonlinear readout outperforms linear readout. So which readout scheme is more likely to be used by the motor cortex? Whether irrelevant signals are the culprits for the performance gap? Unfortunately, all the above issues remain unclear.</p>
<p>One approach to address the above issues is to accurately separate behaviorally-relevant and irrelevant signals at the single-neuron level and then analyze noise-free behaviorally-relevant sig-nals, which enables us to gain a more accurate and comprehensive understanding of the under-lying neural mechanisms. However, this approach is hampered by the fact that the ground truth of behaviorally-relevant signals is unknown, which makes the definition, extraction, and valida-tion of behaviorally-relevant signals a challenging task. As a result, methods of accurate sep-aration remain elusive to date. Existing methods for extracting behaviorally-relevant patterns mainly focus on the latent population level (<bold><italic><xref rid="c5" ref-type="bibr">Churchland et al., 2012</xref></italic></bold>; <bold><italic><xref rid="c39" ref-type="bibr">Sani et al., 2021</xref></italic></bold>; <bold><italic><xref rid="c34" ref-type="bibr">Pandarinath et al., 2018</xref></italic></bold>; <bold><italic><xref rid="c51" ref-type="bibr">Yu et al., 2008</xref></italic></bold>; <bold><italic><xref rid="c53" ref-type="bibr">Zhou and Wei, 2020</xref></italic></bold>; <bold><italic><xref rid="c22" ref-type="bibr">Keshtkaran et al., 2022</xref></italic></bold>) rather than the single-neuron level, and they extract neural activities based on assumptions about specific neural prop-erties, such as linear or nonlinear dynamics (<bold><italic><xref rid="c39" ref-type="bibr">Sani et al., 2021</xref></italic></bold>; <bold><italic><xref rid="c34" ref-type="bibr">Pandarinath et al., 2018</xref></italic></bold>). Although these methods have shown promising results, they fail to capture other parts of behaviorally-relevant neural activity that do not meet their assumptions, thereby providing an incomplete pic-ture of behaviorally-relevant neural activity. To overcome these limitations and obtain accurate behaviorally-relevant signals at the single-neuron level, we propose a novel framework that de-fines, extracts, and validates behaviorally-relevant signals by simultaneously considering such sig-nals’ encoding (behaviorally-relevant signals should be similar to raw signals to preserve the under-lying neuronal properties) and decoding (behaviorally-relevant signals should contain behavioral information as much as possible) properties (see Methods and <xref rid="fig1" ref-type="fig">Fig. 1</xref>). This framework establishes a prerequisite foundation for the subsequent detailed analysis of neural mechanisms.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Semantic illustration of extracting and validating behaviorally-relevant signals.</title>
<p><bold>a-e</bold> The ideal decomposition of raw signals. <bold>a</bold>, The temporal neuronal activity of raw signals, where x-axis denotes time, and y-axis represents firing rate. Raw signals are decomposed to relevant <bold>(b)</bold> and irrelevant <bold>(d)</bold> signals. The red dotted line indicates the decoding performance of raw signals. The red and blue bars represent the decoding performance of relevant and irrelevant signals. The purple bar represents the generating performance of relevant signals, which measures the neural similarity between generated signals and raw signals. The longer the bar, the larger the performance. The ground truth of relevant signals decode information perfectly (<bold>c</bold>, red bar) and is similar to raw signals to some extent (<bold>c</bold>, purple bar), and the ground truth of irrelevant signals contain little behavioral information (<bold>e</bold>, blue bar). <bold>f-h</bold>, Three different cases of behaviorally-relevant signals distillation. <bold>f</bold>, When the model is biased toward generating relevant signals that are similar to raw signals, it will achieve high generating performance, but the decoding performance will suffer due to the inclusion of too many irrelevant signals. As it is dificult for models to extract complete relevant signals, the residuals will also contain some behavioral information. <bold>g</bold>, When the model is biased toward generating signals that prioritize decoding over similarity to raw signals, it will achieve high decoding performance, but the generating performance will be low. Meanwhile, the residuals will contain a significant amount of behavioral information. <bold>h</bold>, When the model balances the trade-off of decoding and generating capabilities of relevant signals, both decoding and generating performance will be good, and the residuals will only contain a little behavioral information.</p></caption>
<graphic xlink:href="515644v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Here, we conducted experiments using datasets recorded from the motor cortex of three mon-keys performing different reaching tasks, where the behavioral variable is movement kinemat-ics. After signal separation by our approach, we first explored how the presence of behaviorally-irrelevant signals affect the analysis of neural activity. We found that behaviorally-irrelevant signals account for a large amount of trial-to-trial neuronal variability, and are evenly distributed across the neural dimensions of behaviorally-relevant signals. Then we explored whether irrelevant sig-nals conceal some facts of neural encoding and decoding. For neural encoding, irrelevant signals obscure the behavioral information encoded by neural responses, especially for neural responses with a large degree of nonlinearity. Surprisingly, neural responses that are usually ignored (weakly-tuned neurons and neural signals composed of small variance PCs) actually encode rich behav-ioral information in complex nonlinear ways. These responses underpin an unprecedented neu-ronal redundancy and reveal that movement behaviors are distributed in a higher-dimensional neural space than previously thought. In addition, we found that the integration of smaller and larger variance PCs results in a synergistic effect, allowing the smaller variance PC signals that can-not be linearly decoded to significantly enhance the linear decoding performance, particularly for finer speed control. This finding suggests that lower variance PC signals are involved in regulat-ing precise motor control. For neural decoding, irrelevant signals complicate information readout. Strikingly, when uncovering small neural components obscured by irrelevant signals, linear de-coders can achieve comparable decoding performance with nonlinear decoders, providing strong evidence for the presence of linear readout in motor cortex. Together, our findings reveal un-expected complex encoding but simple decoding mechanisms in the motor cortex. Finally, our study also has implications for developing accurate and robust brain-machine interfaces (BMIs) and, more generally, provides a powerful framework for separating behaviorally-relevant and ir-relevant signals, which can be applied to other cortical data to uncover more neural mechanisms masked by behaviorally-irrelevant signals.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Framework for defining, extracting, and validating behaviorally-relevant neural signals</title>
<sec id="s55">
<title>What are behaviorally-relevant neural signals?</title>
<p>Since the ground truth of behaviorally-relevant signals is unknown, the definition of behaviorally-relevant signals is not well established yet. In this study, we propose that behaviorally-relevant sig-nals should meet two requirements: (1) they should be similar to raw signals in order to preserve the underlying neuronal properties (encoding requirement), and (2) they should contain behav-ioral information as much as possible (decoding requirement). If the signals meet both of these requirements, we consider them to be effective behaviorally-relevant signals.</p>
</sec>
<sec id="s2b">
<title>How to extract behaviorally-relevant signals?</title>
<p>One way to extract behaviorally-relevant signals is to use a distillation model to generate them from raw signals, while considering the remaining signals as behaviorally-irrelevant. However, due to the unknown ground truth of behaviorally-relevant signals, a key challenge for the model is to determine the optimal degree of similarity between the generated signals and raw signals. If the generated signals are too similar to raw signals, they may contain a large amount of irrelevant information, which would hinder the exploration of neural mechanisms. Conversely, if the gener-ated signals are too dissimilar to raw signals, they may lose behaviorally-relevant information, also hindering the exploration of neural mechanisms. To overcome this challenge, we exploited the trade-off between the similarity of generated signals to raw signals (encoding requirement) and their decoding performance of behaviors (decoding requirement) to extract effective behaviorally-relevant signals (for details, see Methods). The core assumption of our model is that behaviorally-irrelevant signals are noise relative to behaviorally-relevant signals, and thereby irrelevant signals would degrade the decoding generalization of generated behaviorally-relevant signals. Generally, the distillation model is faced with three cases: a bias toward generating (<xref rid="fig1" ref-type="fig">Fig. 1f</xref>), a bias toward decoding (<xref rid="fig1" ref-type="fig">Fig. 1g</xref>), and a proper trade-off (<xref rid="fig1" ref-type="fig">Fig. 1h</xref>). If the distillation model is biased toward extract-ing signals similar to raw signals, the distilled behaviorally-relevant signals will contain an excessive amount of behaviorally-irrelevant information, affecting the decoding generalization of these sig-nals (<xref rid="fig1" ref-type="fig">Fig. 1f</xref>). If the model is biased toward extracting parsimonious signals that are discriminative for decoding, the distilled signals will not be similar enough to raw signals, and some redundant but useful signals will be left in the residuals (<xref rid="fig1" ref-type="fig">Fig. 1g</xref>). Neither of these two cases is desirable because the former loses decoding performance, while the latter loses some useful neural signals, which are not conducive to our subsequent analysis of the relationship between behaviorally-relevant sig-nals and behaviors. The behaviorally-relevant signals we want should be similar to raw signals and preserve the behavioral information maximally, which can be obtained by balancing the encoding and decoding properties of generated behaviorally-relevant signals (<xref rid="fig1" ref-type="fig">Fig. 1h</xref>).</p>
</sec>
<sec id="s2c">
<title>How to validate behaviorally-relevant signals?</title>
<p>To validate the effectiveness of the distilled signals, we proposed three criteria. The first criterion is that the decoding performance of the behaviorally-relevant signals (red bar, <xref rid="fig1" ref-type="fig">Fig.1</xref>) should sur-pass that of raw signals (the red dotted line, <xref rid="fig1" ref-type="fig">Fig.1</xref>). Since decoding models, such as deep neural networks, are more prone to overfit noisy raw signals than behaviorally-relevant signals, the dis-tilled signals should demonstrate better decoding generalization than the raw signals. The second criterion is that the behaviorally-irrelevant signals should contain minimal behavioral information (blue bar, <xref rid="fig1" ref-type="fig">Fig.1</xref>). This criterion can assess whether the distilled signals maximally preserve behav-ioral information from the opposite perspective and effectively exclude undesirable cases, such as over-generated and under-generated signals. Specifically, in the case of over-generation, suppose <italic>z</italic> = <italic>x</italic> + <italic>y</italic>, where <italic>z</italic>, <italic>x</italic>, and <italic>y</italic> represent raw, relevant, and irrelevant signals, respectively. If the dis-tilled relevant signals <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are added extra signals <italic>m</italic> which do not exist in the real behaviorally-relevant signals, that is, <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, then the corresponding residuals <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline3.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> will be equal to the ideal irrelevant sig-nals <italic>y</italic> plus the negative extra signals −<italic>m</italic>, namely, <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline4.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, thus the residuals <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline5.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> contain the amount of information preserved by negative extra signals −<italic>m</italic>. Similarly, in the case of under-generation, if the distilled behaviorally-relevant signals are incomplete and lose some useful information, this lost information will also be reflected in the residuals. In these cases, the distilled signals are not suitable for analysis. The third criterion is that the distilled behaviorally-relevant signals should be similar to raw signals to maintain essential neuronal properties (purple bar, <xref rid="fig1" ref-type="fig">Fig.1</xref>). If the distilled signals do not resemble raw signals, they will not retain the critical characteristics of raw signals, which are not qualified for subsequent analysis. Overall, if the distilled signals satisfy the above three criteria, we consider the distilled signals to be effective.</p>
</sec>
</sec>
<sec id="s2d">
<title>d-VAE extracts effective behaviorally-relevant signals</title>
<p>To demonstrate the effectiveness of our model (d-VAE) in extracting behaviorally-relevant signals, we conducted experiments on the synthetic dataset where the ground truth of relevant and ir-relevant signals are already known (see Methods) and three benchmark datasets with different paradigms (<xref rid="fig2" ref-type="fig">Fig. 2a,e</xref>,i; see Methods for details), and compared d-VAE with four other distillation models, including pi-VAE (<bold><italic><xref rid="c53" ref-type="bibr">Zhou and Wei, 2020</xref></italic></bold>), PSID (<bold><italic><xref rid="c39" ref-type="bibr">Sani et al., 2021</xref></italic></bold>) and LFADS (<bold><italic><xref rid="c34" ref-type="bibr">Pandarinath et al., 2018</xref></italic></bold>), and a vanilla VAE (<bold><italic><xref rid="c24" ref-type="bibr">Kingma and Welling, 2013</xref></italic></bold>). Specifically, we first applied these distillation models to raw signals to obtain the distilled behaviorally-relevant signals, consider-ing the residuals as behaviorally-irrelevant signals. We then evaluated the decoding R<sup>2</sup> between the predicted velocity and actual velocity of the two partition signals using a linear Kalman fil-ter (KF) and a nonlinear artificial neural network (ANN) and measured the neural similarity be-tween behaviorally-relevant and raw signals. Besides, we compared the decoding performance of behaviorally-relevant signals extracted by d-VAE and behaviorally-relevant embeddings extracted by CEBRA (a non-generative method which outperforms pi-VAE) (<bold><italic><xref rid="c42" ref-type="bibr">Schneider et al., 2022</xref></italic></bold>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Evaluation of separated signals.</title>
<p><bold>a</bold>, The obstacle avoidance paradigm <bold>b</bold>, The decoding R<sup>2</sup> between true velocity and predicted velocity of raw signals (purple bars with slash lines) and behaviorally-relevant signals obtained by d-VAE (red), PSID (pink), pi-VAE (green), LFADS (blue), and VAE (light green) on dataset A. Error bars denote mean ± standard deviation (s.d.) across five cross-validation folds. Asterisks represent significance of Wilcoxon rank-sum test with <italic>P &lt;</italic> 0.05, <italic>P &lt;</italic> 0.01. <bold>c</bold>, Same as <bold>b</bold>, but for behaviorally-irrelevant signals obtained by five different methods. <bold>d</bold>, The neural similarity (R<sub>2</sub>) between raw signals and behaviorally-relevant signals extracted by d-VAE, PSID, pi-VAE, LFADS, and VAE. Error bars represent mean ± s.d. across five cross-validation folds. Asterisks indicate significance of Wilcoxon rank-sum test with <italic>P &lt;</italic> 0.01. <bold>e-h</bold> and <bold>i-l</bold>, Same as <bold>a-d</bold>, but for dataset B with the center-out paradigm (<bold>e</bold>) and dataset C with the self-paced reaching paradigm (<bold>i</bold>). <bold>m</bold>, The firing rates of raw signals and distilled signals obtained by d-VAE in five held-out trials under the same condition of dataset B.</p></caption>
<graphic xlink:href="515644v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Overall, d-VAE successfully extracts effective behaviorally-relevant signals that meet the three criteria outlined above on both synthetic and real data. On the synthetic data (<xref rid="figs1" ref-type="fig">Supplementary Fig. S1</xref>), results show that d-VAE can indeed utilize a proper trade-off between generating and decod-ing to extract effective neural signals that are similar to the ground truth signals (<xref rid="figs1" ref-type="fig">Supplementary Fig. S1 a-g</xref>) and outperforms other distillation models (<xref rid="figs1" ref-type="fig">Supplementary Fig. S1 h-k</xref>). In the obstacle avoidance task (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>), the monkey is required to move the ball from the start point (yellow) to the target point (blue) without hitting the obstacle. For the decoding performance of behaviorally-relevant signals (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>), the signals distilled by d-VAE outperform the raw signals (purple bars with slash lines) and the signals distilled by all other distillation models (PSID, pink; pi-VAE, green; LFADS, blue; and VAE, light green) with the KF as well as the ANN. For the decoding performance of behaviorally-irrelevant signals (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>), behaviorally-irrelevant signals obtained by d-VAE achieves the lowest decoding performance compared with behaviorally-irrelevant signals obtained by other approaches. Therefore, the combination of dVAE’s highest decoding performance for behaviorally-relevant signals and lowest decoding performance for behaviorally-irrelevant signals demonstrate its superior ability to extract behaviorally-relevant signals from noisy signals. For the neural similar-ity between behaviorally-relevant and raw signals (<xref rid="fig2" ref-type="fig">Fig. 2d</xref>), the distilled signals obtained by d-VAE achieve the highest performance among competitors (<italic>P &lt;</italic> 0.01, Wilcoxon rank-sum test). Similar results were obtained for the center-out task (<xref rid="fig2" ref-type="fig">Fig. 2e-h</xref>) and the self-paced reaching task (<xref rid="fig2" ref-type="fig">Fig. 2i-l</xref>), indicating the consistency of d-VAE’s distillation ability across a range of motor tasks. To provide a more intuitive illustration of the similarity between raw and distilled signals, we displayed the firing rate of neuronal activity in five trials under the same condition (<xref rid="fig2" ref-type="fig">Fig. 2m</xref>), and results clearly show that the firing pattern of distilled signals is similar to the corresponding raw signals. Finally, we compared d-VAE with CEBRA on the four datasets; results show that d-VAE achieves compara-ble performance with CEBRA using the ANN decoder but outperforms CEBRA with the KF decoder (<xref rid="figs2" ref-type="fig">Supplementary Fig. S2</xref>), demonstrating that d-VAE can effectively extract behavioral information.</p>
<p>In summary, d-VAE distills effective behaviorally-relevant signals that preserve behavioral infor-mation maximally and are similar to raw signals. Meanwhile, the behaviorally-irrelevant signals discarded by d-VAE contain a little behavioral information. Therefore, these signals are reliable for exploring the encoding and decoding mechanisms of relevant signals.</p>
</sec>
<sec id="s2e">
<title>How do behaviorally-irrelevant signals affect the analysis of neural activity at the single-neuron level?</title>
<p>Following signal separation, we first explored how behaviorally-irrelevant signals affect the analysis of neural activity at the single-neuron level. Specifically, we examined the effect of irrelevant signals on two critical properties of neuronal activity: the preferred direction (PD) (<bold><italic><xref rid="c16" ref-type="bibr">Georgopoulos et al., 1986</xref></italic></bold>) and trial-to-trial variability. Our objective was to know how irrelevant signals affect the PD of neurons and whether irrelevant signals contribute significantly to neuronal variability.</p>
<p>To explore how irrelevant signals affect the PD of neurons, we first calculated the PD of both raw and distilled signals separately and then quantified the PD deviation by the angle difference between these two signals. Results show that the PD deviation increases as the neuronal R<sup>2</sup> de-creases (red curve, <xref rid="fig3" ref-type="fig">Fig. 3a,e</xref> and <xref rid="figs3" ref-type="fig">Supplementary Fig. S3a</xref>); neurons with larger R<sup>2</sup> (strongly-tuned neurons) exhibit stable PDs with signal distillation (see example PDs in the inset), while neurons with smaller R<sup>2</sup>s (weakly-tuned neurons) show a larger PD deviation. These results indicate that irrelevant signals have a small effect on strongly-tuned neurons but a large effect on weakly-tuned neurons. One possible reason for the larger PD deviation in weakly-tuned neurons is that they have a lower degree of linear encoding but a higher degree of nonlinear encoding, and highly nonlinear structures are more susceptible to interference from irrelevant signals (<bold><italic><xref rid="c31" ref-type="bibr">Nogueira et al., 2023</xref></italic></bold>). Moreover, after filtering out the behaviorally-irrelevant signals, the cosine tuning fit (R<sup>2</sup>) of neurons increases (<italic>P &lt;</italic> 10<sup>−20</sup>, Wilcoxon signed-rank test; <xref rid="fig3" ref-type="fig">Fig. 3b,f</xref> and <xref rid="figs3" ref-type="fig">Supplementary Fig. S3b</xref>), indicating that irrelevant signals reduce the neurons’ tuning expression. Notably, even after remov-ing the interference of irrelevant signals, the R<sup>2</sup> of neurons remains relatively low. These results demonstrate that the linear encoding model only explains a small fraction of neural responses, and neuronal activity encodes behavioral information in complex nonlinear ways.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>The effect of irrelevant signals on analyzing neural activity at the single-neuron level.</title>
<p><bold>a</bold>, The angle difference (AD) of preferred direction (PD) between raw and distilled signals as a function of the R<sup>2</sup> of raw signals on datasets A, where R<sup>2</sup> represents the proportion of neuronal activity explained by the linear encoding model. Each black point represents a neuron (n = 90). The red curve is the fitting curve between R<sup>2</sup> and AD. Five example larger R<sup>2</sup> neurons’ PDs are shown in the inset plot, where the solid and dotted line arrows represent the PDs of relevant and raw signals, respectively. <bold>b</bold>, Comparison of the cosine tuning fit (R<sup>2</sup>) before and after distillation of single neurons (black points), where the x-axis and y-axis represent neurons’ R<sup>2</sup> of raw and distilled signals, respectively. <bold>c</bold>, Comparison of neurons’ Fano factor (FF) averaged across conditions of raw (x-axis) and distilled (y-axis) signals, where FF is used to measure the neuronal variability of different trials in the same condition. <bold>d</bold>, Boxplots of raw (purple) and distilled (red) signals under different conditions for all neurons (12 conditions). Boxplots represent medians (lines), quartiles (boxes), and whiskers extending to ± 1.5 times the interquartile range. The broken lines represent the mean FF across all neurons. <bold>e-h</bold>, Same as <bold>a-d</bold>, but for dataset B (n=159, 8 conditions). <bold>i</bold>, Example of three neurons’ raw firing activity decomposed into behaviorally-relevant and irrelevant parts using all trials under two conditions (2 of 8 directions) in held-out test sets of dataset B.</p></caption>
<graphic xlink:href="515644v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To investigate whether irrelevant signals significantly contribute to neuronal variability, we com-pared the neuronal variability (measured with the Fano factor (<bold><italic><xref rid="c6" ref-type="bibr">Churchland et al., 2010</xref></italic></bold>), FF) of rel-evant and raw signals. Results show that the condition-averaged FF of each neuron of distilled signals is lower than that of raw signals (<italic>P &lt;</italic> 10<sup>−20</sup>, Wilcoxon signed-rank test; <xref rid="fig3" ref-type="fig">Fig. 3c,g</xref>), and the mean (broken line) and median FFs of all neurons under different conditions are also significantly lower than those of raw signals (<italic>P &lt;</italic> 0.01, Wilcoxon signed-rank test; <xref rid="fig3" ref-type="fig">Fig. 3d,h</xref>), indicating that irrelevant signals significantly contribute to neuronal variability. We then visualized the single-trial neuronal activity of example neurons under different conditions (<xref rid="fig3" ref-type="fig">Fig. 3i</xref> and <xref rid="figs4" ref-type="fig">Supplementary Fig. S4</xref>). Results demonstrate that the patterns of relevant signals are more consistent and stable across different trials than raw signals, and the firing activity of irrelevant signals varies randomly. These results indicate that irrelevant signals significantly contribute to neuronal variability, and eliminat-ing the interference of irrelevant signals enables us observe the changes in neural pattern more accurately.</p>
</sec>
<sec id="s2f">
<title>How do behaviorally-irrelevant signals affect the analysis of neural activity at the population level?</title>
<p>The neural population structure is an essential characteristic of neural activity. Here, we examined how behaviorally-irrelevant signals affect the analysis of neural activity at the population level, in-cluding four aspects: (1) the population properties of relevant and irrelevant signals, (2) the sub-space overlap relationship between the two signal components, (3) how the two partitions con-tribute to raw signals, and (4) the difference in population properties between raw and distilled signals.</p>
<p>To explore the population properties of relevant and irrelevant signals, we separately applied principal component analysis (PCA) on each partition to obtain the corresponding cumulative vari-ance curve in a descending variance order. Our results show that the primary subspace (capturing the top 90% variance) of relevant signals (thick red line, Fig.4a,e and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5a</xref>) is only explained by a few dimensions (7, 13, and 9 for each dataset), indicating that the primary part of behaviorally-relevant signals exists in a low-dimensional subspace. In contrast, the primary sub-space of irrelevant signals (thick blue line, <xref rid="fig4" ref-type="fig">Fig. 4b,f</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5b</xref>) requires more dimensions (46, 81, and 59). The variance distribution of behaviorally-irrelevant signals across dimensions (thick blue line, <xref rid="fig4" ref-type="fig">Fig. 4b,f</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5b</xref>) is more even than behaviorally-relevant signals (thick red line, <xref rid="fig4" ref-type="fig">Fig. 4a,e</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5a</xref>) but not as uniform as Gaus-sian noise <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline24.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>(<bold>0</bold>, <bold>I</bold>) (thin gray line, <xref rid="fig4" ref-type="fig">Fig.4b,f</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5b</xref>), indicating that irrelevant signals are not pure noise but rather bear some significant structure, which may represent infor-mation from other irrelevant tasks.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>The effect of irrelevant signals on analyzing neural activity at the population level.</title>
<p><bold>a,b</bold>, PCA is separately applied on relevant and irrelevant signals to get relevant PCs and irrelevant PCs. The thick lines represent the cumulative variance explained for the signals on which PCA has been performed, while the thin lines represent the variance explained by those PCs for other signals. Red, blue, and gray colors indicate relevant signals, irrelevant signals, and random Gaussian noise <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline23.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (<bold>0</bold>, <bold>I</bold>) (for chance level) where the mean vector is zero and the covariance matrix is the identity matrix. The horizontal lines represent the percentage of variance explained. The vertical lines indicate the number of dimensions accounted for 90% of the variance in behaviorally-relevant (left) and irrelevant (right) signals. For convenience, we defined the principal component subspace describing the top 90% variance as the primary subspace and the subspace capturing the last 10% variance as the secondary subspace. The cumulative variance explained for behaviorally-relevant (<bold>a</bold>) and irrelevant (<bold>b</bold>) signals got by d-VAE on dataset A. <bold>c,d</bold>, PCA is applied on raw signals to get raw PCs. <bold>c</bold>, The bar plot shows the composition of each raw PC. The inset pie plot shows the overall proportion of raw signals, where red, blue, and purple colors indicate relevant signals, irrelevant signals, and the correlation between relevant and relevant signals. The PC marked with a red triangle indicates the last PC where the variance of relevant signals is greater than or equal to that of irrelevant signals. <bold>d</bold>, The cumulative variance explained by raw PCs for different signals, where the thick line represents the cumulative variance explained for raw signals (purple), while the thin line represents the variance explained for relevant (red) and irrelevant (blue) signals. <bold>e-h</bold>, Same as <bold>a-d</bold>, but for dataset B.</p></caption>
<graphic xlink:href="515644v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To investigate the subspace overlap between relevant and irrelevant signals, we calculated how many variances of irrelevant signals can be captured by relevant PCs by projecting irrelevant sig-nals onto relevant PCs and vice versa (<bold><italic><xref rid="c10" ref-type="bibr">Elsayed et al., 2016</xref></italic></bold>; <bold><italic><xref rid="c37" ref-type="bibr">Rouse and Schieber, 2018</xref></italic></bold>; <bold><italic><xref rid="c21" ref-type="bibr">Jiang et al., 2020</xref></italic></bold>)(see Methods). We found that the variance of irrelevant signals increases relatively uniformly over relevant PCs (blue thin line, <xref rid="fig4" ref-type="fig">Fig. 4a,e</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5a</xref>), like random noise’s variance accumulation explained by relevant PCs (gray thin line, <xref rid="fig4" ref-type="fig">Fig. 4a,e</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5a</xref>); sim-ilar results are observed for relevant signals explained by irrelevant PCs (red thin line, <xref rid="fig4" ref-type="fig">Fig. 4b,f</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5b</xref>). These results indicate that relevant PCs can not match the informative dimensions of irrelevant signals and vice versa, suggesting the dimensions of behaviorally-relevant and irrelevant signals are unrelated. It is worth mentioning that the signals obtained by pi-VAE are in contrast to our findings. Its results show that a few relevant PCs can explain a considerable variance of irrelevant signals (thin red line, <xref rid="figs6" ref-type="fig">Supplementary Fig. S6b,f,j</xref>), which indicates that the relevant and irrelevant PCs are closely related. The possible reason is that the pi-VAE leaks a lot of information into the irrelevant signals. Notably, <xref rid="fig4" ref-type="fig">Fig. 4a,e</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5a</xref> show that ir-relevant signals got by d-VAE projecting on behaviorally-relevant primary subspace only capture a little variance information (9%, 12%, and 9%), indicating that the primary subspace of behaviorally-relevant signals is nearly orthogonal to irrelevant space.</p>
<p>To investigate the composition of raw signals by the two partitions, we performed PCA on raw neural signals to obtain raw PCs, and then projected the relevant and irrelevant signals onto these PCs to assess the proportion of variance of raw signals explained by each partition. First, we ana-lyzed the overall proportion of relevant and irrelevant signals that constitute the raw signals (the inset pie plot, <xref rid="fig4" ref-type="fig">Fig. 4c,g</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5c</xref>). The variance of the raw signals is composed of three parts: the variance of relevant signals, the variance of irrelevant signals, and the correlation between relevant and irrelevant signals (see Methods). The results demonstrate that the irrele-vant signals account for a large proportion of raw signals, suggesting the motor cortex encodes considerable information that is not related to the measured behaviors. In addition, there is only a weak correlation between relevant and irrelevant signals, implying that behaviorally-relevant and irrelevant signals are nearly uncorrelated.</p>
<p>We then examined the proportions of relevant and irrelevant signals in each PC of raw signals. We found that relevant signals (red) occupy the dominant proportions in the larger variance raw PCs (before the PC marked with a red triangle), while irrelevant signals (blue) occupy the dominant proportions in the smaller variance raw PCs (after the PC marked with a red triangle) (<xref rid="fig4" ref-type="fig">Fig. 4c,g</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5c</xref>). Similar results are observed in the accumulation of each raw PC (<xref rid="fig4" ref-type="fig">Fig. 4d,h</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5d</xref>). Specifically, the results show that the variance accumulation of raw signals (purple line) in larger variance PCs is mainly contributed by relevant signals (red line), while irrelevant signals (blue line) primarily contribute to the lower variance PCs. These results demonstrate that irrelevant signals have a small effect on larger variance raw PCs but a large effect on smaller variance raw PCs. This finding eliminates the concern that irrelevant signals would significantly affect the top few PCs of raw signals and thus produce inaccurate conclusions. To further validate this finding, we used the top six PCs as jPCA (<bold><italic><xref rid="c5" ref-type="bibr">Churchland et al., 2012</xref></italic></bold>) did to examine the rotational dynamics of distilled and raw signals (<xref rid="figs7" ref-type="fig">Fig. S7</xref>). Results show that the rotational dynamics of distilled signals are similar to those of raw signals.</p>
<p>Finally, to directly compare the population properties of raw and relevant signals, we plotted the cumulative variance curves of raw and relevant signals (<xref rid="figs8" ref-type="fig">Supplementary Fig. S8</xref>). Results (upper left corner curves, <xref rid="figs8" ref-type="fig">Supplementary Fig. S8</xref>) show that the cumulative variance curve of relevant signals (red line) accumulates faster than that of raw signals (purple line) in the preceding larger variance PCs, indicating that the variance of the relevant signal is more concentrated in the larger variance PCs than that of raw signals. Furthermore, we found that the dimensionality of primary subspace of raw signals (26, 64, and 45 for datasets A, B, and C) is significantly higher than that of behaviorally-relevant signals (7, 13, and 9), indicating that behaviorally-irrelevant signals lead to an overestimation of the neural dimensionality of behaviorally-relevant signals.</p>
</sec>
<sec id="s2g">
<title>Distilled behaviorally-relevant signals uncover that smaller R<sup>2</sup> neurons encode rich behavioral information in complex nonlinear ways</title>
<p>The results presented above regarding PDs (<xref rid="fig3" ref-type="fig">Fig. 3</xref> and <xref rid="figs3" ref-type="fig">Fig. S3</xref>) demonstrate that irrelevant signals significantly impact smaller R<sup>2</sup> neurons and weakly impact larger R<sup>2</sup> neurons. Under the interfer-ence of irrelevant signals, it is dificult to explore the amount of behavioral information in neuronal activity. Given that we have accurately separated the behaviorally-relevant and irrelevant signals, we explored whether irrelevant signals would mask some encoded information of neuronal activity, especially for smaller R<sup>2</sup> neurons.</p>
<p>To answer the question, we divided the neurons into two groups of smaller R<sup>2</sup> (R<sup>2</sup> <italic>&lt;</italic>= 0.03) and larger R<sup>2</sup> (R<sup>2</sup> <italic>&gt;</italic> 0.03), and then used decoding models to assess how much information is en-coded in raw and distilled signals. As shown in <xref rid="fig5" ref-type="fig">Fig. 5a</xref>, for the smaller R<sup>2</sup> neuron group, both KF and ANN decode behavioral information poorly on raw signals, but achieve high decoding perfor-mance using relevant signals. Specifically, the KF decoder (left plot, <xref rid="fig5" ref-type="fig">Fig. 5a</xref>) improves the decoding R<sup>2</sup> significantly from 0.044 to 0.616 (improves by about 1300%; <italic>P &lt;</italic> 0.01, Wilcoxon rank-sum test) after signal distillation; the ANN decoder (right plot, <xref rid="fig5" ref-type="fig">Fig. 5a</xref>) improves from 0.374 to 0.753 (im-proves by about 100%; <italic>P &lt;</italic> 0.01, Wilcoxon rank-sum test). For the larger R<sup>2</sup> neuron group, the decoding performance of relevant signals with ANN does not improve much compared with the decoding performance of raw signals, but the decoding performance of relevant signals with KF is significantly better than that of raw signals (<italic>P &lt;</italic> 0.01, Wilcoxon rank-sum test). Similar results are obtained with datasets B (<xref rid="fig5" ref-type="fig">Fig. 5d</xref>) and C (<xref rid="figs9" ref-type="fig">Supplementary Fig. S9d</xref>). These results indicate that irrele-vant signals mask behavioral information encoded by neuronal populations, especially for smaller R<sup>2</sup> neurons with a higher degree of nonlinearity, and that smaller R<sup>2</sup> neurons actually encode rich behavioral information.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Smaller R<sub>2</sub> neurons encode rich behavioral information in complex nonlinear ways.</title>
<p><bold>a</bold>, The comparison of decoding performance between raw (purple) and distilled signals (red) on dataset A with different neuron groups, including smaller R<sup>2</sup> neuron (R<sup>2</sup><italic>&lt;</italic>= 0.03), larger R<sup>2</sup> neuron (R<sup>2</sup><italic>&gt;</italic> 0.03), and all neurons. Error bars indicate mean ± s.d. across five cross-validation folds. Asterisks denote significance of Wilcoxon rank-sum test with <italic>P &lt;</italic> 0.05, <italic>P &lt;</italic> 0.01. <bold>b</bold>, The correlation matrix of all neurons of raw (left) and behaviorally-relevant (right) signals on dataset A. Neurons are ordered to highlight correlation structure (details in Methods). <bold>c</bold>, The decoding performance of KF (left) and ANN (right) with neurons dropped out from larger to smaller Ron dataset A. The vertical gray line indicates the number of dropped neurons at which raw and behaviorally-relevant signals have the greatest performance difference. <bold>d-f</bold>, Same as <bold>a-c</bold>, for dataset B.</p></caption>
<graphic xlink:href="515644v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The fact that the smaller R<sup>2</sup> neurons encode rich information seems unexpected, and interest-ingly, we cannot obtain this rich information by just distilling smaller R<sup>2</sup> neurons. This leads to two alternative cases. The first is that the larger R<sup>2</sup> neurons leak their information to the smaller R<sup>2</sup> neurons, causing them contain too much behavioral information. The second is that the smaller R<sup>2</sup> neurons per se contain a lot of information, and the larger R<sup>2</sup> neurons help the smaller R<sup>2</sup> neurons to restore their original appearance. We first tested the first case and found it does not hold for two reasons. Firstly, our model constrains distilled neuronal activity to be similar to the corresponding original neuronal activity (see <xref rid="fig3" ref-type="fig">Fig. 3i</xref> and <xref rid="figs4" ref-type="fig">Supplementary Fig. S4</xref>), inhibiting the gen-eration of arbitrarily shaped neuronal activity, such as that of other neurons, so it is impossible for our model to add the activity of larger R<sup>2</sup> neurons to that of smaller R<sup>2</sup> neurons. To check this, we compared the neural similarity (R<sup>2</sup>) of each distilled neuron to all raw neurons. We found 78/90 (87%, dataset A), 153/159 (96%, dataset B), 91/91 (100%, dataset C) distilled neurons are most sim-ilar to the corresponding neurons, and the other distilled neurons belong to the top four similar to the corresponding neurons, demonstrating the distilled neuronal activity is similar to the cor-responding raw neuronal activity. Secondly, if this large amount of information is compensated by other neurons, the residuals should also contain a large amount of information, but they only contain a little information (<xref rid="fig2" ref-type="fig">Fig. 2c,g</xref>,k). Given these two reasons, the first case is rejected. Then we tested the second case. To verify the second case, we simulated a situation where the smaller R<sup>2</sup> neurons contain a certain amount of behavioral information, but the behavioral information can-not be decoded from these neurons due to being covered by noise (see Methods), to see if d-VAE can restore these neurons’ original appearance. Results (<xref rid="figs10" ref-type="fig">Supplementary Fig. S10a-c</xref>) show that for smaller R<sup>2</sup> neurons, the decoding R<sup>2</sup> of distilled signals (0.362, 0.401 for KF and ANN) is significantly higher than that of raw signals (0.019, 0.063 for KF and ANN; <italic>P &lt;</italic> 0.01, Wilcoxon rank-sum test), and is more closer to that of ground truth (0.454, 0.467 for KF and ANN). These results approve the second case. Taken together, these results demonstrate that the smaller R<sup>2</sup> neurons indeed contain rich behavioral information.</p>
<p>Given that both smaller and larger R<sup>2</sup> neurons encode rich behavioral information, it is worth noting that the sum of the decoding performance of smaller R<sup>2</sup> neurons and larger R<sup>2</sup> neurons is significantly greater than that of all neurons for relevant signals (red bar, <xref rid="fig5" ref-type="fig">Fig. 5a,d</xref> and <xref rid="figs9" ref-type="fig">Supple-mentary Fig. S9a</xref>), demonstrating that movement parameters are encoded very redundantly in neuronal population. In contrast, we can not find this degree of neural redundancy in raw signals (purple bar, <xref rid="fig5" ref-type="fig">Fig. 5a,d</xref> and <xref rid="figs9" ref-type="fig">Supplementary Fig. S9a</xref>) because the encoded information of smaller R<sup>2</sup> neurons are masked by irrelevant signals. Therefore, these smaller R<sup>2</sup> neurons, which are usually ignored, are actually useful and play a critical role in supporting neural redundancy. Generally, cortical redundancy can arise from neuronal correlations, which are critical for revealing certain aspects of neural circuit organization (<bold><italic><xref rid="c50" ref-type="bibr">Yatsenko et al., 2015</xref></italic></bold>). Accordingly, we visualized the or-dered correlation matrix of neurons (see Methods) for both raw and relevant signals (<xref rid="fig5" ref-type="fig">Fig. 5b,e</xref> and <xref rid="figs9" ref-type="fig">Supplementary Fig. S9b</xref>) and found that the neuronal correlation of relevant signals is stronger than that of raw signals. These results demonstrate that irrelevant signals weaken the neuronal correlation, which may hinder the accurate investigation of neural circuit organization.</p>
<p>Considering the rich redundancy and strong correlation of neuronal activity, we wondered whether the neuronal population could utilize redundant information from other neurons to ex-hibit robustness under the perturbation of neuronal destruction. To investigate this question, we evaluated the decoding performance of dropping out neurons from larger R<sup>2</sup> to smaller R<sup>2</sup> on raw and relevant signals. The results (<xref rid="fig5" ref-type="fig">Fig. 5c,f</xref> and <xref rid="figs9" ref-type="fig">Supplementary Fig. S9c</xref>) show that the decoding performance of the KF and ANN on raw signals (purple line) decreases steadily before the number of neurons marked (vertical gray line), and the remaining smaller R<sup>2</sup> neurons decode behavioral in-formation poorly. In contrast, even if many neurons are lost, the decoding performance of KF and ANN on relevant signals (red line) maintains high accuracy. This finding indicates that behaviorally-relevant signals are robust to the disturbance of neuron drop-out, and smaller R<sup>2</sup> neurons play a critical role in compensating for the failure of larger R<sup>2</sup> neurons. In contrast, this robustness can not be observed in raw signals because irrelevant signals mask neurons’ information and weaken their correlation. Notably, the ANN outperforms the KF when only smaller R<sup>2</sup> neurons are left (<xref rid="fig5" ref-type="fig">Fig. 5c,f</xref> and <xref rid="figs9" ref-type="fig">Supplementary Fig. S9c</xref>), suggesting that smaller R<sup>2</sup> neurons can fully exploit their nonlinear ability to cope with large-scale neuronal destruction.</p>
</sec>
<sec id="s2h">
<title>Distilled behaviorally-relevant signals uncover that signals composed of smaller variance PCs encode rich behavioral information in complex nonlinear ways</title>
<p>The results presented above regarding subspace overlap (<xref rid="fig4" ref-type="fig">Fig. 4</xref> and <xref rid="figs5" ref-type="fig">Fig. S5</xref>) show that irrelevant signals have a small impact on larger variance PCs but dominate smaller variance PCs. Therefore, we aimed to investigate whether irrelevant signals would mask some encoded information of neu-ral population, especially signals composed of smaller variance PCs.</p>
<p>To answer the question, we compared the decoding performance of raw and distilled signals with different raw PC groups. Specifically, we firstly divided the raw PCs into two groups, that is, smaller variance PCs and larger variance PCs, defined by ratio of relevant to irrelevant signals in the raw PCs (the red triangle, see <xref rid="fig4" ref-type="fig">Fig. 4c,g</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5c</xref>). Then we projected raw and distilled signals onto these two PC groups and got the corresponding signals. Results show that, for the smaller variance PC group, both KF and ANN achieve much better performance on distilled signals than raw signals (<italic>P &lt;</italic> 0.01, Wilcoxon rank-sum test, for ANN), whereas for the larger variance PC group, the decoding performance of relevant signals does not improve a lot compared with the decoding performance of raw signals (see <xref rid="fig6" ref-type="fig">Fig. 6a,d</xref> and <xref rid="figs9" ref-type="fig">Supplementary Fig. S9a</xref>). These results demonstrate that irrelevant signals mask the behavioral information encoded by different PC groups, especially for signals composed of smaller variance PCs (smaller variance PC signals), and smaller variance PC signals actually encode rich behavioral information.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Signals composed of smaller variance PCs encode rich behavioral information in complex nonlinear ways.</title>
<p><bold>a</bold>, The comparison of decoding performance between raw (purple) and distilled signals (red) composed of different raw PC groups, including smaller variance PCs (the proportion of irrelevant signals that make up raw PCs is higher than that of relevant signals), larger variance PCs (the proportion of irrelevant signals is lower than that of relevant ones) on dataset A. Error bars indicate mean ± s.d. across five cross-validation folds. Asterisks denote significance of Wilcoxon rank-sum test with <italic><sub>*</sub>P &lt;</italic> 0.05, <italic><sub>**</sub>P &lt;</italic> 0.01. <bold>b</bold>, The cumulative decoding performance of signals composed of cumulative PCs that are ordered from smaller to larger variance using KF (left) and ANN (right) on dataset A. The red patches indicate the decoding ability of the last 10% variance of relevant signals. <bold>c</bold>, The cumulative decoding performance of signals composed of cumulative PCs that are ordered from larger to smaller variance using KF (left) and ANN (right) on dataset A. The red patches indicate the decoding gain of the last 10% variance signals of relevant signals superimposing on their top 90% variance signals. The inset shows the partially enlarged plot for view clearly. <bold>d-f</bold>, Same as <bold>a-c</bold>, but for dataset B.</p></caption>
<graphic xlink:href="515644v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The above results are based on raw PCs. However, raw PCs are biased by irrelevant signals and thus cannot faithfully reflect the characteristics of relevant signals. As we have successfully separated the behaviorally-relevant signals, we aimed to explore how behavioral information of distilled signals is distributed across relevant PCs. To do so, we used decoding models to evalu-ated the amount of behavioral information contained in cumulative PCs of relevant signals (using raw signals as a comparison). The cumulative variance explained by PCs in descending and as-cending order of variance and the dimensionality corresponding to the top 90% variance signals (called primary signals) and the last 10% variance signals (called secondary signals) are shown in <xref rid="figs8" ref-type="fig">Supplementary Fig. S8</xref>.</p>
<p>Here we first investigated secondary signals’ decoding ability solely by accumulating PCs from smaller to larger variance. The results show that, for relevant signals, KF can hardly decode behav-ioral information solely using secondary signals (red line; left plot, <xref rid="fig6" ref-type="fig">Fig. 6b,e</xref> and <xref rid="figs9" ref-type="fig">Supplementary Fig. S9e</xref>), but ANN can decode rich information (red line; right plot, <xref rid="fig6" ref-type="fig">Fig. 6b,e</xref> and <xref rid="figs9" ref-type="fig">Supplementary Fig. S9e</xref>). These results indicate that smaller variance PC signals encode rich information in complex nonlinear ways. In contrast, when using raw signals composed of the same number of dimensions as the secondary signals (purple line, <xref rid="fig6" ref-type="fig">Fig. 6b,e</xref> and <xref rid="figs9" ref-type="fig">Supplementary Fig. S9e</xref>), the amount of infor-mation identified by ANN is significantly smaller than that of relevant secondary signals (<italic>P &lt;</italic> 0.01, Wilcoxon rank-sum test). These results demonstrate that irrelevant signals obscure these neural di-mensions and make them seem insignificant, indicating that behavioral information is distributed in a higher-dimensional subspace than expected from raw signals.</p>
<p>We then investigated the effect of superimposing secondary signals on primary signals by ac-cumulating PCs from larger to lower variance. The results (<xref rid="fig6" ref-type="fig">Fig. 6c,f</xref> and <xref rid="figs9" ref-type="fig">Supplementary Fig. S9f</xref>) show that secondary signals improve the decoding performance of ANN a little but improve the decoding performance of KF a lot. The discrepancy between the two decoders reflects their differ-ent abilities to utilize the information within the signal. KF cannot use the nonlinear information in primary signals as effectively as ANN can and thus requires secondary signals to improve decoding performance. Notably, the improvement of KF using secondary signals is significantly higher than using raw signals composed of the same number of dimensions as the secondary signals (<italic>P &lt;</italic> 0.01, Wilcoxon rank-sum test). These results demonstrate that irrelevant signals conceal the smaller vari-ance PC signals, making their encoded information dificult to be linearly decoded, suggesting that behavioral information exists in a higher-dimensional subspace than anticipated from raw signals. Interestingly, we can find that although secondary signals nonlinearly encode behavioral informa-tion and are decoded poorly by linear decoders, they considerably improve KF performance by superimposing on primary signals (<xref rid="fig6" ref-type="fig">Fig. 6c,f</xref> and <xref rid="figs9" ref-type="fig">Supplementary Fig. S9f</xref> left); and the sum of the sole decoding performance of primary and secondary signals is lower than the decoding perfor-mance of full signals. These results indicate that the combination of smaller and larger variance PCs produces a synergy effect (<bold><italic><xref rid="c30" ref-type="bibr">Narayanan et al., 2005</xref></italic></bold>) enabling secondary signals that cannot be linearly decoded to improve the linear decoding performance.</p>
<p>Finally, we explored which aspect of movement parameters was improved by superimposing the secondary signals on the primary ones with KF. In BMIs, directional control has achieved great success (<bold><italic><xref rid="c16" ref-type="bibr">Georgopoulos et al., 1986</xref></italic></bold>; <bold><italic><xref rid="c19" ref-type="bibr">Hochberg et al., 2012</xref></italic></bold>), but precise speed control, especially at lower speeds such as hold or stop, has always been challenging (<bold><italic><xref rid="c47" ref-type="bibr">Wodlinger et al., 2014</xref></italic></bold>; <bold><italic><xref rid="c20" ref-type="bibr">Inoue et al., 2018</xref></italic></bold>). Thus we hypothesized that these signals might improve the lower-speed velocity. To test this, we divided samples into lower-speed and higher-speed regions and assessed which re-gion improved the most by superimposing the secondary signals (see details in Methods). After superimposing the secondary signals, the absolute improvement ratio of the lower-speed region is significantly higher than that of the higher-speed region (<italic>P &lt;</italic> 0.05, Wilcoxon rank-sum test; Supple-mentary Fig.S11a,b,c). Furthermore, we visualized the relative improvement ratio of five example trials for the two regions, and the results (<xref rid="figs11" ref-type="fig">Supplementary Fig. S11d</xref>) demonstrate that secondary signals significantly improve the estimation of lower speed. These results demonstrate that the secondary signals enhance the lower-speed control, suggesting that smaller variance PC signals are involved in regulating precise motor control.</p>
</sec>
<sec id="s2i">
<title>Distilled behaviorally-relevant signals suggest that motor cortex may use a linear readout mechanism to generate movement behaviors</title>
<p>Understanding the readout mechanism of the motor cortex is crucial for both neuroscience and neural engineering, which remains unclear. By filtering out the interference of behaviorally-irrelevant signals, we found a stunning result: the linear decoder KF achieves comparable performance to that of the nonlinear decoder ANN (<italic>P</italic> = 0.10, 0.15, and 0.55 for datasets A, B, and C, Wilcoxon rank-sum test; <xref rid="fig2" ref-type="fig">Fig. 2b,f</xref>,g). Considering the decoding performance and model complexity (the simplicity principle, also called Occam’s razor), movement behaviors are more likely to be generated by the linear readout, suggesting linear readout is performed in the motor cortex.</p>
<p>However, one might doubt that it is our distillation model that makes signals which originally require nonlinear decoding linearly decodable. To address this concern, we conducted a simulation experiment where linear decoding is significantly inferior to nonlinear decoding to see if signals processed by our model could enable linear decoders to achieve performance similar to that of nonlinear decoders (see Methods). Results (<xref rid="figs10" ref-type="fig">Supplementary Fig. S10d-f</xref>) show that the difference of decoding R<sup>2</sup> between KF and ANN on distilled signals is 0.146, which is comparable to the difference (0.162) on raw signals (<italic>P</italic> = 0.15, Wilcoxon rank-sum test). This demonstrate that our model cannot make linear decoders achieve similar performance as nonlinear decoders, thus eliminating the doubt.</p>
<p>In summary, these findings demonstrate that behaviorally-irrelevant signals significantly com-plicate the readout of behavioral information and provide compelling evidence supporting the notion that the motor cortex uses a linear readout mechanism to generate movement behaviors.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study, we proposed a new perspective for studying neural mechanisms, namely, using sep-arated accurate behaviorally-relevant signals instead of raw signals; and we provided a novel dis-tillation framework to define, extract, and validate behaviorally-relevant signals. By separating behaviorally-relevant and irrelevant signals, we found that neural responses previously considered trivial encode rich behavioral information in complex nonlinear ways, and they play an important role in neural encoding and decoding. Furthermore, we found that linear decoders can achieve comparable performance to that of nonlinear decoders, providing compelling evidence for the presence of linear readout in the motor cortex. Overall, our results reveal unexpected complex encoding but simple decoding mechanisms in the motor cortex.</p>
<sec id="s3a">
<title>Signal separation by d-VAE</title>
<p>Behaviorally-relevant patterns can be extracted either at single-neuron or latent neural popula-tion levels. In our study, we focused on the single-neuron level, aiming to preserve the underlying properties of individual neurons. By maintaining the properties of each neuron, researchers can in-vestigate how the neuronal population performs when one of the neurons is destroyed. This kind of analysis is particularly useful in closed-loop stimulation experiments that use electrophysiolog-ical (<bold><italic><xref rid="c43" ref-type="bibr">Sun et al., 2022</xref></italic></bold>) or optogenetic (<bold><italic><xref rid="c52" ref-type="bibr">Zhang et al., 2022</xref></italic></bold>) interventions. Furthermore, behaviorally-relevant signals also allow for population-level analysis and provide clean benchmark signals to test and compare the variance capture ability of different hypothesis-driven models.</p>
<p>At the single-neuron level, it is common practice to use trial-averaged neuronal responses of the same task parameters to analyze neural mechanisms (<bold><italic><xref rid="c25" ref-type="bibr">Kobak et al., 2016</xref></italic></bold>; <bold><italic><xref rid="c37" ref-type="bibr">Rouse and Schieber, 2018</xref></italic></bold>). However, trial averaging sacrifices single-trial information, thereby providing an incomplete characterization of neural activity. Furthermore, trial-averaged responses still contain a significant amount of behaviorally-irrelevant signals caused by uninstructed movements (<bold><italic><xref rid="c29" ref-type="bibr">Musall et al., 2019</xref></italic></bold>), which can lead to a contaminated version of behaviorally-relevant signals. In contrast, our model is capable of extracting clean behaviorally-relevant neural activity for every single trial. At the la-tent population level, existing latent variable models (<bold><italic><xref rid="c39" ref-type="bibr">Sani et al., 2021</xref></italic></bold>; <bold><italic><xref rid="c34" ref-type="bibr">Pandarinath et al., 2018</xref></italic></bold>; <bold><italic><xref rid="c51" ref-type="bibr">Yu et al., 2008</xref></italic></bold>; <bold><italic><xref rid="c53" ref-type="bibr">Zhou and Wei, 2020</xref></italic></bold>) focus on modeling some specific properties of latent population representations, such as linear or nonlinear dynamics (<bold><italic><xref rid="c39" ref-type="bibr">Sani et al., 2021</xref></italic></bold>; <bold><italic><xref rid="c34" ref-type="bibr">Pandarinath et al., 2018</xref></italic></bold>; <bold><italic><xref rid="c5" ref-type="bibr">Churchland et al., 2012</xref></italic></bold>), temporal smoothness (<bold><italic><xref rid="c51" ref-type="bibr">Yu et al., 2008</xref></italic></bold>), and interpretability (<bold><italic><xref rid="c53" ref-type="bibr">Zhou and Wei, 2020</xref></italic></bold>). Since these models make restrictive assumptions involving characterizing specific neural properties, they fail to capture other parts of behaviorally-relevant signals that do not meet their assumptions, providing no guarantee that the generated signals preserve behavioral information maximally. In contrast, our model aims to extract maximal behavioral information at the single-neuron level. To ensure this, we explicitly constrain the model to generate signals that maintain the neuronal properties while preserving behavioral information to the greatest extent possible. At the population level, dimensionality reduction methods aided by task parameters (<bold><italic><xref rid="c25" ref-type="bibr">Kobak et al., 2016</xref></italic></bold>; <bold><italic><xref rid="c42" ref-type="bibr">Schneider et al., 2022</xref></italic></bold>) are another important way to discover the latent neural embeddings relevant to task parameters, which may provide new insight into neural representations. In con-trast with this class of methods, our model focuses on the signal level, not the latent embedding level.</p>
<p>Although we made every effort, our model is still not able to perfectly extract behaviorally-relevant neural signals, resulting in a small amount of behavioral information leakage in the resid-uals. Nevertheless, the signals distilled by our model are reliable, and the minor imperfections do not affect the conclusions drawn from our analysis. In the future, better models can be developed to extract behaviorally-relevant signals more accurately.</p>
</sec>
<sec id="s3b">
<title>Implications for analyzing neural activity by separation</title>
<p>Studying neural mechanisms through noisy signals is akin to looking at flowers in a fog, which is dificult to discern the truth. Thus, removing the interference of irrelevant signals is necessary and beneficial for analyzing neural activity, whether at the single-neuron level or population level.</p>
<p>At the single-neuron level, trial-to-trial neuronal variability poses a significant challenge to iden-tifying the actual neuronal pattern changes. The variability can arise from various sources, includ-ing meaningless noise (<bold><italic><xref rid="c11" ref-type="bibr">Faisal et al., 2008</xref></italic></bold>), meaningful but behaviorally-irrelevant neural processes (<bold><italic><xref rid="c29" ref-type="bibr">Musall et al., 2019</xref></italic></bold>), and intrinsic components of neural encoding (<bold><italic><xref rid="c44" ref-type="bibr">Walker et al., 2020</xref></italic></bold>). However, it is still unclear to what extent each source contributes to the variability (<bold><italic><xref rid="c11" ref-type="bibr">Faisal et al., 2008</xref></italic></bold>). By sep-arating behaviorally-relevant and irrelevant parts, we could roughly determine the extent to which these two parts contribute to the variability and explore which type of variability these two parts may contain. Our results demonstrate that behaviorally-irrelevant signals are a significant contrib-utor to variability, which may include both meaningless noise and meaningful but behaviorally-irrelevant signals as behaviorally-irrelevant signals are not pure noise and may carry some struc-tures (thick blue line, <xref rid="fig4" ref-type="fig">Fig. 4b,f</xref> and <xref rid="figs5" ref-type="fig">Supplementary Fig. S5b</xref>). Notably, behaviorally-relevant sig-nals also exhibit some variability, which may arise from intrinsic components of neural encoding and provide the neural basis for motor learning (<bold><italic><xref rid="c8" ref-type="bibr">Dhawale et al., 2017</xref></italic></bold>). Moreover, eliminating the variability caused by irrelevant signals enables us to better observe and compare actual neuronal pattern changes and may facilitate the study of learning mechanisms (<bold><italic><xref rid="c38" ref-type="bibr">Sadtler et al., 2014</xref></italic></bold>; <bold><italic><xref rid="c18" ref-type="bibr">Hennig et al., 2021</xref></italic></bold>).</p>
<p>At the population level, determining the neural dimensionality of specific behaviors from raw signals is challenging since it is dificult to discern how many variances correspond to irrelevant signals, which often depend heavily on signal quality. A previous study (<bold><italic><xref rid="c2" ref-type="bibr">Altan et al., 2021</xref></italic></bold>) demon-strated, through simulation experiments involving different levels of noise, that such noise makes methods overestimate the neural dimensionality. Our results, consistent with theirs, indicate that behaviorally-irrelevant signals can cause an overestimation of the neural dimensionality of behaviorally-relevant responses (<xref rid="figs8" ref-type="fig">Supplementary Fig. S8</xref>). These findings highlight the need to filter out irrele-vant signals when estimating the neural dimensionality. It is important to note that the dimension-ality of neural subspaces encoding behavioral information cannot be accurately determined by the variance they capture. Estimating the dimensionality accurately requires considering the amount of behavioral information encoded by signals distributed within these subspaces. This is because the variance of neural signals does not directly reflect the amount of behavioral information they encode. For example, behaviorally-irrelevant signals occupy some neural subspaces without con-taining behavioral information. Furthermore, this perspective of signal separation has broader implications for other studies. For instance, researchers can isolate neural signals corresponding to different behaviors and explore their shared and exclusive patterns to uncover underlying com-mon and unique mechanisms of different behaviors (<bold><italic><xref rid="c15" ref-type="bibr">Gallego et al., 2018</xref></italic></bold>).</p>
</sec>
<sec id="s3c">
<title>Implications for exploring neural mechanisms by separation</title>
<p>At the single-neuron level, previous studies (<bold><italic><xref rid="c4" ref-type="bibr">Carmena et al., 2005</xref></italic></bold>; <bold><italic><xref rid="c30" ref-type="bibr">Narayanan et al., 2005</xref></italic></bold>) have shown that neuronal ensembles redundantly encode movement behaviors in the motor cortex. However, our results reveal a significantly higher level of redundancy than previously reported. Specifically, prior studies found that the decoding performance steadily declines as neurons drop out, which is consistent with our results drawn from raw signals. In contrast, our results show that decoders maintain high performance on distilled signals even when many neurons drop out. Our findings reinforce the idea that movement behavior is redundantly encoded in the motor cortex and demonstrate that the brain is robust enough to tolerate large-scale neuronal destruction while maintaining brain function (<bold><italic><xref rid="c1" ref-type="bibr">Alstott et al., 2009</xref></italic></bold>).</p>
<p>At the population level, previous studies have proposed that motor control is achieved through low-dimensional neural manifolds (<bold><italic><xref rid="c7" ref-type="bibr">Cunningham and Yu, 2014</xref></italic></bold>; <bold><italic><xref rid="c14" ref-type="bibr">Gallego et al., 2017</xref></italic></bold>). However, our results challenge this idea by showing that signals composed of smaller variance PCs nonlinearly encode a significant amount of behavioral information. This suggests that behavioral information is distributed in a higher-dimensional neural space than previously thought. Interestingly, although smaller variance PC signals nonlinearly encode behavioral information, their behavioral informa-tion can be linearly decoded by superimposing them onto larger variance PC signals. This result is consistent with the finding that nonlinear mixed selectivity can yield high dimensional neural re-sponses and thus allow linear readout of behavioral information by downstream neurons (<bold><italic><xref rid="c36" ref-type="bibr">Rigotti et al., 2013</xref></italic></bold>; <bold><italic><xref rid="c12" ref-type="bibr">Fusi et al., 2016</xref></italic></bold>). Moreover, we found that smaller variance PC signals can improve precise motor control, such as lower speed control. Analogously, recent studies have found that smaller variance PCs of hand postures are task-dependent and relate to the precise and complex postures (<bold><italic><xref rid="c48" ref-type="bibr">Yan et al., 2020</xref></italic></bold>). These findings suggest that neural signals composed of lower variance PCs are involved in the regulation of precise motor control.</p>
<p>In the motor cortex, in what form downstream neurons read out behavioral information is still an open question. By filtering out behaviorally-irrelevant signals, our study found that accurate decoding performance can be achieved through linear readout, suggesting that the motor cortex may perform linear readout to generate movement behaviors. Similar findings have been reported in other cortices, such as the inferotemporal cortex (IT) (<bold><italic><xref rid="c27" ref-type="bibr">Majaj et al., 2015</xref></italic></bold>), perirhinal cortex (PRH) (<bold><italic><xref rid="c33" ref-type="bibr">Pagan et al., 2013</xref></italic></bold>), and somatosensory cortex (<bold><italic><xref rid="c31" ref-type="bibr">Nogueira et al., 2023</xref></italic></bold>), supporting the idea of linear readout as a general principle in the brain. However, further experiments are needed to verify this hypothesis across a wider range of cortical regions. While neurons encode significant behavioral information nonlinearly, our study demonstrates that when neuronal assemblies with diverse fir-ing patterns collaborate to decode information, they do not necessarily need to fully exert their nonlinear ability. Instead, a simple linear combination of these assemblies can accomplish the decoding task. This finding suggests that the complexity of encoding mechanisms underlies the simplicity of decoding mechanisms.</p>
<p>With regard to studying decoding mechanisms, recent studies (<bold><italic><xref rid="c35" ref-type="bibr">Pitkow et al., 2015</xref></italic></bold>; <bold><italic><xref rid="c49" ref-type="bibr">Yang et al., 2021</xref></italic></bold>) have focused on investigating how the brain decodes task information in the presence of noise. Unlike their works, we focus on studying the decoding mechanism after removing the noise (irrelevant signals). Although our research perspectives differ, our results support their idea that the brain needs nonlinear operations to suppress noise interference (<bold><italic><xref rid="c49" ref-type="bibr">Yang et al., 2021</xref></italic></bold>). However, in principle, denoising (or filtering out behaviorally-irrelevant signals) and decoding are two sep-arate steps. Using a communication system as a metaphor, the receiver should first denoise the signal received from the sender and then decrypt the message. The same should be true for the brain. Therefore, removing irrelevant signals to study the readout mechanism is reasonable. Addi-tionally, investigating how the brain filters out noise or separates behaviorally-irrelevant factors is a promising research direction (<bold><italic><xref rid="c41" ref-type="bibr">Schneider et al., 2018</xref></italic></bold>); distilled behaviorally-relevant signals can serve as reliable reference signals to facilitate this study. Furthermore, our study reveals that ir-relevant signals are the most critical factor affecting accurate and robust decoding, and achieving accurate and robust linear decoding requires weak neural responses. These findings have two important implications for developing accurate and robust BMIs: designing preprocessing filter-ing algorithms or developing decoding algorithms that include filtering out behaviorally-irrelevant signals, and paying attention to the role of weak neural responses in motor control. More gener-ally, our study provides a powerful framework for separating behaviorally-relevant and irrelevant signals, which can be applied to other cortical data to uncover more hidden neural mechanisms.</p>
</sec>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Dataset and preprocessing</title>
<p>Three datasets with different paradigms are employed, including obstacle avoidance task dataset (<bold><italic><xref rid="c45" ref-type="bibr">Wang et al., 2015</xref></italic></bold>), center-out reaching task dataset (<bold><italic><xref rid="c9" ref-type="bibr">Dyer et al., 2017</xref></italic></bold>), and self-paced reaching task dataset (<bold><italic><xref rid="c32" ref-type="bibr">O’Doherty et al., 2017</xref></italic></bold>).</p>
<p>The first dataset is the obstacle avoidance dataset. An adult male Rhesus monkey was trained to use the joystick to move the computer cursor to bypass the obstacle and reach the target. Neu-ral data were recorded from the monkey’s upper limb area of the dorsal premotor (PMd) using a 96-electrode Utah array (Blackrock Microsystems Inc., USA). Multi-unit activity (MUA) signals are used in the present study. The corresponding behavioral data (velocity) were simultaneously col-lected. There are two days of data (20140106 and 20140107), and each day contains 171 trials on average. All animal handling procedures were authorized by the Animal Care Committee at Zhe-jiang University, China, and conducted following the Guide for Care and Use of Laboratory Animals (China Ministry of Health).</p>
<p>The second dataset is publicly available and provided by Kording Lab (<bold><italic><xref rid="c9" ref-type="bibr">Dyer et al., 2017</xref></italic></bold>). The monkey was trained to complete two-dimensional 8-direction center-out reaching tasks. We used two days of data from subject C (20161007 and 20161011). Each day contains 190 trials on average. Neural data are spike-sorted PMd signals. The behavioral data were simultaneously collected in instantaneous velocity.</p>
<p>The third dataset is publicly available and provided by Sabes Lab (Zenodo dataset) (<bold><italic><xref rid="c32" ref-type="bibr">O’Doherty et al., 2017</xref></italic></bold>). An adult male Rhesus monkey was trained to finish self-paced reaching tasks within an 8-by-8 square grid. There are no inter-trial intervals during the experiment. Neural data were recorded from the monkey’s primary motor cortex (M1) area with a 96-channel silicon microelec-trode array. The neural data are the multi-unit activity (MUA) signals. Hand velocity was obtained from the position through a discrete derivative. The recording period for the data (20170124 01) is about 10 minutes.</p>
<p>For all datasets, the neural signals were binned by a 100ms sliding window without overlap. As a preprocess, we smoothed the neural signals using a moving average filter with three bins. We excluded some electrode activities with low mean firing rates (<italic>&lt;</italic>0.5 Hz mean firing rates across all bins) and did not perform any other pre-selection to select neurons. For the computation of the Fano factor, we chose twelve and fourteen points as the thresholds of trial length for datasets A and B, respectively; trials with a length less than the threshold were discarded (discard about 7% and 2% trials for datasets A and B), trials longer than the threshold were truncated to threshold length from the starting point. Since dataset C has no trial information, FF is not calculated for this dataset. For the analysis of datasets A and B, we selected one day of these two datasets for analysis (20140107 for dataset A and 20161011 for dataset B).</p>
</sec>
<sec id="s4b">
<title>The synthetic dataset</title>
<p>The synthetic dataset is used to demonstrate that d-VAE can extract effective behaviorally-relevant signals that are similar to the ground truth signals. The specific process of generating synthetic data is as follows. First, we selected the top ten R<sup>2</sup> neurons of dataset B (20161011). Second, we used deep neural networks to learn the encoding model between movement kinematics (movement velocity of dataset B) and neural signals. The details of the networks are demonstrated as follows. The networks use two hidden layer multilayer perceptron (MLP) with 500 and 500 hidden units. The activation function is ReLU. A SoftPlus activation function follows the last layer of the networks. The reconstruction loss is the Poisson likelihood function. After learning the encoding model, we used the learned encoding model to generate the ground truth of behaviorally-relevant signals from kinematics. Then we added white Gaussian noise to the behaviorally-relevant signals such that the noisy signals have a signal-to-noise ratio (SNR) of 10dB. We regarded the noisy signals as raw signals and the added Gaussian noise as behaviorally-irrelevant signals. Finally, we separated the synthetic data into five folds for cross-validation model evaluation.</p>
</sec>
<sec id="s4c">
<title>Simulation experiments</title>
<p>In this experiment, we simulated four neurons, including two larger R<sup>2</sup> neurons (greater than 0.4) and two smaller R<sup>2</sup> neurons (lower than 0.03). The larger R<sup>2</sup> neurons are strongly tuned neurons and are added with small Gaussian noise. In contrast, the smaller R<sup>2</sup> neurons strongly nonlinearly encode behavioral information and are added with large Gaussian noise. We regarded the signals without adding Gaussian noise as ground-truth signals and those with Gaussian signals as raw signals. We used the velocity of dataset B (20161011) for simulation. The simulation dataset A includes the four neurons, which simulates the situation where the smaller R<sup>2</sup> neurons contain a lot of behavioral information, but the behavioral information cannot be decoded from these neurons due to being covered by noise. The simulation dataset B includes the two larger R<sup>2</sup> neurons, which simulates the situation where linear decoding is significantly inferior to nonlinear decoding.</p>
<p>The encoding model of larger R<sup>2</sup> neurons is as follows:
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="515644v4_ueqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where the subscript <italic>i</italic> refers to the <italic>i</italic><sup>th</sup> unit of <italic>N</italic> neurons, <italic>N</italic> = 2 in this experiment, <italic>t</italic> represents time, <italic>b</italic> = 30 Hz represents the baseline firing rate of neurons, <italic>m</italic> = 0.25 Hz per cm s<sup>−1</sup> indicates the modulation depth, the PDs of neurons <italic>θ</italic><sub><italic>PDi</italic></sub>(<italic>i</italic>=1, 2) are set to 30<sup>0</sup> and 60<sup>0</sup>, ∊<sub><italic>t</italic></sub>(<italic>t</italic>) = <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline6.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>(0, 0.3) denotes random Gaussian noise where the mean is zero and the standard deviation is 0.3, <bold>v</bold> represents velocity, | <bold>v</bold> | represents the norm of velocity (speed), <italic>b</italic><sub><italic>s</italic></sub> = 0.25 Hz per cm s<sup>−1</sup> denotes speed bias.</p>
<p>The encoding model of smaller R<sup>2</sup> neurons is as follows:
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="515644v4_ueqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>b</italic> = 30 Hz, <italic>m</italic> = 0.25 Hz per cm s<sup>−1</sup>, the PDs of neurons <italic>0</italic><sub><italic>PDi</italic></sub>(<italic>i</italic>=1, 2) are set to 40<sup>0</sup> and 50<sup>0</sup>, ∊<sub><italic>i</italic></sub>(<italic>t</italic>) = <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline7.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>(0, 1) denotes random Gaussian noise where the mean is zero and the standard deviation is one, <italic>b</italic><sub><italic>s</italic></sub> = 0.25 Hz per cm s<sup>−1</sup>, <italic>b</italic><sub><italic>ss</italic></sub> = 0.1 Hz per cm<sup>2</sup> s<sup>−1</sup> denotes the bias of speed square for increasing the encoding nonlinearty of neurons.</p>
</sec>
<sec id="s4d">
<title>Variational autoencoders</title>
<p>Variational autoencoders (VAEs) (<bold><italic><xref rid="c24" ref-type="bibr">Kingma and Welling, 2013</xref></italic></bold>) are famous latent variable generative models and can use latent representations <bold>z</bold> ∼ <italic>p</italic>(<bold>z</bold>) to generate data <bold>x</bold> by conditional distribution <italic>p</italic><sub><italic>θ</italic></sub>(<bold>x</bold> | <bold>z</bold>), that is, <italic>p</italic><sub><italic>θ</italic></sub>(<bold>x</bold>) = <italic>∫</italic><sub><bold>z</bold></sub><italic>p</italic>(<bold>z</bold>)<italic>p</italic><sub><italic>θ</italic></sub>(<bold>x</bold> | <bold>z</bold>)d<bold>z</bold>. The usual training objective is maximum marginal likeli-hood log <italic>p</italic><sub><italic>θ</italic></sub>(<bold>x</bold>). Since directly computing the integration is intractable, VAE propose an amortized inference distribution <italic>q</italic><sub><italic>ф</italic></sub>(<bold>z</bold> | <bold>x</bold>) and jointly optimize the evidence lower bound (ELBO) to the log-likelihood:
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="515644v4_ueqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where the first term of right hand side of <xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref> is called reconstruction term, the second term is called regularization term, <italic>q</italic><sub><italic>ф</italic></sub>(<bold>z</bold> | <bold>x</bold>) is usually parameterized with a neural network called encoder or inference model with parameters, <sub><italic>ф</italic></sub> and <italic>p</italic><sub><italic>θ</italic></sub>(<bold>z</bold> | <bold>x</bold>) is also parameterized with a neural network called decoder or generative model with parameters <italic>θ</italic>. <italic>p</italic>(<bold>z</bold>) is the prior over the latent representa-tions and is set to be Gaussian distribution. <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline8.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>(<bold>0</bold>, <bold>I</bold>) in vallina VAE, <italic>D<sub>KL</sub></italic> denotes the Kullback-Leibler divergence.</p>
</sec>
<sec id="s4e">
<title>Distill-VAE (d-VAE)</title>
<p><bold>Notations. x</bold> ∊ ℝ<italic><sup>n</sup></italic> denotes raw neural signals. <bold>x</bold><sub><italic>r</italic></sub> ∊ ℝ<italic><sup>n</sup></italic> represents behaviorally-relevant signals. <bold>x</bold><sub><italic>i</italic></sub> = <bold>x</bold> − <bold>x</bold><sub><italic>r</italic></sub> ∊ ℝ<italic><sup>n</sup></italic> represents behaviorally-irrelevant signals. <bold>z</bold> ∊ ℝ<italic><sup>d</sup></italic> denotes the latent neural repre-sentations. <bold>y</bold> ∊ ℝ<italic><sup>k</sup></italic> represents kinematics. <italic>f</italic>: ℝ<italic><sup>n</sup></italic> → ℝ<italic><sup>d</sup></italic> represents the inference model (encoder) of VAE. <italic>g</italic>: ℝ<italic><sup>d</sup></italic> → ℝ<italic><sup>n</sup></italic> represents the generative model (decoder) of VAE.</p>
<p>To distill behaviorally-relevant neural signals, our approach utilizes the trade-off between the decoding and generating abilities of distilled behaviorally-relevant signals <bold>x</bold><sub><italic>r</italic></sub>. The basic assump-tion is generated behaviorally-relevant signals that contain behaviorally-irrelevant signals harms decoding ability. Our method for distilling behaviorally-relevant signals consists of three steps, including identifying latent representations <bold>z</bold>, generating behaviorally-relevant signals <bold>x</bold><sub><italic>r</italic></sub>, and de-coding behaviorally-relevant signals <bold>x</bold><sub><italic>r</italic></sub>.</p>
<sec id="s4f">
<title>Identfiying latent representations</title>
<p>Identifying latent representations containing behaviorally-relevant information is the crucial part because latent representations influences the subsequent generation. Effective representations are more likely to generate proper behaviorally-relevant neural signals <bold>x</bold><sub><italic>r</italic></sub>. d-VAE identifies latent representations with inference model <italic>f</italic>, that is, <bold>z</bold> = <italic>f</italic>(<bold>x</bold>), and guides latent representations con-taining behavioral information through an afine map <italic>h</italic>: ℝ<italic><sup>d</sup></italic> → ℝ<italic><sup>k</sup></italic> under the loss <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline9.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula><sub><italic>dec</italic>1</sub>,
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="515644v4_ueqn4.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where MSE(·, ·) denotes mean squared loss. In other words, we encourage latent representations to decode kinematics to distill behaviorally-relevant information.</p>
</sec>
<sec id="s4g">
<title>Generating behaviorally-relevant signals</title>
<p>After identifying latent representations <bold>z</bold>, we send latent representations to the generative model <italic>g</italic> to generate behaviorally-relevant neural signals <bold>x</bold><sub><italic>r</italic></sub>, that is, <bold>x</bold><sub><italic>r</italic></sub> = <italic>g</italic>(<bold>z</bold>). We use following loss to make behaviorally-relevant signals reconstruct raw signals as much as possible:
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="515644v4_ueqn5.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where Poisson(·, ·) denotes Poisson negative log likelihood loss. It is important to note that optimiz-ing the generation of behaviorally-relevant signals to accurately reconstruct noisy raw signals may result in the inclusion of many behaviorally-irrelevant signals in the generated signals, which de-viates from our initial goal of extracting behaviorally-relevant signals. In the following subsection, we will introduce how to avoid generating behaviorally-irrelevant signals.</p>
</sec>
<sec id="s4h">
<title>Decoding behaviorally-relevant signals</title>
<p>As mentioned above, if the generation of behaviorally-relevant signals <bold>x</bold><sub><italic>r</italic></sub> is only guided by <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline10.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula><sub><italic>rec</italic></sub>, gen-erated signals may contain more behaviorally-irrelevant signals. To avoid generated signals con-taining behaviorally-irrelevant signals, we introduce decoding loss <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline11.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula><sub><italic>dec</italic>2</sub> to constrain <bold>x</bold><sub><italic>r</italic></sub> to decode behavioral information. The basic assumption is that behaviorally-irrelevant signals act like noise for decoding behavioral information and are detrimental to decoding. Thus, there is a trade-off between generating and decoding ability of <bold>x</bold><italic><sub>r</sub></italic>: the more behaviorally-irrelevant signals <bold>x</bold><italic><sub>r</sub></italic> contains, the more decoding performance <bold>x</bold><italic><sub>r</sub></italic> loses. The decoding loss <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline12.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula><italic><sub>dec</sub></italic><sub>2</sub> is as follows:
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="515644v4_ueqn6.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We use the same networks <italic>f</italic> and <italic>h</italic> for <bold>x</bold><italic><sub>r</sub></italic> and <bold>x</bold> in our experiment, because <bold>x</bold><italic><sub>r</sub></italic> can act as data augmentation and make <italic>f</italic> distill robust representations without increasing model parameters. In addition, we combine the two decoding loss as one loss:
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="515644v4_ueqn7.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
</p>
</sec>
<sec id="s4i">
<title>Learning the prior distribution with behavioral information</title>
<p>The prior distribution of latent representation is crucial because inappropriate prior assumptions can degrade latent representations and generated neural signals. Vanilla VAE uses a Gaussian prior. <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline13.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>(<bold>0</bold>, <bold>I</bold>) to regularize the space of latent representation <bold>z</bold>. However, in neuroscience, the distribu-tion of latent representations is unknown and may exceed the scope of Gaussian. Therefore we adopt neural networks <italic>m</italic>: ℝ <italic><sup>k</sup></italic> → ℝ <italic><sup>d</sup></italic> to learn the prior representation <bold>z</bold><italic><sub>prior</sub></italic> with kinematics <bold>y</bold>, that is, <bold>z</bold><italic><sub>prior</sub></italic> = <italic>m</italic>(<bold>y</bold>). The prior representation <bold>z</bold><italic><sub>prior</sub></italic> and <bold>z</bold> are aligned by the Kullback–Leibler divergence:
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="515644v4_ueqn8.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Since <bold>z</bold><sub><italic>prior</italic></sub> and <bold>z</bold> are aligned and the generative network <italic>g</italic> models the relationship between <bold>z</bold> and <bold>x</bold><sub><italic>r</italic></sub>, this is equivalent to indirectly establishing a neural encoding model from <bold>y</bold> to <bold>x</bold><sub><italic>r</italic></sub>. Thus, we can observe the change of <bold>z</bold><sub><italic>prior</italic></sub> and <bold>x</bold><sub><italic>r</italic></sub> by changing <bold>y</bold> and can better understand the encoding mechanism of neural signals.</p>
</sec>
<sec id="s4j">
<title>End-to-end optimization</title>
<p>d-VAE is optimized in an end-to-end manner under the following loss:
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="515644v4_ueqn9.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>β</italic> and <italic>a</italic> are hyperparameters, <italic>β</italic> is used to adjust the weight of KL divergence, and <italic>a</italic> deter-mines the trade-off between reconstruction loss <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline14.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula><italic><sub>rec</sub></italic> and decoding loss <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline15.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula><italic><sub>dec</sub></italic>.</p>
<p>In the training stage, we feed raw neural signals into the inference network <italic>f</italic> to get latent representation <bold>z</bold>, which is regularized by <bold>z</bold><sub><italic>prior</italic></sub> coming from kinematics <bold>y</bold> and network <italic>m</italic>. Then we use <bold>z</bold> to decode kinematics <bold>y</bold> by afine layer <italic>h</italic> and send <bold>z</bold> to the generative networks <italic>g</italic> to generate neural signals <bold>x</bold><sub><italic>r</italic></sub>. To ensure that <bold>x</bold><sub><italic>r</italic></sub> preserves decoding ability, we send <bold>x</bold><sub><italic>r</italic></sub> to <italic>f</italic> and <italic>h</italic> to decode <bold>y</bold>. The whole model is trained in an end-to-end manner under the guidance of total loss. Once the model has been trained, we can feed raw neural signals to it to obtain behaviorally-relevant neural signals <bold>x</bold><sub><italic>r</italic></sub>, and we can aslo generate behaviorally-relevant neural signals using the prior distribution <bold>z</bold><sub><italic>prior</italic></sub>.</p>
</sec>
<sec id="s4k">
<title>The identifiability of d-VAE</title>
<p>Recent studies (<bold><italic><xref rid="c23" ref-type="bibr">Khemakhem et al., 2020</xref></italic></bold>) show that to obtain the identifiability of latent variables, we should either restrict the structures of generating models or introduce some additional con-straints on the distribution of latent variables and show that the additive noise models are identifi-able under certain assumptions. pi-VAE (<bold><italic><xref rid="c53" ref-type="bibr">Zhou and Wei, 2020</xref></italic></bold>) has proved that pi-VAE is identifiable under some assumptions. Since d-VAE is a straightforward extension of pi-VAE, it also uses auxiliary variables (kinematics) to constrain the latent variables as pi-VAE did; with the same assumptions, d-VAE is also identifiable.</p>
</sec>
</sec>
<sec id="s4l">
<title>Cross-validation evaluation of models</title>
<p>For each model, we use the five-fold cross-validation manner to assess performance. Specifically, we divide the data into five equal folds. In each experiment, we take one fold for testing and use the rest for training (taking three folds as the training set and one as the validation set). The reported performance is averaged over test sets of five experiments. The validation sets are used to choose hyperparameters. To avoid overfitting, we apply the early stopping strategy. Specifically, we assess the criteria (loss for training distillation methods, R<sup>2</sup> for training ANN and KF) on the validation set every epoch in the training process. The model is saved when the model achieves better validation performance than the earlier epochs. If the model cannot increase by 1% of the best performance previously obtained within specified epochs, we stop the training process.</p>
</sec>
<sec id="s4m">
<title>The strategy for selecting effective behaviorally-relevant signals</title>
<p>As previously mentioned, the hyperparameter <italic>a</italic> plays a crucial role in balancing the trade-off be-tween reconstruction and decoding loss. Once the appropriate value of <italic>a</italic> is determined, we can use this value to obtain accurate behaviorally-relevant signals for subsequent analysis. To deter-mine the optimal value of <italic>a</italic>, we first enumerated different values of <italic>a</italic> to guide d-VAE in distilling the behaviorally-relevant signals. Next, we used ANN to evaluate the decoding R<sup>2</sup> of behaviorally-relevant (D<sub>re</sub>) and irrelevant (D<sub>ir</sub>) signals generated by each <italic>a</italic> value. Finally, we selected the <italic>a</italic> value with the criteria formula 0.75 × D<sub>re</sub> + 0.25 × (1 − D<sub>ir</sub>). The <italic>a</italic> value that obtained the highest criteria score on the validation set was selected as the optimal value.</p>
<p>Note that we did not use neural similarity between behaviorally-relevant and raw signals as a criterion for selecting behaviorally-relevant signals. This is because determining the threshold for neural similarity is challenging. However, not using similarity as a criterion does not affect the se-lection of suitable signals because the decoding performance of behaviorally-irrelevant signals can indirectly reflect the degree of similarity between the generated behaviorally-relevant signals and the raw signals. Specifically, if the generated behaviorally-relevant signals are dissimilar to the raw signals, the behaviorally-irrelevant signals will contain many useful signals. In other words, when the neural similarity between behaviorally-relevant and raw signals is low, the decoding perfor-mance of behaviorally-irrelevant signals is high. Therefore, the decoding performance of irrelevant signals is a reasonable alternative to the neural similarity.</p>
</sec>
<sec id="s4n">
<title>Implementation details for methods</title>
<p>All the VAE-based models use the Poisson observation function. The details of different methods are demonstrated as follows:
<list list-type="bullet">
<list-item><p>d-VAE. The encoder of d-VAE uses two hidden layer multilayer perceptron (MLP) with 300 and 100 hidden units. The activation function of the hidden layers is ReLu. The dimension of the latent variable is set to 50. The decoder of d-VAE is symmetric with the encoder. The last layer of the decoder is followed by a SoftPlus activation function. The prior networks use one hidden layer MLP with 300 units. The <italic>β</italic> is set to 0.001. The <italic>a</italic> is set to 0.3, 0.4, 0.7, and 0.9 for datasets A, B, and C and synthetic dataset.</p></list-item>
<list-item><p>pi-VAE. The original paper utilizes label information (as shown in <xref ref-type="disp-formula" rid="eqn3">Eq. 6</xref>) to approximate the posterior of the latent variable and performs Monte Carlo sampling for decoding during the test stage. However, in our signal generation setting, it is inappropriate to use label informa-tion (kinematics) for extracting behaviorally-relevant signals during the test stage. As a result, we modified the model to exclude the use of label information in approximating the poste-rior. The encoder, decoder, and prior networks of our pi-VAE are kept the same as those in d-VAE.</p></list-item>
<list-item><p>VAE. The parameters of the encoder and decoder of VAE are the same as d-VAE.</p></list-item>
<list-item><p>LFADS. The hidden units of the encoder for the generator’s initial conditions, the controller, the generator are set to 200. The dimensionality of latent factor is set to 50. The dimension-ality of inferred inputs is set to 1. The training strategy follows the practice of the original paper.</p></list-item>
<list-item><p>PSID. The horizon parameter of PSID is set to 3, and the dimensionality of latent variable is set to 6.</p></list-item>
<list-item><p>CEBRA. We use CEBRA-behavior mode. The learning rate is set to 0.003, 0.003, 0.001, and 0.005 for datasets A, B, and C and synthetic dataset. The time offset is set to 10, 10, 15 and 10. The output dimension is set to 10 for all datasets. The number of iterations is set to 5000. The temperature is set to 1, and the temperature mode is set to auto. The batch size is set to 512. We perform a grid search for the learning rate in {0.0001, 0.0005, 0.001, 0.003, 0.005}, the time offset in {5, 10, 15, 20}, and the output dimension in {5, 10, 15, 20, 50}.</p></list-item>
<list-item><p>ANN. ANN has two hidden layers with 300 and 100 hidden units.</p></list-item>
<list-item><p>KF. The matrix parameters of observation and state transition process are optimized with the least square algorithm.</p></list-item>
</list>
</p>
</sec>
<sec id="s4o">
<title>Percentage of explained variance captured in a subspace</title>
<p>We applied PCA to behaviorally-relevant and irrelevant signals to get relevant PCs and irrelevant PCs. Then we used the percentage variance captured (also called alignment index) to quantify how many variances of irrelevant signals can be captured by relevant PCs by projecting irrelevant signals onto the subspace composed of some relevant PCs and vice versa. The percentage of variance captured is:
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="515644v4_ueqn10.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <bold>D</bold> ∊ ℝ<italic><sup>N</sup></italic><sup>×<italic>d</italic></sup> is a <italic>d</italic>-dimensional population basis over <italic>N</italic> neurons, <bold>C</bold> ∊ ℝ<italic><sup>N</sup></italic><sup>×<italic>N</italic></sup> is the covariance matrix of the neural signals, and Tr <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline16.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, where <italic>λ</italic><sub><italic>i</italic></sub> is the <italic>i</italic><sup>th</sup> largest eigenvalue for <bold>C</bold>. The matrices <bold>D</bold> and <bold>C</bold> can be computed for both behaviorally-irrelevant and relevant signals. The per-centage variance is a quantity between 0% and 100%.</p>
</sec>
<sec id="s4p">
<title>The composition of raw signals’ variance</title>
<p>Suppose <bold>x</bold> ∊ ℝ<sup>1×<italic>T</italic></sup>, <bold>y</bold> ∊ ℝ<sup>1×<italic>T</italic></sup>, and <bold>z</bold> ∊ ℝ<sup>1×<italic>T</italic></sup> are the random variables for a single neuron of behaviorally-relevant signals, behaviorally-irrelevant, and raw signals, where <bold>z</bold> = <bold>x</bold> + <bold>y</bold>, <italic>T</italic> denotes the number of samples. The composition of raw signals’ variance is as follows:
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="515644v4_ueqn11.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline17.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, Var, and Cov denote expectation, variance, and covariance, respectively. Thus, the vari-ance of raw signals is composed by the variance of relevant signals Var(<bold>x</bold>), the variance of irrelevant signals Var(<bold>y</bold>), and the correlation between relevant and irrelevant signals 2Cov(x, <bold>y</bold>). If there are <italic>N</italic> neurons, calculate the composition of each neuron separately and then add it up to get the total composition of raw signals.</p>
</sec>
<sec id="s4q">
<title>Reordered correlation matrix of neurons</title>
<p>The correlation matrix is reordered with a simple group method. The order of reordered neurons is determined from raw neural signals, which is then used for behaviorally-relevant signals.</p>
<p>The steps of the group method are as follows:</p>
<p>Step 1: We get the correlation matrix in original neuron order and set a list A that contains all neuron numbers and an empty list B.</p>
<p>Step 2: We select the neuron with the row number of the largest value of the correlation matrix except for the diagonal line and choose nine neurons in list A that are most similar to the selected neuron.</p>
<p>Step 3: Remove these selected neurons from list A, and add these selected neurons in descend-ing order of correlation value in list B.</p>
<p>Step 4: Repeat steps 2 and 3 until list A is empty.</p>
</sec>
<sec id="s4r">
<title>The improvement ratio of lower and higher speed regions</title>
<sec id="s75">
<title>Split lower and higher speed regions</title>
<p>Since the speed ranges of different datasets are different, it is hard to determine a common speed threshold to split lower and higher speed regions. Here we used the squared error as a criterion to split the two speed regions. And for the convenience of calculating the absolute improvement ratio, we need a unified benchmark for comparison. Therefore, we use half of the total squared error as the threshold. Specifically, first, we calculated all samples’ total squared error (<italic>E</italic><sub><italic>p</italic></sub>) between actual velocity and predicted velocity obtained by primary signals only. Then we enumerated the speed from 1 to 50 with a step of 0.1 and calculated the total squared error of selected samples whose speed is less than the enumerated speed. Once the total squared error of selected samples is greater than or equal to the half total squared error of all samples (0.5<italic>E</italic><sub><italic>p</italic></sub>), the enumerated speed is set as the speed threshold. The samples whose speed is less than or equal to the speed threshold belong to lower speed regions, and those whose speed is greater than the speed threshold belong to higher speed regions. The squared error of the lower speed part <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline18.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is approximately equal to that of the higher one <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline19.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, that is, <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline20.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> (the difference is negligible).</p>
</sec>
<sec id="s4s">
<title>The absolute improvement ratio</title>
<p>After splitting the speed regions, we calculated the improvement of the two regions by superim-posing secondary signals to primary signals, and got the squared error of lower <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline21.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and higher <inline-formula><alternatives><inline-graphic xlink:href="515644v4_inline22.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> regions. Then we calculated the absolute improvement ratio (AIR):
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="515644v4_ueqn12.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="515644v4_ueqn13.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula></p>
<p>Since the two regions refer to a common standard (0.5<italic>E</italic><sub><italic>p</italic></sub>), the improvement ratio of the two regions can be directly compared, and that’s why we call it the absolute improvement ratio.</p>
</sec>
<sec id="s4t">
<title>The relative improvement ratio</title>
<p>The relative improvement ratio (RIR) measures the improvement ratio of each sample relative to itself before and after superimposing secondary signals. The relative improvement ratio is com-puted as follows:
<disp-formula id="eqn14">
<alternatives><graphic xlink:href="515644v4_ueqn14.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>i</italic> denotes the <italic>i</italic>th sample.</p>
</sec>
</sec>
</sec>
<sec id="s5">
<title>Data availability</title>
<p>The datasets are available for research purposes from the corresponding author on reasonable request.</p>
</sec>
<sec id="s6">
<title>Code availability</title>
<p>The code will be publically available in github (see <ext-link ext-link-type="uri" xlink:href="https://github.com/eric0li/d-VAE">https://github.com/eric0li/d-VAE</ext-link>).</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We would like to thank Yuxiao Yang, and Huaqin Sun for valuable discussions. This work was partly supported by grants from the National Key R&amp;D Program of China (2018YFA0701400), Key R&amp;D Program of Zhejiang (no. 2022C03011), the Chuanqi Research and Development Center of Zhejiang University, the Starry Night Science Fund of Zhejiang University Shanghai Institute for Advanced Study (SN-ZJU-SIAS-002), the Fundamental Research Funds for the Central Universities.</p>
</ack>
<sec id="s7">
<title>Author contributions</title>
<p>Y.G.L. conceived the study, designed the experiments, analyzed the data, and wrote the manuscript.</p>
<p>X.Y.Z. designed correlation analysis experiments and modified the manuscript. Y.M.W. and Y.Q. supervised this study.</p>
</sec>
<sec id="s8">
<title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Alstott</surname> <given-names>J</given-names></string-name>, <string-name><surname>Breakspear</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hagmann</surname> <given-names>P</given-names></string-name>, <string-name><surname>Cammoun</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sporns</surname> <given-names>O</given-names></string-name>. <article-title>Modeling the impact of lesions in the human brain</article-title>. <source>PLoS computational biology</source>. <year>2009</year>; <volume>5</volume>(<issue>6</issue>):<fpage>e1000408</fpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Altan</surname> <given-names>E</given-names></string-name>, <string-name><surname>Solla</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Perreault</surname> <given-names>EJ</given-names></string-name>. <article-title>Estimating the dimensionality of the manifold underlying multi-electrode neural recordings</article-title>. <source>PLoS computational biology</source>. <year>2021</year>; <volume>17</volume>(<issue>11</issue>):<fpage>e1008591</fpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Azouz</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gray</surname> <given-names>CM</given-names></string-name>. <article-title>Cellular mechanisms contributing to response variability of cortical neurons in vivo</article-title>. <source>Journal of Neuroscience</source>. <year>1999</year>; <volume>19</volume>(<issue>6</issue>):<fpage>2209</fpage>–<lpage>2223</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Carmena</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Lebedev</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Henriquez</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Nicolelis</surname> <given-names>MA</given-names></string-name>. <article-title>Stable ensemble performance with single-neuron vari-ability during reaching movements in primates</article-title>. <source>Journal of Neuroscience</source>. <year>2005</year>; <volume>25</volume>(<issue>46</issue>):<fpage>10712</fpage>–<lpage>10716</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Churchland</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Cunningham</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Kaufman</surname> <given-names>MT</given-names></string-name>, <string-name><surname>Foster</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Nuyujukian</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ryu</surname> <given-names>SI</given-names></string-name>, <string-name><surname>Shenoy</surname> <given-names>KV</given-names></string-name>. <article-title>Neural population dynamics during reaching</article-title>. <source>Nature</source>. <year>2012</year>; <volume>487</volume>(<issue>7405</issue>):<fpage>51</fpage>–<lpage>56</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Churchland</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Cunningham</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Sugrue</surname> <given-names>LP</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Corrado</surname> <given-names>GS</given-names></string-name>, <string-name><surname>Newsome</surname> <given-names>WT</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Hosseini</surname> <given-names>P</given-names></string-name>, <string-name><surname>Scott</surname> <given-names>BB</given-names></string-name>, <etal>et al.</etal> <article-title>Stimulus onset quenches neural variability: a widespread cortical phenomenon</article-title>. <source>Nature neuroscience</source>. <year>2010</year>; <volume>13</volume>(<issue>3</issue>):<fpage>369</fpage>–<lpage>378</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Cunningham</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>BM</given-names></string-name>. <article-title>Dimensionality reduction for large-scale neural recordings</article-title>. <source>Nature neuroscience</source>. <year>2014</year>; <volume>17</volume>(<issue>11</issue>):<fpage>1500</fpage>–<lpage>1509</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Dhawale</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Ölveczky</surname> <given-names>BP</given-names></string-name>. <article-title>The role of variability in motor learning</article-title>. <source>Annual review of neuroscience</source>. <year>2017</year>; <volume>40</volume>:<issue>479</issue>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Dyer</surname> <given-names>EL</given-names></string-name>, <string-name><surname>Gheshlaghi Azar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Perich</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Fernandes</surname> <given-names>HL</given-names></string-name>, <string-name><surname>Naufel</surname> <given-names>S</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Körding</surname> <given-names>KP</given-names></string-name>. <article-title>A cryptography-based approach for movement decoding</article-title>. <source>Nature biomedical engineering</source>. <year>2017</year>; <volume>1</volume>(<issue>12</issue>):<fpage>967</fpage>–<lpage>976</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Elsayed</surname> <given-names>GF</given-names></string-name>, <string-name><surname>Lara</surname> <given-names>AH</given-names></string-name>, <string-name><surname>Kaufman</surname> <given-names>MT</given-names></string-name>, <string-name><surname>Churchland</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Cunningham</surname> <given-names>JP</given-names></string-name>. <article-title>Reorganization between preparatory and movement population responses in motor cortex</article-title>. <source>Nature communications</source>. <year>2016</year>; <volume>7</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Faisal</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Selen</surname> <given-names>LP</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>. <article-title>Noise in the nervous system</article-title>. <source>Nature reviews neuroscience</source>. <year>2008</year>; <volume>9</volume>(<issue>4</issue>):<fpage>292</fpage>– <lpage>303</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Fusi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Rigotti</surname> <given-names>M</given-names></string-name>. <article-title>Why neurons mix: high dimensionality for higher cognition</article-title>. <source>Current opinion in neurobiology</source>. <year>2016</year>; <volume>37</volume>:<fpage>66</fpage>–<lpage>74</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Gallego</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Perich</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Chowdhury</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Solla</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>LE</given-names></string-name>. <article-title>Long-term stability of cortical population dynamics underlying consistent behavior</article-title>. <source>Nature neuroscience</source>. <year>2020</year>; <volume>23</volume>(<issue>2</issue>):<fpage>260</fpage>–<lpage>270</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Gallego</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Perich</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Solla</surname> <given-names>SA</given-names></string-name>. <article-title>Neural manifolds for the control of movement</article-title>. <source>Neuron</source>. <year>2017</year>; <volume>94</volume>(<issue>5</issue>):<fpage>978</fpage>–<lpage>984</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Gallego</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Perich</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Naufel</surname> <given-names>SN</given-names></string-name>, <string-name><surname>Ethier</surname> <given-names>C</given-names></string-name>, <string-name><surname>Solla</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>LE</given-names></string-name>. <article-title>Cortical population activity within a preserved neural manifold underlies multiple motor behaviors</article-title>. <source>Nature communications</source>. <year>2018</year>; <volume>9</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Georgopoulos</surname> <given-names>AP</given-names></string-name>, <string-name><surname>Schwartz</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Kettner</surname> <given-names>RE</given-names></string-name>. <article-title>Neuronal population coding of movement direction</article-title>. <source>Science</source>. <year>1986</year>; <volume>233</volume>(<issue>4771</issue>):<fpage>1416</fpage>–<lpage>1419</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Glaser</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Benjamin</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Chowdhury</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Perich</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Kording</surname> <given-names>KP</given-names></string-name>. <article-title>Machine learning for neural decoding</article-title>. <source>Eneuro</source>. <year>2020</year>; <volume>7</volume>(<issue>4</issue>).</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Hennig</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Oby</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Golub</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Bahureksa</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Sadtler</surname> <given-names>PT</given-names></string-name>, <string-name><surname>Quick</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Ryu</surname> <given-names>SI</given-names></string-name>, <string-name><surname>Tyler-Kabara</surname> <given-names>EC</given-names></string-name>, <string-name><surname>Batista</surname> <given-names>AP</given-names></string-name>, <string-name><surname>Chase</surname> <given-names>SM</given-names></string-name>, <etal>et al.</etal> <article-title>Learning is shaped by abrupt changes in neural engagement</article-title>. <source>Nature Neuroscience</source>. <year>2021</year>; <volume>24</volume>(<issue>5</issue>):<fpage>727</fpage>– <lpage>736</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Hochberg</surname> <given-names>LR</given-names></string-name>, <string-name><surname>Bacher</surname> <given-names>D</given-names></string-name>, <string-name><surname>Jarosiewicz</surname> <given-names>B</given-names></string-name>, <string-name><surname>Masse</surname> <given-names>NY</given-names></string-name>, <string-name><surname>Simeral</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Vogel</surname> <given-names>J</given-names></string-name>, <string-name><surname>Haddadin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cash</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Van Der Smagt</surname> <given-names>P</given-names></string-name>, <etal>et al.</etal> <article-title>Reach and grasp by people with tetraplegia using a neurally controlled robotic arm</article-title>. <source>Nature</source>. <year>2012</year>; <volume>485</volume>(<issue>7398</issue>):<fpage>372</fpage>–<lpage>375</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Inoue</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Mao</surname> <given-names>H</given-names></string-name>, <string-name><surname>Suway</surname> <given-names>SB</given-names></string-name>, <string-name><surname>Orellana</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schwartz</surname> <given-names>AB</given-names></string-name>. <article-title>Decoding arm speed during reaching</article-title>. <source>Nature communica-tions</source>. <year>2018</year>; <volume>9</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Jiang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Saggar</surname> <given-names>H</given-names></string-name>, <string-name><surname>Ryu</surname> <given-names>SI</given-names></string-name>, <string-name><surname>Shenoy</surname> <given-names>KV</given-names></string-name>, <string-name><surname>Kao</surname> <given-names>JC</given-names></string-name>. <article-title>Structure in neural activity during observed and executed move-ments is shared at the neural population level, not in single neurons</article-title>. <source>Cell reports</source>. <year>2020</year>; <volume>32</volume>(<issue>6</issue>):<fpage>108006</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Keshtkaran</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Sedler</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Chowdhury</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Tandon</surname> <given-names>R</given-names></string-name>, <string-name><surname>Basrai</surname> <given-names>D</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Sohn</surname> <given-names>H</given-names></string-name>, <string-name><surname>Jazayeri</surname> <given-names>M</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Pan-darinath</surname> <given-names>C</given-names></string-name>. <article-title>A large-scale neural network training framework for generalized estimation of single-trial popu-lation dynamics</article-title>. <source>Nature Methods</source>. <year>2022</year>; p. <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="book"><string-name><surname>Khemakhem</surname> <given-names>I</given-names></string-name>, <string-name><surname>Kingma</surname> <given-names>D</given-names></string-name>, <string-name><surname>Monti</surname> <given-names>R</given-names></string-name>, <string-name><surname>Hyvarinen</surname> <given-names>A</given-names></string-name>. <source>Variational autoencoders and nonlinear ica: A unifying frame-work</source>. <publisher-loc>In</publisher-loc>: <publisher-name><italic>International Conference on Artiicial Intelligence and Statistics</italic> PMLR</publisher-name>; <year>2020</year>. p. <fpage>2207</fpage>–<lpage>2217</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="other"><string-name><surname>Kingma</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Welling</surname> <given-names>M.</given-names></string-name> <article-title>Auto-encoding variational bayes</article-title>. <source>arXiv preprint arXiv:13126114</source>. <year>2013</year>;.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Kobak</surname> <given-names>D</given-names></string-name>, <string-name><surname>Brendel</surname> <given-names>W</given-names></string-name>, <string-name><surname>Constantinidis</surname> <given-names>C</given-names></string-name>, <string-name><surname>Feierstein</surname> <given-names>CE</given-names></string-name>, <string-name><surname>Kepecs</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mainen</surname> <given-names>ZF</given-names></string-name>, <string-name><surname>Qi</surname> <given-names>XL</given-names></string-name>, <string-name><surname>Romo</surname> <given-names>R</given-names></string-name>, <string-name><surname>Uchida</surname> <given-names>N</given-names></string-name>, <string-name><surname>Machens</surname> <given-names>CK</given-names></string-name>. <article-title>Demixed principal component analysis of neural population data</article-title>. <source>Elife</source>. <year>2016</year>; <volume>5</volume>:<issue>e10989</issue>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name>, <string-name><surname>Douglas</surname> <given-names>PK</given-names></string-name>. <article-title>Interpreting encoding and decoding models</article-title>. <source>Current opinion in neurobiology</source>. <year>2019</year>; <volume>55</volume>:<fpage>167</fpage>–<lpage>179</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Majaj</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>H</given-names></string-name>, <string-name><surname>Solomon</surname> <given-names>EA</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name>. <article-title>Simple learned weighted sums of inferior temporal neuronal fir-ing rates accurately predict human core object recognition performance</article-title>. <source>Journal of Neuroscience</source>. <year>2015</year>; <volume>35</volume>(<issue>39</issue>):<fpage>13402</fpage>–<lpage>13418</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Moreno-Bote</surname> <given-names>R</given-names></string-name>, <string-name><surname>Beck</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kanitscheider</surname> <given-names>I</given-names></string-name>, <string-name><surname>Pitkow</surname> <given-names>X</given-names></string-name>, <string-name><surname>Latham</surname> <given-names>P</given-names></string-name>, <string-name><surname>Pouget</surname> <given-names>A</given-names></string-name>. <article-title>Information-limiting correlations</article-title>. <source>Nature neuroscience</source>. <year>2014</year>; <volume>17</volume>(<issue>10</issue>):<fpage>1410</fpage>–<lpage>1417</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Musall</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kaufman</surname> <given-names>MT</given-names></string-name>, <string-name><surname>Juavinett</surname> <given-names>AL</given-names></string-name>, <string-name><surname>Gluf</surname> <given-names>S</given-names></string-name>, <string-name><surname>Churchland</surname> <given-names>AK</given-names></string-name>. <article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title>. <source>Nature neuroscience</source>. <year>2019</year>; <volume>22</volume>(<issue>10</issue>):<fpage>1677</fpage>–<lpage>1686</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Narayanan</surname> <given-names>NS</given-names></string-name>, <string-name><surname>Kimchi</surname> <given-names>EY</given-names></string-name>, <string-name><surname>Laubach</surname> <given-names>M</given-names></string-name>. <article-title>Redundancy and synergy of neuronal ensembles in motor cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2005</year>; <volume>25</volume>(<issue>17</issue>):<fpage>4207</fpage>–<lpage>4216</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Nogueira</surname> <given-names>R</given-names></string-name>, <string-name><surname>Rodgers</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Bruno</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Fusi</surname> <given-names>S</given-names></string-name>. <article-title>The geometry of cortical representations of touch in rodents</article-title>. <source>Nature Neuroscience</source>. <year>2023</year>; p. <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>O’Doherty</surname> <given-names>JE</given-names></string-name>, <string-name><surname>Cardoso</surname> <given-names>M</given-names></string-name>, <string-name><surname>Makin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Sabes</surname> <given-names>P</given-names></string-name>. <article-title>Nonhuman primate reaching with multichannel sensorimotor cortex electrophysiology</article-title>. <source>Zenodo</source> <ext-link ext-link-type="uri" xlink:href="http://doiorg/105281/zenodo">http://doiorg/105281/zenodo</ext-link>. <year>2017</year>; 583331.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Pagan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Urban</surname> <given-names>LS</given-names></string-name>, <string-name><surname>Wohl</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Rust</surname> <given-names>NC</given-names></string-name>. <article-title>Signals in inferotemporal and perirhinal cortex suggest an untangling of visual target information</article-title>. <source>Nature neuroscience</source>. <year>2013</year>; <volume>16</volume>(<issue>8</issue>):<fpage>1132</fpage>–<lpage>1139</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Pandarinath</surname> <given-names>C</given-names></string-name>, <string-name><surname>O’Shea</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jozefowicz</surname> <given-names>R</given-names></string-name>, <string-name><surname>Stavisky</surname> <given-names>SD</given-names></string-name>, <string-name><surname>Kao</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Trautmann</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Kaufman</surname> <given-names>MT</given-names></string-name>, <string-name><surname>Ryu</surname> <given-names>SI</given-names></string-name>, <string-name><surname>Hochberg</surname> <given-names>LR</given-names></string-name>, <etal>et al.</etal> <article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title>. <source>Nature methods</source>. <year>2018</year>; <volume>15</volume>(<issue>10</issue>):<fpage>805</fpage>–<lpage>815</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Pitkow</surname> <given-names>X</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Angelaki</surname> <given-names>DE</given-names></string-name>, <string-name><surname>DeAngelis</surname> <given-names>GC</given-names></string-name>, <string-name><surname>Pouget</surname> <given-names>A</given-names></string-name>. <article-title>How can single sensory neurons predict behavior?</article-title> <source>Neuron</source>. <year>2015</year>; <volume>87</volume>(<issue>2</issue>):<fpage>411</fpage>–<lpage>423</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Rigotti</surname> <given-names>M</given-names></string-name>, <string-name><surname>Barak</surname> <given-names>O</given-names></string-name>, <string-name><surname>Warden</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>, <string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Fusi</surname> <given-names>S</given-names></string-name>. <article-title>The importance of mixed selectivity in complex cognitive tasks</article-title>. <source>Nature</source>. <year>2013</year>; <volume>497</volume>(<issue>7451</issue>):<fpage>585</fpage>–<lpage>590</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Rouse</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Schieber</surname> <given-names>MH</given-names></string-name>. <article-title>Condition-dependent neural dimensions progressively shift during reach to grasp</article-title>. <source>Cell reports</source>. <year>2018</year>; <volume>25</volume>(<issue>11</issue>):<fpage>3158</fpage>–<lpage>3168</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Sadtler</surname> <given-names>PT</given-names></string-name>, <string-name><surname>Quick</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Golub</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Chase</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Ryu</surname> <given-names>SI</given-names></string-name>, <string-name><surname>Tyler-Kabara</surname> <given-names>EC</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Batista</surname> <given-names>AP</given-names></string-name>. <article-title>Neural constraints on learning</article-title>. <source>Nature</source>. <year>2014</year>; <volume>512</volume>(<issue>7515</issue>):<fpage>423</fpage>–<lpage>426</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Sani</surname> <given-names>OG</given-names></string-name>, <string-name><surname>Abbaspourazad</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>YT</given-names></string-name>, <string-name><surname>Pesaran</surname> <given-names>B</given-names></string-name>, <string-name><surname>Shanechi</surname> <given-names>MM</given-names></string-name>. <article-title>Modeling behaviorally relevant neural dynam-ics enabled by preferential subspace identification</article-title>. <source>Nature Neuroscience</source>. <year>2021</year>; <volume>24</volume>(<issue>1</issue>):<fpage>140</fpage>–<lpage>149</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Saxena</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cunningham</surname> <given-names>JP</given-names></string-name>. <article-title>Towards the neural population doctrine</article-title>. <source>Current opinion in neurobiology</source>. <year>2019</year>; <volume>55</volume>:<fpage>103</fpage>–<lpage>111</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Schneider</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Sundararajan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mooney</surname> <given-names>R</given-names></string-name>. <article-title>A cortical filter that learns to suppress the acoustic consequences of movement</article-title>. <source>Nature</source>. <year>2018</year>; <volume>561</volume>(<issue>7723</issue>):<fpage>391</fpage>–<lpage>395</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="other"><string-name><surname>Schneider</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>MW.</given-names></string-name> <article-title>Learnable latent embeddings for joint behavioral and neural analysis</article-title>. <source>arXiv preprint arXiv:220400673</source>. <year>2022</year>;.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Sun</surname> <given-names>G</given-names></string-name>, <string-name><surname>Zeng</surname> <given-names>F</given-names></string-name>, <string-name><surname>McCartin</surname> <given-names>M</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>ZS</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>J</given-names></string-name>. <article-title>Closed-loop stimulation using a mul-tiregion brain-machine interface has analgesic effects in rodents</article-title>. <source>Science Translational Medicine</source>. <year>2022</year>; <volume>14</volume>(<issue>651</issue>):eabm5868.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Walker</surname> <given-names>EY</given-names></string-name>, <string-name><surname>Cotton</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Tolias</surname> <given-names>AS</given-names></string-name>. <article-title>A neural basis of probabilistic computation in visual cortex</article-title>. <source>Nature Neuroscience</source>. <year>2020</year>; <volume>23</volume>(<issue>1</issue>):<fpage>122</fpage>–<lpage>129</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>F</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Li</surname> <given-names>H</given-names></string-name>, <string-name><surname>Liao</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zheng</surname> <given-names>X</given-names></string-name>, <string-name><surname>Principe</surname> <given-names>JC</given-names></string-name>. <article-title>Quantized attention-gated kernel reinforcement learning for brain–machine interface decoding</article-title>. <source>IEEE transactions on neural networks and learning systems</source>. <year>2015</year>; <volume>28</volume>(<issue>4</issue>):<fpage>873</fpage>–<lpage>886</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Willsey</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Nason-Tomaszewski</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Ensel</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Temmar</surname> <given-names>H</given-names></string-name>, <string-name><surname>Mender</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Costello</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Patil</surname> <given-names>PG</given-names></string-name>, <string-name><surname>Chestek</surname> <given-names>CA</given-names></string-name>. <article-title>Real-time brain-machine interface in non-human primates achieves high-velocity prosthetic finger movements using a shallow feedforward neural network decoder</article-title>. <source>Nature Communications</source>. <year>2022</year>; <volume>13</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Wodlinger</surname> <given-names>B</given-names></string-name>, <string-name><surname>Downey</surname> <given-names>J</given-names></string-name>, <string-name><surname>Tyler-Kabara</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schwartz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Boninger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Collinger</surname> <given-names>J</given-names></string-name>. <article-title>Ten-dimensional anthropomorphic arm control in a human brain-machine interface: dificulties, solutions, and limitations</article-title>. <source>Journal of neural engineering</source>. <year>2014</year>; <volume>12</volume>(<issue>1</issue>):<fpage>016011</fpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Yan</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Goodman</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Moore</surname> <given-names>DD</given-names></string-name>, <string-name><surname>Solla</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Bensmaia</surname> <given-names>SJ</given-names></string-name>. <article-title>Unexpected complexity of everyday manual behaviors</article-title>. <source>Nature communications</source>. <year>2020</year>; <volume>11</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Walker</surname> <given-names>E</given-names></string-name>, <string-name><surname>Cotton</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Tolias</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Pitkow</surname> <given-names>X</given-names></string-name>. <article-title>Revealing nonlinear neural decoding by analyzing choices</article-title>. <source>Nature communications</source>. <year>2021</year>; <volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Yatsenko</surname> <given-names>D</given-names></string-name>, <string-name><surname>Josić</surname> <given-names>K</given-names></string-name>, <string-name><surname>Ecker</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Froudarakis</surname> <given-names>E</given-names></string-name>, <string-name><surname>Cotton</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Tolias</surname> <given-names>AS</given-names></string-name>. <article-title>Improved estimation and interpretation of correlations in neural circuits</article-title>. <source>PLoS computational biology</source>. <year>2015</year>; <volume>11</volume>(<issue>3</issue>):<fpage>e1004083</fpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Yu</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Cunningham</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Santhanam</surname> <given-names>G</given-names></string-name>, <string-name><surname>Ryu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Shenoy</surname> <given-names>KV</given-names></string-name>, <string-name><surname>Sahani</surname> <given-names>M</given-names></string-name>. <article-title>Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</article-title>. <source>Advances in neural information processing systems</source>. <year>2008</year>; <volume>21</volume>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hughes</surname> <given-names>RN</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>N</given-names></string-name>, <string-name><surname>Fallon</surname> <given-names>IP</given-names></string-name>, <string-name><surname>Bakhurin</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>J</given-names></string-name>, <string-name><surname>Severino</surname> <given-names>FPU</given-names></string-name>, <string-name><surname>Yin</surname> <given-names>HH</given-names></string-name>. <article-title>A one-photon endoscope for simultaneous patterned optogenetic stimulation and calcium imaging in freely behaving mice</article-title>. <source>Nature Biomedical Engineering</source>. <year>2022</year>; p. <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wei</surname> <given-names>XX</given-names></string-name>. <article-title>Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2020</year>; <volume>33</volume>:<fpage>7234</fpage>–<lpage>7247</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s9">
<title>Supplementary Information</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Evaluation of separated signals on the synthetic dataset.</title>
<p><bold>a</bold>, The temporal neuronal activity of raw signals (the purple line) of an example test trial, which is decomposed into relevant (<bold>b</bold>) and irrelevant (<bold>c</bold>) signals. <bold>b</bold>, Relevant signals (orange lines) extracted by d-VAE under three distillation cases, where bold gray lines represent ground truth relevant signals. Results show that when <italic>a</italic> = 0.09, the relevant signals are too similar to raw signals but not similar to ground truth; when <italic>a</italic> = 0.9, the relevant signals are well similar to the ground truth; when <italic>a</italic> = 9, the relevant signals are not similar to the ground truth. <bold>c</bold>, Same as <bold>b</bold>, but for irrelevant signals (blue lines). Notably, when <italic>a</italic> = 9, some useful signals are left in irrelevant signals. <bold>d</bold>, The decoding R<sup>2</sup> of distilled relevant signals of three cases. Error bars indicate mean ± s.d. across five cross-validation folds. Results demonstrate that decoding R<sup>2</sup> increases as <italic>a</italic> increases. <bold>e</bold>, Same as <bold>d</bold>, but for irrelevant signals. Notably, when <italic>a</italic> = 9, irrelevant signals will contain large behavioral information. <bold>f</bold>, The neural similarity between relevant and raw signals. Results show that the neural R<sup>2</sup> decreases as <italic>a</italic> increases. <bold>g</bold>, The neural R<sup>2</sup> between relevant signals and the ground truth. Results show that d-VAE can utilize a proper trade-off to extract effective relevant signals that are similar to the ground truth. <bold>h</bold>, The decoding R<sup>2</sup> between true velocity and predicted velocity of raw signals (purple bars with slash lines), the ground truth signals (gray) and behaviorally-relevant signals obtained by d-VAE (red), PSID (pink), pi-VAE (green), LFADS (blue), and VAE (light green) on dataset A. Error bars denote mean ± standard deviation (s.d.) across five cross-validation folds. Asterisks represent significance of Wilcoxon rank-sum test with <italic>*P &lt;</italic> 0.05, <italic>**P &lt;</italic> 0.01. <bold>i</bold>, Same as <bold>h</bold>, but for irrelevant signals. <bold>j</bold>, The neural R<sup>2</sup> between generated relevant signals and raw signals. <bold>k</bold>, Same as <bold>j</bold>, but for the ground truth.</p></caption>
<graphic xlink:href="515644v4_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Decoding performance comparison with CEBRA.</title>
<p><bold>a</bold>, The decoding R<sup>2</sup> comparison between d-VAE and CEBRA on synthetic dataset. The red bar represents the behaviorally-relevant signals extracted by d-VAE, and the light purple bar represents the behaviorally-relevant embeddings extracted by CEBRA. Error bars indicate mean ± s.d. across five cross-validation folds. Asterisks denote significance of Wilcoxon rank-sum test with <italic>*P &lt;</italic> 0.05, <italic>**P &lt;</italic> 0.01. <bold>b-d</bold>, Same as <bold>a</bold>, but for datasets A, B, and C, respectively.</p></caption>
<graphic xlink:href="515644v4_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3.</label>
<caption><title>The effect of irrelevant signals on relevant signals at the single-neuron level.</title>
<p><bold>a,b</bold> Same as <xref rid="fig3" ref-type="fig">Fig. 3</xref>, but for dataset C. <bold>a</bold>, The angle difference (AD) of preferred direction (PD) between raw and distilled signals as a function of the R<sup>2</sup> of raw signals. Each black point represents a neuron (n=91). The red curve is the fitting curve between R<sup>2</sup> and AD. Five example larger R<sup>2</sup> neurons’ PDs are shown in the inset plot, where the solid line arrows represent the PD of relevant signals, and the dotted line arrows represent the PDs of raw signals. <bold>b</bold>, Comparison of the cosine tuning fit (R<sup>2</sup>) before and after distillation of single neurons (black points), where the x-axis and y-axis represent neurons’ R<sup>2</sup> of raw and distilled signals.</p></caption>
<graphic xlink:href="515644v4_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4.</label>
<caption><title>The firing activity of example neurons.</title>
<p><bold>a</bold>, Example of three neurons’ raw firing activity decomposed into behaviorally-relevant and irrelevant parts using all trials in held-out test sets for four conditions (4 of 8 directions) of center-out reaching task. <bold>b</bold>, Example of three neurons’ raw firing activity decomposed into behaviorally-relevant and irrelevant parts using all trials in held-out test sets for four conditions (4 of 12 conditions) of obstacle avoidance task.</p></caption>
<graphic xlink:href="515644v4_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5.</label>
<caption><title>The effect of irrelevant signals on analyzing neural activity at the population level.</title>
<p><bold>a-d</bold>, Same as <xref rid="fig4" ref-type="fig">Fig. 4</xref>, but for dataset C. <bold>a,b</bold>, PCA is separately applied on relevant and irrelevant signals to get relevant PCs and irrelevant PCs. The thick lines represent the cumulative variance explained for the signals on which PCA has been performed, while the thin lines represent the variance explained by those PCs for other signals. Red, blue, and gray colors indicate relevant signals, irrelevant signals, and random Gaussian noise (for chance level). The cumulative variance explained for behaviorally-relevant (<bold>a</bold>) and irrelevant (<bold>b</bold>) signals got by d-VAE. <bold>c,d</bold>, PCA is applied on raw signals to get raw PCs. <bold>c</bold>, The bar plot represents the composition of each raw PC. The inset pie plot shows the overall proportion of raw signals, where red, blue, and purple colors indicate relevant signals, irrelevant signals, and the correlation between relevant and relevant signals. The PC marked with a red triangle indicates the last PC where the variance of relevant signals is greater than or equal to that of irrelevant signals. <bold>d</bold>, The cumulative variance explained by raw PCs for different signals, where the thick lines represent the cumulative variance explained for raw signals(purple), while the thin lines represent the variance explained for relevant (red) and irrelevant (blue) signals.</p></caption>
<graphic xlink:href="515644v4_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S6.</label>
<caption><title>The effect of irrelevant signals obtained by pi-VAE on analyzing neural activity at the population level.</title>
<p><bold>a-l</bold>, Same as <xref rid="fig4" ref-type="fig">Fig. 4</xref> and <xref rid="figs5" ref-type="fig">Fig. S5</xref>, but for pi-VAE. <bold>a,b</bold>, PCA is separately applied on relevant and irrelevant signals to get relevant PCs and irrelevant PCs. The thick lines represent the cumulative variance explained for the signals on which PCA has been performed, while the thin lines represent the variance explained by those PCs for other signals. Red, blue, and gray colors indicate relevant signals, irrelevant signals, and random Gaussian noise (for chance level). The cumulative variance explained for behaviorally-relevant (<bold>a</bold>) and irrelevant (<bold>b</bold>) signals on dataset A. <bold>c,d</bold>, PCA is applied on raw signals to get raw PCs. c, The bar plot represents the composition of each raw PC. The inset pie plot shows the overall proportion of raw signals, where red, blue, and purple colors indicate relevant signals, irrelevant signals, and the correlation between relevant and relevant signals. The PC marked with a red triangle indicates the last PC where the variance of relevant signals is greater than or equal to that of irrelevant signals. <bold>d</bold>, The cumulative variance explained by raw PCs for different signals, where the thick line represents the cumulative variance explained for raw signals(purple), while the thin line represents the variance explained for relevant (red) and irrelevant (blue) signals. <bold>e-h</bold>, <bold>i-l</bold>, Same as <bold>a-d</bold>, but for datasets B and C.</p></caption>
<graphic xlink:href="515644v4_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Figure S7.</label>
<caption><title>The rotational dynamics of raw, relevant, and irrelevant signals.</title>
<p>datasets A and B have twelve and eight conditions, respectively. We get the trial-averaged neural responses for each condition, then apply jPCA to raw, relevant, and irrelevant signals to get the top two jPC, respectively. <bold>a</bold>, The rotational dynamics of raw neural signals. <bold>b</bold>, The rotational dynamics of relevant signals obtained by d-VAE. <bold>c</bold>, The rotational dynamics of irrelevant signals obtained by d-VAE. We can see that the rotational dynamics of behaviorally-relevant signals are similar to that of raw signals, but the rotational dynamics of behaviorally-irrelevant signals are irregular. <bold>d-f</bold>, Same as <bold>a-c</bold>, but for dataset B.</p></caption>
<graphic xlink:href="515644v4_figs7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Figure S8.</label>
<caption><title>The cumulative variance of raw and behaviorally-relevant signals.</title>
<p><bold>a</bold>, PCA is applied separately on raw and distilled behaviorally-relevant signals to get raw PCs and relevant PCs. The cumulative variance of raw (purple) and behaviorally-relevant signals (red) on dataset A (n=90). Two upper left corner curves denote the variance accumulation from larger to smaller variance PCs. Two lower right corner curves indicate accumulation from smaller to larger variance PCs. The horizontal lines represent the 10%, and 90% variance explained. The vertical lines indicate the number of dimensions accounted for the last 10% and top 90% of the variance of behaviorally-relevant (red) and raw (purple) signals. Here we call the subspace composed by PCs of capturing top 90% variance the primary subspace, and the subspace composed by PCs of capturing last 10% variance the secondary subspace. We can see that the dimensionality of the primary subspace of raw signals is significantly higher than that of relevant signals, indicating that irrelevant signals make us overestimate the dimensionality of specific behaviors. <bold>b,c</bold>, Same as <bold>a</bold>, but for datasets B (n=159) and C (n=91).</p></caption>
<graphic xlink:href="515644v4_figs8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Figure S9.</label>
<caption><title>trivial neural activities encode rich behavioral information in complex nonlinear ways.</title>
<p><bold>a-c</bold>, Same as <xref rid="fig5" ref-type="fig">Fig. 5</xref>, but for dataset C (n=91). <bold>d-f</bold>, Same as <xref rid="fig6" ref-type="fig">Fig. 6</xref>, but for dataset C. <bold>a</bold>, The comparison of decoding performance between raw (purple) and distilled signals (red) with different neuron groups, including smaller R<sup>2</sup> neuron (R<sup>2</sup> <italic>&lt;</italic>= 0.03), larger R<sup>2</sup> neuron (R<sup>2</sup> <italic>&gt;</italic> 0.03), and all neurons. Error bars indicate mean ± SD across cross-validation folds. Asterisks denote significance of Wilcoxon rank-sum test with <italic>*P &lt;</italic> 0.05, <italic>**P &lt;</italic> 0.01. <bold>b</bold>, The correlation matrix of all neurons of raw and behaviorally-relevant signals. <bold>c</bold>, The decoding performance of KF (left) and ANN (right) with neurons dropped out from larger to smaller R<sup>2</sup>. The vertical gray lines indicate the number of dropped neurons at which raw and behaviorally-relevant signals have the greatest performance difference. <bold>d</bold>, The comparison of decoding performance between raw (purple) and distilled signals (red) composed of different raw-PC groups, including smaller variance PCs (the proportion of irrelevant signals that make up raw PCs is higher than that of relevant signals), larger variance PCs (the proportion of irrelevant signals is lower than that of relevant ones). Error bars indicate mean ± s.d. across five cross-validation folds. Asterisks denote significance of Wilcoxon rank-sum test with <italic>*P &lt;</italic> 0.05, <italic>**P &lt;</italic> 0.01. <bold>e</bold>, The cumulative decoding performance of signals composed of cumulative PCs that are ordered from smaller to larger variance using KF (left) and ANN (right). The red patches indicate the decoding ability of the last 10% variance of relevant signals. <bold>f</bold>, Same as <bold>e</bold>, but PCs are ordered from larger to smaller variance. The red patches indicate the decoding gain of the last 10% variance signals of relevant signals superimposing on their top 90% variance signals.</p></caption>
<graphic xlink:href="515644v4_figs9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs10" position="float" orientation="portrait" fig-type="figure">
<label>Figure S10.</label>
<caption><title>Simulation results.</title>
<p><bold>a-c</bold>, The simulation dataset A simulated a situation where the smaller R<sup>2</sup> neurons contain a certain amount of behavioral information, but the behavioral information cannot be decoded from these neurons due to being covered by noise. We used this experiment to demonstrate that d-VAE can utilize the larger R<sup>2</sup> neurons to help the smaller R<sup>2</sup> neurons restore their original face. <bold>a</bold>, The comparison of decoding performance between raw (purple), distilled (red), and ground truth (gray) signals with different neuron groups, including smaller R<sup>2</sup> neuron (R<sup>2</sup> <italic>&lt;</italic>= 0.03), larger R<sup>2</sup> neuron (R<sup>2</sup> <italic>&gt;</italic> 0.03), and all neurons. Error bars indicate mean ± s.d. (n=5 folds). Asterisks denote the significance of Wilcoxon rank-sum test with <italic>**P &lt;</italic> 0.01. <bold>b</bold>, The decoding performance of behaviorally-irrelevant signals got by d-VAE with different neuron groups. Error bars indicate mean ± s.d. (n=5 folds). <bold>c</bold>, The neural similarity between distilled behaviorally-relevant (red) and raw signals (purple) and between relevant and ground truth signals (gray) with different neuron groups. Error bars indicate mean ± s.d. (n=5 folds). <bold>d-f</bold>, The simulation dataset B simulated the situation where linear decoding is significantly inferior to nonlinear decoding. We used this experiment to demonstrate that d-VAE can not make the linear decoder achieve similar performance as the nonlinear decoder. <bold>d</bold>, The comparison of decoding performance between raw (purple), relevant (red), and ground truth (gray) signals. Error bars indicate mean ± s.d. (n=5 folds). <bold>e</bold>, The decoding performance of behaviorally-irrelevant signals got by d-VAE. Error bars indicate mean ± s.d. (n=5 folds). <bold>f</bold>, The neural similarity between distilled behaviorally-relevant (red) and raw signals (purple) and between relevant and ground truth signals (gray). Error bars indicate mean ± s.d. (n=5 folds).</p></caption>
<graphic xlink:href="515644v4_figs10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs11" position="float" orientation="portrait" fig-type="figure">
<label>Figure S11.</label>
<caption><title>Smaller variance PC signals preferentially improve lower-speed velocity.</title>
<p><bold>a</bold>, The comparison of absolute improvement ratio between lower-speed (red) and higher-speed (purple) velocity when superimposing secondary signals on primary signals with KF on dataset A. Error bars indicate mean ± s.d. across five cross-validation folds. Asterisks denote significance of Wilcoxon rank-sum test with <italic>*P &lt;</italic> 0.05, <italic>**P &lt;</italic> 0.01. <bold>b, c</bold>, Same as <bold>a</bold>, but for datasets B and C. <bold>d</bold>, The comparison of relative improvement ratio between lower-speed (red patch) and higher-speed (no patch) velocity when superimposing secondary signals on primary signals with KF on dataset B. The first-row plot shows five example trials’ speed profile of the decoded velocity using primary signals (light blue line) and full signals (dark blue line; superimposing secondary signals on primary signals) and the true velocity (red line). The black horizontal line denotes the speed threshold. The second and third-row plots are the same as the first-row plot, but for X and Y velocity. The fourth-row plot shows the relative improvement ratio for each point in trials.</p></caption>
<graphic xlink:href="515644v4_figs11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87881.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Gallego</surname>
<given-names>Juan Alvaro</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Imperial College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents a <bold>useful</bold> method for the extraction of behaviour-related activity from neural population recordings based on a specific deep learning architecture - a variational autoencoder. However, the evidence supporting the scientific claims resulting from the application of this method is <bold>incomplete</bold> as the results may stem, in part, from its properties. The authors should: (1) improve how they benchmark their method, by comparing against additional relevant techniques, and (2) reframe their results considering what observations may be a byproduct of their method, and which do constitute new scientific observations.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87881.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This work seeks to understand how behaviour-related information is represented in the neural activity of the primate motor cortex. To this end, a statistical model of neural activity is presented that enables a non-linear separation of behaviour-related from unrelated activity. As a generative model, it enables the separate analysis of these two activity modes, here primarily done by assessing the decoding performance of hand movements the monkeys perform in the experiments. Several lines of analysis are presented to show that while the neurons with significant tuning to movements strongly contribute to the behaviourally-relevant activity subspace, less or un-tuned neurons also carry decodable information. It is further shown that the discovered subspaces enable linear decoding, leading the authors to conclude that motor cortex read-out can be linear.</p>
<p>Strengths:</p>
<p>In my opinion, using an expressive generative model to analyse neural state spaces is an interesting approach to understanding neural population coding. While potentially sacrificing interpretability, this approach allows capturing both redundancies and synergies in the code as done in this paper. The model presented here is a natural non-linear extension of a previous linear model (PSID) and</p>
<p>Weaknesses:</p>
<p>First, the model in the paper is almost identical to an existing VAE model (TNDM) that makes use of weak supervision with behaviour in the same way [1]. This paper should at least be referenced. If the authors wish they could compare their model to TNDM, which combines a state space model with smoothing similar to LFADS. Given that TNDM achieves very good behaviour reconstructions, it may be on par with this model without the need for a Kalman filter (and hence may achieve better separation of behaviour-related and unrelated dynamics).</p>
<p>Second, in my opinion, the claims regarding identifiability are overstated - this matters as the results depend on this to some extent. Recent work shows that VAEs generally suffer from identifiability problems due to the Gaussian latent space [2]. This paper also hints that weak supervision may help to resolve such issues, so this model as well as TNDM and CEBRA may indeed benefit from this. In addition however, it appears that the relative weight of the KL Divergence in the VAE objective is chosen very small compared to the likelihood (0.1%), so the influence of the prior is weak and the model may essentially learn the average neural trajectories while underestimating the noise in the latent variables. This, in turn, could mean that the model will not autoencode neural activity as well as it should, note that an average R2 in this case will still be high (I could not see how this is actually computed). At the same time, the behaviour R2 will be large simply because the different movement trajectories are very distinct. Since the paper makes claims about the roles of different neurons, it would be important to understand how well their single trial activities are reconstructed, which can perhaps best be investigated by comparing the Poisson likelihood (LFADS is a good baseline model). Taken together, while it certainly makes sense that well-tuned neurons contribute more to behaviour decoding, I worry that the very interesting claim that neurons with weak tuning contain behavioural signals is not well supported.</p>
<p>Third, and relating to this issue, I could not entirely follow the reasoning in the section arguing that behavioural information can be inferred from neurons with weak selectivity, but that it is not linearly decodable. It is right to test if weak supervision signals bleed into the irrelevant subspace, but I could not follow the explanations. Why, for instance, is the ANN decoder on raw data (I assume this is a decoder trained fully supervised) not equal in performance to the revenant distilled signals? Should a well-trained non-linear decoder not simply yield a performance ceiling? Next, if I understand correctly, distilled signals were obtained from the full model. How does a model perform trained only on the weakly tuned neurons? Is it possible that the subspaces obtained with the model are just not optimally aligned for decoding? This could be a result of limited identifiability or model specifics that bias reconstruction to averages (a well-known problem of VAEs). I, therefore, think this analysis should be complemented with tests that do not depend on the model.</p>
<p>Finally, a more technical issue to note is related to the choice to learn a non-parametric prior instead of using a conventional Gaussian prior. How is this implemented? Is just a single sample taken during a forward pass? I worry this may be insufficient as this would not sample the prior well, and some other strategy such as importance sampling may be required (unless the prior is not relevant as it weakly contributed to the ELBO, in which case this choice seems not very relevant). Generally, it would be useful to see visualisations of the latent variables to see how information about behaviour is represented by the model.</p>
<p>Summary:</p>
<p>This paper presents a very interesting analysis, but I have several concerns as to well the analysis supports the main conclusions. I think the work could benefit from an additional complementary analysis that seeks to confirm with another method if weakly tuned neurons indeed show an encoding that differs qualitatively from the strongly tuned ones.</p>
<p>[1] Hurwitz, Cole, et al. &quot;Targeted neural dynamical modeling.&quot; Advances in Neural Information Processing Systems 34 (2021): 29379-29392.</p>
<p>
[2] Hyvarinen, Aapo, Ilyes Khemakhem, and Hiroshi Morioka. &quot;Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning.&quot; arXiv preprint arXiv:2303.16535 (2023).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87881.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Li et al present a method to extract &quot;behaviorally relevant&quot; signals from neural activity. The method is meant to solve a problem which likely has high utility for neuroscience researchers. There are numerous existing methods to achieve this goal some of which the authors compare their method to, though there are notable omissions. However, I do believe that d-VAE is a promising approach that has its own advantages.</p>
<p>That being said, there are issues with the paper as-is. This could have been a straightforward &quot;methods&quot; paper describing their approach and validating it on different ground truth and experimental datasets. Instead, the authors focus on the neuroscientific results and their implications for brain mechanisms. Unfortunately, while the underlying method seems sound and performs well relative to the assessed competition, the scientific results and presentation they put forward were not sufficiently strong to support these claims, especially given the small amount of data (recordings of one monkey per task, with considerable variability between them).</p>
<p>Specific comments</p>
<p>
- Is the apparently increased complexity of encoding vs decoding so unexpected given the entropy, sparseness, and high dimensionality of neural signals (the &quot;encoding&quot;) compared to the smoothness and low dimensionality of typical behavioural signals (the &quot;decoding&quot;) recorded in neuroscience experiments? This is the title of the paper so it seems to be the main result on which the authors expect readers to focus.</p>
<p>- I take issue with the premise that signals in the brain are &quot;irrelevant&quot; simply because they do not correlate with a fixed temporal lag with a particular behavioural feature hand-chosen by the experimenter. As an example, the presence of a reward signal in motor cortex [1] after the movement is likely to be of little use from the perspective of predicting kinematics from time-bin to time-bin using a fixed model across trials (the apparent definition of &quot;relevant&quot; for behaviour here), but an entire sub-field of neuroscience is dedicated to understanding the impact of these reward-related signals on future behaviour. Is there method sophisticated enough to see the behavioural &quot;relevance&quot; of this brief, transient, post-movement signal? This may just be an issue of semantics, and perhaps I read too much into the choice of words here. Perhaps the authors truly treat &quot;irrelevant&quot; and &quot;without a fixed temporal correlation&quot; as synonymous phrases and the issue is easily resolved with a clarifying parenthetical the first time the word &quot;irrelevant&quot; is used. But I remain troubled by some claims in the paper which lead me to believe that they read more deeply into the &quot;irrelevancy&quot; of these components.</p>
<p>- The authors claim the &quot;irrelevant&quot; responses underpin an unprecedented neuronal redundancy and reveal that movement behaviors are distributed in a higher-dimensional neural space than previously thought.&quot; Perhaps I just missed the logic, but I fail to see the evidence for this. The neural space is a fixed dimensionality based on the number of neurons. A more sparse and nonlinear distribution across this set of neurons may mean that linear methods such as PCA are not effective ways to approximate the dimensionality. But ultimately the behaviourally relevant signals seem quite low-dimensional in this paper even if they show some nonlinearity may help.</p>
<p>- Relatedly, I would like to note that the exercise of arbitrarily dividing a continuous distribution of a statistic (the &quot;R2&quot;) based on an arbitrary threshold is a conceptually flawed exercise. The authors read too much into the fact that neurons which have a low R2 w.r.t. PDs have behavioural information w.r.t. other methods. To this reviewer, it speaks more about the irrelevance, so to speak, of the preferred direction metric than anything fundamental about the brain.</p>
<p>- there is an apparent logical fallacy that begins in the abstract and persists in the paper: &quot;Surprisingly, when incorporating often-ignored neural dimensions, behavioral information can be decoded linearly as accurately as nonlinear decoding, suggesting linear readout is performed in motor cortex.&quot; Don't get me wrong: the equivalency of linear and nonlinear decoding approaches on this dataset is interesting, and useful for neuroscientists in a practical sense. However, the paper expends much effort trying to make fundamental scientific claims that do not feel very strongly supported. This reviewer fails to see what we can learn about a set of neurons in the brain which are presumed to &quot;read out&quot; from motor cortex. These neurons will not have access to the data analyzed here. That a linear model can be conceived by an experimenter does not imply that the brain must use a linear model. The claim may be true, and it may well be that a linear readout is implemented in the brain. Other work [2,3] has shown that linear readouts of nonlinear neural activity patterns can explain some behavioural features. The claim in this paper, however, is not given enough</p>
<p>- I am afraid I may be missing something, as I did not understand the fano factor analysis of Figure 3. In a sense the behaviourally relevant signals must have lower FF given they are in effect tied to the temporally smooth (and consistent on average across trials) behavioural covariates. The point of the original Churchland paper was to show that producing a behaviour squelches the variance; naturally these must appear in the behaviourally relevant components. A control distribution or reference of some type would possibly help here.</p>
<p>- The authors compare the method to LFADS. While this is a reasonable benchmark as a prominent method in the field, LFADS does not attempt to solve the same problem as d-VAE. A better and much more fair comparison would be TNDM [4], an extension of LFADS which is designed to identify behaviourally relevant dimensions.</p>
<p>[1] <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0160851">https://doi.org/10.1371/journal.pone.0160851</ext-link></p>
<p>
[2] <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2022.03.31.486635">https://doi.org/10.1101/2022.03.31.486635</ext-link></p>
<p>
[3] <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41593-017-0028-6">https://doi.org/10.1038/s41593-017-0028-6</ext-link></p>
<p>
[4] Hurwitz et al, Targeted Neural Dynamical Modeling, NeurIPS 2021.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87881.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors develop a variational autoencoder (VAE), termed d-VAE (or distill VAE) that aims to tease apart the behaviorally relevant and irrelevant sections of each neuron's firing rate. The input to the VAE is the population activity for a given time step, and the output is the inferred behaviorally relevant section of the population activity at that time step. The residual is referred to as behaviorally irrelevant: total neural activity = behaviorally relevant + behaviorally irrelevant (x = x_r + x_i). The mapping from the raw neural signals (x) to the bottlenecked latent in the autoencoder (called z, z=f(x)) and back to the inferred behaviorally relevant single-neuron activities (x_r = g(z)) is applied per time step (does not incorporate any info from past/future time steps) and, critically, it is nonlinear (f and g are nonlinear feedforward neural networks). The key technical novelty that encourages x_r to encode behaviorally relevant information is a term added to the loss, which penalizes bad linear behavior decoding from the latent z. Otherwise the method is very similar to a prior method called pi-VAE, which should be explained more thoroughly in the manuscript to clearly highlight the technical novelty.</p>
<p>The authors apply their method to 3 non-human primate datasets to infer behaviorally relevant signals and contrast them with the raw neural signals and the residual behaviorally irrelevant signals. As a key performance metric, they compute the accuracy of decoding behavior from the inferred behaviorally relevant signals (x_r) using a linear Kalman filter (KF) or alternatively using a nonlinear feed forward neural network (ANN). They highlight 3 main conclusions in the abstract: first, that single neurons from which behavior is very poorly decodable do encode considerable behavior information in a nonlinear manner, which the ANN can decode. Second, they conclude from various analyses that behavior is occupying a higher dimensional neural space than previously thought. Third, they find that linear KF decoding and nonlinear ANN decoding perform similarly when provided with the inferred behaviorally relevant signals (x_r), from which they conclude that a linear readout must be performed in motor cortex.</p>
<p>The paper is well-written in many places and has high-quality graphics. The questions that it aims to address are also of considerable interest in neuroscience. However, unfortunately, several main conclusions, including but not limited to all 3 conclusions that are highlighted in the abstract, are not fully supported by the results due to confounds, some of which are fundamental to the method. Several statements in the text also seem inaccurate due to use of imprecise language. Moreover, the authors fail to compare with some more relevant existing methods that are specifically designed for extracting behaviorally relevant signals. In addition, for some of the methods they compare with, they do not use an appropriate setup for the benchmark methods, rendering the validation of the proposed method unconvincing. Finally, in many places imprecise language that is not accompanied with an operational definition (e.g., smaller R2 [of what], similar [per what metric]) makes results hard to follow, unless most of the text is read very carefully. Some key details of the methods are also not explained anywhere.</p>
</body>
</sub-article>
</article>