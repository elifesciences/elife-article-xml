<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">103347</article-id>
<article-id pub-id-type="doi">10.7554/eLife.103347</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103347.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A gradual transition toward categorical representations along the visual hierarchy during working memory, but not perception</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Chunharas</surname>
<given-names>Chaipat</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>chaipat.c@chula.ac.th</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1225-2294</contrib-id>
<name>
<surname>Wolff</surname>
<given-names>Michael J</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hettwer</surname>
<given-names>Meike D</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Rademaker</surname>
<given-names>Rosanne L</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<email>rosanne.rademaker@gmail.com</email>
</contrib>
    <aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05jd2pj53</institution-id><institution>Cognitive Clinical and Computational Neuroscience Center of Excellence, Department of Medicine, King Chulalongkorn Memorial Hospital, Chulalongkorn University</institution></institution-wrap>, <city>Bangkok</city>, <country country="TH">Thailand</country></aff>
    <aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ygt2y02</institution-id><institution>Ernst Strüngmann Institute (ESI) for Neuroscience in Cooperation with the Max Planck Society</institution></institution-wrap>, <city>Frankfurt</city>, <country country="DE">Germany</country></aff>
    <aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck School of Cognition, Max Planck Institute of Human Cognitive and Brain Sciences</institution></institution-wrap>, <city>Leipzig</city>, <country country="DE">Germany</country></aff>
    <aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024z2rq82</institution-id><institution>Institute of Systems Neuroscience, Medical Faculty, Heinrich Heine University Düsseldorf</institution></institution-wrap>, <city>Düsseldorf</city>, <country country="DE">Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-03-14">
<day>14</day>
<month>03</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP103347</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-10-07">
<day>07</day>
<month>10</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-10-07">
<day>07</day>
<month>10</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.05.18.541327"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Chunharas et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Chunharas et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-103347-v1.pdf"/>
<abstract>
<title>Abstract</title><p>The ability to stably maintain visual information over brief delays is central to healthy cognitive functioning, as is the ability to differentiate such internal representations from external inputs. One possible way to achieve both is via multiple concurrent mnemonic representations along the visual hierarchy that differ systematically from the representations of perceptual inputs. To test this possibility, we examine orientation representations along the visual hierarchy during perception and working memory. Human participants directly viewed, or held in mind, oriented grating patterns, and the similarity between fMRI activation patterns for different orientations was calculated throughout retinotopic cortex. During direct viewing of grating stimuli, similarity was relatively evenly distributed amongst all orientations, while during working memory the similarity was higher around oblique orientations. We modeled these differences in representational geometry based on the known distribution of orientation information in the natural world: The “veridical” model uses an efficient coding framework to capture hypothesized representations during visual perception. The “categorical” model assumes that different “psychological distances” between orientations result in orientation categorization relative to cardinal axes. During direct perception, the veridical model explained the data well. During working memory, the categorical model gradually gained explanatory power over the veridical model for increasingly anterior retinotopic regions. Thus, directly viewed images are represented veridically, but once visual information is no longer tethered to the sensory world there is a gradual progression to more categorical mnemonic formats along the visual hierarchy.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>visual perception</kwd>
<kwd>visual working memory</kwd>
<kwd>sensory recruitment</kwd>
<kwd>visual cortex</kwd>
<kwd>parietal cortex</kwd>
<kwd>categorization</kwd>
<kwd>representational similarity</kwd>
<kwd>representational geometry</kwd>
<kwd>efficient coding</kwd>
<kwd>RSA</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Based on reviewer comments, we have specified more clearly what is meant by &quot;perception&quot; in our task, and how it relates to prior fMRI work showing generalization from passive perception (i.e., deliberately not using an orientation task during perception). We adapted the behavioral input function. We applied our model using various different fitting approaches. And notably, we also included an entirely new single-parameter model (Supp. Fig. 7) - all to show that our primary results, which are visible by eye in Figure 1C, hold up irrespective of what modelling approaches we use to quantify these effects.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Holding images in mind over a brief delay is central to cognition, as it allows for the retention and manipulation of information that cannot be viewed directly. Visual working memory (VWM) recruits early visual cortex, including primary visual area V1 – as indexed by response patterns recorded with fMRI<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c8">8</xref></sup>. Being the first cortical processing site of visual inputs, the role of V1 during perception is fundamentally different from its role during visual working memory. This is because in the absence of direct visual input, mnemonic information in V1 and other early visual areas must necessarily be generated internally. Famously, sensory recruitment theory posits that higher-order frontal and parietal regions of the brain that are active throughout the working memory delay<sup><xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c20">20</xref></sup>, recruit early sensory areas in a top-down manner in order to maintain high fidelity sensory memories<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>. Alternatively, recurrent processes in local circuits could sustain information over a memory delay<sup><xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c24">24</xref></sup>, although such recurrenc<sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref></sup> y is presumably stronger in more anterior brain areas where higher pyramidal cell spine counts are believed to support increased connection strength of local circuits<sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c26">26</xref></sup>. Irrespective of the exact substrate of working memory maintenance – with inputs from the external sensory world during perception, and from sources within the brain during working memory, it is unlikely that viewed and remembered visual information would be represented in an identical manner in early visual cortex<sup><xref ref-type="bibr" rid="c27">27</xref>–<xref ref-type="bibr" rid="c30">30</xref></sup>. However, it remains an open question how a cortical area like V1, specialized for processing visual inputs, actually represents visual working memories. Are working memory representations just noisier versions of perceptual representations, or do they differ in a fundamental way? Might there be representational transformations along the visual hierarchy as top-down influences play an increasingly larger role during VWM?</p>
<p>Recent work has claimed that early visual cortex (EVC) represents VWM information in a “sensory-like” format that is similar to representations driven by sensory inputs<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>, while more anterior visual areas like the Intraparietal Sulcus (IPS) represent VWM information in a format that is transformed away from the sensory driven response<sup><xref ref-type="bibr" rid="c31">31</xref></sup>. This claim of “sensory-likeness” in EVC comes from the fact that when participants remember an orientation, response patterns are similar to response patterns evoked by directly viewing a stimulus with the same orientation even when that viewed stimulus is not attended. In parietal cortex such cross-generalization from sensory to working memory responses fails, while memories <italic>are</italic> decodable when considering only the response patterns during working memory themselves (i.e., without cross- generalization). The idea that visual representations are transformed away from the “sensory- like” into a different, more abstract format during memory is further supported by work similarly using cross-generalization to decode VWM contents<sup><xref ref-type="bibr" rid="c32">32</xref></sup>. In this study, participants remembered one of two visually distinct features – the orientation of a grating, or the direction of a moving dot cloud. During encoding (i.e., stimulus perception), the two features evoked distinct response patterns in EVC that did not cross-generalize, likely owing to the distinct retinal inputs evoked by the two features. However, these two features share a spatial component (degrees on a circle), and during the memory delay the EVC response patterns for orientation and direction of the stimuli did successfully cross-generalize, likely owing to a “line-like” abstraction that is realizable for both visual features.</p>
<p>However, the possible range of to-be-remembered visual features far outstrips those that can be mapped onto a circle (and into a line). For example, surface features such as contrast or color are not readily abstracted into a “line-like” representation, let alone more complex features such as shapes or objects. Indeed, there is abundant psychophysical work showing that memory reports for numerous (surface) features are biased toward category centers<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>, indicative of categorization at the behavioral level. This means that a more general principle of abstraction remains to be uncovered. Furthermore, we know that retinotopically organized “sensory-like” representations or abstractions are not generally observed outside of early visual cortex<sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. An important step to investigate possible abstraction used for working memory is to consider not only <italic>response patterns</italic>, but also the <italic>representational geometry</italic>. A <italic>response pattern</italic> (also called a “coding scheme”<sup><xref ref-type="bibr" rid="c38">38</xref></sup>) simply refers to the pattern of responses that is measured during an experimental condition. Depending on the measurement technique, this could be a pattern of firing rates across multiple neurons, a pattern of BOLD responses across multiple voxels, or any other kind of response vector measured over a number of units. When it’s possible to cross- generalize from one experimental condition to another – for example from sensory to memory, or from orientation to direction – we know that the <italic>response pattern</italic> is similar in the two conditions. For example, the specific pattern of brain activity in response to a directly viewed stimulus with an orientation of 90° would be similar to the pattern measured when that same 90° stimulus is held in working memory. The <italic>representational geometry</italic> captures a lower-dimensional format of a given stimulus set, and can be invariant to changes in the underlying response patterns<sup><xref ref-type="bibr" rid="c39">39</xref></sup>. The pairwise distances between patterns of responses corresponding to a set of stimuli determine the geometry, meaning that even if the underlying response patterns change (e.g., they are inverted, shifted, or undergo some other transformation), the geometry can remain stable. For example, in the case of orientation the geometry may reveal that adjacent orientations (say 90° and 91°) evoke similar underlying response patterns, and that this similarity drops at increasing distances in orientation space. Such representational geometry can be shared between direct sensory input and working memory maintenance, while a sensed stimulus of 90° may nevertheless evoke a completely different response pattern compared to a 90° stimulus held in working memory. As long as the pattern distances between different orientations are the same during a sensory and memory task, so is the geometry. Indeed, we know that despite dynamics in population response patterns over time, the representational geometry of a stimulus set can remain stable<sup><xref ref-type="bibr" rid="c40">40</xref>,<xref ref-type="bibr" rid="c41">41</xref></sup>.</p>
<p>Here we want to examine the representational geometry during orientation working memory throughout the visual hierarchy, and see how this compares to bottom-up stimulus driven activity elicited by directly viewed stimuli. To ensure active perception of directly viewed sensory stimuli, while avoiding overlap in top-down attentional state, participants attended an orthogonal stimulus feature (contrast) during the sensory task, while holding a precise orientation in mind during the memory task. By looking at the representational geometry we can investigate the representational formats of both perception and working memory in a way that does not depend on response patterns generalizing from one condition to another. Moreover, it allows us to investigate potential systematic differences in the representational geometry between perception and working memory, even when underlying response patterns are still similar enough for successful cross-generalization. To illustrate, what if abstraction in memory happens by compressing part of the stimulus space, making it more categorical? Such compression may warp response patterns for a subset of orientations, without necessarily transforming them to a point where the coding scheme breaks down. In such a case, cross-generalization from perception to working memory could coexist with a change in the representational geometry. Thus, examining the representational geometry allows us more freedom to see if perception and working memory are truly represented similarly, or if the representational formats perhaps differ in fundamental ways.</p>
<p>We introduce a principled approach to investigating categorization across the visual hierarchy. First, we use representational similarity analysis (RSA) to show a clear differentiation between the geometry of perception and working memory representations for orientation in human visual cortex. Second, we model the extent to which the representational geometry is true to a simulated sensory response (the “veridical” model), or abstracted away from the sensory input (the “categorical” model). Our two models are constrained by a single principle, namely, the distribution of orientation information in the natural world<sup><xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup>. By applying these two models to the data, we show that sensory inputs are represented in a largely veridical manner in EVC, adapted to the statistics of the natural visual world (i.e., efficient coding). By contrast, working memories are represented more categorically, in a manner predicted from the same landscape of visual input statistics, but using a higher-order metric based on how different any set of orientations may appear to the observer (i.e., their “psychological distance”). Critically, we show that during working memory the representational format becomes increasingly more categorical along the visual hierarchy, uncovering a gradient of abstraction.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>To examine neural representations during perception and working memory, we analyze existing fMRI recordings from six participants who were either directly viewing oriented grating stimuli during a sensory task, or remembering orientations for later recall during a working memory task (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). During the sensory task, participants saw each oriented grating for 9s at a time, and they had to actively detect a small reduction in contrast (500ms) that happened probabilistically (twice per 9s, meaning it could occur 0 to &gt;2 times per trial). To avoid adaptation, grating contrast was phase reversed every 500ms. During the working memory task, participants briefly (500ms) saw a grating and remembered its orientation for 13s. On two-thirds of memory trials a visual distractor (filtered noise, or another grating with uncorrelated orientation) was presented during the middle portion of the delay (for 11 seconds). Because there was little-to-no quantifiable difference between the different trial types (see ref<sup><xref ref-type="bibr" rid="c31">31</xref></sup>), we analyzed data from all delays combined. For all analyses we use average voxel responses from 4.8–9.6s and 5.6–13.6s after stimulus onset for the sensory and memory task, respectively (replicating the time-windows used in the original publication<sup><xref ref-type="bibr" rid="c31">31</xref></sup>). For the working memory task in particular, this choice of time-window is based on the observation that (1) the BOLD response evoked by the to-be-remembered stimulus is back to baseline ∼6s post stimulus onset in these data<sup><xref ref-type="bibr" rid="c31">31</xref></sup>, (2) decoding is possible for every single TR during the delay<sup><xref ref-type="bibr" rid="c31">31</xref></sup> well beyond the stimulus-evoked BOLD response and despite poorer signal-to-noise of single TR data, and (3) much prior work has shown that stimulus-evoked BOLD alone is insufficient for stimulus information to persist into the memory delay<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Task and main analysis</title><p><bold>(A)</bold> For the sensory task (left), participants viewed a randomly oriented grating for 9 seconds per trial (contrast phase-reversing at 5 Hz) and reported instances of contrast dimming. For the working memory task (right), participants remembered a briefly presented (500 ms) randomly orientated grating for 13 seconds, until a 3 second recall epoch (not depicted). <bold>(B)</bold> For each Region of Interest (ROI) we employed a split-half randomization procedure to create a Representational Similarity Matrix (RSM) for each participant. On each randomization fold, voxel patterns from all trials (300–340 for sensory, 324 for memory) were randomly split in half. For each half of trials, we averaged the voxel patterns for every degree in orientation space within a + 10° window. This resulted in 180 vectors with a length equal to the number of voxels for each split of the data. We then calculated the similarity between each vector (or degree) in one half of the data, to all vectors (or degrees) in the second half of the data, using a Spearman correlation coefficient. This resulted in a 180x180 similarity matrix on each fold. This randomization procedure was repeated 1000 times to generate the final RSM for each ROI and each participant. Across all folds, RSM’s are near-symmetrical around the diagonal, give-or-take some cross-validation noise. <bold>(C)</bold> Representational geometry of orientation during the sensory (top row) and working memory (bottom row) tasks, for retinotopically defined ROI’s (columns) across all participants. During the sensory task, the clear diagonal pattern in early visual areas V1–V3 indicates that orientations adjacent in orientation space are represented more similarly than orientations further away. During the memory task, similarity clusters strongly around oblique orientations (45° and 135°), contrasting starkly with the similarity patterns during perception. Note that the diagonal represents an inherent noise-ceiling, due to the cross-validation procedure used. This noise ceiling shows inhomogeneities across orientation space, demonstrating how certain orientations may be encoded with more noise than others. RSM’s are scaled to the range of correlations within each subplot to ease visual comparison of representational structure between sensory and memory tasks for all ROI’s (exact ranges are shown in <xref rid="figs3" ref-type="fig">Supplementary Figure 3</xref>). For early ROI’s (V1–V4), only visually responsive voxels are included in the analysis. Throughout, 0° (and 180°) denotes vertical, and 90° denotes horizontal.</p></caption>
<graphic xlink:href="541327v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We use cross-validated representational similarity analysis (RSA) – a method that projects neural activation patterns into an abstract space that describes the stimulus (here: orientation)<sup><xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c46">46</xref>–<xref ref-type="bibr" rid="c48">48</xref></sup>. Specifically, we created, for each visual cortical Region of Interest (ROI) and each of the two tasks (sensory and memory), a matrix capturing the similarity between the neural response patterns to all possible orientations using cross-validated correlations (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). To illustrate: When two orientations are represented in a very similar manner, the pattern of voxel responses to the first orientation will correlate strongly with the pattern evoked by the second orientation. Conversely, for orientations with very distinct representations, correlations will be low. Because orientation space is continuous, physically similar orientations (e.g., 10° and 11°) will likely correlate more strongly than physically dissimilar orientations (e.g., 10° and 40°) in areas of the brain that encode orientation information.</p>
<p>The representational similarity matrices (RSM’s) constructed for our visual ROI’s (<xref rid="fig1" ref-type="fig">Figure 1C</xref>; <xref rid="figs1" ref-type="fig">Supplementary Figure 1</xref>) show striking qualitative differences between how orientation is represented during perception (in the sensory task), and working memory (in the memory task). During the sensory task, early visual areas V1–V3 show a strong diagonal component and relatively higher degree of similarity around cardinal orientations (180° in particular, which is vertical), with notable transformations away from this representational geometry primarily along the dorsal stream (V3AB and IPS). During the memory task, there is a prominent clustering of representational similarity around oblique orientations (45° and 135°). This clustering seems to increase along the visual hierarchy and appears most pronounced in area IPS0. These results do not depend on the specific way we bin orientations in our RSA (<xref rid="figs2" ref-type="fig">Supplementary Figure 2</xref>).</p>
<p>To quantify the results in <xref rid="fig1" ref-type="fig">Figure 1C</xref>, we contrast two possible models – the “veridical” and the “categorical” model. The veridical model intends to capture how orientations are represented in a manner that faithfully reflects early visual processing of signals from the external world. The categorical model uses a higher-level concept – the “psychological distance” between orientations – as a basis for abstracting a physically continuous space into discrete categories. Importantly, these two models are jointly constrained by the known distribution of orientation information in the natural world, which has a higher occurrence of cardinal compared to oblique orientations (<xref rid="fig2" ref-type="fig">Figure 2A</xref>; ref<sup><xref ref-type="bibr" rid="c42">42</xref></sup>). According to the “efficient coding” hypothesis, this inhomogeneity in orientation input statistics leads to adaptive changes in the sensory system, with relatively more neural resources dedicated to cardinal compared to oblique orientations<sup><xref ref-type="bibr" rid="c49">49</xref>–<xref ref-type="bibr" rid="c53">53</xref></sup>. As a result, observers demonstrate a higher resolution (e.g., improved discriminability) around cardinal orientations – a phenomenon paradoxically known as the “oblique effect”<sup><xref ref-type="bibr" rid="c54">54</xref>–<xref ref-type="bibr" rid="c56">56</xref></sup>. Orientation reports also tend to be biased away from cardinal axes<sup><xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c57">57</xref>,<xref ref-type="bibr" rid="c58">58</xref></sup>. Together, this suggests a distinct role for cardinal orientations, both with respect to precision and bias.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Modeling the representational similarity of perceived and remembered orientations</title><p><bold>(A)</bold> The distribution of visual orientation in the natural world is inhomogeneous, with higher prevalence of orientations closer to cardinal (90° &amp; 180°) compared to oblique (45° &amp; 135°). The function shown here approximates these input statistics, and is used to constrain both the veridical (in <bold>B</bold>) and categorical (in <bold>C</bold>) models. <bold>(B)</bold> The veridical model is based on the principle of efficient coding – the idea that neural resources are adapted to the statistics of the environment. We model this via 180 idealized orientation tuning functions with amplitudes scaled by the theoretical input statistics function (the top panel shows a subset of tuning functions for illustrational purposes). A vector of neural responses is simulated by computing the activity of all 180 orientation-tuned neurons to a given stimulus orientation. Representational similarity is calculated by correlating simulated neural responses to all possible orientations, resulting in the veridical model RSM (bottom panel). Note that while we chose to modulate tuning curve amplitude, there are multiple ways to warp the stimulus space (e.g., by applying non-uniform changes to gain, tuning width, tuning preference, etc.<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup>). <bold>(C)</bold> In the categorical model, categorization is based on people’s subjective experience of relative similarity between orientations in different parts of orientation space: If orientations in part of the space appear quite similar, they are lumped together into the same category, while the most distinctive looking orientations serve as category boundaries. This is quantified via the “psychological distance” – the sum of derivatives along the input statistics function between any pair of orientations (see top panel). The insert shows an example of orientation-pairs near cardinal (in blue) and oblique (in red) that have the same physical distance, but different psychological distances. The psychological distance between each possible pair of orientations yields the categorical model’s RSM (bottom panel). <bold>(D)</bold> Fits of the veridical (grey) and categorical (teal) models for the sensory (top) and memory (bottom) tasks. During the sensory task, the veridical model better explains the data compared to the categorical model in almost all visual ROI’s (except IPS1–3), indicating a representational scheme that is largely in line with modeled early sensory responses. During the memory task, the categorical model gains increasingly more explanatory power over the veridical model along the visual hierarchy, and explains the data significantly better in V3, V3AB, V4, and IPS0. The Fisher transformed semi-partial correlations (on the y-axis) represent the unique contribution of each model after removing the variance explained by the other model via semi-partial correlations. Dots represent individual participants, and errorbars represent + 1 within-participant SEM. Asterisks indicate the significance level of post-hoc two-sided paired-sample t-tests (*p &lt; 0.05; **p &lt; 0.01; ***p &lt; 0.001) comparing the two models in each ROI.</p></caption>
<graphic xlink:href="541327v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The idea behind the veridical model is that region-wide orientation representations emerge from low-level neural responses to sensory inputs. Specifically, the starting point for this model is a set of idealized tuning functions that tile orientation-space (<xref rid="fig2" ref-type="fig">Figure 2B</xref>, top). The amplitude of each orientation tuning function is scaled by the estimated frequency of occurrence for that orientation in the natural world (i.e., scaled by the theoretical “input statistics” function, see <xref rid="fig2" ref-type="fig">Figure 2A</xref> and Methods). Therefore, tuning functions closer to cardinal orientations have relatively higher amplitudes than those closer to obliques. We modulate tuning curve amplitude (and not other properties like tuning width or density) because of known amplitude differences in the fMRI signal for cardinals compared to obliques<sup><xref ref-type="bibr" rid="c59">59</xref></sup>. For any given stimulus orientation, we can simulate a vector of neural responses by reading out the hypothesized activity from every idealized neural tuning function. Such a simulated response vector can be correlated against simulated responses to all other possible stimulus orientations (analogous to the approach in <xref rid="fig1" ref-type="fig">Figure 1B</xref>), to arrive at the veridical model RSM (<xref rid="fig2" ref-type="fig">Figure 2B</xref>, bottom). Specifically, to preserve the BOLD amplitude differences mentioned above, we use Pearson correlations to generate the model RSM. Thus, here we model a veridical early visual representation by accounting for known inhomogeneities of orientation space, based on the principle of efficient coding<sup><xref ref-type="bibr" rid="c43">43</xref></sup>. Note that our veridical model is a direct consequence of the choice to use amplitude modulation (instead of e.g., tuning width) as well as correlation (instead of e.g., Euclidian distance), and that there are multiple other possible ways to implement inhomogeneities across orientation space and the idea of efficient coding<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c60">60</xref></sup>.</p>
<p>The idea behind the categorical model is to discretize the physically continuous orientation space into plausible higher-level psychological categories. A principled way to categorize orientation is to again rely on the input statistics function (<xref rid="fig2" ref-type="fig">Figure 2A</xref>), and consider how inhomogeneities in orientation space might affect more experiential measures such as perceptual similarity<sup><xref ref-type="bibr" rid="c61">61</xref></sup>. For example, in parts of the orientation space with high resolution (near the cardinals), two physically similar orientations (e.g., 4° apart) can look clearly different from one another, while orientations with the same physical similarity in parts of orientation space with lower resolution (near the obliques) can look indistinguishable. We formalize this “psychological distance” as the distance between any pair of orientations along the theoretical input statistics function (<xref rid="fig2" ref-type="fig">Figure 2C</xref>, top). Returning to our example, two near-cardinal orientations (e.g., 88° and 92°) will have a larger psychological distance (i.e., are relatively far apart along this function) compared to two near- oblique orientations (e.g., 43° and 47°) (compare the <xref rid="fig2" ref-type="fig">Figure 2C</xref> grating inserts in blue versus red, respectively). The psychological distances between all possible orientations make up the categorical model RSM (<xref rid="fig2" ref-type="fig">Figure 2C</xref>, bottom). This RSM shows how orientations in one category (bound by two cardinals) are represented similarly to one another, but dissimilarly from orientations in a second category (on the other side of the cardinals). Thus, here we model how orientation representations can be categorized based on where an orientation is relative to cardinal – the cardinal axes effectively serving as category boundaries.</p>
<p>How well can the representational geometry during the sensory and memory tasks (<xref rid="fig1" ref-type="fig">Figure 1C</xref>) be explained by our veridical (<xref rid="fig2" ref-type="fig">Figure 2B</xref>) and categorical (<xref rid="fig2" ref-type="fig">Figure 2C</xref>) models? Because our two models are not independent, we evaluated the correlation of each model to the data after first removing the variance explained by the other model, which is also known as a semi-partial correlation. Specifically, to look at the unique contribution of the veridical model in explaining the data, we first remove the variance of the categorical model from the veridical model, and then correlate the residuals to the data RSM’s (and vice versa for the categorical model; see Methods). We apply a Fisher transformation to the resulting correlation values to normalize their distribution, and better allow for statistical testing. Transformed correlations are shown in <xref rid="fig2" ref-type="fig">Figure 2D</xref> for our sensory (top) and memory (bottom) tasks. Importantly, these results do not depend on the specific fitting approach, and replicate when we fit each model directly to the data (without first taking the residuals), and also when we use a general linear model to simultaneously fit both models to estimate their beta weights (<xref rid="figs4" ref-type="fig">Supplementary Figure 4</xref>).</p>
<p>During the sensory task, the veridical model does an overall better job at explaining representational similarity than the categorical model (main effect of model: F<sub>(1,5)</sub> = 76.05, p = 0.0003). This advantage is not the same in all ROI’s (interaction of model x ROI: F<sub>(7,35)</sub> = 4.158, p = 0.002), with post-hoc tests showing a difference between the two models in all ROI (except IPS1–3). The fact that the veridical model outperforms the categorical model during perception in the sensory task helps validate our modeling approach, given that the veridical model is founded on what we know about early bottom-up sensory processing.</p>
<p>During the memory task, the categorical model better explains representational similarity than the veridical model (main effect of model: F<sub>(1,5)</sub> = 11.4, p = 0.0198). The extent to which the categorical model outperforms the veridical model differs across ROI’s (interaction of model x ROI: F<sub>(7,35)</sub> = 3.048, p = 0.013), with the categorical model explaining increasingly more of the representational geometry along the visual hierarchy. A significant difference between the two models emerges in area V3, and persists until area IPS0. These results corroborate the qualitative categorization already apparent from the memory task RSM’s in <xref rid="fig1" ref-type="fig">Figure 1C</xref>, and reveal a posterior-to-anterior gradient in terms of categorization strength during working memory, but not perception (interaction of ROI x task x model: F<sub>(7,35)</sub> = 3.226, p = 0.0095). Finally, we also confirmed that the presence of distractors during the delay did not impact the pattern of results in the memory task (<xref rid="figs5" ref-type="fig">Supplementary Figure 5</xref>).</p>
<p>To verify that our modeling results do not critically depend on the exact shape of the theoretical input statistics function in <xref rid="fig2" ref-type="fig">Figure 2A</xref>, we next used behavioral error from an independent psychophysical experiment to constrain both our models (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). A new set of 17 participants each completed 1620 trials of an orientation recall task. For every possible stimulus orientation that was shown (1°–180° in steps of 1°), we calculate the absolute mean recall error across all participants. With 27540 total trials in the experiment, and a sliding window of 3° for more reliable estimates, absolute errors are based on 459 trials for every possible stimulus orientation. We chose the mean absolute error as it takes both response variance and response bias into account. As expected, recall error is inhomogeneous along orientation space, with pronounced differences between cardinals and obliques (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). Specifically, recall is more accurate around cardinal compared to oblique orientations. We generated the veridical and categorical models anew from this psychophysical input function (<xref rid="fig3" ref-type="fig">Figure 3B</xref>), and again fit both models to our empirical RSM’s to see how well they explained the data (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). We replicated the difference between the sensory and memory tasks, and how their representational geometries are better explained by the veridical and categorical models, respectively (interaction ROI x task x model: F<sub>(7,35)</sub> = 4.712; p = 0.001). During the sensory task, the veridical model outperformed the categorical model (main effect of model: F<sub>(1,5)</sub> = 52.55, p = 0.001), and this advantage differed significantly between ROI’s (interaction of model x ROI: F<sub>(7,35)</sub> = 2.984, p = 0.015). During the memory task, the categorical model outperformed the veridical model (main effect of model: F<sub>(1,5)</sub> = 10.6, p = 0.023), and explained increasingly more of the representational geometry in some ROI’s than in others (interaction of model x ROI: F<sub>(7,35)</sub> = 4.539, p = 0.001).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Generating and fitting the veridical and categorical models based on independent behavioral data</title><p><bold>(A)</bold> During an independent psychophysical examination, a new set of participants (N=17) reported the orientation of briefly presented (200ms) and remembered (2s) single gratings by rotating a response dial with a computer mouse (i.e., via method-of-adjustment). For each possible stimulus orientation in the experiment (+1°), we calculated the mean absolute response error across all participants, and smooth the resulting function (Gaussian, over 10°). The absolute error<sup>−1</sup> (y- axis) is plotted against the stimulus orientation shown to participants. From this psychophysical input function, the veridical and categorical models were generated as previously described (see <xref rid="fig2" ref-type="fig">Figure 2B</xref> &amp; <xref rid="fig2" ref-type="fig">2C</xref>). <bold>(B)</bold> Veridical and categorical models generated from the psychophysical input function (in <bold>A</bold>). <bold>(C)</bold> Fits of the veridical (in grey) and categorical (in teal) models based on the independent psychophysical data. During the sensory task (top), the veridical model better explains the data compared to the categorical model in all visual ROI’s except for IPS1–3. During the memory task (bottom), the categorical model better explains the data compared to the veridical model in V2, V2AB, and V4 (and marginally better in IPS0 with p = 0.053). The Fisher transformed semi-partial correlations (on the y-axis) represent the unique contribution of each model after removing the variance explained by the other model. Dots represent individual participants, and errorbars represent + 1 within-participant SEM. Asterisks indicate the significance level of post-hoc two-sided paired-sample t-tests (*p &lt; 0.05; **p &lt; 0.01; ***p &lt; 0.001) comparing the two models in each ROI.</p></caption>
<graphic xlink:href="541327v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>How might we reconcile the observed <italic>differences</italic> in representational geometry between the sensory and memory tasks, with the <italic>overlap</italic> in coding schemes that is evident from the ability to cross-generalize from the sensory to the memory task in EVC<sup><xref ref-type="bibr" rid="c31">31</xref></sup>? To directly relate these two analysis approaches, we modified the typical RSA approach by correlating response patterns from every <italic>perceived</italic> orientation in the sensory task to the response patterns from every <italic>remembered</italic> orientation in the memory task, in what we call “across-task RSA” (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). As expected, our across-task RSM of V1 shows a clear diagonal component that is indicative of overlap in response patterns between perception and working memory, and the ability to cross- generalize. Cross-generalization does not work in IPS, replicating what we know from previous analysis of these data<sup><xref ref-type="bibr" rid="c31">31</xref></sup>. Importantly, this approach also shows how the coding scheme for orientation during the memory task is warped with respect to coding scheme during the sensory task – response patterns for orientations held in working memory are biased towards what would be patterns associated with obliques during perception (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). We validate our “across- task RSA” approach against a conventional multivariate analysis approach known as the inverted encoding model (or “IEM”<sup><xref ref-type="bibr" rid="c62">62</xref></sup>). First, we take into account the predicted gradual drop in representational similarity for orientations at increasing distances from the remembered orientation by creating a “correlation profile” (the sum of correlations between patterns for the remembered and perceived orientations, as shown in grey on top of the panels in <xref rid="fig4" ref-type="fig">Figure 4B</xref>, and for all ROI’s in <xref rid="fig4" ref-type="fig">Figure 4C</xref>). By collapsing the data based on “same” versus “increasingly different” orientations, we’re in essence performing decoding, but by using correlation. A more “peaked” correlation profile indicates more information about the remembered orientation, which we quantify using a previously established fidelity metric (as in <sup><xref ref-type="bibr" rid="c31">31</xref></sup>, see Methods). Finally, we show that this “RSA fidelity” aligns closely with the same fidelity metric applied to results from an IEM (<xref rid="fig4" ref-type="fig">Figure 4D</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Ability to cross-decode using RSA</title><p><bold>(A)</bold> Using across-task representational similarity analysis, we directly compare orientation response patterns recorded during the sensory task (y-axis), to those measured during the memory task (x-axis). Here we show V1 (left subplot) and IPS0 (right subplot) as example ROI’s. The across-task RSM in V1 shows a clear diagonal component, indicating similar response patterns for specific orientations in the sensory and memory tasks. In IPS0 such pattern similarity for matching orientations in the sensory and memory tasks is less evident. <bold>(B)</bold> We want to quantify the extent to which orientations held in working memory evoke response patterns that overlap with response patterns from those same orientations when viewed directly, and how this similarity drops at larger distances in orientation space. First, we center our across-task RSM’s on the remembered orientation (notice the x- axis), and then take the sum of correlations relative to the remembered orientation (plotted on top of the across-task RSM’s in grey). We call this the “correlation profile” of the remembered orientation. In V1 we see that correlations are highest between response patterns from matching perceived and remembered orientations (0° on the x-axis), explaining the ability to cross-decode between sensory and memory tasks as demonstrated in previous work (e.g.,<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>). By contrast, IPS0 shows a much flatter correlation profile. <bold>(C)</bold> Correlation profiles for all retinotopic ROI’s in our study, obtained by performing across-task RSA (left panel). Most ROI’s show a peaked correlation profile, indicative of shared pattern similarity between the same orientations when directly viewed and when remembered. The different offsets along the y-axis for different ROI’s reflect the overall differences in pattern similarity in different areas of the brain, with pattern similarity being highest in area V1. Shaded areas indicate + 1 SEM <bold>(D)</bold> To validate the ability to cross-decode using RSA, we directly compare this new approach (x-axis) to the multivariate analysis performed by Rademaker et al. in 2019 (y-axis). The latter used an inverted encoding model (IEM) that was trained on the sensory task, and tested on the delay period of the memory task. Both the correlation profiles from RSA, and the channel response functions from IEM yield more-or-less peaked functions over orientation space (relative to the remembered orientation) that can be quantified using the same fidelity metric (i.e., convolving with a cosine). Here, we show a high degree of consistency between the fidelity metrics derived with both approaches, and successful cross-generalization from the sensory to the memory task (as indexed by &gt;0 fidelities) in many ROI’s. Each color represents a different ROI, and for each ROI we plot each of the six participants as an individual dot.</p></caption>
<graphic xlink:href="541327v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Thus far, we examined the structure of orientation representations during perception and working memory in individual visual ROI’s, and find that orientation representations differ between sensory and memory tasks. We also find that representational geometry <italic>within the same task</italic> is captured by our models to varying extents in different ROI’s. To move beyond specific patterns of information in local ROI’s (e.g., <xref rid="fig1" ref-type="fig">Figure 1C</xref>; <xref rid="figs1" ref-type="fig">Supplementary Figure 1</xref>), and more formally assess representational geometries across visual cortex, we use a 2<sup>nd</sup> level RSA. In this analysis, similarity between different visual cortical areas is calculated by correlating the RSM from every ROI with that of every other ROI. To this end, we use more fine-grained ROI’s than in previous analyses, allowing us to look at dorsal versus ventral streams, as well as subregions of IPS and Lateral Occipital (LO) cortex. We evaluate how orientation information is represented across visual cortex in this manner separately for the sensory task and the memory task (<xref rid="fig5" ref-type="fig">Figure 5</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Second level RSA</title><p><bold>(A)</bold> To compare how orientation is represented across different regions of visual cortex, RSM’s from fine-grained individual ROI’s (<xref rid="figs1" ref-type="fig">Supplementary Figure 1</xref>) were correlated in a 2<sup>nd</sup> level similarity analysis. For the sensory task (top panel), representational similarity is high among early visual areas; high among the various IPS regions; and high among LO regions. Similarity between these three clusters is relatively low. For the memory task (bottom panel) there is a slight shift in similarity compared to the sensory task, with V1 becoming less similar, and IPS0 becoming more similar, to areas V2–V4. Furthermore, the distinction between areas is generally less pronounced. <bold>(B)</bold> Representational similarity can also be used as an indicator of connectivity between ROI’s based on shared representational geometry: When the geometry is similar, the “connection” is stronger (indicated here by the width of the grey lines connecting different ROI’s). The sum of the strength of these connections in a given ROI (i.e., degree centrality) indicates to which extent a local representational geometry resembles that of other ROI’s. Degree centrality is highest in early visual cortex and lowest in IPS regions, indicating a higher conservation of geometry across early visual cortical regions.</p></caption>
<graphic xlink:href="541327v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>During the sensory task, there is notable shared representational similarity amongst early visual areas (V1–V4) and amongst areas in the intraparietal sulcus (IPS0–3), but low similarity between the two (<xref rid="fig5" ref-type="fig">Figure 5A</xref>, top). During the memory task, orientation geometries across various ROI’s show a somewhat different inter-areal organization (<xref rid="fig5" ref-type="fig">Figure 5A</xref>, bottom). First, the overall similarity between ROI’s is more pronounced during the memory task, with higher overall correlations between ROI’s compared the sensory perception task (r = 0.553 + 0.132 s.d., versus r = 0.32 + 0.082 s.d., respectively, with p &lt; 0.001). This implies that there are fewer transformations of orientation geometry along the visual hierarchy during working memory compared to perception. Second, the cluster of early visual ROI’s with high representational similarity that was observed during perception (i.e., V1–V4), is shifted “upwards” along the visual hierarchy during memory – with V1 becoming less similar, and IPS becoming more similar to rest of EVC.</p>
<p>Finally, we probe the underlying “representational connectivity” structure in individual participants<sup><xref ref-type="bibr" rid="c46">46</xref></sup>. Unlike traditional functional connectivity analysis, the representational connectivity approach does not target covarying activation per se, but rather assumes connections on the basis of shared representational geometry. Visualizing these “connections” in a graph (<xref rid="fig5" ref-type="fig">Figure 5B</xref>) highlights the dense clustering of early visual cortex, weaker connections to LO, and weakest connections to IPS, both during the sensory and memory tasks. Based on this graph, we can compute the degree centrality of each ROI (or “node” in this graph) as the sum of connection strengths to other ROI’s. A high degree centrality denotes high representational similarity to many –or an especially strong representational similarity with some– other brain regions. Highest degree centrality is observed in early visual cortex, suggesting substantial overlap in representational geometry across these early regions. More downstream visual areas (IPS and LO) show the lowest degree centrality, implying a gradual transformation of representational geometry along the visual pathway that results in geometries not present at earlier processing stages. This analysis also shows how <italic>functional</italic> measurements of sensory driven responses to oriented gratings, and even responses during working memory when no stimulus is on the screen, are transformed along functionally defined ROI’s in a manner that is commensurate with the known <italic>anatomical</italic> structure of the visual hierarchy. This further highlights the power of an approach like RSA, whereby functionally measured response patterns are transformed into an abstracted representational space.</p>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Here we compare how a simple visual stimulus is represented when it is either perceived or temporarily held in working memory, and show fundamental differences in the representational geometry of visual perception and visual working memory throughout human retinotopic cortex. By looking at the similarity of response patterns evoked by grating stimuli of different orientations, combined with a novel modeling approach, we are able to demonstrate relatively veridical representations during perception, and more categorical representations during working memory. We also find that the extent to which working memory representations are categorical increases along the visual hierarchy from posterior-to-anterior visual areas – modeling results that can be readily verified by simply looking at the geometries in the data. Importantly, our models make distinct predictions about veridical and categorical representations from a <italic>single</italic> input function based on the statistics of the natural world, which can also be implemented by measuring human behavior with a simple psychophysical task. This makes our modeling approach a potentially powerful tool to apply in other research contexts as well. With clear differences in representational geometry, it seems unlikely that working memory representations are merely noisier versions of perceptual representations, and our data imply a systematic compression of the coding scheme in parts of orientation space as a basis for categorization in working memory. Finally, by looking at inter-area representational similarity we recover known anatomical cortical structure, and observe a high degree of similarity for areas within early visual cortex (EVC), intraparietal sulcus (IPS), and lateral occipital cortex (LO) – but relatively low similarity between these respective regions.</p>
<p>Previous work from our group has claimed that visual working memories are represented in a “sensory-like” manner in early visual cortex<sup><xref ref-type="bibr" rid="c31">31</xref></sup>. This conclusion was drawn from the ability to cross generalize from sensory evoked responses, to responses recorded during the delay of a working memory task (using multivariate decoding techniques). However, there are multiple clues that VWM representations may be abstracted away from sensory evoked responses<sup><xref ref-type="bibr" rid="c30">30</xref>–<xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c63">63</xref>–<xref ref-type="bibr" rid="c66">66</xref></sup>, including the considerable differences between perceptual and working memory geometries unveiled in the present work. From a conceptual point of view there may be good reasons to keep formats distinct, as having identical representations for visual inputs and visual memories might make it difficult to distinguish external reality from internally generated thought<sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c67">67</xref>,<xref ref-type="bibr" rid="c68">68</xref></sup>. Moreover, some sort of transformation of the information held in mind is often necessary to adequately support behavioral goals and motor output<sup><xref ref-type="bibr" rid="c69">69</xref>,<xref ref-type="bibr" rid="c70">70</xref></sup>. How can we reconcile the apparent contradiction between successful sensory-to-memory cross-generalization on the one hand, and the mounting evidence favoring abstraction during VWM on the other?</p>
<p>To understand how perception and working memory can evoke overlapping response patterns (i.e., have an overlapping <italic>coding scheme</italic>) while also differing in their representational format (i.e., the <italic>representational geometry</italic>), we will examine the relationship between multivariate decoding and RSA more closely. First, note that within any kind of task, a high degree of similarity along the diagonal of a cross-validated RSM is a prerequisite for successful multivariate decoding, and vice versa. After all, if a particular stimulus would evoke uncorrelated response patterns every time it is presented (i.e., no similarity along the diagonal), a decoder would not be able to predict such a stimulus from the disparate response patterns that make up its training set (i.e., no decoding). A lower off-diagonal similarity is a second prerequisite, as otherwise the patterns evoked by different stimuli are indistinguishable. Thus, in areas of the brain that care about a certain kind of stimulus, you can expect both a clear diagonal component in the RSM, as well as successful within-task decoding. Things are a bit more nuanced for continuously varying stimuli such as orientation. To illustrate: Two identical orientations might evoke similar patterns of responses, give or take some noise, but so will two orientations that are adjacent in orientation space. While two orientations that are further apart are likely to evoke dissimilar responses. Based on the gradual transition in physical similarity between continuously varying stimuli, one would predict an RSM pattern where similarity gradually drops off at larger distances from the diagonal. In this case, off-diagonal similarity can have meaning, albeit with diminishing returns as representational similarity decreases with increasing stimulus distance.</p>
<p>For our data, this means that the clear diagonal component in the RSM’s during both perception and working memory (see EVC ROI’s in <xref rid="fig1" ref-type="fig">Figure 1C</xref>) are indicative of the ability to decode orientation <italic>within</italic> each of these two tasks. However, such apparent overlap in the represenational geometry around the diagonals does not speak to the overall geometry, nor does it speak to the ability to decode <italic>between</italic> the sensory and memory tasks (via cross generalization). With respect to the overall geometry, we know that even relatively subtle transformations (e.g., shifts or warping) of an otherwise fairly stable underlying coding scheme can lead to dynamics in the low-dimensional geometry<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c71">71</xref></sup>. Vice versa, the representational geometry can remain stable in the presence of dynamics in the coding scheme<sup><xref ref-type="bibr" rid="c39">39</xref>–<xref ref-type="bibr" rid="c41">41</xref></sup>. With respect to cross-generalization, this means that a clear diagonal component in both perception and working memory RSM’s could in theory stem from totally different non-generalizable response patterns in one task compared to the other, as long as the pairwise distances between response pattern are comparable between tasks. We show that in early visual ROI’s the underlying coding schemes during the sensory and memory tasks are sufficiently similar to yield a clear diagonal component in an “across-task RSA” (<xref rid="fig4" ref-type="fig">Figure 4</xref>). Importantly, we validate our across-task RSA approach against a common implementation of multivariate decoding for continuous stimulus spaces (the so-called inverted encoding model<sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c62">62</xref></sup>). More interestingly, the across-task RSM also provides some insight into how the coding scheme may be warped during working memory compared to perception in V1 – we observe biases away from cardinal orientations during working memory, with many remembered orientations resulting in response patterns that are similar to those of directly perceived oblique orientations. To sum up, working memory representations in EVC can be “sensory-like”, in that there is considerable overlap in the response patterns during perception and working memory. At the same time, a systematic warping of the geometry for orientation during the memory task, relative to the sensory task, may result in a more categorical geometry during VWM with high similarity around obliques.</p>
<p>In addition to using cross-generalization from sensory-to-memory responses to conclude that early visual areas store “sensory-like” working memory representations, our previous work drew upon the <italic>lack</italic> of such cross-generalization (in the presence of high within-task decoding performance) to conclude that IPS stores working memories in a format “transformed away” from sensory-like responses<sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c72">72</xref></sup>. However, our current analyses reveal how the representational geometry during working memory is predominantly categorical <italic>throughout</italic> much of the visual hierarchy: A significant benefit of the categorical over the veridical model can be seen in V3– IPS0 when using a theoretical input function (<xref rid="fig2" ref-type="fig">Figure 2A</xref>, 2D bottom panel), and when using the psychophysical input function (<xref rid="fig3" ref-type="fig">Figure 3A</xref>, 3C bottom panel). The reason that cross- generalization from the sensory to the memory task fails in IPS may therefore not be due to a transformation in the representational geometry from earlier-to-later visual areas during working memory.</p>
<p>Instead, IPS might simply process information quite differently during perception than during working memory. Recent recordings from non-human primates reveal that the receptive field of an IPS neuron (in lateral intraparietal “LIP” cortex) that was demarcated by showing the animal visual stimuli on a screen does not necessarily overlap with the receptive field of that same neuron when demarcated by measuring responses during a delayed-match-to-sample task<sup><xref ref-type="bibr" rid="c73">73</xref></sup>. This implies a distinct mechanism for representing sensory inputs and working memory contents at the level of single neurons, which plausibly scales up to the level of population recordings as obtained with fMRI. A second reason why cross-generalization from perception-to-memory might be lacking in IPS is because the sensory input is represented rather weakly in IPS in our sensory task. While the full-field grating stimulus was attended, the feature of interest to our analyses (orientation) was not directly relevant to the participants’ task (detect contrast changes). This means that the sensory task signal-to-noise may have been insufficient for cross decoding. Alternatively, feature-based attention might change or improve stimulus representations in IPS, as it was shown to do in EVC<sup><xref ref-type="bibr" rid="c78">78</xref></sup>. Given the central role of IPS in attention<sup><xref ref-type="bibr" rid="c74">74</xref>–<xref ref-type="bibr" rid="c77">77</xref></sup>, it would be interesting to examine how attending different features of the same stimulus might impact stimulus representations, and whether this could explain the relatively noisy RSM’s we observed in IPS during our sensory task (<xref rid="fig1" ref-type="fig">Figure 1C</xref>, top).</p>
<p>Of course, the attentional state in different tasks matters more generally in terms of the conclusions we can draw. It’s been shown that a decoder trained on patterns of responses to perceived but unattended orientation stimuli (RSVP task at fixation) generalized to response patterns during the working memory delay<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. By using <italic>unattended</italic> gratings, the authors could conclude that orientation-selective responses for remembered gratings depend on the same orientation-selective subpopulations driven during a bottom-up sensory response. In our work, we wanted to additionally control the deployment of spatial attention (presumably much narrower for a task at fixation) by having participants <italic>attend the contrast</italic> of a perceived grating. We already know that cross-generalized (sensory-to-memory) and within-task decoding (memory-to- memory) work well under these conditions<sup><xref ref-type="bibr" rid="c31">31</xref></sup>, so this choice meant we could be confident that possible geometrical differences would not be driven by differences in response patterns (or SNR) between the sensory and memory tasks. For future work it would be interesting to compare the sensory geometry resulting from our contrast attention task to a situation where the grating is not attended (task at fixation), or when the orientation of the grating is attended. We would speculate that in the presence of orientation attention, the geometry of a perceived grating would become even more veridical, as attention can be anchored to relevant attributes of the physical stimulus, spacing orientation representations more regularly throughout the 180° feature space.</p>
<p>A big question in the field of VWM concerns the role of primary visual area V1 during memory maintenance. On the one hand, sensory recruitment theory posits that involvement of area V1 is critical to maintaining highly detailed visual representations, as this is the only site thought to have the resolution to do so<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c79">79</xref></sup>. In support of this theory, many fMRI studies have shown that VWM contents can be decoded from V1<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup>, and correlations between behavioral and decoding performance imply a functional role for V1<sup><xref ref-type="bibr" rid="c72">72</xref>,<xref ref-type="bibr" rid="c80">80</xref></sup>. On the other hand, outside of the fMRI literature there is less evidence to support a neural correlate of VWM in area V1, with a general failure to find sustained firing in EVC<sup><xref ref-type="bibr" rid="c81">81</xref>,<xref ref-type="bibr" rid="c82">82</xref>,<xref ref-type="bibr" rid="c83">83</xref></sup> (but see also<sup><xref ref-type="bibr" rid="c84">84</xref>,<xref ref-type="bibr" rid="c85">85</xref></sup>), which has led some people to conclude that V1 decoding could be epiphenomenal<sup><xref ref-type="bibr" rid="c67">67</xref>,<xref ref-type="bibr" rid="c68">68</xref></sup>. Might our findings speak to this discrepancy between fMRI and single cell recording? Receptive fields in V1 are small, so if there is a representational shift from “veridical” during perception, to more “categorical” during VWM (presumably under the influence of top-down feedback), then working memory contents may be coded by a (subtly) different subset of neurons than those that respond to perceptual input. The reasoning that the same neurons may not code for the same stimulus under different task conditions holds true on several levels. For example, multi-unit activity associated with working memory maintenance was restricted to deep and superficial layers in V1, while such activity during perception also included the input layer 4<sup><xref ref-type="bibr" rid="c86">86</xref></sup>. Thus, even small shifts in the neural code (from one layer to the next, or from one orientation column to the next) may decrease the chance of finding sustained spiking when a one-to-one mapping between perception and working memory is assumed. Only looking at population wide neural coding, as we do here, can uncover working memory contents that has undergone a shift in representational geometry relative to perception.</p>
<p>Relying on population responses from fMRI BOLD does have one obvious caveat, which is the slow temporal profile of the signal. However, stimulus-evoked BOLD from the memory target can likely not explain the geometry during the working memory delay. Ample work has shown that even with identical sensory inputs at encoding, such BOLD does not carry stimulus information into the working memory delay when active maintenance of a stimulus feature is no longer required, with decoding performance for task-irrelevant features quickly dropping to chance<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>. For example, if one of two consecutive stimuli is cued for report immediately after stimulus presentation, the cued target cannot be decoded in the delay that follows<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. When shown a stimulus with two independent features (i.e., orientation and color), only the remembered feature can be decoded during the delay<sup><xref ref-type="bibr" rid="c2">2</xref></sup>. Etc. Finally, clear geometrical differences between the sensory and memory RSM’s would not be expected if both tasks reflected the same response.</p>
<p>A well-known strength of RSA is that it projects response patterns associated with different task conditions (in our case, task conditions are 180 levels of orientation) into an abstracted space where representations can be compared between imaging modalities, species, models, or with behavior<sup><xref ref-type="bibr" rid="c46">46</xref></sup>. Thus, RSA is a powerful tool to sidestep the correspondency problem, allowing us to compare the output of systems that differ greatly. For example, one can construct an RSM from behavioral responses and correlate it with an RSM constructed from neural responses of specific brain regions<sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c47">47</xref></sup>. However, visual perception involves a cascade of processes of increasing complexity, from simple feature-detectors in primary sensory cortex, to more invariant and category-based representations in ventral visual cortex. Behavior is the result of the entire brain working in concert to produce one output<sup><xref ref-type="bibr" rid="c87">87</xref></sup>, which means that even for a very simple stimulus such as orientation, the representational geometry can differ between areas and tasks. Using a behavior-derived RSM as a model could therefore miss a lot of variability in representational geometry across cortical areas, or produce misleading conclusions about a given area representing orientation while ignoring other areas that match behavioral output less. This is why using behavioral measurements (here: psychophysical input function) in a hypothesis-driven manner as the basis for multiple models (here: veridical and categorical), may allow for deeper insight into which specific areas of the brain are involved in multiple underlying components of a single behavior.</p>
<p>Our models are indeed able to quantify notable differences in representational geometry between orientation perception and working memory tasks, as well as differences between various retinotopic areas within a single task. However, there remain patterns in the data that neither of our current models are designed to capture. Most notably, data recorded during the sensory task from area V3AB (and arguably also V4, IPS, and LO) show a representational similarity pattern implying that the obliques (45° and 135°) are quite similarly to one another (<xref rid="fig1" ref-type="fig">Figure 1C</xref>, top row). This pattern hints at another possible form of (object-level) categorization, where all tilted lines, irrespective of their tilt direction, are represented in a similar fashion. This parallels evidence showing that people use the same verbal labeling (“diagonal”) for both obliques<sup><xref ref-type="bibr" rid="c88">88</xref></sup>. Alternatively, this pattern in V3AB could reflect something about the task, because tilt direction (45° or 135°) matters in the memory task due to the orientation recall requirement after the delay. During the sensory task, only contrast is attended. Precise top-down orientation signals during memory may override the tilt-agnostic tendency that certain areas (such as V3AB) might otherwise display.</p>
<p>Another example where we see the data diverge from our models, is in the non-uniformity along the diagonal of our RSM’s. Specifically, for the sensory task there appears to be lower similarity for horizontal (90°) compared to vertical (0°) orientations in many ROI’s, and similarity also appears relatively higher for obliques (45° and 135°) in some ROI (<xref rid="figs6" ref-type="fig">Supplementary Figure 6A</xref>). This non-uniformity implies that already at the level of sensory-driven responses all orientations are not represented equally, which is a well-established finding<sup><xref ref-type="bibr" rid="c59">59</xref>,<xref ref-type="bibr" rid="c60">60</xref>,<xref ref-type="bibr" rid="c89">89</xref>,<xref ref-type="bibr" rid="c90">90</xref></sup>. The higher overall similarity for oblique compared to cardinal orientations is exacerbated in the memory task (<xref rid="figs6" ref-type="fig">Supplementary Figure 6B</xref>). This may be due to a compression of orientation space around oblique orientations, leading to highly similar response patterns in this part of orientation space, while the pattern around cardinals is more distinct. This idea is supported by results from multidimensional scaling (<xref rid="figs6" ref-type="fig">Supplementary Figure 6C</xref>), which takes into account the entire geometrical pattern, showing stronger clustering around obliques compared to cardinals during the memory task.</p>
<p>Our categorical model was designed only to capture an extreme version of inhomogeneities in orientation space between cardinals and obliques. However, these inhomogeneities appear to some extent during the sensory task as well, and differ across the visual hierarchy. To directly test the extent to which these inhomogeneities appear across tasks and ROI, we designed an alternative model that uses a single parameter to modulate the magnitude of inhomogeneities (<xref rid="figs7" ref-type="fig">Supplementary Figure 7A</xref>). Specifically, by varying the exponent of the input function we create a family of model RSM’s that range from having higher similarity around cardinals (as also our veridical model), to diagonal, to higher similarity around obliques (including our categorical model). In fact, this approach closely approximates a previously proposed model<sup><xref ref-type="bibr" rid="c57">57</xref></sup> that was not based on natural input statistics. When we fit this model to our data, we again show a gradual transition toward more categorical representations (with high similarity around obliques) along the visual hierarchy during memory, but not perception (<xref rid="figs7" ref-type="fig">Supplementary Figure 7B</xref>). Of course, we would construct even more complex models, with (many) more parameters, to eventually capture every specific geometrical pattern in every ROI. But this was never our goal. Instead, the main point of our models is to have a simple yet well-motivated quantification of the stark differences in geometry between the sensory and memory task, and the changes in the geometry along the visual hierarchy (in the memory task), that are readily observable by eye (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). We show that these differences can indeed be quantified, and hold independent of the exact fitting approach (<xref rid="figs4" ref-type="fig">Supplementary Figure 4</xref>), binning approach (<xref rid="figs2" ref-type="fig">Supplementary Figure 2</xref>), and modeling approach (<xref rid="figs7" ref-type="fig">Supplementary Figure 7</xref>).</p>
<p>Given the diversity in RSM patterns described above, how might we compare the geometry within a given task, while remaining agnostic to the precise patterns in different regions of interest? To evaluate inter-area representational geometry differences in a more hypothesis-free manner, we used a “representational connectivity” analysis, which subsumes all possible patterns by simply quantifying degree of overlap. This allows us to compare how the visual system orchestrates representations across large swaths of cortex during both perception and working memory. One observation that emerges from this approach is that during VWM we see a shift in the interplay between areas, as compared to during perception. Specifically, during working memory the geometry in V1 becomes more differentiable from the geometry in the rest of EVC, and IPS0 becomes more differentiable from the rest of IPS (but more similar to EVC). In other words, we see that the inter-areal structuring of representational geometry differs between perception and memory. Another observation is that compared to perception, geometries during VWM show higher similarity across visual cortical areas. Such homogeneity might be expected if a unitary categorical top-down signal dominates feedback signals to multiple earlier areas. After all, in the absence of visual input, working memory information in V1 must be coming from within the brain itself, either through feedback connections or local recurrent processing.</p>
<p>Using representational similarity, in combination with the novel modeling approach outlined here, can be a powerful tool for studying representational formats during perception and working memory. Once the input statistics are known, either by deriving them from the environment or behavior, these models can be applied to any feature. Thus, in addition to tapping into a possible categorization for spatial features in EVC<sup><xref ref-type="bibr" rid="c32">32</xref></sup>, our approach has potential for surface-based features such as color or contrast, more complex visual objects such as shapes or faces, as well as stimuli in other sensory modalities. Many of the well-known advantages from RSA approaches also apply here, such as the potential to fit models across the entire brain in a manner that is not restricted to early sensory areas for which the receptive field mapping is known. Furthermore, the approach can be used with different measurement techniques that have a higher temporal resolution, allowing additional queries about the temporal progression of representational geometry as stimuli are encoded, remembered, and recalled.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Stimuli and procedures</title>
<p>We used a publicly available dataset, originally part of a study by Rademaker et al. (2019). This dataset contains both a visual perception task (the “sensory task”) and a visual working memory task (the “memory task”) performed in the scanner, and presented in the paper as Experiment 1 (6 participants with mean age = 28.67, sd = 3.675, and 5 females). The dataset also contains an independent psychophysical experiment, presented in the original paper as Supplementary Figure 9 (21 participants with mean age = 20.12, sd = 2.01, and 14 females. For this psychophysical experiment, only 17 participants were included in the analysis (3 dropped out, and 1 performed at chance level). All participants who contributed to the dataset were neurologically healthy, had normal or corrected-to-normal vision, received monetary reimbursement, and provided their written informed consent. Data were collected at the University of California San Diego.</p>
<p>In the scanner, both the sensory and memory tasks used full-contrast donut-shaped grating stimuli (1.5° and 7° inner and outer radius, respectively) with smoothed edges, a spatial frequency of 2 cycles per degree, random phase, a pseudo-randomized orientation. Stimuli were presented against a uniform grey background, and participants fixated a 0.4° central dot throughout. In the sensory task, donut-shaped gratings were presented in 9 second trials, contrast reversing at 5 Hz. Such donut trials were interleaved with trials showing a circular grating (1.5° radius), and fixation periods (10% of total). Grating contrast was briefly (200ms) and probabilistically reduced to 80% Michelson twice every 9s, and participants’ task was to report such contrast changes. Participants completed a total of 300-320 sensory task trials across 3 separate scanning sessions. The sensory task was also used to localize visually responsive voxels (via a donut &gt; circle contrast), and in our current analysis we use this contrast as a mask for all EVC ROI’s (but not IPS and LO, where all retinotopically defined voxels are included). In the working memory task, a target grating was shown for 500ms, and recalled 13 seconds later by rotating a white line (spanning 7°) for 3 seconds to match the remembered orientation. Between trials, there could be 3, 5, or 8 second fixation intervals. During the delay of two-thirds of memory task trials, a distractor of 50% Michelson contrast could be presented for 11 seconds during the middle portion of the delay. Distractors could be a grating (1/3<sup>rd</sup> of trials) or filtered noise (1/3<sup>rd</sup> of trials), contrast reversing at 4Hz. By ensuring uniform orientations of grating distractors with respect to memory targets, we are able to look at the representations of remembered and distractors orientations independently. Importantly, due to the negligible differences between trials with or without distractors, both in terms of behavior as well as decoding, we collapse the data across all working memory trials for our main analyses. Each participant completed 324 total working memory trials over the course of 3 different scanning sessions.</p>
<p>The independent psychophysical experiment was completed outside of the scanner, and stimuli consisted of gratings presented at 20% Michelson contrast against a uniform grey background. Gratings had a 2° radius, spatial frequency of 2 cycles per degree, random phase, and pseudo- randomized orientation. Each trial started with a 200ms target orientation that was remembered over a 3s delay, and recalled by rotating a dial to match the remembered orientation in an unspeeded manner. Intervals between trials were 800-1000ms. Grating distractors were presented for 200ms during the middle portion of the delay on 90% of trials. As in the scanner, targets and distractors were uncorrelated across trials, allowing for independent analysis of responses to the target. Any biases resulting from distractor presentation were small (Supplementary Figure 9 of ref<sup><xref ref-type="bibr" rid="c30">30</xref></sup>) and did not exert much influence on responses. Each participant completed 1620 trials over the course of several testing sessions.</p>
<p>For more detailed information on scanning and task procedures, please reference Rademaker et al. (2019).</p>
</sec>
<sec id="s4b">
<title>Models</title>
<p>First, the function that constrains both the veridical and categorical models, and emulates the frequency distribution of orientations in the natural world, is described by
<disp-formula>
<graphic xlink:href="541327v2_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Where −π &lt; <italic>x</italic> &lt; π, and <italic>b</italic> is any non-zero baseline (due to z-scoring before fitting this function is scale-free). This function is loosely based on the function defined in <sup><xref ref-type="bibr" rid="c38">38</xref></sup>. Another way to think about this theoretical “input statistics” function, is as the normalized amount of Fisher information at each orientation in orientation-space. We ensured that the results of our model fits were robust to the specific shape of the input function by not only using a theoretical input function based on the statistics of the natural world (<xref rid="fig2" ref-type="fig">Figure 2A</xref>), but by also using a psychophysical input function (generated from independent psychophysical measurements, <xref rid="fig3" ref-type="fig">Figure 3A</xref>) as the basis for our two models. Irrespective of the input function used (theoretical based on previous literature, or psychophysical based on independent data), model generation, as described next, is identical. Note, for the alternative model in <xref rid="figs7" ref-type="fig">Supplementary Figure 7</xref>, this function is also used, with the difference that the exponent (here set to 2) is a free parameter over range [0 inf).</p>
<p>For the veridical model, we assume a set of idealized tuning functions (<xref rid="fig2" ref-type="fig">Figure 2B</xref>, top) and use it to simulate neural responses to all possible stimulus orientations. From these simulated responses we calculate the similarity (rho) between all possible pairs of stimulus orientation (as shown in <xref rid="fig1" ref-type="fig">Figure 1B</xref>), resulting in a veridical model RSM (<xref rid="fig2" ref-type="fig">Figure 2B</xref>, bottom). Each tuning function in the veridical model is defined by a von Mises (circular analogue of a Gaussian distribution),
<disp-formula>
<graphic xlink:href="541327v2_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with a fixed concentration parameter <italic>k</italic> = 6, a center defined by <italic>μ</italic>, and −π &lt; <italic>x</italic> &lt; π. <italic>I</italic><sub>+</sub>(<italic>k</italic>) is the modified Bessel function of order 0. The amplitude <italic>a</italic> of each tuning function varies across orientation space as determined by the height of the input statistics function (i.e., from 0.1–1.1).</p>
<p>For the categorical model, we calculated the psychological distance between all possible pairs of orientation. For any pair of orientations, we sum over the approximated derivatives between these two points along the input statistics function as follows,
<disp-formula>
<graphic xlink:href="541327v2_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>x</italic> is an orientation in orientation space (and wraps around a circle), and <italic>y</italic> is the amount of normalized y-axis information for that orientation.</p>
<p>The veridical and categorical models described above were converted from radians to degree (spanning the entire orientation space from 1° to 180°, in steps of 1°) to match the dimensions of the data (also in degree). To evaluate if the representational structure in the data RSM’s (<xref rid="fig1" ref-type="fig">Figure 1C</xref>) is more or less similar to veridical or categorical model RSM’s (<xref rid="fig2" ref-type="fig">Figure 2B-C</xref>), both data and model RSM’s were normalized before fitting. The correlation between each model with the data RSM’s was done via semi-partial correlations, because the veridical and categorical models are not independent. Specifically, to evaluate the correlation between model A and the data RSM of a given ROI (<italic>RSM<sub>ROI</sub></italic>), we first removed the variance explained by model B (<italic>RSM<sup>B</sup></italic>) from model A (<italic>RSM<sup>A</sup></italic>), to get its model residuals (<italic>ε<sup>B</sup></italic>):
<disp-formula>
<graphic xlink:href="541327v2_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Where <italic>w<sup>B</sup></italic> are the initial weights of model B, and the residuals <italic>ε<sup>B</sup></italic> reflect any pattern in model A unaccounted for by model B. To illustrate, imagine the extreme case where model B explains none of the same variance as model A, then <italic>w<sup>B</sup></italic> would be 0, and the residuals <italic>ε<sup>B</sup></italic> would be equal to model A itself. Next, the residuals of model B are correlated to the data <italic>RSM<sub>ROI</sub></italic>, and we apply a Fisher transformation (also known as inverse hyperbolic tangent, or <italic>tanha<sup>-1</sup></italic> to normalize the bound correlation data (from –1 to 1), to unbound values (–Inf to Inf) suitable for statistical testing.
<disp-formula>
<graphic xlink:href="541327v2_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This transformed correlation gives the amounts of variance explained by model A independent of model B. To summarize, this procedure ensures that any resemblance (big or small) between the two models is accounted for first, after which we obtain the unique contribution made by model A.</p>
<p>To ensure that our results did not depend on the specific fitting approach described above, we also fit our two models directly to the data without taking into account the overlap between the models. The difference in model fits is still informative in this case (showing which model fits the data better), which was confirmed by statistical replication of the main findings. Second, we used general linear regression to fit both models simultaneously in the form of:
<disp-formula>
<graphic xlink:href="541327v2_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Where <italic>β</italic> reflects the beta weight for each of our two model RSM’s. Also using this procedure, the statistics of our effects replicated. For a figure showing both these approaches, see <xref rid="figs4" ref-type="fig">Supplementary Figure 4</xref></p>
</sec>
<sec id="s4c">
<title>Across-task RSA fidelity</title>
<p>To directly relate cross-generalization from decoding (or more specifically, from the inverted encoding model, or “IEM”, as used in ref<sup><xref ref-type="bibr" rid="c30">30</xref></sup>) to our novel “across-task RSA” (<xref rid="fig4" ref-type="fig">Figure 4</xref>), we calculate a fidelity metric to quantify how much information there is about the remembered orientation based on the pattern responses to the perceived orientations. Because orientation is a continuous variable, we also take into account the representational similarity to orientations nearby the remembered one, and the expected drop in similarity at increasingly larger distances in orientation space. For each correlation profile (see <xref rid="fig4" ref-type="fig">Figure 4B–D</xref>) we calculate fidelity in a manner identical to how this has been calculated for IEM channel reconstructions (in ref<sup><xref ref-type="bibr" rid="c30">30</xref></sup>”, and shown here as the y-axis in <xref rid="fig4" ref-type="fig">Figure 4D</xref>). Specifically, we take the correlation value at each degree in orientation space (wrapped onto a 2π circle), and project this vector onto the remembered orientation (centered to zero degrees) via <inline-formula><inline-graphic xlink:href="541327v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>A</italic> is the angle between the remembered orientation (at <italic>0°</italic>) and the degree in orientation space being evaluated (<italic>d</italic>), and <italic>h</italic> is the correlation value at <italic>d</italic> (i.e. the hypotenuse of a right triangle). This procedure was repeated for all 180 degrees in orientation space, and we then calculate the mean of all 180 projected vectors. This fidelity metric captures the amount of information at the remembered orientation, and removes additive offsets.</p>
</sec>
<sec id="s4d">
<title>Analyses of 2<sup>nd</sup> level RSM and representational connectivity</title>
<p>To compare the representational geometry across all retinotopic ROI’s during perception and working memory, we employed two approaches first described by<sup><xref ref-type="bibr" rid="c41">41</xref></sup>: A 2<sup>nd</sup> level RSA and a ‘representational connectivity’ analysis. Note that for these analyses we used the smallest possible ROI’s that were retinotopically defined, meaning we split early visual areas into their dorsal and ventral parts, and used the individual sub-areas of IPS and LO. For the 2<sup>nd</sup> level RSA, we calculated the similarity of each across-subject RSM (as shown in <xref rid="figs1" ref-type="fig">Supplementary Figure 1</xref>) to every other across-subject RSM using spearman correlation (as RSM’s are monotonically, but perhaps not linearly related). This resulted in the 2<sup>nd</sup> level RSM’s in <xref rid="fig5" ref-type="fig">Figure 5A</xref>, showing representational similarity between all retinotopic ROI’s during perception, and during working memory. For the ‘representational connectivity’ we similarly computed Spearman correlations between ROI’s but at a within-subject level as is the recommended procedure<sup><xref ref-type="bibr" rid="c41">41</xref></sup>. Across-subject averages are visualized in a graph (<xref rid="fig5" ref-type="fig">Figure 5B</xref>) where each node signifies a ROI, and each edge signifies the correlation coefficient to each other ROI. Thicker and shorter edges indicate higher similarity. We computed degree centrality for each node as the sum of all edges, depicted by the saturation of each node (less saturation indicating higher degree centrality). Thus, higher degree centrality indicates that a ROI shares its representational geometry either strongly with a few, or somewhat strongly with many, other ROI’s.</p>
</sec>
<sec id="s5">
<title>Statistics</title>
<p>First, for the main analyses (<xref rid="fig2" ref-type="fig">Figures 2</xref> and <xref rid="fig3" ref-type="fig">3</xref>) we tested if model fits differed between task (sensory or memory), model (veridical or categorical), or ROI’s by running a three-way repeated measures ANOVA using R (version 4.1.1) and RStudio (version 1.4.1717). Significant three-way interactions (between task, ROI, and model) were followed up with two targeted two-way repeated-measures ANOVA’s – one for the sensory task, and one for the memory task (as described in the main text). Significant interactions arising from two-way ANOVA’s were further examined via post-hoc two-sided paired-sample t-tests within each task and ROI (uncorrected for multiple comparisons). For the main model fitting results using the theoretical and behavioral input functions (<xref rid="fig2" ref-type="fig">Figure 2D</xref> and <xref rid="fig3" ref-type="fig">3C</xref>, respectively) these t-tests are reported in detail in <xref rid="tbls1" ref-type="table">Supplementary Table 1</xref>.</p>
<p>For the second order RSA (<xref rid="fig5" ref-type="fig">Figure 5</xref>) we tested if there was a significant difference between perception and working memory in terms of the overall correlations between ROIs (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). First, we calculated the mean correlation between all ROI’s (excluding the diagonal) for a given subject within each task. During the sensory task the mean correlation between all ROI’s was r = 0.32 (+0.082 sd), and during the memory task it was r = 0.553 (+ 0.132 sd). Next, we performed a permutation test where we shuffled the assignment of each ROI-ROI correlation pair (i.e., each value in the 2<sup>nd</sup> order RSM, excluding the diagonal) to either the sensory or memory task at random for each subject. On each permutation we then calculated the mean correlation between all ROI’s within the shuffled sensory and shuffled memory RSM’s, and took the difference between these means. A null distribution of differences was generated over 1000 permutations, and compared to the real difference between the mean correlations (i.e., 0.553 – 0.32 = 0.233), which was significantly greater than expected by chance (p &lt; 0.001).</p>
</sec>
<sec id="s6">
<title>Code accessibility</title>
<p>The data are public and can be accessed via the Open Science Framework (OSF) at <ext-link ext-link-type="uri" xlink:href="https://osf.io/dkx6y">https://osf.io/dkx6y</ext-link> which has an accompanying wiki. The code for the analyses in this paper can be found at <ext-link ext-link-type="uri" xlink:href="https://osf.io/ej9db/">https://osf.io/ej9db/</ext-link>.</p>
</sec>
</sec>
</body>
<back>
<sec>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 1:</label>
<caption><p>Orientation representational geometry (as indexed by RSM’s) during sensory perception and working memory for all retinotopically defined ROI’s (across all participants) that were not already shown in <xref rid="fig1" ref-type="fig">Figure 1C</xref>. Here, ROI’s are organized by whether they are located in the dorsal or ventral stream (top and bottom two rows, respectively). Early visual areas V1–V3 were split by their dorsal and ventral portions – used as input to the second- level RSA analysis (<xref rid="fig5" ref-type="fig">Figure 5</xref> of the main text). Areas IPS1–3 (in the dorsal stream) and LO (in the ventral stream) were split based on their respective sub-portions – and similarly used as input to the second-level RSA analyses. All RSM’s are scaled to the range of correlations within each subplot to ease visual comparison of representational structure.</p></caption>
<graphic xlink:href="541327v2_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 2:</label>
<caption><p>With 180 possible target orientations, and a finite number of trials, some form of smoothing or binning is necessary for RSA to yield reliable correlations. In our main analysis we smooth over a window of +10°, which could in theory impact the geometry around categorical boundaries (90° and 180°). In particular, it could induce some smearing of the categorical pattern observed in the memory task. To ensure that this pattern does not critically depend on the way trials are combined, here we show the data for the memory task binned (instead of smoothed) into 12 bins of 15°. On top, we show RSM’s with bins <italic>centered on</italic> the 2 cardinals and the 2 obliques (see inset), meaning that the parts of orientation space highlighted in dark-red are bins that include a cardinal or an oblique orientation. On the bottom, we show the same analysis but with the bins shifted, such that they respect cardinal and oblique boundaries, and bins <italic>fall on either side</italic>. We observe that similarity in bins that include a cardinal (top row) is relatively low, presumably due to the relatively large psychological distance between orientations on different sides of a cardinal orientation, resulting in lower correlations. Nevertheless, there is relatively low similarity around cardinals <italic>also</italic> when we respect the categorical boundary (bottom row), implying these categorical effects are not impacted much by the specific binning approach. Overall, binning or smoothing do not drastically change the geometry (though of course, the resolution of the RSM is much lower with binning).</p></caption>
<graphic xlink:href="541327v2_figs2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 3:</label>
<caption><p>Exact ranges of correlations in the RSM’s from <xref rid="fig1" ref-type="fig">Figure 1C</xref>. To best show the representational structure for sensory and memory representations across ROI’s, and to ease comparison between them, the RSM’s in <xref rid="fig1" ref-type="fig">Figure 1C</xref> are scaled to the range (min-to-max) of correlations within each subplot. But the minimum and maximum correlations are not identical across subplots, therefore, correlation ranges across all participants (black rectangles) and individual participants (grey lines) are shown here for sensory (left) and memory (right) RSM’s.</p></caption>
<graphic xlink:href="541327v2_figs3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 4:</label>
<caption><title>Two alternative fitting approaches.</title><p><bold>(A)</bold> Model weights when fitting the veridical and categorical models directly to the RSM’s (without first taking the residuals), and <bold>(B)</bold> model weights derived with a general linear regression (independent weights for each model). Irrespective of the fitting approach, the geometrical differences between our two tasks are captured by higher “veridical” weights in the sensory task, and more “categorical” weights in the memory task. For the “direct fitting” approach (in <bold>A</bold>) there is a significant 3-way interaction (model x ROI x task, F<sub>(7,35)</sub> = 2.413; p = 0.0398), which we followed up by post-hoc ANOVA’s within the sensory and memory task separately. There is a main effect of model in both the sensory (F<sub>(1,5)</sub> = 40.26; p = 0.001) and memory (F<sub>(1,5)</sub> = 12.47; p = 0.017) tasks that is not the same in all ROI’s (as indexed by model x ROI interactions for sensory F<sub>(7,35)</sub> = 3.262, p = 0.009 and memory F<sub>(7,35)</sub> = 2.791, p = 0.024 tasks). Similarly, for the general linear regression approach (in <bold>B</bold>) there is also a significant 3-way interaction (model x ROI x task, F<sub>(7,35)</sub> = 2.414; p = 0.0398), and main effects of model in both the sensory (F<sub>(1,5)</sub> = 40.28, p = 0.001) and memory (F<sub>(1,5)</sub> = 12.48, p = 0.017) tasks, and this difference between the models is not the same in all ROI’s (as indexed by model x ROI interactions for both sensory F<sub>(7,35)</sub> = 3.26, p = 0.009, and memory F<sub>(7,35)</sub> = 2.79, p = 0.024 tasks). Asterisks indicate the significance level of post-hoc two-sided paired- sample t-tests (*p &lt; 0.05; **p &lt; 0.01) comparing the two models in each ROI.</p></caption>
<graphic xlink:href="541327v2_figs4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 5:</label>
<caption><p>Model fits for the 3 different working memory distractor conditions. Overall, the results split by condition are qualitatively similar to the main results across all trails (<xref rid="fig2" ref-type="fig">Figure 2D</xref>, bottom panel). Two-way ANOVA’s comparing model and ROI showed that the categorical model did a better job at explaining the data in some of the ROI on trials without a distractor (model x ROI interaction: F<sub>(7,35)</sub> = 2.914; p = 0.016; main effect model: F<sub>(1,5)</sub> = 13.07; p = 0.015) and with a grating distractor (model x ROI interaction: F<sub>(7,35)</sub> = 4.3; p = 0.0016; main effect model: F<sub>(1,5)</sub> = 6.344; p = 0.053), indicating increasing differences between the veridical and categorical models along the visual hierarchy. While we see similar trends for the 108 trials with a noise distractor, these effects did not reach significance (model x ROI interaction: F<sub>(7,35)</sub> = 1.812; p = 0.116; main effect model: F<sub>(1,5)</sub> = 1.335; p = 0.3). Nevertheless, despite using only 1/3<sup>rd</sup> of the data in each of these sub-plots, the pattern in the data is highly consistent. Asterisks indicate the significance level of post-hoc two-sided paired-sample t-tests (*p &lt; 0.05; **p &lt; 0.01) comparing the two models in each ROI.</p></caption>
<graphic xlink:href="541327v2_figs5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 6:</label>
<caption><title>Orientation inhomogeneities of the representational geometry</title><p><bold>(A)</bold> To examine the inhomogeneity or representational similarity throughout orientation space, we plot the diagonals of the RSM’s from in <xref rid="fig1" ref-type="fig">Figure 1C</xref>. During the sensory task, we see that similarity tends to be relatively high around vertical orientations (0°/180°) compared to horizontal orientations (90°). For both tasks, oblique orientations are represented relatively more similar, and cardinals less similar. This “oblique” like effect is much exacerbated in the memory task compared to the sensory task. <bold>(B)</bold> We use multidimensional scaling (MDS) to projects high dimensional response patterns into 2 dimensions, in order to better visualize of how orientation space is represented. During the sensory task there is an orderly geometrical progression of orientation space, with the highest similarity between adjacent orientations (and some clustering around cardinal orientations, especially 180°) in early visual areas V1–V3. There’s also a “pinching” of orientation space around the obliques (45° and 135° become very similar) in more anterior visual areas V3AB–IPS and LO. During the memory task, the orientation space geometry remains circular in all ROI’s, with notable clustering of similarity around the obliques.</p></caption>
<graphic xlink:href="541327v2_figs6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 7:</label>
<caption><title>Alternative model based on psychological distance.</title><p><bold>(A)</bold> An alternative way to model the sensory and memory task RSM’s is to vary the degree of similarity that can be expected at cardinals or at obliques. By changing the exponent in the input statistics function <italic>f</italic>(<italic>x</italic>) = &amp;|sin <italic>x</italic>| −1&amp;<sup><italic>exponent</italic></sup> + 0.1 to a free parameter, and using the psychological distance between every pair of orientations (as in the categorical model), we can create a family of input statistics functions (left panel) that modulate the shape of the model RSM’s (right panels) such that we can span any orientation anisotropy ranging from highest similarity around cardinals to highest similarity around obliques (similar to the modeling approach in <sup><xref ref-type="bibr" rid="c57">57</xref></sup>). Each of the input functions in the left panel matches an RSM in the right panels (with the input function overlaid in white). At an exponent of 0.55 we approximate a uniform diagonal RSM, or a “physical” model of orientation space. Note that by modulating the shape of the input function in this manner, we can retrieve models that look very similar to our veridical model (e.g., exponent = 0.4), and a model identical to our categorical model (exponent = 2) in this parametric RSM space. <bold>(B)</bold> We plot the best fitting exponent for the input function for all ROI’s (x-axis) and separately for the memory (dark blue) and sensory (light blue) tasks, and show that those differ significantly (ROI x task interaction: F<sub>(7,35)</sub> = 9.658; p &lt; 0.001). For the sensory task the best fitting exponent stays close to 0.55 for all ROI’s, indicating that an RSM with a close-to uniform diagonal fits the data well. That said, the exponent does differ across ROI’s (main effect of roi, F<sub>(7,35)</sub> = 3.307; p = 0.008), showing that a model with slightly higher similarity around obliques (exponent &gt; 0.55) does better in for example V1, while a model with slightly higher similarity at cardinals (exponent between 0 and 0.55) does better at for example V4. For the memory task we see a gradual increase in the exponent along the visual hierarchy (up to and including IPS0), indicating that a model with increasingly stronger similarity around oblique orientations (i.e., increasingly stronger categorization) is better at explaining the memory geometry for more anterior ROI’s (main effect of roi, F<sub>(7,35)</sub> = 9.213; p &lt; 0.001). Best fitting exponents for individual subjects are shown as dots, and error bars indicate + 1 within-subject SEM.</p></caption>
<graphic xlink:href="541327v2_figs7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Supplementary Table 1:</label>
<caption><p>Post-hoc statistics for two-sided paired t-tests from the theoretical input function based on the statistics in the natural world (in green) and from the psychophysical input function based on independent behavioral measurements (in blue). All significant cells are colored in a lighter shade for the purpose of quick visualization.</p></caption>
<graphic xlink:href="541327v2_tbls1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<ack>
<title>Acknowledgements</title>
    <p>Data acquisition was financed by NEI R01-EY025872 and NIMH R01-MH087214 to John Serences – whose unwavering and continued support has shaped us as scientists. CC is funded by grants for development of new faculty staff, Rachadaphiseksomphot fund, Chulalongkorn University, RLR and MDH are funded by the Max Planck Society, MDH is also funded by the German Federal Ministry of Education and Research (BMBF). We thank the IMP lab at Goethe University &amp; Rademaker lab members at ESI for feedback on the manuscript. We also thank Xue-Xin Wei and Tim Brady for seeding the thoughts that changed our minds, culminating to the modeling work in this paper.</p>
</ack>
    <sec id="nt1">
        <title>Note</title>
        <p>This reviewed preprint has been updated to make minor edits to an affiliation and acknowledgments.</p>
    </sec>
    
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harrison</surname>, <given-names>S. A.</given-names></string-name> &amp; <string-name><surname>Tong</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title>. <source>Nature</source>, <volume>458</volume>, <fpage>632</fpage>–<lpage>635</lpage></mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Ester</surname>, <given-names>E. F.</given-names></string-name>, <string-name><surname>Vogel</surname>, <given-names>E. K.</given-names></string-name> &amp; <string-name><surname>Awh</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Stimulus-specific delay activity in human primary visual cortex</article-title>. <source>Psych. Sci</source>., <volume>20</volume>, <fpage>207</fpage>–<lpage>214</lpage></mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Christophel</surname>, <given-names>T.B.</given-names></string-name>, <string-name><surname>Hebart</surname>, <given-names>M.N.</given-names></string-name> &amp; <string-name><surname>Haynes</surname>, <given-names>J.D</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Decoding the contents of visual short-term memory from human visual and parietal cortex</article-title>. <source>J. Neurosci</source>., <volume>32</volume>, <fpage>12983</fpage>–<lpage>12989</lpage></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riggall</surname>, <given-names>A.C.</given-names></string-name> &amp; <string-name><surname>Postle</surname>, <given-names>B.R</given-names></string-name></person-group>. (<year>2012</year>). <article-title>The relationship between working memory storage and elevated activity as measured with functional magnetic resonance imaging</article-title>. <source>J. Neurosci</source>., <volume>32</volume>, <fpage>12990</fpage>–<lpage>12998</lpage></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ester</surname>, <given-names>E.F.</given-names></string-name>, <string-name><surname>Sprague</surname>, <given-names>T.C.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Parietal and Frontal Cortex Encode Stimulus-Specific Mnemonic Representations during Visual Working Memory</article-title>, <source>Neuron</source>, <volume>87</volume>(<issue>4</issue>), <fpage>893</fpage>–<lpage>905</lpage></mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bettencourt</surname>, <given-names>K.C.</given-names></string-name> &amp; <string-name><surname>Xu</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Decoding the content of visual short-term memory under distraction in occipital and parietal areas</article-title>. <source>Nat. Neurosci</source>., <volume>19</volume>, <fpage>150</fpage>–<lpage>157</lpage></mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lorenc</surname>, <given-names>E.S.</given-names></string-name>, <string-name><surname>Sreenivasan</surname>, <given-names>K.K.</given-names></string-name>, <string-name><surname>Nee</surname>, <given-names>D.E.</given-names></string-name>, <string-name><surname>Vandenbroucke</surname>, <given-names>A.R.E.</given-names></string-name> &amp; <string-name><surname>D’Esposito</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Flexible coding of visual working memory representations during distraction</article-title>. <source>J. Neurosci</source>., <volume>38</volume>, <fpage>5267</fpage>–<lpage>5276</lpage></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Christophel</surname>, <given-names>T. G.</given-names></string-name>, <string-name><surname>Iamshchinina</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Yan</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Allefeld</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Haynes</surname>, <given-names>J. D</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Cortical specialization for attended versus unattended working memory</article-title>. <source>Nat. Neurosci</source>., <volume>21</volume>, <fpage>494</fpage>–<lpage>496</lpage></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fuster</surname>, <given-names>J.M.</given-names></string-name>, &amp; <string-name><surname>Alexander</surname>, <given-names>G.E</given-names></string-name></person-group>., (<year>1971</year>). <article-title>Neuron activity related to short-term memory</article-title>. <source>Science</source>, <volume>173</volume>, <fpage>652</fpage>–<lpage>54</lpage></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Funahashi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bruce</surname>, <given-names>C.J.</given-names></string-name>, &amp; <string-name><surname>Goldman-Rakic</surname>, <given-names>P.S</given-names></string-name></person-group>. (<year>1989</year>). <article-title>Menomonic coding of visual space in the monkey’s dorsolateral prefrontal cortex</article-title>. <source>J. Neurophysiol</source>., <volume>61</volume>, <fpage>331</fpage>–<lpage>49</lpage></mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Funahashi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Chafee</surname>, <given-names>M.V.</given-names></string-name>, <string-name><surname>Goldman-Rakic</surname>, <given-names>P.S</given-names></string-name></person-group>. (<year>1993</year>). <article-title>Prefrontal neuronal activity in rhesus monkeys performing a delayed anti-saccade task</article-title>. <source>Nature</source>, <volume>365</volume>, <fpage>753</fpage>–<lpage>56</lpage></mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname>, <given-names>F.A.</given-names></string-name>, <string-name><surname>Scalaidhe</surname>, <given-names>S.P.</given-names></string-name> and <string-name><surname>Goldman-Rakic</surname>, <given-names>P.S</given-names></string-name></person-group>. (<year>1993</year>). <article-title>Dissociation of object and spatial processing domains in primate prefrontal cortex</article-title>. <source>Science</source>, <volume>260</volume>(<issue>5116</issue>), <fpage>1955</fpage>–<lpage>58</lpage></mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCarthy</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Blamire</surname>, <given-names>A.M.</given-names></string-name>, <string-name><surname>Puce</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nobre</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Bloch</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Hyder</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Goldman-Rakic</surname>, <given-names>P.</given-names></string-name> and <string-name><surname>Shulman</surname>, <given-names>R.G</given-names></string-name></person-group>. (<year>1994</year>). <article-title>Functional magnetic resonance imaging of human prefrontal cortex activation during a spatial working memory task</article-title>. <source>PNAS</source>, <volume>91</volume>(<issue>18</issue>), <fpage>8690</fpage>–<lpage>94</lpage></mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname>, <given-names>H.R.</given-names></string-name> &amp; <string-name><surname>Goldman-Rakic</surname>, <given-names>P.S</given-names></string-name></person-group>. (<year>1994</year>). <article-title>Coactivation of prefrontal cortex and inferior parietal cortex in working memory tasks revealed by 2DG functional mapping in the rhesus monkey</article-title>. <source>Journal of Neuroscience</source>, <volume>14</volume>(<issue>5</issue>), <fpage>2775</fpage>–<lpage>88</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goldman-Rakic</surname>, <given-names>P.S</given-names></string-name></person-group>. (<year>1995</year>) <article-title>Cellular basis of working memory</article-title>. <source>Neuron</source>, <volume>14</volume>, <fpage>477</fpage>–<lpage>485</lpage></mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCarthy</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Puce</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Constable</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Krystal</surname>, <given-names>J.H.</given-names></string-name>, <string-name><surname>Gore</surname>, <given-names>J.C.</given-names></string-name> and <string-name><surname>Goldman-Rakic</surname>, <given-names>P</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Activation of human prefrontal cortex during spatial and nonspatial working memory tasks measured by functional MRI</article-title>. <source>Cerebral cortex</source>, <volume>6</volume>(<issue>4</issue>), <fpage>600</fpage>–<lpage>11</lpage></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname>, <given-names>E.K.</given-names></string-name>, <string-name><surname>Erickson</surname>, <given-names>C.A.</given-names></string-name> &amp; <string-name><surname>Desimone</surname>, <given-names>R</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Neural mechanisms of visual working memory in prefrontal cortex of the macaque</article-title>. <source>J. Neurosci</source>., <volume>16</volume>, <fpage>5154</fpage>–<lpage>5167</lpage></mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chafee</surname>, <given-names>M.V.</given-names></string-name> &amp; <string-name><surname>Goldman-Rakic</surname>, <given-names>P.S</given-names></string-name></person-group>. (<year>1998</year>). <article-title>Matching patterns of activity in primate prefrontal area 8a and parietal area 7ip neurons during a spatial working memory task</article-title>. <source>Journal of neurophysiology</source>, <volume>79</volume>(<issue>6</issue>), <fpage>2919</fpage>–<lpage>40</lpage></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Courtney</surname>, <given-names>S.M.</given-names></string-name>, <string-name><surname>Petit</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Maisog</surname>, <given-names>J.M.</given-names></string-name>, <string-name><surname>Ungerleider</surname>, <given-names>L.G.</given-names></string-name> &amp; <string-name><surname>Haxby</surname>, <given-names>J.V</given-names></string-name></person-group>. (<year>1998</year>). <article-title>An area specialized for spatial working memory in human frontal cortex</article-title>. <source>Science</source>, <volume>279</volume>(<issue>5355</issue>), <fpage>1347</fpage>–<lpage>51</lpage></mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qi</surname>, <given-names>X.-L.</given-names></string-name>, <string-name><surname>Elworthy</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Lambert</surname>, <given-names>B.C.</given-names></string-name> &amp; <string-name><surname>Constantinidis</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Representation of remembered stimuli and task information in the monkey dorsolateral prefrontal and posterior parietal cortex</article-title>. <source>J. Neurophysiol</source>., <volume>113</volume>, <fpage>44</fpage>–<lpage>57</lpage></mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Serences</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Neural mechanisms of information storage in visual short-term memory</article-title>. <source>Vis. Res</source>., <volume>128</volume>, <fpage>53</fpage>–<lpage>67</lpage></mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gayet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Paffen</surname>, <given-names>C.L.E.</given-names></string-name>, <string-name><surname>Van der Stigchel</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Visual Working Memory Storage Recruits Sensory Processing Areas</article-title>. <source>Tics</source>, <volume>22</volume>(<issue>3</issue>), <fpage>189</fpage>–<lpage>190</lpage></mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Compte</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Goldman-Rakic</surname>, <given-names>P.S.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X-J</given-names></string-name></person-group> (<year>2000</year>). <article-title>Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model</article-title>. <source>Cerebral Cortex</source>, <volume>10</volume>(<issue>9</issue>), <fpage>910</fpage>–<lpage>23</lpage></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wimmer</surname> <given-names>K.</given-names></string-name>, <string-name><surname>Nykamp</surname> <given-names>D.Q.</given-names></string-name>, <string-name><surname>Constantinidis</surname> <given-names>C.</given-names></string-name>, <string-name><surname>Compte</surname> <given-names>A</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory</article-title>. <source>Nat Neurosci</source>., <volume>17</volume>(<issue>3</issue>), <fpage>431</fpage>–<lpage>9</lpage></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mejías</surname>, <given-names>J.F.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X-J</given-names></string-name></person-group> (<year>2022</year>). <article-title>Mechanisms of distributed working memory in a large-scale network of macaque neocortex</article-title>. <source>eLife</source>, <volume>11</volume>:<elocation-id>e72136</elocation-id></mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elston</surname> <given-names>G.N.</given-names></string-name>, <string-name><surname>Benavides-Piccione</surname> <given-names>R.</given-names></string-name>, <string-name><surname>Elston</surname> <given-names>A.</given-names></string-name>, <string-name><surname>Manger</surname> <given-names>P.R.</given-names></string-name> &amp; <string-name><surname>Defelipe</surname> <given-names>J</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Pyramidal cells in prefrontal cortex of primates: marked differences in neuronal structure among species</article-title>. <source>Frontiers in Neuroanatomy</source>, <volume>5</volume>(<fpage>2</fpage>)</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chota</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gayet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kenemans</surname> <given-names>J.L.</given-names></string-name>, <string-name><surname>Olivers</surname>, <given-names>C.N.L.</given-names></string-name>, &amp; <string-name><surname>Van der Stigchel</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2023</year>). <article-title>A matter of availability: sharper tuning for memorized than for perceived features</article-title>. <source>Cereb Cortex</source>, <fpage>1</fpage>–<lpage>11</lpage></mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dijkstra</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Bosch</surname>, <given-names>S.E.</given-names></string-name>, &amp; <string-name><surname>van Gerven</surname>, <given-names>M.A.J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Shared neural mechanisms of visual perception and imagery</article-title>. <source>Tics</source>, <volume>23</volume>(<issue>5</issue>), <fpage>423</fpage>–<lpage>434</lpage></mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luo</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Collins</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2023</year>). <article-title>The representational similarity between visual perception and recent perceptual history</article-title>. <source>J. Neurosci</source>., <volume>43</volume>(<issue>20</issue>), <fpage>3658</fpage>–<lpage>65</lpage></mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Parietal-driven visual working memory representations in occipito-temporal cortex</article-title>. <source>Current Biology</source>, <volume>33</volume>, <fpage>4516</fpage>–<lpage>23</lpage></mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rademaker</surname>, <given-names>R.L.</given-names></string-name>, <string-name><surname>Chunharas</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Serences</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>, <fpage>1336</fpage>–<lpage>1344</lpage></mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kwak</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Curtis</surname>, <given-names>C.E</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Unveiling the abstract format of mnemonic representations</article-title>. <source>Neuron</source>, <volume>110</volume>(<issue>11</issue>), <fpage>1822</fpage>–<lpage>28</lpage></mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Panichello</surname>, <given-names>M.F.</given-names></string-name>, <string-name><surname>DePasquale</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Pillow</surname>, <given-names>J.W.</given-names></string-name>, &amp; <string-name><surname>Bischman</surname>, <given-names>T.J</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Error -correcting dynamics in visual working memory</article-title>. <source>Nature Communications</source>, <volume>10</volume>:<fpage>3366</fpage></mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bae</surname>, <given-names>G-Y</given-names></string-name>, <string-name><surname>Olkkonen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Allred</surname>, <given-names>S.R.</given-names></string-name>, &amp; <string-name><surname>Flombaum</surname>. <given-names>J. I.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Why some colors appear more memorable than others: A model combining categories and particulars in color working memory</article-title>. <source>Jep:gen</source>, <volume>144</volume>(<issue>4</issue>): <fpage>744</fpage>–<lpage>63</lpage></mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>, <given-names>X-X.</given-names></string-name>, &amp; <string-name><surname>Stocker</surname>, <given-names>A.A</given-names></string-name></person-group>., (<year>2017</year>). <article-title>LAwful relation between perceptual bias and discriminability</article-title>. <source>PNAS</source>, <volume>114</volume>(<issue>38</issue>): <fpage>10244</fpage>–<lpage>49</lpage></mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Favila</surname>, <given-names>S.E.</given-names></string-name>, <string-name><surname>Kuhl</surname>, <given-names>B.A.</given-names></string-name> &amp; <string-name><surname>Winawer</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Perception and memory have distinct spatial tuning properties in human visual cortex</article-title>. <source>Nat Commun</source>, <volume>13</volume>, <fpage>5864</fpage></mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vo</surname>, <given-names>V.A.</given-names></string-name>, <string-name><surname>Sutterer</surname>, <given-names>D.W.</given-names></string-name>, <string-name><surname>Foster</surname>, <given-names>J.J.</given-names></string-name>, <string-name><surname>Sprague</surname>, <given-names>T.C.</given-names></string-name>, <string-name><surname>Awh</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Shared Representational Formats for Information Maintained in Working Memory and Information Retrieved from Long-Term Memory</article-title>. <source>Cereb Cortex</source>, <volume>32</volume>(<issue>5</issue>), <fpage>1077</fpage>–<lpage>92</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname>, <given-names>M.G</given-names></string-name></person-group>. (<year>2015</year>). <article-title>‘Activity-silent’ working memory in prefrontal cortex: a dynamic coding framework</article-title>. <source>Trends Cog. Sci</source>., <volume>19</volume>, <fpage>394</fpage>–<lpage>405</lpage></mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Wei</surname>, <given-names>X-X</given-names></string-name></person-group> (<year>2021</year>), <article-title>Neural tuning and representational geometry</article-title>. <source>Nat. Rev. Neurosci</source>., <volume>22</volume>, <fpage>703</fpage>–<lpage>18</lpage></mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname>, <given-names>J.D.</given-names></string-name>, <string-name><surname>Bernacchia</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Roy</surname>, <given-names>N.A.</given-names></string-name>, <string-name><surname>Constantinidis</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Romo</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Wang</surname>, <given-names>X-J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex</article-title>. <source>PNAS</source>, <volume>114</volume>(<issue>2</issue>), <fpage>394</fpage>–<lpage>399</lpage></mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spaak</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Watanabe</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Funahashi</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Stokes</surname>, <given-names>M.G</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Stable and Dynamic Coding for Working Memory in Primate Prefrontal Cortex</article-title>. <source>J. Neurosci</source>., <volume>37</volume>(<issue>27</issue>), <fpage>6503</fpage>–<lpage>6516</lpage></mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Girshick</surname>, <given-names>A.R.</given-names></string-name>, <string-name><surname>Landy</surname>, <given-names>M.S.</given-names></string-name>, &amp; <string-name><surname>Simoncelli</surname>, <given-names>E.P</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Cardinal rules: Visual orientation perception reflects knowledge of environmental statistics</article-title>. <source>Nature Neuroscience</source>, <volume>14</volume>(<issue>7</issue>), <fpage>926</fpage>–<lpage>32</lpage></mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname> <given-names>X.X.</given-names></string-name>, <string-name><surname>Stocker</surname> <given-names>A.A</given-names></string-name></person-group>. (<year>2015</year>). <article-title>A Bayesian observer model constrained by efficient coding can explain ’anti- Bayesian’ percepts</article-title>. <source>Nat Neurosci</source>., <volume>18</volume>(<issue>10</issue>), <fpage>1509</fpage>–<lpage>17</lpage></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sprague</surname>, <given-names>T.C.</given-names></string-name>, <string-name><surname>Ester</surname>, <given-names>E.F.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Restoring latent visual working memory representations in human cortex</article-title>. <source>Neuron</source>, <volume>91</volume>: <fpage>694</fpage>–<lpage>707</lpage></mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rose</surname>, <given-names>N.S.</given-names></string-name>, <string-name><surname>LaRocque</surname>, <given-names>J.J.</given-names></string-name>, <string-name><surname>Riggall</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Gosseries</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Starrett</surname>, <given-names>M.J.</given-names></string-name>, <string-name><surname>Meyering</surname>, <given-names>E.E.</given-names></string-name>, &amp; <string-name><surname>Postle</surname>, <given-names>B.R</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Reactivation of latent working memories with transcranial magnetic stimulation</article-title>. <source>Science</source>, <volume>354</volume>(<issue>6316</issue>): <fpage>1136</fpage>–<lpage>39</lpage></mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Bandettini</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Representational similarity analysis – connecting the branches of systems neuroscience</article-title>. <source>Front. Syst. Neurosci</source>., (<issue>2</issue>)</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ruff</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kiani</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bodurka</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Esteky</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Tanaka</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bandettini</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title>. <source>Neuron</source>, <volume>60</volume>(<issue>6</issue>), <fpage>1126</fpage>–<lpage>41</lpage></mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walther</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nili</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ejaz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Diedrichsen</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title>. <source>Neuroimage</source>, <volume>137</volume>, <fpage>188</fpage>–<lpage>200</lpage></mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Attneave</surname>, <given-names>F</given-names></string-name></person-group>. (<year>1954</year>). <article-title>Some informational aspects of visual perception</article-title>. <source>Psychological Review</source>, <volume>61</volume>(<issue>3</issue>), <fpage>183</fpage>–<lpage>193</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shannon</surname>, <given-names>C.E</given-names></string-name></person-group>. (<year>1948</year>). <article-title>A mathematical theory of communication</article-title>. <source>The Bell system technical journal</source>, <volume>27</volume>(<issue>3</issue>), <fpage>379</fpage>–<lpage>423</lpage></mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barlow</surname>, <given-names>H.B</given-names></string-name></person-group>. (<year>1961</year>). <article-title>Possible principles underlying the transformation of sensory messages</article-title>. <source>Sensory communication</source>, <volume>1</volume>(<issue>01</issue>), <fpage>217</fpage>–<lpage>33</lpage></mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olshausen</surname>, <given-names>B.A.</given-names></string-name>, &amp; <string-name><surname>Field</surname>, <given-names>D.J</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title> <source>Vision research</source>, <volume>37</volume>(<issue>23</issue>), <fpage>3311</fpage>–<lpage>3325</lpage></mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name>, &amp; <string-name><surname>Olshausen</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>2001</year>). <article-title>Natural image statistics and neural representation</article-title>. <source>Annual review of neuroscience</source>, <volume>24</volume>(<issue>1</issue>), <fpage>1193</fpage>–<lpage>1216</lpage></mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Appelle</surname> <given-names>S</given-names></string-name></person-group>. (<year>1972</year>). <article-title>Perception and discrimination as a function of stimulus orientation: The oblique effect in man and animals</article-title>. <source>Psychological Bulletin</source>, <volume>78</volume>, <fpage>266</fpage>–<lpage>278</lpage></mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Essock</surname>, <given-names>E.A</given-names></string-name></person-group>. (<year>1980</year>). <article-title>The Oblique Effect of Stimulus Identification Considered with Respect to Two Classes of Oblique Effects</article-title>. <source>Perception</source>, <volume>9</volume>(<issue>1</issue>), <fpage>37</fpage>–<lpage>46</lpage></mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lennie</surname>, <given-names>P</given-names></string-name></person-group>. (<year>1971</year>). <article-title>Distortions of perceived orientation</article-title>. <source>Nature New Biology</source>, <volume>233</volume>(<issue>39</issue>), <fpage>155</fpage>–<lpage>156</lpage></mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henderson</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Biased orientation representations can be explained by experience with nonuniform training set statistics</article-title>. <source>Journal of Vision</source>, <volume>21</volume>(<issue>8</issue>):<fpage>10</fpage></mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Bergen</surname>, <given-names>R.S.</given-names></string-name>, <string-name><surname>Ji Ma</surname>, <given-names>W.J.</given-names></string-name>, <string-name><surname>Pratte</surname>, <given-names>M.S.</given-names></string-name> &amp; <string-name><surname>Jehee</surname>, <given-names>J.F.M.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Sensory uncertainty decoded from visual cortex predicts behavior</article-title>. <source>Nat Neurosci</source>, <volume>18</volume>, <fpage>1728</fpage>–<lpage>1730</lpage></mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Furmanski</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Engel</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2000</year>). <article-title>An oblique effect in human primary visual cortex</article-title>. <source>Nat Neurosci</source>, <volume>3</volume>, <fpage>535</fpage>–<lpage>36</lpage></mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Harrison</surname>, <given-names>W. J.</given-names></string-name>, <string-name><surname>Bays</surname>, <given-names>P.M.</given-names></string-name>, &amp; <string-name><surname>Rideaux</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Neural tuning instantiates prior expectations in the human visual system</article-title>. <source>bioRxiv</source>, <pub-id pub-id-type="doi">10.1101/2023.01.26.525790</pub-id></mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schurgin</surname>, <given-names>M.W.</given-names></string-name>, <string-name><surname>Wixted</surname>, <given-names>J.T.</given-names></string-name> &amp; <string-name><surname>Brady</surname>, <given-names>T.F</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Psychophysical scaling reveals a unified theory of visual memory strength</article-title>. <source>Nat Hum Behav</source>, <volume>4</volume>, <fpage>1156</fpage>–<lpage>1172</lpage></mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brouwer</surname>, <given-names>G. J.</given-names></string-name> &amp; <string-name><surname>Heeger</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Decoding and reconstructing color from responses in human visual cortex</article-title>. <source>J. Neurosci</source>., <volume>29</volume>(<issue>44</issue>), <fpage>13992</fpage>–<lpage>14003</lpage></mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Linde-Domingo</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Spitzer</surname> <given-names>B</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Geometry of visual working memory information in human gaze patterns</article-title>. <source>BioRxiv</source>, <pub-id pub-id-type="doi">10.1101/2022.11.17.516917</pub-id></mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yan</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Christophel</surname>, <given-names>T.B.</given-names></string-name>, <string-name><surname>Allefeld</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Haynes</surname>, <given-names>J-D</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Categorical working memory codes in human visual cortex</article-title>. <source>NeuroImage</source>,  </mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bae</surname>, <given-names>G-Y</given-names></string-name></person-group> (<year>2021</year>). <article-title>Neural evidence for categorical biases in location and orientation representations in a working memory task</article-title>. <source>Neuro Image</source>, <volume>240</volume>:<fpage>118366</fpage></mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Duan</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Curtis</surname>, <given-names>C.E.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Visual memories are abstractions of percepts</article-title>. <source>BioRxiv</source>, doi: <pub-id pub-id-type="doi">10.1101/2023.12.01.569634</pub-id></mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Reevaluating the sensory account of visual working memory storage</article-title>. <source>Tics</source>, <volume>21</volume>(<issue>10</issue>), <fpage>794</fpage>–<lpage>815</lpage></mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Sensory cortex is nonessential in working memory storage</article-title>. <source>Tics</source>, <volume>22</volume>(<issue>3</issue>), <fpage>192</fpage>–<lpage>3</lpage></mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henderson</surname>, <given-names>M.M.</given-names></string-name>, <string-name><surname>Rademaker</surname>, <given-names>R.L.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Flexible utilization of spatial- and motor-based codes for the storage of visuo-spatial information</article-title>. <source>eLife</source>, <volume>11</volume>, <elocation-id>e75688</elocation-id></mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Chekroud</surname>, <given-names>S.R.</given-names></string-name>, <string-name><surname>Stokes</surname>, <given-names>M.G.</given-names></string-name>, &amp; <string-name><surname>Nobre</surname>, <given-names>A.C.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Concurrent visual and motor selection during visual working memory guided action</article-title>. <source>Nat Neurosci</source>, <volume>22</volume>, <fpage>477</fpage>–<lpage>483</lpage></mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname> <given-names>M.J.</given-names></string-name>, <string-name><surname>Jochim</surname> <given-names>J.</given-names></string-name>, <string-name><surname>Akyürek</surname> <given-names>E.G.</given-names></string-name>, <string-name><surname>Buschman</surname> <given-names>T.J.</given-names></string-name>, <string-name><surname>Stokes</surname> <given-names>M.G</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Drifting codes within a stable coding scheme for working memory</article-title>. <source>PLoS Biol</source>, <volume>18</volume>(<issue>3</issue>): <fpage>e3000625</fpage></mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Iamshchinina</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Christophel</surname>, <given-names>T.B.</given-names></string-name>, <string-name><surname>Gayet</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Rademaker</surname>, <given-names>R.L</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Essential considerations for exploring visual working memory storage in the human brain</article-title>. <source>Visual Cognition</source>, <volume>29</volume>(<issue>7</issue>), <fpage>425</fpage>–<lpage>436</lpage></mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ko</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Krug</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Mapping response properties in lateral intraparietal area (LIP) of rhesus macaque</article-title>. <source>SfN abstracts</source> <volume>488</volume>.<fpage>18</fpage>.</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bressler</surname>, <given-names>D.W.</given-names></string-name> &amp; <string-name><surname>Silver</surname>, <given-names>M.A</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Spatial attention improves reliability of fMRI retinotopic mapping signals in occipital and parietal cortex</article-title>. <source>Neuroimage</source>, <volume>53</volume>, <fpage>526</fpage>–<lpage>533</lpage></mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Shulman</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title>. <source>Nat Rev Neurosci</source>, <volume>3</volume>, <fpage>201</fpage>–<lpage>215</lpage></mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Selemon</surname>, <given-names>L.D.</given-names></string-name>, &amp; <string-name><surname>Goldman-Rakic</surname>, <given-names>P.S</given-names></string-name></person-group>. (<year>1988</year>). <article-title>Common cortical and subcortical targets of the dorsolateral prefrontal and posterior parietal cortices in the rhesus monkey: evidence for a distributed neural network subserving spatially guided behavior</article-title>. <source>J Neurosci</source>, <volume>8</volume>, <fpage>4049</fpage>–<lpage>68</lpage></mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silver</surname>, <given-names>M.A.</given-names></string-name> &amp; <string-name><surname>Kastner</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Topographic maps in human frontal and parietal cortex</article-title>. <source>Trends Cogn. Sci</source>., <volume>13</volume>, <fpage>488</fpage>–<lpage>495</lpage></mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jehee</surname>, <given-names>J.F.M.</given-names></string-name>, <string-name><surname>Ling</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Swisher</surname>, <given-names>J.D.</given-names></string-name>, <string-name><surname>van Bergen</surname>, <given-names>R.S.</given-names></string-name>, &amp; <string-name><surname>Tong</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Perceptual learning selectively refines orientation representations in early visual cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>32</volume>, <fpage>16747</fpage>–<lpage>16753</lpage></mixed-citation></ref>
    <ref id="c79"><label>79.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Adam</surname>, <given-names>K.C.S.</given-names></string-name>, <string-name><surname>Rademaker</surname>, <given-names>R.L.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J.T.</given-names></string-name></person-group> (<year>2022</year>). <chapter-title>Evidence for, and challenges to, sensory recruitment models of visual working memory</chapter-title>. <source>Visual Memory</source> <person-group person-group-type="editor"><string-name><surname>Brady</surname><given-names>TF</given-names></string-name><string-name><surname>Bainbridge</surname><given-names>WA</given-names></string-name></person-group> <publisher-name>Routledge</publisher-name></mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ester</surname>, <given-names>E.F.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>D.E.</given-names></string-name>, <string-name><surname>Serences</surname>, <given-names>J.T.</given-names></string-name>, &amp; <string-name><surname>Awh</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2013</year>). <article-title>A Neural Measure of Precision in Visual Working Memory</article-title>. <source>J Cogn Neurosci</source>, <volume>25</volume>(<issue>5</issue>), <fpage>754</fpage>–<lpage>761</lpage></mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leavitt</surname>, <given-names>M.L.</given-names></string-name>, <string-name><surname>Mendoza-Halliday</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Martinez-Trujillo</surname>, <given-names>J.C</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Sustained Activity Encoding Working Memories: Not Fully Distributed</article-title>. <source>Tins</source>, <volume>40</volume>(<issue>6</issue>), <fpage>328</fpage>–<lpage>46</lpage></mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mendoza-Halliday</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Torres</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Martinez-Trujillo</surname>, <given-names>J. C</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Sharp emergence of feature-selective sustained activity along the dorsal visual pathway</article-title>. <source>Nat. Neurosci</source>., <volume>17</volume>, <fpage>1255</fpage>–<lpage>1262</lpage></mixed-citation></ref>
    <ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yiling</surname>, <given-names>Y.</given-names></string-name>, <string-name><given-names>J.</given-names>, <surname>Klon-Lipok</surname></string-name>, <string-name><surname>Shapcott</surname><given-names>K.</given-names></string-name> <string-name><surname>Lazar</surname><given-names>A.</given-names></string-name> <string-name><given-names>W.</given-names>, &amp; <surname>Singer</surname></string-name></person-group> (<year>2024</year>). <article-title>Dynamic fading memory and expectancy effects in the monkey primary visual cortex</article-title>. <source>PNAS</source>, <volume>121</volume>(<issue>8</issue>), <fpage>e2314855121</fpage></mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bisley</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Zaksas</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Droll</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Pasternak</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Activity of neurons in cortical area MT during a memory for motion task</article-title>. <source>J. Neurophysiol</source>., <volume>91</volume>, <fpage>286</fpage>–<lpage>300</lpage></mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zaksas</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Paternak</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Direction signals in the prefrontal cortex and in area MT during a working memory for visual motion task</article-title>. <source>J. Neurosci</source>., <volume>26</volume>, <fpage>11726</fpage>–<lpage>1174</lpage></mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Kerkoerle</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Self</surname>, <given-names>M. W.</given-names></string-name> &amp; <string-name><surname>Roelfsema</surname>, <given-names>P. R.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Layer-specificity in the effects of attention and working memory on activity in primary visual cortex</article-title>. <source>Nat. Comm</source>., <volume>8</volume>, <fpage>13804</fpage></mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ungerleider</surname>, <given-names>L.G.</given-names></string-name>, <string-name><surname>Courtney</surname>, <given-names>S.M.</given-names></string-name> and <string-name><surname>Haxby</surname>, <given-names>J.V</given-names></string-name></person-group>. (<year>1998</year>). <article-title>A neural system for human visual working memory</article-title>. <source>PNAS</source>, <volume>95</volume>(<issue>3</issue>), <fpage>883</fpage>–<lpage>90</lpage></mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Overkott</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Souza</surname>, <given-names>A.S</given-names></string-name></person-group>. (<year>2023</year>). <article-title>The fate of labeled and nonlabelled visual features in working memory</article-title>. <source>Jep:hpp</source>, <volume>49</volume>(<issue>3</issue>), <fpage>384</fpage>–<lpage>407</lpage></mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Serences</surname>, <given-names>J.T.</given-names></string-name>, <string-name><surname>Saproo</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Scolari</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ho.</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Muftuler</surname>, <given-names>L.T.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Estimating the influence of attention on population codes in human visual cortex using voxel-based tuning functions</article-title>. <source>Neuroimage</source>, <volume>44</volume>, <fpage>223</fpage>–<lpage>31</lpage></mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Wolff</surname>, <given-names>M.J.</given-names></string-name>, &amp; <string-name><surname>Rademaker</surname>, <given-names>R.L.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Model mimicry limits conclusions about neural tuning and can mistakenly imply unlikely priors</article-title>. <source>BioRxiv</source>, doi: <pub-id pub-id-type="doi">10.1101/2024.01.31.578040</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103347.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study examined orientation representations along the visual hierarchy during perception and working memory. The authors provide results suggesting that during working memory there is a gradient where representations are more categorical in nature later in the visual hierarchy. The evidence presented is <bold>solid</bold>, most notably a match between behavioral data, though minor weakness can be attributed to the tasks and behaviors not being designed to address this question. The findings should be of interest to a relatively broad audience, namely those interested in the relationship between sensory coding and memory.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103347.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this article, Chunharas and colleagues compared the representational differences of orientation information during a sensory task and a working memory task. By reanalyzing data from a previous fMRI study and applying representational similarity analysis (RSA), they observed that orientation information was represented differently in the two tasks: during visual perception, orientation representation resembled the veridical model, which captures the known naturalistic statistics of orientation information; whereas during visual working memory, a categorical model, which assumes different psychological distances between orientations, better explained the data, particularly in more anterior retinotopic regions. The authors suggest fundamental differences in the representational geometry of visual perception and working memory along the human retinotopic cortex.</p>
<p>Strengths:</p>
<p>Examining the differences in representational geometry between perception and working memory has important implications for the understanding of the nature of working memory. This study presents a carefully-executed reanalysis of previous data to address this question. The authors developed a novel method (model construction combined with RSA) to examine the representational geometry of orientation information under different tasks, and the control analyses provide rich, convincing support for their claims.</p>
<p>Weaknesses:</p>
<p>Although the control analyses are convincing, I still have alternative explanations for some of the results. I'm also concerned about the low sample size (n = 6 in the fMRI experiment). Overall, I think additional analyses may help to further clarify the issues and strengthen the claims.</p>
<p>(1) The central claim of the current study is that orientation information is represented in a veridical manner during the sensory task, and in a categorical manner during working memory. However, In the sensory task, a third type of representational geometry was observed, especially in brain regions from V3AB and beyond. These regions showed a symmetric pattern in which oblique orientations (45 and 135 degrees) appeared more similar to each other. In fact, a similar pattern can even be found in V1-V3, although the effect looked weaker. The authors raised two possible explanations for this in the discussion, one being that participants might have used verbal labels (e.g., diagonal) for both orientations, and the other being a lack of attention to orientation. Either way, this suggests that a veridical model may not be the best fit for these ROIs. How would this symmetric model explain the sensory data, in comparison to the veridical model?</p>
<p>(2) If the symmetric model also explains the sensory data well, I wonder whether this result challenges the authors' central claim, or instead suggests that the sensory task is not ideal for the purpose of the study. One way to address this issue might be to use the sample period of the working memory task as the perception task, as some other studies have been doing (e.g., Kwak &amp; Curtis, 2022). This epoch of data might function as a stronger version of the attention task as the authors discussed in the discussion. What would the representational geometry look like in the sample period? I would also like to note that the current analyses used 5.6-13.6 s after stimulus onset for the memory task, which I think may reflect a mix of sample- and delay-related activity.</p>
<p>(3) When comparing the veridical and categorical models, it is important to first show the significance of each model before making comparisons. For instance, was the veridical model significant in different ROIs in the memory task? And was either model significant in IPS1-3 in the two tasks? I'm asking about this because the two models appear to be both significant in the memory task, whereas only the veridical model was significant in the sensory task (with overall lower correlation coefficients than the categorical model in the memory task).</p>
<p>(4) The current study has a low sample size of six participants. With such a small sample, it would be helpful to show results from individual participants. For example, I appreciate that Figures 2D and 3C showed individual data points, but additionally showing the representational geometry plot (i.e., Figure 1C) for each subject could better illustrate the robustness of the effect. Alternatively, the original paper from which the fMRI data were drawn actually had two fMRI experiments with similar task designs. I wonder if the authors could replicate these patterns using data from the second experiment with seven participants. This might provide even stronger support for the current findings with a more reasonable sample size.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103347.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this manuscript, the authors examined the representational geometry of orientation representations during visual perception and working memory along the visual hierarchy. Using representational similarity analysis, they found that similarity was relatively evenly distributed among all orientations during perception, while higher around oblique orientations during WM. There were some noticeable differences along the visual hierarchy. IPS showed the most pronounced oblique orientation preferences during WM but no clear patterns during perception, likely due to the different task demands for the WM orientation task and the perception contrast discrimination task. The authors proposed two models to capture the differences. The veridical model estimated the representational geometry in perception by assuming an efficient coding framework, while the categorical model estimated the pattern in WM using psychological distances to measure the differences among orientations (including estimates from a separate psychophysical study performed outside the scanner). Therefore, I think this work is valuable and advances our understanding of the transition from perception to memory.</p>
<p>Strengths:</p>
<p>The use of RSA to identify representational biases goes beyond simply relying on response patterns and helps identify how representational formats change from perception to WM. The study nicely leverages ideas about efficient coding to explain perceptual representations that are more veridical, while leaning on ideas about abstractions of percepts that are more categorical-psychological in nature (but see (1) below). Moreover, the match between memory biases of orientation and the patterns estimated with RSA were compelling (but see (2) below). I found the analyses showing how RSA and decoding (eg, cross-generalization) are associated and how/why they may differ to be particularly interesting.</p>
<p>Weaknesses:</p>
<p>(1) The idea that later visual maps (ie, IPS0) encode perceptions of orientation in a veridical form and then in a categorical form during WM is an attractive idea. However, the support is somewhat weakened by a few issues. The RSA plots in Figure 1C for IPS0 appear to show a similar pattern, but just of lower amplitude during perception. But in the model fits either for orientation statistics or estimated from the psychophysics task, the Veridical model fits best for perception and the Categorical model fits best for memory in IPS0. By my eye, the modeled RSMs in Figures 2 &amp; 3 do not look like the observed ones in Figure 1C. Those modeled RSMs look way more categorical than the observed IPS0. They look like something in between.</p>
<p>(2) My biggest concern is the omission of the in-scanner behavioral data. Yes, on the one hand, they used the N=17 outside the scanner psychophysics dataset for the analyses in Figure 3. On the other hand, they do not even mention the behavioral data collected in the scanner along with the BOLD data. Those data had clear oblique effects if I recall correctly. Why use the data from the psychophysics experiment? Also, perhaps a missed opportunity; I wonder if the Veridical/Categorical models fit a single subject's RSA data matches that subject's behavioral biases. That would really be compelling if found.</p>
<p>The data were collected (reanalysis of published study) without consideration for the aims of the current study, and are therefore not optimized to test their goals. The biggest issue is that &quot;The distractors are really distracting me.&quot; I'm somewhat concerned about how the distractors may have impacted the results. I honestly did not notice that the authors were using delay periods that had 11s of distractor stimuli until way into the paper. On the one hand, the &quot;patterns&quot; of the model fits across the ROIs appear to be qualitatively similar. That's good if you want to pool data like the authors did. But, while the authors state on line 350 &quot;..we also confirmed that the presence of distractors during the delay did not impact the pattern of results in the memory task (Supplementary Figure 5).&quot; When looking at Supplementary Figure 5, I noticed that there are a couple of exceptions to this. In the Gratings distractor data, V1 shows a better fit to the Veridical model, while V4 and IPS0 shows no better fit to either model. And in the Noise distractor data, neither model fits better for any ROI. At first glance, I was concerned, but then looking at the No distractor data, the pattern is identical to that of the combined data. Thus, this can be seen as a glass half full/empty issue as almost all of the ROIs show a similar pattern, but still it would concern me if I were leading this study. This gets me to my key question, why even use the distractor trials at all, where the interpretation can get dicey? For instance, the authors have shown in this exact data that the impact of distraction affects the fidelity of representations differently along the visual hierarchy (Rademaker, 2019), consistent with several other studies (eg., Bettencourt &amp; Xu, 2016; Lorenc, 2018; Hallenbeck et al., 2022) and with one of the author's preprints (Rademaker &amp; Serences, 2024). My guess is that without the full dataset, some of the RSA analyses are underpowered. If that is the case, I'm fine with it, but it might be nice to state that.</p>
</body>
</sub-article>
</article>