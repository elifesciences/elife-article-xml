<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">93487</article-id>
<article-id pub-id-type="doi">10.7554/eLife.93487</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93487.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Integrative models of visually guided steering in <italic>Drosophila</italic></article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0006-7642-4055</contrib-id>
<name>
<surname>Canelo</surname>
<given-names>Angel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kim</surname>
<given-names>Hyosun</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kim</surname>
<given-names>Yeon</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Park</surname>
<given-names>Jeongmin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6681-1437</contrib-id>
<name>
<surname>Kim</surname>
<given-names>Anmo J</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>anmokim@hanyang.ac.kr</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046865y68</institution-id><institution>Department of Electronic Engineering, Hanyang University</institution></institution-wrap>, <city>Seoul</city>, <country country="KR">Republic of Korea</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046865y68</institution-id><institution>Department of Artificial Intelligence, Hanyang University</institution></institution-wrap>, <city>Seoul</city>, <country country="KR">Republic of Korea</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Tuthill</surname>
<given-names>John C</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Washington</institution>
</institution-wrap>
<city>Seattle</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Cardona</surname>
<given-names>Albert</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-01-11">
<day>11</day>
<month>01</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-08-20">
<day>20</day>
<month>08</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP93487</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-10-28">
<day>28</day>
<month>10</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-10-14">
<day>14</day>
<month>10</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.11.561122"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-01-11">
<day>11</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93487.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.93487.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.93487.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.93487.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.93487.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Canelo et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Canelo et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-93487-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>How flies adjust their flight direction in response to visual cues has been intensively studied, leading to a detailed understanding of individual neural circuits. However, how these circuits operate collectively in complex visual environments remains unclear. To understand how a mixture of visual stimuli—including those caused by the fly’s own actions—jointly determines its motor program, we developed an integrative model of <italic>Drosophila</italic> visuomotor processing. In particular, we derived simple models from flies’ wing responses to individual visual patterns and combined them through different internal models. We compared the steering behavior of these “virtual flies” with those of flying flies that freely changed their orientation. The results of these experiments supported the idea that, for selective visual patterns, flies employ suppressive mechanisms between competing visuomotor reflexes, consistent with an efference copy-based internal model. Our model provides a formal description of vision-based navigation strategies of <italic>Drosophila</italic> under complex visual environments.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd><italic>Drosophila</italic></kwd>
<kwd>visuomotor behavior</kwd>
<kwd>flight navigation</kwd>
<kwd>efference copy</kwd>
<kwd>complex visual environment</kwd>
</kwd-group>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution>National Research Foundation of Korea (NRF) grant funded by the Korea Government (MSIT)</institution>
</institution-wrap>
</funding-source>
<award-id>NRF2020R1A4A1016840</award-id>
</award-group>
<award-group id="funding-1a">
<funding-source>
<institution-wrap>
<institution>National Research Foundation of Korea (NRF) grant funded by the Korea Government (MSIT)</institution>
</institution-wrap>
</funding-source>
<award-id>NRF2021M3E5D2A01023888</award-id>
</award-group>
<award-group id="funding-1b">
<funding-source>
<institution-wrap>
<institution>National Research Foundation of Korea (NRF) grant funded by the Korea Government (MSIT)</institution>
</institution-wrap>
</funding-source>
<award-id>NRF2022R1A2C2007599</award-id>
</award-group>
<award-group id="funding-1c">
<funding-source>
<institution-wrap>
<institution>National Research Foundation of Korea (NRF) grant funded by the Korea Government (MSIT)</institution>
</institution-wrap>
</funding-source>
<award-id>NRF2022M3E5E8081195</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Two new figures were added (Figures 5 and 6); the main text was updated according to the new experimental results (Figures 5 and 6) and their implications for the interpretation of other data. Figure 2--figure 2 was also newly added to the supplemental figures.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Fruit flies demonstrate impressive agility in flight (<xref ref-type="bibr" rid="c9">Card and Dickinson, 2008</xref>; <xref ref-type="bibr" rid="c26">Fry et al., 2005</xref>; <xref ref-type="bibr" rid="c47">Liu et al., 2019</xref>; <xref ref-type="bibr" rid="c56">Muijres et al., 2014</xref>), and vision plays a pivotal role in the flight control, often translating directly into precise flight maneuvers. Neural circuit mechanisms underlying many of these visuomotor behaviors have been studied intensively in recent years, both structurally and functionally (<xref ref-type="bibr" rid="c23">Fenk et al., 2022</xref>; <xref ref-type="bibr" rid="c38">Joesch et al., 2010</xref>; <xref ref-type="bibr" rid="c69">Takemura et al., 2013</xref>; <xref ref-type="bibr" rid="c75">Wu et al., 2016</xref>; <xref ref-type="bibr" rid="c33">Hardcastle et al., 2021</xref>; <xref ref-type="bibr" rid="c27">Garner et al., 2024</xref>; <xref ref-type="bibr" rid="c39">Keles and Frye, 2017</xref>) (see <xref ref-type="bibr" rid="c17">Currier et al., 2023</xref>; <xref ref-type="bibr" rid="c66">Ryu et al., 2022</xref> for reviews). While we have gained a qualitative grasp of the neural substrates underlying various visual behaviors, a quantitative understanding remains less explored (<xref ref-type="bibr" rid="c1">Ache et al., 2019</xref>; <xref ref-type="bibr" rid="c50">Maisak et al., 2013</xref>; <xref ref-type="bibr" rid="c52">Mauss et al., 2015</xref>; <xref ref-type="bibr" rid="c67">Städele et al., 2020</xref>).</p>
<p>In the pioneering era of in the 1950s to the 1970s, insect vision studies efficiently combined behavioral experiments with computational modeling. Most notably, the Reichardt-Hassenstein elementary motion detector was formulated to explain vision-based steering behaviors observed in beetles (<xref ref-type="bibr" rid="c34">Hassenstein and Reichardt, 1956</xref>). Another example is the model of efference copy (EC) that was put forward to describe the visual behavior of hoverflies (<xref ref-type="bibr" rid="c15">Collett, 1980</xref>; <xref ref-type="bibr" rid="c73">von Holst and Mittelstaedt, 1950</xref>). While some recent studies have offered quantitative models for circuit mechanisms underlying visual feature detection (<xref ref-type="bibr" rid="c6">Borst and Weber, 2011</xref>; <xref ref-type="bibr" rid="c14">Clark et al., 2011</xref>; <xref ref-type="bibr" rid="c32">Gruntman et al., 2018</xref>; <xref ref-type="bibr" rid="c71">Tanaka and Clark, 2020</xref>), less emphasis has been placed on computational models for vision-based behaviors. To contextualize the function of visuomotor circuits within behavior, a phenomenological model integrating these visual functions within a moving animal would be invaluable.</p>
<p>An important consideration in constructing an integrative model is the interaction among different visuomotor circuits tuned to different visual features. Since these circuits in complex visual environments may activate multiple motor programs simultaneously, which may contradict each other, circuit mechanisms by which distinct sensorimotor pathways are integrated adaptively are necessary. A foundational theory regarding the interaction of multiple sensorimotor pathways is the theory of EC, which highlights the need for a motor-related, feedback signal that filters out undesired sensory input (<xref ref-type="bibr" rid="c73">von Holst and Mittelstaedt, 1950</xref>). Previous behavioral studies argued that such a mechanism may indeed exist in some dipteran species (<xref ref-type="bibr" rid="c4">Bender and Dickinson, 2006a</xref>; <xref ref-type="bibr" rid="c15">Collett, 1980</xref>; <xref ref-type="bibr" rid="c35">Heisenberg and Wolf, 1988</xref>). Recent studies in <italic>Drosophila</italic> have uncovered the neural correlates of the EC in an array of visual neurons during both spontaneous and visually evoked rapid flight turns (<xref ref-type="bibr" rid="c24">Fenk et al., 2021</xref>; <xref ref-type="bibr" rid="c42">Kim et al., 2017</xref>, <xref ref-type="bibr" rid="c43">2015</xref>).</p>
<p>In this study, we present an integrative model of <italic>Drosophila</italic> visuomotor behavior, derived from and validated through behavioral experiments. The model accurately predicts the steering responses of flying <italic>Drosophila</italic> to a range of visual patterns, both when presented individually and in combination. In addition, our experimental results provide clear behavioral evidence for suppressive interactions between specific visuomotor responses, consistent with efference copy-like mechanisms. Our model offers a framework for implementing and testing detailed neural circuit models of <italic>Drosophila</italic> visuomotor processing in real-world contexts.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Steering responses of flying <italic>Drosophila</italic> for singly presented visual patterns</title>
<p>To develop a quantitative model of the visuomotor control in flying <italic>Drosophila</italic>, we set out by measuring wing responses to singly presented visual patterns. In particular, we placed a tethered, wild-type flying fly (Oregon R) at the center of a cylindrical LED display and presented a rotating visual pattern on the display (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). To estimate the angular torque generated by the fly, we subtracted the right wingbeat amplitude (R WBA) from the left wingbeat amplitude (L WBA), resulting in a time signal termed L-R WBA (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). Previous studies showed that L-R WBA correlates with the angular torque that a fly exerts on its body (<xref ref-type="bibr" rid="c31">Götz et al., 1979</xref>; <xref ref-type="bibr" rid="c70">Tammero et al., 2004</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Construction of flight control models for singly presented visual patterns.</title>
<p>(<bold>A</bold>) Schematic of the experimental setup (left), a frame captured by the infrared camera (middle), and a simplified schematic of the setup (right). The annulus surrounding the fly schematic represents the visual display as viewed from above. (<bold>B</bold>) Wing responses of a sample fly to three rotating visual patterns: bar, spot, and grating. The bottom row (L-R WBA) represents the angular torque of the fly, calculated by subtracting the right wingbeat amplitude (RWBA) from the left wingbeat amplitude (LWBA). (<bold>C</bold>) L-R WBA traces of a sample fly in response to the three visual patterns. The thick black lines indicate the average across all trials (top) or all flies (bottom). Thin gray lines indicate individual trials (top) or fly averages (bottom). (<bold>D</bold>) Schematic of the position-velocity-based flight control model. (<bold>E</bold>) Average wing responses of a population of flies to the three visual patterns, rotating either in a clockwise (red) or counterclockwise (blue) direction. Top traces show the position of each pattern. Red and blue shadings at the bottom indicate the 95% confidence interval. (<bold>F</bold>) Position and velocity functions estimated from the wing responses in <bold>E</bold>. Light purple shadings indicate the 95% confidence interval.</p></caption>
<graphic xlink:href="561122v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We presented three visual patterns rotating a full circle at 36°/s: a dark vertical bar, a dark spot, and a vertical grating (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). These simple patterns, when tested at different velocities and sizes, were previously shown to elicit robust visuomotor reflexes in flying <italic>Drosophila</italic> (<xref ref-type="bibr" rid="c29">Götz, 1968</xref>; <xref ref-type="bibr" rid="c44">H. Kim et al., 2023</xref>; <xref ref-type="bibr" rid="c49">Maimon et al., 2008</xref>; <xref ref-type="bibr" rid="c70">Tammero et al., 2004</xref>). In response to a clockwise-rotating bar starting from the back, flies generated wing responses corresponding to a fictive rightward turn throughout the duration of the stimulus (<xref rid="fig1" ref-type="fig">Figure 1B-C</xref>). The response increased gradually, peaked after crossing the midline, and then declined. In response to the moving spot, flies exhibited L-R WBA indicative of turning away from the position of the spot (<xref rid="fig1" ref-type="fig">Figure 1B-C</xref>). In response to the moving grating, flies produced strong wing responses that would exert angular torque in the direction of the grating movement, also consistent with previous studies (<xref ref-type="bibr" rid="c12">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="c11">Cellini and Mongeau, 2020</xref>; <xref ref-type="bibr" rid="c70">Tammero et al., 2004</xref>).</p>
</sec>
<sec id="s2b">
<title>Steering control models of flying <italic>Drosophila</italic> for singly presented visual patterns</title>
<p>We built a simple model for these visual behaviors by adopting a classical approach that was originally applied to flying <italic>Musca Domestica</italic> (<xref ref-type="bibr" rid="c60">Poggio and Reichardt, 1973</xref>; <xref ref-type="bibr" rid="c62">Reichardt and Poggio, 1976</xref>). In this approach, the torque response of an animal, <italic>W(·)</italic>, is modeled as a sum of two components: one defined purely by a position response <italic>P(</italic> Ψ<italic>)</italic> and the other by a position-dependent velocity response <inline-formula><inline-graphic xlink:href="561122v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref rid="fig1" ref-type="fig">Figure 1D</xref>).
<disp-formula id="eqn1">
<graphic xlink:href="561122v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where Ψ denotes the position of the pattern.</p>
<p>In this model, the position response can be experimentally obtained by summing the torque responses to a visual pattern rotating in the clockwise and counterclockwise directions, with respect to the same object position.
<disp-formula id="eqn2">
<graphic xlink:href="561122v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="561122v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>W</italic><sub><italic>CW</italic></sub><italic>(</italic>Ψ<italic>)</italic> denotes the wing response for the clockwise-rotating pattern, and <italic>W</italic><sub><italic>CCW</italic></sub><italic>(</italic>Ψ<italic>)</italic> for the counterclockwise-rotating pattern. The velocity response, <inline-formula><inline-graphic xlink:href="561122v2_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, can be obtained similarly, but through subtraction.</p>
<p>To apply this approach, we tested flies with both clockwise- and counterclockwise-rotating visual patterns (<xref rid="fig1" ref-type="fig">Figure 1E</xref>). The wingbeat signals for the patterns were largely symmetric between the two directions, but with subtle asymmetries, which are likely due to experimental variabilities such as misalignment of the fly’s body orientation relative to the display arena. To minimize the influence of these experimental anomalies, we combined the two L-R WBA signals for a pair of symmetrical patterns, after flipping one for its sign as well as the time, and used the combined signal for the further analysis (see Data Analysis in Methods).</p>
<p>When we computed the position functions, <italic>P(</italic>Ψ<italic>)</italic>, for the spot and bar patterns using this method, we found that they were symmetric with respect to the origin (<xref rid="fig1" ref-type="fig">Figure 1F</xref>). That is, for the rotating bar, the position response was positive when the bar was in the left hemifield, but negative when it was in the right hemifield. This result is consistent with fly’s fixation behavior toward a vertical bar, as this position response would have made flies turn toward the bar on both sides. For the spot pattern, the amplitude of the position function was much smaller than that for the bar, and its overall direction was opposite to that of the bar, which would have made flies turn away from the spot.</p>
<p>The velocity function, <italic>V(</italic>Ψ<italic>)</italic>, was nearly zero for the spot, except around the frontal visual field (<xref rid="fig1" ref-type="fig">Figure 1F</xref>), which is consistent with the weak direction dependency in the wing response (<xref rid="fig1" ref-type="fig">Figure 1E</xref>). By contrast, the velocity function for the bar was consistently positive (<xref rid="fig1" ref-type="fig">Figure 1F</xref>), poised to contribute to flight turns in the direction of the bar movement (<xref ref-type="bibr" rid="c62">Reichardt and Poggio, 1976</xref>). The amplitude of the velocity response, however, varied depending on the object position, unlike the nearly constant velocity function reported in blow flies in response to a moving bar (<xref ref-type="bibr" rid="c62">Reichardt and Poggio, 1976</xref>).</p>
<p>In response to the rotating grating, the velocity response was consistently positive, but its amplitude varied depending on position (<xref rid="fig1" ref-type="fig">Figure 1E</xref>). Because the width of each stripe in the grating pattern (15 degrees) was larger than the fruit fly’s inter-ommatidial angle (~5°), flies may have responded to the positions of individual stripes.</p>
<p>However, because we found no periodicity in the wing response and every stripe was designed to be identical (<xref rid="fig1" ref-type="fig">Figure 1E</xref>), we reasoned that the change in the position function was not due to the positional variation of the grating pattern. Instead, the time dependence in the position and velocity response was likely due to non-visual factors such as adaptation or fatigue in the motor system caused by exceptionally large L-R WBA. Furthermore, the relatively slow L-R WBA change at the beginning and end of the grating stimulus suggested the need for a dynamical system model to describe the relationship between the visual stimulus and the wing response. Thus, we expanded our models with a set of dynamical system equations involving a fatigue variable (Figure 1–figure supplement 1), which successfully reproduced L-R WBAs for the three visual patterns as well as the position and velocity responses associated with each pattern. We also performed these experiments and analyses in a different strain of wild type flies (Canton S) and found that the profile of the position and velocity functions remained largely unchanged (Figure 1–figure supplement 2).</p>
</sec>
<sec id="s2c">
<title>Simulating the steering behavior of the virtual fly model to individual visual patterns</title>
<p>To test the steering behavior of the model, we developed an expanded model. In this model, we added a biomechanics block that transforms the torque response of the fly into the actual heading change according to kinematic parameters estimated previously (<xref ref-type="bibr" rid="c19">Michael H Dickinson, 2005</xref>; <xref ref-type="bibr" rid="c65">Ristroph et al., 2010</xref>) (<xref rid="fig2" ref-type="fig">Figure 2A</xref>; see <xref ref-type="disp-formula" rid="eqn4">Equation 4</xref> in Methods and Movie S1). These models feature position and velocity blocks that are conditioned on the type of visual pattern and can now change its body orientation, simulating the visually guided steering of flies. This simulation experiment is reminiscent of the magnetically tethered flight assay, where a flying fly remains fixed at a position but is free to rotate around its yaw axis (<xref ref-type="bibr" rid="c5">Bender and Dickinson, 2006b</xref>; <xref ref-type="bibr" rid="c12">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="c45">G. Kim et al., 2023</xref>; <xref ref-type="bibr" rid="c55">Mongeau and Frye, 2017</xref>). Additionally, we simplified the position and velocity functions for the bar and spot patterns with simple mathematical representations (<xref rid="fig2" ref-type="fig">Figure 2B</xref>; see <xref ref-type="disp-formula" rid="eqn5">Equation 5</xref>-<xref ref-type="disp-formula" rid="eqn7">7</xref> in Methods). For the grating pattern, we approximated the position response as zero and the velocity response as a constant value (<xref rid="fig2" ref-type="fig">Figure 2B</xref>), where no fatigue effect was considered for simplicity (Figure 1–figure supplement 1).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>The flight control models with a biomechanics block predicted the dynamics of the steering behavior to individual visual patterns.</title>
<p><bold>(A)</bold> Schematics of three visuomotor response models with a biomechanics block. <bold>(B)</bold> Simplified version of the position and velocity responses for each pattern. <bold>(C)</bold> Simulation results for the three patterns (bar, spot, and grating) moving in a sigmoidal dynamics. The spot response was plotted with an 180° offset to facilitate comparison. Bar plots on the right show the latency of body angle with respect to the stimulus onset, measured at the 50% point of the pattern movement. <bold>(D)</bold> Simulation results for the three patterns moving in a sinusoidal dynamics. In the bar plots on the right, the amplitude was measured as the peak-to-peak amplitude, and the phase shift was calculated by measuring the peak time of the cross-correlation between the pattern and the fly heading. <bold>(E)</bold> Same as in (C), but for visual patterns remaining static at 0 degree position. The simulation was performed 10 times with a Gaussian noise component (gray lines). The mean response was plotted in thick colored lines. The probability density function of the body angle is shown on the right for each pattern.</p></caption>
<graphic xlink:href="561122v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We tested these models using visual patterns that moved horizontally with three distinct dynamics: a rapid sigmoidal shift, sinusoidal oscillations, and Gaussian noise. The virtual fly model’s response to the sigmoidally shifting visual patterns varied depending on the pattern (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). For the bar, the fly responded quickly toward the bar and fixated on it (see <xref ref-type="disp-formula" rid="eqn8">Equation 8</xref>, <xref ref-type="disp-formula" rid="eqn9">9</xref> in Methods). The spot elicited a heading change in the opposite direction, albeit at a slower pace and with the pattern positioned behind the fly at the end (see <xref ref-type="disp-formula" rid="eqn10">Equation 10</xref> in Methods). For the grating, the response followed a time course similar to that to the bar but ceased early once the visual pattern stopped (see <xref ref-type="disp-formula" rid="eqn11">Equation 11</xref> in Methods). This is due to the absence of positional cue in the grating and consistent with experimental results in a previous study (<xref ref-type="bibr" rid="c15">Collett, 1980</xref>). We also tested our models by incorporating a delay unit between the vision and the biomechanics blocks to take into account different neuronal delays reported previously for each visual pattern (Figure 2–figure supplement 1A-B). The response latencies for these models were considerably longer than the actual wingbeat response latency reported previously to similar visual patterns (<xref ref-type="bibr" rid="c44">H. Kim et al., 2023</xref>). This is likely due to limitations in our models, such as the assumption that the velocity response scales linearly with the pattern velocity.</p>
<p>For sinusoidally oscillating patterns, we measured the peak-to-peak amplitude and the phase shift of the heading responses (<xref rid="fig2" ref-type="fig">Figure 2D</xref> and Figure 2–figure supplement 1C). For the bar and grating stimuli, the response amplitude was comparable to that of the stimulus, but significantly smaller for the spot pattern (<xref rid="fig2" ref-type="fig">Figure 2D</xref>). The phase shift was the largest for the spot, the smallest for the grating, and intermediate for the bar, consistent with the delay observed in response to the rapidly shifting patterns (<xref ref-type="bibr" rid="c44">H. Kim et al., 2023</xref>) (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). We also simulated our models under static visual patterns but with Gaussian noise added to its body torque (<xref rid="fig2" ref-type="fig">Figure 2E</xref>; see <xref ref-type="disp-formula" rid="eqn12">Equation 12</xref> in Methods). We observed fixation to the bar, anti-fixation to the spot, and stabilization to the grating. When no visual cue was present, the virtual fly model drifted randomly (<xref rid="fig2" ref-type="fig">Figure 2E</xref>, bottom).</p>
<p>Finally, one important locomotion dynamics that a flying <italic>Drosophila</italic> exhibits while tracking an object is a rapid orientation change, called a “saccade” (<xref ref-type="bibr" rid="c8">Breugel and Dickinson, 2012</xref>; <xref ref-type="bibr" rid="c13">Censi et al., 2013</xref>; <xref ref-type="bibr" rid="c36">Heisenberg and Wolf, 1979</xref>). For example, while tracking a slowly moving bar, flies perform relatively straight flights interspersed with saccadic flight turns (<xref ref-type="bibr" rid="c16">Collett and Land, 1975</xref>; <xref ref-type="bibr" rid="c55">Mongeau and Frye, 2017</xref>). During this behavior, it has been proposed that visual circuits compute an integrated error of the bar position with respect to the frontal midline and triggers a saccadic turn toward the bar when the integrated value reaches a threshold (<xref ref-type="bibr" rid="c25">Frighetto and Frye, 2023</xref>; <xref ref-type="bibr" rid="c54">Mongeau et al., 2019</xref>; <xref ref-type="bibr" rid="c55">Mongeau and Frye, 2017</xref>). We expanded our bar fixation model to incorporate this behavioral strategy (Figure 2–figure supplement 2). The overall structure of the modified model is akin to the one proposed in a previous study (<xref ref-type="bibr" rid="c55">Mongeau and Frye, 2017</xref>), and the amplitude of a saccadic turn was determined by the sum of the position and velocity functions (Figure 2–figure supplement 2A; see Equation 13 in Methods). When simulated, our model successfully reproduced experimental observations of saccade dynamics across different object velocities (Figure 2–figure supplement 2B-D) (<xref ref-type="bibr" rid="c55">Mongeau and Frye, 2017</xref>). Together, our models faithfully recapitulated the results of previous behavioral observations in response to singly presented visual patterns (<xref ref-type="bibr" rid="c15">Collett, 1980</xref>; <xref ref-type="bibr" rid="c28">Götz, 1987</xref>; <xref ref-type="bibr" rid="c44">H. Kim et al., 2023</xref>; <xref ref-type="bibr" rid="c49">Maimon et al., 2008</xref>; <xref ref-type="bibr" rid="c55">Mongeau and Frye, 2017</xref>).</p>
</sec>
<sec id="s2d">
<title>Steering behaviors of magnetically tethered flying <italic>Drosophila</italic> to rapidly moving visual patterns</title>
<p>We tested the predictions of our models with flies flying in an environment similar to that used in the simulation (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). A fly was tethered to a short steel pin positioned vertically at the center of a vertically oriented magnetic field, allowing it to rotate around its yaw axis with minimal friction (<xref ref-type="bibr" rid="c5">Bender and Dickinson, 2006b</xref>; <xref ref-type="bibr" rid="c12">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="c45">G. Kim et al., 2023</xref>). We captured images of the fly from below in response to visual stimuli and calculated body angle <italic>post hoc</italic> (<xref rid="fig3" ref-type="fig">Figure 3A</xref>; see Methods). To present the visual patterns at a defined position relative to the fly’s heading, we coaxed the fly to align to a reference angle by oscillating a stripe pattern widely across the visual display (“alignment” phase, <xref rid="fig3" ref-type="fig">Figure 3A</xref> bottom). We then presented for 0.5 s the initial frame of the visual pattern (“ready” phase), after which it was rotated rapidly for 0.2 seconds in a sigmoidal dynamics (“go” phase) (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). After the movement, the pattern remained static for 2.5 seconds (“freeze” phase).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Magnetically tethered flight experiments confirmed orientation changes predicted by the virtual fly model.</title>
<p><bold>(A)</bold> Schematic of the magnetically tethered flight assay with an LED display (left). The image acquired from below was analyzed to estimate the body angle (right). The stimulus protocol (bottom) consisted of four phases: alignment, ready, go, and freeze. <bold>(B)</bold> Body orientation responses of a single fly for the bar, spot and grating patterns moving horizontally. <bold>(C)</bold> Same as in (B), but for a population of flies. The population averages were replotted at the bottom to facilitate the comparison of their dynamics. <bold>(D)</bold> Amplitude and latency of the body orientation responses. The box represents the interquartile range (IQR), with the median indicated by the horizontal black line. The whiskers extend to the minimum and maximum values within 1.5 times the IQR. Outliers are denoted by “+” marks beyond the whiskers.</p></caption>
<graphic xlink:href="561122v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For all visual patterns, the fly turned in the direction that was predicted by the models (<xref rid="fig2" ref-type="fig">Figure 2</xref>). Namely, flies turned toward the moving bar, away from the moving spot, and in the direction of the grating (<xref rid="fig3" ref-type="fig">Figure 3B-C</xref>). The mean amplitude was comparable for the three patterns, at around 20 degrees – about half that of the angle of the pattern displacement (<xref rid="fig3" ref-type="fig">Figure 3C-D</xref>). While the small response angle was expected for the grating pattern (<xref rid="fig2" ref-type="fig">Figure 2C</xref>), the bar and spot response amplitudes were unexpectedly small compared to the model predictions. This is in contrast to the match between the model prediction and behavioral results for a slow moving object (Figure 1–figure supplement 1C).</p>
<p>What causes the fly to undershoot the movement of the target object in the magnetically tethered assay? One hypothesis is that strong upward magnetic force or a blunt top end of the steel pin significantly dampens the flies’ flight turns. To determine if our assay imposes additional friction compared to other assays used in previous studies, we analyzed the dynamics of spontaneous saccades during the “freeze” phase (Figure 3–figure supplement 1A). We found their duration and amplitude to be within the range reported previously (<xref ref-type="bibr" rid="c5">Bender and Dickinson, 2006b</xref>; <xref ref-type="bibr" rid="c55">Mongeau and Frye, 2017</xref>) (Figure 3–figure supplement 1B-D). Another potential explanation arises from recent studies demonstrating that proprioceptive feedback provided during flight turns in a magnetically tethered assay strongly dampens the amplitude of wing and head responses (<xref ref-type="bibr" rid="c10">Cellini and Mongeau, 2022</xref>; <xref ref-type="bibr" rid="c64">Rimniceanu et al., 2023</xref>). According to these studies, our models – derived from the behavioral assay with no proprioceptive feedback (<xref rid="fig1" ref-type="fig">Figure 1A</xref>) – are expected to predict larger visual responses than actual flies in a magnetically tethered assay, thus explaining the differences in the response amplitude. Additionally, the response latency was the longest for the spot, and similar between the bar and grating (<xref rid="fig3" ref-type="fig">Figure 3D</xref>), in line with the virtual fly model (<xref rid="fig2" ref-type="fig">Figure 2C</xref>).</p>
<p>With these experimental results in a magnetically tethered assay, we confirmed that our virtual fly models captured essential dynamics of the vision-based flight control in <italic>Drosophila</italic>, when the patterns are presented individually. In natural conditions, however, multiple patterns may be presented simultaneously in a visual scene. This led us to further develop the model to incorporate the ability to control their orientation under more complex visual environments.</p>
</sec>
<sec id="s2e">
<title>Integrative visuomotor models for complex visual environments</title>
<p>In natural environments, distinct visual features in a complex visual scene may activate multiple visuomotor circuits, triggering synergistic or opposing locomotor actions concurrently. For instance, when a fly begins to turn toward or away from a foreground object moving against a static background (<xref rid="fig4" ref-type="fig">Figure 4A</xref>), it experiences rotational optic flow from the background, which immediately activates the visual stability reflex, opposing the object-evoked turn. This raises the question as to how outputs from these circuits are integrated to control a shared actuator, such as a wing motor system.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Three integrative models of the visuomotor control and their predictions in a complex visual environment.</title>
<p>(A) Schematic of the visual environments used in the simulation. A moving bar is presented as a foreground object over a static grating background. (B) A diagram of the addition-only model. Object and background response circuits are joined at their output through addition. (C) A diagram of the graded EC model. An EC block translates the object-evoked motor command into the negative image of the predicted background input to counteract visual feedback. (D) A diagram of the all-or-none EC model. An EC switches off the background response circuit during the object-evoked turn. (E,F,G) Simulation results for the three models. The object position and the heading of the virtual fly model (top), and the associated torques as well as EC signals (bottom). EC signals (ε and <bold>η</bold>) and the fly velocity at the bottom plots of (F) and (G) are not to scale.</p></caption>
<graphic xlink:href="561122v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>This integration problem has been studied across animal sensory systems, typically by analyzing motor-related signals observed in sensory neurons (<xref ref-type="bibr" rid="c3">Bell, 1981</xref>; <xref ref-type="bibr" rid="c15">Collett, 1980</xref>; <xref ref-type="bibr" rid="c42">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="c61">Poulet and Hedwig, 2006</xref>). Building on these results, we developed three integrative models. The first model, termed the “addition-only model”, assumes that the outputs of the object (bar) and the background (grating) response circuits are summed to control the flight orientation (<xref rid="fig4" ref-type="fig">Figure 4B</xref>; see <xref ref-type="disp-formula" rid="eqn14">Equation 14</xref> in Methods). In the second and third models, an EC is used to set priorities between different visuomotor circuits (<xref rid="fig4" ref-type="fig">Figure 4C-D</xref>). In particular, the EC is derived from the object-induced motor command and sent to the background response system to nullify visual input associated with the object-evoked turn (<xref ref-type="bibr" rid="c3">Bell, 1981</xref>; <xref ref-type="bibr" rid="c15">Collett, 1980</xref>; <xref ref-type="bibr" rid="c61">Poulet and Hedwig, 2006</xref>). These motor-related inputs fully suppress sensory processing in some systems (<xref ref-type="bibr" rid="c61">Poulet and Hedwig, 2006</xref>), whereas in others they selectively counteract only the undesirable components of the sensory feedback (<xref ref-type="bibr" rid="c3">Bell, 1981</xref>; <xref ref-type="bibr" rid="c40">Kennedy et al., 2014</xref>). Following these results, we developed two EC-based models: the graded and all-or-none EC models. In the graded EC model (<xref rid="fig4" ref-type="fig">Figure 4C</xref>; see <xref ref-type="disp-formula" rid="eqn15">Equation 15</xref> in Methods), the amplitude of the EC, ε, is adjusted according to the predicted optic flow feedback. In contrast, in the all-or-none EC model (<xref rid="fig4" ref-type="fig">Figure 4D</xref>; see <xref ref-type="disp-formula" rid="eqn16">Equation 16</xref> in Methods), the background response is completely blocked by the binarized (i.e., all or none) EC, <bold>η</bold>, during the object-evoked turn regardless of its amplitude.</p>
<p>When we simulated the addition-only model in response to a vertical bar moving horizontally against a static background, we observed that the virtual fly model reduced the error angle rather slowly, as expected (<xref rid="fig4" ref-type="fig">Figure 4E</xref>, magenta trace). The slowdown was due to the torque generated by the background, <italic>F</italic><sub><italic>BG</italic></sub>, which opposed the object-evoked torque, <italic>F</italic><sub><italic>obj</italic></sub> (<xref rid="fig4" ref-type="fig">Figure 4E</xref>, bottom). In the graded EC model, we observed that the object response was not impeded by background (<xref rid="fig4" ref-type="fig">Figure 4F</xref>, orange trace at top), and the heading dynamics matched those of the simulation with an empty background (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). The torque due to the background remained at zero because the graded EC (<xref rid="fig4" ref-type="fig">Figure 4F</xref>, dotted black trace at bottom) counteracted the background-evoked input (<xref rid="fig4" ref-type="fig">Figure 4F</xref>, solid black trace at bottom). In the all-or-none EC model, the response to the object was not dampened either (<xref rid="fig4" ref-type="fig">Figure 4G</xref>, brown trace at top) because the background response was fully suppressed by the EC signal during the turn (<xref rid="fig4" ref-type="fig">Figure 4G</xref>, dotted brown trace in the bottom).</p>
<p>The primary difference between the two EC-based models emerges when the background changes or moves unexpectedly during a turn. In the graded EC model, unexpected background rotations result in noticeable deviations in turn dynamics, whereas in the all-or-none EC model, the background is completely ignored (Figure 4—figure supplement 1). Additionally, for the graded EC model, we reasoned that the EC amplitude should adapt to match the magnitude of visual feedback, which may vary significantly depending on the visual environment. To incorporate this idea, we expanded our model so that mismatches between predicted and actual sensory feedback were used to dynamically update the EC strength (Figure 4—figure supplement 1B-E). Together, these models propose multiple algorithms for integrating competing visuomotor circuits, with or without EC mechanisms, to guide steering behavior. However, whether and how such mechanisms operate in actual flying <italic>Drosophila</italic> remains to be examined.</p>
</sec>
<sec id="s2f">
<title>Bar and background movement-evoked responses add up linearly</title>
<p>To test whether the visual stability system is suppressed during object-evoked flight turns, we designed a set of visual stimuli in which a translating bar and a rotating random-dot background were superposed close in time (<xref rid="fig5" ref-type="fig">Figure 5A-B</xref>). In these patterns, we varied the onset latency of the background movement relative to that of the bar by −200 ms, 0 ms, and +200 ms (<xref rid="fig5" ref-type="fig">Figure 5B-C</xref>). Additionally, we created pairs of these patterns by inverting the direction of background motion while keeping the bar dynamics identical.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Bar- and background-evoked wing responses did not suppress each other when presented in time close to each other.</title>
<p><bold>(A)</bold> The visual stimulus patterns. A dense starfield background moved either clockwise or counterclockwise by 45 degrees, while the bar always moved clockwise by 45 degrees. <bold>(B)</bold> A superposition pattern in which the background moves first, followed by the bar 100 ms later. <italic>t</italic><sub><italic>BG</italic></sub> and <italic>t</italic><sub><italic>bar</italic></sub> indicate the onset times of the stimuli. <bold>(C)</bold> Body angle measured in response to the superposition patterns. The onset time difference varied between −200 ms, 0 ms, and 200 ms. The background moved either clockwise or counterclockwise (n = 34 - 43 flies). <bold>(D)</bold> Wing response amplitude measured from the dataset used in (C). Error bars indicate the 95% confidence interval. <bold>(E)</bold> The background response component was estimated by computing the difference between body angle traces with the same onset latency but opposite background movement directions (n = 29 flies). <bold>(F)</bold> The background (BG) response component was estimated by averaging the body angle traces with the same onset latency but opposite background movement directions. <bold>(G</bold>,<bold>H)</bold> The amplitudes of the background and bar response components did not change significantly across different onset latencies (one-way ANOVA). <bold>(H)</bold> When the same experiments were conducted with different onset latencies and a shorter BG response, the response components for the bar and background remained unchanged across different onset latencies (n = 18 flies).</p></caption>
<graphic xlink:href="561122v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In response to these patterns, flies exhibited body rotations causally associated with both the background and bar movements (<xref rid="fig5" ref-type="fig">Figure 5C-D</xref>). Because each pattern pair differed only in the direction of background motion, the difference in the post-stimulus body angle within each pair is attributable primarily to the background movement. Interestingly, these differences remained similar across experiments except when the loom started 100 ms before the background movement (<xref rid="fig5" ref-type="fig">Figure 5C-D</xref>). To isolate the contribution of each stimulus, we estimated the background response component by taking half the difference between responses to each pair (<xref rid="fig5" ref-type="fig">Figure 5E</xref>).</p>
<p>Likewise, the bar response component was estimated as the average of the two responses within each pair (<xref rid="fig5" ref-type="fig">Figure 5F</xref>). Supporting the validity of this linear approach, the background response component always began increasing during the background motion, irrespective of bar onset time (black arrows in <xref rid="fig5" ref-type="fig">Figure 5E</xref>). Similarly, the estimated bar response component consistently began during the bar movement (black arrows in <xref rid="fig5" ref-type="fig">Figure 5F</xref>).</p>
<p>Quantifying these components revealed no significant differences in response amplitude across the patterns (<xref rid="fig5" ref-type="fig">Figure 5G</xref>; one-way ANOVA). Moreover, the amplitude of the bar response component was comparable to that evoked by the bar alone, in the absence of background motion (black dotted lines in <xref rid="fig5" ref-type="fig">Figure 5F</xref>; gray bar in <xref rid="fig5" ref-type="fig">Figure 5G</xref>). To test whether the apparent lack of the background response reduction was due to the relatively long background duration (160 ms), we repeated the experiment with a shorter background movement (140 ms; <xref rid="fig5" ref-type="fig">Figure 5H</xref> and Figure 5—figure supplement 1). While the background response component was overall reduced, we again observed no significant differences in bar or background response amplitudes across patterns (<xref rid="fig5" ref-type="fig">Figure 5H</xref>).</p>
<p>Together, these results suggest that flies can respond to unexpected background movement during bar-evoked flight turns, and these responses appear to sum linearly in the steering behavior––even when overlapping in time––favoring the addition-only model and the graded EC model.</p>
</sec>
<sec id="s2g">
<title>Loom- and background movement-evoked flight turns exhibit mutual suppression</title>
<p>Are visually evoked steering responses always combined linearly? A previous study showed that background motion-sensitive neurons receive suppressive inputs during loom-evoked flight turns (<xref ref-type="bibr" rid="c24">Fenk et al., 2021</xref>). To test whether this suppression also occurs at the behavioral level, we repeated the two-pattern superposition experiments using loom and background movement patterns (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). Specifically, we presented a looming disc pattern before, during, and after the background movement (<xref rid="fig6" ref-type="fig">Figure 6A-B</xref>). As in the previous experiment, we created pattern pairs by inverting the direction of background movement.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Loom- and background motion–evoked steering responses exhibit mutual suppression.</title>
<p><bold>(A)</bold> Visual stimulus patterns. <bold>(B)</bold> Superposition patterns, where background rotation began 300 ms before loom pattern onset. <bold>(C)</bold> Body angle changes in response to the superposition patterns with varying onset latencies. <bold>(D)</bold> Amplitude of body angle changes in response to the superposition patterns (n = 21 - 24 flies). <bold>(E)</bold> The background response component was estimated by calculating the difference between body angle responses to superposition patterns with the same onset latency but opposite background rotation directions, then halving this difference (n = 21 flies). <bold>(F)</bold> Amplitude of the background response component, measured as the difference between the average body angle during the 200 ms period immediately before background movement onset and the average body angle during the 200 ms period starting 400 ms after background movement onset. <bold>(G)</bold> The loom response component was estimated by summing body angle responses with the same onset latency but opposite background movement directions, then halving this sum. <bold>(H)</bold> Amplitude of the loom response component, measured as the difference between the average body angle during the 200 ms period immediately before loom movement onset and the average body angle during the 200 ms period starting 400 ms after loom onset.</p></caption>
<graphic xlink:href="561122v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In response to these patterns, we first observed that the difference in body angle within each pair appeared similar across all conditions, except when the loom pattern preceded background motion by 100 ms (<xref rid="fig6" ref-type="fig">Figure 6C-D</xref>). In this condition, the background response component was significantly weaker than the others (<xref rid="fig6" ref-type="fig">Figure 6E-F</xref>). This suggests that the ongoing loom response can strongly suppress the background response, consistent with the previous study that demonstrated the suppression of wide-field motion-sensitive neurons during loom-evoked turns (<xref ref-type="bibr" rid="c24">Fenk et al., 2021</xref>).</p>
<p>When we estimated the loom response component for each visual pattern pair, we were surprised to find that its amplitude was significantly reduced when the background motion and looming disc began simultaneously (<xref rid="fig6" ref-type="fig">Figure 6G-H</xref>). Notably, under this condition, the background response component remained unchanged compared to the response evoked by background motion alone. This suggests that the background-evoked steering response may have suppressed the loom-evoked response when the two patterns started simultaneously, potentially due to a shorter latency in the background response than in the loom response. Indeed, the body angle change in response to the background-only pattern began slightly earlier on average than that for the loom-only pattern, although the loom response exhibited a much steeper onset slope (<xref rid="fig6" ref-type="fig">Figure 6I</xref>).</p>
<p>Together, these results show that loom- and background-evoked steering responses mutually suppress each other. Moreover, whichever pattern activates the steering motor system first appears to suppress the subsequent one. Together, these findings support the all-or-none EC model, acting bidirectionally between loom- and background movement-evoked flight turns.</p>
</sec>
<sec id="s2h">
<title>Flight turns dynamics are insensitive to changes in the static background pattern</title>
<p>We have shown that during loom-evoked, but not bar-evoked, turns, flies fail to respond to a moving background. In the experiments above, background rotation was externally imposed and not linked to any self-movement. In natural environments, however, optic flow feedback arises primarily from self-movement that shifts the compound eye relative to a static background. We therefore tested whether or not the dynamics of visually evoked turns are affected by the optic flow generated by self-movement. Specifically, we reasoned that the dynamics of object-evoked turns should decrease when the intensity of background-evoked optic flow increases, if not fully suppressed (<xref rid="fig4" ref-type="fig">Figure 4E</xref>). Our results with rotating backgrounds (<xref rid="fig5" ref-type="fig">Figures 5</xref> and <xref rid="fig6" ref-type="fig">6</xref>) predict that dynamics of bar-evoked, but not loom-evoked, turns change depending on the surrounding scene.</p>
    <p>We first presented a moving vertical bar against random dot backgrounds of varying density (<xref ref-type="fig" rid="fig7">Figure 7A–B</xref>, Movie S2). Each trial began with a uniform, bright background and a dark vertical bar positioned at the center. At the onset of bar movement, the background either remained unchanged or switched to one of two random dot patterns (<xref rid="fig7" ref-type="fig">Figure 7A</xref>). The central visual field (±55° from center) was kept uniform throughout each trial to preserve the saliency of the bar. Surprisingly, bar-evoked responses were nearly identical across all three background conditions (<xref rid="fig7" ref-type="fig">Figure 7C</xref>). Since turn amplitude was greater in response to dense dot backgrounds than to sparse ones when presented alone (<xref rid="fig7" ref-type="fig">Figure 7B</xref>), flies appeared to ignore background-associated optic flow during bar-evoked turns. This result seemingly contradicts the findings from experiments with externally imposed background motion (<xref rid="fig5" ref-type="fig">Figure 5</xref>).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Dynamics of object-evoked flight turns were not affected by the background-dependent optic flow intensity.</title>
<p>(A) Temporal profile of the visual stimulus used to test the bar response when the background changes from the uniform to the dense-dot background. In each frame, the horizontal midline of the display is sampled and plotted over time (top). Sample pattern images at the moment of the bar movement onset (bottom middle). (B) Body angle changes to the rotation of the random dot backgrounds in two different densities (left). The amplitude is significantly larger for the dense background than the sparse one (right, Wilcoxon rank sum test). The background pattern was moved horizontally by 45 degrees in 200 ms. (C) Average body orientation traces in response to horizontally moving bars when the initial uniform, bright background is kept the same or changed to one of the two random-dot backgrounds. The thick colored lines in the top plots represent the population average, whereas the gray lines represent an average for individual flies. Box-and-whisker plots depict the amplitude (middle) and latency (bottom) of the body angle change. Error bars (bottom) indicate 95% confidence interval. (D) Same as in (C), but for different combinations of background patterns.</p></caption>
<graphic xlink:href="561122v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To corroborate this finding, we repeated the experiment using dot patterns that filled the entire panoramic visual field, with the background transition occurring 0.25 seconds before bar movement onset (<xref rid="fig7" ref-type="fig">Figure 7D</xref>, Figure 7—figure supplement 1A). We tested four background transition conditions: sparse-to-sparse, sparse-to-dense, dense-to-dense, and dense-to-sparse. Across all conditions, flight turn dynamics—measured via body orientation and head yaw angle—remained unaffected (<xref rid="fig7" ref-type="fig">Figure 7D</xref>; Figure 7—figure supplement 1B).</p>
<p>We next repeated the above experiment with a looming disc pattern and observed no significant change in the body angle response across the background dot density (Figure 7–figure supplement 2C-E). This result is consistent with the behavior of the all-or-none EC model (<xref rid="fig4" ref-type="fig">Figure 4</xref>) and is also supported by the moving background experiment (<xref rid="fig6" ref-type="fig">Figure 6</xref>).</p>
<p>For bar-evoked turns, the dynamics of flight turns appear to be insensitive to the background dot density, seemingly contradicting the additive integration of responses to a moving bar with those to a moving background (<xref rid="fig5" ref-type="fig">Figure 5</xref>). If the responses to the two visual features were combined linearly in flying flies, then the bar-evoked turn would be expected to slow down significantly when the static background pattern changed, for example, from the uniform to the dense-dot background (<xref rid="fig4" ref-type="fig">Figure 4E</xref>). Why is the flight turn dynamics unaffected by the optic flow intensity generated by a static background? Since the linear integration of the bar and moving background responses excluded the all-or-none EC model (<xref rid="fig5" ref-type="fig">Figure 5</xref>), one potential explanation is that flies may employ a graded, rapidly adapting EC during bar-evoked turns. Graded EC signals can act to selectively suppress responses to the optic flow feedback associated with the static background, but not the moving background. In this scenario, the amplitude of the graded EC would need to be updated almost instantaneously when the static background changed (<xref rid="fig7" ref-type="fig">Figure 7A-D</xref>).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p><italic>Drosophila</italic> visuomotor circuits have been dissected systematically in recent years, but in a fragmented manner for limited sensory and behavioral contexts. To build an integrative framework for quantitatively testing these circuits and their interactions in complex visual environments, we developed a computational model of vision-based steering control. In this model, multiple visual features collectively determine flight heading through an EC-based or additive mechanism. The model captured the key features of steering behaviors for flying <italic>Drosophila</italic> in response to singly presented patterns as well as to their superpositions. Our results also demonstrated the mutual suppression between the optic-flow-evoked turns and loom-evoked turns. Overall, our study provides a computational framework for implementing and testing detailed models for feedforward and feedback neural circuits in the <italic>Drosophila</italic> visuomotor processing.</p>
<sec id="s3a">
<title>Models of <italic>Drosophila</italic> visuomotor processing</title>
<p>Visual systems extract features from the environment by calculating spatiotemporal relationships of neural activities within an array of photoreceptors. In <italic>Drosophila</italic>, these calculations occur initially on a local scale in the peripheral layers of the optic lobe (<xref ref-type="bibr" rid="c25">Frighetto and Frye, 2023</xref>; <xref ref-type="bibr" rid="c32">Gruntman et al., 2018</xref>; <xref ref-type="bibr" rid="c41">Ketkar et al., 2020</xref>). When these features enter the central brain through visual projection neurons, they are integrated across a large visual field via either many columnar cells converging into a small brain region or via tangential cells having wide-field dendrites (<xref ref-type="bibr" rid="c27">Garner et al., 2024</xref>; <xref ref-type="bibr" rid="c33">Hardcastle et al., 2021</xref>; <xref ref-type="bibr" rid="c37">Isaacson et al., 2023</xref>; <xref ref-type="bibr" rid="c52">Mauss et al., 2015</xref>; <xref ref-type="bibr" rid="c75">Wu et al., 2016</xref>). These higher-order features are subsequently relayed to other central brain regions or descending neural circuits, which ultimately control action (<xref ref-type="bibr" rid="c2">Aymanns et al., 2022</xref>; <xref ref-type="bibr" rid="c59">Namiki et al., 2018</xref>).</p>
<p>The mechanisms for key visual computations such as direction-dependent motion computation, collision avoidance, stability reflex, object tracking, and navigation are under active investigation, and <italic>in-silico</italic> models for some of these sensory computations are already available (<xref ref-type="bibr" rid="c6">Borst and Weber, 2011</xref>; <xref ref-type="bibr" rid="c32">Gruntman et al., 2018</xref>; <xref ref-type="bibr" rid="c46">Lappalainen et al., 2024</xref>). On the motor side, the neuromechanical interaction between the animal’s actuator and the external world has been modeled in recent studies (<xref ref-type="bibr" rid="c48">Lobato-Rios et al., 2022</xref>; <xref ref-type="bibr" rid="c53">Melis et al., 2024</xref>; <xref ref-type="bibr" rid="c74">Wang-Chen et al., 2024</xref>). What remains to be explored is the mapping of the features in the visual space onto a coordinated activation pattern of motor neurons by higher-order brain circuits, in a manner that optimizes the organism’s objective functions (i.e., survival and reproduction). Our study provides an integrative model of the visuomotor mapping in <italic>Drosophila</italic>, at the phenomenological level. This model can be further refined in the future based on studies that reveal the structural and functional organization of the underlying neural circuits.</p>
</sec>
<sec id="s3b">
<title>Efference copy in <italic>Drosophila</italic> vision</title>
<p>Under natural conditions, various visual features in the environment may concurrently activate multiple motor programs. Because these may interfere with one another, it is crucial for the central brain to coordinate between the motor signals originating from different sensory circuits. Among such coordination mechanisms, the EC mechanisms were hypothesized to counteract so-called reafferent visual input, those caused specifically by self-movement (<xref ref-type="bibr" rid="c15">Collett, 1980</xref>; <xref ref-type="bibr" rid="c73">von Holst and Mittelstaedt, 1950</xref>). Recent studies reported such EC-like signals in <italic>Drosophila</italic> visual neurons during spontaneous as well as loom-evoked flight turns (<xref ref-type="bibr" rid="c24">Fenk et al., 2021</xref>; <xref ref-type="bibr" rid="c42">Kim et al., 2017</xref>, <xref ref-type="bibr" rid="c43">2015</xref>). One type of EC-like signals were identified in a group of wide-field visual motion-sensing neurons that were shown to control the neck movement for the gaze stability (<xref ref-type="bibr" rid="c42">Kim et al., 2017</xref>). The EC-like signals in these cells were bidirectional depending on the direction of flight turns, and their amplitudes were quantitatively tuned to those of the expected visual input across cell types. Although amplitude varies among cell types, it remains inconclusive whether it also varies within a given cell type to match the amplitude of expected visual feedback, thereby implementing the graded EC signal. A more recent study examined EC-like signal amplitude in the same visual neurons for loom-evoked turns, across events (<xref ref-type="bibr" rid="c24">Fenk et al., 2021</xref>). Although the result showed a strong correlation between wing response and the EC-like inputs, the authors pointed that this apparent correlation could stem from noisy measurement of all-or-none motor-related inputs. Thus, these studies did not completely disambiguate between graded vs. all-or-none EC signaling. Another type of EC-like signals observed in the visual circuit tuned to a moving spot exhibited characteristics consistent with all-or-none EC. That is, it entirely suppressed visual signaling, irrespective of the direction of the self-generated turn (<xref ref-type="bibr" rid="c43">Kim et al., 2015</xref>; <xref ref-type="bibr" rid="c72">Turner et al., 2022</xref>).</p>
<p>Efference-copy (EC)–like signals have been reported in several <italic>Drosophila</italic> visual circuits, yet their behavioral role remains unclear. Indirect evidence comes from a behavioral study showing that the dynamics of spontaneously generated flight turns were unaffected by unexpected background motion (<xref ref-type="bibr" rid="c4">Bender and Dickinson, 2006a</xref>). Likewise, our behavioral experiments showed that, during loom-evoked turns, responses to background motion are suppressed in an all-or-none manner (<xref rid="fig6" ref-type="fig">Figures 6</xref> and <xref rid="fig7" ref-type="fig">7</xref>). Consistent with this, motor-related inputs recorded in visual neurons exhibit nearly identical dynamics during spontaneous and loom-evoked turns (<xref ref-type="bibr" rid="c24">Fenk et al., 2021</xref>). Together, these behavioral and physiological parallels support the idea that a common efference-copy mechanism operates during both spontaneous and loom-evoked flight turns.</p>
<p>Unlike loom-evoked turns, bar-evoked turn dynamics changed in the presence of moving backgrounds (<xref rid="fig5" ref-type="fig">Figure 5</xref>), a result compatible with both the addition-only and graded EC models. However, when the static background was updated just before a bar-evoked turn—thereby altering the amplitude of optic flow—the turn dynamics remained unaffected (<xref rid="fig5" ref-type="fig">Figures 5</xref> and <xref rid="fig7" ref-type="fig">7</xref>), clearly contradicting the addition-only model. Thus, the graded EC model is the only one consistent with both findings. If a graded EC mechanism were truly at work, however, an unexpected background change should have modified turn dynamics because of the mismatch between expected and actual visual feedback (Figure 4–figure supplement 1)—yet we detected no such effect at any time scale examined (Figure 7–figure supplement 1). This mismatch would be ignored only if the amplitude of the graded EC adapted to environmental changes almost instantaneously—a mechanism that seems improbable given the limited computational capacity of the <italic>Drosophila</italic> brain. In electric fish, for example, comparable adjustments take more than 10 minutes (<xref ref-type="bibr" rid="c3">Bell, 1981</xref>; <xref ref-type="bibr" rid="c57">Muller et al., 2019</xref>). Further investigation is needed to clarify how reorienting flies ignore optic flow generated by static backgrounds, potentially by engaging EC mechanisms not captured by the models tested in this study.</p>
<p>Why would <italic>Drosophila</italic> rely on the all-or-none EC mechanism instead of the graded one for loom-evoked turns? A graded EC must be adjusted adaptively depending on the environment, as the amplitude of visual feedback varies with both the dynamics of self-generated movement and environmental conditions (e.g., empty vs. cluttered visual backgrounds) (Figure 4—figure supplement 1). Recent studies on electric fish have suggested that a large array of neurons in a multi-layer network is crucial for generating a modifiable efference copy signal matched to the current environment (<xref ref-type="bibr" rid="c57">Muller et al., 2019</xref>). Given their small-sized brain, flies might opt for a more economical design for suppressing unwanted visual inputs regardless of the visual environment. Circuits mediating such a type of EC were identified in the cricket auditory system during stridulation (<xref ref-type="bibr" rid="c61">Poulet and Hedwig, 2006</xref>), for example. Our study strongly suggests the existence of a similar circuit in the <italic>Drosophila</italic> visual system.</p>
<p>We tested the hypothesis that efference-copy (EC) signals guide action selection by suppressing specific visuomotor reflexes when multiple visual features compete. An alternative motif with a similar function is mutual inhibition between motor pathways (<xref ref-type="bibr" rid="c22">Edwards, 1991</xref>; <xref ref-type="bibr" rid="c58">Mysore and Kothari, 2020</xref>). In <italic>Drosophila</italic>, descending neurons form dense lateral connections (<xref ref-type="bibr" rid="c7">Braun et al., 2024</xref>), offering a substrate for such competitive interactions. Determining whether—and how—EC and mutual inhibition operate will require recordings from the neurons that ensure visual stability, which remain unidentified. Mapping these pathways and assessing how they are modulated by visual and behavioral context are important goals for future work.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Fly stocks and rearing</title>
<p>We used female Oregon-R <italic>Drosophila melanogaster</italic> 2-7 days post-eclosion for the experiments, unless mentioned otherwise. Flies were reared on standard cornmeal agar in 25°C incubators with a 12h-12h light/dark cycle and in 70% humidity. For all experiments, flies were cold-anesthetized at 4°C and tethered either to a tungsten pin for the rigidly tethered experiments (<xref rid="fig1" ref-type="fig">Figure 1</xref>) or to a steel pin for the magnetically tethered experiments (<xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig6" ref-type="fig">6</xref>).</p>
</sec>
<sec id="s4b">
<title>Visual display</title>
<p>We used a modular LED display system composed of 8×8 dot matrix LED arrays (<xref ref-type="bibr" rid="c63">Reiser and Dickinson, 2008</xref>). The visual stimuli were displayed on the interior of a cylindrical display arena featuring green LED pixels with a 570-nm peak wavelength, facing inward. The display covered 360 degrees (96 pixels) in the azimuth and 94 degrees (32 pixels) in elevation. From the animal’s perspective, each LED pixel subtended less than or equal to 3.75 degrees, which is narrower than the inter-ommatidial angle of the fly (~5 degrees) (<xref ref-type="bibr" rid="c30">Götz, 1965</xref>).</p>
</sec>
<sec id="s4c">
<title>Magnetic tethering setup</title>
<p>We constructed a magnetic tethering setup following the designs used in previous studies (<xref ref-type="bibr" rid="c21">Duistermars and Frye, 2008</xref>; <xref ref-type="bibr" rid="c42">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="c55">Mongeau and Frye, 2017</xref>). The support frame for the two neodymium magnets was designed in Fusion 360 (Autodesk, Inc.) and manufactured using a 3D printer (Ender 3 S1, Creality) (see <xref rid="fig3" ref-type="fig">Figure 3(A)</xref>). The bottom magnet had a ring shape with an outer diameter of 32 mm, an inner diameter of 13 mm, and a height of 10 mm. The top magnet was cylinder-shaped with a diameter of 12 mm and a height of 10 mm. The distance between the top of the bottom magnet and the bottom of the top magnet was 25 mm. A V-shaped jewel bearing (Swiss Jewel Company) was attached to the bottom surface of the top magnet using epoxy and was used to anchor a steel pin-tethered fly. We positioned a Prosilica GE-680C camera (Allied Vision Inc.) with a macro lens (Infinistix, 1.5x zoom, 94-mm working distance) on the ground to capture the fly’s behavior from below. To illuminate the fly without interfering with its vision, we affixed four infrared LEDs (850-nm peak wavelength) concentrically to the top surface of the bottom magnet support. Furthermore, we applied matte black acrylic paint (Chroma Inc.) to the bottom surface of the top magnet to minimize background glare in the acquired images and to the top surface of the bottom magnet to minimize visual interference to flies.</p>
</sec>
<sec id="s4d">
<title>Visual stimuli</title>
<p>To estimate the position and velocity functions, we measured flies’ wing responses in response to three visual patterns moving at the speed of 36°/s for 10 seconds (<xref rid="fig1" ref-type="fig">Figure 1</xref>). The bar pattern was a dark vertical stripe with a width of 15° (4 pixels) on a bright background that moved laterally from the back to back for a full rotation in either clockwise or counterclockwise directions. The spot pattern was a 2×2-pixel dark square on a bright background and moved in the same dynamics as the bar. The grating pattern consisted of 12 cycles of dark and bright vertical stripes, each with a width of 15° (4 pixels). For the magnetically tethered flight experiments (<xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig6" ref-type="fig">6</xref>), the patterns moved rapidly 45 degrees in a sigmoid dynamics. Each pattern consisted of 4 distinct phases: alignment, ready, go, and freeze. The vertical pattern used for the alignment consisted of 3 dark, 2 bright, 1 dark, 2 bright, and 3 dark pixel vertical stripes, spanning a total of 41.25° (11 pixels) in width. To align the orientation of the fly to the reference angle, this pattern moved horizontally in a sinusoidal dynamics for 4-5 seconds at the frequency 1 period/s, with its envelope amplitude decreasing linearly (Movie S2). The bar pattern turned to a dark uniform bar over a bright background with a width of 11.25° (3 pixels) and was used for the rest of the stimulus period. The random dot pattern was created by adding dark pixels at a random position in the display arena. The sparse dot pattern had these dots in 7% of the total pixels, while the dense dot pattern had 40%. All patterns were created and played at the rate of 50 frames per second.</p>
</sec>
<sec id="s4e">
<title>Behavioral experiment</title>
<p>For the rigidly tethered flight experiments (<xref rid="fig1" ref-type="fig">Figure 1</xref>), we glued the anterior-dorsal part of the fly’s thorax to a tungsten pin using a UV-cured glue (Bondic). The other end of the pin was inserted into a pin receptacle located at the center of the visual display. The fly body orientation was tilted down from the horizontal by 30° to match the normal flight attitude (<xref ref-type="bibr" rid="c18">David, 1978</xref>). The fly was illuminated by four 850-nm LEDs on the ring-shaped platform positioned on the top of the camera (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). The fly was imaged from the bottom by a GE-680 camera at 60 frame/s with a macro zoom lens (MLM3X-MP, Computar, NC, USA) and an infrared long-pass filter. Wingbeat signals were analyzed in real-time by FView and motmot package (<xref ref-type="bibr" rid="c68">Straw and Dickinson, 2009</xref>). For the magnetically tethered flight experiments (<xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig6" ref-type="fig">6</xref>), we instead used a short steel pin to levitate the fly by a magnetic field.</p>
</sec>
<sec id="s4f">
<title>Data analysis</title>
<p>For quantifying wing responses from the rigidly tethered flight experiment, we first subtracted the left wingbeat amplitude from the right wingbeat amplitude to calculate the body angle for each recording (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). For each fly, we determined the stimulus-triggered body angle for every pattern, provided that the number of trials in which the animal continuously flew was at least 6. We then computed the mean body angle across the trials for each fly, and from these individual means, we determined the population-averaged body angle (<xref rid="fig1" ref-type="fig">Figure 1</xref>). In the magnetically tethered flight experiment, we captured images of the flies at 60 frames per second (<xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig6" ref-type="fig">6</xref>). The camera was externally triggered by an Ubuntu computer via a USB-connected microcontroller board (Teensy, PJRC), and the trigger signal was stored on the Windows computer using WinEDR (University of Strathclyde, Glasgow), along with the stimulus type and position signals.The acquired images, along with their timestamps, were saved on the Ubuntu computer using FView (<xref ref-type="bibr" rid="c68">Straw and Dickinson, 2009</xref>). To analyze body kinematics (<xref rid="fig3" ref-type="fig">Figs. 3</xref>,<xref rid="fig6" ref-type="fig">6</xref>), we wrote a custom machine vision code in Matlab. Namely, we calculated the body angle by binarizing each frame, performing an eroding operation to remove small non-fly objects, and then obtaining the orientation of the largest region using <italic>regionprops()</italic> function in Matlab (Movie S3). To measure the amplitude of the visually evoked responses (<xref rid="fig3" ref-type="fig">Figs 3D</xref>, <xref ref-type="fig" rid="fig6">6B-D</xref>), we subtracted the mean body angle during the 300-ms interval immediately prior to the stimulus onset from the mean in the 300-ms interval starting 200 ms after the onset. The 50% latency was measured as the time at which the body angle crosses the 50% of the total pattern displacement from the baseline, with respect to the pattern movement onset (<xref rid="fig3" ref-type="fig">Figs. 3E</xref>). Additionally, we measured the head angle (Figure 7–figure supplement 1B) by tracking the position of both antennae with respect to the neck (Movie S3) through the deep learning-based software DeepLabCut (<xref ref-type="bibr" rid="c51">Mathis et al., 2018</xref>). The body kinematic variables were stored separately from the stimulus parameters and combined <italic>post hoc</italic> for further analyses. To synchronize these signals, we inserted a 2-s pause in the camera trigger at the beginning and end of each experiment. These no-trigger intervals appeared in both sets of data files and were used to align kinematic variables acquired from the image data to the stimulus parameters. To compensate for any asymmetry that may exist in the rigidly or magnetically tethered flight experiments, each pattern was presented multiple times in two opposing directions after inverting all the frames with respect to the front midline. We then inverted all the kinematic signals in the counterclockwise trials and merged them with the clockwise trials.</p>
<p>To analyze spontaneous saccades (Figure 3—figure supplement 1), we applied a low-pass filter with a 15 Hz cutoff frequency to the body angle traces. We then computed the derivative of the filtered signal to obtain angular velocities. A velocity exceeding a threshold of 250 deg/s (in absolute value) was considered a spontaneous saccade. We further analyzed all spontaneous saccades occurring during the freeze period (Figure 3—figure supplement 1A-C). For each detected saccade event, we extracted a body angle segment centered at the peak velocity, spanning 120 ms before and after. Within each segment, we further trimmed the signal from the point it crossed 20% of the full amplitude range to the point it crossed 80% of the range. The amplitude and duration were calculated based on this trimmed portion (Figure 3—figure supplement 1D).</p>
</sec>
<sec id="s4g">
<title>Statistical analysis</title>
<p>All statistical analysis was performed in Matlab. Heading responses to random dot patterns in the two different densities were analyzed by Wilcoxon rank-sum test (<xref rid="fig7" ref-type="fig">Figure 7B</xref>). Responses to moving bars (<xref rid="fig7" ref-type="fig">Figure 7C-D</xref>) and to looming discs (Figure 7–figure supplement 1D-E) for different background profiles were analyzed by one-way ANOVA test.</p>
</sec>
<sec id="s4h">
<title>Modeling visually guided flight control in <italic>Drosophila</italic> (<xref rid="fig2" ref-type="fig">Figure 2</xref>)</title>
<p>The orientation of the fly is obtained through a second order dynamical system that represents the biomechanics of the fly’s body on the left side and the visually-evoked torque response on the right (<xref rid="fig2" ref-type="fig">Figure 2A</xref>).
<disp-formula id="eqn4">
<graphic xlink:href="561122v2_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where Ψ<sub><italic>fly</italic></sub> is the angular position of the fly, <inline-formula><inline-graphic xlink:href="561122v2_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> Ψ̇<sub><italic>fly</italic></sub> the angular velocity, <inline-formula><inline-graphic xlink:href="561122v2_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the angular acceleration, <italic>I</italic> = 6·10<sup>−14</sup> kg m<sup>2</sup> the moment of inertia of the fly’s body, β = 1·10<sup>−11</sup> kg m<sup>2</sup> s<sup>-1</sup> the drag coefficient of its wings (<xref ref-type="bibr" rid="c20">Michael H. Dickinson, 2005</xref>; <xref ref-type="bibr" rid="c65">Ristroph et al., 2010</xref>), Ψ<sub><italic>e</italic></sub> the error angle between the fly and the pattern, and <inline-formula><inline-graphic xlink:href="561122v2_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the error velocity.</p>
<p>We approximated the experimentally obtained position and velocity functions with the following mathematical representations for each pattern (<xref rid="fig2" ref-type="fig">Figure 2B</xref>).</p>
<disp-formula id="eqn5">
<graphic xlink:href="561122v2_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn6">
<graphic xlink:href="561122v2_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7">
<graphic xlink:href="561122v2_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<p>To solve <xref ref-type="disp-formula" rid="eqn4">Equation 4</xref> using an ODE solver (ode45 in Matlab), we reformulated it into an array of first-order ODEs as in the following.
<disp-formula id="eqn8">
<graphic xlink:href="561122v2_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the visually evoked torque response, <italic>F</italic>, is defined as follows for different visual patterns.
<disp-formula id="eqn9">
<graphic xlink:href="561122v2_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn10">
<graphic xlink:href="561122v2_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn11">
<graphic xlink:href="561122v2_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>To simulate the noisy steering behavior of flies in a static visual environment (<xref rid="fig2" ref-type="fig">Figure 2E</xref>), we added a Gaussian noise, <italic>n</italic>, to the torque, as follows.
<disp-formula id="eqn12">
<graphic xlink:href="561122v2_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
To incorporate an integrate-and-saccade strategy during bar tracking, we expanded our bar response model with a saccade mechanism reported in a previous study (<xref ref-type="bibr" rid="c55">Mongeau and Frye, 2017</xref>) (Figure 2–figure supplement 2A). Specifically, the error angle is integrated through a leaky integrator, and a saccade was generated when the integrated error angle (Q) crossed a threshold (C). The amplitude of the saccade was determined by the position and velocity functions estimated experimentally (<xref rid="fig2" ref-type="fig">Figure 2</xref>). We picked the values of <italic>C</italic> and <italic>τ</italic> (Figure 2–figure supplement 2C) that minimize the objective function given by <inline-formula><inline-graphic xlink:href="561122v2_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Where <italic>Ψ</italic><sub><italic>ampl</italic></sub> is the saccade amplitude and <italic>t</italic><sub><italic>interval</italic></sub> is the inter-saccade interval. Their ground-truth values are <italic>Ψ</italic><sub><italic>truth</italic></sub> = 9 degrees and <italic>t</italic><sub><italic>truth</italic></sub> = 0.3 s, which are approximated from previous work (<xref ref-type="bibr" rid="c55">Mongeau and Frye, 2017</xref>).</p>
</sec>
<sec id="s4i">
<title>Integrative models of the visuomotor control (<xref rid="fig4" ref-type="fig">Figure 4</xref>)</title>
<p>The addition-only model (<xref rid="fig4" ref-type="fig">Figure 4B</xref>) was developed by additively joining the torque response from the bar system, <italic>F</italic><sub><italic>bar</italic></sub>, and that from the grating system, <italic>F</italic><sub><italic>BG</italic></sub>, as in the following.
<disp-formula id="eqn14">
<graphic xlink:href="561122v2_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn14a">
<graphic xlink:href="561122v2_eqn14a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>For the graded EC-based model (<xref rid="fig4" ref-type="fig">Figure 4C</xref>), we defined the integration of the torque responses through an additional ODE that created an EC, ε, according to the fly velocity, <inline-formula><inline-graphic xlink:href="561122v2_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The EC signal, ε, was then subtracted from the grating torque.
<disp-formula id="eqn15">
<graphic xlink:href="561122v2_eqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>For the all-or-none EC-based model (<xref rid="fig4" ref-type="fig">Figure 4D</xref>), the grating torque, <italic>F</italic><sub><italic>BG</italic></sub>, was switched off to 0 when the bar-evoked torque became non-zero.
<disp-formula id="eqn16">
<graphic xlink:href="561122v2_eqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where η is an indicator function that takes the value of 1 when |<italic>F</italic><sub><italic>bar</italic></sub>| &gt; 1. 5 10 and 0 elsewhere.</p>
</sec>
<sec id="s4j">
<title>Auto-tuning mechanism for the graded EC-based model (Figure 4 – figure supplement 1)</title>
<p>We added a multi-layer-perceptron (MLP) to the graded EC-based model (<xref ref-type="disp-formula" rid="eqn15">Equation 15</xref>). The output of the MLP is used to calculate the EC (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). The equations and pseudocode for these computations are as follows:
<disp-formula id="eqn17">
<graphic xlink:href="561122v2_eqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn17a">
<graphic xlink:href="561122v2_eqn17a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where M = 2000 is the number of iterations, T = 2.5 s is the time range, Δ<italic>V</italic><sub><italic>BG</italic></sub> is the change in the visual feedback, <italic>α</italic> is the output of the MLP, r<sub>h</sub> is the output of the hidden layer, r<sub>in</sub> is the input, Ψ<sub>fly0</sub> is the heading of the fly in a fixed visual feedback condition, and γ=1·10<sup>−4</sup> is the learning rate.</p>
</sec>
<sec id="s4k">
<title>Virtual fly simulator</title>
<p>We have developed a graphic user interface (GUI) application in Matlab where users can simulate the magno-tethered flying fly for three visual patterns and their superpositions (e.g., bar and grating). The GUI also allows users to select from three integrative models (i.e., the addition-only and EC-based models) and choose both the displayed pattern and the maximum angular position amplitude of the stimulus. During the simulation, the GUI plots real-time heading and torque traces, as well as a simplified fruit fly that rotates in response to visual patterns (Movie S1). Finally, simulation results can be saved as a video file.</p>
</sec>
</sec>

</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We would like to thank all the lab members for the discussion and comments on the manuscript. We acknowledge the use of ChatGPT (<ext-link ext-link-type="uri" xlink:href="https://chat.openai.com/">https://chat.openai.com/</ext-link>) to assist in refining the language and clarity of this manuscript. This research was supported by the Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) under Grant 2020-0-01373, Artificial Intelligence Graduate School Program (Hanyang University); by Basic Science Research Program, and the Bio &amp; Medical Technology Development Program through the National Research Foundation of Korea (NRF) grant funded by the Korea Government (MSIT) under Grant NRF2020R1A4A1016840, NRF2021M3E5D2A01023888, NRF2022R1A2C2007599, NRF2022M3E5E8081195; by Tech Challenge Program for Future Policing funded by Ministry of Science and ICT (MSIT) &amp; Korean National Police Agency (KNPA) under Grant RS-2023-00243032.</p>
</ack>
<sec id="d1e2033" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Code availability</title>
<p>The essential code used to generate the primary results and conduct the simulations for this study is available in our GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/nisl-hyu/flightsim">https://github.com/nisl-hyu/flightsim</ext-link>).</p>
</sec>
<sec id="s6">
<title>Author contributions</title>
<p>A.C. and A.J.K. conceived the study and wrote the manuscript. A.C., with input from A.J.K., performed all the simulations. Behavioral experiments were designed by A.C. and A.J.K., carried out by H.K, Y.K. and J.P, and analyzed by A.C. and A.J.K.</p>
</sec>
</sec>
<sec id="suppd1e2033" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp4">
<label>Supplemental Figures</label>
<media xlink:href="supplements/561122_file04.pdf"/>
</supplementary-material>
    <supplementary-material id="supp1">
        <label>Movie S1</label>
        <media xlink:href="supplements/561122_file01.mp4"/>
    </supplementary-material>
    <supplementary-material id="supp2">
        <label>Movie S2</label>
        <media xlink:href="supplements/561122_file02.mp4"/>
    </supplementary-material>
    <supplementary-material id="supp3">
        <label>Movie S3</label>
        <media xlink:href="supplements/561122_file03.mp4"/>
    </supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ache</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Polsky</surname> <given-names>J</given-names></string-name>, <string-name><surname>Alghailani</surname> <given-names>S</given-names></string-name>, <string-name><surname>Parekh</surname> <given-names>R</given-names></string-name>, <string-name><surname>Breads</surname> <given-names>P</given-names></string-name>, <string-name><surname>Peek</surname> <given-names>MY</given-names></string-name>, <string-name><surname>Bock</surname> <given-names>DD</given-names></string-name>, <string-name><surname>von Reyn</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Card</surname> <given-names>GM</given-names></string-name></person-group>. <year>2019</year>. <article-title>Neural basis for looming size and velocity encoding in the Drosophila giant fiber escape pathway</article-title>. <source>Curr Biol</source> <volume>29</volume>:<fpage>1073</fpage>–<lpage>1081</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aymanns</surname> <given-names>F</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>C-L</given-names></string-name>, <string-name><surname>Ramdya</surname> <given-names>P.</given-names></string-name></person-group> <year>2022</year>. <article-title>Descending neuron population dynamics during odor-evoked and spontaneous limb-dependent behaviors</article-title>. <source>eLife</source> <volume>11</volume>:<elocation-id>e81527</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.81527</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bell</surname> <given-names>CC</given-names></string-name></person-group>. <year>1981</year>. <article-title>An efference copy which is modified by reafferent input</article-title>. <source>Science</source> <volume>214</volume>:<fpage>450</fpage>–<lpage>453</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bender</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2006a</year>. <article-title>A comparison of visual and haltere-mediated feedback in the control of body saccades in Drosophila melanogaster</article-title>. <source>J Exp Biol</source> <volume>209</volume>:<fpage>4597</fpage>–<lpage>4606</lpage>. doi:<pub-id pub-id-type="doi">10.1242/jeb.02583</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bender</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2006b</year>. <article-title>Visual stimulation of saccades in magnetically tethered Drosophila</article-title>. <source>J Exp Biol</source> <volume>209</volume>:<fpage>3170</fpage>–<lpage>3182</lpage>. doi:<pub-id pub-id-type="doi">10.1242/jeb.02369</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Borst</surname> <given-names>A</given-names></string-name>, <string-name><surname>Weber</surname> <given-names>F.</given-names></string-name></person-group> <year>2011</year>. <article-title>Neural action fields for optic flow based navigation: a simulation study of the fly lobula plate network</article-title>. <source>PLOS One</source> <volume>6</volume>:<fpage>e16303</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0016303.t002</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Braun</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hurtak</surname> <given-names>F</given-names></string-name>, <string-name><surname>Wang-Chen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ramdya</surname> <given-names>P.</given-names></string-name></person-group> <year>2024</year>. <article-title>Descending networks transform command signals into population motor control</article-title>. <source>Nature</source> <volume>630</volume>:<fpage>686</fpage>–<lpage>694</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41586-024-07523-9</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breugel</surname> <given-names>FV</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2012</year>. <article-title>The visual control of landing and obstacle avoidance in the fruit fly Drosophila melanogaster</article-title>. <source>J Exp Biol</source> <volume>215</volume>:<fpage>1783</fpage>–<lpage>1798</lpage>. doi:<pub-id pub-id-type="doi">10.1242/jeb.066498</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Card</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2008</year>. <article-title>Visually mediated motor planning in the escape response of Drosophila</article-title>. <source>Curr Biol CB</source> <volume>18</volume>:<fpage>1300</fpage>–<lpage>1307</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2008.07.094</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cellini</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mongeau</surname> <given-names>J-M.</given-names></string-name></person-group> <year>2022</year>. <article-title>Nested mechanosensory feedback actively damps visually guided head movements in Drosophila</article-title>. <source>eLife</source> <volume>11</volume>:<elocation-id>e80880</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.80880</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cellini</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mongeau</surname> <given-names>J-M.</given-names></string-name></person-group> <year>2020</year>. <article-title>Active vision shapes and coordinates flight motor responses in flies</article-title>. <source>Proc Natl Acad Sci</source> <volume>117</volume>:<fpage>23085</fpage>–<lpage>23095</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1920846117</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cellini</surname> <given-names>B</given-names></string-name>, <string-name><surname>Salem</surname> <given-names>W</given-names></string-name>, <string-name><surname>Mongeau</surname> <given-names>J-M.</given-names></string-name></person-group> <year>2022</year>. <article-title>Complementary feedback control enables effective gaze stabilization in animals</article-title>. <source>Proc Natl Acad Sci</source> <volume>119</volume>:<fpage>e2121660119</fpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.2121660119</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Censi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Straw</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Sayaman</surname> <given-names>RW</given-names></string-name>, <string-name><surname>Murray</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2013</year>. <article-title>Discriminating external and internal causes for heading changes in freely flying Drosophila</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>:<fpage>e1002891</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1002891</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clark</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Bursztyn</surname> <given-names>L</given-names></string-name>, <string-name><surname>Horowitz</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Schnitzer</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Clandinin</surname> <given-names>TR</given-names></string-name></person-group>. <year>2011</year>. <article-title>Defining the computational structure of the motion detector in Drosophila</article-title>. <source>Neuron</source> <volume>70</volume>:<fpage>1165</fpage>–<lpage>1177</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2011.05.023</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Collett</surname> <given-names>T.</given-names></string-name></person-group> <year>1980</year>. <article-title>Angular tracking and the optomotor response an analysis of visual reflex interaction in a hoverfly</article-title>. <source>J Comp Physiol</source> <volume>140</volume>:<fpage>145</fpage>–<lpage>158</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Collett</surname> <given-names>TS</given-names></string-name>, <string-name><surname>Land</surname> <given-names>MF</given-names></string-name></person-group>. <year>1975</year>. <article-title>Visual control of flight behaviour in the hoverfly Syritta pipiens L</article-title>. <source>J Comp Physiol A Neuroethol Sens Neural Behav Physiol</source> <volume>99</volume>:<fpage>1</fpage>–<lpage>66</lpage>. doi:<pub-id pub-id-type="doi">10.1007/bf01464710</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Currier</surname> <given-names>TA</given-names></string-name>, <string-name><surname>Pang</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Clandinin</surname> <given-names>TR</given-names></string-name></person-group>. <year>2023</year>. <article-title>Visual processing in the fly, from photoreceptors to behavior</article-title>. <source>Genetics</source> <volume>224</volume>:<fpage>iyad064</fpage>. doi:<pub-id pub-id-type="doi">10.1093/genetics/iyad064</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>David</surname> <given-names>CT</given-names></string-name></person-group>. <year>1978</year>. <article-title>The relationship between body angle and flight speed in free-flying Drosophila</article-title>. <source>Physiol Entomol</source> <volume>3</volume>:<fpage>191</fpage>–<lpage>195</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1365-3032.1978.tb00148.x</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dickinson Michael</surname> <given-names>H.</given-names></string-name></person-group> <year>2005</year>. <article-title>The initiation and control of rapid flight maneuvers in fruit flies</article-title>. <source>Integr Comp Biol</source> <volume>45</volume>:<fpage>274</fpage>–<lpage>281</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dickinson Michael</surname> <given-names>H.</given-names></string-name></person-group> <year>2005</year>. <article-title>The Initiation and Control of Rapid Flight Maneuvers in Fruit Flies</article-title>. <source>Integr Comp Biol</source> <volume>45</volume>:<fpage>274</fpage>–<lpage>281</lpage>. doi:<pub-id pub-id-type="doi">10.1093/icb/45.2.274</pub-id></mixed-citation></ref>
    <ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duistermars</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>M.</given-names></string-name></person-group> <year>2008</year>. <article-title>A magnetic tether system to investigate visual and olfactory mediated flight control in Drosophila</article-title>. <source>J Vis Exp 1063</source>. doi:<pub-id pub-id-type="doi">10.3791/1063</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edwards</surname> <given-names>D.</given-names></string-name></person-group> <year>1991</year>. <article-title>Mutual inhibition among neural command systems as a possible mechanism for behavioral choice in crayfish</article-title>. <source>J Neurosci</source> <volume>11</volume>:<fpage>1210</fpage>–<lpage>1223</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.11-05-01210.1991</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fenk</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Avritzer</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Weisman</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Nair</surname> <given-names>A</given-names></string-name>, <string-name><surname>Randt</surname> <given-names>LD</given-names></string-name>, <string-name><surname>Mohren</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Siwanowicz</surname> <given-names>I</given-names></string-name>, <string-name><surname>Maimon</surname> <given-names>G.</given-names></string-name></person-group> <year>2022</year>. <article-title>Muscles that move the retina augment compound eye vision in Drosophila</article-title>. <source>Nature</source> <volume>612</volume>:<fpage>116</fpage>–<lpage>122</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41586-022-05317-5</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fenk</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Maimon</surname> <given-names>G.</given-names></string-name></person-group> <year>2021</year>. <article-title>Suppression of motion vision during course-changing, but not course-stabilizing, navigational turns</article-title>. <source>Curr Biol</source> <volume>31</volume>:<fpage>4608</fpage>-<lpage>4619.e3</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2021.09.068</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frighetto</surname> <given-names>G</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name></person-group>. <year>2023</year>. <article-title>Columnar neurons support saccadic bar tracking in Drosophila</article-title>. <source>eLife</source> <volume>12</volume>:<elocation-id>e83656</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.83656</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fry</surname> <given-names>SN</given-names></string-name>, <string-name><surname>Sayaman</surname> <given-names>R</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2005</year>. <article-title>The aerodynamics of hovering flight in Drosophila</article-title>. <source>J Exp Biol</source> <volume>208</volume>:<fpage>2303</fpage>–<lpage>2318</lpage>. doi:<pub-id pub-id-type="doi">10.1242/jeb.01612</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garner</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kind</surname> <given-names>E</given-names></string-name>, <string-name><surname>Lai</surname> <given-names>JYH</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>A</given-names></string-name>, <string-name><surname>Houghton</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sancer</surname> <given-names>G</given-names></string-name>, <string-name><surname>Wolff</surname> <given-names>T</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Wernet</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>SS</given-names></string-name></person-group>. <year>2024</year>. <article-title>Connectomic reconstruction predicts visual features used for navigation</article-title>. <source>Nature</source> <volume>634</volume>:<fpage>181</fpage>–<lpage>190</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41586-024-07967-z</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Götz</surname> <given-names>KG</given-names></string-name></person-group>. <year>1987</year>. <article-title>Course-control, metabolism and wing interference during ultralong tethered flight in drosophila melanogaster</article-title>. <source>J Exp Biol</source> <volume>128</volume>:<fpage>35</fpage>–<lpage>46</lpage>. doi:<pub-id pub-id-type="doi">10.1242/jeb.128.1.35</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Götz</surname> <given-names>KG</given-names></string-name></person-group>. <year>1968</year>. <article-title>Flight control in Drosophila by visual perception of motion</article-title>. <source>Kybernetik</source> <volume>4</volume>:<fpage>199</fpage>–<lpage>208</lpage>. doi:<pub-id pub-id-type="doi">10.1007/BF00272517</pub-id></mixed-citation></ref>
    <ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Götz</surname> <given-names>KG</given-names></string-name></person-group>. <year>1965</year>. <article-title>Die optischen übertragungseigenschaften der komplexaugen von Drosophila</article-title>. <source>Kybernetik</source> <volume>2</volume>:<fpage>215</fpage>–<lpage>221</lpage>. doi:<pub-id pub-id-type="doi">10.1007/BF00306417</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Götz</surname> <given-names>KG</given-names></string-name>, <string-name><surname>Hengstenberg</surname> <given-names>B</given-names></string-name>, <string-name><surname>Biesinger</surname> <given-names>R.</given-names></string-name></person-group> <year>1979</year>. <article-title>Optomotor control of wing beat and body posture in Drosophila</article-title>. <source>Biol Cybern</source> <volume>35</volume>:<fpage>101</fpage>–<lpage>112</lpage>. doi:<pub-id pub-id-type="doi">10.1007/BF00337435</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gruntman</surname> <given-names>E</given-names></string-name>, <string-name><surname>Romani</surname> <given-names>S</given-names></string-name>, <string-name><surname>Reiser</surname> <given-names>MB</given-names></string-name></person-group>. <year>2018</year>. <article-title>Simple integration of fast excitation and offset, delayed inhibition computes directional selectivity in Drosophila</article-title>. <source>Nat Neurosci</source> <volume>21</volume>:<fpage>250</fpage>–<lpage>257</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41593-017-0046-4</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hardcastle</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Omoto</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Kandimalla</surname> <given-names>P</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>B-CM</given-names></string-name>, <string-name><surname>Keleş</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Boyd</surname> <given-names>NK</given-names></string-name>, <string-name><surname>Hartenstein</surname> <given-names>V</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name></person-group>. <year>2021</year>. <article-title>A visual pathway for skylight polarization processing in Drosophila</article-title>. <source>eLife</source> <volume>10</volume>:<elocation-id>e63225</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.63225</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hassenstein</surname> <given-names>B</given-names></string-name>, <string-name><surname>Reichardt</surname> <given-names>W.</given-names></string-name></person-group> <year>1956</year>. <article-title>Systemtheoretische analyse der zeit-, reihenfolgen-und vorzeichenauswertung bei der bewegungsperzeption des rüsselkäfers chlorophanus</article-title>. <source>Z Für Naturforschung</source> <volume>11</volume>:<fpage>513</fpage>–<lpage>524</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heisenberg</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wolf</surname> <given-names>R.</given-names></string-name></person-group> <year>1988</year>. <article-title>Reafferent control of optomotor yaw torque in Drosophila melanogaster</article-title>. <source>J Comp Physiol A</source> <volume>163</volume>:<fpage>373</fpage>–<lpage>388</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heisenberg</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wolf</surname> <given-names>R.</given-names></string-name></person-group> <year>1979</year>. <article-title>On the fine structure of yaw torque in visual flight orientation of Drosophila melanogaster</article-title>. <source>J Comp Physiol</source> <volume>130</volume>:<fpage>113</fpage>–<lpage>130</lpage>. doi:<pub-id pub-id-type="doi">10.1007/bf00611046</pub-id></mixed-citation></ref>
    <ref id="c37"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Isaacson</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Eliason</surname> <given-names>JLM</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Lott</surname> <given-names>GK</given-names></string-name>, <string-name><surname>Tabachnik</surname> <given-names>T</given-names></string-name>, <string-name><surname>Rowell</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Edwards</surname> <given-names>AW</given-names></string-name>, <string-name><surname>Korff</surname> <given-names>WL</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Branson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Reiser</surname> <given-names>MB</given-names></string-name></person-group>. <year>2023</year>. <article-title>Small-field visual projection neurons detect translational optic flow and support walking control</article-title>. <source>bioRxiv</source> doi:<pub-id pub-id-type="doi">10.1101/2023.06.21.546024</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Joesch</surname> <given-names>M</given-names></string-name>, <string-name><surname>Schnell</surname> <given-names>B</given-names></string-name>, <string-name><surname>Raghu</surname> <given-names>SV</given-names></string-name>, <string-name><surname>Reiff</surname> <given-names>DF</given-names></string-name>, <string-name><surname>Borst</surname> <given-names>A.</given-names></string-name></person-group> <year>2010</year>. <article-title>ON and OFF pathways in Drosophila motion vision</article-title>. <source>Nature</source> <volume>468</volume>:<fpage>300</fpage>–<lpage>304</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature09545</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keles</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name></person-group>. <year>2017</year>. <article-title>Object-detecting neurons in Drosophila</article-title>. <source>Curr Biol</source> <volume>27</volume>:<fpage>680</fpage>–<lpage>687</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2017.01.012</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kennedy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wayne</surname> <given-names>G</given-names></string-name>, <string-name><surname>Kaifosh</surname> <given-names>P</given-names></string-name>, <string-name><surname>Alviña</surname> <given-names>K</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sawtell</surname> <given-names>NB</given-names></string-name></person-group>. <year>2014</year>. <article-title>A temporal basis for predicting the sensory consequences of motor commands in an electric fish</article-title>. <source>Nat Neurosci</source> <volume>17</volume>:<fpage>416</fpage>–<lpage>422</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ketkar</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Sporar</surname> <given-names>K</given-names></string-name>, <string-name><surname>Gür</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ramos-Traslosheros</surname> <given-names>G</given-names></string-name>, <string-name><surname>Seifert</surname> <given-names>M</given-names></string-name>, <string-name><surname>Silies</surname> <given-names>M.</given-names></string-name></person-group> <year>2020</year>. <article-title>Luminance information is required for the accurate estimation of contrast in rapidly changing visual contexts</article-title>. <source>Curr Biol</source> <volume>30</volume>:<fpage>657</fpage>-<lpage>669.e4</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2019.12.038</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Fenk</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Lyu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Maimon</surname> <given-names>G.</given-names></string-name></person-group> <year>2017</year>. <article-title>Quantitative predictions orchestrate visual signaling in Drosophila</article-title>. <source>Cell</source> <volume>168</volume>:<fpage>280</fpage>-<lpage>294.e12</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cell.2016.12.005</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Fitzgerald</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Maimon</surname> <given-names>G.</given-names></string-name></person-group> <year>2015</year>. <article-title>Cellular evidence for efference copy in Drosophila visuomotor processing</article-title>. <source>Nat Neurosci</source> <volume>18</volume>:<fpage>1247</fpage>–<lpage>1255</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname> <given-names>G</given-names></string-name>, <string-name><surname>An</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ha</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name></person-group>. <year>2023</year>. <article-title>A deep learning analysis of Drosophila body kinematics during magnetically tethered flight</article-title>. <source>J Neurogenet</source> <volume>37</volume>:<fpage>47</fpage>–<lpage>56</lpage>. doi:<pub-id pub-id-type="doi">10.1080/01677063.2023.2210682</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname> <given-names>H</given-names></string-name>, <string-name><surname>Park</surname> <given-names>H</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name></person-group>. <year>2023</year>. <article-title>A visuomotor circuit for evasive flight turns in Drosophila</article-title>. <source>Curr Biol</source> <volume>33</volume>:<fpage>321</fpage>-<lpage>335.e6</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2022.12.014</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lappalainen</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Tschopp</surname> <given-names>FD</given-names></string-name>, <string-name><surname>Prakhya</surname> <given-names>S</given-names></string-name>, <string-name><surname>McGill</surname> <given-names>M</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shinomiya</surname> <given-names>K</given-names></string-name>, <string-name><surname>Takemura</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gruntman</surname> <given-names>E</given-names></string-name>, <string-name><surname>Macke</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Turaga</surname> <given-names>SC</given-names></string-name></person-group>. <year>2024</year>. <article-title>Connectome-constrained networks predict neural activity across the fly visual system</article-title>. <source>Nature</source> <volume>634</volume>:<fpage>1132</fpage>–<lpage>1140</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41586-024-07939-3</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sane</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Mongeau</surname> <given-names>J-M</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cheng</surname> <given-names>B.</given-names></string-name></person-group> <year>2019</year>. <article-title>Flies land upside down on a ceiling using rapid visually mediated rotational maneuvers</article-title>. <source>Sci Adv</source> <volume>5</volume>:<fpage>eaax1877</fpage>. doi:<pub-id pub-id-type="doi">10.1126/sciadv.aax1877</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lobato-Rios</surname> <given-names>V</given-names></string-name>, <string-name><surname>Ramalingasetty</surname> <given-names>ST</given-names></string-name>, <string-name><surname>Özdil</surname> <given-names>PG</given-names></string-name>, <string-name><surname>Arreguit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ijspeert</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Ramdya</surname> <given-names>P.</given-names></string-name></person-group> <year>2022</year>. <article-title>NeuroMechFly, a neuromechanical model of adult Drosophila melanogaster</article-title>. <source>Nat Methods</source> <volume>19</volume>:<fpage>620</fpage>–<lpage>627</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41592-022-01466-7</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maimon</surname> <given-names>G</given-names></string-name>, <string-name><surname>Straw</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2008</year>. <article-title>A simple vision-based algorithm for decision making in flying Drosophila</article-title>. <source>Curr Biol</source> <volume>18</volume>:<fpage>464</fpage>–<lpage>470</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maisak</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Haag</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ammer</surname> <given-names>G</given-names></string-name>, <string-name><surname>Serbe</surname> <given-names>E</given-names></string-name>, <string-name><surname>Meier</surname> <given-names>M</given-names></string-name>, <string-name><surname>Leonhardt</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schilling</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bahl</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Dickson</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Reiff</surname> <given-names>DF</given-names></string-name>, <string-name><surname>Hopp</surname> <given-names>E</given-names></string-name>, <string-name><surname>Borst</surname> <given-names>A.</given-names></string-name></person-group> <year>2013</year>. <article-title>A directional tuning map of Drosophila elementary motion detectors</article-title>. <source>Nature</source> <volume>500</volume>:<fpage>212</fpage>–<lpage>216</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature12320</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mamidanna</surname> <given-names>P</given-names></string-name>, <string-name><surname>Cury</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Abe</surname> <given-names>T</given-names></string-name>, <string-name><surname>Murthy</surname> <given-names>VN</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M.</given-names></string-name></person-group> <year>2018</year>. <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat Neurosci</source> <volume>21</volume>:<fpage>1281</fpage>–<lpage>1289</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mauss</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Pankova</surname> <given-names>K</given-names></string-name>, <string-name><surname>Arenz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Borst</surname> <given-names>A.</given-names></string-name></person-group> <year>2015</year>. <article-title>Neural circuit to integrate opposing motions in the visual field</article-title>. <source>Cell</source> <volume>162</volume>:<fpage>351</fpage>–<lpage>362</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cell.2015.06.035</pub-id></mixed-citation></ref>
    <ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Melis</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Siwanowicz</surname> <given-names>I</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2024</year>. <article-title>Machine learning reveals the control mechanics of an insect wing hinge</article-title>. <source>Nature</source> <fpage>1</fpage>–<lpage>9</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41586-024-07293-4</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mongeau</surname> <given-names>J-M</given-names></string-name>, <string-name><surname>Cheng</surname> <given-names>KY</given-names></string-name>, <string-name><surname>Aptekar</surname> <given-names>J</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name></person-group>. <year>2019</year>. <article-title>Visuomotor strategies for object approach and aversion in Drosophila melanogaster</article-title>. <source>J Exp Biol</source> <volume>222</volume>:<fpage>jeb193730</fpage>. doi:<pub-id pub-id-type="doi">10.1242/jeb.193730</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mongeau</surname> <given-names>J-M</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name></person-group>. <year>2017</year>. <article-title>Drosophila spatiotemporally integrates visual signals to control saccades</article-title>. <source>Curr Biol</source> <volume>27</volume>:<fpage>2901</fpage>-<lpage>2914.e2</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2017.08.035</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Muijres</surname> <given-names>FT</given-names></string-name>, <string-name><surname>Elzinga</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Melis</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2014</year>. <article-title>Flies evade looming targets by executing rapid visually directed banked turns</article-title>. <source>Science</source> <volume>344</volume>:<fpage>172</fpage>–<lpage>177</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1248955</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Muller</surname> <given-names>SZ</given-names></string-name>, <string-name><surname>Zadina</surname> <given-names>AN</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>LF</given-names></string-name>, <string-name><surname>Sawtell</surname> <given-names>NB</given-names></string-name></person-group>. <year>2019</year>. <article-title>Continual learning in a multi-layer network of an electric fish</article-title>. <source>Cell</source> <volume>179</volume>:<fpage>1382</fpage>-<lpage>1392.e10</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cell.2019.10.020</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mysore</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Kothari</surname> <given-names>NB</given-names></string-name></person-group>. <year>2020</year>. <article-title>Mechanisms of competitive selection: A canonical neural circuit framework</article-title>. <source>eLife</source> <volume>9</volume>:<elocation-id>e51473</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.51473</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Namiki</surname> <given-names>S</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Korff</surname> <given-names>W</given-names></string-name>, <string-name><surname>Card</surname> <given-names>GM</given-names></string-name></person-group>. <year>2018</year>. <article-title>The functional organization of descending sensory-motor pathways in Drosophila</article-title>. <source>eLife</source> <volume>7</volume>:<elocation-id>e10806</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/elife.34272</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poggio</surname> <given-names>T</given-names></string-name>, <string-name><surname>Reichardt</surname> <given-names>W.</given-names></string-name></person-group> <year>1973</year>. <article-title>A theory of the pattern induced flight orientation of the fly Musca domestica</article-title>. <source>Kybernetik</source> <volume>12</volume>:<fpage>185</fpage>–<lpage>203</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poulet</surname> <given-names>JFA</given-names></string-name>, <string-name><surname>Hedwig</surname> <given-names>B.</given-names></string-name></person-group> <year>2006</year>. <article-title>The cellular basis of a corollary discharge</article-title>. <source>Science</source> <volume>311</volume>:<fpage>518</fpage>–<lpage>522</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1120847</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reichardt</surname> <given-names>W</given-names></string-name>, <string-name><surname>Poggio</surname> <given-names>T.</given-names></string-name></person-group> <year>1976</year>. <article-title>Visual control of orientation behaviour in the fly: Part I. A quantitative analysis</article-title>. <source>Q Rev Biophys</source> <volume>9</volume>:<fpage>311</fpage>–<lpage>375</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reiser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2008</year>. <article-title>A modular display system for insect behavioral neuroscience</article-title>. <source>J Neurosci Methods</source> <volume>167</volume>:<fpage>127</fpage>–<lpage>139</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.07.019</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rimniceanu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Currea</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name></person-group>. <year>2023</year>. <article-title>Proprioception gates visual object fixation in flying flies</article-title>. <source>Curr Biol</source> <volume>33</volume>:<fpage>1459</fpage>-<lpage>1471.e3</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2023.03.018</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ristroph</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bergou</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Ristroph</surname> <given-names>G</given-names></string-name>, <string-name><surname>Coumes</surname> <given-names>K</given-names></string-name>, <string-name><surname>Berman</surname> <given-names>GJ</given-names></string-name>, <string-name><surname>Guckenheimer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>ZJ</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>I.</given-names></string-name></person-group> <year>2010</year>. <article-title>Discovering the flight autostabilizer of fruit flies by inducing aerial stumbles</article-title>. <source>Proc Natl Acad Sci</source> <volume>107</volume>:<fpage>4820</fpage>–<lpage>4824</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ryu</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>SY</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>AJ</given-names></string-name></person-group>. <year>2022</year>. <article-title>From photons to behaviors: Neural implementations of visual behaviors in Drosophila</article-title>. <source>Front Neurosci</source> <volume>16</volume>:<fpage>883640</fpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Städele</surname> <given-names>C</given-names></string-name>, <string-name><surname>Keleş</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Mongeau</surname> <given-names>J-M</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name></person-group>. <year>2020</year>. <article-title>Non-canonical receptive field properties and neuromodulation of feature-detecting neurons in flies</article-title>. <source>Curr Biol</source> <volume>30</volume>:<fpage>2508</fpage>-<lpage>2519.e6</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2020.04.069</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Straw</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2009</year>. <article-title>Motmot, an open-source toolkit for realtime video acquisition and analysis</article-title>. <source>Source Code Biol Med</source> <volume>4</volume>:<fpage>5</fpage>. doi:<pub-id pub-id-type="doi">10.1186/1751-0473-4-5</pub-id></mixed-citation></ref>
    <ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takemura</surname> <given-names>S-Y</given-names></string-name>, <string-name><surname>Bharioke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vitaladevuni</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rivlin</surname> <given-names>PK</given-names></string-name>, <string-name><surname>Katz</surname> <given-names>WT</given-names></string-name>, <string-name><surname>Olbris</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Plaza</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Winston</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>T</given-names></string-name>, <string-name><surname>Horne</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Fetter</surname> <given-names>RD</given-names></string-name>, <string-name><surname>Takemura</surname> <given-names>S</given-names></string-name>, <string-name><surname>Blazek</surname> <given-names>K</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>L-A</given-names></string-name>, <string-name><surname>Ogundeyi</surname> <given-names>O</given-names></string-name>, <string-name><surname>Saunders</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Shapiro</surname> <given-names>V</given-names></string-name>, <string-name><surname>Sigmund</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Scheffer</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Meinertzhagen</surname> <given-names>IA</given-names></string-name>, <string-name><surname>Chklovskii</surname> <given-names>DB</given-names></string-name></person-group>. <year>2013</year>. <article-title>A visual motion detection circuit suggested by Drosophila connectomics</article-title> <source>Nature</source> <volume>500</volume>:<fpage>175</fpage>–<lpage>181</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature12450</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tammero</surname> <given-names>LF</given-names></string-name>, <string-name><surname>Frye</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <year>2004</year>. <article-title>Spatial organization of visuomotor reflexes in Drosophila</article-title>. <source>J Exp Biol</source> <volume>207</volume>:<fpage>113</fpage>–<lpage>122</lpage>. doi:<pub-id pub-id-type="doi">10.1242/jeb.00724</pub-id></mixed-citation></ref>
    <ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanaka</surname> <given-names>R</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>DA</given-names></string-name></person-group>. <year>2020</year>. <article-title>Object-displacement-sensitive visual neurons drive freezing in Drosophila</article-title>. <source>Curr Biol</source> <fpage>1</fpage>–<lpage>28</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2020.04.068</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turner</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Krieger</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pang</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Clandinin</surname> <given-names>TR</given-names></string-name></person-group>. <year>2022</year>. <article-title>Visual and motor signatures of locomotion dynamically shape a population code for feature detection in Drosophila</article-title>. <source>eLife</source> <volume>11</volume>:<elocation-id>e82587</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.82587</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>von Holst</surname> <given-names>E</given-names></string-name>, <string-name><surname>Mittelstaedt</surname> <given-names>H.</given-names></string-name></person-group> <year>1950</year>. <article-title>The principle of reafference: Interactions between the central nervous system and the peripheral organs</article-title>. <source>Naturwissenschaften</source> <volume>37</volume>:<fpage>464</fpage>–<lpage>476</lpage>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang-Chen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Stimpfling</surname> <given-names>VA</given-names></string-name>, <string-name><surname>Lam</surname> <given-names>TKC</given-names></string-name>, <string-name><surname>Özdil</surname> <given-names>PG</given-names></string-name>, <string-name><surname>Genoud</surname> <given-names>L</given-names></string-name>, <string-name><surname>Hurtak</surname> <given-names>F</given-names></string-name>, <string-name><surname>Ramdya</surname> <given-names>P.</given-names></string-name></person-group> <year>2024</year>. <article-title>NeuroMechFly v2: simulating embodied sensorimotor control in adult Drosophila</article-title>. <source>Nat Methods</source> <volume>21</volume>:<fpage>2353</fpage>–<lpage>2362</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41592-024-02497-y</pub-id></mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Nern</surname> <given-names>A</given-names></string-name>, <string-name><surname>Williamson</surname> <given-names>WR</given-names></string-name>, <string-name><surname>Morimoto</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Reiser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Card</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>GM</given-names></string-name></person-group>. <year>2016</year>. <article-title>Visual projection neurons in the Drosophila lobula link feature detection to distinct behavioral programs</article-title>. <source>eLife</source> <volume>5</volume>:<elocation-id>e21022</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.21022</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93487.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Tuthill</surname>
<given-names>John C</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Washington</institution>
</institution-wrap>
<city>Seattle</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study investigates the implementation of an efference copy mechanism in the visual flight control system of Drosophila, a topic of broad interest to sensorimotor neuroscientists. Although the behavioral data and computational analyses are each individually <bold>solid</bold>, there is limited quantitative evaluation of how the model predictions compare to the experimental data.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93487.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study provides an integrative model of the visuomotor control in Drosophila melanogaster. This model presents an experimentally derived model based on visually evoked wingbeat pattern recordings of three strategically selected visual stimulus types with well-established behavioral response characteristics. By testing variations of these models, the authors demonstrate that the virtual model behavior can recapitulate the recorded wing beat behavioral results and those recorded by others for these specific stimuli when presented individually. Yet, the novelty of this study and their model is that it allows predictions for natural visual scenes in which multiple visual stimuli occur simultaneously and may have opposite or enhancing effects on behavior. Testing three models that would allow interactions of these visual modalities, the authors show that using a visual efference copy signal allows visual streams to interact, replicating behavior recorded when multiple stimuli are presented simultaneously. Importantly, they validated the prediction of this model in real flies using magnetically tethered flies, e.g., presenting moving bars with varying backgrounds. In conclusion, the presented manuscript presents a commendable effort in developing and demonstrating the validity of a mixture model that enables predictions of Drosophila behavior in natural visual environments.</p>
<p>The manuscript employs a thorough, logical approach, combining computational modeling with experimental behavioral validation using magnetically tethered flies. This iterative integration of simulation and empirical behavioral evidence enhances the credibility of the findings. The quantitative models and validating behavioral experiments make this a valuable contribution to the field. This study is well executed and addresses a significant gap in the modeling of fly behavior and holistic understanding of visuomotor behaviors.</p>
<p>The associated code base is well documented and readily produces all figures in the document.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93487.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The fly visual circuit and its behavioral response to simple visual stimuli have been well investigated, yet how they respond to more complex visual patterns is less understood. Canelo et al. first characterized a fly's steering to simple stimuli and examined how the combination of those stimuli impacts behavior. Combining behavioral experiments and simulation, the authors found that, for some combinations, a behavioral response can be explained by a linear summation of responses to individual stimuli. However, for looming and background motion combinations, the behavioral response to one was suppressed by the other. Furthermore, the effect was dependent on the onset timing of the pair of stimuli.</p>
<p>Strength:</p>
<p>The authors tested various visual stimulus patterns and time delays between combinations of visual stimuli and found novel interactions in behavior. Their findings support the idea that, depending on the visual context, additional mechanisms kick into the visual-motor circuit to coordinate steering behavior flexibly.</p>
<p>Weakness:</p>
<p>The manuscript does not provide conclusive evidence on the presence of an efference copy signal, though there appears to be an intention to associate it with the result. However, demonstrating it is likely to be beyond the main scope of the revised version.</p>
<p>The goal of this manuscript is to understand how the fly's steering behavior is coordinated upon complex visual stimuli, and a number of experiments and simulations support their conclusion.</p>
<p>The behavioral findings presented in this paper will be helpful in further dissecting the underlying neural mechanisms of contextual sensory processing and in understanding visual processing in other species.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93487.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Canelo et al. used a combination of mathematical modeling and behavioral experiments to ask how flies orient to visual features and stabilize their gaze. In particular, the authors propose three models of visuomotor control, which lead to specific experimental predictions. With the goal of teasing out the suggested models, the authors design three flight experiments: 1) a bar-background experiment, 2) a looming-background experiment, and 3) a bar-background statistics experiment. The authors claim that: experiment 1 data favor the addition-only and graded EC model; experiment 2 data favor the all-or-none EC model; experiment 3 appears to suggest a graded EC model.</p>
<p>While the study is interesting, there are major issues with the conceptual framework. In general, there is a major disconnect between model and animal data. The manuscript lacks a statistical framework to support or refute the proposed models. In the end, it is unclear what are the main conclusions of the manuscript and contributions to the field.</p>
<p>Strengths:</p>
<p>They ask a significant question related to efference copies during volitional movement.</p>
<p>The figures are overall clear and salient.</p>
<p>Weaknesses:</p>
<p>Comparison of model to fly data:</p>
<p>
In general, the manuscript suffers from a lack of quantitative comparisons between proposed models and fly data, which compromises the main findings of the work. While Figure 1-Fig. supplement 1 shows a direct comparison between experiment and model predictions, puzzlingly there is no such quantitative comparison in the main manuscript for the faster moving stimuli. Please overlay model predictions and experimental data and provide statistical comparisons throughout. The 3 proposed models are hypotheses, but there is no statistical framework to reject or support the models/hypotheses. Further, there is a disconnect between the new flight experiments and models. In fact, we do not see the model predictions for the set of experimental conditions tested in Figs. 5-7.</p>
<p>Concerns about mechanical model: I have several concerns regarding the biomechanics block in Figure 2:</p>
<p>(1) The inertia coefficient, derived from free flight studies. does not take into account the fact that the center of rotation and center of mass do not align in the magnetic tether (see Bender &amp; Dickinson, 2006 for estimates). This must be corrected using the parallel axis theorem. As the authors compare the model prediction to experimental data in a magnetic tether, it is critical that they revise their analysis.</p>
<p>(2) According to their chosen inertia and damping constants, they would estimate that the I/C time constant is ~1E-3 ms, which is much much smaller than what has been estimated for yaw turns in the magnetic tether (200 ms; Bender &amp; Dickinson, 2006) or free flight saccades (~17 ms; see Cheng et al., 2010; 10.1242/jeb.038778). The bottom line is that the current model underestimates the influence of inertia in turn manoeuvres, i.e. the aerodynamic damping is cranked up too high relative to yaw inertia. This may explain the mismatch between data and model that the authors posit, &quot;What causes the fly to undershoot the movement of the target object in the magnetically tethered assay? One hypothesis is that strong upward magnetic force or a blunt top end of the steel pin significantly dampens the flies' flight turns.&quot;</p>
<p>Loom response experiment:</p>
<p>
As nicely shown by 10.1242/jeb.02369, visual stimulation of looming stimuli in the magnetic tether evokes saccades. Is it the case as well in Fig. 6? Without showing individual trials, it is not possible to know whether this is the case. If indeed saccades are present, then the authors will have to reframe their results given the physiological evidence for saccade-related cancellation signals and the three proposed models.</p>
<p>Minor comments:</p>
<p>Missing Equation 13 for saccade model in Methods.</p>
<p>For the discussion and results related to flight responses to the mismatch between expected and actual visual feedback, which is germane to the proposed models, the authors should integrate a discussion of a recent paper which directly tested this idea through an augmented reality system: 10.1016/j.cub.2023.11.045. In particular, the authors argue that the optomotor response is not particularly flexible because it may not rely on an internal model, as suggested by recent physiological evidence (Fenk et al.). How do these findings relate to the 3 proposed models within your work?</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93487.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Canelo</surname>
<given-names>Angel</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0006-7642-4055</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Kim</surname>
<given-names>Hyosun</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kim</surname>
<given-names>Yeon</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Park</surname>
<given-names>Jeongmin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kim</surname>
<given-names>Anmo J</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6681-1437</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>The manuscript &quot;Drosophila Visuomotor Integration: An Integrative Model and Behavioral Evidence of Visual Efference Copy&quot; provides an integrative model of the visuomotor control in Drosophila melanogaster. This model presents an experimentally derived model based on visually evoked wingbeat pattern recordings of three strategically selected visual stimulus types with well-established behavioral response characteristics. By testing variations of these models, the authors demonstrate that the virtual model behavior can recapitulate the recorded wing beat behavioral results and those recorded by others for these specific stimuli when presented individually. Yet, the novelty of this study and their model is that it allows predictions for natural visual scenes in which multiple visual stimuli occur simultaneously and may have opposite or enhancing effects on behavior. Testing three models that would allow interactions of these visual modalities, the authors show that using a visual efference copy signal allows visual streams to interact, replicating behavior recorded when multiple stimuli are presented simultaneously. Importantly, they validated the prediction of this model in real flies using magnetically tethered flies, e.g., presenting moving bars with varying backgrounds. In conclusion, the presented manuscript presents a commendable effort in developing and demonstrating the validity of a mixture model that allows predictions of the behavior of Drosophila in natural visual environments.</p>
<p>Strengths:</p>
<p>Overall, the manuscript is well-structured and clear in its presentation, and the modeling and experimental research are methodically conducted and illustrated in visually appealing and easy-to-understand figures and their captions.</p>
<p>The manuscript employs a thorough, logical approach, combining computational modeling with experimental behavioral validation using magnetically tethered flies. This iterative integration of simulation and empirical behavioral evidence enhances the credibility of the findings.</p>
<p>The associated code base is well documented and readily produces all figures in the document.</p>
<p>Suggestions:</p>
<p>However, while the experiments provide evidence for the use of a visual efference copy, the manuscript would be even more impressive if it presented specific predictions for the neural implementation or even neurophysiological data to support this model. Or, at the very least, a thorough discussion. Nonetheless, these models and validating behavioral experiments make this a valuable contribution to the field; it is well executed and addresses a significant gap in the modeling of fly behavior and holistic understanding of visuomotor behaviors.</p>
</disp-quote>
<p>We appreciate the reviewer’s thoughtful comments on the strengths and weaknesses of our manuscript. We agree that biophysically realistic model reflecting the structure of neural circuits as well as physiological data from them would be invaluable. However, we are currently unable to provide physiological evidence for EC-based suppression, nor provide circuit architecture for efference copy-based suppression of the stability circuit because the neural pathway underlying this behavior remains unidentified. Extensive recordings from the HS/VS system have revealed cell-type-specific motor-related inputs during both spontaneous and loom-evoked flight turns (Fenk et al., 2021; Kim et al., 2017, 2015). These studies predicted suppression of the optomotor stability response during such turns, and our new experiments confirmed this suppression specifically during loom-evoked turns (Figures 5, 6). However, these neurons are primarily involved in the head optomotor response, not the body optomotor response. We hope to extend our current model in future studies to incorporate more cellular-level detail, as the feedforward circuits underlying stability behavior become more clearly defined.</p>
<disp-quote content-type="editor-comment">
<p>Here are a few points that should be addressed:</p>
<p>(1) The biomechanics block (Figure 2) should be elaborated on, to explain its relevance to behavior and relation to the underlying neural mechanisms.</p>
</disp-quote>
<p>We appreciate this suggestion. The mathematical representation of the biomechanics block has been developed by other groups in previous studies (Fry et al., 2003; Ristroph et al., 2010). We used exactly the same model, and its parameters were identical to those used in one of those studies (Fry et al., 2003; Ristroph et al., 2010), in which the parameters were estimated from the stabilizing response in response to magnetic “stumbling” pulses. In the previous version of the manuscript, we had a description of the biomechanics block in the Method section (see Equation 4). In response to the reviewer’s comment, we have made a few changes in Figure 2A and expanded the associated description in the main text, as follows.</p>
<p>(Line 160) “To test the orientation behavior of the model, we developed an expanded model, termed “virtual fly model” hereafter. In this model, we added a biomechanics block that transforms the torque response of the fly to the actual heading change according to kinematic parameters estimated previously (Michael H Dickinson, 2005; Ristroph et al., 2010) (Figure 2A, see Equation 4 in Methods and Movie S1). The virtual fly model, featuring position and velocity blocks that are conditioned on the type of the visual pattern, can now change its body orientation, simulating the visual orientation behavior of flies in the free flight condition.”</p>
<disp-quote content-type="editor-comment">
<p>(2) It is unclear how the three integrative models with different strategies were chosen or what relevance they have to neural implementation. This should be explained and/or addressed.</p>
</disp-quote>
<p>Thank you for this valuable comment. We selected the three models based on previous studies investigating visuomotor integration across multiple species, under conditions where multiple sensory cues are presented simultaneously.</p>
<p>The addition-only model represents the simplest hypothesis, analogous to the “additive model” proposed by Tom Collett in his 1980 study (Collett, 1980). We used this model as a baseline to illustrate behavior in the absence of any efference copy mechanism. Notably, some modeling studies have proposed linear (additive) integration for multimodal sensory cues at the behavioral level (Liu et al., 2023; Van der Stoep et al., 2021). However, experimental evidence demonstrating strictly linear integration—either behaviorally or physiologically—remains limited. In our study, new data (Figure 5) show that bar-evoked and background movement-evoked locomotor responses are combined linearly, supporting the addition-only model.</p>
<p>The graded efference copy model has been most clearly demonstrated in the cerebellum-like circuit of Mormyrid fish during electrosensation (Bell, 1981; Kennedy et al., 2014). In this system, the efference copy signal forms a negative image of the predicted reafferent input and undergoes plastic changes as the environment changes—an idea that inspired our modifiable efference copy model (Figure 4–figure supplement 1). The all-or-none efference copy model is exemplified in the sensory systems of smaller organisms, such as the auditory neurons of crickets during stridulation (Poulet and Hedwig, 2006). Notably, in crickets, the motor-related input is referred to as corollary discharge rather than efference copy. Typically, “efference copy” refers to a graded, subtractive motor-related signal, while “corollary discharge” denotes an all-or-none signal, both counteracting the sensory consequences of self-generated actions. In this manuscript, we use the term efference copy more broadly, encompassing both types of motor-related feedback signals (Sommer and Wurtz, 2008).</p>
<p>In response to this comment, we have made the following changes in the main text to enhance its accessibility to general readers.</p>
<p>(Line#268) “This integration problem has been studied across animal sensory systems, typically by analyzing motor-related signals observed in sensory neurons (Bell, 1981; Collett, 1980; Kim et al., 2017; Poulet and Hedwig, 2006). Building on the results of these studies, we developed three integrative models. The first model, termed the “addition-only model”, assumes that the outputs of the object (bar) and the background (grating) response circuits are summed to control the flight orientation (Figure 4B, see Equation 14 in Methods).”</p>
<p>(Line#272) “In the second and third models, an EC is used to set priorities between different visuomotor circuits (Figure 4C,D). In particular, the EC is derived from the object-induced motor command and sent to the object response system to nullify visual input associated with the object-evoked turn (Bell, 1981; Collett, 1980; Poulet and Hedwig, 2006). These motor-related inputs fully suppress sensory processing in some systems (Poulet and Hedwig, 2006), whereas in others they selectively counteract only the undesirable components of the sensory feedback (Bell, 1981; Kennedy et al., 2014).”</p>
<disp-quote content-type="editor-comment">
<p>(3) There should be a discussion of how the visual efference could be represented in the biological model and an evaluation of the plausibility and alternatives.</p>
</disp-quote>
<p>Thank you for this helpful comment. We have now added the following discussion to share our perspective on the circuit-level implementation of the visual efference copy in Drosophila.</p>
<p>(Line#481) “Efference copy in Drosophila vision</p>
<p>Under natural conditions, various visual features in the environment may concurrently activate multiple motor programs. Because these may interfere with one another, it is crucial for the central brain to coordinate between the motor signals originating from different sensory circuits. Among such coordination mechanisms, the EC mechanisms were hypothesized to counteract so-called reafferent visual input, those caused specifically by self-movement (Collett, 1980; von Holst and Mittelstaedt, 1950). Recent studies reported such EC-like signals in Drosophila visual neurons during spontaneous as well as loom-evoked flight turns (Fenk et al., 2021; Kim et al., 2017, 2015). One type of EC-like signals were identified in a group of wide-field visual motion-sensing neurons that were shown to control the neck movement for the gaze stability (Kim et al., 2017). The EC-like signals in these cells were bidirectional depending on the direction of flight turns, and their amplitudes were quantitatively tuned to those of the expected visual input across cell types. Although amplitude varies among cell types, it remains inconclusive whether it also varies within a given cell type to match the amplitude of expected visual feedback, thereby implementing the graded EC signal. A more recent study examined EC-like signal amplitude in the same visual neurons for loom-evoked turns, across events (Fenk et al., 2021). Although the result showed a strong correlation between wing response and the EC-like inputs, the authors pointed that this apparent correlation could stem from noisy measurement of all-or-none motor-related inputs.</p>
<p>Thus, these studies did not completely disambiguate between graded vs. all-or-none EC signaling. Another type of EC-like signals observed in the visual circuit tuned to a moving spot exhibited characteristics consistent with all-or-none EC. That is, it entirely suppressed visual signaling, irrespective of the direction of the self-generated turn (Kim et al., 2015; Turner et al., 2022).</p>
<p>Efference-copy (EC)–like signals have been reported in several Drosophila visual circuits, yet their behavioral role remains unclear. Indirect evidence comes from a behavioral study showing that the dynamics of spontaneously generated flight turns were unaffected by unexpected background motion (Bender and Dickinson, 2006a). Likewise, our behavioral experiments showed that, during loom-evoked turns, responses to background motion are suppressed in an all-or-none manner (Figures 6 and 7). Consistent with this, motor-related inputs recorded in visual neurons exhibit nearly identical dynamics during spontaneous and loom-evoked turns (Fenk et al., 2021). Together, these behavioral and physiological parallels support the idea that a common efference-copy mechanism operates during both spontaneous and loom-evoked flight turns.</p>
<p>Unlike loom-evoked turns, bar-evoked turn dynamics changed in the presence of moving backgrounds (Figure 5), a result compatible with both the addition-only and graded EC models. However, when the static background was updated just before a bar-evoked turn—thereby altering the amplitude of optic flow—the turn dynamics remained unaffected (Figures 5 and 7), clearly contradicting the addition-only model. Thus, the graded EC model is the only one consistent with both findings. If a graded EC mechanism were truly at work, however, an unexpected background change should have modified turn dynamics because of the mismatch between expected and actual visual feedback (Figure 4–figure supplement 1)—yet we detected no such effect at any time scale examined (Figure 7–figure supplement 1). This mismatch would be ignored only if the amplitude of the graded EC adapted to environmental changes almost instantaneously—a mechanism that seems improbable given the limited computational capacity of the Drosophila brain. In electric fish, for example, comparable adjustments take more than 10 minutes (Bell, 1981; Muller et al., 2019). Further investigation is needed to clarify how reorienting flies ignore optic flow generated by static backgrounds, potentially by engaging EC mechanisms not captured by the models tested in this study.</p>
<p>Why would Drosophila rely on the all-or-none EC mechanism instead of the graded one for loom-evoked turns? A graded EC must be adjusted adaptively depending on the environment, as the amplitude of visual feedback varies with both the dynamics of self-generated movement and environmental conditions (e.g., empty vs. cluttered visual backgrounds) (Figure 4—figure supplement 1). Recent studies on electric fish have suggested that a large array of neurons in a multi-layer network is crucial for generating a modifiable efference copy signal matched to the current environment (Muller et al., 2019). Given their small-sized brain, flies might opt for a more economical design for suppressing unwanted visual inputs regardless of the visual environment. Circuits mediating such a type of EC were identified in the cricket auditory system during stridulation (Poulet and Hedwig, 2006), for example. Our study strongly suggests the existence of a similar circuit in the Drosophila visual system.</p>
<p>We tested the hypothesis that efference-copy (EC) signals guide action selection by suppressing specific visuomotor reflexes when multiple visual features compete. An alternative motif with a similar function is mutual inhibition between motor pathways (Edwards, 1991; Mysore and Kothari, 2020). In Drosophila, descending neurons form dense lateral connections (Braun et al., 2024), offering a substrate for such competitive interactions. Determining whether—and how—EC and mutual inhibition operate will require recordings from the neurons that ensure visual stability, which remain unidentified. Mapping these pathways and assessing how they are modulated by visual and behavioral context are important goals for future work.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>It has been widely proposed that the neural circuit uses a copy of motor command, an efference copy, to cancel out self-generated sensory stimuli so that intended movement is not disturbed by the reafferent sensory inputs. However, how quantitatively such an efference copy suppresses sensory inputs is unknown. Here, Canelo et al. tried to demonstrate that an efference copy operates in an all-or-none manner and that its amplitude is independent of the amplitude of the sensory signal to be suppressed. Understanding the nature of such an efference copy is important because animals generally move during sensory processing, and the movement would devastatingly distort that without a proper correction. The manuscript is concise and written very clearly. However, experiments do not directly demonstrate if the animal indeed uses an efference copy in the presented visual paradigms and if such a signal is indeed non-scaled. As it is, it is not clear if the suppression of behavioral response to the visual background is due to the act of an efference copy (a copy of motor command) or due to an alternative, more global inhibitory mechanism, such as feedforward inhibition at the sensory level or attentional modulation. To directly uncover the nature of an efference copy, physiological experiments are necessary. If that is technically challenging, it requires finding a behavioral signature that unambiguously reports a (copy of) motor command and quantifying the nature of that behavior.</p>
</disp-quote>
<p>We thank the reviewer for this insightful and constructive comment. We agree that our current behavioral evidence does not directly identify the underlying circuit mechanism, and that direct recordings from visual neurons modulated by an efference copy would be critical for distinguishing between potential mechanisms.</p>
<p>A prerequisite for such physiological investigations would be the identification of both (1) the feedforward neurons directly involved in the optomotor response, and (2) the neurons conveying motor-related signals to the optomotor circuit. Despite efforts by several research groups, the location of the feedforward circuit mediating the optomotor response remains elusive. This limitation has prevented us from obtaining direct cellular evidence of flight turn-associated suppression of optomotor signaling.</p>
<p>In light of the reviewer’s suggestion, we expanded our investigation to strengthen the behavioral evidence for efference copy (EC) mechanisms. In addition to our earlier experiments involving unexpected changes in the static background, we examined how object-evoked flight turns influence the optomotor stability reflex and vice versa (Figures 5 and 6). To quantify the interaction between different visuomotor behaviors, we systematically varied the temporal relationship between two types of visual motion—loom versus moving background, or moving bar versus moving background—and measured the resulting behavioral responses.</p>
<p>Our findings support pattern- and time-specific suppressive mechanisms acting between flight turns associated with the different visual patterns. Specifically:</p>
<p>The responses to a moving bar and a moving background add linearly, even when presented in close temporal proximity.</p>
<p>Loom-evoked turns and the optomotor stability reflex mutually suppress each other in a time-specific manner.</p>
<p>For both loom- and moving bar-evoked flight turns, changes in the static background had no measurable effect on the dynamics of the object-evoked responses.</p>
<p>These results provide a detailed behavioral characterization of a suppressive interaction between distinct visuomotor responses. This, in turn, offers correlative evidence supporting the involvement of an efference copy-like mechanism acting on the visual system. While similar efference copy mechanisms have been documented in other parts of the visual system, we acknowledge that our findings do not exclude alternative explanations. In particular, it is still possible that lateral inhibition within the central brain or ventral nerve cord contributes to the suppression we observed.</p>
<p>Ultimately, definitive proof will require identifying the specific neurons that convey efference copy signals and demonstrating that silencing these neurons abolishes the behavioral suppression. Until such experiments are feasible, our behavioral approach provides an important contribution toward understanding the nature of sensorimotor integration in this system.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>Canelo et al. used a combination of mathematical modeling and behavioral experiments to ask whether flies use an all-or-none EC model or a graded EC model (in which the turn amplitude is modulated by wide-field optic flow). Particularly, the authors focus on the bar-ground discrimination problem, which has received significant attention in flies over the last 50-60 years. First, they use a model by Poggio and Reichardt to model flight response to moving small-field bars and spots and wide-field gratings. They then simulate this model and compare simulation results to flight responses in a yaw-free tether and find generally good agreement. They then ask how flies may do bar-background discrimination (i.e. complex visual environment) and invoke different EC models and an additive model (balancing torque production due to background and bar movement). Using behavioral experiments and simulation supports the notion that flies use an all-or-none EC since flight turns are not influenced by the background optic flow. While the study is interesting, there are major issues with the conceptual framework.</p>
<p>Strengths:</p>
<p>They ask a significant question related to efference copies during volitional movement.</p>
<p>The methods are well detailed and the data (and statistics) are presented clearly.</p>
<p>The integration of behavioral experiments and mathematical modeling of flight behavior.</p>
<p>The figures are overall very clear and salient.</p>
<p>Weaknesses:</p>
<p>Omission of saccades: While the authors ask a significant question related to the mechanism of bar-ground discrimination, they fail to integrate an essential component of the Drosophila visuomotor responses: saccades. Indeed, the Poggio and Reichardt model, which was developed almost 50 years ago, while appropriate to study body-fixed flight, has a severe limitation: it does not consider saccades. The authors identify this major issue in the Discussion by citing a recent switched, integrate-and-fire model (Mongeau &amp; Frye, 2017). The authors admit that they &quot;approximated&quot; this model as a smooth pursuit movement. However, I disagree that it is an approximation; rather it is an omission of a motor program that is critical for volitional visuomotor behavior. Indeed, saccades are the main strategy by which Drosophila turn in free flight and prior to landing on an object (i.e. akin to a bar), as reported by the Dickinson group (Censi et al., van Breugel &amp; Dickinson [not cited]). Flies appear to solve the bar-ground discrimination problem by switching between smooth movement and saccades (Mongeau &amp; Frye, 2017; Mongeau et al., 2019 [not cited]). Thus, ignoring saccades is a major issue with the current study as it makes their model disconnected from flight behavior, which has been studied in a more natural context since the work of Poggio.</p>
</disp-quote>
<p>Thank you for this helpful comment. We agree that including saccadic turns is essential and qualitatively improves the model. In the revised manuscript, we therefore expanded our bar-tracking model to incorporate an integrate-and-saccade strategy, now presented in Figure 2—figure supplement</p>
<p>The manuscript now introduces this result as follows:</p>
<p>(Line#190) “Finally, one important locomotion dynamics that a flying Drosophila exhibits while tracking an object is a rapid orientation change, called a “saccade” (Breugel and Dickinson, 2012; Censi et al., 2013; Heisenberg and Wolf, 1979). For example, while tracking a slowly moving bar, flies perform relatively straight flights interspersed with saccadic flight turns (Collett and Land, 1975; Mongeau and Frye, 2017). During this behavior, it has been proposed that visual circuits compute an integrated error of the bar position with respect to the frontal midline and triggers a saccadic turn toward the bar when the integrated value reaches a threshold (Frighetto and Frye, 2023; Mongeau et al., 2019; Mongeau and Frye, 2017). We expanded our bar fixation model to incorporate this behavioral strategy (Figure 2--figure supplement 2). The overall structure of the modified model is akin to the one proposed in a previous study (Mongeau and Frye, 2017), and the amplitude of a saccadic turn was determined by the sum of the position and velocity functions (Figure 2--figure supplement 2A; see Equation 13 in Methods). When simulated, our model successfully reproduced experimental observations of saccade dynamics across different object velocities (Figure 2--figure supplement 2B-D) (Mongeau and Frye, 2017). Together, our models faithfully recapitulated the results of previous behavioral observations in response to singly presented visual patterns (Collett, 1980; Götz, 1987; H. Kim et al., 2023; Maimon et al., 2008; Mongeau and Frye, 2017).”</p>
<p>Apart from Figures 1 and 2, most of our data—whether from simulations or behavioral experiments—use brief visual patterns lasting 200 ms or less. These stimuli trigger a single, rapid orientation change reminiscent of a saccadic flight turn. In this part of the paper, we essentially have examined how multiple visuomotor pathways interact to determine the direction of object-evoked turns when several visual patterns occur simultaneously.</p>
<disp-quote content-type="editor-comment">
<p>Critically, recent work showed that a group of columnar neurons (T3) appear specialized for saccadic bar tracking through integrate-and-fire computations, supporting the notion of parallel visual circuits for saccades and smooth movement (Frighetto &amp; Frye, 2023 [not cited]).</p>
</disp-quote>
<p>Thanks for bringing up this critical issue. We have now added this paper in the following part of the manuscript.</p>
<p>(Line#193) “During this behavior, it has been proposed that visual circuits compute an integrated error of the horizontal bar position with respect to the frontal midline and triggers a saccadic turn toward the bar when the integrated value reaches a threshold (Frighetto and Frye, 2023; Mongeau and Frye, 2017).”</p>
<p>(Line#462) “Visual systems extract features from the environment by calculating spatiotemporal relationships of neural activities within an array of photoreceptors. In Drosophila, these calculations occur initially on a local scale in the peripheral layers of the optic lobe (Frighetto and Frye, 2023; Gruntman et al., 2018; Ketkar et al., 2020).”</p>
<disp-quote content-type="editor-comment">
<p>A major theme of this work is bar fixation, yet recent work showed that in the presence of proprioceptive feedback, flies do not actually center a bar (Rimniceanu &amp; Frye, 2023). Furthermore, the same study found that yaw-free flies do not smoothly track bars but instead generate saccades. Thus prior work is in direct conflict with the work here. This is a major issue that requires more engagement by the authors.</p>
</disp-quote>
<p>Thank you for your thoughtful comments and for drawing our attention to this important paper. In our experiments, bar fixation on oscillating vertical objects emerges during the “alignment” phase of the magneto-tether protocol. The pattern movement dynamics was similar those used by Rimniceanu &amp; Frye (2023), yet the two studies differ in a key respect: Rimniceanu &amp; Frye employed a motion-defined bar, whereas we presented a dark vertical bar against a uniform or random-dot background. The alignment success rate—defined as the proportion of trials in which the fly’s body angle is within ±25° of the target—was about 50 % (data not shown). Our alignment pattern consisted of three vertical stripes spanning ~40° horizontally; when we replaced it with a single, narrower stripe, the success rate was lowered (data not shown). These observations suggest that bar fixation in the magnetically tethered assay is less robust than in the rigid-tethered assay, although flies still orient toward highly salient vertical objects.</p>
<p>We also observed that bar-evoked turns were elicited more reliably when the bar moved rapidly (45° in 200 ms) in the magneto-tether assay, although the turn magnitude was significantly smaller than the actual bar displacement (Figure 3).</p>
<p>In response to the reviewer’s comment, we now added the following description in the paper regarding the bar fixation behavior, citing Rimniceanu&amp;Frye 2023.</p>
<p>(Line#239) “Another potential explanation arises from recent studies demonstrating that proprioceptive feedback provided during flight turns in a magnetically tethered assay strongly dampens the amplitude of wing and head responses (Cellini and Mongeau, 2022; Rimniceanu et al., 2023).”</p>
<disp-quote content-type="editor-comment">
<p>Relevance of the EC model: EC-related studies by the authors linked cancellation signals to saccades (Kim et al, 2014 &amp; 2017). Puzzlingly, the authors applied an EC model to smooth movement, when the authors' own work showed that smooth course stabilizing flight turns do not receive cancellation signals (Fenk et al., 2021). Thus, in Fig. 4C, based on the state of the field, the efference copy signal should originate from the torque commands to initiate saccades, and not from torque to generate smooth movement. As this group previously showed, cancellation signals are quantitatively tuned to that of the expected visual input during saccades. Importantly, this tuning would be to the anticipated saccadic turn optic flow. Thus the authors' results supporting an all-or-none model appear in direct conflict with the author's previous work. Further, the addition-only model is not particularly helpful as it has been already refuted by behavioral experiments (Rimneceanu &amp; Frye, Mongeau &amp; Frye).</p>
</disp-quote>
<p>Thank you for this constructive comment. Efference copy is best established for brief, discrete actions like flight saccades. While motor-related modulation of visual processing has been reported across short- and long-duration behaviours (Chiappe et al., 2010; Fujiwara et al., 2017; Kim et al., 2015, 2017; Maimon et al., 2010; Turner et al., 2022), only flight saccade-associated signals exhibit the temporal profile appropriate to cancel reafferent input. However, von Holst &amp; Mittelstaedt (1950) originally formulated efference copy to explain the smooth optomotor response of hoverflies. In HS/VS recordings in previous studies, however, we could not detect membrane-potential changes tied to baseline wing-beat amplitude (data not shown), but further work is needed.</p>
<p>Note that visually evoked flight turns analyzed in this paper have relatively fast dynamics. Fenk et al. (2021) showed that HS cells carry EC-like motor signals during both loom-evoked turns and spontaneous saccades. Building on this, we tested whether object-evoked rapid turns modulate other visuomotor pathways. Although Fenk et al. also found that optomotor turns lack motor input to HS cells, the authors did not test whether the optomotor pathway suppresses other reflexes, such as loom-evoked turns. Our new behavioral data (Figure 6) show that optomotor turns indeed suppress loom-evoked turns, suggesting a potential EC signal arising from the optomotor pathway that inhibits loom-responsive visual neurons.</p>
<p>In Kim et al. (2017), the authors argued that HS/VS neurons receive a “quantitatively tuned” efference copy that varies across cell types: yaw-sensitive LPTCs are strongly suppressed, roll-sensitive cells receive intermediate input, and pitch-sensitive cells receive little or none. We also showed that when the amplitude of ongoing visual drive changes, the amplitude of saccade-related potentials (SRPs) scales linearly. This proportionality does not imply a genuinely graded EC, however, because SRP amplitude could vary solely through changes in driving force (Vm – Vrest) with a fixed EC conductance. Crucially, SRPs do not fully suppress feed-forward visual signalling, arguing against an all-or-none EC mechanism.</p>
<p>How, then, can the cellular and behavioural data be reconciled? Silencing HS/VS neurons—or their primary inputs, the T4/T5 neurons—does not markedly diminish the optomotor response in flight (Fenk et al., 2014; Kim et al., 2017), indicating the presence of additional, as-yet-unidentified pathways.</p>
<p>Physiological recordings from other visual neurons that drive the optomotor response in flying Drosophila are therefore needed to determine how strongly they are suppressed during loom-evoked turns.</p>
<disp-quote content-type="editor-comment">
<p>Behavioral evidence for all-or-none EC model: The authors state &quot;unless the stability reflex is suppressed during the flies' object evoked turns, the turns should slow down more strongly with the dense background than the sparse one&quot;. This hypothesis is based on the fact that the optomotor response magnitude is larger with a denser background, as would be predicted by an EMD model (because there are more pixels projected onto the eye). However, based on the authors' previous work, the EC should be tuned to optic flow and thus the turning velocity (or amplitude). Thus the EC need not be directly tied to the background statistics, as they claim. For instance, I think it would be important to distinguish whether a mismatch in reafferent velocity (optic flow) links to distinct turn velocities (and thus position). This would require moving the background at different velocities (co- and anti-directionally) at the onset of bar motion. Overall, there are alternative hypotheses here that need to be discussed and more fully explored (as presented by Bender &amp; Dickinson and in work by the Maimon group).</p>
</disp-quote>
<p>We appreciate the reviewer’s important suggestion. In response, we performed the recommended experiment. In Figures 5 and 6 of the revised manuscript, we now present how bar- or loom-evoked flight turns affect the response to a moving background pattern. These experiments revealed that bar-evoked turns do not suppress the optic flow response, whereas loom-evoked turns strongly suppress it. Specifically, when background motion began 100 ms after the onset of loom expansion, the response to the background was significantly suppressed. Although weak residual responses to the background motion were observed in this case, this could be due to background motion occurring outside of the suppression interval, which may correspond in duration to the duration of flight turns (Figure 6C,D).</p>
<p>The lack of suppression of the optic flow response during and after bar-evoked turns appears to suggest that the responses are added linearly (Figure 5), seemingly contradicting the lack of dynamic change when the background dot density was altered (Figure 7, Figure 7–figure supplement 1). That is, the experimental result in Figure 5 supports either an addition-only or a graded efference copy (EC) model. However, the result in Figure 7 supports an all-or-none EC model. If a graded EC were used, the amplitude of the EC should be updated almost instantaneously when the static background changes.</p>
<p>Another possibility is that the optic flow during self-generated turns in a static background is extremely weak compared to the optic flow input generated by physically moving the pattern, perhaps due to the rapid nature of head movements. Indeed, detailed kinematic analysis of head movement during spontaneous saccades in blow flies revealed that the head reaches the target angle before the body completes the orientation change, making the effective speed of reafferent optic flow higher than the speed of body rotation (Hateren and Schilstra, 1999). To test these hypotheses, further experiments will be needed for bar-evoked flight turns.</p>
<disp-quote content-type="editor-comment">
<p><bold>Publishing the reviewed preprint:</bold></p>
<p>(1) The Reviewed Preprint (including the full text of the preprint we reviewed, the eLife assessment, and public reviews) will typically be published in two weeks' time.</p>
<p>Please let us know if you would like to provide provisional author responses to be posted at the same time (if so, please send these by email). Please do not resubmit within the next two/three weeks, as we will need to publish the first version of the Reviewed Preprint first.</p>
<p>If there are any factual errors in the eLife assessment or public reviews, or other issues we should be aware of, please let us know as soon as possible.</p>
<p>(2) After publication of the Reviewed Preprint, you can use the link below to submit a revised version. There is no deadline to resubmit. Before resubmitting, please ensure that you update the preprint at the preprint server to correspond with the revised version. Upon submitting a revised version, we will ask the editors and reviewers if it's appropriate to update their assessment and public reviews, which will be included alongside the revised Reviewed Preprint. At that time we will also post the recommendations to the authors and the author responses you provide with the revised version. In the author response, please respond to the public reviews (where relevant) and the recommendations to the authors.</p>
<p>(3) Alternatively, you can proceed with the current version of the Reviewed Preprint (once published), without revisions, and request an eLife Version of Record. See the Author Guide for further information: <ext-link ext-link-type="uri" xlink:href="https://elife-rp.msubmit.net/html/elife-rp_author_instructions.html#vor">https://elife-rp.msubmit.net/html/elife-rp_author_instructions.html#vor</ext-link>. However, most authors decide to request a Version of Record after a round of revision.</p>
<p>(4) After publication of eLife's Reviewed Preprint, you also have the option to submit/publish in another journal instead: if you choose to do this, please let us know so we can update our records.</p>
<p>The reviewers identified two key revisions that could improve the assessment of the paper:</p>
<p>(1) Consideration of saccades within the model framework (outlined by reviewer 3).</p>
<p>(2) Addition of physiology data to support the conclusions of the paper (outlined by reviewer 2). If this is not feasible within the timescale of revisions, the paper would need to be revised to clarify that the model leads to a hypothesis that would need to be tested with future physiology experiments.</p>
</disp-quote>
<p>Thank you for these comments.</p>
<p>Regarding revision point #1, we have added Figure 2–figure supplement 2, where we incorporated our position-velocity model (estimated in Figure 1) into the framework of the integrate-and-saccade model. A detailed description of this model is now provided in the main text (Lines 190–203).</p>
<p>For revision point #2, obtaining electrophysiological evidence for efference copy remains challenging, as neither the visual neurons nor the efference-copy neuron has been identified for the wing optomotor response. As suggested by the reviewers, we have revised the title of the paper to reduce emphasis on efference copy and have noted electrophysiological recordings as a direction for future work.</p>
<p>old title: A visual efference copy-based navigation algorithm in Drosophila for complex visual environments</p>
<p>new title: Integrative models of visually guided steering in Drosophila</p>
<disp-quote content-type="editor-comment">
<p>Specific recommendations are detailed below.</p>
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>To directly demonstrate if an efference copy is non-scaled, the following experiments can be helpful: record from HS/VS cells and examine the relation between the amplitude of the succade-suppression signal vs. succade amplitude.</p>
</disp-quote>
<p>Thanks for raising this important point. We previously carried out the suggested analysis for loom-evoked saccades in Fenk et al. (2021). There, significant correlations emerged between wing-response amplitude and saccade-related potentials (Figures 2F and 3C). However, we did not interpret the strong correlation (r ≈ 0.8) as evidence for a graded efference copy, because the amplitude of saccade-related potentials appeared to be bimodal. Upon presentation of the looming stimulus, flies either executed large evasive turns or showed minimal changes in wing-stroke amplitude. Large wing responses were accompanied by strong, saturated suppression of HS-cell membrane potential, whereas trials without wing responses produced only weak modulations—reflected in the bimodal distribution of saccade-related potential amplitudes (Figure 3C).</p>
<p>Importantly, in rigidly tethered preparations—where these potentials are typically measured—the absence of proprioceptive feedback can itself drive wingbeat amplitudes to saturation during saccades. We therefore reasoned that the lack of intermediate-sized flight saccades would naturally yield correspondingly saturated saccade-related potentials, even if a graded EC system is in play.</p>
<p>In Kim et al. (2017), we also performed a comprehensive analysis of spontaneous saccade-related potentials across all HS/VS cell types. When we later examined the relationship between saccade amplitude and the corresponding saccade-related potentials in each cell type, we could not find any statistically significant correlation (unpublished data).</p>
<disp-quote content-type="editor-comment">
<p>measure how much a weak visual stimulus and a strong visual stimulus are suppressed by the suppression signal. If the signal is non-scaled, visual stimuli should always be suppressed independently of their intensities.</p>
</disp-quote>
<p>Thank you for this important suggestion. As mentioned in our response to the previous comment, we believe it is not feasible to record from neurons responsible for the body optomotor response at this point, as their identity remains unknown. Regarding the HS/VS cells, our previous study showed that HS cells are not always fully suppressed. The changes in saccade-related potential amplitude can be described as a linear function of the pre-saccadic visually-evoked membrane potential (Figure 7 in Kim et al., 2017).</p>
<disp-quote content-type="editor-comment">
<p>As suggested by Fenk et al. 2014 (doi: 10.1016/j.cub.2014.10.042), HS cells might also be responsive to a moving bar. If that is the case, and if you present a bar and background (either sparse or dense) in a closed-loop manner to a head-fixed fly, HS cells might be sensitive only to the bar but not to the background (independently of the density).</p>
</disp-quote>
<p>Thanks for pointing out this important issue. HS cells indeed respond strongly to the horizontal movement of a vertical bar, as expected given that their receptive fields are formed by the integration of local optic flow vectors. In one of our previous studies (Supplemental Figure 1 in Kim et al., 2015), we showed that the response amplitude to a single vertical bar is roughly equivalent to that elicited by a vertical grating composed of 12 bars of the same size. Therefore, we believe that HS cells are likely to contribute to the head response to a moving vertical bar. In a body-fixed flight simulator, HS cells would respond only to the bar if the bar runs in a closed loop with a static background. In this scenario, HS cells are likely to play a role in the head optomotor response.</p>
<p>Note also that the role of HS cells in the wing optomotor response remains unresolved. Unilateral activation of HS cells has been shown to elicit locomotor turns in walking Drosophila (Fujiwara et al., 2017), as well as in flying individuals (unpublished data from our lab). However, a previous study also showed that strong silencing of HS/VS cells significantly reduced the head optomotor response, but not the wing optomotor response (Kim et al., 2017).</p>
<disp-quote content-type="editor-comment">
<p>If neurophysiology is technically challenging, an alternative way might pay attention to a head movement that exclusively follows the background (Fox et al., 2014 (doi: 10.1242/jeb.080192)). Because HS cells are thought to promote head rotation to background motion, a non-scaled suppression signal on HS cells would always suppress the head rotation independently of the background density.</p>
</disp-quote>
<p>Thanks for this helpful comment. We have analyzed head movements during bar-evoked flight turns (Figure 7–figure supplement 1B) and found no significant changes across different background dot densities. We think that this might suggest that HS cells are unlikely to receive suppressive inputs during bar-evoked turns, akin to the lack of modulation during optomotor turns.</p>
<disp-quote content-type="editor-comment">
<p>Another way to separate a potential efference copy from other mechanisms (more global inhibition) is the directionality. A global inhibition would suppress the response to the background even if the background moves in the same direction as self-motion, but the efference copy would not.</p>
</disp-quote>
<p>Thanks for this important point. In Heisenberg and Wolf, 1979, it was proposed that modulation might be bidirectional, with behavioral effects observed only for perturbations in the “unexpected” direction. In our new data on loom-evoked turns (Figure 6), the suppression appears equally strong for background motion in either direction, supporting an all-or-none suppression mechanism.</p>
<disp-quote content-type="editor-comment">
<p>Besides, in general, it is unclear if you think an efference copy operates both in smooth pursuits and saccades or if such a signal is only present during saccades. Your previous neurophysiological work supports the latter. Are your behavioral results consistent with the previous saccade suppression idea, or do you propose a new type of efference copy that also operates in smooth pursuits?</p>
</disp-quote>
<p>Thanks for raising this important point. von Holst and Mittelstaedt (1950) originally introduced the concept of efference copy to explain the smooth optomotor response. We previously analyzed electrophysiological recordings from HS cells for membrane-potential changes associated with slow deviations in wing-steering angle but found none. However, this negative result does not entirely rule out modulation of visual processing during smooth flight turns, given the slow drift in membrane potential observed in most whole-cell recordings.</p>
<p>In this study, We examined only the interactions among visuomotor pathways during these rapid flight turns as the dynamics of visually evoked turns are almost as rapid as spontaneous saccades. Our data reveal that interactions between distinct visuomotor reflexes are more diverse than previously appreciated.</p>
<disp-quote content-type="editor-comment">
<p>Minor comments:</p>
<p>Line 108, 109: match the description between here and the labels in Fig. 1F.</p>
</disp-quote>
<p>Thank you for indicating this issue. We have defined the general equation to obtain the position and velocity components in the main text lines 108,109, but due to a slight asymmetry in the data (Fig. 1E) we used the approach indicated in Fig. 1F. and explained in lines 113-117.</p>
<disp-quote content-type="editor-comment">
<p>Fig.1 F: If the position-dependent component is due to fatigue, the tuning curve's shape is likely changed (shrunk or extended) depending on the stimulus speed. How can you generalize the tuning curve shown here? Does the result hold even if the stimulus speed/contrast/spatial frequency is changed?</p>
</disp-quote>
<p>We appreciate this indication. We believed that fatigue may be the reason why the wing response to the grating stimulus showed that significant decay (Fig. 1E). As you mention, the stimulus speed would increase the amplitude of the fly’s response up to a saturation point. We addressed this in our model by multiplying the derived value by the angular velocity of the grating.</p>
<p>Regarding the contrast, and spatial frequency we did not test it experimentally, instead, we simulated our model for changing visual feedback (Fig. 4A, B), which can be seen as increasing/decreasing contrast of a grating. An increase in the contrast would increase the response of the fly to the grating and so will contribute to dampening the response to the foreground object (Fig. 4C).</p>
<p>Line 233-255: Here, the description sounds like you will consider several parallel objects (e.g., two stripes) in the visual field instead of the combination of the figure and background (which is referred to in the following paragraph).</p>
<p>Thank you for pointing it out. Indeed it was slightly ambiguous. We have addressed this by explaining the specific situation of a combination of an object and the background in lines 231-233.</p>
<p>Figure 6C: you kept the foreground visual field between sparse and dense random dot backgrounds to keep the bar's saliency. Is it sure that this does not influence the difference in the fly's response to these two backgrounds (in Figure 6B)?</p>
<p>This is a good point that we have also discussed internally. We also carried out similar experiments with a fully covered background and found no significant differences (Figure 7–figure supplement 1).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>Identify and analyze flight saccade dynamics in the raw trajectories (e.g., Fig. 3B). There should be some since the bar is near the 'sweet spot' for triggering saccades (see Mongeau &amp; Frye, 2017).</p>
</disp-quote>
<p>Thank you for bringing up this interesting point. In previous work, it was reported that the fly fixated on a vertical bar through saccadic turns rather than smooth-tracking (Mongeau &amp; Frye, 2017). When the bar width was thin (&lt;15 deg) there was barely one saccade per second (Mongeau &amp; Frye, 2017, Fig. 4). In our magno tether essay (Fig. 3A, B) the object width was 11.25 degrees, and the object moved for a short time window, and so the fly only generated the saccade related to the onset of the object. It could not be considered as a saccade some small turns of a few degrees that are likely related to small perturbations in comparison to those previously reported (Mongeau &amp; Frye, 2017). Additionally, in our protocol (Fig. 3A) from onset time (‘go’ mark), only a single object moved, within an empty background, so in principle there is no trigger for a switch to a smooth movement. We addressed this in lines x-x.</p>
<disp-quote content-type="editor-comment">
<p>Consider updating the Poggio model with flight saccades (switched, integrate-and-fire).</p>
</disp-quote>
<p>We appreciate this suggestion. Following previous work (Mongeau et al., 2017), we expanded our model to include a saccade mechanism: the torque produced by the summed position- and velocity-dependent components is now replaced by an integrate-and-fire saccade (Figure 2—figure supplement 2). We optimized the saccade interval and amplitude so that both vary linearly with stimulus amplitude and faithfully reproduce the kinematic properties reported previously (Mongeau et al., 2017).</p>
<disp-quote content-type="editor-comment">
<p>Please engage more with the literature, especially work that directly conflicts with your conclusions (see above). Also, highly relevant work by Bender &amp; Dickinson was not sufficiently discussed. Spot results presented in Fig. 3 should be contextualized in light of the work of Mongeau et al., 2019, who performed similar experiments and identified a switch in saccade valence.</p>
</disp-quote>
<p>We appreciate your pointing out the relevant previous work. We have added references to the following papers and tried to describe the relationship between our data and previous ones.</p>
<p>Bender &amp; Dickinson 2006</p>
<p>(Line#162) “This simulation experiment is reminiscent of the magnetically tethered flight assay, where a flying fly remains fixed at a position but is free to rotate around its yaw axis (Bender and Dickinson, 2006b; Cellini et al., 2022; G. Kim et al., 2023; Mongeau and Frye, 2017).”</p>
<p>(Line#218) “We tested the predictions of our models with flies flying in an environment similar to that used in the simulation (Figure 3A). A fly was tethered to a short steel pin positioned vertically at the center of a vertically oriented magnetic field, allowing it to rotate around its yaw axis with minimal friction (Bender and Dickinson, 2006b; Cellini et al., 2022; G. Kim et al., 2023).”</p>
<p>(Line#238) “To determine if our assay imposes additional friction compared to other assays used in previous studies, we analyzed the dynamics of spontaneous saccades during the “freeze” phase (Figure 3–figure supplement 1A). We found their duration and amplitude to be within the range reported previously (Bender and Dickinson, 2006b; Mongeau and Frye, 2017) (Figure 3–figure supplement 1B-D).</p>
<p>Mongeau et al., 2019</p>
<p>(Line#196) “During this behavior, it has been proposed that visual circuits compute an integrated error of the bar position with respect to the frontal midline and triggers a saccadic turn toward the bar when the integrated value reaches a threshold (Frighetto and Frye, 2023; Mongeau et al., 2019; Mongeau and Frye, 2017). We expanded our bar fixation model to incorporate this behavioral strategy (Figure 2–figure supplement 2).”</p>
<p>This paper shows that the dynamics of saccadic flight turns elicited by a rotating bar or spot determine whether flies display attraction or aversion. In that study, the visual stimulus—a bar or spot—rotated slowly at a constant 75 deg s⁻¹. By contrast, in our Figure 3 the object moves much faster, driving the neural “integrator” to saturation and triggering an almost immediate flight turn. In Mongeau et al. (2019), saccades occur at variable times and their amplitudes and directions are more stochastic, again reflecting the slower stimulus speed. Because these differences all arise from the disparity in object speed, we did not cite Mongeau et al. (2019) in Figure 3 or the associated text.</p>
<p>In addition to the two papers cited above, we have incorporated several relevant studies on the Drosophila visuomotor control identified through the reviewers’ insightful comments. Examples include:</p>
<p>Frighetto G, Frye MA. 2023 (Line#195, 464)</p>
<p>Rimniceanu et al., 2023 (Line#241)</p>
<p>Cellini &amp; Mongeau 2020 (Line#91)</p>
<p>Cellini &amp; Mongeau 2022 (Line#241)</p>
<p>Cellini et al., 2022 (LIne#91, 162, 218)</p>
<disp-quote content-type="editor-comment">
<p>Many citations are not in the proper format (e.g. using numbers rather than authors' last name).</p>
</disp-quote>
<p>Thank you for letting us know. We have changed the remaining citations to the proper format.</p>
</body>
</sub-article>
</article>