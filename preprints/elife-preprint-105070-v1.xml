<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">105070</article-id>
<article-id pub-id-type="doi">10.7554/eLife.105070</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.105070.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Real-Time Closed-Loop Feedback System For Mouse Mesoscale Cortical Signal And Movement Control: CLoPy</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6814-2812</contrib-id>
<name>
<surname>Gupta</surname>
<given-names>Pankaj K</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0093-4490</contrib-id>
<name>
<surname>Murphy</surname>
<given-names>Timothy H</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>thmurphy@mail.ubc.ca</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03rmrcq20</institution-id><institution>Department of Psychiatry, University of British Columbia</institution></institution-wrap>, <city>Vancouver</city>, <country>Canada</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03rmrcq20</institution-id><institution>Djavad Mowafaghian Centre for Brain Health (DMCBH), University of British Columbia</institution></institution-wrap>, <city>Vancouver</city>, <country>Canada</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Gallego</surname>
<given-names>Juan Alvaro</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Imperial College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>King</surname>
<given-names>Andrew J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2025-01-06">
<day>06</day>
<month>01</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP105070</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-11-14">
<day>14</day>
<month>11</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-11-06">
<day>06</day>
<month>11</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.11.02.619716"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Gupta &amp; Murphy</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Gupta &amp; Murphy</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-105070-v1.pdf"/>
<abstract>
<title>Summary</title><p>We present the implementation and efficacy of an open-source closed-loop neurofeedback (CLNF) and closed-loop movement feedback (CLMF) system. In CLNF, we measure mm-scale cortical mesoscale activity with GCaMP6s and provide graded auditory feedback (within ∼50 ms) based on changes in dorsal-cortical activation within regions of interest (ROI) and with a specified rule. Single or dual ROIs (ROI1, ROI2) on the dorsal cortical map were selected as targets. Both motor and sensory regions supported closed-loop training in male and female mice. Mice modulated activity in rule-specific target cortical ROIs to get increasing rewards over days (RM ANOVA p=2.83e-5) and adapted to changes in ROI rules (RM ANOVA p=8.3e-10, <xref rid="tbl4" ref-type="table">Table 4</xref> for different rule changes). In CLMF, feedback was based on tracking a specified body movement, and rewards were generated when the behavior reached a threshold. For movement training, the group that received graded auditory feedback performed significantly better (RM-ANOVA p=9.6e-7) than a control group (RM-ANOVA p=0.49) within four training days. Additionally, mice can learn a change in task rule from left forelimb to right forelimb within a day, after a brief performance drop on day 5. Offline analysis of neural data and behavioral tracking revealed changes in the overall distribution of ΔF/F<sub>0</sub> values in CLNF and body-part speed values in CLMF experiments. Increased CLMF performance was accompanied by a decrease in task latency and cortical ΔF/F<sub>0</sub> amplitude during the task, indicating lower cortical activation as the task gets more familiar.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/pankajkgupta/clopy">https://github.com/pankajkgupta/clopy</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.20383/102.0400">https://doi.org/10.20383/102.0400</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Most investigations study brain activity and behavior as separate channels that do not interact in real time. Assessments are typically made post hoc, and experimental contingencies are not dependent on regional brain activity fluctuations. In contrast, closed-loop brain stimulation/manipulation requires a continuous dialogue between the brain, subject, and goal-directed outcome (<xref ref-type="bibr" rid="c1">1</xref>). The concept of closed-loop feedback has been around for nearly 50 years, with the pioneering work of (<xref ref-type="bibr" rid="c2">2</xref>). Surprisingly, the application of closed-loop methods has been relatively limited in the rodent literature (<xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c11">11</xref>). Some of these reasons are rooted in technical (high-dimensional data, real-time, low latency) and logistical challenges (compactness, portability, device interface), while others are related to the inherent complexity of the behavior and neurophysiology of rodents (undesired movements, neural and behavioral metrics for evaluation). An even smaller subset of papers employ modern genetically encoded sensor mesoscopic imaging and closed-loop manipulation as we do (<xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref>). Animal models offer the ability to optimize how and where brain activity is monitored and by what mechanism to best produce the closed-loop feedback.</p>
<p>We present the implementation and efficacy of a closed-loop feedback system in head-fixed mice (<xref rid="fig1" ref-type="fig">Figure 1</xref>), employing two types of graded feedback: 1) Closed-loop neurofeedback (CLNF), where feedback is derived from neuronal activity, and 2) Closed-loop movement feedback (CLMF), where feedback is based on observed body movement. These approaches provide a foundational understanding of the potential of closed-loop feedback systems. We term this new Python-based platform for Closed-Loop Feedback Training System CLoPy and provide all software, hardware schematics, and protocols to adapt it to various experimental scenarios.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Schematic of the Closed-Loop Feedback Training System (CLoPy) for Neurofeedback and Specified Movement Feedback.</title><p>A) The components of CLoPy are presented in a block diagram. Modular components such as the configuration file, camera factory, audio tone generator, and reward delivery system are displayed and are utilized by both the Closed-Loop Neurofeedback (CLNF) and Closed-Loop Movement Feedback (CLMF) systems. The configuration file (config.ini) stored all configurable parameters of the system, including camera settings, feedback parameters, reward thresholds, number of trials, and the duration of trial and rest periods, under an experiment-specific section. The camera factory was an abstract class that provided a unified interface for a programmable camera to the CLNF and CLMF systems. This abstraction allowed the core system to remain independent of the specific camera libraries required for image streaming. Camera-specific routines were implemented in separate “brain_camera_stream” or “behavior_camera_stream” classes, which inherited functions from the “camera_factory” superclass and ran in independent thread processes. The Region of Interest (ROI) manager was used by the CLNF core to maintain a list of ROIs, as well as routines to perform rule-specific operations on them, as specified in config.ini. An ROI could be defined as a rectangle (with the upper-left corner coordinates, height, and width) or as a circle (with center coordinates and radius). The audio tone generator mapped the target activity (ΔF/F<sub>0</sub> in CLNF, and control-point speed in CLMF) to graded audio tone frequencies. It generated audio signals at 44.1 kHz sampling based on the specified frequency and sent the signal to the audio output. Reward delivery was controlled by opening a solenoid valve for a specified duration, which was managed in a separate process thread. The CLNF core was the main program responsible for running the CLNF system. It utilized config.ini, the camera factory, the ROI manager, and integrated the audio tone generator and reward delivery functions. The system also saved the recorded data and configuration parameters with unique identifiers. The CLMF core, similarly, was the primary program responsible for operating the CLMF system. It utilized config.ini, the camera factory, and DeepLabCut-Live, integrating the audio tone generator and reward delivery functions. This module also saved the data and configuration parameters with unique identifiers.</p></caption>
<graphic xlink:href="619716v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2">
<title>Results</title>
<p>Current work and previous studies have shown that mice can achieve volitional control of brain activity aided by the representation of brain activity as an external variable through feedback (<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>). Mice can learn these tasks robustly, and interestingly, can adapt to changes in the task rules. While brain activity can be controlled through feedback, other variables such as movements have been less studied, in part because their analysis in real time is more challenging. Our goal has been to deliver a robust, cross-platform, and cost-effective platform for closed-loop feedback experiments. We have designed and tested a behavioral paradigm where head-fixed mice learn an association between their cortical or behavioral activity, external feedback, and rewards. We tested our CLNF system on Raspberry Pi for its compactness, general-purpose input/output (GPIO) programmability, and wide community support, while the CLMF system was tested on an Nvidia Jetson GPU device. While our investigations center around mesoscale cortical imaging of genetically encoded calcium sensors or behavior imaging, the approach could be adapted to any video or microscopy-dependent signal where relative changes in brightness or keypoint behavior are observed and feedback is given based on a specific rule. This system benefits from advances in pose estimation (<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>) and employs strategies to improve real-time processing of pre-defined keypoints on compact computers such as the Nvidia Jetson. We have constructed a software and hardware-based platform built around open-source components.</p>
<p>To examine the performance of the closed-loop system, we used water-restricted adult transgenic mice that expressed GCaMP6s widely in the cortex (details in the methods section and in <xref rid="tbl4" ref-type="table">Table 4</xref> and <xref rid="tbl5" ref-type="table">Table 5</xref>). Using the transcranial window imaging technique, we assessed the ability of these animals to control brain regions of interest and obtain water rewards. After an initial period of habituation to manual head fixation, mice were switched to closed-loop task training. In training, we had two primary groups: 1) a single cortical ROI linked to water reward and the subject of auditory feedback (Animation 1A, 1B, 1C), or 2) a pairing between two cortical ROIs where auditory feedback and water rewards were given based on the difference in activity between sites (Animation 1D, 1E, 1F). Our results indicated that both training paradigms were able to lead mice to obtain a significantly larger number of rewards over time.</p>
<p>For neurofeedback-CLNF, we calculate a ΔF/F<sub>0</sub> value (GCAMP signal) which represents a relative change of intensity in the region(s) of interest. These calculations are made in near real-time on the Raspberry Pi and are used to control the GPIO output pins that provide digital signals for closed-loop feedback and water rewards to water-restricted mice. In 1-ROI experiments, we mapped the range of average ΔF/F<sub>0</sub> values in the ROI (<xref rid="fig2" ref-type="fig">Figure 2</xref>) to a range of audio frequencies (1 kHz - 22 kHz), which acted as feedback to the animal. For 2-ROI experiments, the magnitude of the ΔF/F<sub>0</sub> activity difference between the ROIs, based on the specified rule (e.g., ROI1-ROI2), was mapped to the range of audio frequencies. We confirmed that these frequencies were accurately generated and mapped by audio recordings (supplementary figure 2 and see Methods) obtained at 200 kHz using an ultrasonic microphone (Dodotronic, Ultramic UM200K) positioned within the recording chamber ∼10 cm from the audio speaker. A previous version of the CLNF system was found to have non-linear audio generation above 10 kHz, partly due to problems in the audio generation library and partly due to the consumer-grade speaker hardware we were employing. This was fixed by switching to the Audiostream (<ext-link ext-link-type="uri" xlink:href="https://github.com/kivy/audiostream">https://github.com/kivy/audiostream</ext-link>) library for audio generation and testing the speakers to make sure they could output the commanded frequencies (supplementary figure 2). To confirm the timing of feedback latency, LED lights were triggered instead of water rewards (supplementary Figure 1), and the delay was calculated between the detected event (green LED ON for CLNF and paw movement for CLMF) and the red LED flash. In the case of CLNF, the camera recording brain activity was used to record both the flashing green and red LEDs. Temporal traces of green and red LED pixels were extracted from the recorded video, and the average delay between the green and red LEDs becoming bright was calculated as the delay in closed-loop feedback for CLNF experiments. Performing this analysis indicated that the Raspberry Pi system could provide reliable graded feedback within ∼63 ± 15 ms for CLNF experiments.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2</label>
<caption><title>Setup of Real-time Feedback for GCaMP6 Cortical Activity and Movements</title><p>A) GCaMP-based Closed-loop Feedback and Reward System: Mice with transcranial windows were head-fixed beneath an imaging camera, with the cortical window illuminated using 440 nm (for reflectance) and 470 nm (for GCaMP excitation) light. i) Epifluorescence at 520 nm and reflectance at 440 nm were captured at 15 fps using a bandpass filter, integrated within the cortical imaging system. ii) The captured images were simultaneously saved and processed to compute ΔF/F<sub>0</sub> in real-time using a Raspberry Pi 4B model. Pre-selected regions of interest (ROIs) were continuously monitored, and rule-specific activation was calculated based on the ΔF/F<sub>0</sub> signal. The left panel displayed wide-field cortical GCaMP6 fluorescence (green) and reflectance (blue), while the right panel showed the real-time calculated and corrected ΔF/F<sub>0</sub> map, generated using a moving average of the captured images. The target ROIs were marked as green (R1) and red (R2), although a single ROI could also be selected for monitoring. iii) For example, as shown on the ΔF/F<sub>0</sub> map, ROIs R1 and R2 were continuously monitored, and the average activity across these ROIs was calculated. iv) When the task rule was defined as “R1 - R2 &gt; threshold”, the difference between R1 and R2 activities was mapped to a non-linear function that generated graded audio tone frequencies (ranging from 1 kHz to 22 kHz), as illustrated in the figure. Task rules could be modified within the setup on any given day, and the corresponding activation levels were automatically mapped to the audio frequencies. The “threshold”, expressed in ΔF/F<sub>0</sub> units, was adjustable based on the experimental design. v) Upon reaching the rule-specific threshold for activity, in addition to the increase in audio tone frequency, a water reward was delivered to the head-fixed mouse. Mice expressing GCaMP6, with surgically implanted cortical windows and a head-bar, were positioned beneath the imaging camera, with GCaMP6 excitation light at 470 nm. A secondary wavelength of 440 nm was used for continuous reflectance signals to measure hemodynamic changes, which were then applied to correct the fluorescence signal. The RGB camera was equipped with bandpass filters that allowed only 520 nm epifluorescence and 440 nm reflectance to be simultaneously collected.</p><p>B) Closed-loop Behavior Feedback and Reward Setup: A specialized transparent head-fixation chamber was custom-designed using 3mm-thick plexiglass material (3D model available) to enable multi-view behavioral imaging and real-time tracking of body parts. The rectangular chamber was equipped with two strategically positioned mirrors—one at the bottom, angled at 45 degrees, and one at the front, angled at 30 degrees—facilitating multi-view imaging of the head-fixed mouse with a single camera. i) A Dalsa CCD camera was connected to a PC for widefield cortical imaging during the session. ii) Auditory feedback was provided using a non-linear function mapping paw speeds to corresponding audio tone frequencies. iii) The head-fixed mouse, positioned in the transparent chamber, was able to freely move its body parts, while its behavior was continuously recorded. This setup allowed for the capture of three distinct views of the mouse—side, front, and bottom profiles—and enabled the real-time tracking of multiple body parts, including the snout, left and right forelimbs, left and right hindlimbs, and the base of the tail. iv) Video frames of the behavior were processed in real-time on a GPU (Nvidia Jetson Orin), which tracked the body parts using a custom pre-trained model.</p></caption>
<graphic xlink:href="619716v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For the CLMF experiments, an Omron Sentech STC-MCCM401U3V USB3 Vision camera, connected to the Nvidia Jetson via its Python software developer kit (SDK), was used to calculate the feedback delay. The incoming stream of frames was processed in real-time using a custom deep-neural-network model that was trained using DeepLabCut (<xref ref-type="bibr" rid="c16">16</xref>) and DeepLabCut-Live (<xref ref-type="bibr" rid="c15">15</xref>), designed to track previously defined points on the mouse. The model was incrementally improved by fine-tuning and re-training 26 times, using 2080 manually labeled frames spanning 52 videos of 10 mice. The pre-trained model is available for anyone to use and fine-tune to adapt for similar platforms. The model was integrated into our CLMF program and deployed on an Nvidia Jetson device for real-time inference of tracked points. A Python-based virtual environment using Conda was created to install all software dependencies. The coordinates of the tracked points for each frame were appended to a list, forming a temporal sequence referred to as “tracks.” Upon offline analysis, these real-time tracks were found to be both accurate and stable throughout the duration of the videos.</p>
<p>To calculate the feedback delay for movements, a red LED was placed within the behavior camera’s field of view. Whenever a threshold-crossing movement was detected in the real-time tracking, the system triggered the LED to turn on. Temporal traces of the tracked left forelimb and the pixel brightness of the red LED were then extracted from the recorded video. By comparing these traces, the average delay between the detected movements and the LED illumination was calculated to be 67±20 ms, which represents the delay in the closed-loop feedback for the CLMF experiments.</p>
<p>For CLMF, mice were headfixed in a specially designed head-fixing chamber (<xref rid="fig2" ref-type="fig">Figure 2Biii</xref>, design guide and 3D model in methods) to achieve multiview behavioral recording from a single camera using mirrors (see methods section). In brief, mice are headfixed in a transparent rectangular tunnel (top view) with a mirror at the front (front view) and at the bottom (bottom view) that allows multiple views of the mouse body. The body parts that we track are: snout-top (snout in the top view), tail-top (base of the tail in the top view), snout-front (snout in the front view), FLL-front (left forelimb in the front view), FLR-front (right forelimb in the front view), snout-bottom (snout in the bottom view), FLL-bottom (left forelimb in the bottom view), FLR-bottom (right forelimb in the bottom view), HLL-bottom (left hindlimb in the bottom view), HLR-bottom (right hindlimb in the bottom view), tail-bottom (base of the tail in the bottom view) (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). The rationale for selecting these body parts in a particular view was that they needed to be visible at all times to avoid misclassification in real-time tracking. By combining the tracks of a body part in different views, we can form a 3D track of the body part. In a 3D coordinate space having X, Y, and Z axes, tracked points (xb, yb) in the bottom view were treated as being in the 3D X-Y plane, and tracked points (xf, yf) in the front view were treated as being in the 3D Y-Z plane. Thus, X = xb, Y = yb, Z = yf formed tracked points in 3D for a given body part that was tracked in multiple views.</p>
<p>For example, FLL-front and FLL-bottom were tracking the left forelimb in front and bottom views, and by combining the tracks of these two points, we obtained a 3D track of the left forelimb (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). Although these 3D tracks are available in real-time, for our CLMF experiments, we used 2D tracks (xb, yb) for behavioral feedback. Audio output channels and GPIO pins on the Nvidia Jetson were used for audio feedback and reward delivery, respectively. Tracks of each body part were used to calculate the speed of those points in real-time, and a selected body part (also referred to as a control point) was mapped to a function generating proportional audio frequency (same as in CLNF, details in the method section). In the software we have developed, one can also choose to calculate acceleration, radial distance, angular velocity, etc., from these tracks and map it to the function generating varying audio frequency feedback. For our work, a range of spontaneous speeds was calculated from a baseline recording before the start of the training. The threshold speed to receive a reward was also calculated from the baseline recording and set at a value that would have yielded a reward rate of one reward per two-minute period (translates to a basal performance of 25% in our trial task structure). This thresholding step was done to allow the mice to discover task rules and keep them motivated.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3</label>
<caption><title>Experimental Protocol and Trial Structure</title><p>A) Experimental Protocol (detailed in Methods): In brief, 90-day-old transgenic male and female mice were implanted with a transcranial window and allowed to recover for a minimum of 7 days. Following recovery, the mice were placed in the experimental rig for approximately 45 minutes per day, for a minimum of 3 days, to undergo habituation. One day before the start of the experiment, the mice were placed on a water-restriction protocol (as detailed in Methods). Closed-loop experiment training commenced on day 1, during which mice were required to modulate either their target brain activity (GCaMP signals in regions of interest) or target behavior (tracked paw-speed) during daily sessions of approximately 45 minutes over the course of 10 days. Throughout this period, both cortical and behavioral activities were recorded. Body weight was monitored daily, and supplementary water was provided to any mouse that lost more than 1% of its body weight. After the final experimental session on day 10, the mice were removed from the water-restriction protocol.</p><p>B) Trial Structure of Cortical GCaMP-based Feedback Sessions: Each trial was preceded by a minimum of 10 seconds of rest, which was extended if the mouse was not stable. Once the mouse remained stable and refrained from moving its limbs, the trial began with a basal audio tone of 1 kHz. The mice then had 30 seconds to increase rule-based activations (in the selected ROI) up to a threshold value to receive a water reward. A trial ended as soon as the activation reached the threshold, triggering a reward delivery, or timed out after 30 seconds with an overhead buzzer serving as a negative signal. Both the reward and the negative signal were delivered within 1 second after the audio ceased at the end of each trial.</p><p>C) Dorsal Cortical ΔF/F<sub>0</sub> Activity: Dorsal cortical ΔF/F<sub>0</sub> activity was recorded and overlaid with a subset of Allen CCF coordinates, which could be selected as the center of candidate regions of interest (ROIs).</p><p>D) Trial Structure of Behavior Feedback Sessions: The behavioral feedback trials followed a similar structure to the cortical feedback trials, with each trial preceded by at least 10 seconds of rest. The trial began with a basal tone of 1 kHz, which increased in frequency as the mouse’s paw speed increased.</p><p>E) Forelimb Tracking during Feedback Sessions: Forelimb tracking was performed in both the left (blue) and right (green) forelimbs using a camera coordinate system on day 4 of training with mouse FM2, which received feedback based on left forelimb speed. The forelimbs were tracked in 3D, leveraging multiple camera views captured using mirrors positioned at the bottom and front of the setup (<xref rid="fig2" ref-type="fig">Figure 2B</xref>).</p></caption>
<graphic xlink:href="619716v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Both CLNF and CLMF experiments shared a similar experimental protocol (surgery, habituation, then several days of training, <xref rid="fig3" ref-type="fig">Figure 3A</xref>). A daily session starts with a 30-sec rest period (no rewards or task feedback) followed by 60 trials (maximum 30 sec each) with a 10-serest c between trials in CLNF, and a minimum of 10 sec rest or until tracked points are stable for 5 sec in CLMF (<xref rid="fig3" ref-type="fig">Figure 3B</xref>, <xref rid="fig3" ref-type="fig">3D</xref>). After the habituation period, a spontaneous session of 30 minutes was recorded where mice were not given any feedback or rewards.</p>
<p>The spontaneous session was used to establish baseline levels of GCaMP activity (in target ROI(s) for CLNF experiments) or speed of a target body part for CLMF experiments. This was done to calculate the animal-specific threshold for future training sessions. A success in the trial resulted in a water drop reward that was delivered 1 sec after the end of the trial, and a failed trial ended with a buzzer vibrator 1 sec after the end of the trial (<xref rid="fig3" ref-type="fig">Figure 3B</xref>, <xref rid="fig3" ref-type="fig">3D</xref>).</p>
<sec id="s2a">
<title>Mice can explore and learn an arbitrary task, rule, and target conditions</title>
<p>CLNF training (<xref rid="fig2" ref-type="fig">Figure 2A</xref>) required real-time image processing, feedback, and reward generation. Feedback was a graded auditory tone mapped to the relative changes in selected cortical ROI(s) or movement speed of a tracked body part. Training was conducted using multiple sets of cortical ROI(s) on both male and female mice (see <xref rid="tbl4" ref-type="table">Table 4</xref>), wherein the task was to increase the activity in the selected ROI(s) according to the rule (also referred to as ‘the rule’ in the future) to above a predetermined threshold (<xref rid="fig5" ref-type="fig">Figure 5A</xref>, <xref rid="fig5" ref-type="fig">5B</xref>). Fluorescence activity changes (ΔF/F<sub>0</sub>) were calculated using a running baseline of 5 seconds, with single or dual ROIs on the dorsal cortical map selected as targets. In general, all ROIs assessed that encompassed sensory, pre-motor, and motor areas were capable of supporting increased reward rates over time (<xref rid="fig4" ref-type="fig">Figure 4A</xref>, Animation 1). A ΔF/F<sub>0</sub> threshold value was calculated from a baseline session on day 0 that would have allowed 25% performance. Starting from this basal performance of around 25% on day 1, mice (CLNF No-rule-change, n=28 and CLNF Rule-change, n=13) were able to discover the task rule and perform above 80% over ten days of training (<xref rid="fig4" ref-type="fig">Figure 4A</xref>, RM ANOVA p=2.83e-5), and Rule-change mice even learned a change in ROIs or rule reversal (<xref rid="fig4" ref-type="fig">Figure 4A</xref>, RM ANOVA p=8.3e-10, <xref rid="tbl4" ref-type="table">Table 4</xref> for different rule changes). There were no persistent significant differences between male and female mice (Supplementary Figure 3A). To investigate whether certain ROI(s) were better than others in terms of performance, we performed linear regression of the success rate over the days and, based on the slope of the fitted line, discovered ROI rules that yielded statistically different progression (fast and relatively slower) of success rate from the mean slope of all ROIs (Supplementary Figure 3C). We visualized these significantly different ROI-rule-based success rates and segregated them into fast and slow based on mean slope (&gt;=0.095 slope was designated fast, else slow) of the progression (Supplementary Figure 3D). Our analysis revealed that certain ROI rules (see description in methods) lead to a greater increase in success rate over time than others (Supplementary Figure 3D).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Closed-loop feedback helped mice learn the task and achieve superior performance in CLNF and CLMF in both experiments.</title><p>A) Mice (in <xref rid="tbl4" ref-type="table">Table 4</xref>) were able to learn the CLNF task over several sessions, with performance above 70% by the 10th session (RM ANOVA p=2.83e-5). The rule change (in pink, day 11) led to a sharp decline in performance (ANOVA, p=8.7e-9), but the mice were able to adapt and learn the task rule change (RM ANOVA p=8.3e-10; see <xref rid="tbl4" ref-type="table">Table 4</xref> for different rule changes). The method to determine the ROI(s) used in the changed task rule is described in the methods section.</p><p>B) Three groups were employed for CLMF experiments. The “Rule-change” group (n=8, received feedback, in pink) was trained with task rule mapping auditory feedback to the speed of the left forelimb and was able to perform above a 70% success rate in four days. The task rule mapping was changed from the left to the right forelimb on day 5, so the rewards as well as audio frequencies would now be controlled by the right forelimb. Surprisingly, the “Rule-change” mice were able to discover the change in rule and started performing above 70% within a day of the rule change, on day 6. The “No-rule-change” group (n=4, received audio feedback, no rule change, in green) and the “No-feedback” group (n=4, no graded audio feedback, no rule change, in blue) were control groups to investigate the role of audio feedback. The performance of the “No-feedback” mice, who did not receive the graded feedback, was never on par (RM-ANOVA p=0.49) with the “No-rule-change” group that received the feedback (RM-ANOVA p=9.6e-7).</p><p>C) Task latencies in each group follow the trend of their performance. Rule change (n=8) and no rule change (n=4) task latencies gradually came down, with an exception on day 5 for rule change when the task rule was changed. No feedback (n=4) task latencies are never on par with the groups that received feedback.</p><p>D) CLMF Rule-change (n=8) behavior, we looked at the maximum speeds of the left and right forelimbs. The paw with the maximum speed follows the task rule and switches with the change in the task rule. It is worth noting that the task was not restrictive on other body parts; i.e., they were free to move other body parts along with the control point.</p><p>E) Reward-aligned average (n=4) ΔF/F<sub>0</sub> signals associated with the target rule on day1 and day9 (top plot). Kernel density estimate (KDE) of target ΔF/F<sub>0</sub> values during the whole session on day1 and day9 of 1-ROI experiments (bottom plot).</p><p>F) Reward-aligned average (n=4) target paw speed on day1 and day10 (top plot). Kernel density estimate (KDE) of target paw speeds on day1 and day10 (bottom plot).</p><p>G) In the context of CLNF 2-ROI experiments, bivariate distribution of ROI1 and ROI2 ΔF/F<sub>0</sub> values during whole sessions on day9 and day19, with densities projected on the marginal axes. The task rule on day9 was “ROI1-ROI2 &gt; thresh.” as opposed to “ROI2-ROI1 &gt; thresh.” on day19. The bivariate distribution is significantly different (Multivariate two-sample permutation test, p=2.3e-12) on these days, indicating a robust change in activity within these brain regions.</p><p>H) Joint (bivariate) distribution of left and right paw speeds during whole session on day4 and day10 of CLMF. Left and right forelimbs were control-point (CP) on day4 and day10 respectively. There is a visible bias in the bivariate distribution towards the CP on respective days.</p></caption>
<graphic xlink:href="619716v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>CLNF: Cortical activity during the closed-loop-neurofeedback training.</title><p>A) Target-rule-based ΔF/F<sub>0</sub> traces in green on day 1 with rule-1 (top row), on day 10 with rule-1 (second row), on day 11 with new rule-2 (third row), and on day 19 with rule-2 (fourth row). Shared regions are trial periods and regions between grey areas are rest periods. The grey horizontal line depicts the threshold above which mice would receive a reward. Golden stars show the rewards received, and short vertical lines in black show the spout licks.</p><p>B) Representative reward-centered average cortical responses of the 2ROI experiment on labeled days. ROI1 (green) and ROI2 (pink) are overlaid on the brain maps. The task rule on day 1 and day 4 was “ROI1-ROI2 &gt; thresh,” as opposed to “ROI2-ROI1 &gt; thresh” on day 11 and day 19.</p><p>C) Linear regression on ROI1 and ROI2 ΔF/F<sub>0</sub> during whole sessions. The regression fit inclines towards ROI1 in sessions where rule was “ROI1-ROI2 &gt; thresh.” (day1 slope=0.44, day6 slope=0.32) while it leans towards ROI2 after the task rule switch to “ROI2-ROI1 &gt; thresh.” (day11 slope=0.76, day17 slope=0.80).</p></caption>
<graphic xlink:href="619716v1_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In CLMF training, the real-time speed of a selected (also referred to as control point (CP) in terms of tracked points) point was mapped to the graded audio tone generator function. We trained the mice with the FLL bottom as the CP for auditory feedback and reward generation (example trial in Animation 2). Mice reached 80% performance (<xref rid="fig4" ref-type="fig">Figure 4B</xref>, CLMF Rule-change, No-rule-change) in the task within four days of training (RM ANOVA, p = 8.03e-7). They were eliciting the target behavior (i.e., moving CP at high speed) more frequently on later days compared to the first day of training (<xref rid="fig4" ref-type="fig">Figure 4F</xref>), and the correlations of CP speed profiles became more pronounced during the trial periods as compared to the rest period (<xref rid="fig6" ref-type="fig">Figure 6C</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>CLMF: Speed of the tracked target body part and cortical activity.</title><p>A) Left forelimb speed (black), target threshold (grey line), and rewards (golden stars) during a sample period in a session on day 1 of the closed-loop training (top row). Shaded regions are trial periods with interspersed rest periods in white. Left forelimb speed, and rewards on day4 (second row). The target body part was changed from the left forelimb to the right forelimb on day5 (third row). Thus, day5 is the first training day with the new rule. Right forelimb speed, and rewards on day 10 of the training (fourth row).</p><p>B) Reward-centered average cortical responses on days corresponding to rows in A. The target threshold was crossed at -1 s, and the reward was delivered at 0 s. Notice the task rule change on day 5.</p><p>C) Correlation matrix showing pairwise correlations of left and right forelimb speed profiles. Top row: During rewarded trials, over the training sessions of CLMF Rule-change (left), No-rule-change (middle), and No-feedback (right). High correlations (dark cells) between speed profiles of CP indicate a unilateral bias in the movement. It is worth noting the drastic changes in correlations as the control-point was changed from left forelimb to right forelimb in Rule-change mice on day4. Bottom row: During rest periods, over the training sessions of CLMF Rule-change (left), No-rule-change (middle), and No-feedback (right).</p></caption>
<graphic xlink:href="619716v1_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2b">
<title>Mice can rapidly adapt to changes in the task rule</title>
<p>We have assessed how mice respond to changes in closed-loop feedback rules. CLNF Rule-change mice went through a change in regulated cortical ROI(s) (<xref rid="fig5" ref-type="fig">Figure 5A</xref>, 5B) after being trained on an initial set of ROI(s) (<xref rid="tbl4" ref-type="table">Table 4</xref>, Rule-change). The new ROI(s) were chosen from a list of candidate ROI(s) for which the cortical activations were still low (see “Determining ROI(s) for change in CLNF task rule” under Methods). Reward threshold values were then recalculated for the ROI(s). A full list of ROI(s) we tried in separate mice is listed in <xref rid="tbl4" ref-type="table">Table 4</xref>. Mouse performance degraded on the first day of the rule switch to the new ROIs (ANOVA, p=8.7e-9), compared to the previous day (<xref rid="fig4" ref-type="fig">Figure 4A</xref>), and quickly recovered within five days (RM ANOVA p=8.3e-10) of further training. All new cortical ROI(s) appeared to support recovery to the pre-perturbation success rate (<xref rid="tbl4" ref-type="table">Table 4</xref>), and data were pooled.</p>
<p>Similarly, in the CLMF experiments, the target body part was changed from FLL to FLR for Rule-change mice on day 5 and was associated with a significant drop in their success rate from 75-80% to ∼40% (ANOVA p=0.008, <xref rid="fig4" ref-type="fig">Figure 4B</xref>). Surprisingly, mice quickly adapted to the rule change and started performing above 70% within a day on day 6 (example trial in Animation 3). Looking closely at their reward rate on day 5 (day of rule change), they had a higher reward rate in the second half of the session as compared to the first half, indicating they were adapting to the rule change within one session. As the mice were learning the task (increasing performance), task latency decreased (<xref rid="fig4" ref-type="fig">Figure 4C</xref>). Task latency in this context is the time taken by mice to perform the task within a trial period of 30 s. We included all trials, both successful and unsuccessful, in our calculation. Given that the maximum trial duration is 30 s, the longest possible task latency was capped at 30 s. Following the trend in task performance, the task latency started decreasing during the first four days but increased on day 5 (rule change) and started to drop again afterward.</p>
<p>We also examined the average paw speeds and distributions during trial periods. It is worthwhile noting that for the rule-change group, left forelimb speeds were higher than right forelimb speeds from day 1 to day 4 (when task rule was to move left forelimb). When the rule was switched from left forelimb to right forelimb on day 5, left forelimb speeds dropped below the right forelimb speeds (<xref rid="fig4" ref-type="fig">Figure 4D</xref>).</p>
</sec>
<sec id="s2c">
<title>Graded feedback helps in task exploration during learning, but not after learning</title>
<p>To investigate the role of audio feedback in our task, we also had a group with no task-related graded audio feedback (CLMF No-feedback) and instead received audio with a constant frequency (1 kHz) throughout the trials. CLMF No-rule-change mice who received continuous graded auditory feedback significantly improved their task performance (CLMF No-rule-change RM-ANOVA p = 9.6e-7) and outperformed the CLMF No-feedback mice (<xref rid="fig4" ref-type="fig">Figure 4B</xref>) very early (No-feedback RM-ANOVA p = 0.49), indicating the positive role of graded feedback for task exploration and learning the association. When graded audio feedback was removed for CLMF Rule-change mice on day 10, it did not affect their task performance, indicating the feedback was not essential to keep performing the task that they had already learned.</p>
</sec>
<sec id="s2d">
<title>Cortical responses became focal and more correlated as mice learned the rewarded behavior in CLMF</title>
<p>There are reports of cortical plasticity during motor learning tasks, both at cellular and mesoscopic scales (<xref ref-type="bibr" rid="c17">17</xref>–<xref ref-type="bibr" rid="c19">19</xref>), supporting the idea that neural efficiency could improve with learning. As mice become proficient in a task, their brain activity becomes more focused and less widespread, reflecting more efficient neural processing (<xref rid="fig7" ref-type="fig">Figure 7</xref>). We noticed the peak activity in different regions of the cortex around the rewarding movement decreased ΔF/F<sub>0</sub> in amplitude over days (<xref rid="fig7" ref-type="fig">Figure 7A</xref>, <xref rid="fig7" ref-type="fig">7B</xref>, <xref rid="fig7" ref-type="fig">7C</xref>, <xref rid="fig7" ref-type="fig">7E</xref>). To quantify this, we measured the peak ΔF/F<sub>0</sub> (ΔF/F<sub>0</sub>peak) value in the time window from -1s to +1s relative to the body part speed threshold crossing event in each cortical region. Consistent with our visual observations, ΔF/F<sub>0</sub>peak in several cortical regions gradually decreased over sessions, including the olfactory bulb, sensory forelimb, and primary visual cortex (<xref rid="fig7" ref-type="fig">Figure 7A</xref>, <xref rid="fig7" ref-type="fig">7B</xref>, <xref rid="fig7" ref-type="fig">7C</xref>). Notably, when the task rule was changed from FLL to FLR in the CLMF Rule-change mice, we observed a significant increase in ΔF/F<sub>0</sub>peak in regions including the olfactory bulb (OB), forelimb cortex (FL), hindlimb cortex (HL), and primary visual cortex (V1), as shown in <xref rid="fig7" ref-type="fig">Figure 7</xref>. We believe the decrease in ΔF/F<sub>0</sub>peak is unlikely to be driven by changes in movement, as movement amplitudes did not decrease significantly during these periods (<xref rid="fig7" ref-type="fig">Figure 7D</xref> CLMF Rule-change). However, the decrease in ΔF/F<sub>0</sub>peak followed the same trend as task latency (<xref rid="fig4" ref-type="fig">Figure 4C</xref>), suggesting that the decrease in ΔF/F<sub>0</sub>peak is especially prominent in trials where mice were prepared to make the fast movement.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Cortical dynamics and network changes during longitudinal CLMF training.</title><p>A) Reward-centered average responses in the olfactory bulb decrease over the days as performance increases.</p><p>B) Cortical responses become focal and closely aligned to the paw movement (green line) and reward (cyan line) events on day 10 for group-1 (received feedback) as compared to group-3 (no-feedback).</p><p>C) ΔF/F<sub>0</sub> peak values during successful trials (pink) and during rest (cyan) over the ten-day training period in the olfactory bulb (OB) (left, day 1-day 4 p-value=0.025, day 4-day 5 p-value=0.008), forelimb area (FL) (center, day 1-day 4 p-value=0.008, day 4-day 5 p-value=0.04), and primary visual cortex (right, day 1-day 4 p-value=0.04, day 4-day 5 p-value=0.002). Statistical significance was assessed using the Mann-Whitney test and corrected for multiple comparisons with the Benjamini-Hochberg procedure.</p><p>D) Average movement (mm) of different tracked body-parts during trials in CLMF Rule-change (left), No-rule-change (center), No-feedback (right).</p><p>E) Correlations between cortical activation on each training session in barrel-cortex (BC, top left), anterolateral motor cortex (ALM, top right), secondary motor cortex (M2, bottom left), and retrosplenial cortex (RS, bottom right).</p></caption>
<graphic xlink:href="619716v1_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>These results suggest that motor learning led to less cortical activation across multiple regions, which may reflect more efficient processing of movement-related activity. In addition to ΔF/F<sub>0</sub>peak reflecting signs of potentially more efficient cortical signaling, intracortical GCAMP transients measured over days became more stereotyped in kinetics and were more correlated (to each other) as the task performance increased over the sessions (<xref rid="fig7" ref-type="fig">Figure 7E</xref>).</p>
<p>Analysis of pairwise correlations between cortical regions (referred to as seed pixel correlation maps) revealed distinct network activations during rest and trial periods (<xref rid="fig8" ref-type="fig">Figure 8</xref>). While the general structure of the correlation maps remained consistent between trial and rest periods, certain correlations, such as between V1 and RS, were heightened during task trials, whereas others, such as between M1, M2, FL, and HL, consistently increased over the training sessions (<xref rid="fig8" ref-type="fig">Figure 8</xref>).</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8:</label>
<caption><title>CLMF cortex-wide seed pixel correlation maps. Pairwise correlations between activity at cortical locations (also referred to as seed pixel locations).</title><p>A) Top row: No-rule-change average seed pixel correlation map during trial periods (left), during rest periods (middle), and difference of average correlation map during trial and rest (right). Bottom row: No-feedback average seed pixel correlation map during trial periods (left), during rest periods (middle), and difference of average correlation map during trial and rest (right).</p><p>B) Significant increase in pairwise seed pixel correlations as RM-ANOVA p-value (Bonferroni corrected) matrix between training sessions over the days (left) and between trial vs rest periods (right) for CLMF No-rule-change mice.</p><p>C) Significant increase in pairwise seed pixel correlations as RM-ANOVA p-value (Bonferroni corrected) matrix between training sessions over the days (left) and between trial vs rest periods (right) for CLMF No-feedback mice.</p></caption>
<graphic xlink:href="619716v1_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To statistically examine the differences in seed pixel correlations during trials and rest periods, as well as how these correlations changed over training sessions (Day 1-10), we conducted a two-way repeated measures ANOVA (RM-ANOVA) on the seed pixel correlation maps for each day. The two variables for the RM-ANOVA were experimental condition (trial vs. rest) and session number (Day 1-10). This analysis generated two distinct matrices of Bonferroni-corrected p-values (<xref rid="fig8" ref-type="fig">Figure 8</xref>), one corresponding to each variable, which segregated the seed pixel correlations that were different between trial and rest periods and those that changed over the training sessions.</p>
</sec>
<sec id="s2e">
<title>Distinct task- and reward-related cortical dynamics</title>
<p>In our closed-loop experiment, mice were trained to associate a specific body movement with a reward. Over the course of 10 training sessions, the mice progressively learned the task, as evidenced by increased task performance across sessions (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). The acquisition of this learned behavior was consistent with previous findings on motor learning, where reinforcement and task repetition lead to the refinement of motor skills (<xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c21">21</xref>). Importantly, the reward was provided 1 s after a successful trial to study the timing and nature of cortical activation in response to both the task and the reward. Concurrent with behavioral training, widefield cortical activity was recorded observing distinct patterns of neural activation that evolved as the mice learned the task.</p>
<p>During the early sessions (days 1 to 3), cortical activity was observed to be spatially widespread and engaged multiple cortical regions. Temporally, the activity spanned both task-related and reward-related events, with no clear distinction between the two phases (<xref rid="fig7" ref-type="fig">Figure 7B</xref>, left). This broad activation pattern is consistent with the initial stages of learning, where the brain recruits extensive cortical networks to process novel tasks and integrate sensory feedback (<xref ref-type="bibr" rid="c22">22</xref>).</p>
<p>As the mouse performance improved in the later sessions (Days 8 to 10), the cortical activity became more segregated both spatially and temporally (<xref rid="fig7" ref-type="fig">Figure 7B</xref>, middle). This segregation was particularly notable in mice that received closed-loop feedback (Rule-change), where the spatiotemporal patterns of cortical activation were more closely aligned with the specific task and reward events. This transition from widespread to segregated activation is indicative of the brain’s optimization of neural resources as the task becomes more familiar, a phenomenon that has been reported in studies of skill acquisition and motor learning (<xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c18">18</xref>). Previous studies have shown that feedback, especially when provided in a temporally precise manner, can accelerate the cortical plasticity associated with learning (<xref ref-type="bibr" rid="c23">23</xref>). Overall, these findings highlight the importance of closed-loop feedback in motor learning and suggest that real-time neurofeedback can enhance the specificity of cortical representations, potentially leading to more efficient and robust learning outcomes.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<sec id="s3a">
<title>Flexible, cost-effective, and open-source system for a range of closed-loop experiments</title>
<p>We developed a highly adaptable, cost-effective, and open-source system tailored for a wide range of closed-loop neuroscience experiments (CLNF, CLMF). Our system is built on readily available hardware components, such as Raspberry Pi and Nvidia Jetson platforms, and leverages Python-based software for real-time data processing, analysis, and feedback implementation. The modular approach ensures that the system can be easily customized to meet the specific requirements of different experimental paradigms. Our study demonstrates the effectiveness of a versatile and cost-effective closed-loop feedback system for modulating brain activity and behavior in head-fixed mice. By integrating real-time feedback based on cortical GCaMP imaging and behavior tracking, we provide strong evidence that such closed-loop systems can be instrumental in exploring the dynamic interplay between brain activity and behavior. The system’s ability to provide graded auditory feedback and rewards in response to specific neural and behavioral events showcases its potential for advancing research in neurofeedback and behavior modulation.</p>
<p>The hardware backbone of our system is designed around the Raspberry Pi 4B+ and Nvidia Jetson platforms, chosen for their compactness, low cost, high computational power, and wide community support. These platforms have been demonstrated to be effective in neuroscience research, for several applications (<xref ref-type="bibr" rid="c24">24</xref>–<xref ref-type="bibr" rid="c27">27</xref>). By utilizing open-source software frameworks such as Python, the system can be modified and extended to incorporate additional functionalities or adapt to new experimental protocols.</p>
<p>Our Python-based software stack includes libraries for real-time data acquisition, signal processing, and feedback control. For instance, we utilize OpenCV for video processing and DeepLabCut-Live (<xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref>) for behavioral tracking, which are essential components in experiments requiring precise monitoring and feedback based on animal behavior. Additionally, we have integrated libraries for handling neural data streams, such as NumPy and SciPy, which facilitate the implementation of complex experimental designs (<xref ref-type="bibr" rid="c28">28</xref>) involving multiple data sources and feedback modalities. By integrating the system with LED drivers and opsin-expressing transgenic mouse lines, it is straightforward to achieve precise temporal control over neural activation, enabling the study of causal relationships between neural circuit activity and behavior (<xref ref-type="bibr" rid="c29">29</xref>).</p>
<p>The cost-effectiveness of our system is a significant advantage, making it accessible to a broader range of research labs, including those with limited funding. The use of off-the-shelf components and open-source software drastically reduces the overall cost compared to commercially available systems, which often require expensive proprietary hardware and software licenses (<xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref>). Furthermore, the open-source nature of our system promotes collaboration and knowledge sharing within the research community, as other labs can freely modify, improve, and distribute the system without any licensing restrictions.</p>
<p>The system’s flexibility is demonstrated by its successful application across various closed-loop experimental paradigms (CLNF, CLMF). For example, in our study, we utilized the system to implement real-time feedback based on intracellular calcium-induced fluorescence imaging in awake, behaving mice. The system provided auditory feedback in response to changes in cortical activation, allowing us to explore the role of real-time feedback in modulating both neural activity and behavior.</p>
</sec>
<sec id="s3b">
<title>Importance of Closed-Loop Feedback Systems</title>
<p>Closed-loop feedback systems have gained recognition for their ability to modulate neural circuits and behavior in real time, an approach that aligns with the principles of motor learning and neuroplasticity. The ability of mice to learn and adapt to tasks based on cortical or behavioral feedback underscores the parallels between closed-loop systems and natural proprioceptive mechanisms, where continuous sensory feedback is crucial for motor coordination and spatial awareness. This study builds upon the foundational work of (<xref ref-type="bibr" rid="c32">32</xref>), who first explored the potential of closed-loop neurofeedback, and expands its application to modern neuroscience by incorporating optical brain-computer interfaces.</p>
<p>Our findings are consistent with previous research showing that rodents can achieve volitional control over externally represented variables linked to their behavior (<xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c33">33</xref>). Moreover, the rapid adaptation observed in mice when task rules were altered demonstrates the system’s capacity to facilitate learning and neuroplasticity, even when the conditions for achieving rewards are modified. The quick recovery of task performance after a rule change, as evidenced by the improved performance within days of training, highlights the robustness of the closed-loop feedback mechanism.</p>
</sec>
<sec id="s3c">
<title>Neuroplasticity and Cortical Dynamics</title>
<p>The observation that cortical responses became more focused as mice learned the rewarded behavior aligns with established theories of motor learning, where neural efficiency improves with practice (<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref>). As mice became more proficient in the task, the widespread cortical activity observed during the initial training sessions became more regionally localized, indicating more efficient neural processing. The reduction in ΔF/F<sub>0</sub>peak values across sessions suggests that the brain becomes more efficient at processing task-relevant information, a phenomenon consistent with the optimization of neural circuits observed in skilled motor learning (<xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c21">21</xref>).</p>
<p>The distinct spatiotemporal patterns of cortical activation observed in mice receiving closed-loop feedback further support the role of real-time feedback in enhancing cortical plasticity. The pronounced segregation of task-related and reward-related cortical dynamics in the later training sessions indicates that closed-loop feedback facilitates the refinement of neural circuits involved in motor learning. These findings are in line with previous studies that have demonstrated the importance of temporally precise feedback in accelerating cortical reorganization and enhancing learning outcomes (<xref ref-type="bibr" rid="c23">23</xref>).</p>
</sec>
<sec id="s3d">
<title>Future Directions and Implications</title>
<p>Looking ahead, there are several promising directions for expanding the capabilities of our closed-loop feedback system. These include the integration of more advanced imaging techniques, such as multi-photon microscopy, to provide higher-resolution data on neural activity, as well as the incorporation of optogenetic stimulation to achieve more precise control over neural circuits. Additionally, the system’s scalability could be tested in larger animal models or even human subjects, potentially paving the way for translational applications in neurorehabilitation and brain-computer interfaces. The open-source nature of our system also encourages collaboration and knowledge sharing within the research community, fostering innovation and accelerating the development of new experimental tools. As more labs adopt and refine this system, we anticipate that it will contribute to a deeper understanding of the mechanisms underlying brain-behavior dynamics and the development of novel therapeutic interventions for neurological disorders.</p>
<p>In conclusion, our study highlights the significant potential of closed-loop feedback systems for advancing neuroscience research. By providing a flexible, cost-effective, and open-source platform, we offer a valuable tool for exploring the complex interactions between brain activity and behavior, with implications for both basic research and clinical applications.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and methods</title>
<sec id="s4a">
<title>Animals</title>
<p>Mouse protocols were approved by the University of British Columbia Animal Care Committee (ACC) and followed the Canadian Council on Animal Care and Use guidelines (protocol A22-0054). A total of 56 mice (postnatal 104-140) were used in this study: 26 female and 30 male transgenic C57BL/6 mice expressing GCaMP6s were used. CLNF experiments (n=40, 17 females, 23 males) were done with tetO-GCaMP6s x CAMK tTA (Wekselblatt et al., 2016), and CLMF experiments (n=16, 9 females, 7 males) were done with Ai94 from the Allen Institute for Brain Science, crossed to Emx1–cre and CaMK2-tTA line (Jackson Labs) (Madisen et al., 2015). Mice were housed in a conventional facility in plastic cages with micro-isolator tops and kept on a normal 12 hr. light cycle with lights on at 7 AM. Most experiments were performed toward the end of the mouse light cycle. Mice that were unable to achieve a success rate of 70% after 7 days of training in CLNF experiments were excluded from the study (total 6 mice).</p>
</sec>
<sec id="s4b">
<title>Animal surgery, chronic transcranial window preparation</title>
<p>Animals were anesthetized with isoflurane, and a transcranial window was installed as previously described (<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>) and in an amended and more extensive protocol described here. A sterile field was created by placing a surgical drape over the previously cleaned surgical table, and surgical instruments were sterilized with a hot bead sterilizer for 20 s (Fine Science Tools; Model 18000–45). Mice were anesthetized with isoflurane (2% induction, 1.5% maintenance in air) and then mounted in a stereotactic frame with the skull level between lambda and bregma. The eyes were treated with eye lubricant (Lacrilube; <ext-link ext-link-type="uri" xlink:href="https://www.well.ca">www.well.ca</ext-link>) to keep the cornea moist, and body temperature was maintained at 37°C using a feedback-regulated heating pad monitored by a rectal probe. Lidocaine (0.1 ml, 0.2%) was injected under the scalp, and mice also received a 0.5 ml subcutaneous injection of a saline solution containing buprenorphine (2 mg/ml), atropine (3 μg/ml), and glucose (20 mM). The fur on the head of the mouse (from the cerebellar plate to near the eyes) was removed using a fine battery-powered beard trimmer, and the skin was prepared with a triple scrub of 0.1% Betadine in water followed by 70% ethanol. Respiration rate and response to toe pinch were checked every 10–15 min to maintain the surgical anesthetic plane.</p>
<p>Before starting the surgery, a cover glass was cut with a diamond pen (Thorlabs, Newton, NJ, USA; Cat#: S90W) to the size of the final cranial window (∼9 mm diameter). A skin flap extending over both hemispheres approximately 3 mm anterior to bregma and to the posterior end of the skull and down lateral was cut and removed. A #10 scalpel (curved) and sterile cotton tips were used to gently wipe off any fascia or connective tissue on the skull surface, making sure it was completely clear of debris and dry before proceeding. The clear version of C and B-Metabond (Parkell, Edgewood, NY, USA; Product: C and B Metabond) dental cement was prepared by mixing 1 scoop of C and B Metabond powder (Product: S399), 7 drops of C and B Metabond Quick Base (Product: S398), and one drop of C and B Universal catalyst (Product: S371) in a ceramic or glass dish (do not use plastic). Once the mixture reaches a consistency that makes it stick to the end of a wooden stir stick, a titanium fixation bar (22.2 × 2.7 × 3.2 mm) was placed so that there was a 4 mm posterior space between the bar edge and bregma, by applying a small amount of dental cement and holding it pressed against the skull until the cement partially dried (1–2 min). With the bar in place, a layer of dental adhesive was applied directly on the intact skull. The precut cover glass was gently placed on top of the mixture before it solidified (within 1 min), taking care to avoid bubble formation. If necessary, extra dental cement was applied around the edge of the cover slip to ensure that all the exposed bone was covered, and that the incision site was sealed at the edges. The skin naturally tightens itself around the craniotomy, and sutures are not necessary. The mixture remains transparent after it solidifies, and one should be able to clearly see large surface veins and arteries at the end of the procedure. Once the dental cement around the coverslip is completely solidified (up to 20 min), the animal received a second subcutaneous injection of saline (0.5 ml) with 20 mM of glucose and was allowed to recover in the home cage with an overhead heat lamp and intermittent monitoring (hourly for the first 4 hr. and every 4–8 hr. thereafter for activity level). Then, the mouse was allowed to recover for 7 days before task training.</p>
</sec>
<sec id="s4c">
<title>Water deprivation and habituation to experiments</title>
<p>Around 10–21 days after the surgery, the animals were placed on a schedule of water deprivation. Given the variation in weight due to initial ad libitum water consumption, the mouse weight was defined 24 hr. after the start of water restriction. If mice did not progress well through training, they were still given up to 1 ml of water daily, there was also a 15% maximal weight loss criterion used for supplementation (see detailed protocol). All mice were habituated for 5 days before data collection. Awake mice were head-fixed and placed in a dark imaging chamber for training and data collection for each session. High performing animals were able to maintain body weight and gain weight towards pre-surgery and water restriction values.</p>
</sec>
<sec id="s4d">
<title>CLNF setup and behavior experiments</title>
<p>An imaging rig was developed using two Raspberry Pi 4B+ single-board computers, designated as “brain-pi” (master device for initiating all recordings) for widefield GCaMP imaging and “behavior-pi” (slave device waiting for a trigger to start recording) for simultaneous behavior recording. Both devices were connected to the internet via Ethernet cables and communicated with each other through 3.3 V transistor-transistor logic (TTL) via general-purpose input outputs (GPIOs). To synchronize session initiation, GPIO pin #17 on the brain-pi (configured as an output) was connected to GPIO pin #17 on the behavior-pi (configured as an input), allowing the brain-pi to send a TTL signal to the behavior-pi at the start of each session.</p>
<p>Additional hardware components were integrated into the brain-pi setup. GPIO pin#27 (output) was connected to a solenoid (Gems Sensor, 45M6131) circuit to deliver water rewards, while GPIO pin#12 (output) was linked to a buzzer (Adafruit product #1739) positioned under the head-fixing apparatus to signal trial failures. GPIO pin#21 (output) was used to trigger a LED driver controlling both short blue (447.5 nm) and long blue (470 nm) LEDs, which were essential for cortical imaging. A speaker was connected to the brain-pi’s 3.5 mm audio jack to provide auditory output (PulseAudio driver) during the experiments. Both the brain-pi and behavior-pi devices were equipped with compatible external hard drives, connected via USB 3.0 ports, to store imaging and behavioral data, ensuring reliable data capture throughout the experimental sessions.</p>
<p>Mice with implanted transcranial windows on the dorsal cortex were head-fixed in a transparent acrylic tube (1.5-inch outer diameter, 1 ⅛ inch inner diameter) and placed such that the imaging camera (RGB Raspberry Pi Camera, OmniVision OV5647 CMOS sensor) was above the window, optimally focused on the cortical surface for GCaMP imaging. The GCaMP imaging cameras had lenses with a focal length of 3.6 mm and a field of view of ∼10.2 × 10.2 mm, leading to a pixel size of ∼40 μm, and were equipped with triple-bandpass filters (<underline>Chroma 69013m</underline>), which allowed for the separation of GCaMP epifluorescence signals and 447 nm reflectance signals into the green and blue channels, respectively. The depth of field (∼3 mm) was similar to previous reports (<xref ref-type="bibr" rid="c38">38</xref>), which provided both a large focal volume over which to collect fluorescence and made the system less sensitive to potential changes in the z-axis position. To reduce image file size, we binned data at 256 × 256 pixels on the camera for brain imaging data. Brain images were saved as 8-bit RGB stacks in HDF5 file format. We manually fixed the camera frame rate to 15 Hz, turned off automatic exposure and auto white balance, and set white balance gains to unity. For both green epifluorescence and blue reflection channels, we adjusted the intensity of illumination so that all values were below 180 out of 256 grey levels (higher levels increase the chance of cross-talk and saturation).</p>
<p>Mesoscale GCaMP imaging can often be performed with single-wavelength illumination (<xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c40">40</xref>). However, in this experiment, we utilized dual-LED illumination of the cortex (<xref ref-type="bibr" rid="c24">24</xref>). One LED (short-wavelength blue, 447 nm Royal Blue Luxeon Rebel LED SP-01-V4 paired with a Thorlabs FB 440-10 nm bandpass filter) monitored light reflectance to account for hemodynamic changes (<xref ref-type="bibr" rid="c41">41</xref>), while the second LED (long-wavelength blue, 470 nm Luxeon Rebel LED SP-01-B6 combined with a Chroma 480/30 nm filter) was used to excite GCaMP for green epifluorescence. Both signals were captured simultaneously using an RGB camera. For each mouse, light from both the excitation and reflectance LEDs was channeled into a single liquid light guide, positioned to illuminate the cortex (<xref rid="fig2" ref-type="fig">Figure 2v</xref>) (<xref ref-type="bibr" rid="c24">24</xref>). Further specifics are outlined in the accompanying Parts List and assembly instructions document. A custom-built LED driver, controlled by a Raspberry Pi, activated each LED at the beginning of the session and deactivated them at the session’s end. This on-off illumination shift was later used in post hoc analysis to synchronize frames from the brain and behavior cameras.</p>
<p>Before initiating the experimental session, key session parameters were configured in the config.ini file, as detailed in the “Key Configuration Parameters” section below. Additionally, we ensured that the waterspout was positioned appropriately for the mouse to access and consume the reward. To launch the experiment, open a terminal on the brain-pi (master) device, ensuring that the current directory is set to clopy/brain/. The experiment can be started by entering the command python3 &lt;script_name&gt;.py, where &lt;script_name&gt; corresponds to the appropriate Python script for the session (Supplementary Figure 6). We provide two pre-defined scripts in the codebase: one for 1ROI and another for 2ROI experiments. Both scripts are functionally similar. Upon execution, the script prompts for the “mouse_id,” which can be entered as an array of characters, followed by pressing ‘Enter.’ Upon initialization, two preview windows appear. The first window displays live captured images with intensity values overlaid in the green and blue channels, which allow for adjustments to LED brightness levels. The second window displays real-time ΔF/F<sub>0</sub> values overlaid on cortical locations, enabling brain alignment checks. These preview windows are used to assess imaging quality and ensure appropriate settings before starting the experiment (Supplementary Figure 6). Once all settings are confirmed, pressing the ‘Esc’ key starts the experiment. At this point, only one window displaying incoming images is shown. The experiment begins with a rest period of duration specified in the config.ini file, followed by alternating trial and rest periods. Data acquired during the session are saved in real-time on the device. For more detailed and the latest instructions on system setup, please refer to the project’s GitHub page.</p>
</sec>
<sec id="s4e">
<title>CLMF setup and behavior experiments</title>
<p>An additional imaging rig was developed utilizing an Nvidia-Jetson Orin device (8-core ARM Cortex CPU, 2048 CUDA cores, 64 GB memory), which served as the master device responsible for triggering all recordings. The Nvidia-Jetson was connected to an Omron Sentech STC-MCCM401U3V USB3 Vision camera for behavior imaging and real-time pose tracking. A personal computer (serving as the slave device) running EPIX software and frame grabber (PIXCI® E4 Camera Link Frame Grabber) was connected to a Pantera TF 1M60 CCD camera (Dalsa) for widefield GCaMP imaging. Both the Nvidia-Jetson and the EPIX PC were linked via transistor-transistor logic (TTL) for communication. For session synchronization, the GPIO pin #17 on the Nvidia-Jetson, configured as an output, was connected to the trigger input pin of the EPIX PC. This configuration enabled the Nvidia-Jetson to send a TTL signal to initiate frame capture on the EPIX system at the start of each session. Additionally, this same pin was connected to an LED driver to trigger the activation of a long blue LED (470 nm) for GCaMP imaging simultaneously.</p>
<p>Several hardware components were integrated into the Nvidia-Jetson setup to support experimental protocols. GPIO pin #13 (output) was linked to a solenoid circuit to deliver water rewards, while GPIO pin #7 (output) was connected to a buzzer placed under the head-fixation apparatus to signal trial failures. Auditory feedback during the experiments was provided through a speaker connected to the Nvidia-Jetson’s audio output. Both the Nvidia-Jetson and EPIX PC were equipped with sufficient storage space to ensure the reliable capture and storage of behavioral video recordings and widefield image stacks, respectively, throughout the experimental sessions.</p>
<p>Mice, implanted with a transcranial window over the dorsal cortex, were head-fixed in a custom-made transparent, rectangular chamber (dimensions provided). The chamber was designed using CAD software and fabricated from a 3 mm-thick acrylic sheet via laser cutting. Complete model files, acrylic sheet specifications, and assembly instructions are available on the GitHub repository. A mirror was positioned at the bottom and front of the chamber (<xref rid="fig2" ref-type="fig">Figure 2B</xref>) to allow multiple views of the mouse for improved tracking accuracy. The Omron Sentech camera was equipped with an infrared (IR)-only filter to exclusively capture IR illumination, effectively blocking other light sources and ensuring consistent behavioral imaging for accurate pose tracking. For widefield GCaMP imaging, the Dalsa camera was equipped with two front-to-front lenses (50 mm, f/1.435 mm and f/2; Nikon Nikkor) and a bandpass emission filter (525/36 nm, Chroma). The 12-bit images were captured at a frame rate of 30 Hz (exposure time of 33.3 ms) with 8×8 on-chip spatial binning (resulting in 128×128 pixels) using EPIX XCAP v3.8 imaging software. The cortex was illuminated using a blue LED (473 nm, Thorlabs) with a bandpass filter (467–499 nm) to excite calcium indicators, and the blue LED was synchronized with frame acquisition via TTL signaling. GCaMP fluorescence image stacks were automatically saved to disk upon session completion.</p>
<p>Prior to starting a session, key experimental parameters were configured within the config.ini file (see “Key Configuration Parameters” below for details). The waterspout was positioned close to the mouse to allow easy access for licking and consuming rewards. To initiate the experiment on the Nvidia-Jetson (master device), the terminal’s current directory was set to clopy/behavior/. The experiment was launched by executing the command python3 &lt;script_name&gt;.py, where &lt;script_name&gt; corresponds to the Python script responsible for running the experiment (Supplementary Figure 6). The provided script maps the speed of a specific control-point (FLL_bottom) to the audio feedback and executes 60 trials. Once launched, the program prompts for the “mouse_id,” which could be entered as an array of characters, followed by pressing Enter. A preview window displaying a live behavioral view was then presented, allowing for adjustments of IR brightness levels and the waterspout (Supplementary Figure 6). Once imaging quality and settings were confirmed to be optimal, pressing the “esc” key started the experiment. During the experiment, the tracked points were overlaid on the real-time video, and the session alternated between rest and trial periods. All acquired data were saved locally on the Nvidia-Jetson during the experiment and automatically transferred to the EPIX PC after completion. Additional setup instructions and system details can be found on the associated GitHub page.</p>
</sec>
<sec id="s4f">
<title>Key configuration parameters</title>
<p>Prior to running the experiment script, session parameters were set in the config.ini file under a designated configuration section. This file contains multiple config sections, each tailored to a different experiment type, such as brain-pi or behavior-pi. The appropriate config section is specified in the experiment script. Key parameters in this section include:
<list list-type="bullet">
<list-item><p>vid_source: Specifies the class responsible for the image stream, which may come from a programmable camera or another video source.</p></list-item>
<list-item><p>data_root: Directory path for saving the recorded sessions and current configuration.</p></list-item>
<list-item><p>raw_image_file: File name for saving the image stream.</p></list-item>
<list-item><p>resolution: Image stream resolution in (x, y) pixels.</p></list-item>
<list-item><p>framerate: Number of frames per second from the image stream.</p></list-item>
<list-item><p>awb_mode: Auto-white-balance mode (only used in CLNF, True for Behavior-Pi, False for Brain-Pi).</p></list-item>
<list-item><p>shutter_speed: Sets the camera sensor exposure.</p></list-item>
<list-item><p>dff_history: The duration (in seconds) used to calculate ΔF/F<sub>0</sub>, only used in CLNF.</p></list-item>
<list-item><p>ppmm: Pixels-per-mm value at the focal plane of the camera.</p></list-item>
<list-item><p>bregma: Y, X pixel coordinates of the bregma on the dorsal cortex in the image frame, only used in CLNF.</p></list-item>
<list-item><p>seeds_mm: List of cortical locations for inclusion in the closed-loop training rule, each defined by a name and coordinates relative to bregma (in mm), only used in CLNF.</p></list-item>
<list-item><p>roi_operation: Specifies the ROI(s) and operation (+ or -) for closed-loop training.</p></list-item>
<list-item><p>roi_size: Size of the ROI(s) in mm.</p></list-item>
<list-item><p>n_tones: Number of distinct audio frequencies for graded auditory feedback.</p></list-item>
<list-item><p>reward_delay: Delay (in seconds) after crossing the threshold.</p></list-item>
<list-item><p>reward_threshold: Threshold value in terms of ΔF/F<sub>0</sub>, determined from baseline sessions.</p></list-item>
<list-item><p>adaptive_threshold: A setting for adjusting the reward threshold. If set to 0, the threshold remains constant throughout the session; if set to 1, the threshold increases or decreases by 0.02 steps based on the reward rate.</p></list-item>
<list-item><p>total_trials: Total number of trials in the session.</p></list-item>
<list-item><p>max_trial_dur: Maximum trial duration (in seconds).</p></list-item>
<list-item><p>success_rest_dur: Rest duration after a successful trial (in seconds).</p></list-item>
<list-item><p>fail_rest_dur: Rest duration after a failed trial (in seconds).</p></list-item>
<list-item><p>initial_rest_dur: Rest period at the beginning of the session before the first trial (in seconds).</p></list-item>
<list-item><p>summary_file: File name to save the experiment summary as comma-separated values (CSV).</p></list-item>
<list-item><p>summary_header: List of variable names to be saved in the summary file.</p></list-item>
<list-item><p>dlc_model_path: Path of the DeepLabCut-Live model for real time pose tracking (used only in CLMF)</p></list-item>
<list-item><p>control_point: name of the tracked point for closed-loop feedback (used only in CLMF).</p></list-item>
<list-item><p>speed_threshold: Speed threshold for a success in a trial and receive reward (used only in CLMF).</p></list-item>
</list>
Further details on these parameters can be accessed via the GitHub repository.</p>
</sec>
<sec id="s4g">
<title>CLoPy platform</title>
<p>Closed-Loop Feedback Training System (CLoPy) is an open-source software and hardware system implemented in the Python (&gt;=3.8) programming language. This work is accompanied by a package to replicate the system, reproduce figures in this publication, and an extensive supplemental guide with full construction illustrations and parts lists to build the platform used. See <ext-link ext-link-type="uri" xlink:href="https://github.com/pankajkgupta/clopy">https://github.com/pankajkgupta/clopy</ext-link> for details and accompanying acquisition and analysis code. We also provide model files for machined and 3D-printed parts in the repository; links to neural and behavioral data can also be found at the federated research data repository-<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.20383/102.0400">https://doi.org/10.20383/102.0400</ext-link>.</p>
<p>While the presented CLNF experiments were conducted on a Raspberry Pi 4B+ device, the system can be used on any other platform where the Python runtime environment is supported. Similarly, CLMF experiments were conducted on an Nvidia Jetson Orin device, but it can be deployed on any other device with a GPU for real-time inference. For any programmable camera to be used with the system, one can implement a wrapper Python class that implements the CameraFactory interface functions for integration with the system.</p>
</sec>
<sec id="s4h">
<title>Graded feedback (Online ΔF/F<sub>0</sub>-Audio mapping)</title>
<p>The auditory feedback was proportional to the magnitude of the neural activity. We translated fluorescence levels into the appropriate feedback frequency and played the frequency on speakers mounted on two sides of the imaging platform. Frequencies used for auditory feedback ranged from 1 to 22 kHz in quarter-octave increments (<xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c42">42</xref>). When a target was hit, a circuit driven solenoid delivered a reward to mice.</p>
<p>To map the fluorescence activity <italic>F</italic> to the quarter-octave index <italic>n</italic>, we used linear scaling. Let <italic>F</italic><sub><italic>min</italic></sub>and <italic>F</italic><sub><italic>max</italic></sub>be the minimum and maximum fluorescence values.<italic>n</italic><sub><italic>max</italic></sub>The maximum number of quarter-octave steps between 1 kHz and 22 kHz.</p>
<p>The number of quarter-octave steps between 1 kHz and 22 kHz can be calculated as:
<disp-formula>
<graphic xlink:href="619716v1_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The quarter-octave index <italic>n</italic> can be mapped linearly from fluorescence activity <italic>F</italic> as:
<disp-formula>
<graphic xlink:href="619716v1_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Thus, the final equation mapping fluorescence activity to audio frequency is:
<disp-formula>
<graphic xlink:href="619716v1_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This equation allows us to map any fluorescence activity <italic>F</italic> within the range [<italic>F</italic><sub><italic>min</italic></sub>, <italic>F</italic><sub><italic>max</italic></sub>] to a corresponding frequency within the 1–22 kHz range, in quarter-octave increments.</p>
</sec>
<sec id="s4i">
<title>Checking the dynamic range of graded auditory feedback</title>
<p>To assess the dynamic range of audio signals from the speakers, we used an Ultra microphone (Dodotronic) to record feedback signals generated during a session, detect the frequencies, and compare the detected frequencies through our speakers to what was commanded through the program. We verified a linear relationship between 1 and 22 kilohertz (Supplementary figure 2). Analysis of GCAMP imaging experiments indicated that baseline activity values were associated with mapped sounds in the 6±4 kHz range. At reward points, which are threshold-dependent (Delta F over F values), the commanded auditory feedback values were significantly higher (18±2 kHz range).</p>
</sec>
<sec id="s4j">
<title>Determining reward threshold based on a baseline session</title>
<p>Before starting the experiments, and after the habituation period, a baseline session (day 0) of the same duration as the experiments is recorded. This session is similar to the experimental sessions in the coming days, except that they do not receive any rewards. Offline analysis of this session is used to establish a threshold value for the target activity (see the target activity section to read more). In brief, for 1 ROI experiments, target activity is the average ΔF/F<sub>0</sub> activity in that ROI. For 2 ROI experiments, target activity is based on the specified rule in the config.ini file. For example, target activity for the rule “ROI1-ROI2” would be “average ΔF/F<sub>0</sub> activity in ROI1 - average ΔF/F<sub>0</sub> activity in ROI2.”</p>
</sec>
<sec id="s4k">
<title>Determining ROI(s) for changes in CLNF task rules</title>
<p>Dorsal cortical widefield activity is dynamic, and ongoing spatiotemporal motifs involve multiple regions changing activity. Choosing ROI(s) for CLNF has some caveats and requires some considerations before choosing. It was relatively straightforward for initial training experiments where baseline (day 0) sessions were used to establish a threshold for the selected ROI on day 1. Mice learn to modulate the selected ROI over the training sessions. Interestingly, other cortical ROIs were also changing along with the target ROI as mice were learning the task. We needed to be careful when changing the ROI on day 11 because if we changed to an ROI that also changes along with the previous ROI, mice could keep getting rewards without realizing any change. To address this issue, we analyzed the neural data from day 1 to day 10 and found potential ROIs for which the threshold crossings did not increase significantly or were not on par with the previous ROI, and one of these ROIs was chosen for the rule change.</p>
</sec>
<sec id="s4l">
<title>Online ΔF/F<sub>0</sub> calculation</title>
<p>In calcium imaging, ΔF/F<sub>0</sub> is often used to represent the change in fluorescence relative to a baseline fluorescence (F<sub>0</sub>), which helps in normalizing the data. For CLNF real-time feedback, we computed ΔF/F<sub>0</sub> online, frame by frame.</p>
<p>Let <italic>F</italic>(<italic>t</italic>) represent the fluorescence signal at time.<italic>t</italic>. A running baseline <italic>F</italic><sub>0</sub>(<italic>t</italic>)is estimated by applying a sliding window to the fluorescence signal to calculate a moving average over a window of size <italic>N</italic>.
<disp-formula>
<graphic xlink:href="619716v1_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Once the running baseline.<italic>F</italic><sub>0</sub>(<italic>t</italic>)is computed, the ΔF/F₀ at time <italic>t</italic> is calculated as:
<disp-formula>
<graphic xlink:href="619716v1_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Each incoming frame contained both a green channel (capturing GCaMP6s fluorescence) and a short blue channel (representing blood volume reflectance) (<xref ref-type="bibr" rid="c43">43</xref>–<xref ref-type="bibr" rid="c45">45</xref>). These frames were appended to a doubly-ended queue (deque), a Python data structure, for running baseline <italic>F</italic><sub>0</sub>(<italic>t</italic>)The length of the deque.<italic>N</italic> was defined by the product of two parameters from the configuration file: “dff_history” (in seconds) and “framerate” (in frames per second), which determined the number of frames used for the running baseline. For the CLNF experiments, we used a dff_history of 6 s and a framerate of 15 fps, resulting in a deque length of 90 frames. When the deque reached full capacity, appending a new frame automatically removed the oldest frame, ensuring an updated running ΔF/F<sub>0</sub> throughout the experiment, regardless of session duration. This method effectively avoided memory limitations over time. For each channel (green and blue), the running F<sub>0</sub> was subtracted from each incoming frame to obtain ΔF, followed by ΔF/F<sub>0</sub> calculations, applied per pixel as a vectorized operation in Python. To correct for hemodynamic artifacts (<xref ref-type="bibr" rid="c43">43</xref>), we subtracted the blue excitation and epifluorescence channel ΔF/F<sub>0</sub> from the green channel ΔF/F<sub>0</sub>.</p>
<p>It is important to note that while blood volume reflectance is typically captured using green light (<xref ref-type="bibr" rid="c43">43</xref>), we used short blue light due to technical constraints associated with the Raspberry Pi camera’s rolling shutter which made strobing infeasible. The short blue light (447 nm) with a 440 ± 5 nm filter is close to the hemoglobin isosbestic point and has been shown to correlate well with the 530 nm green light signal as a proxy for hemodynamic activity (<xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c25">25</xref>). Additionally, the 447 nm LED would be expected to produce minimal green epifluorescence at the low power settings used in our experiments (<xref ref-type="bibr" rid="c46">46</xref>). Previous studies have evaluated and compared the performance of corrected versus uncorrected signals using this method (<xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c41">41</xref>).</p>
</sec>
<sec id="s4m">
<title>Offline ΔF/F<sub>0</sub> calculation</title>
<p>For offline analysis of GCaMP6s fluorescence in CLNF experiments, the green and blue channels (<xref ref-type="bibr" rid="c43">43</xref>–<xref ref-type="bibr" rid="c45">45</xref>) were converted to ΔF/F<sub>0</sub> values. For each channel, a baseline image (F<sub>0</sub>) was computed by averaging across all frames of the recording session. The F<sub>0</sub> was then subtracted from each individual frame producing a difference image (ΔF). This difference was divided by F<sub>0</sub>, resulting in the fractional change in intensity (ΔF/F<sub>0</sub>) for each pixel as a function of time. To further correct for hemodynamic artifacts (Ma et al., 2016), the blue channel reflected light ΔF/F<sub>0</sub> signal, reflecting blood volume changes, was subtracted from the green channel ΔF/F<sub>0</sub> signal, isolating the corrected GCaMP6s fluorescence response from any potential confounding vascular contributions.</p>
<p>Offline analysis of GCaMP6s fluorescence in CLMF experiments involved processing the green epifluorescence channel. We did not collect the hemodynamic signal in this experiment because we intended to employ the second channel for optogenetic stimulation (not included in this article). A baseline image (F<sub>0</sub>) was computed by averaging across all frames of the recording session. The F<sub>0</sub> was then subtracted from each individual frame, producing a difference image (ΔF). This difference was divided by F<sub>0</sub>, resulting in the fractional change in intensity (ΔF/F<sub>0</sub>) for each pixel as a function of time.</p>
</sec>
<sec id="s4n">
<title>Seed-pixel correlation matrices</title>
<p>Widefield cortical image stacks were registered to the Allen Mouse Brain Atlas (<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c48">48</xref>) and segmented into distinct regions, including the olfactory bulb (OB), anterolateral motor cortex (ALM), primary motor cortex (M1), secondary motor cortex (M2), sensory forelimb (FL), sensory hind limb (HL), barrel cortex (BC), primary visual cortex (V1), and retrosplenial cortex (RS) in both the left and right cortical hemispheres. The average activity in a 0.4 x 0.4 mm² area (equivalent to a 10 x 10 pixel region) centered on these regions (also referred to as seed pixels) was calculated, representing the activity within each region. These signals were then used to generate temporal plots. The temporal plots were epoched into trial and rest conditions, and correlations between the regions were computed, resulting in correlation matrices for each condition across each day. Each element of the matrix represented the pairwise correlation between two cortical regions. For each pair of cortical regions, we obtained correlation values during both trial and rest periods for every session, creating a time series (over sessions) with two conditions (trial and rest).</p>
</sec>
<sec id="s4o">
<title>Data and code availability</title>
<p>The source data used in this paper is available here- <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.20383/102.0400">https://doi.org/10.20383/102.0400</ext-link>. Code to replicate the system, recreate the figures, and associated pre-processed data are publicly available and hosted on GitHub - <ext-link ext-link-type="uri" xlink:href="https://github.com/pankajkgupta/clopy">https://github.com/pankajkgupta/clopy</ext-link>. Any additional information required to reanalyze the data reported in this work is available from the lead contact upon request.</p>
</sec>
<sec id="s4p">
<title>Statistics</title>
<p>Various statistical tests were performed to support the analysis presented in accompanying figures. For p-value matrices in <xref rid="fig8" ref-type="fig">Figure 8B</xref>, <xref rid="fig8" ref-type="fig">8C</xref>, a two-way repeated-measures ANOVA with Bonferroni post-hoc correction was used to test the significance of correlation changes across two factors: sessions (days 1– 10) and condition (trial vs. rest). Significant changes in the correlation matrices along these two variables showed complementary patterns (<xref rid="fig8" ref-type="fig">Figure 8</xref>). Changes across sessions involved the bilateral M1, M2, FL, HL, and BC regions, while changes between the trial and rest conditions involved bilateral OB, ALM, V1, and RS regions.</p>
<p>A list of statistical tests used in a figure, its purpose and data used are summarized in the table below.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>List of statistical tests performed</title></caption>
<graphic xlink:href="619716v1_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="619716v1_tbl1a.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="619716v1_tbl1b.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="619716v1_tbl1c.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s4q">
<title>CLNF rules</title>
<p>The task rules for CLNF experiments involved cortical activity at selected locations. These locations were standard Allen Mouse Brain Atlas (<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c48">48</xref>) CCF coordinates. The table below lists all the task rules we tested and what they meant in the context of CLNF experiments.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><title>List of CLNF task rules with their description</title></caption>
<graphic xlink:href="619716v1_tbl2.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="619716v1_tbl2a.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s4r">
<title>CLMF rules</title>
<p>The task rules for CLMF experiments involved points on body parts that were tracked (listed in results section). The table below lists the task rules (i.e. tracked points) we tested and what they meant in the context of CLMF experiments.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><title>List of CLMF task rules with their description</title></caption>
<graphic xlink:href="619716v1_tbl3.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4:</label>
<caption><title>List of mice – CLNF</title></caption>
<graphic xlink:href="619716v1_tbl4.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5:</label>
<caption><title>List of mice – CLMF</title></caption>
<graphic xlink:href="619716v1_tbl5.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by Canadian Institutes of Health Research (CIHR) foundation grant FDN-143209 and project grant PJT-180631 (to T.H.M.). T.H.M. was also supported by the Brain Canada Neurophotonics Platform, a Heart and Stroke Foundation of Canada grant in aid, the National Science and Engineering Council of Canada (NSERC; GPIN-2022-03723), and a Leducq Foundation grant. This work was supported by resources made available through the Dynamic Brain Circuits cluster and the NeuroImaging and NeuroComputation Centre at the UBC Djavad Mowafaghian Centre for Brain Health (RRID SCR_019086) and made use of the DataBinge forum, and computational resources and services provided by Advanced Research Computing (ARC) at the University of British Columbia. We thank Pumin Wang and Cindy Jiang for surgical assistance, Jamie Boyd and Jeffrey M LeDue for technical assistance.</p>
</ack>
<sec id="d1e1541" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1546">
<label>Supplemental Figs and powerpoint slide show animation</label>
<media xlink:href="supplements/619716_file02.zip"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clancy</surname> <given-names>KB</given-names></string-name>, <string-name><surname>Koralek</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Costa</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Feldman</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Carmena</surname> <given-names>JM</given-names></string-name></person-group>. <article-title>Volitional modulation of optically recorded calcium signals during neuroprosthetic learning</article-title>. <source>Nat Neurosci</source>. <year>2014</year> <month>Jun</month>;<volume>17</volume>(<issue>6</issue>):<fpage>807</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3712</pub-id></mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fetz</surname> <given-names>EE</given-names></string-name></person-group>. <article-title>Operant Conditioning of Cortical Unit Activity [Internet]</article-title>. Vol. <volume>163</volume>, <source>Science</source>. <year>1969</year>. p. <fpage>955</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1126/science.163.3870.955</pub-id></mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clancy</surname> <given-names>KB</given-names></string-name>, <string-name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></string-name></person-group>. <article-title>The sensory representation of causally controlled objects</article-title>. <source>Neuron</source>. <year>2021</year> <month>Feb</month> <day>17</day>;<volume>109</volume>(<issue>4</issue>):<fpage>677</fpage>–<lpage>89.e4.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2020.12.001</pub-id></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prsa</surname> <given-names>M</given-names></string-name>, <string-name><surname>Galiñanes</surname> <given-names>GL</given-names></string-name>, <string-name><surname>Huber</surname> <given-names>D</given-names></string-name></person-group>. <article-title>Rapid Integration of Artificial Sensory Feedback during Operant Conditioning of Motor Cortex Neurons</article-title>. <source>Neuron</source>. <year>2017</year> <month>Feb</month> <day>22</day>;<volume>93</volume>(<issue>4</issue>):<fpage>929</fpage>–<lpage>39.e6.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.023</pub-id></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Srinivasan</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Maimon</surname> <given-names>BE</given-names></string-name>, <string-name><surname>Diaz</surname> <given-names>M</given-names></string-name>, <string-name><surname>Song</surname> <given-names>H</given-names></string-name>, <string-name><surname>Herr</surname> <given-names>HM</given-names></string-name></person-group>. <article-title>Closed-loop functional optogenetic stimulation</article-title>. <source>Nat Commun</source>. <year>2018</year> <month>Dec</month> <day>13</day>;<volume>9</volume>(<issue>1</issue>):<fpage>5303</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-07721-w</pub-id></mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knudsen</surname> <given-names>EB</given-names></string-name>, <string-name><surname>Wallis</surname> <given-names>JD</given-names></string-name></person-group>. <article-title>Closed-loop theta stimulation in the orbitofrontal cortex prevents reward-based learning</article-title>. <source>Neuron</source>. <year>2020</year> <month>May</month> <day>6</day>;<volume>106</volume>(<issue>3</issue>):<fpage>537</fpage>–<lpage>47.e4.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2020.02.003</pub-id></mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paz</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Davidson</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Frechette</surname> <given-names>ES</given-names></string-name>, <string-name><surname>Delord</surname> <given-names>B</given-names></string-name>, <string-name><surname>Parada</surname> <given-names>I</given-names></string-name>, <string-name><surname>Peng</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Closed-loop optogenetic control of thalamus as a tool for interrupting seizures after cortical injury</article-title>. <source>Nat Neurosci</source>. <year>2013</year> <month>Jan</month>;<volume>16</volume>(<issue>1</issue>):<fpage>64</fpage>–<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3269</pub-id></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ching</surname> <given-names>S</given-names></string-name>, <string-name><surname>Liberman</surname> <given-names>MY</given-names></string-name>, <string-name><surname>Chemali</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Westover</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Kenny</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Solt</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Real-time closed-loop control in a rodent model of medically induced coma using burst suppression</article-title>. <source>Anesthesiology</source>. <year>2013</year> <month>Oct</month>;<volume>119</volume>(<issue>4</issue>):<fpage>848</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1097/ALN.0b013e31829d4ab4</pub-id></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rosin</surname> <given-names>B</given-names></string-name>, <string-name><surname>Slovik</surname> <given-names>M</given-names></string-name>, <string-name><surname>Mitelman</surname> <given-names>R</given-names></string-name>, <string-name><surname>Rivlin-Etzion</surname> <given-names>M</given-names></string-name>, <string-name><surname>Haber</surname> <given-names>SN</given-names></string-name>, <string-name><surname>Israel</surname> <given-names>Z</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Closed-loop deep brain stimulation is superior in ameliorating parkinsonism</article-title>. <source>Neuron</source>. <year>2011</year> <month>Oct</month> <day>20</day>;<volume>72</volume>(<issue>2</issue>):<fpage>370</fpage>–<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2011.08.023</pub-id></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Widge</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Moritz</surname> <given-names>CT</given-names></string-name></person-group>. <article-title>Pre-frontal control of closed-loop limbic neurostimulation by rodents using a brain-computer interface</article-title>. <source>J Neural Eng</source>. <year>2014</year> <month>Apr</month>;<volume>11</volume>(<issue>2</issue>):<fpage>024001</fpage>. <pub-id pub-id-type="doi">10.1088/1741-2560/11/2/024001</pub-id></mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname> <given-names>G</given-names></string-name>, <string-name><surname>Zeng</surname> <given-names>F</given-names></string-name>, <string-name><surname>McCartin</surname> <given-names>M</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Closed-loop stimulation using a multiregion brain-machine interface has analgesic effects in rodents</article-title>. <source>Sci Transl Med</source>. <year>2022</year> <month>Jun</month> <day>29</day>;<volume>14</volume>(<issue>651</issue>):<elocation-id>eabm5868</elocation-id>. <pub-id pub-id-type="doi">10.1126/scitranslmed.abm5868</pub-id></mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Luo</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>X</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>H</given-names></string-name>, <string-name><surname>Li</surname> <given-names>T</given-names></string-name>, <string-name><surname>He</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>JF</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Volitional modulation of neuronal activity in the external globus pallidus by engagement of the cortical-basal ganglia circuit</article-title>. <source>J Physiol [Internet]</source>. <year>2024</year> Jul 9; <ext-link ext-link-type="uri" xlink:href="https://physoc.onlinelibrary.wiley.com/doi/10.1113/JP286046">https://physoc.onlinelibrary.wiley.com/doi/10.1113/JP286046</ext-link></mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neely</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Koralek</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Athalye</surname> <given-names>VR</given-names></string-name>, <string-name><surname>Costa</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Carmena</surname> <given-names>JM</given-names></string-name></person-group>. <article-title>Volitional Modulation of Primary Visual Cortex Activity Requires the Basal Ganglia</article-title>. <source>Neuron</source>. <year>2018</year> <month>Mar</month> <day>21</day>;<volume>97</volume>(<issue>6</issue>):<fpage>1356</fpage>–<lpage>68.e4.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.051</pub-id></mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Forys</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Xiao</surname> <given-names>D</given-names></string-name>, <string-name><surname>Gupta</surname> <given-names>P</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>TH</given-names></string-name></person-group>. <article-title>Real-time selective markerless tracking of forepaws of head fixed mice using deep neural networks</article-title>. <source>eNeuro</source>. <year>2020</year> <month>Jun</month> <day>15</day>;<volume>7</volume>(<issue>3</issue>):<elocation-id>ENEURO.0096–20.2020</elocation-id>. <pub-id pub-id-type="doi">10.1523/ENEURO.0096-20.2020</pub-id></mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kane</surname> <given-names>GA</given-names></string-name>, <string-name><surname>Lopes</surname> <given-names>G</given-names></string-name>, <string-name><surname>Saunders</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>MW</given-names></string-name></person-group>. <article-title>Real-time, low-latency closed-loop feedback using markerless posture tracking</article-title>. <source>Elife</source>. <year>2020</year> <month>Dec</month> <day>8</day>;<volume>9</volume>. <pub-id pub-id-type="doi">10.7554/eLife.61909</pub-id></mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mamidanna</surname> <given-names>P</given-names></string-name>, <string-name><surname>Cury</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Abe</surname> <given-names>T</given-names></string-name>, <string-name><surname>Murthy</surname> <given-names>VN</given-names></string-name>, <string-name><surname>Mathis</surname> <given-names>MW</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat Neurosci</source>. <year>2018</year> <month>Sep</month>;<volume>21</volume>(<issue>9</issue>):<fpage>1281</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Makino</surname> <given-names>H</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>C</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>AN</given-names></string-name>, <string-name><surname>Kondapaneni</surname> <given-names>N</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>X</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Transformation of Cortex-wide Emergent Properties during Motor Learning</article-title>. <source>Neuron</source>. <year>2017</year> <month>May</month> <day>17</day>;<volume>94</volume>(<issue>4</issue>):<fpage>880</fpage>–<lpage>90.e8.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.015</pub-id></mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huber</surname> <given-names>D</given-names></string-name>, <string-name><surname>Gutnisky</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Peron</surname> <given-names>S</given-names></string-name>, <string-name><surname>O’Connor</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Wiegert</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Tian</surname> <given-names>L</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Multiple dynamic representations in the motor cortex during sensorimotor learning</article-title>. <source>Nature</source>. <year>2012</year> <month>Apr</month> <day>25</day>;<fpage>484</fpage>(<lpage>7395</lpage>):473–8. <pub-id pub-id-type="doi">10.1038/nature11039</pub-id></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname> <given-names>WE</given-names></string-name>, <string-name><surname>Kauvar</surname> <given-names>IV</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>MZ</given-names></string-name>, <string-name><surname>Richman</surname> <given-names>EB</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Chan</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Global Representations of Goal-Directed Behavior in Distinct Cell Types of Mouse Neocortex</article-title>. <source>Neuron</source>. <year>2017</year> <month>May</month> <day>17</day>;<volume>94</volume>(<issue>4</issue>):<fpage>891</fpage>– <lpage>907.e6.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.017</pub-id></mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Diedrichsen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Flanagan</surname> <given-names>JR</given-names></string-name></person-group>. <article-title>Principles of sensorimotor learning</article-title>. <source>Nat Rev Neurosci</source>. <year>2011</year> <month>Oct</month> <day>27</day>;<volume>12</volume>(<issue>12</issue>):<fpage>739</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3112</pub-id></mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krakauer</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Hadjiosif</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>AL</given-names></string-name>, <string-name><surname>Haith</surname> <given-names>AM.</given-names></string-name></person-group> <article-title>Motor Learning</article-title>. <source>Compr Physiol [Internet]</source>. <year>2019</year> <month>Mar</month> <day>14</day>;<volume>9</volume>(<issue>2</issue>):<fpage>613</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1002/cphy.c170043</pub-id></mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peters</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>SX</given-names></string-name>, <string-name><surname>Komiyama</surname> <given-names>T</given-names></string-name></person-group>. <article-title>Emergence of reproducible spatiotemporal activity during motor learning</article-title>. <source>Nature</source>. <year>2014</year> <month>Jun</month> <day>12</day>;<volume>510</volume>(<issue>7504</issue>):<fpage>263</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1038/nature13235</pub-id></mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ganguly</surname> <given-names>K</given-names></string-name>, <string-name><surname>Carmena</surname> <given-names>JM</given-names></string-name></person-group>. <article-title>Emergence of a stable cortical map for neuroprosthetic control</article-title>. <source>PLoS Biol</source>. <year>2009</year> <month>Jul</month>;<volume>7</volume>(<issue>7</issue>):<fpage>e1000153</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.1000153</pub-id></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Michelson</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Bolaños</surname> <given-names>F</given-names></string-name>, <string-name><surname>Bolaños</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Balbi</surname> <given-names>M</given-names></string-name>, <string-name><surname>LeDue</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>TH</given-names></string-name></person-group>. <article-title>Meso-Py: Dual Brain Cortical Calcium Imaging in Mice during Head-Fixed Social Stimulus Presentation</article-title>. <source>eNeuro</source>. <year>2023</year> <month>Dec</month>;<volume>10</volume>(<issue>12</issue>). <pub-id pub-id-type="doi">10.1523/ENEURO.0096-23.2023</pub-id></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murphy</surname> <given-names>TH</given-names></string-name>, <string-name><surname>Michelson</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Boyd</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Fong</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bolanos</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Bierbrauer</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Automated task training and longitudinal monitoring of mouse mesoscale cortical circuits using home cages</article-title>. <source>Elife</source>. <year>2020</year> <month>May</month> <day>15</day>;<volume>9</volume>. <pub-id pub-id-type="doi">10.7554/eLife.55964</pub-id></mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silasi</surname> <given-names>G</given-names></string-name>, <string-name><surname>Boyd</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Bolanos</surname> <given-names>F</given-names></string-name>, <string-name><surname>LeDue</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Scott</surname> <given-names>SH</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>TH</given-names></string-name></person-group>. <article-title>Individualized tracking of self-directed motor learning in group-housed mice performing a skilled lever positioning task in the home cage</article-title>. <source>J Neurophysiol</source>. <year>2018</year> <month>Jan</month> <day>1</day>;<volume>119</volume>(<issue>1</issue>):<fpage>337</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00115.2017</pub-id></mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dhillon</surname> <given-names>NS</given-names></string-name>, <string-name><surname>Sutandi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vishwanath</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lim</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Cao</surname> <given-names>H</given-names></string-name>, <string-name><surname>Si</surname> <given-names>D</given-names></string-name></person-group>. <article-title>A Raspberry Pi-Based Traumatic Brain Injury Detection System for Single-Channel Electroencephalogram</article-title>. <source>Sensors</source>. <year>2021</year> <month>Apr</month> <day>15</day>;<volume>21</volume>(<issue>8</issue>). <pub-id pub-id-type="doi">10.3390/s21082779</pub-id></mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akam</surname> <given-names>T</given-names></string-name>, <string-name><surname>Lustig</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rowland</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Kapanaiah</surname> <given-names>SK</given-names></string-name>, <string-name><surname>Esteve-Agraz</surname> <given-names>J</given-names></string-name>, <string-name><surname>Panniello</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Open-source, Python-based, hardware and software for controlling behavioural neuroscience experiments</article-title>. <source>Elife</source>. <year>2022</year> <month>Jan</month> <day>19</day>;<volume>11</volume>. <pub-id pub-id-type="doi">10.7554/eLife.67846</pub-id></mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname> <given-names>C</given-names></string-name>, <string-name><surname>Lavoie</surname> <given-names>A</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>SX</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>BH</given-names></string-name></person-group>. <article-title>Light Up the Brain: The Application of Optogenetics in Cell-Type Specific Dissection of Mouse Brain Circuits</article-title>. <source>Front Neural Circuits</source>. <year>2020</year> <month>Apr</month> <day>24</day>;<volume>14</volume>:<fpage>18</fpage>. <pub-id pub-id-type="doi">10.3389/fncir.2020.00018</pub-id></mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname> <given-names>C</given-names></string-name>, <string-name><surname>Mehler</surname> <given-names>DMA</given-names></string-name></person-group>. <article-title>Open science challenges, benefits and tips in early career and beyond</article-title>. <source>PLoS Biol</source>. <year>2019</year> <month>May</month>;<volume>17</volume>(<issue>5</issue>):<fpage>e3000246</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3000246</pub-id></mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>White</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Amarante</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Kravitz</surname> <given-names>AV</given-names></string-name>, <string-name><surname>Laubach</surname> <given-names>M</given-names></string-name></person-group>. <article-title>The Future Is Open: Open-Source Tools for Behavioral Neuroscience Research</article-title>. <source>eNeuro</source>. <year>2019</year> <month>Aug</month> <day>9</day>;<volume>6</volume>(<issue>4</issue>). <pub-id pub-id-type="doi">10.1523/ENEURO.0223-19.2019</pub-id></mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fetz</surname> <given-names>EE</given-names></string-name></person-group>. <article-title>Operant conditioning of cortical unit activity</article-title>. <source>Science</source>. <year>1969</year> <month>Feb</month> <day>28</day>;<fpage>163</fpage>(<lpage>3870</lpage>):955–8. <pub-id pub-id-type="doi">10.1126/science.163.3870.955</pub-id></mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neely</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Piech</surname> <given-names>DK</given-names></string-name>, <string-name><surname>Santacruz</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Maharbiz</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Carmena</surname> <given-names>JM</given-names></string-name></person-group>. <article-title>Recent advances in neural dust: towards a neural interface platform</article-title>. <source>Curr Opin Neurobiol</source>. <year>2018</year> <month>Jun</month>;<volume>50</volume>:<fpage>64</fpage>–<lpage>71</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2017.12.010</pub-id></mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Makino</surname> <given-names>H</given-names></string-name>, <string-name><surname>Hwang</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Hedrick</surname> <given-names>NG</given-names></string-name>, <string-name><surname>Komiyama</surname> <given-names>T</given-names></string-name></person-group>. <article-title>Circuit Mechanisms of Sensorimotor Learning</article-title>. <source>Neuron</source>. <year>2016</year> <month>Nov</month> <day>23</day>;<volume>92</volume>(<issue>4</issue>):<fpage>705</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.029</pub-id></mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname> <given-names>CB</given-names></string-name>, <string-name><surname>Celikel</surname> <given-names>T</given-names></string-name>, <string-name><surname>Feldman</surname> <given-names>DE</given-names></string-name></person-group>. <article-title>Long-term depression induced by sensory deprivation during cortical map plasticity in vivo</article-title>. <source>Nat Neurosci</source>. <year>2003</year> <month>Mar</month>;<volume>6</volume>(<issue>3</issue>):<fpage>291</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1038/nn1012</pub-id></mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silasi</surname> <given-names>G</given-names></string-name>, <string-name><surname>Xiao</surname> <given-names>D</given-names></string-name>, <string-name><surname>Vanni</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>ACN</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>TH</given-names></string-name></person-group>. <article-title>Intact skull chronic windows for mesoscopic wide-field imaging in awake mice</article-title>. <source>J Neurosci Methods</source>. <year>2016</year> <month>Jul</month>;<volume>267</volume>:<fpage>141</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.04.012</pub-id></mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vanni</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>TH</given-names></string-name></person-group>. <article-title>Mesoscale transcranial spontaneous activity mapping in GCaMP3 transgenic mice reveals extensive reciprocal connections between areas of somatomotor cortex</article-title>. <source>J Neurosci</source>. <year>2014</year> <month>Nov</month> <day>26</day>;<volume>34</volume>(<issue>48</issue>):<fpage>15931</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1818-14.2014</pub-id></mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lim</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Mohajerani</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Ledue</surname> <given-names>J</given-names></string-name>, <string-name><surname>Boyd</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>TH</given-names></string-name></person-group>. <article-title>In vivo large-scale cortical mapping using channelrhodopsin-2 stimulation in transgenic mice reveals asymmetric and reciprocal relationships between cortical areas</article-title>. <source>Front Neural Circuits</source>. <year>2012</year> <month>Mar</month> <day>15</day>;<volume>6</volume>:<fpage>11</fpage>. <pub-id pub-id-type="doi">10.3389/fncir.2012.00011</pub-id></mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilad</surname> <given-names>A</given-names></string-name>, <string-name><surname>Helmchen</surname> <given-names>F</given-names></string-name></person-group>. <article-title>Spatiotemporal refinement of signal flow through association cortex during learning</article-title>. <source>Nat Commun</source>. <year>2020</year> <month>Apr</month> <day>8</day>;<volume>11</volume>(<issue>1</issue>):<fpage>1744</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-020-15534-z</pub-id></mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nakai</surname> <given-names>N</given-names></string-name>, <string-name><surname>Sato</surname> <given-names>M</given-names></string-name>, <string-name><surname>Yamashita</surname> <given-names>O</given-names></string-name>, <string-name><surname>Sekine</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Fu</surname> <given-names>X</given-names></string-name>, <string-name><surname>Nakai</surname> <given-names>J</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Virtual reality-based real-time imaging reveals abnormal cortical dynamics during behavioral transitions in a mouse model of autism</article-title>. <source>Cell Rep</source>. <year>2023</year> <month>Apr</month> <day>25</day>;<volume>42</volume>(<issue>4</issue>):<fpage>112258</fpage>. <pub-id pub-id-type="doi">10.1016/j.celrep.2023.112258</pub-id></mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xiao</surname> <given-names>D</given-names></string-name>, <string-name><surname>Vanni</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Mitelut</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Chan</surname> <given-names>AW</given-names></string-name>, <string-name><surname>LeDue</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Xie</surname> <given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Mapping cortical mesoscopic networks of single spiking cortical or sub-cortical neurons</article-title>. <source>Elife</source>. <year>2017</year> <month>Feb</month> <day>4</day>;<volume>6</volume>. <pub-id pub-id-type="doi">10.7554/elife.19976</pub-id></mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Han</surname> <given-names>YK</given-names></string-name>, <string-name><surname>Köver</surname> <given-names>H</given-names></string-name>, <string-name><surname>Insanally</surname> <given-names>MN</given-names></string-name>, <string-name><surname>Semerdjian</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Bao</surname> <given-names>S</given-names></string-name></person-group>. <article-title>Early experience impairs perceptual discrimination</article-title>. <source>Nat Neurosci</source>. <year>2007</year> <month>Sep</month>;<volume>10</volume>(<issue>9</issue>):<fpage>1191</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1038/nn1941</pub-id></mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Shaik</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>SH</given-names></string-name>, <string-name><surname>Kozberg</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Thibodeaux</surname> <given-names>DN</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>HT</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Wide-field optical mapping of neural activity and brain haemodynamics: considerations and novel approaches</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2016</year> <month>Oct</month> <day>5</day>;<fpage>371</fpage>(<lpage>1705</lpage>). <pub-id pub-id-type="doi">10.1098/rstb.2015.0360</pub-id></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wekselblatt</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Flister</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Piscopo</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Niell</surname> <given-names>CM</given-names></string-name></person-group>. <article-title>Large-scale imaging of cortical dynamics during sensory perception and behavior</article-title>. <source>J Neurophysiol</source>. <year>2016</year> <month>Jun</month> <day>1</day>;<volume>115</volume>(<issue>6</issue>):<fpage>2852</fpage>–<lpage>66</lpage>. <pub-id pub-id-type="doi">10.1152/jn.01056.2015</pub-id></mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Valley</surname> <given-names>MT</given-names></string-name>, <string-name><surname>Moore</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Zhuang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mesa</surname> <given-names>N</given-names></string-name>, <string-name><surname>Castelli</surname> <given-names>D</given-names></string-name>, <string-name><surname>Sullivan</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Separation of hemodynamic signals from GCaMP fluorescence measured with wide-field imaging</article-title>. <source>J Neurophysiol</source>. <year>2020</year> <month>Jan</month> <day>1</day>;<volume>123</volume>(<issue>1</issue>):<fpage>356</fpage>–<lpage>66</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00304.2019</pub-id></mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dana</surname> <given-names>H</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>TW</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shields</surname> <given-names>BC</given-names></string-name>, <string-name><surname>Guo</surname> <given-names>C</given-names></string-name>, <string-name><surname>Looger</surname> <given-names>LL</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Thy1-GCaMP6 transgenic mice for neuronal population imaging in vivo</article-title>. <source>PLoS One</source>. <year>2014</year> <month>Sep</month> <day>24</day>;<volume>9</volume>(<issue>9</issue>):<fpage>e108697</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0108697</pub-id></mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Ding</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Li</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Royall</surname> <given-names>J</given-names></string-name>, <string-name><surname>Feng</surname> <given-names>D</given-names></string-name>, <string-name><surname>Lesnar</surname> <given-names>P</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>The Allen Mouse Brain Common Coordinate Framework: A 3D reference atlas</article-title>. <source>Cell</source>. <year>2020</year> <month>May</month> <day>14</day>;<volume>181</volume>(<issue>4</issue>):<fpage>936</fpage>–<lpage>53.e20.</lpage> <pub-id pub-id-type="doi">10.1016/j.cell.2020.04.007</pub-id></mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Chon</surname> <given-names>U</given-names></string-name>, <string-name><surname>Vanselow</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Cheng</surname> <given-names>KC</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>Y</given-names></string-name></person-group>. <article-title>Enhanced and unified anatomical labeling for a common mouse brain atlas [Internet]</article-title>. <source>bioRxiv</source>; <year>2019</year>. <pub-id pub-id-type="doi">10.1101/636175</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105070.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Gallego</surname>
<given-names>Juan Alvaro</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Imperial College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents a platform to implement closed-loop experiments in mice based on auditory feedback. The authors provide <bold>solid</bold> evidence that their platform enables a variety of closed-loop experiments using neural or movement signals, indicating that it will be a <bold>valuable</bold> resource to the neuroscience community. However, the demonstration experiments could be strengthened by increasing the sample size for several groups in the neurofeedback experiments, as well as a more thorough description of the results in the text.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105070.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors provide a resource to the systems neuroscience community, by offering their Python-based CLoPy platform for closed-loop feedback training. In addition to using neural feedback, as is common in these experiments, they include a capability to use real-time movement extracted from DeepLabCut as the control signal. The methods and repository are detailed for those who wish to use this resource. Furthermore, they demonstrate the efficacy of their system through a series of mesoscale calcium imaging experiments. These experiments use a large number of cortical regions for the control signal in the neural feedback setup, while the movement feedback experiments are analyzed more extensively.</p>
<p>Strengths:</p>
<p>The primary strength of the paper is the availability of their CLoPy platform. Currently, most closed-loop operant conditioning experiments are custom built by each lab and carry a relatively large startup cost to get running. This platform lowers the barrier to entry for closed-loop operant conditioning experiments, in addition to making the experiments more accessible to those with less technical expertise.</p>
<p>Another strength of the paper is the use of many different cortical regions as control signals for the neurofeedback experiments. Rodent operant conditioning experiments typically record from the motor cortex and maybe one other region. Here, the authors demonstrate that mice can volitionally control many different cortical regions not limited to those previously studied, recording across many regions in the same experiment. This demonstrates the relative flexibility of modulating neural dynamics, including in non-motor regions.</p>
<p>Finally, adapting the closed-loop platform to use real-time movement as a control signal is a nice addition. Incorporating movement kinematics into operant conditioning experiments has been a challenge due to the increased technical difficulties of extracting real-time kinematic data from video data at a latency where it can be used as a control signal for operant conditioning. In this paper they demonstrate that the mice can learn the task using their forelimb position, at a rate that is quicker than the neurofeedback experiments.</p>
<p>Weaknesses:</p>
<p>There are several weaknesses in the paper that diminish the impact of its strengths. First, the value of the CLoPy platform is not clearly articulated to the systems neuroscience community. Similarly, the resource could be better positioned within the context of the broader open-source neuroscience community. For an example of how to better frame this resource in these contexts, I recommend consulting the pyControl paper. Improving this framing will likely increase the accessibility and interest of this paper to a less technical neuroscience audience, for instance by highlighting the types of experimental questions CLoPy can enable.</p>
<p>While the dataset contains an impressive amount of animals and cortical regions for the neurofeedback experiment, and an analysis of the movement-feedback experiments, my excitement for these experiments is tempered by the relative incompleteness of the dataset, as well as its description and analysis in the text. For instance, in the neurofeedback experiment, many of these regions only have data from a single mouse, limiting the conclusions that can be drawn. Additionally, there is a lack of reporting of the quantitative results in the text of the document, which is needed to better understand the degree of the results. Finally, the writing of the results section could use some work, as it currently reads more like a methods section.</p>
<p>Suggestions for improved or additional experiments, data or analyses:</p>
<p>Not necessary for this paper, but it would be interesting to see if the CLNF group could learn without auditory feedback.</p>
<p>There are no quantitative results in the results section. I would add important results to help the reader better interpret the data. For example, in: &quot;Our results indicated that both training paradigms were able to lead mice to obtain a significantly larger number of rewards over time,&quot; You could show a number, with an appropriate comparison or statistical test, to demonstrate that learning was observed.</p>
<p>For: &quot;Performing this analysis indicated that the Raspberry Pi system could provide reliable graded feedback within ~63 {plus minus} 15 ms for CLNF experiments.&quot; The LED test shows the sending of the signal, but the actual delay for the audio generation might be longer. This is also longer than the 50 ms mentioned in the abstract.</p>
<p>It could be helpful to visualize an individual trial for each experiment type, for instance how the audio frequency changes as movement speed / calcium activity changes.</p>
<p>The sample sizes are small (n=1) for a few groups. I am excited by the variety of regions recorded, so it could be beneficial for the authors to collect a few more animals to beef up the sample sizes.</p>
<p>I am curious as to why 60 trials sessions were used. Was it mostly for the convenience of a 30 min session, or were the animals getting satiated? If the former, would learning have occurred more rapidly with longer sessions?</p>
<p>Figure 4 E is interesting, it seems like the changes in the distribution of deltaF was in both positive and negative directions, instead of just positive. I'd be curious as to the author's thoughts as to why this is the case. Relatedly, I don't see Figure 4E, and a few other subplots, mentioned in the text. As a general comment, I would address each subplot in the text.</p>
<p>For: &quot;In general, all ROIs assessed that encompassed sensory, pre-motor, and motor areas were capable of supporting increased reward rates over time,&quot; I would provide a visual summary showing the learning curves for the different types of regions.</p>
<p>Relatedly, I would further explain the fast vs slow learners, and if they mapped onto certain regions.</p>
<p>Also I would make the labels for these plots (e.g. Supp Fig3) more intuitive, versus the acronyms currently used.</p>
<p>The CLMF animals showed a decrease in latency across learning, what about the CLNF animals? There is currently no mention in the text or figures.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105070.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this work, Gupta &amp; Murphy present several parallel efforts. On one side, they present the hardware and software they use to build a head-fixed mouse experimental setup that they use to track in &quot;real-time&quot; the calcium activity in one or two spots at the surface of the cortex. On the other side, the present another setup that they use to take advantage of the &quot;real-time&quot; version of DeepLabCut with their mice. The hardware and software that they used/develop is described at length, both in the article and in a companion GitHub repository. Next, they present experimental work that they have done with these two setups, training mice to max out a virtual cursor to obtain a reward, by taking advantage of auditory tone feedback that is provided to the mice as they modulate either (1) their local cortical calcium activity, or (2) their limb position.</p>
<p>Strengths:</p>
<p>This work illustrates the fact that thanks to readily available experimental building blocks, body movement and calcium imaging can be carried using readily available components, including imaging the brain using an incredibly cheap consumer electronics RGB camera (RGB Raspberry Pi Camera). It is a useful source of information for researchers that may be interested in building a similar setup, given the highly detailed overview of the system. Finally, it further confirms previous findings regarding the operant conditioning of the calcium dynamics at the surface of the cortex (Clancy et al. 2020) and suggests an alternative based on deeplabcut to the motor tasks that aim to image the brain at the mesoscale during forelimb movements (Quarta et al. 2022).</p>
<p>Weaknesses:</p>
<p>This work covers 3 separate research endeavors: (1) The development of two separate setups, their corresponding software. (2) A study that is highly inspired from the Clancy et al. 2020 paper on the modulation of the local cortical activity measured through a mesoscale calcium imaging setup. (3) A study of the mesoscale dynamics of the cortex during forelimb movements learning. Sadly, the analyses of the physiological data appears uncomplete, and more generally the paper tends to offer overstatements regarding several points:</p>
<p>
- In contrast to the introductory statements of the article, closed-loop physiology in rodents is a well-established research topic. Beyond auditory feedback, this includes optogenetic feedback (O'Connor et al. 2013, Abbasi et al. 2018, 2023), electrical feedback in hippocampus (Girardeau et al. 2009), and much more.</p>
<p>
- The behavioral setups that are presented are representative of the state of the art in the field of mesoscale imaging/head fixed behavior community, rather than a highly innovative design. In particular, the closed-loop latency that they achieve (&gt;60 ms) may be perceived by the mice. This is in contrast with other available closed-loop setups.</p>
<p>
- Through the paper, there are several statements that point out how important it is to carry out this work in a closed-loop setting with an auditory feedback, but sadly there is no &quot;no feedback&quot; control in cortical conditioning experiments, while there is a no-feedback condition in the forelimb movement study, which shows that learning of the task can be achieved in the absence of feedback.</p>
<p>
- The analysis of the closed-loop neuronal data behavior lacks controls. Increased performance can be achieved by modulating actively only one of the two ROIs, this is not clearly analyzed (for instance looking at the timing of the calcium signal modulation across the two ROIs. It seems that overall ROIs1 and 2 covariate, in contrast to Clancy et al. 2020. How can this be explained?</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105070.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The study demonstrates the effectiveness of a cost-effective closed-loop feedback system for modulating brain activity and behavior in head-fixed mice. Authors have tested real-time closed-loop feedback system in head-fixed mice two types of graded feedback: 1) Closed-loop neurofeedback (CLNF), where feedback is derived from neuronal activity (calcium imaging), and 2) Closed-loop movement feedback (CLMF), where feedback is based on observed body movement. It is a python based opensource system, and authors call it CLoPy. The authors also claim to provide all software, hardware schematics, and protocols to adapt it to various experimental scenarios. This system is capable and can be adapted for a wide use case scenario.</p>
<p>Authors have shown that their system can control both positive (water drop) and negative reinforcement (buzzer-vibrator). This study also shows that using the close loop system mice have shown better performance, learnt arbitrary task and can adapt to change in the rule as well. By integrating real-time feedback based on cortical GCaMP imaging and behavior tracking authors have provided strong evidence that such closed-loop systems can be instrumental in exploring the dynamic interplay between brain activity and behavior.</p>
<p>Strengths:</p>
<p>Simplicity of feedback systems designed. Simplicity of implementation and potential adoption.</p>
<p>Weaknesses:</p>
<p>Long latencies, due to slow Ca2+ dynamics and slow imaging (15 FPS), may limit the application of the system.</p>
<p>Major comments:</p>
<p>(1) Page 5 paragraph 1: &quot;We tested our CLNF system on Raspberry Pi for its compactness, general-purpose input/output (GPIO) programmability, and wide community support, while the CLMF system was tested on an Nvidia Jetson GPU device.&quot; Can these programs and hardware be integrated with windows-based system and a microcontroller (Arduino/ Tency). As for the broad adaptability that's what a lot of labs would already have (please comment/discuss)?</p>
<p>(2) Hardware Constraints: The reliance on Raspberry Pi and Nvidia Jetson (is expensive) for real-time processing could introduce latency issues (~63 ms for CLNF and ~67 ms for CLMF). This latency might limit precision for faster or more complex behaviors, which authors should discuss in the discussion section.</p>
<p>(3) Neurofeedback Specificity: The task focuses on mesoscale imaging and ignores finer spatiotemporal details. Sub-second events might be significant in more nuanced behaviors. Can this be discussed in the discussion section?</p>
<p>(4) The activity over 6s is being averaged to determine if the threshold is being crossed before the reward is delivered. This is a rather long duration of time during which the mice may be exhibiting stereotyped behaviors that may result in the changes in DFF that are being observed. It would be interesting for the authors to compare (if data is available) the behavior of the mice in trials where they successfully crossed the threshold for reward delivery and in those trials where the threshold was not breached. How is this different from spontaneous behavior and behaviors exhibited when they are performing the test with CLNF?</p>
</body>
</sub-article>
</article>