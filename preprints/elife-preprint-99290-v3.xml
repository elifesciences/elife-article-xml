<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99290</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99290</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99290.3</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Neural dynamics of visual working memory representation during sensory distraction</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0003-8682-3920</contrib-id>
<name>
<surname>Degutis</surname>
<given-names>Jonas Karolis</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>j.karolis.degutis@maxplanckschools.de</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8440-1156</contrib-id>
<name>
<surname>Weber</surname>
<given-names>Simon</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8879-5666</contrib-id>
<name>
<surname>Soch</surname>
<given-names>Joram</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1786-6954</contrib-id>
<name>
<surname>Haynes</surname>
<given-names>John-Dylan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/001w7jn25</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin and Berlin Center for Advanced Neuroimaging, Charité Universitätsmedizin Berlin, corporate member of the Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff>
<aff id="a2"><label>2</label><institution>Max Planck School of Cognition</institution>, <city>Leipzig</city>, <country country="DE">Germany</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Department of Psychology, Humboldt-Universität zu Berlin</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Research Training Group “Extrospection” and Berlin School of Mind and Brain, Humboldt-Universität zu Berlin</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Institute of Psychology, Otto von Guericke University</institution></institution-wrap>, <city>Mageburg</city>, <country country="DE">Germany</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap>, <city>Leipzig</city>, <country country="DE">Germany</country></aff>
<aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043j0f473</institution-id><institution>German Center for Neurodegenerative Diseases</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff>
<aff id="a8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v4gjf40</institution-id><institution>Research Cluster of Excellence “Science of Intelligence”, Technische Universität Berlin</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff>
<aff id="a9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042aqky30</institution-id><institution>Collaborative Research Center “Volition and Cognitive Control”, Technische Universität Dresden</institution></institution-wrap>, <city>Dresden</city>, <country country="DE">Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Xue</surname>
<given-names>Gui</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-08-27">
<day>27</day>
<month>08</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-06-05">
<day>05</day>
<month>06</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99290</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-23">
<day>23</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-05-23">
<day>23</day>
<month>05</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.12.589170"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-08-27">
<day>27</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99290.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.99290.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99290.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99290.1.sa0">Reviewer #2 (Public Review):</self-uri>
</event>
<event>
<event-desc>Reviewed preprint v2</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-04-11">
<day>11</day>
<month>04</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99290.2"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.99290.2.sa3">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99290.2.sa2">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99290.2.sa1">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.99290.2.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Degutis et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Degutis et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99290-v3.pdf"/>
<abstract>
<title>Abstract</title><p>Recent studies have provided evidence for the concurrent encoding of sensory percepts and visual working memory contents (VWM) across visual areas; however, it has remained unclear how these two types of representations are concurrently present. Here, we reanalyzed an open-access fMRI dataset where participants memorized a sensory stimulus while simultaneously being presented with sensory distractors. First, we found that the VWM code in several visual regions did not fully generalize between different time points, suggesting a dynamic code. A more detailed analysis revealed that this was due to shifts in coding spaces across time. Second, we collapsed neural signals across time to assess the degree of interference between VWM contents and sensory distractors, specifically by testing the alignment of their encoding spaces. We find that VWM and feature-matching sensory distractors are encoded in coding spaces that do not fully overlap, but the separation decreases when distractors negatively impact behavioral performance in recalling the target. Together, these results indicate a role of dynamic coding and temporally stable coding spaces in helping multiplex perception and VWM within visual areas.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>visual working memory</kwd>
<kwd>dynamic coding</kwd>
<kwd>neural subspace</kwd>
<kwd>sensory distractor</kwd>
<kwd>multiplexing</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Small typos, an explanation for percent variance explained in Figures 2 and 3, and a further explanation of trial selection in one of the simulations.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>To successfully achieve behavioral goals, humans rely on the ability to remember, update, and ignore information. Visual working memory (VWM) allows for a brief maintenance of visual stimuli that are no longer present within the environment (<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref>). Previous studies have revealed that the contents of VWM are present throughout multiple visual areas, starting from V1 (<xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c12">12</xref>). These findings raised the question of how areas that are primarily involved in visual perception can also maintain VWM information without interference between the two contents. Recent studies that had participants remember a stimulus while simultaneously being presented with sensory stimuli during the delay period have found supporting evidence that both VWM contents and sensory percepts are multiplexed in occipital and parietal regions (<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref>). However, the mechanism employed in order to segregate bottom-up visual input from VWM contents remains poorly understood.</p>
<p>One proposed mechanism to achieve the separation between sensory and memory representations is dynamic coding (<xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c17">17</xref>): the change of the population code encoding VWM representations across time. Recent work has shown that the format of VWM might not be as persistent and stable throughout the delay as previously thought (<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref>). Frontal regions display dynamic population coding across the delay during the maintenance of category (<xref ref-type="bibr" rid="c20">20</xref>) and spatial contents in the absence of interference (<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref>), and also show dynamic recoding of the memoranda after sensory distraction (<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c24">24</xref>). The visual cortex in humans displays dynamic coding of contents during high load trials (<xref ref-type="bibr" rid="c25">25</xref>) and during a spatial VWM task (<xref ref-type="bibr" rid="c26">26</xref>). However, it is not yet clear whether dynamic coding of VWM might help evade sensory distraction in human visual areas.</p>
<p>Another line of evidence suggests that perception could potentially be segregated from VWM representations using stable non-overlapping coding spaces (<xref ref-type="bibr" rid="c27">27</xref>). For example, evidence from neuroanatomy indicates that the sensory bottom-up visual pathway primarily projects to the cytoarchitectonic Layer 4 in V1, while feedback projections culminate in superficial and deep layers of the cortex (<xref ref-type="bibr" rid="c28">28</xref>). Functional results are in line with neuroanatomy by showing that VWM signals preferentially activate the superficial and deep layers in humans (<xref ref-type="bibr" rid="c29">29</xref>) and non-human primates (<xref ref-type="bibr" rid="c30">30</xref>), while perceptual signals are more prevalent in the middle layers (<xref ref-type="bibr" rid="c31">31</xref>). In addition to laminar separation, regional multiplexing of multiple items could potentially rely on rotated representations, as seen in memory and sensory representations orthogonally coded in the auditory cortex (<xref ref-type="bibr" rid="c32">32</xref>) and in the storage of a sequence of multiple spatial locations in the prefrontal cortex (PFC) (<xref ref-type="bibr" rid="c33">33</xref>). Non-overlapping orthogonal representations have also been seen in both humans and trained recurrent neural networks as a way of segregating attended and unattended VWM representations (<xref ref-type="bibr" rid="c34">34</xref>–<xref ref-type="bibr" rid="c36">36</xref>).</p>
<p>Here we investigated whether the concurrent presence of VWM and sensory information is compatible with predictions offered by dynamic coding or by stable non-aligned coding spaces. For this, we reanalyzed an open-access fMRI dataset by Rademaker et al. (<xref ref-type="bibr" rid="c8">8</xref>) where participants performed a delayed-estimation VWM task with and without sensory distraction. To investigate dynamic coding we employed a temporal cross-decoding analysis that assessed how well the multivariate code encoding VWM generalizes from one time point to another (<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c37">37</xref>–<xref ref-type="bibr" rid="c39">39</xref>), and a temporal neural subspace analysis that examined a sensitive way of looking at alignment of neural populations coding for VWM at different time points. To assess the non-overlapping coding hypothesis, we used neural subspaces (<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c32">32</xref>) to see whether temporally stable representations of the VWM target and the sensory distractor are coded in separable neural populations. Finally, we examined the multivariate VWM code changes during distractor trials when compared to the no-distractor VWM format.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Temporal cross-decoding in distractor and no-distractor trials</title>
<p>In the previously published study (<xref ref-type="bibr" rid="c8">8</xref>) two groups of participants completed two VWM experiments where on a given trial they were asked to remember an orientation of a grating, which they had to then recall at the end of the trial. In the first experiment the delay period was either left blank (no-distractor) or a noise or randomly oriented grating distractor was presented (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). To investigate the dynamics of the VWM code, we examined how the multivariate pattern of activity encoding VWM memoranda changed across the duration of the delay period. To do so, we ran a temporal cross-decoding analysis where we trained a decoder (periodic support vector regression, see (<xref ref-type="bibr" rid="c40">40</xref>)) on the target orientation, separately for each time point and tested on all time points in turn in a cross-validated fashion. If the information encoding VWM memoranda were to have the same code, the trained decoder would generalize to other time points, indicated by similar decoding accuracies on the diagonal and off-diagonal elements of the matrix. However, if the code exhibited dynamic properties, despite information about the memonda being present (above-chance decoding on the diagonal of the matrix), both off-diagonal elements corresponding to a given on-diagonal element would have lower decoding accuracies (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). Such off-diagonal elements are considered an indication of a dynamic code.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Task and temporal cross-decoding.</title>
<p><bold>a)</bold> On each trial an oriented grating was presented for the 0.5 s followed by a delay period of 13 s (<xref ref-type="bibr" rid="c8">8</xref>). In a third of the trials a noise distractor was presented for 11 s during the middle of the delay; in another third another orientation grating was presented; one third of trials had no-distractor during the delay. <bold>b)</bold> Illustration of dynamic coding elements. An off-diagonal element had to have a lower decoding accuracy compared to both corresponding diagonal elements (see Methods for details). <bold>c)</bold> Temporal generalization of the multivariate code encoding VWM representations in three conditions across occipital and parietal regions. Across-participant mean temporal cross-decoding of no-distractor trials. Black outlines: matrix elements showing above-chance decoding (cluster-based permutation test; <italic>p</italic> &lt; 0.05). Blue outlines with dots: dynamic coding elements; parts of the cross-decoding matrix where the multivariate code fails to generalize (off-diagonal elements having lower decoding accuracy than their corresponding two diagonal elements; conjunction between two cluster-based permutation tests; <italic>p</italic> &lt; 0.05). <bold>d)</bold> Same as c), but noise distractor trials. <bold>e)</bold> Same as c), but orientation distractor trials. <bold>f)</bold> Dynamicism index; the proportion of dynamic coding elements across time. High values indicate a dynamic non-generalizing code, while low values indicate a generalizing code. Time indicates the time elapsed since the onset of the delay period.</p></caption>
<graphic xlink:href="589170v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We ran the temporal cross-decoding analysis in the first of two experiments for the three VWM delay conditions: no-distractor, noise distractor and orientation distractor (feature-matching distractor). First, we examined each element of the cross-decoding matrix to test whether decoding accuracies were above chance. In all three conditions and throughout all ROIs, we found clusters where decoding was above chance (<xref rid="fig1" ref-type="fig">Fig. 1c-e</xref>, black outline; nonparametric cluster-permutation test against null; all clusters <italic>p</italic> &lt; 0.05) from as early as 4 s after the onset of the delay period. We found that decoding on the diagonal was highest during no-distractor compared to noise and orientation distractor trials in most regions of interest (ROI; <xref rid="fig4" ref-type="fig">Fig. 4a</xref>).</p>
<p>Second, we examined off-diagonal elements to assess whether there was any indication that they reflected a non-generalizing dynamic code (see Methods for full details). Despite a high degree of temporal generalization, we found dynamic coding clusters in all three conditions. Some degree of dynamic coding was observed in all ROIs but LO2 in the noise distractor and no-distractor trials, while it was only present in V1, V2, V3, V4, and IPS in the orientation distractor condition (<xref rid="fig1" ref-type="fig">Fig. 1c-e</xref>, blue outline). The difference between noise and orientation distractor conditions could not be explained by the amount of information present in each ROI, as the decoding accuracy of the diagonal was similar across all ROIs in both the noise and orientation distractor conditions (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). We saw a nominally larger number of dynamic coding elements in V1, V2 and V3AB during the noise distractor condition and in V3 during the no-distractor condition (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>).</p>
<p>To qualitatively compare the amount of dynamic coding in the three conditions across the delay period, we calculated a dynamicism index (<xref ref-type="bibr" rid="c22">22</xref>) (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>; see Methods), which measured the multivariate code’s uniqueness at each time point; more precisely, the proportion of dynamic elements corresponding to each diagonal element. High values indicate dynamic code and low values indicate a generalizing code. Across all conditions, most dynamic elements occurred between the encoding and early delay periods (4-8 s), and the late delay and retrieval (14.4-16.8 s). Interestingly, during the noise distractor trials in V1 we also saw dynamic coding during the middle of the delay period; the multivariate code not only changed during the onset and offset of the noise stimulus, but also during its presentation and throughout the extent of the delay.</p>
<p>We also ran the same temporal cross-decoding analysis on the second experiment from (<xref ref-type="bibr" rid="c8">8</xref>). Participants performed the same type of VWM task with three conditions: a blank delay (no-distractor), a naturalistic distractor, and a flickering orientation distractor (<xref rid="figs1" ref-type="fig">Fig. S1a</xref>). The decoding accuracies across all conditions, including the no-distractor condition, were nominally lower in the second experiment compared to the first. In some ROIs the information about the target was not present during the delay period (chance decoding on the diagonal; <xref rid="figs1" ref-type="fig">Fig. S1b-d</xref>). Similarly to the first experiment’s orientation trials, the flickering orientation condition did not exhibit high levels of dynamic coding of the target (<xref rid="figs1" ref-type="fig">Fig. S1b-d</xref>). Due to the decreased amount of target information present across timepoints in the two distractor conditions (<xref rid="figs1" ref-type="fig">Fig. S1e</xref>), we do not further investigate the temporal dynamics of VWM storage in the second experiment.</p>
<p>We conducted several simulations to investigate whether changes in signal-to-noise ratio (SNR) could lead to dynamic coding clusters in our temporal cross-decoding analyses. This could occur if a decoder trained at one SNR fails to generalize to another, because each may rely on different features within the data. Specifically, we varied the added noise level in either a simulated dataset of voxel responses or the empirical results from V1 in the no-distractor or noise distractor trials in the first experiment. We then trained on a given noise level and tested on all other noise levels, corresponding to the temporal cross-decoding analysis. We see an absence of dynamic elements in the SNR cross-decoding matrix, as the decoding accuracy more strongly depends on the training data rather than test data. This results in some off-diagonal values in the decoding matrix that are higher, rather than smaller, than corresponding on-diagonal elements (<xref rid="figs9" ref-type="fig">Fig. S9a</xref>). In the second and third simulations, we used empirical data. To follow the initial decrease and subsequent increase in decoding accuracy found in most ROIs (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>), we initially decreased and then increased the SNR in the train and test axes of the matrix (<xref rid="figs9" ref-type="fig">Fig. S9b-c</xref>). Similarly to the first simulation, the cross-decoding matrix lacked dynamic elements, with the decoding accuracy of most off-diagonal elements exceeding their corresponding on-diagonal elements (<xref rid="figs9" ref-type="fig">Fig. S9b-c</xref>). These simulations indicate that SNR differences are unlikely to give rise to dynamic coding clusters.</p>
</sec>
<sec id="s2b">
<title>Dynamics of VWM neural subspaces across time</title>
<p>The temporal cross-decoding analysis of the first experiment revealed more dynamic coding in the early visual cortex primarily during the early and late delay phase and a more generalized coding throughout the delay in higher-order regions. In order to understand the nature of these effects in more detail, we conducted a separate series of analyses that directly assessed the neural subspaces in which the orientations were encoded and how these potentially changed across time. Specifically, we followed a previous methodological framework (<xref ref-type="bibr" rid="c26">26</xref>) and applied a principal component analysis (PCA) to the high-dimensional activity patterns at each time point to identify the two axes that explained maximal variance across orientations (see <xref rid="fig2" ref-type="fig">Fig. 2</xref> and Methods).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Assessing the dynamics of neural subspaces in V1-V3AB.</title>
<p><bold>a)</bold> Schematic illustration of the neural subspace analysis. A given data matrix (voxels x orientation bins) was subjected to a principal components analysis and the first two dimensions were used to define a neural subspace onto which a left-out test data matrix was projected. This resulted in a matrix of two coordinates for each orientation bin and was visualized (see right). The x and y axes indicate the first two principal components. Each color depicts an angular bin. <bold>b)</bold> Schematic illustration of the calculation of an above-baseline principal angle (aPA). A principal angle (PA) is the angle between the 2D PCA-based neural subspaces (as in <bold>a</bold>) for two different time points t<sub>1</sub>, t<sub>2</sub>. A small angle would indicate alignment of coding spaces; an angle of above-baseline would indicate a shift in the coding space. The aPA is the angle for a comparison between two time points (t<sub>1</sub>, t<sub>2</sub>) minus the angle between cross-validated pairs of the same time points. <bold>c)</bold> Each row shows a projection that was estimated for one of two time ranges (middle and late delay) and then applied to all time points (using independent, split-half cross-validated data). Opacity increases from early to late time points. For visualization purposes the subspaces were estimated on a participant-aggregated ROI (<xref ref-type="bibr" rid="c26">26</xref>). The axes represent the first two principal components, with labels indicating the percent of total explained variance. (<xref ref-type="fig" rid="figs2">Fig. S2</xref> depicts the same projections as neural trajectories. d) aPA between all pairwise time point comparisons (nonparametric permutation test against null; FDR-corrected <italic>p</italic> &lt; 0.05) averaged across 1,000 split-half iterations. Corresponding <italic>p</italic>-values found in <xref ref-type="table" rid="tbls1">Supplementary Table 1</xref>.</p></caption>
<graphic xlink:href="589170v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>First, we visualized the consistency of the neural subspaces across time. For this, we computed low-dimensional 2D neural subspaces for a given time point and projected left-out data from six time points during the delay onto this subspace (<xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c32">32</xref>). A projection of data from a single time point resulted in four orientation bin values placed within the subspace (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>, colored circles indicate orientation). Taking into account projected data from all timepoints, if the VWM code were generalizing, we would see a clustering of orientation points in a subspace; however, if orientation points were scattered around the neural subspace, this would show a non-generalizing code.</p>
<p>We examined the projections in a combined ROI spanning V1-V3AB aggregated across participants. We projected left-out data from all six time point bins onto subspaces generated from the early (7.2 s), middle (12 s), and late (16.8 s) time point data for each of the three conditions. Overall, the results showed generalization across time with some exceptions (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>, <xref rid="figs2" ref-type="fig">Fig. S2</xref>). The clustering of orientation bins in the no-distractor condition was most pronounced (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). In contrast, the noise distractor trials showed a resemblance of some degree of dynamic coding, as seen by less variance explained by early time points projected onto the middle subspace and the early and middle time points projected onto late subspace (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>, <xref rid="figs2" ref-type="fig">Fig. S2</xref>).</p>
<p>To quantify the visualized changes, we measured the alignment between each pair of subspaces by calculating the above-baseline principal angle (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>) within the combined V1-V3AB ROI. The above-baseline principal angle (aPA) measures the alignment between the 2D subspaces encoding the VWM representations: the higher the angle, the smaller the alignment between two subspaces and an indication of a changed neural coding space. Unlike in the projection of data from time points, the aPA was calculated participant-wise. Using a split-half approach, we measured the aPA between each split-pair of subspaces and subtracted the angles measured within each of the subspaces with the latter acting as a null baseline.</p>
<p>All three conditions showed significant aPAs (<xref rid="figs2" ref-type="fig">Fig, 2d</xref>; cyan stars; permutation test; <italic>p</italic> &lt; 0.05, FDR-corrected). Corresponding to the results from the cross-decoding analysis, the early (4.8s) and late (16.8) delay subspaces showed the highest number of significant pairwise aPAs in all conditions, with noise distractor trials having all pairwise aPAs including the early and late subspaces being significant. The three conditions each had two significant aPAs between timepoints in the middle of the delay period.</p>
</sec>
<sec id="s2c">
<title>Alignment between distractor and target subspaces in orientation distractor trials</title>
<p>Next, we assessed any similarity in encoding between the memorized orientation targets and the orientation distractors by focusing on those trials where both occurred. First, we examined whether the encoding of the sensory distractor in the first experiment is stable across its entire presentation duration (1.5 s - 12.5 s after target onset) using the same approach as for the VWM target (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>). We found stable coding of the distractor in all ROIs with only a few dynamic elements in V2 and V3 (<xref rid="figs3" ref-type="fig">Fig. S3</xref>). We then assessed whether the sensory distractor had a similar code to the VWM target by examining whether the multivariate code across time generalizes from the target to the distractor and vice versa. When cross-decoded, the sensory distractor (<xref rid="figs3" ref-type="fig">Fig. S3c</xref>) and target orientation (<xref rid="figs3" ref-type="fig">Fig. S3b</xref>) had lower decoding accuracies in the early visual cortex compared to when trained and tested on the same label-type, indicative of a non-generalizing code. Such a difference was not seen in higher-order visual regions, as the decoding of the sensory distractor was low to begin with (<xref rid="figs3" ref-type="fig">Fig. S3a-c</xref>).</p>
<p>Since we found minimal dynamics in the encoding of the distractor (<xref rid="figs3" ref-type="fig">Fig. S3a</xref>) and target (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>) in the first experiment, we focused on temporally stable neural subspaces that encoded the target and sensory distractor. We computed stable neural subspaces where we disregarded the temporal variance by averaging across the whole delay period and binned the trials either based on the target orientation (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, left subpanel) or the distractor orientation (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, right subpanel). We then projected left-out data binned based on the target (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, green quadrilateral) or the distractor (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, gray quadrilateral). This projection provided us with both a baseline (as when training and testing on the same label) and a cross-generalization. Unsurprisingly, the target subspace explained the left-out target data well (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, left subpanel, green quadrilateral); however, the target subspace explained less variance of the left-out distractor data (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, left subpanel, gray quadrilateral), as qualitatively seen from the smaller spread of the sensory distractor orientations. A similar but less pronounced dissociation between projections was seen in the distractor subspace (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, left, quadrilateral in green) with the distractor subspace better explaining the left-out distractor data. We quantified the difference between the target and distractor subspaces and found a significant aPA between them (<italic>p</italic> = 0.0297, one-tailed nonparametric permutation test; <xref rid="fig3" ref-type="fig">Fig. 3b</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Generalization between target and distractor codes in orientation distractor VWM trials in V1-V3AB.</title>
<p><bold>a)</bold> Left: projection of left-out target (green) and sensory distractor (gray) onto an orientation VWM target neural subspace. Right: same as left, but the projections are onto the sensory distractor subspace. The axes represent the first two principal components, with labels indicating the percent of total explained variance. <bold>b</bold>) Principal angle between the sensory distractor and orientation VWM target subspaces (p = 0.0297, one-tailed permutation test of sample mean), averaged across 1,000 split-half iterations. Errorbars indicate SEM across participants. <bold>c</bold>) Same as a), but for flickering orientation distractor trials in the second experiment. <bold>d</bold>) Same as b), but for flickering orientation distractor trials in the second experiment (<italic>p</italic> &lt; 0.001, one-tailed permutation test of sample mean). The same figure for individual ROIs can be seen in <xref ref-type="fig" rid="figs5">Fig. S5</xref>.</p></caption>
<graphic xlink:href="589170v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Cross-decoding between distractor and no-distractor conditions in Experiment 1.</title>
<p><bold>a)</bold> Decoding accuracy (feature continuous accuracy; FCA) across time for train and test on no-distractor trials (purple), train and test on noise distractor trials (dark green) and train and test on orientation distractor trials (light green). Horizontal lines indicate clusters where there is a difference between two time courses (all clusters <italic>p</italic> &lt; 0.05; nonparametric cluster permutation test, see color code on the right). <bold>b)</bold> Decoding accuracy as a proportion of no-distractor decoding estimated on the averaged delay period (4-16.8s). Nonparametric permutation tests compared the decoding accuracy of each analysis to the no-distractor decoding baseline (indicated as a dashed line) and between a decoder trained and tested on distractor trials (noise- or orientation-within) and a decoder trained on no-distractor trials and tested on distractor trials (noise or orientation-cross). FDR-corrected across ROIs. * <italic>p</italic> &lt; 0.05, *** <italic>p</italic> &lt; 0.001. Corresponding <italic>p</italic>-values found in <xref rid="tbls2" ref-type="table">Supplementary Table 2</xref>.</p></caption>
<graphic xlink:href="589170v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We ran the same analyses on the flickering orientation distractor trials in the second experiment, in which the flickering distractor trials negatively impacted target recall compared to the no-distractor trials (<xref ref-type="bibr" rid="c8">8</xref>). Similarly to the first experiment, we find reliable temporal cross-decoding of the distractor (<xref rid="figs4" ref-type="fig">Fig. S4a</xref>) and a non-generalizing code when trained and tested on the target and distractor, respectively, and vice versa (<xref rid="figs4" ref-type="fig">Fig. S4b-c</xref>). As the target (<xref rid="figs1" ref-type="fig">Fig. S1c</xref>) and distractor (<xref rid="figs4" ref-type="fig">Fig. S4a</xref>) displayed limited coding dynamics, we also computed stable neural subspaces of the target and distractor. As in the first experiment, the target subspace explained the left-out target data well (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>, left subpanel, green quadrilateral); however, the same subspace explained the left-out distractor better than in the first experiment (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>, left subpanel, gray quadrilateral), as seen from the larger overlap between the projected distractor and target quadrilaterals. Similarly, there was a large overlap between the projections in the distractor subspace (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>, right subpanel). The aPA was significant (<italic>p</italic> &lt; 0.001, one-tailed nonparametric permutation test; <xref rid="fig3" ref-type="fig">Fig. 3d</xref>), but nominally smaller than in the first experiment. These results from the first and second experiments provide evidence for the presence of separable stable neural subspaces that might enable the multiplexing of VWM and perception across the extent of the delay period and the separation of these subspaces is impacted by behavioral performance.</p>
</sec>
<sec id="s2d">
<title>Impact of distractors on VWM multivariate code</title>
<p>To further assess the impact of distractors on the available VWM information, we examined the decoding accuracies of distractor and no-distractor trials across time in the first experiment. Decoding accuracy was higher in the no-distractor trials compared to both orientation and noise distractor trials across all ROIs, but IPS (<xref rid="fig4" ref-type="fig">Fig. 4a, red</xref> and blue lines, <italic>p</italic> &lt; 0.05, cluster permutation test) across several stages of the delay period. To further assess how distractors affected the delay period information, we increased sensitivity by collapsing across it, because time courses were comparable in all conditions (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). To assess to which degree VWM encoding generalized from no-distractor to distractor trials, we trained a decoder on no-distractor trials and tested it on both types of distractor trials (<xref rid="fig4" ref-type="fig">Fig. 4b</xref> noise- and orientation-cross). We expressed the decoding accuracy of each distractor condition as a proportion of the decoding accuracy in the no-distractor condition. Values close to one indicate comparable information, while values below one mean the decoder does not generalize well. We found that the cross-decoding accuracies were significantly lower than the no-distractor in all ROIs but V4 (in both noise and orientation) and LO2 (only noise). Thus, in most areas the decoder did not generalize well from the no-distractor to distractor conditions. However, the total amount of information in distractor trials was generally slightly lower (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). Thus, we also compared the generalization to a decoder trained and tested on the same distractor condition (<xref rid="fig4" ref-type="fig">Fig. 4b</xref> noise- and orientation-within), which might thus be able to extract more information. We found that indeed information recovered in areas V2 and V3AB in the noise distractor condition</p>
<p>(<xref rid="fig4" ref-type="fig">Fig. 4b</xref>, pairwise permutation test). Thus, there was more information in the noise distractor condition, but it was not accessible to a decoder trained only on no-distractor trials. Additionally, a temporal cross-decoding analysis where all training time points were no-distractor trials had less dynamic coding in early visual regions (<xref rid="figs7" ref-type="fig">Fig. S7</xref>) when compared to the temporal cross-decoding matrix when trained and tested on noise distractor trials (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>). These results indicate a change in the VWM format between the noise distractor and no-distractor trials.</p>
<p>We find a similar pattern of results in the second experiment’s naturalistic distractor condition where there is a recovery of information in V3 and LO2 and LO1 in the flickering orientation distractor condition (<xref rid="figs6" ref-type="fig">Fig. S6</xref>).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We examined the dynamics of visual working memory (VWM) with and without distractors and explored the impact of sensory distractors on the coding spaces of VWM contents in visual areas by reanalyzing previously published data (<xref ref-type="bibr" rid="c8">8</xref>). In two experiments, participants completed a task during which they had to maintain an orientation stimulus in VWM. In the first experiment the delay period either had no-distractor, an orientation distractor, or a noise distractor were presented. The second experiment had either no-distractor, flickering orientation distractor or naturalistic distractor trials. We assessed two potential mechanisms that could help concurrently maintain the superimposed sensory and memory representations. First, we examined whether changes were observable in the multivariate code for memory contents across time, which we term dynamic coding. For this we used two different analyses: temporal cross-classification and a direct assessment of angles between coding spaces. We found evidence for dynamic coding in all conditions of the first experiment, but there were differences in these dynamics between conditions and regions. Dynamic coding was most pronounced during the noise distractor trials in early visual regions. Second, we assessed the complementary question of temporally stable coding spaces. We computed the stable neural subspaces by averaging across the delay period and saw that coding of the VWM target and concurrent sensory distractors occurred in different stable neural subspaces. This overlap was less pronounced in the second experiment where the flickering orientation distractor impacted behavioral performance when recalling the memory target (<xref ref-type="bibr" rid="c8">8</xref>). Finally, we observed that the format of the multivariate VWM code during the noise distraction differs from the VWM code when distractors were not present.</p>
<p>Dynamic encoding of VWM contents has been previously repeatedly examined. Temporal cross-decoding analyses have been used in a number of non-human primate electrophysiology and human fMRI studies (<xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c41">41</xref>). Spaak et al. (<xref ref-type="bibr" rid="c22">22</xref>) found dynamic coding in the non-human primate PFC during a spatial VWM task. They observed a change in the multivariate code between different stages; specifically a first shift between the encoding and maintenance periods, and also a second shift between the maintenance and retrieval periods. The initial transformation between the encoding and maintenance periods might recode the percept of the target into a stable VWM representation, whereas the second might transform the stable memoranda into a representation suited for initiation of motor output. A similar dynamic coding pattern was also observed in human visual regions using neuroimaging (<xref ref-type="bibr" rid="c26">26</xref>). In this study, in all three conditions we find a comparable pattern of results, where the multivariate code changes between the early delay and middle delay, and middle delay and late delay periods.</p>
<p>When noise distractors are added to the delay period we find evidence of additional coding shifts in V1 during the middle of the delay. Previous research in non-human primates has shown that the presentation of a distractor induces a change in multivariate encoding for VWM in lateral PFC (lPFC) (<xref ref-type="bibr" rid="c23">23</xref>). More precisely, a lack of generalization was observed between the population code encoding VWM before the presentation of a distractor (first half of the delay) and after its presentation (the second half). Additionally, continuous shifts in encoding have been observed in the extrastriate cortex throughout the extent of the delay period when decoding multiple remembered items at high VWM load (<xref ref-type="bibr" rid="c25">25</xref>). The dynamic code has been interpreted to enable multiplexing of representations when the visual cortex is overloaded by the maintenance of multiple stimuli at once. Future research could examine how properties of the distractor and of the target stimulus could interact to lead to dynamic coding. One intriguing hypothesis is that distractors that perturb the activity of feature channels that are used to encode VWM representations induce changes in its coding space over time. It is important to note that in the first experiment, the activation of the encoded target features was highest for the noise stimulus. Thus the shared spatial frequencies between noise distractor and the VWM contents potentially contribute to a more pronounced dynamic coding effect.</p>
<p>In a complementary analysis we directly assessed subspaces in which orientations were encoded in VWM. We defined the subspaces for three different time windows, early, middle and late. We find no evidence that the identity of orientations is confusable across time, e.g. we do not observe 45° at one given time point being recoded as 90° from a different time point. Such dynamics have been previously observed in the rotation of projected angles within a fixed neural subspace (<xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c34">34</xref>). Rather, we find a decreased generalization between neural subspaces at different time points, as previously observed in a spatial VWM task (<xref ref-type="bibr" rid="c26">26</xref>). These results suggest that the temporal dynamics across the VWM trial periods are driven by changes in the coding subspace of VWM. We do observe a preservation of the topology of the projected angles, as more similar angles remained closer together (e.g. the bin containing 45° was always closer to the bin containing 0° and 90°). Such a topology has been seen in V4 during a color perception task (<xref ref-type="bibr" rid="c42">42</xref>).</p>
<p>We also find evidence that the VWM contents are encoded in a different way depending on whether a noise distractor is presented or not. The decoder trained on no-distractor trials does not generalize well, presumably because it fails to fully access all the information present in noise distractor trials. If the decoders are trained directly on the distractor conditions the VWM related information is much higher. Additionally, we see that the code generalizes better across time when training on no-distractor trial time points and testing on noise distractor trials. This may imply that by training our decoder on the no-distractor trials we are able to uncover an underlying stable population code encoding VWM in noise distractor trials. Consistent with this finding, Murray et al. (<xref ref-type="bibr" rid="c21">21</xref>) demonstrated that subspaces derived on the delay period could still generalize to the more dynamic encoding and retrieval periods, albeit not perfectly.</p>
<p>Interestingly, we found limited dynamic coding in the orientation distractor condition; primarily a change in the code between the early delay and middle delay periods was observed. Nonetheless, we find distinct temporally stable coding spaces in which sensory distractors and memory targets are encoded. These results correspond to prior research demonstrating a rotated format between perception and memory representations (<xref ref-type="bibr" rid="c32">32</xref>), attended and unattended VWM representations in both humans and recurrent-neural networks trained on a 2-back VWM (<xref ref-type="bibr" rid="c34">34</xref>) and serial retro-cueing tasks (<xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c43">43</xref>). Additionally, similar rotation dynamics have been observed between multiple spatial VWM locations stored in the non-human primate lPFC (<xref ref-type="bibr" rid="c33">33</xref>). Considering the consistency of these results across different paradigms, we speculate that separate coding spaces might be a general mechanism of how feature-matching items can be concurrently multiplexed within visual regions. With growing evidence of the relationship between VWM capacity and neural resources available within the visual cortex (<xref ref-type="bibr" rid="c44">44</xref>–<xref ref-type="bibr" rid="c46">46</xref>), further research could examine the number of feature-matching items that can be stored in non-aligned coding spaces.</p>
<p>In this study, the first experiment did not yield a behavioral deficit in the feature-matching orientation distractor trials, while the second experiment impacted target recall in the feature-matching flickering orientation trials (<xref ref-type="bibr" rid="c8">8</xref>). There is evidence from behavioral and neural studies that show interactions between perception and VWM: feature-matching distractors behaviorally bias retrieved VWM contents (<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c48">48</xref>); VWM representations influence perception (<xref ref-type="bibr" rid="c49">49</xref>–<xref ref-type="bibr" rid="c52">52</xref>); neural visual VWM representations in the early visual cortices are biased towards distractors (<xref ref-type="bibr" rid="c53">53</xref>); and the fidelity of VWM neural representations within the visual cortex negatively correlates with behavioral errors when recalling VWM during a sensory distraction task (<xref ref-type="bibr" rid="c54">54</xref>). In cases where a distractor does induce a drop in recall accuracy or biases the recalled VWM target, VWM and the sensory distractor neural subspaces might overlap more, as we see in the second experiment. However, it remains to be seen whether the degree of change or rotation between subspaces correlates with trial-to-trial behavior.</p>
<p>To our surprise, we did not observe a significant difference in the coding format of VWM between orientation distractor and no-distractor trials. Our initial expectation was that the VWM coding might undergo changes due to the target representation avoiding the distractor stimulus. However, the presence of a generalizing code between no-distractor and orientation distractor trials both in the first and second experiments, along with the non-aligned coding spaces between the target and distractor in both orientation trials, suggests an alternative explanation. We suggest that the sensory distractor stimulus occupies a distinct coding space throughout its presentation during the delay, while the coding space of the target remains the same in both orientation and no-distractor trials. Layer-specific coding differences in perception and VWM might explain these findings (<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c55">55</xref>). Specifically, the sensory distractor neural subspace might predominantly reside in the bottom-up middle layers of early visual cortices, while the neural subspace encoding VWM might primarily occupy the superficial and deep layers.</p>
<p>We provide evidence for two types of mechanisms found in visual areas during the presence of both VWM and sensory distractors. First, our findings show dynamic coding of VWM within the human visual cortex during sensory distraction and indicate that such activity is not only present within the lPFC. Second, we find that VWM and feature-matching sensory distractors are encoded in shifted coding spaces, but the overlap between these subspaces increases in trials that negatively affect recall fidelity. Taking into account previous findings, we posit that different coding spaces within the same region might be a more general mechanism of segregating feature-matching stimuli. In sum, these results provide possible mechanisms of how VWM and perception are concurrently present within visual areas.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants, stimuli, procedure, and preprocessing</title>
<p>The following section is a brief explanation of parts of the methods covered in Rademaker et al. (<xref ref-type="bibr" rid="c8">8</xref>). Readers may refer to that paper for details. We reanalyzed data from Experiment 1 and 2.</p>
<p>In Experiment 1 six participants performed two tasks while in the scanner: a VWM task and a perceptual localizer task. In the perceptual localizer task, either a donut-shaped or a circle-shaped grating was presented in 9 second blocks. The participants had to respond whenever the grating dimmed. There were a total of 20 donut-shaped and 20 circle-shaped gratings in one run. Participants completed a total of 15-17 runs.</p>
<p>The visual VWM task began with the presentation of a colored 100% valid cue which indicated the type of trial: no-distractor, orientation distractor, or noise distractor. Following the cue, the target orientation grating was presented centrally for 500 ms, followed by a 13 s delay period. In the trials with the distractor, a stimulus of the same shape and size as the target grating was presented centrally for 11 s in the middle of the delay period (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). The orientation and noise distractors reversed contrast at 4 Hz. At the end of the delay, a probe stimulus bar appeared at a random orientation. The participants had to align the bar to the target orientation and had to respond in 3 s. The orientations for the VWM sample were pseudo-randomly chosen from six orientation bins each consisting of 30 orientations. The orientation distractor and sample were counterbalanced in order not to have the same orientation presented as a distractor. Each run consisted of four trials of each condition. Across three sessions participants completed 27 runs of the task resulting in a total of 108 trials per condition.</p>
<p>In Experiment 2, seven participants performed the same type of VWM task with three trial types: no-distractor, flickering orientation distractor, and a naturalistic (gazebo or face) distractor (<xref rid="figs1" ref-type="fig">Fig. S1a</xref>). The memory target presentation was the same as in Experiment 1. However, unlike Experiment 1, where distractors exhibited contrast reversals, the distractors in Experiment 2 alternated between being displayed on and off at 4 Hz. In the naturalistic distractor condition, a full set of 22 unique face images or 22 unique gazebo images was presented in a randomly shuffled sequence. In the flickering grating distractor condition, 22 gratings were shown, each sharing the same orientation but with a randomly assigned phase. As in Experiment 1, the orientations of both the target and distractor gratings were pseudo-randomly drawn from six predefined orientation bins.</p>
<p>The data were acquired using a simultaneous multi-slice EPI sequence with a TR of 800 ms, TE of 35 ms, flip angle of 52°, and isotropic voxels of 2 mm. The data were preprocessed using FreeSurfer and FSL and time-series were z-scored across time for each voxel.</p>
</sec>
<sec id="s4b">
<title>Voxel selection</title>
<p>We used the same regions of interest (ROI) as in Rademaker et al. (<xref ref-type="bibr" rid="c8">8</xref>), which were derived using retinotopic mapping. In contrast to the original study, we reduced the size of our ROIs by selecting voxels that reliably responded to both the donut-shaped orientation perception task and the no-distractor VWM task. In order to select reliably activating voxels, we calculated four tuning functions for each voxel: two from the perceptual localizer and two from the no-distractor VWM task. The tuning functions spanned the continuous feature space in bins of 30°. Thus, to calculate the tuning functions, we ran a split-half analysis using stratified sampling where we binned all trials into six bins (of 30°). For both halves, tuning functions were estimated using a GLM that included six orientation regressors (one for each bin) and assumed an additive noise component independent and identically distributed across trials. We calculated Pearson correlations between the no-distractor memory and the perception tuning functions across the six parameter estimates extracted from the GLM, thus generating one memory-memory and one perception-perception correlation coefficient for each voxel.</p>
<p>The same analysis was additionally performed 1,000 times on randomly permuted orientation labels to generate a null distribution for each participant and each ROI. These distributions were used to check for the reliability of voxel activation to perception and no-distractor VWM. After performing Fisher z-transformation on the correlations, we selected voxels that had a value above the 75th percentile of the null distributions in both the memory-memory and perception-perception correlations. This population of voxels was then used for all subsequent analyses. IPS included reliable voxels from retinotopically derived IPS0, IPS1, and IPS2. The number and proportion of voxels selected for each experiment is seen in <xref rid="figs8" ref-type="fig">Fig. S8</xref>.</p>
</sec>
<sec id="s4c">
<title>Periodic support vector regression</title>
<p>We used periodic support vector regression (pSVR) to predict the target orientation from the multivariate BOLD activity (<xref ref-type="bibr" rid="c40">40</xref>). PSVR uses a regression approach to estimate the sine and cosine components of a given orientation independently and therefore accounts for the circular nature of stimuli. In order to have a proper periodic function, orientation labels from the range [0°, 180°) were projected into the range [0, 2π).</p>
<p>We used the support vector regression algorithm using a non-linear radial basis function (RBF) kernel implemented in LIBSVM (<xref ref-type="bibr" rid="c56">56</xref>) for orientation decoding. Specifically, sine and cosine components of the presented orientations were predicted based on multivariate fMRI signals from a set of voxels at specific time points within a trial (see <italic>Temporal Generalization</italic>). In each cross-validation fold, we rescaled the training data voxel activation into the range [0, 1] and applied the training data parameters to rescale the test data. For each participant we had a total of three iterations in our cross-validation, where we trained on two thirds (i.e. two sessions) and tested on one third of the data (i.e. the left-out session). We selected three iterations in order to mitigate training and test data leakage (see <italic>Temporal Generalization</italic>).</p>
<p>After pSVR-based analysis, reconstructed orientations were obtained by plugging the predicted sine and cosine components into the four-quadrant inverse tangent:
<disp-formula>
<graphic xlink:href="589170v4_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>x</italic><sub><italic>P</italic></sub> and <italic>y</italic><sub><italic>P</italic></sub> are pSVR outputs in the test set. Prediction accuracy was measured as the trial-wise absolute angular deviation between predicted orientation and actual orientation:
<disp-formula>
<graphic xlink:href="589170v4_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where θ is the labeled orientation and <italic>θ</italic><sub><italic>P</italic></sub> is the predicted orientation. This measure was then transformed into a trial-wise feature continuous accuracy (FCA) (<xref ref-type="bibr" rid="c57">57</xref>) as follows:
<disp-formula>
<graphic xlink:href="589170v4_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The final across-trial accuracy was the mean of the trial-wise FCAs. Mean FCA was calculated across predicted orientations from all test sets after cross-validation was complete. The FCA is an equivalent measurement to standard accuracy measured in decoding analyses falling into the range between 0 and 100%, but extended to the continuous domain. In the case of random guessing, the expected angular deviation is π/2, resulting in chance-level FCA at 50%.</p>
</sec>
<sec id="s4d">
<title>Temporal cross-decoding</title>
<p>To determine the underlying stability of the VWM code, we ran a temporal cross-decoding analysis using pSVR (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). We trained on data from a given time point and then predicted orientations for all time points, using the presented targets as labels. We trained on two-thirds of the trials per iteration and tested on the left-out third. Training and test data were never taken from the same trials, both when testing on the same and different time points.</p>
<p>We used a cluster-based approach to test for significance for above-chance decoding clusters (<xref ref-type="bibr" rid="c58">58</xref>). To determine whether the size of the cluster of the above-chace values was significantly larger than chance, we calculated a summed t-value for each cluster. We then generated a null distribution by randomly permuting the sign of the estimated above-chance accuracy (each FCA value was subtracted by 50%, such that 0 corresponds to chance level) of all components within the temporal cross-decoding matrix. We calculated the summed t-value for the largest randomly occurring above-chance cluster. This procedure was repeated 1000 times to estimate a null distribution. The empirical summed t-value of each cluster was then compared to the null distribution to determine significance (<italic>p</italic> &lt; 0.05; without control of multiple cluster comparisons).</p>
<p>Dynamic coding clusters were defined as elements within the temporal cross-decoding matrix where the multivariate code at a given time point did not fully generalize to another time point; in other words, an off-diagonal element was significantly smaller in accuracy compared to its two corresponding on-diagonal elements (<italic>a<sub>ij</sub> &lt; a<sub>ii</sub></italic> and <italic>a<sub>ij</sub> &lt; a<sub>jj</sub></italic>, <xref rid="fig1" ref-type="fig">Fig. 1b</xref>). In order to test for significance of these clusters, we ran two cluster-permutation tests as done in previous studies to define dynamic clusters (<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c26">26</xref>). In each test, we subtracted one or the other corresponding diagonal elements from the off-diagonal elements (<italic>a<sub>ij</sub> – a<sub>ii</sub></italic> and <italic>a<sub>ij</sub> – a<sub>jj</sub></italic>). We then ran the same sign permutation test as for the above-chance decoding cluster for both comparisons. An off-diagonal element was deemed dynamic, if both tests were significant (<italic>p</italic> &lt; 0.05) and both corresponding diagonal elements were part of the above-chance decoding cluster.</p>
<p>Following (<xref ref-type="bibr" rid="c22">22</xref>), we also computed the dynamicism index as a proportion of elements across time that were dynamic. Specifically, we calculated the proportion of (off-diagonal) dynamic elements corresponding to a diagonal time point in both columns (corresponding to the test time points) and rows (corresponding to the train time points) of the temporal cross-decoding matrix.</p>
</sec>
<sec id="s4e">
<title>Temporal cross-decoding simulations</title>
<p>To address the possibility that the dynamic clusters in the temporal cross-decoding analysis might arise as a result of the decoder picking up on different features within the signal as a function of the signal-to-noise ratio (SNR), we ran three temporal cross-decoding simulations where the train and test data had varying levels of SNR.</p>
<p>In the first simulation, we created a dataset of 200 voxels that had a sine or cosine response function to orientations between 1° to 180°, the same orientations as the remembered target. A circular shift was applied to each voxel to vary preferred (or maximal) responses of each simulated voxel. This resulted in a dataset that captured the neural population’s response to all orientations. We then assessed the decoding performance under different SNR conditions during training and testing. In total, we ran seven iterations of the simulation, which correspond to the number of subjects in the second experiment. For each iteration, we randomly selected 108 responses from the full set of 180 for training, and then independently sampled another 108 from the same full set for testing. This ensured that the same orientation could appear in both sets, consistent with the structure of the original experiment. To increase variability, the selected trials differed in each iteration. Random white noise was applied to the data and thus the SNR was independently scaled according to the specified levels for train and test data. We then use the same pSVR decoder as in the temporal cross-decoding analysis to train and test. We plot the SNR cross-decoding matrix as temporal decoding matrices (<xref rid="figs9" ref-type="fig">Fig. S9a</xref>).</p>
<p>The second and third simulations were conducted to investigate whether increased noise levels would induce the decoder to rely on different features of the no-distractor and noise distractor data from the first experiment. We used empirical data from the primary visual cortex (V1) under the no-distractor and noise distractor condition for the second and third simulations, respectively. Data from time points 5.6–8.8 seconds after stimulus onset (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>, V1) were averaged across five TRs. As in the first simulation, SNR was systematically manipulated by added white noise. Additionally, to see whether the initial decrease in SNR and subsequent increase (as seen first during the delay and and increase during the response period) would result in dynamic coding clusters, we initially increased and subsequently decreased the amplitude of added noise. The same pSVR decoder was used to train and test on the data in a cross-validated fashion with different levels of added noise. We plot the cross-decoding matrix as in the first simulation (<xref rid="figs9" ref-type="fig">Fig. S9b-c</xref>).</p>
</sec>
<sec id="s4f">
<title>Neural subspaces</title>
<p>We adapted the method from (<xref ref-type="bibr" rid="c26">26</xref>) to calculate two-dimensional neural subspaces encoding VWM information at a given time point. To do so, we used principal component analysis (PCA). To maximize power, we binned trial-wise fMRI activations into four equidistant bins of 45 degrees and averaged the signal across all trials within a bin (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). The data matrix <bold>X</bold> was defined as a <italic>p</italic> × <italic>v</italic> matrix where <italic>p</italic> = 4 was the four orientation bins, and <italic>v</italic> was the number of voxels. We mean-centered the columns (i.e. each voxel) of the data matrix.</p>
<p>This analysis focused on the time points from 4 s to 17.6 s after delay onset. The first TRs were not used since the temporal cross-decoding results showed no above-chance decoding. We averaged across every three TRs leading to six non-overlapping temporal bins resulting in six <bold>X</bold> matrices. We calculated the principal components (PCs) using eigendecomposition of the covariance matrix for each <bold>X</bold> and defined the matrix <bold>V</bold> using the two largest eigenvalues as a <italic>v</italic> × 2 matrix, resulting in six neural subspaces, one for each non-overlapping temporal bin.</p>
</sec>
<sec id="s4g">
<title>Neural subspaces across time</title>
<p>For visualization purposes, we used three out of the total of six neural subspaces from the following time points: early (7.2 s), middle (12 s), and late (16.8 s). Following the aforementioned procedure, these subspaces were calculated on half of the trials, as we projected the left-out data onto the subspaces. The left-out data were binned into six temporal bins between 4 s and 17.6 s after target onset with no overlap just like in the calculation of the six subspaces. The projection resulted in a <italic>p</italic> × 2 matrix <bold>P</bold> for each projected time bin (resulting in a total of six <bold>P</bold> matrices). We use distinct colors to plot the temporal trajectories of each orientation bin across time in a 2D subspace flattened (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>) and not flattened (<xref rid="figs2" ref-type="fig">Fig. S2</xref>) across the time dimension. Importantly, the visualization analysis was done on a combined participant-aggregated V1-V3AB region, which included all reliable voxels across the four regions and all six participants (see <italic>Voxel Selection</italic>).</p>
<p>To measure the alignment between coding spaces at different times, we calculated an above-baseline principal angle (aPA) between all subspaces (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). We used the MATLAB function <bold><monospace>subspace</monospace></bold> for an implementation of the method proposed by (<xref ref-type="bibr" rid="c59">59</xref>) to measure the angle between two <bold>V</bold> matrices. This provided us with a possible principal angle between 0-90°; the higher the angle, the larger the difference between the two subspaces. In order to avoid overfitting and as in the visualization analysis, we used a split-half approach to compute the aPA between subspaces. Half of the binned trials were used to calculate <bold>V<sub>i,A</sub></bold> and <bold>V<sub>j,A</sub></bold> and half for <bold>V<sub>i,B</sub> V<sub>j,B</sub></bold>, where <bold>A</bold> and <bold>B</bold> refer to the two halves of the split and and <bold>i</bold> and <bold>j</bold> refer to the two time bins compared. For significance testing, the within-subspace angle (the angle between two splits of the data within a given temporal bin (i.e. <bold>V<sub>i,A</sub></bold> and <bold>V<sub>i,B</sub></bold>)) was subtracted from the between-subspace PA (the angle between two different temporal bins (e.g. <bold>V<sub>i,A</sub></bold> and <bold>V<sub>j,B</sub></bold>)). Unlike the visualization analysis, the PA was calculated per participant 1,000 times using different splits of the data on a combined V1-V3AB region that included the reliable voxels across the four regions (see <italic>Voxel Selection</italic>). The final aPA value was an average across all iterations for each participant.</p>
</sec>
<sec id="s4h">
<title>Sensory distractor and orientation VWM target neural subspaces</title>
<p>For the orientation VWM target and sensory distractor neural subspace, we followed the aforementioned subspace analysis, but instead of calculating subspaces on six temporal bins, we averaged across the 4-17.6 s delay period and calculated a single subspace. As in the previous analysis, we split the orientation VWM trials in half. We then binned the trials either based on the target orientation or the sensory distractor. For visualization purposes, we projected the left-out data averaged based on the sensory distractor and the target onto subspaces derived from both the sensory distractor and target subspaces. As in the previous visualization, the analysis was run on a participant-aggregated V1-V3AB region.</p>
<p>To calculate the aPA we had the following subspaces: <bold>V<sub>Target,A</sub></bold>, <bold>V<sub>Dist,A</sub></bold>, <bold>V<sub>Target,B</sub></bold> and <bold>V<sub>Dist,B</sub></bold>, where the subspaces were calculated on trials binned either based on the target orientation or the sensory distractor. The aPA was calculated by subtracting the within-subspace angle (<bold>V<sub>Target,A</sub></bold> and <bold>V<sub>Target,B</sub></bold>, <bold>V<sub>Dist,A</sub></bold> and <bold>V<sub>Dist,B</sub></bold>) from the sensory distractor and working memory angle (<bold>V<sub>Target,A</sub></bold> and <bold>V<sub>Dist,B</sub></bold>, <bold>V<sub>Target,B</sub></bold> and <bold>V<sub>Dist,A</sub></bold>). The split-half aPA analysis was performed 1,000 times and the final value was an average across these iterations for each participant.</p>
</sec>
</sec>
</body>
<back>
<sec id="s8">
<title>Supplementary Figures</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 1.</label>
<caption><title>Task and temporal cross-decoding of Experiment 2.</title>
<p><bold>a)</bold> On each trial an oriented grating was presented for the 0.5 s followed by a delay period of 13 s. In a third of the trials a naturalistic distractor was presented for 11 s during the middle of the delay; in another third another a flickering orientation grating was presented; one third of trials had no-distractor during the delay. <bold>b)</bold> Temporal generalization of the multivariate code encoding VWM representations in three conditions across occipital and parietal regions. Across-participant mean temporal cross-decoding of no-distractor trials. Black outlines: matrix elements showing above-chance decoding (cluster-based permutation test; <italic>p</italic> &lt; 0.05). Blue outlines with dots: dynamic coding elements; parts of the cross-decoding matrix where the multivariate code fails to generalize (off-diagonal elements having lower decoding accuracy than their corresponding two diagonal elements; conjunction between two cluster-based permutation tests; <italic>p</italic> &lt; 0.05). <bold>c)</bold> Same as b), but noise distractor trials. Dynamic coding elements depicted in red. <bold>d)</bold> Same as c), but orientation distractor trials. <bold>e)</bold>. Decoding accuracy (feature continuous accuracy; FCA) across time for train and test on no-distractor trials (purple), train and test on naturalistic distractor trials (dark green) and train and test on flickering orientation distractor trials (light green). Horizontal lines indicate clusters where there is a difference between two time courses (all clusters <italic>p</italic> &lt; 0.05; nonparametric cluster permutation test, see color code on the right).</p></caption>
<graphic xlink:href="589170v4_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 2.</label>
<caption><title>Neural trajectories across time.</title> <p>Same as <xref rid="fig2" ref-type="fig">Figure 2c</xref>), but the time dimension is on the z-axis.</p></caption>
<graphic xlink:href="589170v4_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 3.</label>
<caption><title>Temporal cross-decoding of distactor and memory target in orientation distractor trials in Experiment 1.</title>
<p><bold>a)</bold> Across-participant mean temporal cross-decoding of the sensory distractor. Black outlines: matrix elements showing above-chance decoding (cluster-based permutation test; <italic>p</italic> &lt; 0.05). Blue outlines with dots: dynamic coding element (conjunction between two cluster-based permutation tests; <italic>p</italic> &lt; 0.05). <bold>b)</bold> Same as a), but the decoder was trained on the target and tested on the sensory distractor in orientation VWM trials. <bold>c)</bold> Same as a), but trained on the sensory distractor and tested on the target.</p></caption>
<graphic xlink:href="589170v4_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 4.</label>
<caption><title>Temporal cross-decoding of distactor and memory target in flickering orientation distractor trials in Experiment 2.</title>
<p><bold>a)</bold> Across-participant mean temporal cross-decoding of the sensory distractor. Black outlines: matrix elements showing above-chance decoding (cluster-based permutation test; <italic>p</italic> &lt; 0.05). Red outlines with dots: dynamic coding element (conjunction between two cluster-based permutation tests; <italic>p</italic> &lt; 0.05). <bold>b)</bold> Same as a), but the decoder was trained on the target and tested on the sensory distractor in orientation VWM trials. <bold>c)</bold> Same as a), but trained on the sensory distractor and tested on the target.</p></caption>
<graphic xlink:href="589170v4_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 5.</label>
<caption><title>Stable coding spaces of memory target and distractor in each ROI.</title>
<p><bold>a)</bold> Experiment 1. Left: projection of left-out target (green) and sensory distractor (gray) onto an orientation VWM target neural subspace. Right: same as left, but the projections are onto the sensory distractor subspace. <bold>b)</bold> Experiment 2. Same as a), but for flickering orientation distractor trials. <bold>c)</bold> Experiment 1. Principal angle between the sensory distractor and orientation VWM target subspaces in each ROI (from V1 to LO2: <italic>p</italic> = 0.019, 0.045, 0.034, 0.034, 0.109, 0.045, 0, 0.034; one-tailed permutation test of sample mean, FDR-corrected), averaged across 1,000 split-half iterations. Errorbars indicate SEM across participants. <bold>d)</bold> Experiment 2. Same as c) but for flickering orientation distractor trial (from V1 to LO2: <italic>p</italic> = 0.078, 0, 0, 0.078, 0.263, 0.214, 0.177, 0.263; one-tailed permutation test of sample mean, FDR-corrected).</p></caption>
<graphic xlink:href="589170v4_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 6.</label>
<caption><title>Cross-decoding between distractor and no-distractor conditions in Experiment 2.</title>
<p>Decoding accuracy as a proportion of no-distractor decoding estimated on the averaged delay period (4-16.8s). Nonparametric permutation tests compared the decoding accuracy of each analysis to the no-distractor decoding baseline (indicated as a dashed line) and between a decoder trained and tested on distractor trials (noise- or orientation-within) and a decoder trained on no-distractor trials and tested on distractor trials (noise or orientation-cross). FDR-corrected across ROIs. * <italic>p</italic> &lt; 0.05, ** <italic>p &lt;</italic> 0.01, *** <italic>p</italic> &lt; 0.001. Corresponding <italic>p</italic>-values found in <xref rid="tbls23" ref-type="table">Supplementary Table 3</xref>.</p></caption>
<graphic xlink:href="589170v4_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 7.</label>
<caption><title>Temporal cross-decoding generalization between distractor and no-distractor VWM trials.</title>
<p><bold>a)</bold> Across-participant mean temporal cross-decoding of noise distractor trials when trained on no-distractor trials. <bold>b)</bold> Same as a), but orientation distractor trials trained on no-distractor trials.</p></caption>
<graphic xlink:href="589170v4_figs7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 8.</label>
<caption><title>Selected voxels.</title>
<p>Top: Experiment 1. Number of voxels (left) and proportion of voxels selected in an ROI (right). Errorbars indicate SEM across participants. Bottom: same as top, but for Experiment 2.</p></caption>
<graphic xlink:href="589170v4_figs8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 9.</label>
<caption><title>Simulations.</title>
<p><bold>a)</bold> SNR-dependent decoding accuracy, obtained from simulated voxel responses. SNR decreases from left to right (training axis) and bottom to top (testing axis). <bold>b)</bold> SNR-dependent decoding accuracy, obtained from no-distractor data from Experiment 1. SNR first decreases and then increases in both training and test axes. <bold>c)</bold> SNR-dependent decoding accuracy, obtained from noise distractor data from Experiment 1. SNR first decreases and then increases in both training and test axes.</p></caption>
<graphic xlink:href="589170v4_figs9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Supplementary Table 1.</label>
<caption><title>FDR-corrected <italic>p</italic>-values corresponding to</title><p><xref rid="fig2" ref-type="fig">Figure 2d</xref>.</p></caption>
<graphic xlink:href="589170v4_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Supplementary Table 2.</label>
<caption><title>FDR-corrected <italic>p</italic>-values corresponding to</title><p><xref rid="fig4" ref-type="fig">Figure 4b</xref>.</p></caption>
<graphic xlink:href="589170v4_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls3" orientation="portrait" position="float">
<label>Supplementary Table 3.</label>
<caption><title>FDR-corrected <italic>p</italic>-values corresponding to Supplementary Figure 6.</title></caption>
<graphic xlink:href="589170v4_tbls3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<ack>
<title>Acknowledgements</title>
<p>J.K.D. was funded by the Max Planck Society and BMBF (as part of the Max Planck School of Cognition). J.D.H. was supported by the Deutsche Forschungsgemeinschaft (DFG, Exzellenzcluster Science of Intelligence); SFB 940 “Volition and Cognitive Control”; and SFB-TRR 295 “Retuning dynamic motor network disorders using neuromodulation”. S.W. was supported by Deutsche Forschungsgemeinschaft (DFG) Research Training Group 2386 451 and EXC 2002/1 “Science of Intelligence.” We thank Rosanne Rademaker, Chaipat Chunharas, and John Serences for collecting and sharing their data open access, without which this reanalysis would not have been possible. We also thank Rosanne Rademaker, Michael Wolff, Amir Rawal, and Maria Servetnik for extensive discussions of the results. We also thank Vivien Chopurian and Thomas Christophel for their feedback on the manuscript.</p>
</ack>
<sec id="d1e1628" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Author contributions</title>
<p>Conceptualization, J.K.D.; Methodology, J.K.D, S.W., J.S., J.-D.H.; Formal Analysis, J.K.D.; Software, J.K.D, S.W., J.S.; Visualization, J.K.D.; Funding Acquisition, J.K.D., J.-D.H.; Writing - Original Draft Preparation, J.K.D.; Writing – Review &amp; Editing, J.K.D., S.W., J.S., J.-D.H. Supervision, J.-D.H.</p>
</sec>
<sec id="s5" sec-type="data-availability">
<title>Data availability</title>
<p>The preprocessed data are shared open-access at <ext-link ext-link-type="uri" xlink:href="https://osf.io/dkx6y/">https://osf.io/dkx6y/</ext-link>. The analysis scripts and results are shared at <ext-link ext-link-type="uri" xlink:href="https://github.com/degutis/WM_dynamicCoding">https://github.com/degutis/WM_dynamicCoding</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://osf.io/jq3ma/">https://osf.io/jq3ma/</ext-link>, respectively.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name>, <string-name><surname>D’Esposito</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Persistent activity in the prefrontal cortex during working memory</article-title>. <source>Trends Cogn Sci</source>. <year>2003</year> Sep;<volume>7</volume>(<issue>9</issue>):<fpage>415</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name></person-group>. <article-title>Cellular basis of working memory</article-title>. <source>Neuron</source>. <year>1995</year> Mar 1;<volume>14</volume>(<issue>3</issue>):<fpage>477</fpage>–<lpage>85</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>D’Esposito</surname> <given-names>M</given-names></string-name>, <string-name><surname>Postle</surname> <given-names>BR</given-names></string-name></person-group>. <article-title>The Cognitive Neuroscience of Working Memory</article-title>. <source>Annu Rev Psychol</source>. <year>2015</year> Jan 3;<volume>66</volume>(<issue>1</issue>):<fpage>115</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fuster</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Alexander</surname> <given-names>GE</given-names></string-name></person-group>. <article-title>Neuron Activity Related to Short-Term Memory</article-title>. <source>Science</source>. <year>1971</year> Aug 13;<fpage>173</fpage>(3997):652–4.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harrison</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Tong</surname> <given-names>F</given-names></string-name></person-group>. <article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title>. <source>Nature</source>. <year>2009</year> Apr;<fpage>458</fpage>(7238):632–5.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Christophel</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Hebart</surname> <given-names>MN</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name></person-group>. <article-title>Decoding the Contents of Visual Short-Term Memory from Human Visual and Parietal Cortex</article-title>. <source>J Neurosci</source>. <year>2012</year> Sep 19;<volume>32</volume>(<issue>38</issue>):<fpage>12983</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ester</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Sprague</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Serences</surname> <given-names>JT</given-names></string-name></person-group>. <article-title>Parietal and Frontal Cortex Encode Stimulus-Specific Mnemonic Representations during Visual Working Memory</article-title>. <source>Neuron</source>. <year>2015</year> Aug;<volume>87</volume>(<issue>4</issue>):<fpage>893</fpage>–<lpage>905</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rademaker</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Chunharas</surname> <given-names>C</given-names></string-name>, <string-name><surname>Serences</surname> <given-names>JT</given-names></string-name></person-group>. <article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title>. <source>Nat Neurosci</source>. <year>2019</year> Aug;<volume>22</volume>(<issue>8</issue>):<fpage>1336</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riggall</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Postle</surname> <given-names>BR</given-names></string-name></person-group>. <article-title>The Relationship between Working Memory Storage and Elevated Activity as Measured with Functional Magnetic Resonance Imaging</article-title>. <source>J Neurosci</source>. <year>2012</year> Sep 19;<volume>32</volume>(<issue>38</issue>):<fpage>12990</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Serences</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Ester</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Vogel</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Awh</surname> <given-names>E</given-names></string-name></person-group>. <article-title>Stimulus-Specific Delay Activity in Human Primary Visual Cortex</article-title>. <source>Psychol Sci</source>. <year>2009</year> Feb;<volume>20</volume>(<issue>2</issue>):<fpage>207</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Christophel</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Klink</surname> <given-names>PC</given-names></string-name>, <string-name><surname>Spitzer</surname> <given-names>B</given-names></string-name>, <string-name><surname>Roelfsema</surname> <given-names>PR</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name></person-group>. <article-title>The Distributed Nature of Working Memory</article-title>. <source>Trends Cogn Sci</source>. <year>2017</year> Feb;<volume>21</volume>(<issue>2</issue>):<fpage>111</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name>, <string-name><surname>Sprague</surname> <given-names>TC</given-names></string-name></person-group>. <article-title>Persistent Activity During Working Memory From Front to Back</article-title>. <source>Front Neural Circuits</source>. <year>2021</year> Jul 21;<volume>15</volume>:<issue>696060</issue>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Iamshchinina</surname> <given-names>P</given-names></string-name>, <string-name><surname>Christophel</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Gayet</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rademaker</surname> <given-names>RL</given-names></string-name></person-group>. <article-title>Essential considerations for exploring visual working memory storage in the human brain</article-title>. <source>Vis Cogn</source>. <year>2021</year> Aug 9;<volume>29</volume>(<issue>7</issue>):<fpage>425</fpage>–<lpage>36</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bettencourt</surname> <given-names>KC</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>Y</given-names></string-name></person-group>. <article-title>Decoding the content of visual short-term memory under distraction in occipital and parietal areas</article-title>. <source>Nat Neurosci</source>. <year>2016</year> Jan;<volume>19</volume>(<issue>1</issue>):<fpage>150</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Muhle-Karbe</surname> <given-names>PS</given-names></string-name>, <string-name><surname>Myers</surname> <given-names>NE</given-names></string-name></person-group>. <article-title>Theoretical distinction between functional states in working memory and their corresponding neural states</article-title>. <source>Vis Cogn</source>. <year>2020</year> Sep 13;<volume>28</volume>(<issue>5–8</issue>):<fpage>420</fpage>–<lpage>32</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname> <given-names>MG</given-names></string-name></person-group>. <article-title>‘Activity-silent’ working memory in prefrontal cortex: a dynamic coding framework</article-title>. <source>Trends Cogn Sci</source>. <year>2015</year> Jul;<volume>19</volume>(<issue>7</issue>):<fpage>394</fpage>–<lpage>405</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stroud</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Duncan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lengyel</surname> <given-names>M</given-names></string-name></person-group>. <article-title>The computational foundations of dynamic coding in working memory</article-title>. <source>Trends Cogn Sci</source>. <year>2024</year> Apr;<issue>S1364661324000536</issue>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Lundqvist</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bastos</surname> <given-names>AM</given-names></string-name></person-group>. <article-title>Working Memory 2.0</article-title>. <source>Neuron</source>. <year>2018</year> Oct 24;<volume>100</volume>(<issue>2</issue>):<fpage>463</fpage>–<lpage>75</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sreenivasan</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name>, <string-name><surname>D’Esposito</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Revisiting the role of persistent neural activity during working memory</article-title>. <source>Trends Cogn Sci</source>. <year>2014</year> Feb;<volume>18</volume>(<issue>2</issue>):<fpage>82</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meyers</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Freedman</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Kreiman</surname> <given-names>G</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Poggio</surname> <given-names>T</given-names></string-name></person-group>. <article-title>Dynamic Population Coding of Category Information in Inferior Temporal and Prefrontal Cortex</article-title>. <source>J Neurophysiol</source>. <year>2008</year> Sep;<volume>100</volume>(<issue>3</issue>):<fpage>1407</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Bernacchia</surname> <given-names>A</given-names></string-name>, <string-name><surname>Roy</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Constantinidis</surname> <given-names>C</given-names></string-name>, <string-name><surname>Romo</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name></person-group>. <article-title>Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex</article-title>. <source>Proc Natl Acad Sci</source>. <year>2017</year> Jan 10;<volume>114</volume>(<issue>2</issue>):<fpage>394</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spaak</surname> <given-names>E</given-names></string-name>, <string-name><surname>Watanabe</surname> <given-names>K</given-names></string-name>, <string-name><surname>Funahashi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Stokes</surname> <given-names>MG</given-names></string-name></person-group>. <article-title>Stable and Dynamic Coding for Working Memory in Primate Prefrontal Cortex</article-title>. <source>J Neurosci</source>. <year>2017</year> Jul 5;<volume>37</volume>(<issue>27</issue>):<fpage>6503</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parthasarathy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Herikstad</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bong</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Medina</surname> <given-names>FS</given-names></string-name>, <string-name><surname>Libedinsky</surname> <given-names>C</given-names></string-name>, <string-name><surname>Yen</surname> <given-names>SC</given-names></string-name></person-group>. <article-title>Mixed selectivity morphs population codes in prefrontal cortex</article-title>. <source>Nat Neurosci</source>. <year>2017</year> Dec;<volume>20</volume>(<issue>12</issue>):<fpage>1770</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parthasarathy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Tang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Herikstad</surname> <given-names>R</given-names></string-name>, <string-name><surname>Cheong</surname> <given-names>LF</given-names></string-name>, <string-name><surname>Yen</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Libedinsky</surname> <given-names>C</given-names></string-name></person-group>. <article-title>Time-invariant working memory representations in the presence of code-morphing in the lateral prefrontal cortex</article-title>. <source>Nat Commun</source>. <year>2019</year> Nov 1;<volume>10</volume>(<issue>1</issue>):<fpage>4995</fpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sreenivasan</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Vytlacil</surname> <given-names>J</given-names></string-name>, <string-name><surname>D’Esposito</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Distributed and Dynamic Storage of Working Memory Stimulus Information in Extrastriate Cortex</article-title>. <source>J Cogn Neurosci</source>. <year>2014</year> May 1;<volume>26</volume>(<issue>5</issue>):<fpage>1141</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>HH</given-names></string-name>, <string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name></person-group>. <article-title>Neural population dynamics of human working memory</article-title>. <source>Curr Biol</source>. <year>2023</year> Sep;<volume>33</volume>(<issue>17</issue>):<fpage>3775</fpage>–<lpage>3784.</lpage> </mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lorenc</surname> <given-names>ES</given-names></string-name>, <string-name><surname>Mallett</surname> <given-names>R</given-names></string-name>, <string-name><surname>Lewis-Peacock</surname> <given-names>JA</given-names></string-name></person-group>. <article-title>Distraction in Visual Working Memory: Resistance is Not Futile</article-title>. <source>Trends Cogn Sci</source>. <year>2021</year> Mar;<volume>25</volume>(<issue>3</issue>):<fpage>228</fpage>–<lpage>39</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Felleman</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Van Essen</surname> <given-names>DC</given-names></string-name></person-group>. <article-title>Distributed Hierarchical Processing in the Primate Cerebral Cortex</article-title>. <source>Cereb Cortex</source>. <year>1991</year> Jan 1;<volume>1</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawrence</surname> <given-names>SJD</given-names></string-name>, <string-name><surname>van Mourik</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kok</surname> <given-names>P</given-names></string-name>, <string-name><surname>Koopmans</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Norris</surname> <given-names>DG</given-names></string-name>, <string-name><surname>de Lange</surname> <given-names>FP</given-names></string-name></person-group>. <article-title>Laminar Organization of Working Memory Signals in Human Visual Cortex</article-title>. <source>Curr Biol</source>. <year>2018</year> Nov;<volume>28</volume>(<issue>21</issue>):<fpage>3435</fpage>–<lpage>3440.</lpage> </mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Kerkoerle</surname> <given-names>T</given-names></string-name>, <string-name><surname>Self</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Roelfsema</surname> <given-names>PR</given-names></string-name></person-group>. <article-title>Layer-specificity in the effects of attention and working memory on activity in primary visual cortex</article-title>. <source>Nat Commun</source>. <year>2017</year> Apr 28;<volume>8</volume>(<issue>1</issue>):<fpage>13804</fpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawrence</surname> <given-names>SJD</given-names></string-name>, <string-name><surname>Norris</surname> <given-names>DG</given-names></string-name>, <string-name><surname>de Lange</surname> <given-names>FP</given-names></string-name></person-group>. <article-title>Dissociable laminar profiles of concurrent bottom-up and top-down modulation in the human visual cortex</article-title>. <source>eLife</source>. <year>2019</year> May 7;<volume>8</volume>:<elocation-id>e44422</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.44422</pub-id></mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Libby</surname> <given-names>A</given-names></string-name>, <string-name><surname>Buschman</surname> <given-names>TJ</given-names></string-name></person-group>. <article-title>Rotational dynamics reduce interference between sensory and memory representations</article-title>. <source>Nat Neurosci</source>. <year>2021</year> May;<volume>24</volume>(<issue>5</issue>):<fpage>715</fpage>–<lpage>26</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xie</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>P</given-names></string-name>, <string-name><surname>Li</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Song</surname> <given-names>W</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Geometry of sequence working memory in macaque prefrontal cortex</article-title>. <source>Science</source>. <year>2022</year> Feb 11;<fpage>375</fpage>(6581):632–9.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wan</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Menendez</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Postle</surname> <given-names>BR</given-names></string-name></person-group>. <article-title>Priority-based transformations of stimulus representation in visual working memory. Buschman T, editor</article-title>. <source>PLOS Comput Biol</source>. <year>2022</year> <month>Jun</month> <day>2</day>;<volume>18</volume>(<issue>6</issue>):<fpage>e1009062</fpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Wan</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Ardalan</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fulvio</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Postle</surname> <given-names>BR</given-names></string-name></person-group>. <article-title>Representing context and priority in working memory</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.10.24.563608</pub-id> <year>2023</year></mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Loon</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Olmos-Solis</surname> <given-names>K</given-names></string-name>, <string-name><surname>Fahrenfort</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Olivers</surname> <given-names>CN</given-names></string-name></person-group>. <article-title>Current and future goals are represented in opposite patterns in object-selective cortex</article-title>. <source>eLife</source>. <year>2018</year> Nov 6;<volume>7</volume>:<elocation-id>e38677</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.38677</pub-id></mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Kusunoki</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sigala</surname> <given-names>N</given-names></string-name>, <string-name><surname>Nili</surname> <given-names>H</given-names></string-name>, <string-name><surname>Gaffan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Duncan</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Dynamic Coding for Cognitive Control in Prefrontal Cortex</article-title>. <source>Neuron</source>. <year>2013</year> Apr;<volume>78</volume>(<issue>2</issue>):<fpage>364</fpage>–<lpage>75</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Degutis</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Chaimow</surname> <given-names>D</given-names></string-name>, <string-name><surname>Haenelt</surname> <given-names>D</given-names></string-name>, <string-name><surname>Assem</surname> <given-names>M</given-names></string-name>, <string-name><surname>Duncan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Dynamic layer-specific processing in the prefrontal cortex during working memory</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.10.27.564330</pub-id> <year>2023</year></mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anders</surname> <given-names>S</given-names></string-name>, <string-name><surname>Heinzle</surname> <given-names>J</given-names></string-name>, <string-name><surname>Weiskopf</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ethofer</surname> <given-names>T</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name></person-group>. <article-title>Flow of affective information between communicating brains</article-title>. <source>NeuroImage</source>. <year>2011</year> Jan;<volume>54</volume>(<issue>1</issue>):<fpage>439</fpage>–<lpage>46</lpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weber</surname> <given-names>S</given-names></string-name>, <string-name><surname>Christophel</surname> <given-names>T</given-names></string-name>, <string-name><surname>Görgen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Soch</surname> <given-names>J</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>J</given-names></string-name></person-group>. <article-title>Working memory signals in early visual cortex are present in weak and strong imagers</article-title>. <source>Hum Brain Mapp</source>. <year>2024</year> Feb 15;<volume>45</volume>(<issue>3</issue>):<fpage>e26590</fpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cavanagh</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Towers</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Wallis</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Hunt</surname> <given-names>LT</given-names></string-name>, <string-name><surname>Kennerley</surname> <given-names>SW</given-names></string-name></person-group>. <article-title>Reconciling persistent and dynamic hypotheses of working memory coding in prefrontal cortex</article-title>. <source>Nat Commun</source>. <year>2018</year> Dec;<volume>9</volume>(<issue>1</issue>):<fpage>3498</fpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brouwer</surname> <given-names>GJ</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name></person-group>. <article-title>Decoding and Reconstructing Color from Responses in Human Visual Cortex</article-title>. <source>J Neurosci</source>. <year>2009</year> Nov 4;<volume>29</volume>(<issue>44</issue>):<fpage>13992</fpage>–<lpage>4003</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Piwek</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Stokes</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Summerfield</surname> <given-names>C.</given-names></string-name></person-group> <article-title>A recurrent neural network model of prefrontal brain activity during a working memory task</article-title>. <source>PLOS Comput Biol</source>  <volume>19</volume>:<elocation-id>e1011555</elocation-id>. <year>2023</year></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Konkle</surname> <given-names>T</given-names></string-name>, <string-name><surname>Rhee</surname> <given-names>JY</given-names></string-name>, <string-name><surname>Nakayama</surname> <given-names>K</given-names></string-name>, <string-name><surname>Alvarez</surname> <given-names>GA</given-names></string-name></person-group>. <article-title>Processing multiple visual objects is limited by overlap in neural channels</article-title>. <source>Proc Natl Acad Sci</source>. <year>2014</year> Jun 17;<volume>111</volume>(<issue>24</issue>):<fpage>8955</fpage>–<lpage>60</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franconeri</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Alvarez</surname> <given-names>GA</given-names></string-name>, <string-name><surname>Cavanagh</surname> <given-names>P</given-names></string-name></person-group>. <article-title>Flexible cognitive resources: competitive content maps for attention and memory</article-title>. <source>Trends Cogn Sci</source>. <year>2013</year> Mar;<volume>17</volume>(<issue>3</issue>):<fpage>134</fpage>–<lpage>41</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sprague</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Ester</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Serences</surname> <given-names>JT</given-names></string-name></person-group>. <article-title>Reconstructions of Information in Visual Spatial Working Memory Degrade with Memory Load</article-title>. <source>Curr Biol</source>. <year>2014</year> Sep;<volume>24</volume>(<issue>18</issue>):<fpage>2174</fpage>–<lpage>80</lpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rademaker</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Bloem</surname> <given-names>IM</given-names></string-name>, <string-name><surname>De Weerd</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sack</surname> <given-names>AT</given-names></string-name></person-group>. <article-title>The impact of interference on short-term memory for visual orientation</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2015</year>;<volume>41</volume>(<issue>6</issue>):<fpage>1650</fpage>–<lpage>65</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mallett</surname> <given-names>R</given-names></string-name>, <string-name><surname>Mummaneni</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lewis-Peacock</surname> <given-names>JA</given-names></string-name></person-group>. <article-title>Distraction biases working memory for faces</article-title>. <source>Psychon Bull Rev</source>. <year>2020</year> Apr;<volume>27</volume>(<issue>2</issue>):<fpage>350</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teng</surname> <given-names>C</given-names></string-name>, <string-name><surname>Kravitz</surname> <given-names>DJ</given-names></string-name></person-group>. <article-title>Visual working memory directly alters perception</article-title>. <source>Nat Hum Behav</source>. <year>2019</year> Aug;<volume>3</volume>(<issue>8</issue>):<fpage>827</fpage>–<lpage>36</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kang</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>SW</given-names></string-name>, <string-name><surname>Blake</surname> <given-names>R</given-names></string-name>, <string-name><surname>Woodman</surname> <given-names>GF</given-names></string-name></person-group>. <article-title>Visual working memory contaminates perception</article-title>. <source>Psychon Bull Rev</source>. <year>2011</year> Oct;<volume>18</volume>(<issue>5</issue>):<fpage>860</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gayet</surname> <given-names>S</given-names></string-name>, <string-name><surname>Paffen</surname> <given-names>CLE</given-names></string-name>, <string-name><surname>Van Der Stigchel</surname> <given-names>S</given-names></string-name></person-group>. <article-title>Information Matching the Content of Visual Working Memory Is Prioritized for Conscious Access</article-title>. <source>Psychol Sci</source>. <year>2013</year> Dec;<volume>24</volume>(<issue>12</issue>):<fpage>2472</fpage>–<lpage>80</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gayet</surname> <given-names>S</given-names></string-name>, <string-name><surname>Guggenmos</surname> <given-names>M</given-names></string-name>, <string-name><surname>Christophel</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Paffen</surname> <given-names>CLE</given-names></string-name>, <string-name><surname>Van Der Stigchel</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Visual Working Memory Enhances the Neural Response to Matching Visual Input</article-title>. <source>J Neurosci</source>. <year>2017</year> Jul 12;<volume>37</volume>(<issue>28</issue>):<fpage>6638</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lorenc</surname> <given-names>ES</given-names></string-name>, <string-name><surname>Sreenivasan</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Nee</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Vandenbroucke</surname> <given-names>ARE</given-names></string-name>, <string-name><surname>D’Esposito</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Flexible Coding of Visual Working Memory Representations during Distraction</article-title>. <source>J Neurosci</source>. <year>2018</year> Jun 6;<volume>38</volume>(<issue>23</issue>):<fpage>5267</fpage>–<lpage>76</lpage>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hallenbeck</surname> <given-names>GE</given-names></string-name>, <string-name><surname>Sprague</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Rahmati</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sreenivasan</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name></person-group>. <article-title>Working memory representations in visual cortex mediate distraction effects</article-title>. <source>Nat Commun</source>. <year>2021</year> Aug 5;<volume>12</volume>(<issue>1</issue>):<fpage>4714</fpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Iamshchinina</surname> <given-names>P</given-names></string-name>, <string-name><surname>Kaiser</surname> <given-names>D</given-names></string-name>, <string-name><surname>Yakupov</surname> <given-names>R</given-names></string-name>, <string-name><surname>Haenelt</surname> <given-names>D</given-names></string-name>, <string-name><surname>Sciarra</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mattern</surname> <given-names>H</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Perceived and mentally rotated contents are differentially represented in cortical depth of V1</article-title>. <source>Commun Biol</source>. <year>2021</year> Dec;<volume>4</volume>(<issue>1</issue>):<fpage>1069</fpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>CJ</given-names></string-name></person-group>. <article-title>LIBSVM: A library for support vector machines</article-title>. <source>ACM Trans Intell Syst Technol</source>. <year>2011</year> Apr;<volume>2</volume>(<issue>3</issue>):<fpage>1</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pilly</surname> <given-names>PK</given-names></string-name>, <string-name><surname>Seitz</surname> <given-names>AR</given-names></string-name></person-group>. <article-title>What a difference a parameter makes: A psychophysical comparison of random dot motion algorithms</article-title>. <source>Vision Res</source>. <year>2009</year> Jul;<volume>49</volume>(<issue>13</issue>):<fpage>1599</fpage>–<lpage>612</lpage>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name></person-group>. <article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title>. <source>J Neurosci Methods</source>. <year>2007</year> Aug;<volume>164</volume>(<issue>1</issue>):<fpage>177</fpage>–<lpage>90</lpage>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bjorck</surname> <given-names>A</given-names></string-name>, <string-name><surname>Golub</surname> <given-names>GH</given-names></string-name></person-group>. <article-title>Numerical Methods for Computing Angles Between Linear Subspaces</article-title>. <source>Math Comput</source>. <year>1973</year> Jul;<volume>27</volume>(<issue>123</issue>):<fpage>579</fpage>–<lpage>94</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99290.3.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Xue</surname>
<given-names>Gui</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study reports a reanalysis of one experiment of a previously-published report to characterize the dynamics of neural population codes during visual working memory in the presence of distracting information. This paper presents <bold>solid</bold> evidence that working memory representations are dynamic and distinct from sensory representations of intervening distractions. This research will be of interest to cognitive neuroscientists working on the neural bases of visual perception and memory.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99290.3.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this study, the authors re-analyzed a public dataset (Rademaker et al, 2019, Nature Neuroscience) which includes fMRI and behavioral data recorded while participants held an oriented grating in visual working memory (WM) and performed a delayed recall task at the end of an extended delay period. In that experiment, participants were pre-cued on each trial as to whether there would be a distracting visual stimulus presented during the delay period (filtered noise or randomly-oriented grating). In this manuscript, the authors focused on identifying whether the neural code in retinotopic cortex for remembered orientation was 'stable' over the delay period, such that the format of the code remained the same, or whether the code was dynamic, such that information was present, but encoded in an alternative format. They identify some timepoints - especially towards the beginning/end of the delay - where the multivariate activation pattern fails to generalize to other timepoints, and interpret this as evidence for a dynamic code. Additionally, the authors compare the representational format of remembered orientation in the presence vs absence of a distracting stimulus, averaged over the delay period. This analysis suggested a 'rotation' of the representational subspace between distracting orientations and remembered orientations, which may help preserve simultaneous representations of both remembered and viewed stimuli. Intriguingly, this rotation was a bit smaller for Expt 2, in which the orientation distractor had a greater behavioral impact on the participants' behavioral working memory recall performance, suggesting that more separation between subspaces is critical for preserving intact working memory representations.</p>
<p>Strengths:</p>
<p>(1) Direct comparisons of coding subspaces/manifolds between timepoints, task conditions, and experiments is an innovative and useful approach for understanding how neural representations are transformed to support cognition</p>
<p>(2) Re-use of existing dataset substantially goes beyond the authors' previous findings by comparing geometry of representational spaces between conditions and timepoints, and by looking explicitly for dynamic neural representations</p>
<p>(3) Simulations testing whether dynamic codes can be explained purely by changes in data SNR are an important contribution, as this rules out a category of explanations for the dynamic coding results observed</p>
<p>Weaknesses:</p>
<p>(1) Primary evidence for 'dynamic coding', especially in early visual cortex, appears to be related to the transition between encoding/maintenance and maintenance/recall, but the delay period representations seem overall stable, consistent with some previous findings. However, given the simulation results, the general result that representations may change in their format appears solid, though the contribution of different trial phases remains important for considering the overall result.</p>
<p>(2) Converting a continuous decoding metric (angular error) to &quot;% decoding accuracy&quot; serves to obfuscate the units of the actual results. Decoding precision (e.g., sd of decoding error histogram) would be more interpretable and better related to both the previous study and behavioral measures of WM performance.</p>
<p>Comments on revised version:</p>
<p>The authors have addressed all my previous concerns.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99290.3.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Degutis</surname>
<given-names>Jonas Karolis</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0003-8682-3920</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Weber</surname>
<given-names>Simon</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8440-1156</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Soch</surname>
<given-names>Joram</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8879-5666</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Haynes</surname>
<given-names>John-Dylan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1786-6954</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the previous reviews</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1:</bold></p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>(1) At several places in the reply to reviewers and the manuscript, when discussing the new simulations conducted, the authors mention they break the 180 trials into a train/test split of 108/108 - is this value correct? If so, how? (pg 19 of updated manuscript)</p>
</disp-quote>
<p>Thank you for pointing this out; it was not clearly explained. We have now added the explanation to the Methods section:</p>
<p>“For each iteration, we randomly selected 108 responses from the full set of 180 for training, and then independently sampled another 108 from the same full set for testing. This ensured that the same orientation could appear in both sets, consistent with the structure of the original experiment.”</p>
<disp-quote content-type="editor-comment">
<p>(2) I appreciate the authors have added the variance explained of principal components to the axes of Fig. 3, though it took me a while to notice this, and this isn't described in the figure caption at all. It would likely help readers to directly explain what the % means on each axis of Fig. 3.</p>
</disp-quote>
<p>Thank you, we have now added a description in both Fig. 2 and 3:</p>
<p>“The axes represent the first two principal components, with labels indicating the percent of total explained variance.”</p>
<disp-quote content-type="editor-comment">
<p>(3) I believe there is a typo/missing word in the new paragraph on pg 15: &quot;neural visual WM representations in the early visual cortices are [[biased]] towards distractors&quot; (I think the bracketed word may be omitted as a typo)</p>
</disp-quote>
<p>Thank you - fixed.</p>
</body>
</sub-article>
</article>