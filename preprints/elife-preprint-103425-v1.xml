<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">103425</article-id>
<article-id pub-id-type="doi">10.7554/eLife.103425</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103425.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Dual-format attentional template during preparation in human visual cortex</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Yilin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liu</surname>
<given-names>Taosheng</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Jia</surname>
<given-names>Ke</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<email>kjia@zju.edu.cn</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Theeuwes</surname>
<given-names>Jan</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0333-4957</contrib-id>
<name>
<surname>Gong</surname>
<given-names>Mengyuan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>gongmy426@zju.edu.cn</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Department of Psychology and Behavioral Sciences, Zhejiang University</institution></institution-wrap>, <city>Hangzhou</city>, <country>China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05hs6h993</institution-id><institution>Department of Psychology, Michigan State University</institution></institution-wrap>, <city>East Lansing</city>, <country>USA</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Liangzhu Laboratory, MOE Frontier Science Center for Brain Science and Brain-machine Integration, State Key Laboratory of Brain-machine Intelligence, Zhejiang University</institution></institution-wrap>, <city>Hangzhou</city>, <country>China</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0310dsa24</institution-id><institution>Department of Neurobiology, Affiliated Mental Health Center &amp; Hangzhou Seventh People’s Hospital, Zhejiang University School of Medicine</institution></institution-wrap>, <city>Hangzhou</city>, <country>China</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>NHC and CAMS Key Laboratory of Medical Neurobiology, Zhejiang University</institution></institution-wrap>, <city>Hangzhou</city>, <country>China</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008xxew50</institution-id><institution>Department of Experimental and Applied Psychology, Vrije Universiteit Amsterdam</institution></institution-wrap>, <city>Amsterdam</city>, the <country>Netherlands</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Zhang</surname>
<given-names>Xilin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2024-12-03">
<day>03</day>
<month>12</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP103425</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-10-03">
<day>03</day>
<month>10</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-10-03">
<day>03</day>
<month>10</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.12.602176"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Chen et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Chen et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-103425-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Goal-directed attention relies on forming internal templates of key information relevant for guiding behavior, particularly when preparing for upcoming sensory inputs. However, evidence on how these attentional templates is represented during preparation remains controversial. Here, we combine functional magnetic resonance imaging (fMRI) with an orientation cueing task to isolate preparatory activity from stimulus-evoked responses. Using multivariate pattern analysis, we found decodable information of the to-be-attended orientation during preparation; yet preparatory activity patterns were different from those evoked when actual orientations were perceived. When perturbing the neural activity by means of a visual impulse (‘pinging’ technique), the preparatory activity patterns in visual cortex resembled those associated with perceiving these orientations. The observed differential patterns with and without the impulse perturbation suggest a predominantly non-sensory format and a latent, sensory-like format of representation during preparation. Furthermore, the emergence of the sensory-like template coincided with enhanced information connectivity between V1 and frontoparietal areas and was associated with improved behavioral performance. This dual-format mechanism suggests that during preparation the brain encodes more detailed template information beyond its immediate use, potentially providing advantages for adaptive attentional control. Consistent with recent theories of non-veridical, ‘good-enough’ attentional template for initial guidance, our findings established a neural basis for implementing two representational formats in different functional states during preparation: a predominantly non-sensory format for guidance and a latent sensory-like format for prospective stimulus processing.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Attentional selection</kwd>
<kwd>representational format</kwd>
<kwd>fMRI</kwd>
<kwd>decoding</kwd>
<kwd>visual cortex</kwd>
<kwd>impulse perturbation</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Section on Materials and Methods updated to clarify</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>To address the challenge of processing the overwhelming amount of sensory inputs from the external environment, the brain must allocate attentional resources to prioritize the processing of task-relevant information. Importantly, humans can proactively prepare for stimulus selection before the arrival of sensory inputs (<xref ref-type="bibr" rid="c49">Summerfield &amp; De Lange, 2014</xref>). For example, when preparing to hail a taxi on the road, we tend to form a mental representation of the defining features of a taxi (e.g., yellow with a car-like shape). This ability relies on the formation of attentional templates — mental representations of the target — to accelerate stimulus selection and resolve perceptual competition by enhancing task-relevant information and suppressing irrelevant information (<xref ref-type="bibr" rid="c6">Desimone &amp; Duncan, 1995</xref>; <xref ref-type="bibr" rid="c24">Kastner et al., 1999</xref>). While most attentional models posit that attentional templates during stimulus processing reflect veridical representations of the target (<xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c33">Malcolm &amp; Henderson, 2009</xref>), the nature of the template during preparation remains less understood.</p>
<p>A classical view suggests that attentional template during preparation may reflect veridical target features, analogous to the representational format during stimulus selection. However, evidence supporting this account has been mixed. For example, while some previous fMRI studies have demonstrated that preparatory activity contains target information similar to the sensory responses to the corresponding targets (<xref ref-type="bibr" rid="c27">Kok et al., 2014</xref>; <xref ref-type="bibr" rid="c30">Lewis-Peacock et al., 2015</xref>; <xref ref-type="bibr" rid="c48">Stokes et al., 2009</xref>), more recent electrophysiological studies suggest that, if anything, this template is engaged only shortly before the expected arrival of sensory input rather than being continuously active (<xref ref-type="bibr" rid="c15">Grubert &amp; Eimer, 2018</xref>; <xref ref-type="bibr" rid="c35">Myers et al., 2015</xref>). Notably, in some cases the template is even largely undetectable during preparation (<xref ref-type="bibr" rid="c55">Wen et al., 2019</xref>). Alternatively, an emerging view suggests a non-veridical template suffices for guiding attention during preparation, where precise processing of stimuli may be unnecessary at this stage. Support for this notion comes from the identification of attentional signals during preparation that differ from neural signals observed during perceptual target processing (<xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>). Recent theories of visual search also propose a non-veridical, “good-enough” template for early attentional guidance (<xref ref-type="bibr" rid="c56">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="c61">Yu et al., 2023</xref>). However, it remains unclear whether a “good-enough” template for search also applies to preparatory attention.</p>
<p>The notion that there may be a sensory and non-sensory attentional template might not be as far-fetched as it seems. Indeed, it is feasible that during preparation, following stimulus presentation, attentional signals undergo a transformation from a non-sensory to a sensory-like template. Previous behavioral (<xref ref-type="bibr" rid="c17">Hamblin-Frohman &amp; Becker, 2021</xref>; <xref ref-type="bibr" rid="c60">Yu et al., 2022</xref>) and neural studies (<xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c55">Wen et al., 2019</xref>) are generally consistent with this idea of coarse-to-fine transitions, suggesting that during preparation, a sensory-like template may not be initially necessary but only becomes relevant when the stimulus needs to be identified. However, if and in what way the brain coordinates these non-sensory and sensory-like templates remains unclear. Here, we propose that during preparation, a sensory-like template may be stored in a latent (e.g., activity-silent) state concurrently with a non-sensory template. This idea parallels recent findings from working memory studies, which suggest that information intended for proactive use is kept in activity-silent traces to support future behavior (<xref ref-type="bibr" rid="c46">Stokes, 2015</xref>; <xref ref-type="bibr" rid="c58">Wolff et al., 2015</xref>, <xref ref-type="bibr" rid="c59">2017</xref>). The present study seeks to determine the possibility of the latent, sensory-like template during the preparation for discriminating an upcoming stimulus.</p>
<p>To test these hypotheses, participants engaged in a cuing task in which they prepared during an extended period of time for the presentation of a compound stimulus grating containing the cued orientation and a distractor orientation. In addition, in order to be able to construct the sensory-format representations (leftward and rightward orientation), single orientations were presented during the perception task. Critically, we used a “pinging” technique combined with multivariate decoding methods, which has been shown to be effective in retrieving information from latent brain states (<xref ref-type="bibr" rid="c8">Duncan et al., 2023</xref>; <xref ref-type="bibr" rid="c58">Wolff et al., 2015</xref>, <xref ref-type="bibr" rid="c59">2017</xref>; <xref ref-type="bibr" rid="c62">Zhang &amp; Luo, 2023</xref>). In the standard condition (<italic>No-Ping session</italic>), the preparation period was devoid of visual impulses. During preparation, the neural activity patterns in visual and frontoparietal areas could discriminate between the orientations that participants were preparing for. Yet, neural activity patterns evoked by the perception of orientations were distinct from those evoked during preparation for upcoming orientations, suggesting a predominantly non-sensory template during preparation. By contrast, when we presented a high-contrast, task-irrelevant impulse stimulus during preparation (<italic>Ping session</italic>), neural activity patterns evoked by the perception of orientations were similar to those activated by the preparation for orientations in the visual cortex, suggesting the existence of a latent, sensory-like format of representation during preparatory attention. Furthermore, the emergence of sensory-like template coincided with enhanced information connectivity between V1 and frontoparietal areas and was associated with improved behavioral performance. Our findings provide evidence for the co-existence of two formats of attentional templates (non-sensory vs. sensory-like) during preparation, as well as a novel neural mechanism for their maintenance in different functional states (active vs. latent). We propose that this dual-format representation may serve to increase flexibility of attentional control.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Behavioral performance during the attention tasks</title>
<p>In an orientation cueing task, participants were shown a color cue indicating the reference orientation (45° or 135°) to attend to during preparation period (a delay of 5.5 s or 7.5 s) with or without the impulse perturbation (<xref rid="fig1" ref-type="fig">Fig 1A</xref>). This was followed by the presentation of a compound stimulus consisting of two oriented gratings. During the stimulus selection period (after the gratings appeared), participants were tasked with discriminating a small angular offset of the cued grating from the cued reference orientation. The angular offset was individually thresholded before the scanning sessions (mean offset = 2.50° in the No-Ping session and 2.52° in the Ping session) without significant difference between two sessions (independent t-test: t(19) = 0.085, <italic>p</italic> = 0.932, Cohen’s d = −0.027). Participants’ discrimination performance showed no significant difference between two attended orientations in either the No-Ping (paired t-test: t(19) = 1.439, p = 0.166, Cohen’s d = −0.321) or the Ping session (paired t-test: t(19) = 0.494, p = 0.627, Cohen’s d = 0.122; <xref rid="fig1" ref-type="fig">Fig 1C</xref>). A two-way mixed ANOVA (attended orientation × session) revealed neither significant main effects (attended orientation: F(1,38) = 0.392, <italic>p</italic> = 0.535, η<sub>p</sub><sup>2</sup> = 0.01; session: F(1,38) = 0.001, <italic>p</italic> = 0.970, η<sub>p</sub><sup>2</sup> &lt; 0.001) nor interaction effect (F(1,38) = 1.811, <italic>p</italic> = 0.186, η<sub>p</sub><sup>2</sup> = 0.045). Bayesian analyses provided moderate evidence to support the null hypothesis (BF<sub>excl</sub> &gt; 3.633), suggesting comparable performance levels between two sessions and two attended orientations.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig 1.</label>
<caption><title>Experiment procedure and behavioral performance.</title>
<p>(A) Attention tasks. Note that only long-delay trials are shown. A small proportion of short-delay trials (20%, with a delay of 1.5 s or 3.5 s) were included to create temporal uncertainty and encourage consistent active preparation during the delay. Both component gratings were flickering at 10 Hz between white and black, so that luminance could not confound either the task strategy (e.g., attending to luminance) or neural measures. The inset shows two sets of color-orientation mapping, which were reversed halfway through the experiment to minimize the impact of cue-induced sensory difference on neural activity. (B) Perception task. Similar to the attention task, the single-orientation grating also flickered at 10 Hz between white and black. (C) Behavioral accuracy in the attention tasks in the No-Ping and Ping sessions. Each dot represents one subject’s data. Error bars denote standard error of the means (SEM).</p></caption>
<graphic xlink:href="602176v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2b">
<title>A default, non-sensory representation of attentional template during preparation</title>
<p>The first aim of this study was to determine whether attentional signals during preparation are encoded in a sensory-like or non-sensory format. To address this, we first examined whether, in the attention task during the No-Ping condition, the distributed neural pattern contained feature-specific information. We trained and tested separate classifiers to predict the attended orientation during the preparation and stimulus selection periods (<xref rid="fig2" ref-type="fig">Fig 2A</xref>, “Attention decoding”; see Materials and Methods for details). This analysis was performed for each of the four regions along the visual hierarchy, including primary visual cortex (V1), extrastriate visual cortex (EVC), intraparietal sulcus (IPS) and prefrontal cortex (PFC). The average decoding accuracies for both preparation and stimulus selection periods were significantly above chance level in each region (Permutation analyses: <italic>ps</italic> &lt; 0.004 across regions, <xref rid="fig2" ref-type="fig">Fig 2B</xref>), indicating that the brain maintained reliable information about the attended feature both before and after the onset of the compound grating. Next, we examined whether the preparatory activity reflected a sensory-like format of attentional template (<xref rid="fig2" ref-type="fig">Fig 2A</xref>, “Cross-task generalization”, see Materials and Methods). We trained a classifier using data from the perception task (leftward vs. rightward orientation; <xref rid="fig1" ref-type="fig">Fig 1B</xref>) and tested its performance on data from the preparation period in the attention task (attend leftward vs. attend rightward). However, this cross-task generalization analysis yielded no significant effects (<italic>ps</italic> &gt; 0.132 across the regions). In contrast, we observed above-chance generalization from the perception task to the stimulus selection period (<italic>ps</italic> &lt; 0.001 across regions, <xref rid="fig2" ref-type="fig">Fig 2C</xref>), confirming previous findings of the sensory-like attentional template following stimulus presentation (<xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c55">Wen et al., 2019</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig 2.</label>
<caption><title>MVPA for the No-Ping session and Ping session.</title>
<p>(A) Schematic illustration of the decoding of attended orientation (attend leftward vs. attend rightward) in the attention task (left panel) and the cross-task generalization analysis from perception task to the attention task (right panel). The four regions are shown on a representative right hemisphere as colored areas: V1 is marked in red, EVC in yellow, IPS in cyan and PFC in purple. (B) Decoding accuracy during preparation and stimulus selection periods across regions in the No-Ping and (D) Ping session. (C) Cross-task generalization performance from the perception task to the preparatory periods and the stimulus selection periods across regions in the No-Ping and (E) Ping session. The dashed lines represent the theoretical chance level (0.5). Each dot represents one subject’s data. Error bars denote SEM.</p></caption>
<graphic xlink:href="602176v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Before drawing conclusions based on the lack of generalization from the perception task to preparatory attention, we considered two alternative explanations to rule out potential confounds. First, the robust attention decoding during preparation ruled out the possibility that participants were not actively engaged in the task during preparation (<xref rid="fig2" ref-type="fig">Fig 2B</xref>, unfilled bars). Second, the generalizable effect from the perception task to the stimulus selection period across regions (<xref rid="fig2" ref-type="fig">Fig 2B</xref>, filled bars) argues against the possibility of low statistical power. Overall, these findings suggest that the preparatory attention and sensory processing of features have distinct formats, presumably reflecting a non-sensory format of representation during the preparation. These results replicate those of a previous fMRI study using motion stimuli with a similar design(<xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>). Furthermore, consistent with previous studies(<xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>), univariate analysis did not reveal any reliable difference in overall BOLD responses between attention orientations (Supplementary Information and S1 Fig).</p>
</sec>
<sec id="s2c">
<title>A latent, sensory-like attentional template during preparation revealed by visual impulse</title>
<p>The second aim of our study was to examine whether a latent, sensory-like template exists during preparation. While this precise template may not be necessary for preparation, it is relevant for subsequent target selection and discrimination (i.e., select the cued grating from the compound stimulus and discriminate a small angular offset between the cued grating and the reference orientation). To test this hypothesis, we perturbed the neural activity by means of a visual impulse during the preparation period in the Ping session (<xref rid="fig1" ref-type="fig">Fig 1A</xref>, right panel). Using the same analyses as those performed in the No- Ping session, robust attentional signals were observed during both preparation and stimulus selection periods (Permutation analyses: <italic>ps</italic> &lt; 0.001 across regions; <xref rid="fig2" ref-type="fig">Fig 2D</xref>). Importantly, the cross-task generalization analyses indicated that the visual impulse led to above-chance generalization from the perception task to preparation period (<xref rid="fig2" ref-type="fig">Fig 2E</xref>, unfilled bars) in V1 and EVC (<italic>ps</italic> &lt; 0.001), but not in IPS and PFC (<italic>ps</italic> &gt; 0.584), along with generalizable effects from the perception task to the stimulus selection periods (<italic>ps</italic> &lt; 0.036 across regions; <xref rid="fig2" ref-type="fig">Fig 2E</xref>; filled bars). These results suggest that different brain areas are involved in coding for sensory-like templates. To further evaluate whether the cross-task generalization from the perception task to the preparation period was statistically different with and without visual impulse, we conducted a two-way mixed ANOVA (session × region) on the generalization performance. The analysis revealed main effects of region (F(3,114) = 5.220, <italic>p</italic> = 0.002, η<sub>p</sub><sup>2</sup> = 0.121), session (F(1,38) = 7.321, <italic>p</italic> = 0.010, η<sub>p</sub><sup>2</sup> = 0.162), and importantly, a significant interaction effect (F(3,114) = 3.964, <italic>p</italic> = 0.010, η<sub>p</sub><sup>2</sup> = 0.094), supporting the observation that the visual impulse led to significantly increased decoding accuracy in V1 (independent t-test: t(38) = 3.145<italic>, p</italic> = 0.003; Cohen’s d = 0.995) and EVC (independent t-test: t(38) = 2.153, <italic>p</italic> = 0.038, Cohen’s d = 0.681), but not in the frontoparietal regions (<italic>ps</italic> &gt; 0.374). This dissociable result between the two sessions further supports the activation of a latent, sensory-like template by the visual impulse during preparatory attention.</p>
<p>To further solidify this conclusion, following analyses were used to examine several alternative possibilities. First, we examined whether the impulse-driven generalization resulted from stronger feature information in the Ping compared to No-Ping session during the perception task. This was not the case, as evidenced by comparable levels of decodable orientation information between the Ping and No-Ping sessions (see Supplementary Information and S2 Fig). Next, we asked whether the increased generalization was due to generally stronger attentional signals in the Ping session during the attention tasks. This was not the case, as the two-way mixed ANOVAs (session × region) on attention decoding accuracy revealed neither a significant main effect of session nor an interaction effect during both the preparation (<italic>ps</italic> &gt; 0.519; BF<sub>excl</sub> &gt; 3.247) and stimulus selection periods (<italic>ps</italic> &gt; 0.336; BF<sub>excl</sub> &gt; 3.297), suggesting comparable amount of attentional information between the two sessions. Therefore, the findings of impulse-driven sensory-like template in the visual cortex during preparation cannot be explained by general differences between two sessions.</p>
</sec>
<sec id="s2d">
<title>Matching preparatory attention to sensory template: impact on neural representation and behavior</title>
<p>The reported decoding accuracy from the cross-task generalization analysis quantifies the degree to which differences in neural activity pattern between two conditions are shared across attention and perception tasks. However, it does not directly measure how similar the neural patterns are when attending to an orientation compared to perceiving that orientation. Unlike decoding accuracies, Mahalanobis distance provides a continuous measure for characterizing representational geometries between different conditions (<xref ref-type="bibr" rid="c32">Mahalanobis, 1936</xref>). To further corroborate our findings of the impulse-driven sensory-like template, we calculated the Mahalanobis distance between each attention condition during preparation and each perception condition (see Materials and Methods). If the patterns of activity reflect a sensory-like template, we would expect greater pattern similarity (smaller distance) between “attend leftward” and “perceive leftward” than between “attend leftward” and “perceive rightward”, and vice versa for the “attend rightward” conditions (see <xref rid="fig3" ref-type="fig">Fig 3A</xref> for a schematic of the four pair-wise distance measures), leading to an interaction between attended and perceived orientation conditions.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Orientation-selective attentional modulations on neural pattern distances during preparation.</title>
<p>(A) Schematic illustration of the representational distance (mean Mahalanobis distances) between each of the attention conditions and each of the perception conditions. Colored arrows indicate measures of the pair-wise Mahalanobis distance. The right panel shows two attention trials (red indicates attend-to-leftward and green indicates attend-to-rightward) to the distribution of each perception condition (shown in a cloud of light-colored dots). (B) Mahalanobis distance between preparatory attention condition and perceived orientation condition in the No-Ping and (C) Ping sessions. (D) Behavioral performance (as indexed by reaction time) for “strong modulation” (high AMI) trials and “weak modulation” (low AMI) trials, sorted by V1 preparatory activity in the No-Ping and (E) Ping sessions. Each dot represents one subject’s data. Error bars denote SEM. * <italic>p</italic> &lt; 0.05, ** <italic>p</italic> &lt; 0.01.</p></caption>
<graphic xlink:href="602176v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We used a two-way repeated-measures ANOVA (attended orientation × perceived orientation) on the Mahalanobis distance, separately for each session and each region. During the preparatory period in the No-Ping session, no significant interaction effects were observed across regions (<italic>ps</italic> &gt; 0.443; <xref rid="fig3" ref-type="fig">Fig 3B</xref>). In contrast, the same analyses applied to the Ping session revealed significant interaction effects in visual areas (V1: F(1,19) = 9.335, <italic>p</italic> = 0.007, η<sub>p</sub><sup>2</sup> = 0.329; EVC: F(1,19) = 8.563, <italic>p</italic> = 0.009, η<sub>p</sub><sup>2</sup> = 0.311; <xref rid="fig3" ref-type="fig">Fig 3C</xref>), but not for frontoparietal regions (<italic>ps</italic> &gt; 0.213). This cross-region difference is consistent with the function of sensory areas in encoding precise neural representations for basic visual features. To directly compare whether the attentional modulation on Mahalanobis distance was statistically different with and without the visual impulse, we used a three-way mixed ANOVA (session × attended orientation × perceived orientation). The analysis revealed a significant three-way interaction in V1 (F(1,38) = 5.00, <italic>p</italic> = 0.031, η<sub>p</sub><sup>2</sup> = 0.116), suggesting that by “pinging” the brain, the attentional template during preparation became more similar to the perception of corresponding orientation. We also calculated the Mahalanobis distance between neural patterns evoked by the superimposed gratings during the stimulus selection period and each condition in the perception task, finding similar results (see Supplementary Information and S3 Fig). This result was expected, as feature-based attention is known to selectively enhance the representation of task-relevant features while filtering out task-irrelevant ones.</p>
<p>The continuous nature of the Mahalanobis distance also made it possible to further investigate potential neural-behavioral correlations. We examined whether activating a sensory-like template during preparation would benefit subsequent orientation processing. In particular, we calculated attentional modulation indices (AMIs) based on trialwise Mahalanobis distance in V1. The index was calculated as follows: AMI = (D<sub>different</sub> – D<sub>same</sub>)/(D<sub>different</sub> + D<sub>same</sub>), where D<sub>same</sub> and D<sub>different</sub> are the measured distance (D) in the Same (e.g., attend and perceive the same orientation) and Different (e.g., attend and perceive different orientations) orientation condition, respectively (see Methods and Materials). We then sorted the AMI values in a descending order and selected the top-ranked 25% of trials (i.e., 36 trials) and bottom-ranked 25% of trials to represent “strong modulation” and “weak modulation” trials, respectively and calculated reaction time and accuracy for each group of trials. The analysis revealed a faster response in the “strong modulation” than “weak modulation” trials in Ping session (paired t-test: t(19) = −2.746, <italic>p</italic> = 0.013, Cohen’s d = −0.614; <xref rid="fig3" ref-type="fig">Fig 3E</xref>), but not in the No-Ping session (paired t-test: t(19) = −1.487, <italic>p</italic> = 0.154, Cohen’s d = −0.332; <xref rid="fig3" ref-type="fig">Fig 3D</xref>). These effects were not observed for accuracy (<italic>ps</italic> = 0.163). These results suggest that the impulse-driven sensory-like template in primary visual cortex is functionally relevant to subsequent attentional selection, providing evidence for the prospective use of sensory-like template in this task. In addition, we did not observe such behavioral differences in analogous analyses using data from the stimulus selection period in either session (<italic>ps</italic> &gt; 0.230), which might due to the potential dilution by strong stimulus-evoked responses during the stimulus selection period.</p>
</sec>
<sec id="s2e">
<title>Activating sensory-like template strengthens the informational connectivity between sensory and frontoparietal areas</title>
<p>Selective attention is generally believed to rely on coordinated network activity (<xref ref-type="bibr" rid="c5">Corbetta &amp; Shulman, 2002</xref>). In particular, studies have shown that functional connectivity between sensory and frontoparietal areas was modulated by attentional control (<xref ref-type="bibr" rid="c4">Bressler et al., 2008</xref>; <xref ref-type="bibr" rid="c39">Rosenberg et al., 2020</xref>). Given that the impulse-driven sensory-like template facilitated behavior, we reasoned that it may also enhance network communication. Thus, we examined informational connectivity measures to explore how the impulse altered network function during the attention task. We used a method that allows inference based on multivoxel pattern information rather than univariate BOLD response (<xref ref-type="bibr" rid="c22">Jia et al., 2020</xref>; <xref ref-type="bibr" rid="c36">Ng et al., 2021</xref>). For each ROI, we calculated the cross-validated Mahalanobis distance from each attention trial (from one left-out run) to the distribution of each attended orientation (all trials from remaining runs) during preparation (see Methods and Materials). To quantify the degree of attentional modulation during preparation, we calculated the AMI based on trialwise Mahalanobis distance and generated a time course of AMI values across trials. Pearson correlation was used to estimate the covariation between each pair of ROIs, the resulting correlation coefficients were transformed using Fisher’s z-transform for statistical inference (<xref rid="fig4" ref-type="fig">Fig 4A</xref>). The analysis revealed numerically higher levels of connectivity in Ping than in No- Ping session, this impulse-driven increase in connectivity reached statistical significance in two pairs (<xref rid="fig4" ref-type="fig">Fig 4B</xref>): V1-IPS (independent t-test: t(38) = 2.566<italic>, p</italic> = 0.014; Cohen’s d = 0.812) and V1-PFC (independent t-tests: t(38) = 3.158<italic>, p</italic> = 0.003; Cohen’s d = 0.999). The enhanced functional connectivity between V1 and frontoparietal areas driven by the impulse may potentially facilitate information flow among areas to improve attentional control. Additionally, the same analysis of AMI based on cross-validated Mahalanobis distance during the stimulus selection period showed no significant differences in information connectivity between No-Ping and Ping sessions (<italic>ps</italic> &gt; 0.224; Supplementary Information and S4A Fig). The lack of changes in long-range connectivity during stimulus selection period may be attributed to a general rise in connectivity caused by strong sensory inputs in this period, which could have attenuated any potential impacts of visual impulses. Furthermore, connectivity analysis based on mean BOLD response over time did not reveal any significant changes in inter-cortical connections between the two sessions (<italic>ps</italic> &gt; 0.136; S4B Fig), suggesting that the impulse-driven increased information connectivity between V1 and higher-order areas was unlikely contributed by the overall changes of BOLD response.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig 4.</label>
<caption><title>Information connectivity analysis.</title>
<p>(A) Schematic illustration of the procedure for the information connectivity analysis in the space of two hypothetical voxels. For each region, we calculated the Mahalanobis distance of the attention trial (from one left-out run) from two attention distributions (all trials from remaining runs). Red and green dots indicate activity patterns from two trials (right panel). The brain image shows an example pair of intercortical information connectivity between V1 and PFC. The time series (lower-left panel) consisted of attentional modulation index (AMI) based on the Mahalanobis distance. (B) Between-region information connectivity in the No-Ping session and Ping session (left panel), and differences in connectivity between the two sessions (right panel). * <italic>p</italic> &lt; 0.05, ** <italic>p</italic> &lt; 0.01.</p></caption>
<graphic xlink:href="602176v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>While there is ample evidence that the brain can maintain an attentional template of an upcoming target before sensory information is presented (<xref ref-type="bibr" rid="c6">Desimone &amp; Duncan, 1995</xref>; <xref ref-type="bibr" rid="c24">Kastner et al., 1999</xref>; <xref ref-type="bibr" rid="c49">Summerfield &amp; De Lange, 2014</xref>), its representational format remains unclear. To address this, we used an orientation cueing paradigm with separated preparation and stimulus selection periods and applied MVPA to decode neural activity patterns associated with feature-specific attentional information of the upcoming target. The analyses showed robust attentional information both before and after the presentation of the compound grating, suggesting sustained maintenance of attentional templates throughout a trial. Importantly, while the decoders trained on the perception of single orientations could not generalize to preparation until the stimulus selection period (<italic>No-Ping session</italic>), perturbing the brain with a visual impulse resulted in generalizable activity patterns during preparation in the visual cortex (<italic>Ping session</italic>). These results suggest a predominantly non-sensory format of representation, with a sensory-like template in a latent state during feature-based preparation in the visual cortex. Furthermore, impulse-driven sensory-like template was accompanied by enhanced information connectivity between V1 and frontoparietal areas, as well as enhanced orientation-specific neural modulations of neural distances in the visual areas that predicted levels of behavioral performance. The differences between the Ping and No- Ping sessions could not be attributed to differences in sensory information from the perception task, overall strength of preparatory attention, or differences in eye position (S5 Fig). Therefore, our findings suggest dual-format neural representations (non-sensory vs. sensory-like) operating in different functional states (active vs. latent). This mechanism may give rise to flexible attentional control, allowing effective transition from coarse to fine attentional templates at various processing stages (initial guidance vs. precise discrimination).</p>
<p>Recent advances in theories of visual search differentiated between the “guiding template” and “target template” based on measures of behavioral performance and eye movements (<xref ref-type="bibr" rid="c56">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="c61">Yu et al., 2023</xref>). According to these theories, early attentional guidance typically depends on a non-veridical template that includes only the most diagnostic information, whereas later, target-match processes utilize more precise codes to optimize decision accuracy (<xref ref-type="bibr" rid="c25">Kerzel, 2019</xref>; <xref ref-type="bibr" rid="c43">Scolari et al., 2012</xref>; <xref ref-type="bibr" rid="c60">Yu et al., 2022</xref>). Our study reveals a parallel coding mechanism in the context of feature-based attention, expanding upon these theoretical notions in three key aspects. First, we provide neural evidence for a default, predominantly non-sensory template during preparation, indicating that the concept of a “guiding template”, as proposed by visual search theories (<xref ref-type="bibr" rid="c56">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="c61">Yu et al., 2023</xref>), also applies to preparatory attention in a non-search context. This highlights a shared functional role of a non-veridical attentional template in early guidance across different scenarios. Second, despite the theoretical notion that the brain maintains a more veridical template with detailed target information than is typically utilized to form the guiding template (<xref ref-type="bibr" rid="c56">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="c61">Yu et al., 2023</xref>), direct evidence supporting this hypothesis is currently lacking. We provide evidence for this notion and propose a plausible neural implementation for preserving a more veridical, sensory-like template in the latent state. A natural question is why the sensory-like template remains latent during preparation. We note that our task requires both coarse and fine featural information. Specifically, during preparation, the brain needs to establish a guiding template that aids in distinguishing the target from the distinct distractor; hence a coarse, non-veridical template suffices. During stimulus selection, the fine discrimination task (reporting the tilted direction of a small angular offset) necessitates the representation of a precise template. It is thus both advantageous and efficient to maintain a sensory-like template in a latent state during preparation, which could facilitate the processing of sensory stimulus in the future. This idea is analogous to recent development in working memory research, which suggests that mnemonic information intended for future use often remains in a latent, activity-silent state (<xref ref-type="bibr" rid="c47">Stokes et al., 2020</xref>; <xref ref-type="bibr" rid="c58">Wolff et al., 2015</xref>, <xref ref-type="bibr" rid="c59">2017</xref>). Finally, information connectivity between visual and higher-order frontoparietal regions was enhanced by visual impulse during preparation, suggesting improved information flow across the relevant areas allowing enhanced attentional control. This enhanced attentional control may contribute to refined sensory representations of target in early visual cortex, facilitating transitions from a non-sensory to sensory-like format template. Future studies may adopt layer-specific fMRI to infer the direction of this improved information flow (<xref ref-type="bibr" rid="c21">Jia et al., 2023</xref>) and explore the relationship between long-range connections and the utilization of different versions of target templates.</p>
<p>Our study reveals a latent, sensory-like attentional template that extends beyond previous findings of latent working memory representations (<xref ref-type="bibr" rid="c58">Wolff et al., 2015</xref>, <xref ref-type="bibr" rid="c59">2017</xref>). Unlike previous studies, which used the “pinging technique” to differentiate active and latent representations of <italic>different</italic> items (e.g., attended vs. unattended memory item), our study demonstrated the active and latent representations of a single item in different formats (i.e., non-sensory vs. sensory-like). Moreover, while the “pinging technique” did not evoke sensory-like neural patterns during memory retention (<xref ref-type="bibr" rid="c59">Wolff et al., 2017</xref>), we identified sensory-like format of representations during preparatory attention. A potential neural mechanism for this sensory-like template is via an “activity-silent” mechanism through short-term changes in synaptic weights, originally proposed for working memory storage (<xref ref-type="bibr" rid="c34">Mongillo et al., 2008</xref>). However, whether pinging identifies an activity-silent mechanism is currently debated (<xref ref-type="bibr" rid="c2">Barbosa et al., 2021</xref>; <xref ref-type="bibr" rid="c42">Schneegans &amp; Bays, 2017</xref>). An alternative possibility is that the visual impulse amplified a subtle but active representation of the sensory template during preparation. Distinguishing between these two accounts likely requires neurophysiological measurements, which are beyond the scope of the current study. Nevertheless, our findings provide evidence for dual-format attentional template including a latent, sensory template and offer insights into how the brain encodes information beyond immediate uses to support adaptive behavior.</p>
<p>The non-generalizable activity patterns from perception to preparatory attention, in the absence of visual impulse, suggests a default, predominantly non-sensory template during preparation. This finding is largely consistent with electrophysiological studies (<xref ref-type="bibr" rid="c35">Myers et al., 2015</xref>; <xref ref-type="bibr" rid="c55">Wen et al., 2019</xref>) and our prior fMRI work on preparatory attention to motion directions (<xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>), but differs from some previous neuroimaging studies that demonstrated sensory-like templates during preparation (<xref ref-type="bibr" rid="c27">Kok et al., 2014</xref>; <xref ref-type="bibr" rid="c37">Peelen &amp; Kastner, 2011</xref>; <xref ref-type="bibr" rid="c48">Stokes et al., 2009</xref>). One potential account for these discrepancies is that those studies used cue-only trials where the target was expected but not actually presented, contrasting with our task where the target was shown on every trial with temporally separated preparation and stimulus selection periods. This seemingly subtle difference may significantly impact the formats of the neural representations. Because cue-only trials increased the likelihood of target appearance at the subsequent time point, sensory template may be activated due to modulations of temporal expectations (<xref ref-type="bibr" rid="c15">Grubert &amp; Eimer, 2018</xref>). This explanation is consistent with theories suggesting differential influences of expectation and attention on neural activity: expectation reflects visual interpretations of stimuli due to sensory uncertainty, whereas attention is guided based on the task relevance of sensory information (<xref ref-type="bibr" rid="c40">Rungratsameetaweemana &amp; Serences, 2019</xref>; <xref ref-type="bibr" rid="c50">Summerfield &amp; Egner, 2009</xref>, <xref ref-type="bibr" rid="c51">2016</xref>). Our finding of a predominantly non-sensory format may indicate an optimized coding strategy employed by the brain to effectively and robustly represent information for future use. This aligns with the proposed role of attention in modulating sensory representations to encode only currently relevant information at a minimal cost.</p>
<p>We acknowledge that our findings cannot pinpoint the exact format of this non-sensory template. Previous behavioral studies on visual search have suggested a candidate mechanism for target-defining features. For instance, when searching for simple features, such as an orientation or a color (<xref ref-type="bibr" rid="c29">Kong et al., 2017</xref>; <xref ref-type="bibr" rid="c57">Wolfe et al., 1992</xref>), participants tend to use categorical attributes (e.g., steep vs. shallow; left-tilted vs. right-tilted) to enhance search efficiency. The categorical template is particularly advantageous when features are consistent and predictable (<xref ref-type="bibr" rid="c19">Hout et al., 2017</xref>). In our task, the angular relations between the target and distractor orientation were defined by categorical attributes (e.g., left-tilted vs. right-tilted) and remained consistent across trials, making a categorical template feasible during preparatory attention. Furthermore, employing a categorical template allows for greater tolerance of stimulus variability, which is also useful in our study where the actual target orientation varied around the reference orientations across trials. Future studies are needed to address the nature of the non-sensory template during preparation as well as task parameters that might modulate them.</p>
<p>In summary, the current study suggests that there are two formats of attentional templates each having a distinct functional state: a default, non-sensory format and a latent, sensory-like format. This dual-format representation aligns with theories on the dual-function of attentional template for different task goals (<xref ref-type="bibr" rid="c18">Hout &amp; Goldinger, 2015</xref>; <xref ref-type="bibr" rid="c61">Yu et al., 2023</xref>). The current findings provide a plausible neural implementation for these theories by demonstrating different formats in different functional states. This mechanism likely reflects an optimized coding scheme that effectively balances processing efforts and demands, particularly well-suited for flexible control and transitions from coarse to fine task demands in visually guided behavior.</p>
</sec>
<sec id="s4">
<title>Materials and methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Twenty individuals participated in the No-Ping session (11 females, mean age = 22.9) and twenty individuals participated in the Ping session (14 females, mean age = 23.7). Among them, 14 participants took part in both sessions, while 12 of them took part in only one session. The sample size was comparable to previous studies using similar attention tasks (<xref ref-type="bibr" rid="c1">Baldauf &amp; Desimone, 2014</xref>; <xref ref-type="bibr" rid="c13">Gong &amp; Liu, 2020a</xref>; <xref ref-type="bibr" rid="c14">2020b</xref>; <xref ref-type="bibr" rid="c16">Guo et al., 2012</xref>; <xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c31">Liu &amp; Hou, 2013</xref>). Because our primary interest is the generalization from the perception task to attention task, we used the minimal effect size of decoding accuracy across regions (one-sample t-tests: <italic>d</italic> = 0.868) from our previous study with a similar design (<xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>), and used G*Power (Version 3.1) (<xref ref-type="bibr" rid="c10">Faul et al., 2007</xref>)to confirm that this sample size is sufficient to detect a cross-task generalization effect with a power greater than 95% (a = 0.05). All participants were right-handed and had a normal or corrected-to-normal vision. Participants provided written informed consent according to the study protocol approved by the Institutional Review Board at Zhejiang University (2020-06-001). They were paid ¥200 (∼$27.4) for their participation in each session.</p>
</sec>
<sec id="s4b">
<title>Stimuli and Apparatus</title>
<p>Stimuli were generated using Psychtoolbox (<xref ref-type="bibr" rid="c3">Brainard, 1997</xref>; <xref ref-type="bibr" rid="c26">Kleiner et al., 2007</xref>) implemented in MATLAB. The stimuli were presented on an LCD monitor (resolution: 1920 × 1080, refresh rate: 60 Hz) during behavioral training, at a viewing distance of 90 cm in a dark room. During the fMRI scans, stimuli were projected to a screen via a MR-compatible LCD projector (PT-011, Jiexin Technology Co., Ltd., Shenzhen, China) with the same resolution and refresh rate as the LCD monitor during behavioral training. Participants viewed the screen via an angled mirror attached to the head coil at a viewing distance of 115 cm. Angular stimulus size was the same across behavioral and fMRI sessions.</p>
<p>The orientation stimuli were square-wave gratings (1.3 cycles per deg, duty cycle: 10%) in a circular aperture (inner radius: 1.5°; outer radius: 6°). The gratings flashed on a gray background at 10 Hz, alternating between black and white. There were two types of stimuli: two overlapping gratings orientated leftward (∼135°) and rightward (∼45°), or a single grating with one of the two orientations (∼135° or ∼45°). Here, we refer to the 45° and 135° orientations as the reference orientations. The impulse stimulus was a high-contrast, white (at the maximum projector output level) circular disk that covered the same area as the orientation stimulus (radius: 6°).</p>
</sec>
<sec id="s4c">
<title>Experimental Procedures and Tasks</title>
<p>Each participant completed at least two fMRI sessions on different days. One session was used for defining ROIs (see Definition of Regions of Interests), while the remaining sessions were used for the main experiment (see Attention task and Perception task). Before the scanning sessions, participants were trained to familiarize with the tasks in a separate behavioral session. The procedures and tasks were similar to our previous work (<xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>).</p>
<p><italic>Attention task.</italic> We used a cueing paradigm (<xref rid="fig1" ref-type="fig">Fig 1A</xref>). Each trial began with a color cue (red or green) for 0.5 s to indicate the reference orientation of the upcoming target (leftward vs. rightward orientation). In the No-Ping session, the cue was followed by a blank display during the preparation period; in the Ping session, a task-irrelevant, high-luminance visual impulse (“ping”, 0.1 s) occurred at either 0.5 s (for short delays of 1.5 s and 3.5 s) or 2.5 s (for long delays of 5.5 s and 7.5 s) after the onset of the cue display. The orders of these sessions were counterbalanced across participants who completed both. Following the preparatory period, two superimposed gratings were then shown for 1 s. The target grating was shown with a small angular offset with respect to the cued reference orientation, whereas the distractor grating was shown in the uncued reference orientation (e.g., if rightward orientation was cued, the rightward grating was shown in 45°± d and the leftward grating was shown in 135°). Note that the angular offset was determined individually based on the threshold obtained during the training session (at least 3 blocks, 30 trials/block), using a staircase procedure (Best Parameter Estimation by Sequential Testing, Best PEST), as implemented in the Palamedes Toolbox (<xref ref-type="bibr" rid="c38">Prins &amp; Kingdom, 2009</xref>). Participants used a keypad to report whether the attended orientation was more leftward or rightward relative to the reference orientation. Each trial was separated by an inter-trial interval of 3 s to 7 s (2 s per step). Trial-by-trial feedback (“correct” or “incorrect”) was provided in the training session but not during scanning. Instead, the percentage of correct responses was provided at the end of each run in the scanning session to avoid the impact of trial-level feedback on neural activity.</p>
<p>To prevent the cue-related sensory difference from contributing to neural activity, we reversed the mapping between colors and orientations halfway through the experiment (e.g., red indicated “attend leftward orientation” and green indicated “attend rightward orientation” in the first half of the runs, and vice versa for the second half of the runs), with the order counterbalanced across subjects. The mapping of colors and orientations was reversed only once in the middle of the experiment to prevent misremembering of the color-orientation associations. To reduce temporal expectancy over a fixed period, the preparatory period (i.e., cue-to-stimulus interval) varied from 1.5 s to 7.5 s with different probabilities (10% for 1.5 s or 3.5 s each, 40% for 5.5 s or 7.5 s each). The long-delay trials (5.5 s or 7.5 s) were selected for subsequent analyses, as they allow the separation of the preparatory activity from the grating-evoked response during fMRI scanning. The short-delay trials were included to encourage a sustained maintenance of attention throughout the entire preparation period.</p>
<p><italic>Perception task.</italic> On each trial of the perception task (<xref rid="fig1" ref-type="fig">Fig 1B</xref>), a single grating was shown for 1 s, followed by an inter-trial interval between 3 s to 7 s. To equate the sensory inputs between attention and perception tasks, the orientation was shifted away from the reference orientation by the same angular offset as that used in the attention task with each individual participant’s own threshold. Participants performed the same orientation discrimination task by comparing the single orientation to the reference orientation. We provided the percentage of correct responses at the end of each run as feedback.</p>
</sec>
<sec id="s4d">
<title>Eye Tracking and Analysis</title>
<p>To evaluate the stability of visual fixation, we used Eyelink Portable Duo system (SR Research, Ontario, Canada) to monitor each observer’s eye position during the training session at a sampling rate of 500 Hz. One participant’s data was not used due to the unstable recording of the eye. The data were then analyzed using custom Matlab code.</p>
<p>To examine whether participants adopted a space-based strategy during the preparatory period in the attention task, such as directing their gaze leftward in attend leftward trials, and vice versa for the attend rightward trials, we analyzed the participants’ eye positions recorded during the training session. Horizontal and vertical eye positions were analyzed separately. Paired t-tests were performed to compare horizontal and vertical eye positions between two attention conditions. A two-way mixed ANOVA (2 sessions × 2 attended orientations) was applied to the horizontal and vertical positions, respectively.</p>
</sec>
<sec id="s4e">
<title>fMRI Data Acquisition</title>
<p>Imaging was performed on a Siemens 3T scanner (MAGNETOM Prisma, Siemens Healthcare, Erlangen, Germany) using a 20-channel coil at Zhejiang University (Hangzhou, China). For each participant, we acquired high-resolution T1-weighted anatomical images (field of view, 240 × 240 mm, 208 sagittal slices; 0.9 mm<sup>3</sup> resolution), T2*-weighted echo-planar functional images consisting of 46 slices (TR, 2 s; TE, 34 ms; flip angle, 50°; matrix size, 80 × 80; in-plane resolution, 3 × 3 mm; slice thickness, 3 mm, interleaved, no gap) and a 2D T1-weighted anatomical image (0.8 × 0.8 × 3 mm) for aligning functional data to high-resolution anatomical data.</p>
</sec>
<sec id="s4f">
<title>fMRI Data Preprocessing</title>
<p>Data analyses were performed using mrTools (<xref ref-type="bibr" rid="c11">Gardner et al., 2018</xref>) and custom code in Matlab. For each run, functional data were preprocessed with head motion correction, linear detrending and temporal high pass filtering at 0.01 Hz. Data were converted to percentage signal change by dividing the time course of each voxel by its mean signals in each run. We concatenated the 6 runs of the attention task and the 3 runs of the perception task separately for further analysis. One of the attention runs in one subject was excluded due to low accurate performance (&lt;50%).</p>
</sec>
<sec id="s4g">
<title>Definition of Regions of Interests (ROI)</title>
<sec id="s4g1">
<title>Visual and parietal ROIs</title>
<p>Following previous work (<xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>; Gong et al., 2020b; <xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>), for each observer, we ran a separate retinotopic mapping session to obtain ROIs in occipital and parietal areas. Observers viewed four runs of rotating wedges (i.e., clockwise and counterclockwise) and two runs of rings (i.e., expanding and contracting) to map the polar angle and radial components, respectively (<xref ref-type="bibr" rid="c7">DeYoe et al., 1996</xref>; <xref ref-type="bibr" rid="c9">Engel, 1997</xref>; <xref ref-type="bibr" rid="c44">Sereno et al., 1995</xref>). Borders between areas were defined as the phase reversals in a polar angle map of the visual field. Phase maps were visualized on computationally flattened representations of the cortical surface, which were generated from the high-resolution anatomical image using FreeSurfer (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu">http://surfer.nmr.mgh.harvard.edu</ext-link>) and custom Matlab code.</p>
<p>To help identify the topographic areas in parietal areas, we ran 2 runs of memory-guided saccade task modeled after previous studies (<xref ref-type="bibr" rid="c28">Konen &amp; Kastner, 2008</xref>; <xref ref-type="bibr" rid="c41">Schluppeck et al., 2006</xref>; <xref ref-type="bibr" rid="c45">Sereno et al., 2001</xref>). Observers fixated at the screen center while a peripheral (∼10° radius) target dot was flashed for 500 ms. The flashed target was quickly masked by a ring of 100 distractor dots randomly positioned within an annulus (8.5° – 10.5°). The mask remained on screen for 3 s, after which participants were instructed to make a saccade to the memorized target position, then immediately saccade back to the central fixation. The position of the peripheral target shifted around the annulus from trial to trial in either a clockwise or counterclockwise order. Data from the memory-guided saccade task were analyzed using the same phase encoding method as the wedge and ring data. Therefore, the following regions of interest (ROIs) in each hemisphere were identified after the completion of this session: V1, V2, V3, V3A/B, V4, V7/IPS0, IPS1 to IPS4.</p>
</sec>
<sec id="s4g2">
<title>Frontal ROIs</title>
<p>Following previous work (<xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>; Gong et al., 2020b; <xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>), we used a deconvolution approach by fitting each voxel’s time series from the attention task with a general linear model (GLM) to determine the event-related activations in the brain (see Supplementary Materials: Deconvolution). For each voxel, we computed the goodness of fit measure (r<sup>2</sup> value), which indicates the amount of variance explained by the deconvolution model (<xref ref-type="bibr" rid="c11">Gardner et al., 2018</xref>). The r<sup>2</sup> value represents the degree to which the voxel’s time series is correlated with the task events, regardless of any differential responses among conditions. Based on the task-related activation (as indexed by r<sup>2</sup> value) and anatomical criteria, we defined two frontal areas in each hemisphere that were active during the attention task: one is located superior to the precentral sulcus and near the superior frontal sulcus (FEF) and the other is located towards the inferior precentral sulcus, close to the junction with the inferior frontal sulcus (IFJ).</p>
</sec>
<sec id="s4g3">
<title>Groups of Region</title>
<p>To characterize the patterns of neural response across cortical hierarchy and streamline data presentation, we grouped results from the nine areas into four groups based on functional and anatomical considerations: primary visual cortex (V1); extrastriate visual cortex (EVC), consisting of V2, V3, V3ab and V4; IPS, consisting of IPS0 to IPS4; prefrontal cortex (PFC), consisting of FEF and IFJ. Individual areas within each group exhibited qualitatively similar results.</p>
</sec>
</sec>
<sec id="s4h">
<title>Multivoxel Pattern Analysis (MVPA)</title>
<sec id="s4h1">
<title>Decoding of attended orientation</title>
<p>To test if multivariate patterns of activity represent information of the attended orientation, separate MVPA analyses were applied on the activity patterns for the preparation and stimulus selection periods. Following previous work (<xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>; Gong et al., 2020b; <xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>), for this analysis, we extracted fMRI signals from raw time series in the long delay trials with correct behavioral responses (∼72 trials per attention condition); short delay trials were excluded as they could not provide enough data points to measure preparatory activity. We then obtained averaged BOLD response in a 2 s window for each voxel and each trial in a given ROI, separately for preparatory activity (4 to 6 s after the onset of the cue) and stimulus-evoked activity (4 to 6 s after the onset of the gratings). The response amplitudes across two attention conditions in each ROI were further z-normalized, separately for the preparation and stimulus-related activity. These normalized single-trial BOLD responses were used for the MVPA. We trained a classifier using the Fisher linear discriminant (FLD) analysis to discriminate between two attended orientations (leftward vs. rightward) and tested its performance with a leave-one-run-out cross-validation scheme. This process was repeated until each run was tested once and the decoding accuracy (i.e., the proportion of correctly classified trials) was averaged across the cross-validation folds. The statistical significance of decoding accuracy was evaluated by comparing it to the chance level obtained from a permutation test (see Permutation test). To assess if the decoding accuracy differed between No-Ping and Ping experiments, we performed two-way mixed ANOVAs (2 sessions × 4 regions) on the decoding accuracy.</p>
</sec>
<sec id="s4h2">
<title>Cross-task generalization from the perception task to attention task</title>
<p>Following previous work (<xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>), to test whether the neural patterns in the preparatory and stimulus selection periods from the attention task reflected sensory processing of isolated features, we trained an FLD classifier using the normalized BOLD responses from the perception task (4 to 6 s after the trial onset) to discriminate leftward vs. rightward orientation. Then, we tested this classifier on the normalized response from the independent runs of attention task to discriminate between attend leftward vs. attend rightward orientations, separately for preparation and stimulus selection periods. The significance of decoding accuracy was compared to the chance level obtained from a permutation test (see Permutation test). To assess if the generalization performance differed between No-Ping and Ping sessions, we performed two-way mixed ANOVAs (2 sessions × 4 regions) on the decoding accuracy.</p>
</sec>
<sec id="s4h3">
<title>Neural distance between attended and perceived orientations</title>
<p>The decoding accuracy from the cross-task generalization test reflects a discretized readout of the pattern similarity between different conditions. However, employing continuous similarity measures, such as Mahalanobis distance (<xref ref-type="bibr" rid="c32">Mahalanobis, 1936</xref>), could be more reliable compared to decoding accuracy (<xref ref-type="bibr" rid="c54">Walther et al., 2016</xref>). Therefore, we calculated the Mahalanobis distance to quantify the pattern similarity between two attended orientations and two perceived orientations. For each participant and each ROI, we have M points (i.e., M trials for each attended orientation) in the N-dimensional space (N = 100, number of voxels). For each data point in the attended orientation condition, we computed its distance to each of the orientation distributions (from the perception task). Averaged distance values were then calculated for each combination of attended orientation and perceived orientation pairs. A sensory-like hypothesis would predict smaller distance between the distribution of the attended orientation (e.g., attend leftward) and the distribution of corresponding orientation (e.g., perceive leftward) compared to the alternative orientation (e.g., perceive rightward). Two-way repeated-measures ANOVA (2 attended orientations × 2 perceived orientations) was applied on the Mahalanobis distance, separately for each region and each session.</p>
</sec>
<sec id="s4h4">
<title>Neural-behavioral relationships</title>
<p>We tested if the representation format during preparatory attention was associated with subsequent behavior. For each trial, we calculated the Mahalanobis distance between the attention conditions (attend leftward and attend rightward) and the perceived orientations (leftward and rightward orientation). We estimated the attentional modulation index (AMI) based on these distance values. This index measures how much attention modulated the pattern similarity for the Same orientation condition (e.g., attend and perceive the same orientation) relative to the Different orientation condition (e.g., attend and perceive different orientations). The index was calculated as follows: AMI = (D<sub>different</sub> – D<sub>same</sub>)/(D<sub>different</sub> + D<sub>same</sub>), where D<sub>same</sub> and D<sub>different</sub> are the measured distance (D) in the Same and Different orientation condition, respectively. Next, we sorted the single-trial AMI values in descending order and selected top-ranked 25% trials and bottom-ranked 25% trials to represent “strong modulation” and “weak modulation” trials, respectively. We then extracted behavioral responses on these selected trials and calculated the reaction time and accuracy for each trial type. Paired t-tests were used to compare between “strong modulation” and “weak modulation” trials in each session.</p>
</sec>
<sec id="s4h5">
<title>Informational connectivity analysis</title>
<p>We used Informational Connectivity (IC) to examine shared changes in pattern discriminability over time, a method that allows inference based on multivoxel pattern information rather than overall BOLD response (<xref ref-type="bibr" rid="c22">Jia et al., 2020</xref>; <xref ref-type="bibr" rid="c36">Ng et al., 2021</xref>). To track the flow of multivariate information across time (i.e., across trials), we measured the fluctuations (covariance) in pattern-based discriminability by calculating the Mahalanobis distance of each trial to the two attended orientations, using a leave-one-run-out cross-validation scheme. For each ROI, we calculated the Mahalanobis distance between the pattern of activity for each attention trial from one left-out run and the distribution of each attended orientation of the remaining runs. To quantify the degree of attentional modulation, we calculated the AMI using the same formula as mentioned above, where D<sub>same</sub> and D<sub>different</sub> are the measured distance (D) in the Same and Different condition. This index measures how much the pattern similarity increased for the same attention condition (e.g., attend leftward to attend leftward) relative to the different attention condition (e.g., attend leftward to attend rightward). A positive AMI indicates relative proximity to the same attention condition, whereas a negative AMI indicates relative proximity to the different attention condition. A time course of AMI values was generated across runs and pairwise correlated between ROIs using Pearson correlation analysis and Fisher z-transformed. Independent t-tests were used to compare the connectivity between No-Ping and Ping sessions.</p>
</sec>
</sec>
<sec id="s4i">
<title>Permutation test to evaluate classifier performance</title>
<p>Following previous work (<xref ref-type="bibr" rid="c23">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="c12">Gong et al., 2022</xref>), for each brain area, we evaluated the statistical significance of the observed decoding accuracy using a permutation test scheme. We first shuffled the trial labels in the training data and trained the same FLD classifier on the shuffled data. We then tested the classifier on the (unshuffled) test data to obtain decoding accuracy. For each ROI and each participant, we repeated this procedure 1000 times to compute a null distribution of decoding accuracy. To compute the group-level significance, we averaged the 20 null distributions to obtain a single null distribution of 1000 values for each ROI. To determine if the observed decoding accuracy significantly exceeds the chance level, we compared the observed value to the 95 percentiles of this group-level distribution (corresponding to <italic>p</italic> = 0.05). Note that these ROIs were pre-defined with strong priors as their activation in attention tasks has been consistently reported in the literature. Nevertheless, for those analyses where multiple comparisons were performed across regions, we applied a Bonferroni-correction to adjust the <italic>p</italic>-values.</p>
</sec>
<sec id="s4j">
<title>Bayesian analysis</title>
<p>To evaluate the strength of evidence for the null hypothesis, we conducted Bayesian analyses (<xref ref-type="bibr" rid="c53">Wagenmakers, 2007</xref>) using standard priors as implemented in JASP Version 0.17.1 (<xref ref-type="bibr" rid="c20">JASP Team, 2023</xref>). We performed Bayesian t-tests and computed Bayes factor (BF<sub>01</sub>) to compare between two attention conditions (attend leftward vs. attend rightward). Additionally, we used Bayesian repeated-measures ANOVA and computed the exclusion Bayes factors (BF<sub>excl</sub>) to assess the evidence for excluding specific effects across all models. A Bayes factor (BF) greater than 1 provides support for the null hypothesis. Specifically, a BF between 1 and 3 indicates weak evidence, a BF between 3 and 10 indicates moderate evidence, and a BF greater than 10 indicates strong evidence (<xref ref-type="bibr" rid="c52">Van Doorn et al., 2021</xref>).</p>
</sec>
<sec id="s4k">
<title>Approach to handle partially overlapped samples</title>
<p>Our study used partially overlapping samples, with 14 out of 20 participants completing both No-Ping and Ping sessions, while the remainder completed one of the two sessions. The most important analyses entailed assessing whether decoding accuracy was above chance, for which we used the permutation-based method (see above) within each session. Thus, these analyses were unaffected by the partially overlapping samples. In a few analyses where we compared across sessions, we used statistical tests treating “session” as a between-subject factor. We believe this is a reasonable approach, as a between-subject test is more conservative than a within-subject test, such that any significant effect emerged should be a genuine effect. To be certain, we also conducted additional analyses with “session” as a within-subject factor on the subset of data from the 14 participants who completed both sessions in a counterbalanced order. The results were highly similar to those reported in the main text.</p>
</sec>
</sec>
</body>
<back>
<sec id="s6" sec-type="data-availability">
<title>Data Availability</title>
<p>All data, analyses, and task codes have been made publicly available via the Open Science Framework at <ext-link ext-link-type="uri" xlink:href="https://osf.io/ghaxv/">https://osf.io/ghaxv/</ext-link></p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by National Science and Technology Innovation 2030—Major Project 2021ZD0200409, National Natural Science Foundation of China (32371087, 32300855, 3200784), Fundamental Research Funds for the Central University (226-2024-00118) and Non-profit Central Research Institute Fund of Chinese Academy of Medical Sciences 2023-PT310-01.</p>
</ack>
<sec id="s5">
<title>Additional information</title>
<sec id="s5a">
<title>Author contributions</title>
<p>Original idea by K.J. and M.G. Experiment programmed by Y.C. Data collected by Y.C. Analysis programmed by Y.C., K.J. and M.G. Manuscript drafted by Y.C., T.L., and M.G. Manuscript edited and revised by T.L., J. T., K.J., and M.G.</p>
</sec>
<sec id="s7">
<title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
</sec>
<sec id="d1e1351" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1329">
<label>Supplementary Information</label>
<media xlink:href="supplements/602176_file02.docx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baldauf</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Neural mechanisms of object-based attention</article-title>. <source>Science</source>, <volume>344</volume>(<issue>6182</issue>), <fpage>424</fpage>–<lpage>427</lpage>. <pub-id pub-id-type="doi">10.1126/science.1247003</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barbosa</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lozano-Soldevilla</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Compte</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Pinging the brain with visual impulses reveals electrically active, not activity-silent, working memories</article-title>. <source>PLOS Biology</source>, <volume>19</volume>(<issue>10</issue>), <fpage>e3001436</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3001436</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname>, <given-names>D. H</given-names></string-name></person-group>. (<year>1997</year>). <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), <fpage>433</fpage>–<lpage>436</lpage>. <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bressler</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Sylvester</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Shulman</surname>, <given-names>G. L.</given-names></string-name>, &amp; <string-name><surname>Corbetta</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Top-down control of human visual cortex by frontal and parietal cortex in anticipatory visual spatial attention</article-title>. <source>The Journal of Neuroscience</source>, <volume>28</volume>(<issue>40</issue>), <fpage>10056</fpage>–<lpage>10061</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1776-08.2008</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Shulman</surname>, <given-names>G. L</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>3</volume>(<issue>3</issue>), <fpage>201</fpage>–<lpage>215</lpage>. <pub-id pub-id-type="doi">10.1038/nrn755</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Duncan</surname>, <given-names>J</given-names></string-name></person-group>. (<year>1995</year>). <article-title>Neural mechanisms of selective visual attention</article-title>. <source>Annual Review of Neuroscience</source>, <volume>18</volume>(<issue>1</issue>), <fpage>193</fpage>–<lpage>222</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DeYoe</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Carman</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Bandettini</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Glickman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wieser</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cox</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Neitz</surname>, <given-names>J</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Mapping striate and extrastriate visual areas in human cerebral cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>93</volume>(<issue>6</issue>), <fpage>2382</fpage>–<lpage>2386</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.93.6.2382</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duncan</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>van Moorselaar</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Pinging the brain to reveal the hidden attentional priority map using encephalography</article-title>. <source>Nature Communications</source>, <volume>14</volume>(<issue>1</issue>), <fpage>4749</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-023-40405-8</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engel</surname>, <given-names>S</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Retinotopic organization in human visual cortex and the spatial precision of functional MRI</article-title>. <source>Cerebral Cortex</source>, <volume>7</volume>(<issue>2</issue>), <fpage>181</fpage>–<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/7.2.181</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Faul</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Erdfelder</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lang</surname>, <given-names>A. G.</given-names></string-name>, &amp; <string-name><surname>Buchner</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2007</year>). <article-title>G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title>. <source>Behavior Research Methods</source>, <volume>39</volume>(<issue>2</issue>), <fpage>175</fpage>–<lpage>191</lpage>. <pub-id pub-id-type="doi">10.3758/BF03193146</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gardner</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Merriam</surname>, <given-names>E. P.</given-names></string-name>, <string-name><surname>Schluppeck</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Besle</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Heeger</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>2018</year>). <article-title>mrTools: Analysis and visualization package for functional magnetic resonance imaging data (Version 4.7) [Computer software]</article-title>. <source>Zenodo</source>. <pub-id pub-id-type="doi">10.5281/ZENODO.1299483</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gong</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Preparatory attention to visual features primarily relies on non-sensory representation</article-title>. <source>Scientific Reports</source>, <volume>12</volume>(<issue>1</issue>), <fpage>21726</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-022-26104-2</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gong</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2020a</year>). <article-title>Biased neural representation of feature-based attention in the human frontoparietal network</article-title>. <source>Journal of Neuroscience</source>, <volume>40</volume>(<issue>43</issue>), <fpage>8386</fpage>–<lpage>8395</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gong</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2020b</year>). <article-title>Continuous and discrete representations of feature-based attentional priority in human frontoparietal network</article-title>. <source>Cognitive Neuroscience</source>, <volume>11</volume>(<issue>1–2</issue>), <fpage>47</fpage>–<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1080/17588928.2019.1601074</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grubert</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Eimer</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2018</year>). <article-title>The time course of target template activation processes during preparation for visual search</article-title>. <source>The Journal of Neuroscience</source>, <volume>38</volume>(<issue>44</issue>), <fpage>9527</fpage>–<lpage>9538</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0409-18.2018</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Preston</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Das</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Giesbrecht</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Eckstein</surname>, <given-names>M. P</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Feature-independent neural coding of target detection during search of natural scenes</article-title>. <source>Journal of Neuroscience</source>, <volume>32</volume>(<issue>28</issue>), <fpage>9499</fpage>– <lpage>9510</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5876-11.2012</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamblin-Frohman</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Becker</surname>, <given-names>S. I</given-names></string-name></person-group>. (<year>2021</year>). <article-title>The attentional template in high and low similarity search: Optimal tuning or tuning to relations?</article-title> <source>Cognition</source>, <volume>212</volume>, <fpage>104732</fpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2021.104732</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hout</surname>, <given-names>M. C.</given-names></string-name>, &amp; <string-name><surname>Goldinger</surname>, <given-names>S. D</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Target templates: The precision of mental representations affects attentional guidance and decision-making in visual search. <italic>Attention, Perception</italic></article-title>, <source>&amp; Psychophysics</source>, <volume>77</volume>(<issue>1</issue>), <fpage>128</fpage>–<lpage>149</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-014-0764-6</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hout</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Robbins</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Godwin</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>Fitzsimmons</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Scarince</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Categorical templates are more useful when features are consistent: Evidence from eye movements during search for societally important vehicles. <italic>Attention, Perception</italic></article-title>, <source>&amp; Psychophysics</source>, <volume>79</volume>(<issue>6</issue>), <fpage>1578</fpage>–<lpage>1592</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-017-1354-1</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>JASP Team</collab></person-group>. (<year>2023</year>). <source>Jasp</source> (Version <version>0.17.1</version>) [Computer software].</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kourtzi</surname>, <given-names>Z</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Ultra-high field imaging of human visual cognition</article-title>. <source>Annual Review of Vision Science</source>, <volume>9</volume>(<issue>1</issue>), <fpage>479</fpage>–<lpage>500</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-111022-123830</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zamboni</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kemper</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Rua</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Goncalves</surname>, <given-names>N. R.</given-names></string-name>, <string-name><surname>Ng</surname>, <given-names>A. K. T.</given-names></string-name>, <string-name><surname>Rodgers</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kourtzi</surname>, <given-names>Z</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Recurrent processing drives perceptual plasticity</article-title>. <source>Current Biology</source>, <volume>30</volume>(<issue>21</issue>), <fpage>4177</fpage>–<lpage>4187.e4.</lpage> <pub-id pub-id-type="doi">10.1016/j.cub.2020.08.016</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jigo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gong</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Neural determinants of task performance during feature-based attention in human cortex</article-title>. <source>eNeuro</source>, <volume>5</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1523/ENEURO.0375-17.2018</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pinsk</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>De Weerd</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Increased activity in human visual cortex during directed attention in the absence of visual stimulation</article-title>. <source>Neuron</source>, <volume>22</volume>(<issue>4</issue>), <fpage>751</fpage>–<lpage>761</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(00)80734-5</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kerzel</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The precision of attentional selection is far worse than the precision of the underlying memory representation</article-title>. <source>Cognition</source>, <volume>186</volume>, <fpage>20</fpage>–<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2019.02.001</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kleiner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brainard</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Pelli</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2007</year>). <article-title>What’s new in Psychtoolbox-3?</article-title> <source>Perception</source> <volume>36</volume>(<issue>14</issue>):<fpage>1</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Failing</surname>, <given-names>M. F.</given-names></string-name>, &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Prior expectations evoke stimulus templates in the primary visual cortex</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>26</volume>(<issue>7</issue>), <fpage>1546</fpage>–<lpage>1554</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00562</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Konen</surname>, <given-names>C. S.</given-names></string-name>, &amp; <string-name><surname>Kastner</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Representation of eye movements and stimulus motion in topographically organized areas of human posterior parietal cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>28</volume>(<issue>33</issue>), <fpage>8361</fpage>–<lpage>8375</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1930-08.2008</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kong</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Alais</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Van Der Burg</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Orientation categories used in guidance of attention in visual search can differ in strength. <italic>Attention, Perception</italic></article-title>, <source>&amp; Psychophysics</source>, <volume>79</volume>(<issue>8</issue>), <fpage>2246</fpage>–<lpage>2256</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-017-1387-5</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lewis-Peacock</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Drysdale</surname>, <given-names>A. T.</given-names></string-name>, &amp; <string-name><surname>Postle</surname>, <given-names>B. R</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Neural evidence for the flexible control of mental representations</article-title>. <source>Cerebral Cortex</source>, <volume>25</volume>(<issue>10</issue>), <fpage>3303</fpage>–<lpage>3313</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhu130</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Hou</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2013</year>). <article-title>A hierarchy of attentional priority signals in human frontoparietal cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>42</issue>), <fpage>16606</fpage>–<lpage>16616</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1780-13.2013</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahalanobis</surname>, <given-names>P. C</given-names></string-name></person-group>. (<year>1936</year>). <article-title>On generalised distance in statistics</article-title>. <source>National Institute of Science — 3India, Proceedings</source>, <volume>2</volume>, <fpage>49</fpage>–<lpage>55</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malcolm</surname>, <given-names>G. L.</given-names></string-name>, &amp; <string-name><surname>Henderson</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2009</year>). <article-title>The effects of target template specificity on visual search in real-world scenes: Evidence from eye movements</article-title>. <source>Journal of Vision</source>, <volume>9</volume>(<issue>11</issue>), <fpage>8</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1167/9.11.8</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mongillo</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Barak</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Tsodyks</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Synaptic theory of working memory</article-title>. <source>Science</source>, <volume>319</volume>(<issue>5869</issue>), <fpage>1543</fpage>–<lpage>1546</lpage>. <pub-id pub-id-type="doi">10.1126/science.1150769</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Myers</surname>, <given-names>N. E.</given-names></string-name>, <string-name><surname>Walther</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Wallis</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Stokes</surname>, <given-names>M. G.</given-names></string-name>, &amp; <string-name><surname>Nobre</surname>, <given-names>A. C</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Temporal dynamics of attention during encoding versus maintenance of working memory: complementary views from event-related potentials and alpha-band oscillations</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>27</volume>(<issue>3</issue>), <fpage>492</fpage>– <lpage>508</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00727</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ng</surname>, <given-names>A. K. T.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Goncalves</surname>, <given-names>N. R.</given-names></string-name>, <string-name><surname>Zamboni</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kemper</surname>, <given-names>V. G.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Welchman</surname>, <given-names>A. E.</given-names></string-name>, &amp; <string-name><surname>Kourtzi</surname>, <given-names>Z</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Ultra-High-Field neuroimaging reveals fine-scale processing for 3D perception</article-title>. <source>The Journal of Neuroscience</source>, <volume>41</volume>(<issue>40</issue>), <fpage>8362</fpage>–<lpage>8374</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0065-21.2021</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, &amp; <string-name><surname>Kastner</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2011</year>). <article-title>A neural basis for real-world visual search in human occipitotemporal cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>108</volume>(<issue>29</issue>), <fpage>12125</fpage>–<lpage>12130</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1101042108</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Prins</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Kingdom</surname>, <given-names>F. A. A.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Palamedes: Matlab routines for analyzing psychophysical data</article-title>. <source>Palamedes Toolbox</source>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rosenberg</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Scheinost</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Greene</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Avery</surname>, <given-names>E. W.</given-names></string-name>, <string-name><surname>Kwon</surname>, <given-names>Y. H.</given-names></string-name>, <string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Ramani</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Qiu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Constable</surname>, <given-names>R. T.</given-names></string-name>, &amp; <string-name><surname>Chun</surname>, <given-names>M. M</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Functional connectivity predicts changes in attention observed across minutes, days, and months</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>7</issue>), <fpage>3797</fpage>–<lpage>3807</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1912226117</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rungratsameetaweemana</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J. T</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Dissociating the impact of attention and expectation on early sensory processing</article-title>. <source>Current Opinion in Psychology</source>, <volume>29</volume>, <fpage>181</fpage>–<lpage>186</lpage>. <pub-id pub-id-type="doi">10.1016/j.copsyc.2019.03.014</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schluppeck</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Curtis</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Glimcher</surname>, <given-names>P. W.</given-names></string-name>, &amp; <string-name><surname>Heeger</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Sustained activity in topographic areas of human posterior parietal cortex during memory-guided saccades</article-title>. <source>The Journal of Neuroscience</source>, <volume>26</volume>(<issue>19</issue>), <fpage>5098</fpage>–<lpage>5108</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5330-05.2006</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneegans</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Bays</surname>, <given-names>P. M</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Restoration of fMRI decodability does not imply latent working memory states</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>29</volume>(<issue>12</issue>), <fpage>1977</fpage>–<lpage>1994</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01180</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scolari</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Byers</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J. T</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Optimal deployment of attentional gain during fine discriminations</article-title>. <source>The Journal of Neuroscience</source>, <volume>32</volume>(<issue>22</issue>), <fpage>7723</fpage>–<lpage>7733</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5558-11.2012</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sereno</surname>, <given-names>M. I.</given-names></string-name>, <string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Reppas</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Kwong</surname>, <given-names>K. K.</given-names></string-name>, <string-name><surname>Belliveau</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Rosen</surname>, <given-names>B. R.</given-names></string-name>, &amp; <string-name><surname>Tootell</surname>, <given-names>R. B. H</given-names></string-name></person-group>. (<year>1995</year>). <article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title>. <source>Science</source>, <volume>268</volume>(<issue>5212</issue>), <fpage>889</fpage>–<lpage>893</lpage>. <pub-id pub-id-type="doi">10.1126/science.7754376</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sereno</surname>, <given-names>M. I.</given-names></string-name>, <string-name><surname>Pitzalis</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Martinez</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2001</year>). <article-title>Mapping of contralateral space in retinotopic coordinates by a parietal cortical area in humans</article-title>. <source>Science</source>, <volume>294</volume>(<issue>5545</issue>), <fpage>1350</fpage>–<lpage>1354</lpage>. <pub-id pub-id-type="doi">10.1126/science.1063695</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname>, <given-names>M. G</given-names></string-name></person-group>. (<year>2015</year>). <article-title>‘Activity-silent’ working memory in prefrontal cortex: A dynamic coding framework</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>19</volume>(<issue>7</issue>), <fpage>394</fpage>–<lpage>405</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2015.05.004</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Muhle-Karbe</surname>, <given-names>P. S.</given-names></string-name>, &amp; <string-name><surname>Myers</surname>, <given-names>N. E</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Theoretical distinction between functional states in working memory and their corresponding neural states</article-title>. <source>Visual Cognition</source>, <volume>28</volume>(<issue>5–8</issue>), <fpage>420</fpage>–<lpage>432</lpage>. <pub-id pub-id-type="doi">10.1080/13506285.2020.1825141</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name>, &amp; <string-name><surname>Duncan</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Shape-specific preparatory activity mediates attention to targets in human visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>106</volume>(<issue>46</issue>), <fpage>19569</fpage>–<lpage>19574</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0905306106</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Expectation in perceptual decision making: Neural and computational mechanisms</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>15</volume>(<issue>11</issue>), <fpage>745</fpage>–<lpage>756</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3838</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Egner</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Expectation (and attention) in visual cognition</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>13</volume>(<issue>9</issue>), <fpage>403</fpage>–<lpage>409</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2009.06.003</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Egner</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Feature-based attention and feature-based expectation</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>20</volume>(<issue>6</issue>), <fpage>401</fpage>–<lpage>404</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2016.03.008</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Doorn</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Van Den Bergh</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Böhm</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Dablander</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Derks</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Draws</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Etz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Evans</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Gronau</surname>, <given-names>Q. F.</given-names></string-name>, <string-name><surname>Haaf</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Hinne</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kucharský</surname>, <given-names>Š.</given-names></string-name>, <string-name><surname>Ly</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Marsman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Matzke</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Gupta</surname>, <given-names>A. R. K. N.</given-names></string-name>, <string-name><surname>Sarafoglou</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Stefan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Voelkel</surname>, <given-names>J. G.</given-names></string-name>, &amp; <string-name><surname>Wagenmakers</surname>, <given-names>E.-J.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The JASP guidelines for conducting and reporting a Bayesian analysis</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>28</volume>(<issue>3</issue>), <fpage>813</fpage>–<lpage>826</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-020-01798-5</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wagenmakers</surname>, <given-names>E.-J</given-names></string-name></person-group>. (<year>2007</year>). <article-title>A practical solution to the pervasive problems ofp values</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>14</volume>(<issue>5</issue>), <fpage>779</fpage>–<lpage>804</lpage>. <pub-id pub-id-type="doi">10.3758/BF03194105</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walther</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nili</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ejaz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Diedrichsen</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title>. <source>NeuroImage</source>, <volume>137</volume>, <fpage>188</fpage>–<lpage>200</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Duncan</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Mitchell</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The time-course of component processes of selective attention</article-title>. <source>NeuroImage</source>, <volume>199</volume>, <fpage>396</fpage>–<lpage>407</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.067</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfe</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Guided Search 6.0: An updated model of visual search</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>28</volume>(<issue>4</issue>), <fpage>1060</fpage>–<lpage>1092</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-020-01859-9</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfe</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Friedman-Hill</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Stewart</surname>, <given-names>M. I.</given-names></string-name>, &amp; <string-name><surname>O’Connell</surname>, <given-names>K. M</given-names></string-name></person-group>. (<year>1992</year>). <article-title>The role of categorization in visual search for orientation</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>18</volume>(<issue>1</issue>), <fpage>34</fpage>–<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1037/0096-1523.18.1.34</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Myers</surname>, <given-names>N. E.</given-names></string-name>, &amp; <string-name><surname>Stokes</surname>, <given-names>M. G</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Revealing hidden states in visual working memory using electroencephalography</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>9</volume>. <pub-id pub-id-type="doi">10.3389/fnsys.2015.00123</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Jochim</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Akyürek</surname>, <given-names>E. G.</given-names></string-name>, &amp; <string-name><surname>Stokes</surname>, <given-names>M. G</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Dynamic hidden states underlying working-memory-guided behavior</article-title>. <source>Nature Neuroscience</source>, <volume>20</volume>(<issue>6</issue>), <fpage>864</fpage>–<lpage>871</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4546</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Hanks</surname>, <given-names>T. D.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Attentional guidance and match decisions rely on different template information during visual search</article-title>. <source>Psychological Science</source>, <volume>33</volume>(<issue>1</issue>), <fpage>105</fpage>–<lpage>120</lpage>. <pub-id pub-id-type="doi">10.1177/09567976211032225</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Becker</surname>, <given-names>S. I.</given-names></string-name>, <string-name><surname>Boettcher</surname>, <given-names>S. E. P.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Good-enough attentional guidance</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>27</volume>(<issue>4</issue>), <fpage>391</fpage>–<lpage>403</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2023.01.007</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Luo</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Feature-specific reactivations of past information shift current neural encoding thereby mediating serial bias behaviors</article-title>. <source>PLOS Biology</source>, <volume>21</volume>(<issue>3</issue>), <fpage>e3002056</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3002056</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103425.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Xilin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>By combining the 'pinging' technique with fMRI-based multivariate decoding, this <bold>important</bold> study examined the nature of the representation of the attentional template during preparation. While the findings are very interesting and the experimental evidence is <bold>solid</bold>, the methodological (e.g., the manipulation of attention, the potential cross-contamination between attention and working memory, and the representational distance analysis) and interpretation confounds (e.g., more thorough clarification of &quot;pinging&quot; and dual-format attentional templates) need to be addressed. The work will be of interest to researchers in psychology, cognitive science, and neuroscience.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103425.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The aim of the experiment reported in this paper is to examine the nature of the representation of a template of an upcoming target. To this end, participants were presented with compound gratings (consisting of tilted to the right and tilted to the left lines) and were cued to a particular orientation - red left tilt or blue right tilt (counterbalanced across participants). There are two directly compared conditions: (i) no ping: where there was a cue, that was followed by a 5.5-7.5s delay, then followed by a target grating in which the cued orientation deviated from the standard 45 degrees; and (ii) ping condition in which all aspects were the same with the only difference that a ping (visual impulse presented for 100ms) was presented after the 2.5 seconds following the cue. There was also a perception task in which only the 45 degrees to the right or to the left lines were presented. It was observed that during the delay, only in the ping condition, were the authors able to decode the orientation of the to-be-reported target using the cross-task generalization. Attention decoding, on the other hand, was decoded in both ping and non-ping conditions. It is concluded that the visual system has two different functional states associated with a template during preparation: a predominantly non-sensory representation for guidance and a latent sensory-like for prospective stimulus processing.</p>
<p>Strengths:</p>
<p>There is so much to be impressed with in this report. The writing of the manuscript is incredibly clear. The experimental design is clever and innovative. The analysis is sophisticated and also innovative - the cross-task decoding, the use of Mahalanobis distance as a function of representational similarity, the fact that the question is theoretically interesting, and the excellent figures.</p>
<p>Weaknesses:</p>
<p>While I think that this is an interesting study that addresses an important theoretical question, I have several concerns about the experimental paradigm and the subsequent conclusions that can be drawn.</p>
<p>(1) Why was V1 separated from the rest of the visual cortex, and why the rest of the areas were simply lumped into an EVC ROI? It would be helpful to understand the separation into ROIs.</p>
<p>(2) It would have been helpful to have a behavioral measure of the &quot;attended&quot; orientation to show that participants in fact attended to a particular orientation and were faster in the cued condition. The cue here was 100% valid, so no such behavioral measure of attention is available here.</p>
<p>(3) As I was reading the manuscript I kept thinking that the word attention in this manuscript can be easily replaced with visual working memory. Have the authors considered what it is about their task or cognitive demand that makes this investigation about attention or working memory?</p>
<p>(4) If I understand correctly, the only ROI that showed a significant difference for the cross-task generalization is V1. Was it predicted that only V1 would have two functional states? It should also be made clear that the only difference where the two states differ is V1.</p>
<p>(5) My primary concern about the interpretation of the finding is that the result, differences in cross-task decoding within V1 between the ping and no-ping condition might simply be explained by the fact that the ping condition refocuses attention during the long delay thus &quot;resharpening&quot; the template. In the no-ping condition during the 5.5 to 7.5 seconds long delay, attention for orientation might start getting less &quot;crisp.&quot; In the ping condition, however, the ping itself might simply serve to refocus attention. So, the result is not showing the difference between the latent and non-latent stages, rather it is the difference between a decaying template representation and a representation during the refocused attentional state. It is important to address this point. Would a simple tone during the delay do the same? If so, the interpretation of the results will be different.</p>
<p>(6) The neural pattern distances measured using Mahalanobis values are really great! Have the authors tried to use all of the data, rather than the high AMI and low AMI to possibly show a linear relationship between response times and AMI?</p>
<p>(7) After reading the whole manuscript I still don't understand what the authors think the ping is actually doing, mechanistically. I would have liked a more thorough discussion, rather than referencing previous papers (all by the co-author).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103425.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In the present study, the authors investigated the nature of attentional templates during the preparatory period of goal-directed attention. By combining the use of 'pinging' the neural activity with a visual impulse and fMRI-based multivariate decoding, the authors found that the nature of the neural representations of the prospective feature target during the preparatory period was contingent on the presence of the 'pinging' impulse. While the preparatory representations contained highly similar information content as the perceptual representations when the pinging impulse was introduced, they fundamentally differed from perceptual representations in the absence of the pinging impulse. Based on these findings, the authors proposed a dual-format mechanism in which both a &quot;non-sensory&quot; template and a latent &quot;sensory&quot; template coexisted during attentional preparation. The former actively guides activity in the preparatory state, and the latter is utilized for future stimulus processing.</p>
<p>Strengths:</p>
<p>Overall, I think this is an interesting study that introduced a novel perspective concerning the nature of neural representations during attentional processing. Methodologically, the present study combines an innovative utilization of the pinging technique in working memory studies and fMRI-based multivariate pattern analysis. The method is sound and the results are convincing. While I appreciate the conceptual elegance of the dual-format idea proposed by the authors, there are several questions that need to be addressed more thoroughly to clarify some of the potential ambiguities of the results and to increase the plausibility of the author's theory.</p>
<p>Weaknesses:</p>
<p>(1) The origin of the latent sensory-like representation. By 'pinging' the neural activity with a high-contrast, task-irrelevant visual stimulus during the preparation period, the authors identified the representation of the attentional feature target that contains the same information as perceptual representations. The authors interpreted this finding as a 'sensory-like' template is inherently hosted in a latent form in the visual system, which is revealed by the pinging impulse. However, I am not sure whether such a sensory-like template is essentially created, rather than revealed, by the pinging impulses. First, unlike the classical employment of the pinging technique in working memory studies, the (latent) representation of the memoranda during the maintenance period is undisputed because participants could not have performed well in the subsequent memory test otherwise. However, this appears not to be the case in the present study. As shown in Figure 1C, there was no significant difference in behavioral performance between the ping and the no-ping sessions (see also lines 110-125, pg. 5-6). In other words, it seems to me that the subsequent attentional task performance does not necessarily rely on the generation of such sensory-like representations in the preparatory period and that the emergence of such sensory-like representations does not facilitate subsequent attentional performance either. In such a case, one might wonder whether such sensory-like templates are really created, hosted, and eventually utilized during the attentional process. Second, because the reference orientations (i.e. 45 degrees and 135 degrees) have remained unchanged throughout the experiment, it is highly possible that participants implicitly memorized these two orientations as they completed more and more trials. In such a case, one might wonder whether the 'sensory-like' templates are essentially latent working memory representations activated by the pinging as was reported in Wolff et al. (2017), rather than a functional signature of the attentional process.</p>
<p>(2) The coexistence of the two types of attentional templates. The authors interpreted their findings as the outcome of a dual-format mechanism in which 'a non-sensory template' and a latent 'sensory-like' template coexist (e.g. lines 103-106, pg. 5). While I find this interpretation interesting and conceptually elegant, I am not sure whether it is appropriate to term it 'coexistence'. First, it is theoretically possible that there is only one representation in either session (i.e. a non-sensory template in the no-ping session and a sensory-like template in the ping session) in any of the brain regions considered. Second, it seems that there is no direct evidence concerning the temporal relationship between these two types of templates, provided that they commonly emerge in both sessions. Besides, due to the sluggish nature of fMRI data, it is difficult to tell whether the two types of templates temporally overlap.</p>
<p>(3) The representational distance. The authors used Mahalanobis distance to quantify the similarity of neural representation between different conditions. According to the authors' hypothesis, one would expect greater pattern similarity between 'attend leftward' and 'perceived leftward' in the ping session in comparison to the no-ping session. However, this appears not to be the case. As shown in Figures 3B and C, there was no major difference in Mahalanobis distance between the two sessions in either ROI and the authors did not report a significant main effect of the session in any of the ANOVAs. Besides, in all the ANOVAs, the authors reported only the statistic term corresponding to the interaction effect without showing the descriptive statistics related to the interaction effect. It is strongly advised that these descriptive statistics related to the interaction effect should be included to facilitate a more effective and intuitive understanding of their data.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103425.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper discusses how non-sensory and latent, sensory-like attentional templates are represented during attentional preparation. Using multivariate pattern analysis, they found that visual impulses can enhance the decoding generalization from perception to attention tasks in the preparatory stage in the visual cortex. Furthermore, the emergence of the sensory-like template coincided with enhanced information connectivity between V1 and frontoparietal areas and was associated with improved behavioral performance. It is an interesting paper with supporting evidence for the latent, sensory-like attentional template, but several problems still need to be solved.</p>
<p>(1) The title is &quot;Dual-format Attentional Template,&quot; yet the supporting evidence for the non-sensory format and its guiding function is quite weak. The author could consider conducting further generalization analysis from stimulus selection to preparation stages to explore whether additional information emerges.</p>
<p>(2) In Figure 2, the author did not find any decodable sensory-like coding in IPS and PFC, even during the impulse-driven session, indicating that these regions do not represent sensory-like information. However, in the final section, the author claimed that the impulse-driven sensory-like template strengthens informational connectivity between sensory and frontoparietal areas. This raises a question: how can we reconcile the lack of decodable coding in these frontoparietal regions with the reported enhancement in network communication? It would be helpful if the author provided a clearer explanation or additional evidence to bridge this gap.</p>
<p>(3) Given that the impulse-driven sensory-like template facilitated behavior, the author proposed that it might also enhance network communication. Indeed, they observed changes in informational connectivity. However, it remains unclear whether these changes in network communication have a direct and robust relationship with behavioral improvements.</p>
<p>(4) I'm uncertain about the definition of the sensory-like template in this paper. Is it referring to the Ping impulse-driven condition or the decodable performance in the early visual cortex? If it is the former, even in working memory, whether pinging identifies an activity-silent mechanism is currently debated. If it's the latter, the authors should consider whether a causal relationship - such as &quot;activating the sensory-like template strengthens the informational connectivity between sensory and frontoparietal areas&quot; - is reasonable.</p>
</body>
</sub-article>
</article>