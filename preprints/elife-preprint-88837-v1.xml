<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">88837</article-id>
<article-id pub-id-type="doi">10.7554/eLife.88837</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.88837.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Immunology and Inflammation</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Context-Aware Amino Acid Embedding Advances Analysis of TCR-Epitope Interactions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-5976-0673</contrib-id>
<name>
<surname>Zhang</surname>
<given-names>Pengfei</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bang</surname>
<given-names>Seojin</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref></contrib>
<contrib contrib-type="author">
<name>
<surname>Cai</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3528-6833</contrib-id>
<name>
<surname>Lee</surname>
<given-names>Heewook</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">†</xref>
</contrib>
<aff id="a1"><label>1</label><institution>School of Computing and Augmented Intelligence, Arizona State University</institution>, Tempe, AZ, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Biodesign Institute, Arizona State University</institution>, Tempe, AZ, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Zhao</surname>
<given-names>Siming</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Dartmouth College</institution>
</institution-wrap>
<city>Lebanon</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Choi</surname>
<given-names>Murim</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Seoul National University</institution>
</institution-wrap>
<city>Seoul</city>
<country>Republic of Korea</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label><sup>†</sup></label>Correspondence: <email>heewook.lee@asu.edu</email></corresp>
<fn id="n1"><label>*</label><p>Now at Google.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-07-24">
<day>24</day>
<month>07</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP88837</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-05-21">
<day>21</day>
<month>05</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-04-14">
<day>14</day>
<month>04</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.04.12.536635"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Zhang et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Zhang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-88837-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Accurate prediction of binding interaction between T cell receptors (TCRs) and host cells is fundamental to understanding the regulation of the adaptive immune system as well as to developing data-driven approaches for personalized immunotherapy. While several machine learning models have been developed for this prediction task, the question of how to specifically embed TCR sequences into numeric representations remains largely unexplored compared to protein sequences in general. Here, we investigate whether the embedding models designed for protein sequences, and the most widely used BLOSUM-based embedding techniques are suitable for TCR analysis. Additionally, we present our context-aware amino acid embedding models (<monospace>catELMo</monospace>) designed explicitly for TCR analysis and trained on 4M unlabeled TCR sequences with no supervision. We validate the effectiveness of <monospace>catELMo</monospace> in both supervised and unsupervised scenarios by stacking the simplest models on top of our learned embeddings. For the supervised task, we choose the binding affinity prediction problem of TCR and epitope sequences and demonstrate notably significant performance gains (up by at least 14% AUC) compared to existing embedding models as well as the state-of-the-art methods. Additionally, we also show that our learned embeddings reduce more than 93% annotation cost while achieving comparable results to the state-of-the-art methods. In TCR clustering task (unsupervised), <monospace>catELMo</monospace> identifies TCR clusters that are more homogeneous and complete about their binding epitopes. Altogether, our <monospace>catELMo</monospace> trained without any explicit supervision interprets TCR sequences better and negates the need for complex deep neural network architectures.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label><title>Introduction</title>
<p>T cell receptors (TCRs) play critical roles in adaptive immune systems as they enable T cells to distinguish abnormal cells from healthy cells. TCRs carry this important function by binding to antigens presented by major histocompatibility complex (MHC) and recognizing whether the antigens are self or foreign [<xref ref-type="bibr" rid="c1">1</xref>]. It is widely accepted that the third complementarity-determining region (CDR3) of the TCR<italic>β</italic> chain is the most important in determining its binding specificity to epitope—a part of an antigen [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>]. The advent of publicly available databases of TCR-epitope cognate pairs opened the door to computational methods to predict the binding affinity of a given pair of TCR and epitope sequences. Computational prediction of binding affinity is important as it can drastically reduce the cost and the time needed to narrow down a set of candidate TCR targets, thereby accelerating the development of personalized immunotherapy leading to vaccine development and cancer treatment [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>]. Computational prediction is challenging primarily due to 1) many-to-many binding characteristics [<xref ref-type="bibr" rid="c6">6</xref>] and 2) the limited amount of currently available data.</p>
<p>Despite the challenges, many deep neural networks have been leveraged to predict binding affinity between TCRs and epitopes [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>]. While each model has its own strengths and weaknesses, they all suffer from poor generalizability when applied to unseen epitopes, not present in the training data [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. In order to alleviate this, we focus mainly on embedding, as embedding an amino acid sequence into a numeric representation is the very first step needed to train and run a deep neural network. Furthermore, a ‘good’ embedding has been shown to boost downstream performance even with a few numbers of downstream samples [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>].</p>
<p>BLOSUM matrices [<xref ref-type="bibr" rid="c16">16</xref>] are widely used for representing amino acids into biological-related numeric vectors in TCR analysis [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c17">17</xref>]. However, BLOSUM matrices are static embedding methods as they always map an amino acid to the same vector regardless of its context. For example, in static word embedding, the word “mouse” in phrases “a mouse in desperate search of cheese” and “to click, press and release the left mouse button” will be embedded as the same numeric representation even though it is used in different contexts. Similarly, the amino acid residue <monospace>G</monospace> appearing five times in a TCR<italic>β</italic> CDR3 sequence <monospace>CASGGTGGANTGQLYF</monospace> may play different roles in binding to antigens as each occurrence has a different position and neighboring residues. The loss of such contextual information from static embedding may inevitably compromise model performances [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>].</p>
<p>Recent successes of large language models [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>] have been prompting new research applying text embedding techniques to amino acid embedding. Large language models are generally trained on a large text corpus in a self-supervised manner where no labels are required [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. A large number of (unlabeled) protein sequences has been available via high quality and manually curated databases such as UniProt [<xref ref-type="bibr" rid="c22">22</xref>]. With the latest development of targeted sequencing assays of TCR repertoire, a large number (unlabeled) of TCR sequences has also been accessible to the public via online databases such as ImmunoSEQ [<xref ref-type="bibr" rid="c23">23</xref>]. These databases have allowed researchers to develop large-scale amino acid embedding models that can be used for various downstream tasks. Asgari et al. [<xref ref-type="bibr" rid="c24">24</xref>] first utilized Word2vec [<xref ref-type="bibr" rid="c20">20</xref>] model with 3-mers of amino acids to learn embeddings of protein sequences. By considering a 3-mer amino acids as a word and a protein sequence as a sentence, they learn amino acid representations by predicting the context of a given target 3-mer in a large corpus of surrounding ones. Yang et al. [<xref ref-type="bibr" rid="c25">25</xref>] applied Doc2vec [<xref ref-type="bibr" rid="c21">21</xref>] models to protein sequences with different sizes of <italic>k</italic>-mers in a similar manner to Asgari et al. and showed better performance over sparse one-hot encoding. One-hot encoding produces static embeddings, like BLOSUM, which leads to the loss of positional and contextual information.</p>
<p>Later, SeqVec [<xref ref-type="bibr" rid="c26">26</xref>] and ProtTrans [<xref ref-type="bibr" rid="c27">27</xref>] experimented with dynamic protein sequence embeddings via multiple context-aware language models [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c28">28</xref>], showing advantages across multiple tasks. Note that the aforementioned amino acid embedding models were designed for protein sequence analysis. Although these models may have learned general representations of protein sequences, it does not necessarily signify their generalization performance on TCR-related downstream tasks.</p>
<p>Here, we explore strategies to develop amino acid embedding models and emphasize the importance of using ‘good’ amino acid embeddings for a significant performance gain in TCR-related downstream tasks. It includes neural network depth, architecture, types and numbers of training samples, and parameter initialization. Based on our experimental observation, we propose <monospace>catELMo</monospace>, whose architecture is adapted from ELMo (Embeddings from Language Models [<xref ref-type="bibr" rid="c18">18</xref>]), a bi-directional context-aware language model. <monospace>catELMo</monospace> is trained on more than four million TCR sequences collected from ImmunoSEQ [<xref ref-type="bibr" rid="c23">23</xref>] in an unsupervised manner, by contextualizing amino acid inputs and predicting the next amino acid token. We compare its performance with state-of-the-art amino acid embedding methods on two TCR-related downstream tasks. In TCR-epitope binding affinity prediction application, <monospace>catELMo</monospace> significantly outperforms the state-of-the-art method by at least 14% (absolute improvement) of AUCs. We also show <monospace>catELMo</monospace> achieves an equivalent performance to the state-of-the-art method while dramatically reducing downstream training sample annotation cost (more than 93% absolute reduction). In the epitope-specific TCR clustering application, <monospace>catELMo</monospace> also achieves comparable to or better cluster results than state-of-the-art methods.</p>
</sec>
<sec id="s2">
<label>2</label><title>Results</title>
<p><monospace>catELMo</monospace> is a bi-directional amino acid embedding model that learns contextualized amino acid representations (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>), treating an amino acid as a word and a sequence as a sentence. It learns patterns of amino acid sequences with its self-supervision signal, by predicting each the next amino acid token given its previous tokens. It has been trained on 4,173,895 TCR<italic>β</italic> CDR3 sequences (52 million of amino acid tokens) from ImmunoSEQ [<xref ref-type="bibr" rid="c23">23</xref>] (<xref rid="tbl1" ref-type="table">Table 1</xref>). <monospace>catELMo</monospace> yields a real-valued representation vector for a sequence of amino acids, which can be used as input features of various downstream tasks. We evaluated <monospace>catELMo</monospace> on two different TCR-related downstream tasks, and compared its performance with existing amino acid embedding methods, namely BLOSUM62 [<xref ref-type="bibr" rid="c16">16</xref>], Yang et al. [<xref ref-type="bibr" rid="c25">25</xref>], ProtBert [<xref ref-type="bibr" rid="c27">27</xref>], SeqVec [<xref ref-type="bibr" rid="c26">26</xref>], and TCRBert [<xref ref-type="bibr" rid="c29">29</xref>]. We also investigated various components of <monospace>catELMo</monospace> in order to account for its high performance, including the neural network architecture, layer depth and size, types of training data, and the size of downstream training data.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Methods overview.</title>
<p><bold>a</bold>) <monospace>catELMo</monospace> is an ELMo-based bi-directional amino acid sequence representation model trained on TCR sequences. It takes a sequence of amino acid strings as input and predicts the right (or left) next amino acid tokens. <monospace>catELMo</monospace> consists of a charCNN layer and four bidirectional LSTM layers followed by a softmax activation. For a given TCR sequence of length <italic>L</italic>, each layer returns <italic>L</italic> vectors of length 1,024. The size of an embedded TCR sequence, therefore, is [5<italic>, L,</italic> 1024]. Global average pooling with respect to the length of TCR, <italic>L</italic>, is applied to get a representation vector with a size of 1, 024. <bold>b</bold>) TCR-epitope binding affinity prediction task. An embedding method (e.g., <monospace>catELMo</monospace>) is applied to both TCR and epitope sequences. The embedding vectors are then fed into a neural network of three linear layers for training a binding affinity prediction model. The model predicts whether a given TCR and epitope sequence bind or not. <bold>c</bold>) Epitope-specific TCR sequences clustering. The hierarchical clustering algorithm is applied to TCR embedding vectors to group TCR sequences based on their epitope-specificity.</p></caption>
<graphic xlink:href="536635v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Data Summary.</title>
<p>The number of unique epitopes, TCRs, and TCR-epitope pairs used for <monospace>catELMo</monospace> and downstream tasks analysis.</p></caption>
<graphic xlink:href="536635v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>We briefly summarize the two downstream tasks here and refer further details to <xref rid="s4d" ref-type="sec">Section 4.4</xref>. The first downstream task is TCR-epitope binding affinity prediction (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). All embedding models compared were to embed input sequences into the identical prediction model. Each prediction model was trained on 300,016 TCR-epitope binding and non-binding pairs (1:1 ratio), embedded by each embedding model. We used a neural network with three linear layers for the prediction model, which takes a pair of TCR and epitope as input and returns a binding affinity (0–1) of the pair. The prediction performance was evaluated on testing sets each defined by two types of splitting methods [<xref ref-type="bibr" rid="c10">10</xref>], called TCR and epitope splits. The testing set of TCR split has no TCRs overlapped with training and validation sets, allowing us to measure out-of-sample TCR performance. Similarly, the testing set of epitope split has no epitopes overlapped with training and validation sets, allowing us to measure out-of-sample epitope performance. For a fair comparison, a consistent embedding method was applied to both TCR and epitope sequences within a single prediction model. The second task is epitope-specific TCR clustering that aims at grouping TCRs that bind to the same epitope (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>). We tested with TCR sequences of human and mouse species sampled from McPAS [<xref ref-type="bibr" rid="c30">30</xref>] database.</p>
<p>We applied the hierarchical clustering [<xref ref-type="bibr" rid="c31">31</xref>] and reported normalized mutual information (NMI) [<xref ref-type="bibr" rid="c32">32</xref>] to qualify the goodness of the clustering partition of TCR sequences.</p>
<sec id="s2a">
<label>2.1</label><title><monospace>catELMo</monospace> outperforms the existing embedding methods at discriminating binding and non-binding TCR-epitope pairs</title>
<p>We investigated the downstream performance of TCR-epitope binding affinity prediction models trained using <monospace>catELMo</monospace> embeddings. In order to compare performance across different embedding methods, we used the identical downstream model architecture for each method. The competing embedding methods compared are BLOSUM62 [<xref ref-type="bibr" rid="c16">16</xref>], Yang et al. [<xref ref-type="bibr" rid="c25">25</xref>], ProBert [<xref ref-type="bibr" rid="c27">27</xref>], SeqVec [<xref ref-type="bibr" rid="c26">26</xref>] and TCRBert [<xref ref-type="bibr" rid="c29">29</xref>]. We observed that the prediction model using <monospace>catELMo</monospace> embeddings significantly outperformed those using existing amino acid embedding methods in both TCR (<xref rid="fig2" ref-type="fig">Figure 2a, b</xref>) and epitope (<xref rid="fig2" ref-type="fig">Figure 2d, e</xref>) split. In TCR split, where no TCRs in the testing set exist in the training and validation set, <monospace>catELMo</monospace>’s prediction performance was significantly greater than the second best method (p-value <italic>&lt;</italic> 6.28 <italic>×</italic> 10<sup>−23</sup>, <xref rid="tbl2" ref-type="table">Table 2</xref>). It achieved AUC 96.04 which was 14 points higher than that of the second-highest performing method, while the rest of the methods performed worse than or similar to BLOSUM62. In epitope split, where no epitopes in the testing set exist in the training and validation set, the prediction model using <monospace>catELMo</monospace> also outperformed others with even larger performance gaps. <monospace>catELMo</monospace> significantly boosted 17 points of AUCs than the second-highest performing method (p-value <italic>&lt;</italic> 1.18 <italic>×</italic> 10<sup>−7</sup>, <xref rid="tbl3" ref-type="table">Table 3</xref>). Similar performance gains from <monospace>catELMo</monospace> were also observed in other metrics such as Precision, Recall, and F1 scores (<xref rid="figs1" ref-type="fig">Supplementary Fig. 1</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Comparison of the amino acid embedding methods for TCR-epitope binding affinity prediction task.</title>
<p>We obtained TCR and epitope embeddings and used them as input features of binding affinity prediction model. A binding affinity prediction model is trained on each embedding method’s embedding dataset. The prediction performance comparison on <bold>a), b), c)</bold> TCR split and <bold>d), e), f)</bold> epitope split. <bold>a), d)</bold> Receiver Operating Characteristic (ROC) curve and <bold>b), e)</bold> AUC of the prediction model trained on different embedding methods. Error bars represent standard deviations over 10 trials. <bold>c), f)</bold> AUCs of the prediction model trained on different portions of downstream datasets. Error bands represent 95% confidence intervals over 10 trials.</p></caption>
<graphic xlink:href="536635v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>TCR-epitope binding affinity prediction performance of TCR split.</title>
<p>Average and standard deviation of 10 trials are reported. P-values are from two-sample t-tests between <monospace>catELMo</monospace> and the second best method (<underline>underlined</underline>).</p></caption>
<graphic xlink:href="536635v1_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><title>TCR-epitope binding affinity prediction performance of epitope split.</title>
<p>Average and standard deviation of 10 trials are reported. P-values are from two-sample t-tests between <monospace>catELMo</monospace> and the second best method (<underline>underlined</underline>).</p></caption>
<graphic xlink:href="536635v1_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>We also visually observed that <monospace>catELMo</monospace> aided the model to better discriminate binding and non-binding TCRs for the five most frequent epitopes that appeared in our collected TCR-epitope pairs (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). These five epitopes account for a substantial portion of our dataset, comprising 14.73% (44,292 pairs) of the total TCR-epitope pairs collected. For visualization, we performed t-SNE [<xref ref-type="bibr" rid="c35">35</xref>] on the top fifty principal components of the last latent vectors of each prediction model. Each point represents a pair of TCR-epitope, colored by epitope (lighter shade for positive binding and darker shade for negative binding). Different degrees of overlapping between positive pairs and negative ones in regard to the same epitope can be seen in the t-SNE plots. For example, most of the binding and non-binding data points from SeqVec embeddings are barely separated within each epitope group. On the other hand, the t-SNE plot of <monospace>catELMo</monospace> exhibits noticeable contrast between binding and non-binding pairs, indicating that <monospace>catELMo</monospace> aids the prediction model to distinguish labels. We also observed <monospace>catELMo</monospace> outperformed the other embedding methods in discriminating binding and non-binding TCRs for almost all individual epitopes. As shown in <xref rid="figs2" ref-type="fig">Supplementary Fig. 2</xref>, the prediction model using <monospace>catELMo</monospace> embeddings achieved the highest AUCs in 39 out of 40 epitopes, and the second highest AUC on an epitope (<monospace>GTSGSPIVNR</monospace>) with only 1.09% lower than the highest score.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>tSNE visualization for top five frequent epitopes.</title>
<p>We visually compared the embedding models on TCR-epitope binding affinity prediction task. We conduct tSNE analysis on the top 50 principle components of the last hidden layer features of the TCR-epitope binding affinity prediction models for out-of-sample epitopes. The clearer the boundary between positive pairs (lighter shade) and negative pairs (darker shade) associated with the same epitope sequence, the better the model is at discriminating binding and non-binding pairs.</p></caption>
<graphic xlink:href="536635v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<label>2.2</label><title><monospace>catELMo</monospace> reduces a significant amount of annotation cost for achieving comparable prediction power</title>
<p>Language models trained on large corpus are known to improve downstream task performance with a smaller number of downstream training data [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c19">19</xref>]. Similarly in TCR-epitope binding, we show that <monospace>catELMo</monospace> trained entirely on unlabeled TCR sequences facilitates its downstream prediction model to achieve the same performance with a significantly smaller amount of TCR-epitope training pairs (i.e., epitope-labeled TCR sequence). We trained a binding affinity prediction model for each <italic>k</italic>% of downstream data (i.e., <monospace>catELMo</monospace> embeddings of TCR-epitope pairs) where <italic>k</italic> = 1, 2<italic>, · · ·,</italic> 10, 20, 30<italic>, · · ·,</italic> 100. The widely used BLOSUM62 embedding matrix was used as a comparison baseline under the same <italic>k</italic>s as it performs better than or is comparable to the other embedding methods.</p>
<p>A positive log-linear relationship between the number of (downstream) training data and AUCs was observed for both TCR and epitope split (<xref rid="fig2" ref-type="fig">Fig. 2c, f</xref>). The steeper slope in <monospace>catELMo</monospace> suggests that prediction models utilizing <monospace>catELMo</monospace> embeddings exhibit higher performance gain per number of training pairs compared to the BLOSUM62-based models. In TCR split, we observed that <monospace>catELMo</monospace>’s binding affinity prediction models with just 7% of the training data significantly outperform ones that use a full size of BLOSUM62 embeddings (p-value = 0.0032, <xref rid="fig2" ref-type="fig">Fig. 2c</xref>). <monospace>catELMo</monospace> with just 3%, 4%, and 6% of the downstream training data achieved similar performances to when using a full size of Yang et al., ProtBert, and SeqVec embeddings, respectively. Similarly, in epitope split, we showed <monospace>catELMo</monospace>’s prediction models with just 3% of training data achieved equivalent performance as ones built on a full size of BLOSUM62 embeddings (p-value = 0.8531, <xref rid="fig2" ref-type="fig">Fig. 2f</xref>). Compare to the other embedding methods, <monospace>catELMo</monospace> with just 1%, 2%, and 5% of the downstream training data achieved similar or better performance than when using a full size of Yang et al., ProtBert, and SeqVec embeddings, separately. Similar performance gains from <monospace>catELMo</monospace> were also observed in other metrics such as Precision, Recall, and F1 scores (<xref rid="figs3" ref-type="fig">Supplementary Fig. 3</xref>). Achieving accurate prediction with a small amount of training data is important for TCR analysis as obtaining the binding affinity of TCR-epitope pairs is costly.</p>
</sec>
<sec id="s2c">
<label>2.3</label><title><monospace>catELMo</monospace> allows clustering of TCR sequences with high performance</title>
<p>Clustering TCRs of similar binding profiles is important in TCR repertoire analysis as it facilitates discoveries of TCR clonotypes that are condition-specific [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>]. In order to demonstrate that <monospace>catELMo</monospace> embeddings can be used for other TCR-related downstream tasks, we performed hierarchical clustering [<xref ref-type="bibr" rid="c31">31</xref>] using each method’s embedding (<monospace>catELMo</monospace>, BLOSUM62 [<xref ref-type="bibr" rid="c16">16</xref>], Yang et al. [<xref ref-type="bibr" rid="c25">25</xref>], ProBert [<xref ref-type="bibr" rid="c27">27</xref>], SeqVec [<xref ref-type="bibr" rid="c26">26</xref>] and TCRBert [<xref ref-type="bibr" rid="c29">29</xref>]) and evaluated the identified clusters against the ground-truth TCR groups labeled by their binding epitopes. We additionally compared our results with state-of-the-art TCR clustering methods, TCRdist [<xref ref-type="bibr" rid="c37">37</xref>] and GIANA [<xref ref-type="bibr" rid="c38">38</xref>], both of which were developed from BLOSUM62 matrix (see <xref rid="s4d2" ref-type="sec">Section 4.4.2</xref>). Normalized mutual information (<italic>NMI</italic>) [<xref ref-type="bibr" rid="c32">32</xref>] is used to measure the clustering quality. <xref rid="fig4" ref-type="fig">Fig. 4a</xref> demonstrates NMI clustering comparison results of all TCR sequences bound to the top eight epitopes, covering both human and mouse species. We find that the cluster model built on <monospace>catELMo</monospace> embeddings maintains either the best or second best NMI scores compared with ones that are computed on other embeddings. To investigate whether this observation remains true on individual species, we conducted the same clustering analysis on human and mouse species, separately. We showcase NMI comparison for the top eight epitopes in human (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>) and mouse (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>) species and observe a similar pattern that clustering results with <monospace>catELMo</monospace> achieve the highest or second-highest NMI scores. Altogether, <monospace>catELMo</monospace> embedding can assist TCR clustering with no supervision while achieving similar or better performance than other state-of-the-art methods in both human and mouse species.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Comparison of the amino acid embedding methods for epitope-specific TCR clustering.</title>
<p>Hierarchical clustering is applied on the McPAS [<xref ref-type="bibr" rid="c30">30</xref>] database. We cluster TCRs binding to the top eight epitopes of <bold>a</bold>) both human and mouse species, <bold>b</bold>) only human epitopes, and <bold>c</bold>) only mouse epitopes. Larger NMI scores indicate TCR sequences that bind to the same epitope are grouped together in the same cluster and TCR sequences that do not bind to the same epitope are separated apart in different clusters.</p></caption>
<graphic xlink:href="536635v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<label>2.4</label><title>ELMo-based architecture is preferable to BERT-based architecture in TCR embedding models</title>
<p>We observed <monospace>catELMo</monospace> using ELMo-based architecture outperformed the model using embeddings of TCRBert which uses BERT (<xref rid="tbl4" ref-type="table">Table 4</xref>). The performance differences were approximately 15 AUCs in TCR split (p-value <italic>&lt;</italic> 3.86<italic>×</italic>10<sup>−30</sup>) and 19 AUCs in epitope split (p-value <italic>&lt;</italic> 3.29<italic>×</italic>10<sup>−8</sup>). Because TCRBert was trained on a smaller amount of TCR sequences (around 0.5 million sequences) than <monospace>catELMo</monospace>, we further compared <monospace>catELMo</monospace> with various sizes of BERT-like models trained on the same dataset as <monospace>catELMo</monospace>: BERT-Tiny-TCR, BERT-Base-TCR, and BERT-Large-TCR having a stack of 2, 12, and 30 Transformer layers respectively (see <xref rid="s4f2" ref-type="sec">Section 4.6.2</xref>). Note that BERT-Base-TCR uses the same number of Transformer layers as TCRBert. Additionally, we compared different versions of <monospace>catELMo</monospace> by varying the number of BiLSTM layers (2, 4–default, and 8, see <xref rid="s4f1" ref-type="sec">Section 4.6.1</xref>). As summarized in <xref rid="tbl4" ref-type="table">Table 4</xref>, TCR-epitope binding affinity prediction models trained on <monospace>catELMo</monospace> embeddings (AUC 96.04 and 94.70 on TCR and epitope split) consistently outperformed models trained on these Transformer-based embeddings (AUC 81.23–81.91 and 74.20–74.94 on TCR and epitope split). The performance gaps between <monospace>catELMo</monospace> and Transformer-based models (14 AUCs in TCR split and 19 AUCs in epitope split) were statistically significant (p-values <italic>&lt;</italic> 6.72 <italic>×</italic> 10<sup>−26</sup> and <italic>&lt;</italic> 1.55 <italic>×</italic> 10<sup>−7</sup> for TCR and epitope split respectively). We observed that TCR-epitope binding affinity prediction models trained on <monospace>catELMo</monospace>-based embeddings consistently outperformed the ones using Transformer-based embeddings (<xref rid="tbl4" ref-type="table">Table 4</xref>, <xref rid="tbl5" ref-type="table">5</xref>). Even the worst-performed BiLSTM-based embedding model achieved higher AUCs than the best-performed Transformer-based embeddings at discriminating binding and non-binding TCR-epitope pairs in both TCR (p-value <italic>&lt;</italic> 2.84 <italic>×</italic> 10<sup>−28</sup>) and epitope split (p-value <italic>&lt;</italic> 5.86 <italic>×</italic> 10<sup>−6</sup>).</p>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4.</label>
<caption><title>AUCs of TCR-epitope binding affinity prediction models built on BERT-based embedding models.</title>
<p>Average and standard deviation of 10 trials are reported.</p></caption>
<graphic xlink:href="536635v1_tbl4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5.</label>
<caption><title>AUCs of TCR-epitope binding affinity prediction models trained on different sizes of <monospace>catELMo</monospace> embeddings.</title>
<p>Average and standard deviation of 10 trials are reported.</p></caption>
<graphic xlink:href="536635v1_tbl5.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2e">
<label>2.5</label><title>Within-domain transfer learning is preferable to cross-domain transfer learning in TCR analysis</title>
<p>We showed that <monospace>catELMo</monospace>, trained on TCR sequences, significantly outperformed amino acid embedding methods trained on generic protein sequences. <monospace>catELMo</monospace>-Shallow and SeqVec shared the same architecture consisting of character-level convolutional layers and a stack of two bi-directional LSTM layers but were trained on different types of training data. <monospace>catELMo</monospace>-Shallow was trained on TCR sequences (about 4 million) while SeqVec was trained on generic protein sequences (about 33 million). Although <monospace>catELMo</monospace>-Shallow was trained on a relatively smaller amount of sequences compared to SeqVec, the binding affinity prediction model built on <monospace>catELMo</monospace>-Shallow embeddings (AUC 95.67 in TCR split and 86.32 in epitope split) significantly outperformed the one built on SeqVec embeddings (AUC 81.61 in TCR split and 76.71 in epitope split) by 14.06 and 9.61 on TCR and epitope split respectively. This suggests that knowledge transfer within the same domain is preferred whenever possible in TCR analysis.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label><title>Discussion</title>
<p><monospace>catELMo</monospace> is an effective embedding model that brings substantial performance improvement in TCR-related downstream tasks. Our study emphasizes the importance of choosing the right embedding models. The embedding of amino acids into numeric vectors is the very first and crucial step that enables the training of a deep neural network. It has been previously demonstrated that a well-designed embedding can lead to significantly improved results on downstream analysis [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>]. The reported performance of <monospace>catELMo</monospace> embedding on TCR-epitope binding affinity prediction and TCR clustering tasks indicates that <monospace>catELMo</monospace> is able to learn patterns of amino acid sequences more effectively than state-of-the-art embedding methods. While all other methods compared (except BLOSUM62) leverage a large number of unlabeled amino acid sequences, only our prediction model using <monospace>catELMo</monospace> significantly outperforms widely used BLOSUM62 and other models such as netTCR [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c17">17</xref>] and ATM-TCR [<xref ref-type="bibr" rid="c10">10</xref>] trained on paired (TCR-epitope) samples only (<xref rid="tbl6" ref-type="table">Table 6</xref>). Our work suggests the need for developing sophisticated strategies to train amino acid embedding models that can enhance the performance of TCR-related downstream tasks even while requiring less amount of data and simpler prediction model structures.</p>
<table-wrap id="tbl6" orientation="portrait" position="float">
<label>Table 6.</label>
<caption><title>AUCs of TCR-epitope binding affinity prediction models comparison with state-of-the-art prediction models.</title>
<p>All models are trained on the same dataset. Average and standard deviation of 10 trials are reported.</p></caption>
<graphic xlink:href="536635v1_tbl6.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Two important observations made from our experiments are 1) the type of data used for training amino acid embedding models is far more important than the amount of data and 2) ELMo-based embedding models consistently perform much better than BERT-based embedding models. While previously developed amino acid embedding models such as SeqVec and ProtBert were trained on 184- and 1,690-times more amino acid tokens compared to the training data used for <monospace>catELMo</monospace>, the prediction models using SeqVec and ProtBert performed poorly compared to the model using <monospace>catELMo</monospace> (see <xref rid="s2a" ref-type="sec">Sections 2.1</xref> and <xref rid="s2c" ref-type="sec">2.3</xref>). SeqVec and ProtBert were trained based on generic protein sequences, whereas <monospace>catELMo</monospace> was trained on a collection of TCR sequences from pooled TCR repertoires across many samples, indicating that the use of TCR data to train embedding models is more critical than much larger amount of generic protein sequences.</p>
<p>In the field of natural language processing, Transformer-based models have been bolstered as the superior embedding model [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c19">19</xref>]. However, for TCR-related downstream tasks, <monospace>catELMo</monospace> using biLSTM layer-based design outperforms BERT using Transformer layers (see <xref rid="s2d" ref-type="sec">Section 2.4</xref>). While it is difficult to pinpoint the reasons, the bi-directional architecture to predict the next token based on its previous tokens in ELMo may mimic the interaction process of TCR and epitope sequences either from left to right or from right to left. In contrast, BERT uses Transformer encoder layers that attend tokens both on the left and right to predict a masked token, refer to as masked language modeling. As the Transformer layer can be along with the next token prediction objectives, it remains as a future work to investigate Transformer with causal language models, such as GPT-3 [<xref ref-type="bibr" rid="c14">14</xref>], for amino acid embedding. Additionally, the clear differences of TCR sequences compared to natural languages are 1) the compact vocabulary size (20 standard amino acids vs. over 170k English words) and 2) the length of peptides in TCRs being smaller than the number of words in sentences or paragraphs in natural languages. These differences may allow <monospace>catELMo</monospace> to learn sequential dependence without losing long-term memory from the left end.</p>
<p>Often in classification problems in life sciences, the difference in the number of available positive and negative data can be very large and TCR-epitope binding affinity prediction problem is no exception. In fact, the number of experimentally generated non-binding pairs are practically non-existent and obtaining experimental negative data is costly [<xref ref-type="bibr" rid="c17">17</xref>]. This requires researchers to come up with a strategy to generate negative samples and it can be non-trivial. A common practice is to sample new TCRs from repertoires and pair them with existing epitopes [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c39">39</xref>], a strategy we also used. Another approach is to randomly shuffle TCR-epitope pairs within positive binding dataset, resulting in TCRs and epitopes that are not known to bind paired together [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>]. The prediction model consistently outperformed the other embedding methods by large margins in both TCR and epitope splits as shown in <xref rid="figs4" ref-type="fig">Supplementary Fig 4</xref>. The model using <monospace>catELMo</monospace> achieves 24% and 36% higher AUCs over the second best embedding method for TCR (p-value <italic>&lt;</italic> 1.04 <italic>×</italic> 10<sup>−18</sup>, <xref rid="tbls1" ref-type="table">Supplementary Table 1</xref>) and epitope (p-value <italic>&lt;</italic> 6.26 <italic>×</italic> 10<sup>−14</sup>, <xref rid="tbls2" ref-type="table">Supplementary Table 2</xref>) split, respectively. Moreover, we observe that using <monospace>catELMo</monospace> embeddings, prediction models that are trained with only 2% downstream samples still statistically outperform ones that are built on a full size of BLOSUM62 embeddings in TCR split (p-value = 0.0005). Similarly, with only 1% training samples, <monospace>catELMo</monospace> reaches comparable results as BLOSUM62 with a full size of downstream samples in epitope split (p-value = 0.1438). In other words, <monospace>catELMo</monospace> dramatically reduces about 98% annotation cost (<xref rid="tbls3" ref-type="table">Supplementary Table 3</xref>). This confirms that the embeddings from <monospace>catELMo</monospace> maintain high performance regardless of the methodology used to generate negative samples.</p>
<p>Parameter fine-tuning in neural networks is a training scheme where initial weights of the network are set to the weights of a pre-trained network. Fine-tuning has been shown to bring performance gain to the model over using random initial weights [<xref ref-type="bibr" rid="c40">40</xref>]. We investigated the possibility of performance boost of our prediction model using fine-tuned <monospace>catELMo</monospace>. Since SeqVec shares the same architecture with <monospace>catELMo</monospace>-Shallow and is trained on generic protein sequences, we used the weights of SeqVec as initial weights when fine-turning <monospace>catELMo</monospace>-Shallow. We compared the performance of binding affinity prediction models using the fine-tuned <monospace>catELMo</monospace>-Shallow and vanilla <monospace>catELMo</monospace>-Shallow (trained from scratch with random initial weights from a standard normal distribution). We observed that the performance when using fine-tuned <monospace>catELMo</monospace>-Shallow embeddings was significantly improved by approximately 2 points AUCs in TCR split (p-value <italic>&lt;</italic> 4.17 <italic>×</italic> 10<sup>−9</sup>) and 9 points AUCs in epitope split (p-value <italic>&lt;</italic> 5.46 <italic>×</italic> 10<sup>−7</sup>).</p>
<p>For fair comparison purposes, when predicting the binding affinity of TCR-epitope pairs, we ensured that the same embedding model is applied to both TCR and epitope sequences in one single prediction model. However, we have observed improved performance when using <monospace>catELMo</monospace> to embed TCRs only and using the widely-used BLOSUM62 matrix to embed epitope sequences. This suggests that an embedding model specifically designed for epitope may further enhance the prediction performance (<xref rid="tbls4" ref-type="table">Supplementary Table 4</xref>). However, training a robust language model for amino acid sequences typically requires a large amount of data, as noted in previous studies [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>]. Less than 13k unique linear epitope records were found under the following filtering conditions [<xref ref-type="bibr" rid="c34">34</xref>]: T cell assay performed with restricted to human host MHC Class I. This amount of publicly available linear epitope sequences, however, is very limited compared to TCR repertoires, which presents a challenge in training a large embedding model for epitope sequences.</p>
</sec>
<sec id="s4">
<label>4</label><title>Methods</title>
<p>We first present data used for training the amino acid embedding models and the downstream tasks. We then review existing amino acid embedding methods and their usage on TCR-related tasks. We introduce our approach, <monospace>catELMo</monospace>, a bi-directional amino acid embedding method that computes contextual representation vectors of amino acids of a TCR (or epitope) sequence. We describe in detail how to apply <monospace>catELMo</monospace> to two different TCR-related downstream tasks. Lastly, we provide details on the experimental design, including the methods and parameters used in comparison and ablation studies.</p>
<sec id="s4a">
<label>4.1</label><title>Data</title>
<sec id="s4a1">
<title>TCRs for training <monospace>catELMo</monospace></title>
<p>We collected 5,893,249 TCR sequences from repertoires of seven projects in the ImmunoSEQ database: HIV [<xref ref-type="bibr" rid="c41">41</xref>], SARS-CoV2 [<xref ref-type="bibr" rid="c23">23</xref>], Epstein Barr Virus [<xref ref-type="bibr" rid="c42">42</xref>], Human Cytomegalovirus [<xref ref-type="bibr" rid="c43">43</xref>], Influenza A [<xref ref-type="bibr" rid="c44">44</xref>], Mycobacterium Tuberculosis [<xref ref-type="bibr" rid="c45">45</xref>], and Cancer Neoantigens [<xref ref-type="bibr" rid="c46">46</xref>]. CDR3 sequences of TCR<italic>β</italic> chains were used to train the amino acid embedding models as those are the major segment interacting with epitopes [<xref ref-type="bibr" rid="c2">2</xref>] and exist in large numbers. We excluded duplicated copies and sequences containing wildcards such as ‘*’ or ‘X’. Altogether, we obtained 4,173,895 TCR sequences (52,546,029 amino acid tokens) of which 85% were used for training and 15% were used for testing.</p>
</sec>
<sec id="s4a2">
<title>TCR-epitope pairs for binding affinity prediction</title>
<p>We collected TCR-epitope pairs known to bind each other from three publicly available databases: IEDB [<xref ref-type="bibr" rid="c34">34</xref>], VDJdb [<xref ref-type="bibr" rid="c33">33</xref>], and McPAS [<xref ref-type="bibr" rid="c30">30</xref>]. Unlike the (unlabeled) TCR dataset for <monospace>catELMo</monospace> training, each TCR is annotated with an epitope known to bind each other, which we referred to as a TCR-epitope pair. We only used pairs with human MHC class I epitopes and CDR3 sequences of the TCR<italic>β</italic> chain. We further filtered out sequences containing wildcards such as ‘*’ or ‘X’. For VDJdb, we excluded pairs with a confidence score of 0 as it means a critical aspect of sequencing or specificity validation is missing. We removed duplicated copies and merged datasets collected from the three databases. Altogether, we obtained 150,008 unique TCR-epitope pairs known to bind to each other having 140,675 unique TCRs and 982 unique epitopes. We then generated the same number of non-binding TCR-epitope pairs as negative samples by randomly pairing each epitope of the positive pairs with a TCR sampled from the healthy TCR repertoires of ImmunoSEQ. We note that it includes no identical TCR sequences with the TCRs used for training the embedding models. Altogether, we obtained 300,016 TCR-epitope pairs where 150,008 pairs are positive and 150,008 pairs are negative.</p>
</sec>
<sec id="s4a3">
<title>TCRs for antigen-specific TCR clustering</title>
<p>We collected 9,822 unique TCR sequences of humans and mice hosts from McPAS [<xref ref-type="bibr" rid="c30">30</xref>]. Each TCR is annotated with an epitope known to bind, which is used as a ground-truth label for TCR clustering. We excluded TCR sequences that bind to neoantigen pathogens or multiple epitopes and only used CDR3 sequences of TCR<italic>β</italic> chain. We composed three subsets for different experimental purposes. The first dataset contains both human and mice TCRs. We used TCRs associated with the top eight frequent epitopes, resulting in 5,607 unique TCRs. The second dataset consists of only human TCRs, and the third dataset consists of only mouse TCRs. In a similar manner, we selected TCRs that bind to the top eight frequent epitopes. As a result, we obtained 5,528 unique TCR sequences for the second dataset and 1,322 unique TCR sequences for the third dataset.</p>
</sec>
</sec>
<sec id="s4b">
<label>4.2</label><title>Amino acid embedding methods</title>
<p>In this section, we review amino acid embedding methods previously proposed. There are two categories of the existing approaches: static and context-aware embedding methods. Static embedding method represents an amino acid as a static representation vector remaining the same regardless of its context. Context-aware embedding method, however, represents an amino acid differently in accordance with its context. Context-aware embedding is also called dynamic embedding in contrast to static embedding. We explain the key ideas of various embedding methods, and introduce their usage in previous works.</p>
<sec id="s4b1">
<label>4.2.1</label><title>Static embeddings</title>
<sec id="s4b1a">
<title>BLOSUM</title>
<p>BLOSUM [<xref ref-type="bibr" rid="c16">16</xref>] is a scoring matrix where each element represents how likely an amino acid residue is to be substituted by another over evolutionary time. It has been commonly used to measure alignment scores between two protein sequences. There are various BLOSUM matrices such as BLOSUM45, BLOSUM62, and BLOSUM80 where a matrix with a higher number is used for the alignment of less divergent sequences. BLOSUM have also served as the de facto standard embedding method for various TCR analyses. For example, BLOSUM62 was used to embed TCR and epitope sequences for training deep neural network models predicting their binding affinity [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c17">17</xref>]. BLOSUM62 was also used to embed TCR sequences for antigen-specific TCR clustering and TCR repertoire clustering. GIANA [<xref ref-type="bibr" rid="c38">38</xref>] clustered TCRs based on the Euclidean distance between TCR embeddings. TCRdist [<xref ref-type="bibr" rid="c37">37</xref>] used BLOSUM62 matrix to compute the dissimilarity matrix between TCR sequences for clustering.</p>
</sec>
<sec id="s4b1b">
<title>Word2vec and Doc2vec</title>
<p>Word2vec [<xref ref-type="bibr" rid="c20">20</xref>] and Doc2vec [<xref ref-type="bibr" rid="c21">21</xref>] are a family of embedding models to learn a single linear mapping of words, which takes a one-hot word indicator vector as input and returns a real-valued word representation vector as output. There are two types of Word2vec architectures: continuous bag-of-words (CBOW) and skip-gram. CBOW predicts a word from its surrounding words in a sentence. It embeds each input word via a linear map, sums all input words’ representations, and applies a softmax layer to predict an output word. Once training is completed, the linear mapping is used to obtain a representation vector of a word. On the contrary, skip-gram predicts the surrounding words given a word while it also uses a linear mapping to obtain a representation vector. Doc2vec is a model further generalized from Word2vec, which introduces a paragraph vector representing paragraph identity as an additional input. Doc2vec also has two types of architectures: distributed memory (DM) and distributed bag-of-words (DBOW). DM predicts a word from its surrounding words and the paragraph vector, while DBOW uses the paragraph vector to predict randomly sampled context words. In a similar way, linear mapping is used to obtain a continuous representation vector of a word.</p>
<p>Several studies adapted Word2vec and Doc2vec to embed amino acid sequences [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c25">25</xref>]. ProtVec [<xref ref-type="bibr" rid="c24">24</xref>] is the first Word2vec representation model trained on a large number of amino acid sequences. Its embeddings were used for several downstream tasks such as protein family classification, disordered protein visualization, and classification. Kimothi et al. [<xref ref-type="bibr" rid="c47">47</xref>] adapted Doc2vec to embed amino acid sequences for protein sequence classification and retrieval. Yang et al. [<xref ref-type="bibr" rid="c25">25</xref>] trained Doc2vec models on 524,529 protein sequences of UniProt [<xref ref-type="bibr" rid="c22">22</xref>] database. They considered a <italic>k</italic>-mer amino acids as a word, and a protein sequence as a paragraph. They trained DM models to predict a word from <italic>w</italic> surrounding words and a paragraph with various sizes of <italic>k</italic> and <italic>w</italic>.</p>
</sec>
</sec>
<sec id="s4b2">
<label>4.2.2</label><title>Context-aware embeddings</title>
<sec id="s4b2a">
<title>ELMo</title>
<p>ELMo [<xref ref-type="bibr" rid="c18">18</xref>] is a deep context-aware word embedding model trained on a large corpus. It learns each token’s (e.g., a word) contextual representation in forward and backward directions using a stack of two bi-directional LSTM layers. Each word of a text string is first mapped into a numerical representation vector via the character-level convolutional layers. The forward (left-to-right) pass learns a token’s contextual representation depending on itself and the previous context in which it is used. The backward (left-to-right) pass learns a token’s representation depending on itself and its subsequent context.</p>
<p>ELMo is less commonly implemented for amino acid embedding than Transformer-based deep neural networks. One example is SeqVec [<xref ref-type="bibr" rid="c26">26</xref>]. It is an amino acid embedding model using ELMo’s architecture. It feeds each amino acid as a training token of size 1, and learns its contextual representation both forward and backward within a protein sequence. The data was collected from UniRef50 [<xref ref-type="bibr" rid="c48">48</xref>], which consists of 9 billion amino acid tokens and 33 million protein sequences. SeqVec was applied to several protein-related downstream tasks such as secondary structure and long intrinsic disorder prediction, and subcellular localization.</p>
</sec>
<sec id="s4b2b">
<title>BERT</title>
<p>BERT [<xref ref-type="bibr" rid="c19">19</xref>] is a large language model leveraging Transformer [<xref ref-type="bibr" rid="c49">49</xref>] layers to learn context-aware word embeddings jointly conditioned on both directions. BERT is learned for two objectives. One is the masked language model to learn contextual relationships between words in a sentence. It aims to predict the original value of masked words. The other is the next sentence prediction which aims to learn the dependency between consecutive sentences. It feeds a pair of sentences as input and predicts whether the first sentence in the pair is contextually followed by the second sentence.</p>
<p>BERT’s architecture has been used in several amino acid embedding methods [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c29">29</xref>]. They treated an amino acid residue as a word and a protein sequence as a sentence. ProtBert [<xref ref-type="bibr" rid="c27">27</xref>] was trained on 216 million protein sequences (88 billion amino acid tokens) of UniRef100 [<xref ref-type="bibr" rid="c48">48</xref>]. It was applied for several protein sequence applications such as secondary structure prediction and sub-cellular localization. ProteinBert [<xref ref-type="bibr" rid="c50">50</xref>] combined language modeling and gene ontology annotation prediction together during training. It was applied to protein secondary structure, remote homology, fluorescence and stability prediction. TCRBert [<xref ref-type="bibr" rid="c29">29</xref>] was trained on 47,040 TCR<italic>β</italic> and 4,607 TCR<italic>α</italic> sequences of PIRD [<xref ref-type="bibr" rid="c51">51</xref>] dataset and evaluated on TCR-antigen binding prediction and TCR engineering tasks.</p>
</sec>
</sec>
</sec>
<sec id="s4c">
<label>4.3</label><title>Our approach: <monospace>catELMo</monospace></title>
<p>We propose <monospace>catELMo</monospace>, a bi-directional amino acid embedding model designed for TCR analysis. <monospace>catELMo</monospace> adapts ELMo’s architecture to learn context-aware representations of amino acids. It is trained on TCR sequences, which is different from the existing amino acid embedding models such as SeqVec trained on generic protein sequences. As illustrated in <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, <monospace>catELMo</monospace> is composed of a character CNN (CharCNN) [<xref ref-type="bibr" rid="c52">52</xref>] layer converting each one-hot encoded amino acid token to a continuous representation vector, a stack of four bi-directional LSTM [<xref ref-type="bibr" rid="c53">53</xref>] layers learning contextual relationship between amino acid residues, and a softmax layer predicting the next (or previous) amino acid residue.</p>
<p>Given a sequence of <italic>N</italic> amino acid tokens, (<italic>t</italic><sub>1</sub><italic>, t</italic><sub>2</sub><italic>,…, t<sub>N</sub></italic>), CharCNN maps each one-hot encoded amino acid token <italic>t<sub>k</sub></italic> to a latent vector <italic>c<sub>k</sub></italic> through seven convolutional layers with kernel sizes ranging from 1 to 7, and the numbers of filters of 32, 32, 64, 128, 256, 512, and 1,024, each of which is followed by a max-pooling layer, resulting in a 1,024-dimensional vector. The output of the CharCNN, (<italic>c</italic><sub>1</sub><italic>, c</italic><sub>2</sub><italic>,…, c<sub>N</sub></italic>), is then fed into a stack of four bidirectional LSTM layers consisting of forward and backward passes. For the forward pass, the sequence of the CharCNN output is fed into the first forward LSTM layer followed by the second forward LSTM layer, and so on. Each LSTM cell in every forward layer has 4,096 hidden states and returns a 512-dimensional representation vector. Each output vector of the last LSTM layer is then fed into a softmax layer to predict the right next amino acid token. Residual connection is applied between the first and second layers and between the third and fourth layers to prevent gradient vanishing. Similarly, the sequence of the CharCNN output is fed into the backward pass, in which each cell returns a 512-dimensional representation vector. Unlike the forward layer, each output vector of the backward layer followed by a softmax layer aims to predict the left next amino acid token.</p>
<p>Through the forward and backward passes, <monospace>catELMo</monospace> models the joint probability of a sequence of amino acid tokens. The forward pass aims to predict the next right amino acid token given its left previous tokens, which is <italic>P</italic> (<italic>t<sub>k</sub>|t</italic><sub>1</sub><italic>, t</italic><sub>2</sub><italic>,…, t<sub>k</sub></italic><sub>−1</sub>; <italic>θ<sub>c</sub>, θ<sub>fw</sub>, θ<sub>s</sub></italic>) for each <italic>k</italic>-th cell where <italic>θ<sub>c</sub></italic> indicates parameters of CharCNN, <italic>θ<sub>fw</sub></italic> indicates parameters of the forward layers, and <italic>θ<sub>s</sub></italic>indicates parameters of the softmax layer. The joint probability of all amino acid tokens for the forward pass is defined as:</p>
<disp-formula>
<alternatives><graphic xlink:href="536635v1_ueqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<p>The backward pass aims to predict the next left amino acid token given its right previous tokens. Similarly, the joint probability of all amino acid tokens for the backward pass is defined as:</p>
<disp-formula>
<alternatives><graphic xlink:href="536635v1_ueqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<p>where <italic>θ<sub>bw</sub></italic> indicates parameters of the backward layers. During <monospace>catELMo</monospace> training, the combined log-likelihood of the forward and backward passes is jointly optimized, which is defined as:</p>
<disp-formula>
<alternatives><graphic xlink:href="536635v1_ueqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<p>Note that the forward and backward layers have their own weights (<italic>θ<sub>fw</sub></italic> and <italic>θ<sub>bw</sub></italic>). This helps to avoid information leakage that a token used to predict its right tokens in forward layers is undesirably used again to predict its own status in backward layers.</p>
<p>For each amino acid residue, <monospace>catELMo</monospace> computes five representation vectors of length 1,024: one from CharCNN and four from BiLSTM layers. Those vectors are averaged over and yield an amino acid representation vector of length 1,024. A sequence of amino acids is then represented by an element-wise average of all amino acids’ representation vectors, resulting in a representation vector of length 1,024. For example, <monospace>catELMo</monospace> computes a representation for each amino acid in a TCR sequence <italic>CASSPTSGGQETQY F</italic> as a vector of length 1,024. The sequence is then represented by averaging over 15 amino acid representation vectors, which is a vector with a length of 1,024. <monospace>catELMo</monospace> is trained up to 10 epochs with a batch size of 128 on two NVIDIA RTX 2080 GPUs. We follow the default experimental settings of ELMo unless otherwise specified.</p>
</sec>
<sec id="s4d">
<label>4.4</label><title>Downstream tasks</title>
<p>We evaluate the amino acid embedding models’ generalization performance on two downstream tasks: TCR-epitope binding affinity prediction and epitope-specific TCR clustering.</p>
<sec id="s4d1">
<label>4.4.1</label><title>TCR-epitope binding affinity prediction</title>
<p>Computational approaches that predict TCR-epitope binding affinity benefit rapid TCR screening for a target antigen and improve personalized immunotherapy. Recent computational studies [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c17">17</xref>] formulated it as a binary classification problem that predicts a binding affinity score (0–1) given a pair of TCR and epitope sequences.</p>
<p>We evaluate <monospace>catELMo</monospace> based on the prediction performance of a binding affinity prediction model trained on its embedding, and compare it with the state-of-the-art amino acid embeddings (further demonstrated in <xref rid="s4e" ref-type="sec">Section 4.5</xref>). We first obtain different types of TCR and epitope embeddings from <monospace>catELMo</monospace> and the comparison methods. To measure the generalized prediction performance of binding affinity prediction models, we split each method’s dataset into training (64%), validation (16%), and testing (20%) sets. We use two splitting strategies established in the previous work [<xref ref-type="bibr" rid="c10">10</xref>]: TCR split and epitope split. TCR split was designed to measure the models’ prediction performance on out-of-sample TCRs where no TCRs in the testing set exist in the training and validation set. Epitope split was designed to measure the models’ prediction performance on out-of-sample epitopes where no epitopes in the testing set exist in the training and validation set.</p>
<p>The downstream model architecture is the same across all embedding methods, having three linear layers where the last layer returns a binding affinity score (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). Taking <monospace>catELMo</monospace> as an example, we first obtain a <monospace>catELMo</monospace> representation of length 1,024 for each sequence. We then feed the TCR representation to a linear layer with 2,048 neurons, followed by a Sigmoid Linear Units (SiLU) activation function [<xref ref-type="bibr" rid="c54">54</xref>], batch normalization [<xref ref-type="bibr" rid="c55">55</xref>], and 0.3 rate dropout [<xref ref-type="bibr" rid="c56">56</xref>]. Similarly, we feed the epitope representation to another linear layer with 2,048 neurons, followed by the same layers. The outputs of TCR and epitope layers are then concatenated (4,096 neurons) and passed into a linear layer with 1,024 neurons, followed by a SiLU activation function, batch normalization, and 0.3 rate dropout. Finally, we append the last linear layer with a neuron followed by a sigmoid activation function to obtain the binding affinity score ranging from 0 to 1. The models are trained to minimize a binary cross-entropy loss via Adam optimizer [<xref ref-type="bibr" rid="c57">57</xref>]. We set the batch size as 32 and the learning rate as 0.001. We stop the training if either the validation loss does not decrease for 30 consecutive epochs or it iterates over 200 epochs. Finally, we compare and report AUC scores of binding affinity prediction models of the different embedding methods.</p>
</sec>
<sec id="s4d2">
<label>4.4.2</label><title>Epitope-specific TCR clustering</title>
<p>Clustering TCRs is the first and fundamental step in TCR repertoire analysis as it can potentially identify TCR clonotypes that are condition-specific [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>]. We perform hierarchical clustering [<xref ref-type="bibr" rid="c31">31</xref>] to <monospace>catELMo</monospace> and the state-of-the-art amino acid embeddings (further demonstrated in <xref rid="s4e" ref-type="sec">Section 4.5</xref>). We also obtain clusters from the existing TCR clustering approaches (TCRdist and GIANA). Both methods are developed on the BLOSUM62 matrix and apply nearest neighbor search to cluster TCR sequences. GIANA used the CDR3 of TCR<italic>β</italic> chain and V gene, while TCRdist predominantly experimented with CDR1, CDR2, and CDR3 from both TCR<italic>α</italic> and TCR<italic>β</italic> chains. We evaluate the identified clusters of each method against the ground-truth TCR groups labeled by their binding epitopes.</p>
<p>We first obtain different types of TCR embeddings from <monospace>catELMo</monospace> and the comparison methods. All embedding methods except BLOSUM62 yield the same size representation vectors regardless of TCR length. For BLOSUM62 embedding, we pad the sequences so that all sequences are mapped to the same size vectors (further demonstrated in <xref rid="s4e" ref-type="sec">Section 4.5</xref>). We then perform hierarchical clustering on TCR embeddings of each method. In detail, the clustering algorithm starts with each TCR as a cluster with size 1. It repeatedly merges the closest two clusters based on the Euclidean distance between TCR embeddings until it reaches the target number of clusters.</p>
<p>We compare the normalized mutual information (NMI) between the identified cluster and the ground-truth. NMI is a harmonic mean between homogeneity and completeness. Homogeneity measures how many TCRs in a cluster bind to the same epitope, while completeness measures how many TCRs binding to the same epitope are clustered together. A higher value indicates a better clustering result. It ranges from zero to one where zero indicates no mutual information found between the identified clusters and the ground-truth clusters and one indicates a perfect correlation.</p>
</sec>
</sec>
<sec id="s4e">
<label>4.5</label><title>Comparison studies</title>
<p>We demonstrate how we implement existing amino acid embedding methods to compare with <monospace>catELMo</monospace> for the two TCR-related downstream tasks.</p>
<sec id="s4e1">
<title>BLOSUM62</title>
<p>Among various typs of BLOSUM matrices, we use BLOSUM62 as it has been widely used in many TCR-related models [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c38">38</xref>]. We obtain embeddings by mapping each amino acid to a vector of length 24 via BLOSUM62 matrix. Since TCRs (or epitopes) have varied lengths of the sequences, we pad each sequence using IMGT [<xref ref-type="bibr" rid="c58">58</xref>] method. If a TCR sequence is shorter than the predefined length 20 (or 22 for epitopes) [<xref ref-type="bibr" rid="c10">10</xref>], we add zero-padding to the middle of the sequence. Otherwise, we remove amino acids from the middle of the sequence until it reaches the target length. For each TCR, we flatten 20 amino acid embedding vectors of length 24 into a vector of length 480. For each epitope, we flatten 22 amino acid embedding vectors of length 24 into a vector of length 528.</p>
</sec>
<sec id="s4e2">
<title>Yang et al</title>
<p>We select the 3-mer model with a window size of 5 to embed TCR and epitope sequences, which is the best combination obtained from a grid search. Each 3-mer is embedded as a numeric vector of length 64. The vectors are averaged to represent a whole sequence, resulting in a vector of length 64.</p>
</sec>
<sec id="s4e3">
<title>SeqVec and ProtBert</title>
<p>We embed each amino acid as a numeric vector of length 1,024. The vectors are element-wisely averaged to represent a whole sequence, resulting in a vector of length 1,024.</p>
</sec>
<sec id="s4e4">
<title>TCRBert</title>
<p>We embed each amino acid as a numeric vector of length 768. The vectors are element-wisely averaged to represent a whole sequence with a vector of length 768.</p>
</sec>
</sec>
<sec id="s4f">
<label>4.6</label><title>Ablation studies</title>
<p>We provide details of our experimental design and ablation studies.</p>
<sec id="s4f1">
<label>4.6.1</label><title>Depth of <monospace>catELMo</monospace></title>
<p>We investigate the effect of various depths of <monospace>catELMo</monospace> on TCR-epitope binding affinity prediction performance. We compare <monospace>catELMo</monospace> with different numbers of BiLSTM layers, specifically <monospace>catELMo</monospace>-Shallow, <monospace>catELMo</monospace>, and <monospace>catELMo</monospace>-Deep with 2, 4 and 8 layers respectively. Other hyperparameters and the training strategy remained the same as described in <xref rid="s4c" ref-type="sec">Section 4.3</xref>. For each amino acid residue, we average the output vectors of CharCNN and four (or two, eight) BiLSTM, resulting in a numerical vector of length 1,024. We then element-wisely average over all amino acids’ representations to represent a whole sequence, resulting in a numerical vector of length 1,024. Embeddings from various depths are used to train binding affinity prediction models, resulting in three sets of downstream models. All settings of the downstream models remain the same as described in <xref rid="s4d1" ref-type="sec">Section 4.4.1</xref>. The downstream models’ prediction performance is compared to investigate the optimal depth of <monospace>catELMo</monospace>.</p>
</sec>
<sec id="s4f2">
<label>4.6.2</label><title>Neural architecture of <monospace>catELMo</monospace></title>
<p>We compare <monospace>catELMo</monospace> with BERT-based amino acid embedding models using another context-aware architecture, Transformer, which has shown outstanding performance in natural language processing tasks. We train different sizes of BERT, a widely used Transformer-based model, for amino acid embedding, named BERT-Base-TCR, BERT-Tiny-TCR, and BERT-Large-TCR. Each model has 2, 12, and 30 Transformer layers and returns 768, 768, and 1024 sizes of embeddings for each amino acid token. Their objectives, however, only consist of the masked language prediction and do not include the next sentence prediction. For each TCR sequence, 15% of amino acid tokens are masked out and the model is trained to recover the masked tokens based on the remaining ones. The models are trained on the same training set as <monospace>catELMo</monospace> for 10 epochs. Other parameter settings are the same as TCRBert, which is included as one of the comparison models. All other settings remain the same as described in <xref rid="s4d1" ref-type="sec">Section 4.4.1</xref>. TCRBert and BERT-Base-TCR share the same architecture, whereas TCRBert is trained on fewer training samples (PIRD). The embedding of a whole TCR sequence is obtained by average pooling over all amino acid representations. Embeddings from each model are used to train binding affinity prediction models, resulting in three sets of downstream models. The prediction performance of the downstream prediction models is compared to evaluate the architecture of <monospace>catELMo</monospace>.</p>
</sec>
<sec id="s4f3">
<label>4.6.3</label><title>Size of downstream data</title>
<p>We investigate how much downstream data <monospace>catELMo</monospace> can save in training a binding affinity prediction model while achieving the same performance with a model trained on a full size of data. We train the same model on different portion of <monospace>catELMo</monospace> embedding dataset. In detail, we randomly select <italic>k</italic>% of binding and <italic>k</italic>% of non-binding TCR-epitope pairs from training (and validation) data (<italic>k</italic> = 1, 2<italic>, · · ·,</italic> 10, 20<italic>, · · ·,</italic> 100), obtain <monospace>catELMo</monospace> embeddings for those, and feed them to train TCR-epitope binding affinity prediction models. Note that the TCR-epitope binding affinity prediction models in this experiment differ only in the number of training and validation pairs, meaning that the same testing set is used for different <italic>k</italic>s. We run ten times of experiments for each <italic>k</italic> and report their average and standard deviation of AUC, recall, precision, and F1 scores. We compare their performance to those trained on a full size of the other embedding datasets. For a more detailed investigation, we also perform the same experiment on BLOSUM62 embeddings and compare it with ours.</p>
</sec>
</sec>
</sec>
<sec id="s5">
<title>Data availability</title>
<p>Datasets used in downstream tasks and embedding models are described in detail in <xref rid="s4a" ref-type="sec">Section 4.1</xref>. Those datasets have been combined and summarized by us and are accessible at <ext-link ext-link-type="uri" xlink:href="https://github.com/Lee-CBG/">https://github.com/Lee-CBG/</ext-link> catELMo/tree/main/datasets.</p>
</sec>
<sec id="s6">
<title>Code availability</title>
<p>The implementation of <monospace>catELMo</monospace> as well as all of our trained embedding models are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Lee-CBG/catELMo">https://github.com/Lee-CBG/catELMo</ext-link>.</p>
</sec>
<sec id="s7">
<title>Authors’ contributions</title>
<p>HL conceived the idea for the overall project. PZ, SB and HL designed the method and wrote the manuscript. PZ carried out the experiments. MC collected data and participated in project discussions.</p>
</sec>
<sec id="s8">
<title>Ethics declarations</title>
<p>The authors declare no competing interests.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><surname>Attaf</surname> <given-names>M</given-names></string-name>, <string-name><surname>Legut</surname> <given-names>M</given-names></string-name>, <string-name><surname>Cole</surname> <given-names>DK</given-names></string-name>, <string-name><surname>Sewell</surname> <given-names>AK</given-names></string-name>. <article-title>The T cell antigen receptor: the Swiss army knife of the immune system</article-title>. <source>Clinical &amp; Experimental Immunology</source>. <year>2015</year>;<volume>181</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>18</lpage>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><surname>Davis</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Bjorkman</surname> <given-names>PJ</given-names></string-name>. <article-title>T-cell antigen receptor genes and T-cell recognition</article-title>. <source>Nature</source>. <year>1988</year>;<volume>334</volume>(<issue>6181</issue>):<fpage>395</fpage>–<lpage>402</lpage>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><surname>Krogsgaard</surname> <given-names>M</given-names></string-name>, <string-name><surname>Davis</surname> <given-names>MM</given-names></string-name>. <article-title>How T cells ‘see’ antigen</article-title>. <source>Nature Immunology</source>. <year>2005</year>;<volume>6</volume>(<issue>3</issue>):<fpage>239</fpage>–<lpage>45</lpage>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><surname>Sbai</surname> <given-names>H</given-names></string-name>, <string-name><surname>Mehta</surname> <given-names>A</given-names></string-name>, <string-name><surname>DeGroot</surname> <given-names>A</given-names></string-name>. <article-title>Use of T cell epitopes for vaccine development</article-title>. <source>Current drug targets-Infectious disorders</source>. <year>2001</year>;<volume>1</volume>(<issue>3</issue>):<fpage>303</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><surname>Schumacher</surname> <given-names>TN</given-names></string-name>. <article-title>T-cell-receptor gene therapy</article-title>. <source>Nature Reviews Immunology</source>. <year>2002</year>;<volume>2</volume>(<issue>7</issue>):<fpage>512</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><surname>Sewell</surname> <given-names>AK</given-names></string-name>. <article-title>Why must T cells be cross-reactive?</article-title> <source>Nature Reviews Immunology</source>. <year>2012</year>;<volume>12</volume>(<issue>9</issue>):<fpage>669</fpage>–<lpage>77</lpage>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><surname>Springer</surname> <given-names>I</given-names></string-name>, <string-name><surname>Besser</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tickotsky-Moskovitz</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dvorkin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Louzoun</surname> <given-names>Y</given-names></string-name>. <article-title>Prediction of specific TCR-peptide binding from large dictionaries of TCR-peptide pairs</article-title>. <source>Frontiers in immunology</source>. <year>2020</year>:<fpage>1803</fpage>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><surname>Jokinen</surname> <given-names>E</given-names></string-name>, <string-name><surname>Huuhtanen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mustjoki</surname> <given-names>S</given-names></string-name>, <string-name><surname>Heinonen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lähdesmäki</surname> <given-names>H</given-names></string-name>. <article-title>Predicting recognition between T cell receptors and epitopes with TCRGP</article-title>. <source>PLoS computational biology</source>. <year>2021</year>;<volume>17</volume>(<issue>3</issue>):<fpage>e1008814</fpage>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><surname>Jurtz</surname> <given-names>VI</given-names></string-name>, <string-name><surname>Jessen</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Bentzen</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Jespersen</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Mahajan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vita</surname> <given-names>R</given-names></string-name>, <etal>et al.</etal> <article-title>NetTCR: sequence-based prediction of TCR binding to peptide-MHC complexes using convolutional neural networks</article-title>. <source>BioRxiv</source>. <year>2018</year>:<volume>433706</volume>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><surname>Cai</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>P</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>H</given-names></string-name>. <article-title>ATM-TCR: TCR-epitope binding affinity prediction using a multi-head self-attention model</article-title>. <source>Frontiers in immunology</source>. <year>2022</year>;<volume>13</volume>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><surname>Weber</surname> <given-names>A</given-names></string-name>, <string-name><surname>Born</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rodriguez Martínez</surname> <given-names>M</given-names></string-name>. <article-title>TITAN: T-cell receptor specificity prediction with bimodal attention networks</article-title>. <source>Bioinformatics</source>. <year>2021</year>;<volume>37</volume>:<fpage>i237</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><surname>Lu</surname> <given-names>T</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Zhu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Jiang</surname> <given-names>P</given-names></string-name>, <string-name><surname>Xiao</surname> <given-names>X</given-names></string-name>, <etal>et al.</etal> <article-title>Deep learning-based prediction of the T cell receptor–antigen binding specificity</article-title>. <source>Nature Machine Intelligence</source>. <year>2021</year>;<volume>3</volume>(<issue>10</issue>):<fpage>864</fpage>–<lpage>75</lpage>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><surname>Moris</surname> <given-names>P</given-names></string-name>, <string-name><surname>De Pauw</surname> <given-names>J</given-names></string-name>, <string-name><surname>Postovskaya</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gielis</surname> <given-names>S</given-names></string-name>, <string-name><surname>De Neuter</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bittremieux</surname> <given-names>W</given-names></string-name>, <etal>et al.</etal> <article-title>Current challenges for unseen-epitope TCR interaction prediction and a new perspective derived from image classification</article-title>. <source>Briefings in Bioinformatics</source>. <year>2021</year>;<volume>22</volume>(<issue>4</issue>):<fpage>bbaa318</fpage>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><surname>Brown</surname> <given-names>T</given-names></string-name>, <string-name><surname>Mann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ryder</surname> <given-names>N</given-names></string-name>, <string-name><surname>Subbiah</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kaplan</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Dhariwal</surname> <given-names>P</given-names></string-name>, <etal>et al.</etal> <article-title>Language models are few-shot learners</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2020</year>;<volume>33</volume>:<fpage>1877</fpage>–<lpage>901</lpage>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><surname>Raffel</surname> <given-names>C</given-names></string-name>, <string-name><surname>Shazeer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Roberts</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Narang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Matena</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal> <article-title>Exploring the limits of transfer learning with a unified text-to-text transformer</article-title>. <source>Journal of Machine Learning Research</source>. <year>2020</year>;<volume>21</volume>(<issue>140</issue>):<fpage>1</fpage>–<lpage>67</lpage>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><surname>Henikoff</surname> <given-names>S</given-names></string-name>, <string-name><surname>Henikoff</surname> <given-names>JG</given-names></string-name>. <article-title>Amino acid substitution matrices from protein blocks</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1992</year>;<volume>89</volume>(<issue>22</issue>):<fpage>10915</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><surname>Montemurro</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schuster</surname> <given-names>V</given-names></string-name>, <string-name><surname>Povlsen</surname> <given-names>HR</given-names></string-name>, <string-name><surname>Bentzen</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Jurtz</surname> <given-names>V</given-names></string-name>, <string-name><surname>Chronister</surname> <given-names>WD</given-names></string-name>, <etal>et al.</etal> <article-title>NetTCR-2.0 enables accurate prediction of TCR-peptide binding by using paired TCR<italic>α</italic> and <italic>β</italic> sequence data</article-title>. <source>Communications biology</source>. <year>2021</year>;<volume>4</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><surname>Peters</surname> <given-names>ME</given-names></string-name>, <string-name><surname>Neumann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Iyyer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gardner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>C</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>Deep Contextualized Word Representations</article-title>. <source>Association for Computational Linguistics</source>; <year>2018</year>. p. <fpage>2227</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><surname>Devlin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toutanova</surname> <given-names>K</given-names></string-name>. <article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>. <source>Association for Computational Linguistics</source>; <year>2019</year>. p. <fpage>4171</fpage>–<lpage>86</lpage>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><surname>Mikolov</surname> <given-names>T</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Corrado</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dean</surname> <given-names>J</given-names></string-name>. <article-title>Efficient Estimation of Word Representations in Vector Space. International Conference on Learning Representations</article-title>, <source>Workshop Track Proceedings</source>; <year>2013</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><surname>Le</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Mikolov</surname> <given-names>T</given-names></string-name>. <article-title>Distributed representations of sentences and documents</article-title>. <source>PMLR. International conference on machine learning</source>; <year>2014</year>. p. <fpage>1188</fpage>–<lpage>96</lpage>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><surname>Apweiler</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bairoch</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>CH</given-names></string-name>, <string-name><surname>Barker</surname> <given-names>WC</given-names></string-name>, <string-name><surname>Boeckmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ferro</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal> <article-title>UniProt: the universal protein knowledgebase</article-title>. <source>Nucleic acids research</source>. <year>2004</year>;<volume>32</volume>:<fpage>D115</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><surname>Nolan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vignali</surname> <given-names>M</given-names></string-name>, <string-name><surname>Klinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dines</surname> <given-names>JN</given-names></string-name>, <string-name><surname>Kaplan</surname> <given-names>IM</given-names></string-name>, <string-name><surname>Svejnoha</surname> <given-names>E</given-names></string-name>, <etal>et al.</etal> <article-title>A large-scale database of T-cell receptor beta (TCR<italic>β</italic>) sequences and binding associations from natural and synthetic exposure to SARS-CoV-2</article-title>. <source>Research square</source>. <year>2020</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><surname>Asgari</surname> <given-names>E</given-names></string-name>, <string-name><surname>Mofrad</surname> <given-names>MR</given-names></string-name>. <article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>. <source>PloS one</source>. <year>2015</year>;<volume>10</volume>(<issue>11</issue>):<fpage>e0141287</fpage>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Bedbrook</surname> <given-names>CN</given-names></string-name>, <string-name><surname>Arnold</surname> <given-names>FH</given-names></string-name>. <article-title>Learned protein embeddings for machine learning</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>15</issue>):<fpage>2642</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><surname>Heinzinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Elnaggar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Dallago</surname> <given-names>C</given-names></string-name>, <string-name><surname>Nechaev</surname> <given-names>D</given-names></string-name>, <string-name><surname>Matthes</surname> <given-names>F</given-names></string-name>, <etal>et al.</etal> <article-title>Modeling aspects of the language of life through transfer-learning protein sequences</article-title>. <source>BMC bioinformatics</source>. <year>2019</year>;<volume>20</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><surname>Elnaggar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Heinzinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dallago</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rehawi</surname> <given-names>G</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>L</given-names></string-name>, <etal>et al.</etal> <article-title>ProtTrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2021</year>:<fpage>1</fpage>–<lpage>1</lpage>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="confproc"><string-name><surname>Lan</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Goodman</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gimpel</surname> <given-names>K</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>P</given-names></string-name>, <string-name><surname>Soricut</surname> <given-names>R</given-names></string-name>. <article-title>ALBERT: A lite BERT for self-supervised learning of language representations</article-title>. <conf-name>8th International Conference on Learning Representations, ICLR 2020</conf-name>, <conf-loc>Addis Ababa, Ethiopia</conf-loc>, <conf-date>April 26-30, 2020</conf-date>; <year>2020</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="other"><string-name><surname>Wu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Yost</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Daniel</surname> <given-names>B</given-names></string-name>, <string-name><surname>Belk</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Xia</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Egawa</surname> <given-names>T</given-names></string-name>, <etal>et al.</etal> <article-title>TCR-BERT: learning the grammar of T-cell receptors for flexible antigen-xbinding analyses</article-title>. <source>bioRxiv</source>. <year>2021</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><surname>Tickotsky</surname> <given-names>N</given-names></string-name>, <string-name><surname>Sagiv</surname> <given-names>T</given-names></string-name>, <string-name><surname>Prilusky</surname> <given-names>J</given-names></string-name>, <string-name><surname>Shifrut</surname> <given-names>E</given-names></string-name>, <string-name><surname>Friedman</surname> <given-names>N</given-names></string-name>. <article-title>McPAS-TCR: a manually curated catalogue of pathology-associated T cell receptor sequences</article-title>. <source>Bioinformatics</source>. <year>2017</year>;<volume>33</volume>(<issue>18</issue>):<fpage>2924</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><surname>Ward</surname> <given-names>Jr JH</given-names></string-name>. <article-title>Hierarchical grouping to optimize an objective function</article-title>. <source>Journal of the American statistical association</source>. <year>1963</year>;<volume>58</volume>(<issue>301</issue>):<fpage>236</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><surname>Strehl</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ghosh</surname> <given-names>J</given-names></string-name>. <article-title>Cluster Ensembles—A knowledge reuse framework for combining multiple partitions</article-title>. <source>J Mach Learn Res</source>. <year>2002</year>;<volume>3</volume>(<issue>Dec</issue>):<fpage>583</fpage>–<lpage>617</lpage>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><surname>Shugay</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bagaev</surname> <given-names>DV</given-names></string-name>, <string-name><surname>Zvyagin</surname> <given-names>IV</given-names></string-name>, <string-name><surname>Vroomans</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Crawford</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Dolton</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal> <article-title>VDJdb: a curated database of T-cell receptor sequences with known antigen specificity</article-title>. <source>Nucleic acids research</source>. <year>2018</year>;<volume>46</volume>(<issue>D1</issue>):<fpage>D419</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><surname>Vita</surname> <given-names>R</given-names></string-name>, <string-name><surname>Mahajan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Overton</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Dhanda</surname> <given-names>SK</given-names></string-name>, <string-name><surname>Martini</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cantrell</surname> <given-names>JR</given-names></string-name>, <etal>et al.</etal> <article-title>The immune epitope database (IEDB): 2018 update</article-title>. <source>Nucleic acids research</source>. <year>2019</year>;<volume>47</volume>(<issue>D1</issue>):<fpage>D339</fpage>–<lpage>43</lpage>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><surname>Van der Maaten</surname> <given-names>L</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>G</given-names></string-name>. <article-title>Visualizing data using t-SNE</article-title>. <source>Journal of machine learning research</source>. <year>2008</year>;<volume>9</volume>(<issue>11</issue>).</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><string-name><surname>Purtic</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pitcher</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Van Oers</surname> <given-names>NS</given-names></string-name>, <string-name><surname>Wülfing</surname> <given-names>C</given-names></string-name>. <article-title>T cell receptor (TCR) clustering in the immunological synapse integrates TCR and costimulatory signaling in selected T cells</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2005</year>;<volume>102</volume>(<issue>8</issue>):<fpage>2904</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><surname>Dash</surname> <given-names>P</given-names></string-name>, <string-name><surname>Fiore-Gartland</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Hertz</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>GC</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>S</given-names></string-name>, <string-name><surname>Souquette</surname> <given-names>A</given-names></string-name>, <etal>et al.</etal> <article-title>Quantifiable predictive features define epitope-specific T cell receptor repertoires</article-title>. <source>Nature</source>. <year>2017</year>;<volume>547</volume>(<issue>7661</issue>):<fpage>89</fpage>–<lpage>93</lpage>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Zhan</surname> <given-names>X</given-names></string-name>, <string-name><surname>Li</surname> <given-names>B</given-names></string-name>. <article-title>GIANA allows computationally-efficient TCR clustering and multi-disease repertoire classification by isometric transformation</article-title>. <source>Nature communications</source>. <year>2021</year>;<volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><string-name><surname>Gielis</surname> <given-names>S</given-names></string-name>, <string-name><surname>Moris</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bittremieux</surname> <given-names>W</given-names></string-name>, <string-name><surname>De Neuter</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ogunjimi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Laukens</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>Detection of enriched T cell epitope specificity in full T cell receptor sequence repertoires</article-title>. <source>Frontiers in immunology</source>. <year>2019</year>;<volume>10</volume>:<fpage>2820</fpage>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><string-name><surname>Yosinski</surname> <given-names>J</given-names></string-name>, <string-name><surname>Clune</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bengio</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Lipson</surname> <given-names>H</given-names></string-name>. <article-title>How transferable are features in deep neural networks?</article-title> <source>Advances in Neural Information Processing Systems</source>. <year>2014</year>;<volume>27</volume>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><string-name><surname>Chan</surname> <given-names>HY</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Garliss</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Kwaa</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Blankson</surname> <given-names>JN</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>KN</given-names></string-name>. <article-title>T cell receptor sequencing-based assay identifies cross-reactive recall CD8+ T cell clonotypes against autologous HIV-1 epitope variants</article-title>. <source>Frontiers in immunology</source>. <year>2020</year>;<volume>11</volume>:<fpage>591</fpage>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><string-name><surname>Gil</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kamga</surname> <given-names>L</given-names></string-name>, <string-name><surname>Chirravuri-Venkata</surname> <given-names>R</given-names></string-name>, <string-name><surname>Aslan</surname> <given-names>N</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>F</given-names></string-name>, <string-name><surname>Ghersi</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal> <article-title>Epstein-barr virus epitope– major histocompatibility complex interaction combined with convergent recombination drives selection of diverse t cell receptor <italic>α</italic> and <italic>β</italic> repertoires</article-title>. <source>MBio</source>. <year>2020</year>;<volume>11</volume>(<issue>2</issue>):<fpage>e00250</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><string-name><surname>De Neuter</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bartholomeus</surname> <given-names>E</given-names></string-name>, <string-name><surname>Elias</surname> <given-names>G</given-names></string-name>, <string-name><surname>Keersmaekers</surname> <given-names>N</given-names></string-name>, <string-name><surname>Suls</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jansens</surname> <given-names>H</given-names></string-name>, <etal>et al.</etal> <article-title>Memory CD4+ T cell receptor repertoire data mining as a tool for identifying cytomegalovirus serostatus</article-title>. <source>Genes &amp; Immunity</source>. <year>2019</year>;<volume>20</volume>(<issue>3</issue>):<fpage>255</fpage>–<lpage>60</lpage>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><string-name><surname>Herati</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Muselman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vella</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bengsch</surname> <given-names>B</given-names></string-name>, <string-name><surname>Parkhouse</surname> <given-names>K</given-names></string-name>, <string-name><surname>Del Alcazar</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal> <article-title>Successive annual influenza vaccination induces a recurrent oligoclonotypic memory response in circulating T follicular helper cells</article-title>. <source>Science immunology</source>. <year>2017</year>;<volume>2</volume>(<issue>8</issue>):<fpage>eaag2152</fpage>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><string-name><surname>DeWitt</surname> <given-names>WS</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Wilburn</surname> <given-names>DB</given-names></string-name>, <string-name><surname>Sherwood</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vignali</surname> <given-names>M</given-names></string-name>, <string-name><surname>Day</surname> <given-names>CL</given-names></string-name>, <etal>et al.</etal> <article-title>A diverse lipid antigen-specific TCR repertoire is clonally expanded during active tuberculosis</article-title>. <source>The Journal of Immunology</source>. <year>2018</year>;<volume>201</volume>(<issue>3</issue>):<fpage>888</fpage>–<lpage>96</lpage>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><string-name><surname>Rajamanickam</surname> <given-names>V</given-names></string-name>, <string-name><surname>Ballesteros-Merino</surname> <given-names>C</given-names></string-name>, <string-name><surname>Samson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Ross</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bernard</surname> <given-names>B</given-names></string-name>, <string-name><surname>Fox</surname> <given-names>BA</given-names></string-name>, <etal>et al.</etal> <article-title>Treatment-induced immune cell priming as a potential explanation for an outstanding anti-tumor response in a patient with metastatic colorectal cancer</article-title>; <year>2021</year>. <source>Available from</source>: <ext-link ext-link-type="uri" xlink:href="https://clients.adaptivebiotech.com/">https://clients.adaptivebiotech.com/</ext-link> pub/cd09abd3-5c8b-465f-ae98-bf3f810b3229.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="other"><string-name><surname>Kimothi</surname> <given-names>D</given-names></string-name>, <string-name><surname>Soni</surname> <given-names>A</given-names></string-name>, <string-name><surname>Biyani</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hogan</surname> <given-names>JM</given-names></string-name>. <article-title>Distributed Representations for Biological Sequence Analysis</article-title>. <source>CoRR</source>. <year>2016</year>;<fpage>abs/1608.05949</fpage>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><string-name><surname>Suzek</surname> <given-names>BE</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>H</given-names></string-name>, <string-name><surname>McGarvey</surname> <given-names>P</given-names></string-name>, <string-name><surname>Mazumder</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>CH</given-names></string-name>. <article-title>UniRef: comprehensive and non-redundant UniProt reference clusters</article-title>. <source>Bioinformatics</source>. <year>2007</year>;<volume>23</volume>(<issue>10</issue>):<fpage>1282</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><string-name><surname>Vaswani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname> <given-names>N</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gomez</surname> <given-names>AN</given-names></string-name>, <etal>et al.</etal> <article-title>Attention is all you need</article-title>. <source>Advances in neural information processing systems</source>. <year>2017</year>;<volume>30</volume>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><string-name><surname>Brandes</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ofer</surname> <given-names>D</given-names></string-name>, <string-name><surname>Peleg</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Rappoport</surname> <given-names>N</given-names></string-name>, <string-name><surname>Linial</surname> <given-names>M</given-names></string-name>. <article-title>ProteinBERT: A universal deep-learning model of protein sequence and function</article-title>. <source>Bioinformatics</source>. <year>2022</year>;<volume>38</volume>(<issue>8</issue>):<fpage>2102</fpage>–<lpage>10</lpage>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>W</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wei</surname> <given-names>X</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>K</given-names></string-name>, <string-name><surname>Du</surname> <given-names>W</given-names></string-name>, <etal>et al.</etal> <article-title>PIRD: pan immune repertoire database</article-title>. <source>Bioinformatics</source>. <year>2020</year>;<volume>36</volume>(<issue>3</issue>):<fpage>897</fpage>–<lpage>903</lpage>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="confproc"><string-name><surname>Kim</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Jernite</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Sontag</surname> <given-names>D</given-names></string-name>, <string-name><surname>Rush</surname> <given-names>AM</given-names></string-name>. <article-title>Character-aware neural language models</article-title>. <publisher-loc>In</publisher-loc>: <conf-name>Thirtieth AAAI conference on artificial intelligence</conf-name>; <year>2016</year>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><string-name><surname>Hochreiter</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schmidhuber</surname> <given-names>J</given-names></string-name>. <article-title>Long short-term memory</article-title>. <source>Neural computation</source>. <year>1997</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1735</fpage>–<lpage>80</lpage>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><string-name><surname>Elfwing</surname> <given-names>S</given-names></string-name>, <string-name><surname>Uchibe</surname> <given-names>E</given-names></string-name>, <string-name><surname>Doya</surname> <given-names>K</given-names></string-name>. <article-title>Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</article-title>. <source>Neural Networks</source>. <year>2018</year>;<volume>107</volume>:<fpage>3</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="confproc"><string-name><surname>Ioffe</surname> <given-names>S</given-names></string-name>, <string-name><surname>Szegedy</surname> <given-names>C</given-names></string-name>. <article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title>. In: <conf-name>International conference on machine learning</conf-name>. <source>PMLR</source>; <year>2015</year>. p. <fpage>448</fpage>-<lpage>56</lpage>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><string-name><surname>Srivastava</surname> <given-names>N</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>G</given-names></string-name>, <string-name><surname>Krizhevsky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name>, <string-name><surname>Salakhutdinov</surname> <given-names>R</given-names></string-name>. <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>The journal of machine learning research</source>. <year>2014</year>;<volume>15</volume>(<issue>1</issue>):<fpage>1929</fpage>–<lpage>58</lpage>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><string-name><surname>Kingma</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Ba</surname> <given-names>J.</given-names></string-name> <article-title>Adam: A method for stochastic optimization</article-title>. <source>CoRR</source>. <year>2014</year>;<fpage>abs/1412.6980</fpage>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><string-name><surname>Lefranc</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Pommié</surname> <given-names>C</given-names></string-name>, <string-name><surname>Kaas</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Duprat</surname> <given-names>E</given-names></string-name>, <string-name><surname>Bosc</surname> <given-names>N</given-names></string-name>, <string-name><surname>Guiraudou</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal> <article-title>IMGT unique numbering for immunoglobulin and T cell receptor constant domains and Ig superfamily C-like domains</article-title>. <source>Developmental &amp; Comparative Immunology</source>. <year>2005</year>;<volume>29</volume>(<issue>3</issue>):<fpage>185</fpage>–<lpage>203</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s9">
<title>Supplementary Materials</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Prediction performance of TCR-epitope binding affinity prediction models trained on different embedding methods.</title>
<p>The average and standard deviation (error bar) of 10 trials are reported. <bold>a)</bold> AUC, precision, recall, and F1 of TCR split, and <bold>b)</bold> epitope split.</p></caption>
<graphic xlink:href="536635v1_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>AUCs of the top 40 frequent out of sample epitopes in the binding affinity prediction task.</title></caption>
<graphic xlink:href="536635v1_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>AUCs of TCR-epitope binding affinity prediction models trained on different portions of downstream datasets.</title>
<p>The average and interval of 95% confidence (error band) of 10 trials are reported. <bold>a)</bold> AUC, precision, recall, and F1 of TCR split, and <bold>b)</bold> epitope split.</p></caption>
<graphic xlink:href="536635v1_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Prediction performance of TCR-epitope binding affinity prediction models trained on different embeddings where negative samples are generated by random shuffling.</title>
<p>The average and standard deviation (error bar) of 10 trials are reported. <bold>a)</bold> AUC, precision, recall, and F1 of TCR split and <bold>b)</bold> epitope split.</p></caption>
<graphic xlink:href="536635v1_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>TCR-epitope binding affinity prediction performance of TCR split where negative samples are generated by random shuffling.</title>
<p>Average and standard deviation of 10 trials are reported. P-values are from two-sample t-tests between <monospace>catELMo</monospace> and the second best method (<underline>underlined</underline>).</p></caption>
<graphic xlink:href="536635v1_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>TCR-epitope binding affinity prediction performance of epitope split where negative samples are generated by random shuffling.</title>
<p>Average and standard deviation of 10 trials are reported. P-values are from two-sample t-tests between <monospace>catELMo</monospace> and the second best method (<underline>underlined</underline>).</p></caption>
<graphic xlink:href="536635v1_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><title>AUCs of TCR-epitope binding affinity prediction model trained on different portions of downstream <monospace>catELMo</monospace> embeddings where negative samples are generated by random shuffling.</title>
<p>The average and standard deviation of 10 trials are reported.</p></caption>
<graphic xlink:href="536635v1_tbls3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls4" orientation="portrait" position="float">
<label>Table 4.</label>
<caption><title>AUCs of TCR-epitope binding affinity prediction models trained on <monospace>catELMo</monospace> and BLOSUM62 embeddings.</title>
<p>The average and standard deviation of 10 trials are reported.</p></caption>
<graphic xlink:href="536635v1_tbls4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88837.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhao</surname>
<given-names>Siming</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Dartmouth College</institution>
</institution-wrap>
<city>Lebanon</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study provides a <bold>valuable</bold> tool for predicting binding between immune cells receptors and antigens based on protein sequence data. Its improvement over existing methods is supported by <bold>solid</bold> analysis, though more details on data, architectures and benchmarking are needed to fully justify this claim. This study will be of interest to immunologists and computational biologists.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88837.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this manuscript, the authors described a computational method catELMo for embedding TCR CDR3 sequences into numeric vectors using a deep-learning-based approach, ELMo. The authors applied catELMo to two applications: supervised TCR-epitope binding affinity prediction and unsupervised epitope-specific TCR clustering. In both applications, the authors showed that catELMo generated significantly better binding prediction and clustering performance than other established TCR embedding methods. However, there are a few major concerns that need to be addressed.</p>
<p>1. There are other TCR CDR3 embedding methods in addition to TCRBert. The authors may consider incorporating a few more methods in the evaluation, such as TESSA (PMCID: PMC7799492), DeepTCR (PMCID: PMC7952906) and the embedding method in ATM-TCR (reference 10 in the manuscript). TESSA is also the embedding method in pMTnet, which is another TCR-epitope binding prediction method and is the reference 12 mentioned in this manuscript.</p>
<p>2. The TCR training data for catELMo is obtained from ImmunoSEQ platform, including SARS-CoV2, EBV, CMV, and other disease samples. Meanwhile, antigens related to these diseases and their associated TCRs are extensively annotated in databases VDJdb, IEDB and McPAS-TCR. The authors then utilized the curated TCR-epitope pairs from these databases to conduct the evaluations for eptitope binding prediction and TCR clustering. Therefore, the training data for TCR embedding may already be implicitly tuned for better representations of the TCRs used in the evaluations. This seems to be true based on Table 4, as BERT-Base-TCR outperformed TCRBert. Could catELMo be trained on PIRD as TCRBert to demonstrate catELMo's embedding for TCRs targeting unseen diseases/epitopes?</p>
<p>3. In the application of TCR-epitope binding prediction, the authors mentioned that the model for embedding epitope sequences was catElMo, but how about for other methods, such as TCRBert? Do the other methods also use catELMo-embedded epitope sequences as part of the binding prediction model, or use their own model to embed the epitope sequences? Since the manuscript focuses on TCR embedding, it would be nice for other methods to be evaluated on the same epitope embedding (maybe adjusted to the same embedded vector length). Furthermore, the authors found that catELMo requires less training data to achieve better performance. So one would think the other methods could not learn a reasonable epitope embedding with limited epitope data, and catELMo's better performance in binding prediction is mainly due to better epitope representation.</p>
<p>4. In the epitope binding prediction evaluation, the authors generated the test data using TCR-epitope pairs from VDJdb, IEDB, McPAS, which may be dominated by epitopes from CMV. Could the authors show accuracy categorized by epitope types, i.e. the accuracy for TCR-CMV pair and accuracy for TCR-SARs-CoV2 separately?</p>
<p>5. In the unsupervised TCR clustering evaluation, since GIANA and TCRdist direct outputs the clustering result, so they should not be affected by hierarchical clusters. Why did the curves of GIANA and TCRdist change in Figure 4 when relaxing the hierarchical clustering threshold?</p>
<p>6. In the unsupervised TCR clustering evaluation, the authors examined the TCR related to the top eight epitopes. However, there are much more epitopes curated in VDJdb, IEDB and McPAS-TCR. In real application, the potential epitopes is also more complex than just eight epitopes. Could the authors evaluate the clustering result using all the TCR data from the databases?</p>
<p>7. In addition to NMI, it is important to know how specific each TCR cluster is. Could the authors add the fraction of pure clusters in the results? Pure cluster means all the TCRs in the cluster are binding to the same epitope, and is a metric used in the method GIANA.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88837.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In the manuscript, the authors highlighted the importance of T-cell receptor (TCR) analysis and the lack of amino acid embedding methods specific to this domain. The authors proposed a novel bi-directional context-aware amino acid embedding method, catELMo, adapted from ELMo (Embeddings from Language Models), specifically designed for TCR analysis. The model is trained on TCR sequences from seven projects in the ImmunoSEQ database, instead of the generic protein sequences. They assessed the effectiveness of the proposed method in both TCR-epitope binding affinity prediction, a supervised task, and the unsupervised TCR clustering task. The results demonstrate significant performance improvements compared to existing embedding models. The authors also aimed to provide and discuss their observations on embedding model design for TCR analysis: 1) Models specifically trained on TCR sequences have better performance than models trained on general protein sequences for the TCR-related tasks; and 2) The proposed ELMo-based method outperforms TCR embedding models with BERT-based architecture. The authors also provided a comprehensive introduction and investigation of existing amino acid embedding methods. Overall, the paper is well-written and well-organized.</p>
<p>The work has originality and has potential prospects for immune response analysis and immunotherapy exploration. TCR-epitope pair binding plays a significant role in T cell regulation. Accurate prediction and analysis of TCR sequences are crucial for comprehending the biological foundations of binding mechanisms and advancing immunotherapy approaches. The proposed embedding method presents an efficient context-aware mathematical representation for TCR sequences, enabling the capture and analysis of their structural and functional characteristics. This method serves as a valuable tool for various downstream analyses and is essential for a wide range of applications.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88837.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Here, the authors trained catElMo, a new context-aware embedding model for TCRβ CDR3 amino acid sequences for TCR-epitope specificity and clustering tasks. This method benchmarked existing work in protein and TCR language models and investigated the role that model architecture plays in the prediction performance. The major strength of this paper is comprehensively evaluating common model architectures used, which is useful for practitioners in the field. However, some key details were missing to assess whether the benchmarking study is a fair comparison between different architectures. Major comments are as follows:</p>
<p>- It is not clear why epitope sequences were also embedded using catELMo for the binding prediction task. Because catELMO is trained on TCRβ CDR3 sequences, it's not clear what benefit would come from this embedding. Were the other embedding models under comparison also applied to both the TCR and epitope sequences? It may be a fairer comparison if a single method is used to encode epitope sequence for all models under comparison, so that the performance reflects the quality of the TCR embedding only.</p>
<p>
- The tSNE visualization in Figure 3 is helpful. It makes sense that the last hidden layer features separate well by binding labels for the better performing models. However, it would be useful to know if positive and negative TCRs for each epitope group also separate well in the original TCR embedding space. In other words, how much separation between these groups is due to the neural network vs just the embedding?</p>
<p>
- To generate negative samples, the author randomly paired TCRs from healthy subjects to different epitopes. This could produce issues with false negatives if the epitopes used are common. Is there an estimate for how frequently there might be false negatives for those commonly occurring epitopes that most populations might also have been exposed to? Could there be a potential batch effect for the negative sampled TCR that confounds with the performance evaluation?</p>
<p>
- Most of the models being compared were trained on general proteins rather than TCR sequences. This makes their comparison to catELMO questionable since it's not clear if the improvement is due to the training data or architecture. The authors partially addressed this with BERT-based models in section 2.4. This concern would be more fully addressed if the authors also trained the Doc2vec model (Yang et al, Figure 2) on TCR sequences as baseline models instead of using the original models trained on general protein sequences. This would make clear the strength of context-aware embeddings if the performance is worse than catElmo and BERT.</p>
</body>
</sub-article>
</article>